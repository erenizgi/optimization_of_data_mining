{
    "author": "cyyever",
    "message": "Use torch.expm1 and torch.log1p for better numerical results (#40860)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "6254bb4a6811f2a6e87d973b27ed4e878b7ce6f4",
    "files": [
        {
            "sha": "e9054f609a77795e952d7bd748e064c2b5e8bae1",
            "filename": "src/transformers/activations.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6254bb4a6811f2a6e87d973b27ed4e878b7ce6f4/src%2Ftransformers%2Factivations.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6254bb4a6811f2a6e87d973b27ed4e878b7ce6f4/src%2Ftransformers%2Factivations.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Factivations.py?ref=6254bb4a6811f2a6e87d973b27ed4e878b7ce6f4",
            "patch": "@@ -204,9 +204,9 @@ def __init__(\n         with_vector_loads=False,\n     ):\n         super().__init__()\n-        self.alpha_p = nn.Parameter(torch.log(torch.exp(torch.tensor(alpha_p_init, dtype=dtype)) - 1).unsqueeze(0))\n+        self.alpha_p = nn.Parameter(torch.log(torch.expm1(torch.tensor(alpha_p_init, dtype=dtype))).unsqueeze(0))\n         self.alpha_n = nn.Parameter(\n-            torch.log(torch.exp(torch.tensor(alpha_n_init - beta, dtype=dtype)) - 1).unsqueeze(0)\n+            torch.log(torch.expm1(torch.tensor(alpha_n_init - beta, dtype=dtype))).unsqueeze(0)\n         )\n         self.register_buffer(\"beta\", torch.tensor(beta, dtype=dtype))\n         self.register_buffer(\"eps\", torch.tensor(eps, dtype=dtype))"
        },
        {
            "sha": "79e6d97ddbe74458c733596da6575375c4e6dcc5",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6254bb4a6811f2a6e87d973b27ed4e878b7ce6f4/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6254bb4a6811f2a6e87d973b27ed4e878b7ce6f4/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=6254bb4a6811f2a6e87d973b27ed4e878b7ce6f4",
            "patch": "@@ -338,7 +338,7 @@ def forward(\n         # Use temperature tuning from https://huggingface.co/papers/2501.19399) to NoROPE layers\n         if self.attn_temperature_tuning and not self.use_rope:\n             attn_scales = (\n-                torch.log(torch.floor((cache_position.float() + 1.0) / self.floor_scale) + 1.0) * self.attn_scale + 1.0\n+                torch.log1p(torch.floor((cache_position.float() + 1.0) / self.floor_scale)) * self.attn_scale + 1.0\n             )\n             attn_scales = attn_scales.view((1, input_shape[-1], 1, 1)).expand((*input_shape, 1, 1))  # batch size > 1\n             query_states = (query_states * attn_scales).to(query_states.dtype)"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}