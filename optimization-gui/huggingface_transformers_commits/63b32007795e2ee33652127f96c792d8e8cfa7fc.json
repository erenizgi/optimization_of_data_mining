{
    "author": "ydshieh",
    "message": "Use `--gpus all` in workflow files (#39752)\n\ngpu all\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "63b32007795e2ee33652127f96c792d8e8cfa7fc",
    "files": [
        {
            "sha": "59b4ce01d563b8a3a41c252ae1ab2c14d025595e",
            "filename": ".github/workflows/doctest_job.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63b32007795e2ee33652127f96c792d8e8cfa7fc/.github%2Fworkflows%2Fdoctest_job.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/63b32007795e2ee33652127f96c792d8e8cfa7fc/.github%2Fworkflows%2Fdoctest_job.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fdoctest_job.yml?ref=63b32007795e2ee33652127f96c792d8e8cfa7fc",
            "patch": "@@ -31,7 +31,7 @@ jobs:\n       group: aws-g5-4xlarge-cache\n     container:\n       image: huggingface/transformers-all-latest-gpu\n-      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     steps:\n       - name: Update clone\n         working-directory: /transformers"
        },
        {
            "sha": "9b4bca0ae59ad3bf2061a3625a2312d99eff1738",
            "filename": ".github/workflows/doctests.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/63b32007795e2ee33652127f96c792d8e8cfa7fc/.github%2Fworkflows%2Fdoctests.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/63b32007795e2ee33652127f96c792d8e8cfa7fc/.github%2Fworkflows%2Fdoctests.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fdoctests.yml?ref=63b32007795e2ee33652127f96c792d8e8cfa7fc",
            "patch": "@@ -18,7 +18,7 @@ jobs:\n       group: aws-g5-4xlarge-cache\n     container:\n       image: huggingface/transformers-all-latest-gpu\n-      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     outputs:\n       job_splits: ${{ steps.set-matrix.outputs.job_splits }}\n       split_keys: ${{ steps.set-matrix.outputs.split_keys }}"
        },
        {
            "sha": "67aedfc6c090233e9ecb88449e61948e3d2b53ba",
            "filename": ".github/workflows/self-push.yml",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/63b32007795e2ee33652127f96c792d8e8cfa7fc/.github%2Fworkflows%2Fself-push.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/63b32007795e2ee33652127f96c792d8e8cfa7fc/.github%2Fworkflows%2Fself-push.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-push.yml?ref=63b32007795e2ee33652127f96c792d8e8cfa7fc",
            "patch": "@@ -36,7 +36,7 @@ jobs:\n       group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-all-latest-gpu-push-ci\n-      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     outputs:\n       matrix: ${{ steps.set-matrix.outputs.matrix }}\n       test_map: ${{ steps.set-matrix.outputs.test_map }}\n@@ -136,7 +136,7 @@ jobs:\n       group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-all-latest-gpu-push-ci\n-      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     env:\n       # For the meaning of these environment variables, see the job `Setup`\n       CI_BRANCH_PUSH: ${{ github.event.ref }}\n@@ -362,7 +362,7 @@ jobs:\n       group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-pytorch-deepspeed-latest-gpu-push-ci\n-      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     env:\n       # For the meaning of these environment variables, see the job `Setup`\n       CI_BRANCH_PUSH: ${{ github.event.ref }}"
        },
        {
            "sha": "4b482c28fbb72d883c4c912545db05f0e3fb5471",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/63b32007795e2ee33652127f96c792d8e8cfa7fc/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/63b32007795e2ee33652127f96c792d8e8cfa7fc/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=63b32007795e2ee33652127f96c792d8e8cfa7fc",
            "patch": "@@ -55,7 +55,7 @@ jobs:\n       group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-all-latest-gpu\n-      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     outputs:\n       folder_slices: ${{ steps.set-matrix.outputs.folder_slices }}\n       slice_ids: ${{ steps.set-matrix.outputs.slice_ids }}\n@@ -219,7 +219,7 @@ jobs:\n       group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-all-latest-gpu\n-      options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n+      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n     steps:\n       - name: Update clone\n         working-directory: /transformers"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 7,
        "deletions": 7
    }
}