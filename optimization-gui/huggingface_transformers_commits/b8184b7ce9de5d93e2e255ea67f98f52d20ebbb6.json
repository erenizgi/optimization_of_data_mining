{
    "author": "remi-or",
    "message": "Make cache_config not mandatory (#40316)\n\n* Relaxed assumptions on cache_config\n\n* Review compliance\n\n* Style\n\n* Styyyle\n\n* Removed default and added args\n\n* Rebase mishapfix\n\n* Propagate args to TorchExportableModuleForDecoderOnlyLM\n\n* Fix the test I wanted  fixed in this PR\n\n* Added some AMD expectation related to cache tests",
    "sha": "b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6",
    "files": [
        {
            "sha": "cd9b12847f3aa5c22f9fb3ae7c779aa56ea5cc02",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 63,
            "deletions": 38,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6",
            "patch": "@@ -201,7 +201,10 @@ class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):\n     def __init__(\n         self,\n         model: PreTrainedModel,\n-    ):\n+        batch_size: Optional[int] = None,\n+        max_cache_len: Optional[int] = None,\n+        device: Optional[torch.device] = None,\n+    ) -> None:\n         \"\"\"\n         Initializes the exportable module.\n \n@@ -214,20 +217,19 @@ def __init__(\n         super().__init__()\n \n         config = model.config.get_text_config()\n-        _generation_config = model.generation_config\n \n         if not hasattr(config, \"use_cache\") or config.use_cache is False:\n             raise ValueError(\"The model must have caching enabled to be performant.\")\n \n         if hasattr(config, \"layer_types\") and getattr(config, \"sliding_window\", None) is not None:\n-            self.model = TorchExportableModuleWithHybridCache(model)\n+            self.model = TorchExportableModuleWithHybridCache(model, batch_size, max_cache_len, device)\n         else:\n             # If `layer_types` is not specified explicitly in the config or `sliding_window` is null,\n             # there is only 1 type of layers, so export will use `StaticCache` by default.\n             logging.info(\n                 \"Using `StaticCache` for export as `layer_types` is not specified or `sliding_window` is `null` in the config.\"\n             )\n-            self.model = TorchExportableModuleWithStaticCache(model)\n+            self.model = TorchExportableModuleWithStaticCache(model, batch_size, max_cache_len, device)\n         # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n         ALL_MASK_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_mask_without_vmap)\n         ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n@@ -471,17 +473,27 @@ class TorchExportableModuleWithStaticCache(torch.nn.Module):\n     def __init__(\n         self,\n         model: PreTrainedModel,\n-    ):\n+        batch_size: Optional[int] = None,\n+        max_cache_len: Optional[int] = None,\n+        device: Optional[torch.device] = None,\n+    ) -> None:\n         \"\"\"\n         Initializes the wrapper module with the pretrained model.\n \n         Args:\n             model (`PreTrainedModel`): The pretrained model to wrap. The model must have caching\n                 enabled and use a 'static' caching implementation.\n+            batch_size (`Optional[int]`): The batch size of the model. If not provided, we check if a value can be found\n+                in `generation_config.cache_config` and otherwise we raise a ValueError.\n+            max_cache_len (`Optional[int]`): The maximum cache length for generation. Same mechanism as `batch_size` if\n+                not provided.\n+            device (`Optional[torch.device]`): The device to use. If not provided, we check if a value can be found\n+                in `generation_config.cache_config` and otherwise we use `model.device` (no error is raised).\n \n         Raises:\n             AssertionError: If the pretrained model does not have caching enabled or if it does\n             not use a 'static' caching implementation in `model.generation_config`.\n+            ValueError: If `batch_size` or `max_cache_len` is not provided, either as an argument or in `cache_config`.\n         \"\"\"\n         super().__init__()\n \n@@ -494,16 +506,6 @@ def __init__(\n                 \"The model must have a generation config to be exported with static caching. \"\n                 \"Please set `generation_config` in `model`.\"\n             )\n-        if \"batch_size\" not in generation_config.cache_config:\n-            raise ValueError(\n-                \"The model's generation config must specify a batch_size in its cache_config. \"\n-                'Try GenerationConfig( ... cache_config={\"batch_size\": 1, ...} ...)'\n-            )\n-        if \"max_cache_len\" not in generation_config.cache_config:\n-            raise ValueError(\n-                \"The model's generation config must specify a max_cache_len in its cache_config. \"\n-                'Try GenerationConfig( ... cache_config={\"max_cache_len\": 4096, ...} ...)'\n-            )\n         if not generation_config.use_cache:\n             raise AssertionError(\n                 \"The model must have caching enabled to be exported with static caching. \"\n@@ -515,15 +517,26 @@ def __init__(\n                 \"Please set `generation_config.cache_implementation='static'`.\"\n             )\n \n+        cache_config = {} if generation_config.cache_config is None else generation_config.cache_config\n+\n+        # Ensure batch_size and max_cache_len are set\n+        if batch_size is None:\n+            batch_size = cache_config.get(\"batch_size\", None)\n+            if batch_size is None:\n+                raise ValueError(\"batch_size must be provided, either as an argument or in cache_config.\")\n+        if max_cache_len is None:\n+            max_cache_len = cache_config.get(\"max_cache_len\", None)\n+            if max_cache_len is None:\n+                raise ValueError(\"max_cache_len must be provided, either as an argument or in cache_config.\")\n+        # Infer device if not provided\n+        if device is None:\n+            device = cache_config.get(\"device\", model.device)\n+\n+        # Initialize the static cache\n         self.model = model\n-        self.static_cache = StaticCache(\n-            max_cache_len=generation_config.cache_config.get(\"max_cache_len\"),\n-            config=config,\n-        )\n-        batch_size = generation_config.cache_config.get(\"batch_size\")\n+        self.static_cache = StaticCache(max_cache_len=max_cache_len, config=config)\n         head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n         num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n-        device = generation_config.cache_config.get(\"device\")\n         dtype = self.model.dtype\n         # We need this call to initialize all the layers (otherwise it's done lazily, which is not exportable)\n         self.static_cache.early_initialization(batch_size, num_heads, head_dim, dtype, device)\n@@ -639,48 +652,60 @@ class TorchExportableModuleWithHybridCache(torch.nn.Module):\n     def __init__(\n         self,\n         model: PreTrainedModel,\n-    ):\n+        batch_size: Optional[int] = None,\n+        max_cache_len: Optional[int] = None,\n+        device: Optional[torch.device] = None,\n+    ) -> None:\n         \"\"\"\n         Initializes the exportable module.\n \n         Args:\n             model (`PreTrainedModel`): The pretrained model to wrap.\n-\n+            batch_size (`Optional[int]`): The batch size of the model. If not provided, we check if a value can be found\n+                in `generation_config.cache_config` and otherwise we raise a ValueError.\n+            max_cache_len (`Optional[int]`): The maximum cache length for generation. Same mechanism as `batch_size` if\n+                not provided.\n+            device (`Optional[torch.device]`): The device to use. If not provided, we check if a value can be found\n+                in `generation_config.cache_config` and otherwise we use `model.device` (no error is raised).\n         Raises:\n-            AssertionError: If the model doesn't have the expected configuration for an hybrid StaticCache.\n+            AssertionError: If the model doesn't have the expected configuration for hybrid StaticCache.\n+            ValueError: If `batch_size` or `max_cache_len` is not provided, either as an argument or in `cache_config`.\n         \"\"\"\n         super().__init__()\n         self.model = model\n         config = model.config.get_text_config()\n         generation_config = model.generation_config\n \n+        # Sanity checks\n         if generation_config is None:\n             raise AssertionError(\n                 \"The model must have a generation config to be exported with static caching. \"\n                 \"Please set `generation_config` in `model`.\"\n             )\n-        if \"batch_size\" not in generation_config.cache_config:\n-            raise ValueError(\n-                \"The model's generation config must specify a batch_size in its cache_config. \"\n-                'Try GenerationConfig( ... cache_config={\"batch_size\": 1, ...} ...)'\n-            )\n-        if \"max_cache_len\" not in generation_config.cache_config:\n-            raise ValueError(\n-                \"The model's generation config must specify a max_cache_len in its cache_config. \"\n-                'Try GenerationConfig( ... cache_config={\"max_cache_len\": 4096, ...} ...)'\n-            )\n         if not config.use_cache:\n             raise AssertionError(\"Model must have caching enabled.\")\n \n+        cache_config = {} if generation_config.cache_config is None else generation_config.cache_config\n+        # Ensure batch_size and max_cache_len are set\n+        if batch_size is None:\n+            batch_size = cache_config.get(\"batch_size\", None)\n+            if batch_size is None:\n+                raise ValueError(\"batch_size must be provided, either as an argument or in cache_config.\")\n+        if max_cache_len is None:\n+            max_cache_len = cache_config.get(\"max_cache_len\", None)\n+            if max_cache_len is None:\n+                raise ValueError(\"max_cache_len must be provided, either as an argument or in cache_config.\")\n+        # Infer device if not provided\n+        if device is None:\n+            device = cache_config.get(\"device\", model.device)\n+\n         # Initialize the cache\n-        self.cache = StaticCache(config=config, max_cache_len=generation_config.cache_config.get(\"max_cache_len\"))\n+        self.cache = StaticCache(config=config, max_cache_len=max_cache_len)\n         head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n         num_heads = getattr(config, \"num_key_value_heads\", config.num_attention_heads)\n-        max_batch_size = generation_config.cache_config.get(\"batch_size\")\n-        device = generation_config.cache_config.get(\"device\")\n         dtype = self.model.dtype\n         # We need this call to initialize all the layers (otherwise it's done lazily, which is not exportable)\n-        self.cache.early_initialization(max_batch_size, num_heads, head_dim, dtype, device)\n+        self.cache.early_initialization(batch_size, num_heads, head_dim, dtype, device)\n \n         # Register all key and value cache tensors as buffers\n         for i in range(len(self.cache)):"
        },
        {
            "sha": "097c82a0e5a0c9731a90d0134b91b9a1f0ce1371",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6",
            "patch": "@@ -416,6 +416,9 @@ def test_export_static_cache(self):\n                 (\"cuda\", 8): [\n                     \"Hello I am doing a project on the 1990s and I need to know what the most popular music was in the 1990s. I have been looking on the internet and I have\"\n                 ],\n+                (\"rocm\", (9, 5)): [\n+                    \"Hello I am doing a project on the 1990s and I need to know what the most popular music was in the 1990s. I have been looking on the internet and I have\"\n+                ],\n             }\n         )\n         EXPECTED_TEXT_COMPLETION = expectations.get_expectation()"
        },
        {
            "sha": "423640cb31b15900642690e0e652dbda0ef0b342",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6",
            "patch": "@@ -255,6 +255,9 @@ def test_export_static_cache(self):\n                 (\"cuda\", 8): [\n                     \"Hello I am doing a project for my class and I am having trouble with the code. I am trying to make a\"\n                 ],\n+                (\"rocm\", (9, 5)): [\n+                    \"Hello I am doing a project for my school and I need to know how to make a program that will take a number\"\n+                ],\n             }\n         )\n         EXPECTED_TEXT_COMPLETION = EXPECTED_TEXT_COMPLETIONS.get_expectation()\n@@ -320,7 +323,7 @@ def test_export_hybrid_cache(self):\n \n         # Export + hybrid cache\n         model.eval()\n-        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model, batch_size=1, max_cache_len=1024)\n         exported_program = exportable_module.export(\n             input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n             cache_position=torch.tensor([0], dtype=torch.long, device=model.device),"
        },
        {
            "sha": "99ea97561f788c3726b4fc5a08b44f4018793ea2",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6",
            "patch": "@@ -733,7 +733,7 @@ def test_export_text_only_with_hybrid_cache(self):\n \n         # Export + hybrid cache\n         model.eval()\n-        exportable_module = TorchExportableModuleForDecoderOnlyLM(model)\n+        exportable_module = TorchExportableModuleForDecoderOnlyLM(model, batch_size=1, max_cache_len=1024)\n         exported_program = exportable_module.export(\n             input_ids=torch.tensor([[1]], dtype=torch.long, device=model.device),\n             cache_position=torch.tensor([0], dtype=torch.long, device=model.device),"
        },
        {
            "sha": "c3961947192ab70fde24d11baaf40d24be75bdfb",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=b8184b7ce9de5d93e2e255ea67f98f52d20ebbb6",
            "patch": "@@ -257,7 +257,7 @@ def test_export_static_cache(self):\n                 \"My favourite condiment is 100% natural, organic, gluten free, vegan, and vegetarian. I love to use\"\n             ],\n             (\"rocm\", (9, 5)): [\n-                \"My favourite condiment is 100% natural, organic and vegan. I love to use it in my cooking, but\"\n+                \"My favourite condiment is 100% natural, organic, gluten free, vegan, and vegetarian. I love to use\"\n             ]\n         })  # fmt: off\n         EXPECTED_TEXT_COMPLETION = expected_text_completions.get_expectation()"
        }
    ],
    "stats": {
        "total": 113,
        "additions": 72,
        "deletions": 41
    }
}