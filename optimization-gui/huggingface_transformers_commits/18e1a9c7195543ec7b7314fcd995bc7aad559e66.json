{
    "author": "ylacombe",
    "message": "Fix parametrization-based weight norm (#33275)\n\n* refactor weight_norm + propose uniformed solution to reconcile meta load_state_dict with classic loading\r\n\r\n* make style\r\n\r\n* fix sew\r\n\r\n* fix sew and sew_d tests",
    "sha": "18e1a9c7195543ec7b7314fcd995bc7aad559e66",
    "files": [
        {
            "sha": "d406976663605399a5654f6b35ac36b303acb535",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 25,
            "deletions": 8,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -818,7 +818,6 @@ def _move_model_to_meta(model, loaded_state_dict_keys, start_prefix):\n def _load_state_dict_into_meta_model(\n     model,\n     state_dict,\n-    loaded_state_dict_keys,  # left for now but could be removed, see below\n     start_prefix,\n     expected_keys,\n     device_map=None,\n@@ -847,8 +846,6 @@ def _load_state_dict_into_meta_model(\n     # - deepspeed zero 3 support\n     # - need to copy metadata if any - see _load_state_dict_into_model\n     # - handling error_msgs - mimicking the error handling in module._load_from_state_dict()\n-    # - Is there a situation where some keys aren't in `loaded_state_dict_keys` and in which case\n-    #   they won't get loaded.\n \n     error_msgs = []\n \n@@ -868,6 +865,18 @@ def _load_state_dict_into_meta_model(\n             # We add only the first key as an example\n             new_key = key.replace(\"beta\", \"bias\")\n             renamed_beta[key] = new_key if not renamed_beta else renamed_beta\n+\n+        # To reproduce `_load_state_dict_into_model` behaviour, we need to manually rename parametrized weigth norm, if necessary.\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            if \"weight_g\" in key:\n+                new_key = key.replace(\"weight_g\", \"parametrizations.weight.original0\")\n+            if \"weight_v\" in key:\n+                new_key = key.replace(\"weight_v\", \"parametrizations.weight.original1\")\n+        else:\n+            if \"parametrizations.weight.original0\" in key:\n+                new_key = key.replace(\"parametrizations.weight.original0\", \"weight_g\")\n+            if \"parametrizations.weight.original1\" in key:\n+                new_key = key.replace(\"parametrizations.weight.original1\", \"weight_v\")\n         if new_key:\n             old_keys.append(key)\n             new_keys.append(new_key)\n@@ -884,8 +893,7 @@ def _load_state_dict_into_meta_model(\n     is_torch_e4m3fn_available = hasattr(torch, \"float8_e4m3fn\")\n \n     for param_name, param in state_dict.items():\n-        # First part of the test is always true as load_state_dict_keys always contains state_dict keys.\n-        if param_name not in loaded_state_dict_keys or param_name not in expected_keys:\n+        if param_name not in expected_keys:\n             continue\n \n         if param_name.startswith(start_prefix):\n@@ -4132,6 +4140,18 @@ def _fix_key(key):\n                 return key.replace(\"beta\", \"bias\")\n             if \"gamma\" in key:\n                 return key.replace(\"gamma\", \"weight\")\n+\n+            # to avoid logging parametrized weight norm renaming\n+            if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+                if \"weight_g\" in key:\n+                    return key.replace(\"weight_g\", \"parametrizations.weight.original0\")\n+                if \"weight_v\" in key:\n+                    return key.replace(\"weight_v\", \"parametrizations.weight.original1\")\n+            else:\n+                if \"parametrizations.weight.original0\" in key:\n+                    return key.replace(\"parametrizations.weight.original0\", \"weight_g\")\n+                if \"parametrizations.weight.original1\" in key:\n+                    return key.replace(\"parametrizations.weight.original1\", \"weight_v\")\n             return key\n \n         original_loaded_keys = loaded_keys\n@@ -4376,7 +4396,6 @@ def _find_mismatched_keys(\n                 error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n                     model_to_load,\n                     state_dict,\n-                    loaded_keys,\n                     start_prefix,\n                     expected_keys,\n                     device_map=device_map,\n@@ -4453,7 +4472,6 @@ def _find_mismatched_keys(\n                         new_error_msgs, offload_index, state_dict_index = _load_state_dict_into_meta_model(\n                             model_to_load,\n                             state_dict,\n-                            loaded_keys,\n                             start_prefix,\n                             expected_keys,\n                             device_map=device_map,\n@@ -4609,7 +4627,6 @@ def _load_pretrained_model_low_mem(\n         error_msgs = _load_state_dict_into_meta_model(\n             model,\n             state_dict,\n-            loaded_state_dict_keys,\n             start_prefix,\n             expected_keys=expected_keys,\n             hf_quantizer=hf_quantizer,"
        },
        {
            "sha": "549f98b59dda6437b2f7a2e4d35c2bfcae3883ce",
            "filename": "src/transformers/models/dac/modeling_dac.py",
            "status": "modified",
            "additions": 24,
            "deletions": 20,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdac%2Fmodeling_dac.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -494,33 +494,37 @@ def _init_weights(self, module):\n             nn.init.constant_(module.bias, 0)\n \n     def apply_weight_norm(self):\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n         for layer in self.quantizer.quantizers:\n-            nn.utils.weight_norm(layer.in_proj)\n-            nn.utils.weight_norm(layer.out_proj)\n+            weight_norm(layer.in_proj)\n+            weight_norm(layer.out_proj)\n \n-        nn.utils.weight_norm(self.encoder.conv1)\n-        nn.utils.weight_norm(self.encoder.conv2)\n+        weight_norm(self.encoder.conv1)\n+        weight_norm(self.encoder.conv2)\n \n         for layer in self.encoder.block:\n-            nn.utils.weight_norm(layer.conv1)\n-            nn.utils.weight_norm(layer.res_unit1.conv1)\n-            nn.utils.weight_norm(layer.res_unit1.conv2)\n-            nn.utils.weight_norm(layer.res_unit2.conv1)\n-            nn.utils.weight_norm(layer.res_unit2.conv2)\n-            nn.utils.weight_norm(layer.res_unit3.conv1)\n-            nn.utils.weight_norm(layer.res_unit3.conv2)\n+            weight_norm(layer.conv1)\n+            weight_norm(layer.res_unit1.conv1)\n+            weight_norm(layer.res_unit1.conv2)\n+            weight_norm(layer.res_unit2.conv1)\n+            weight_norm(layer.res_unit2.conv2)\n+            weight_norm(layer.res_unit3.conv1)\n+            weight_norm(layer.res_unit3.conv2)\n \n-        nn.utils.weight_norm(self.decoder.conv1)\n-        nn.utils.weight_norm(self.decoder.conv2)\n+        weight_norm(self.decoder.conv1)\n+        weight_norm(self.decoder.conv2)\n \n         for layer in self.decoder.block:\n-            nn.utils.weight_norm(layer.conv_t1)\n-            nn.utils.weight_norm(layer.res_unit1.conv1)\n-            nn.utils.weight_norm(layer.res_unit1.conv2)\n-            nn.utils.weight_norm(layer.res_unit2.conv1)\n-            nn.utils.weight_norm(layer.res_unit2.conv2)\n-            nn.utils.weight_norm(layer.res_unit3.conv1)\n-            nn.utils.weight_norm(layer.res_unit3.conv2)\n+            weight_norm(layer.conv_t1)\n+            weight_norm(layer.res_unit1.conv1)\n+            weight_norm(layer.res_unit1.conv2)\n+            weight_norm(layer.res_unit2.conv1)\n+            weight_norm(layer.res_unit2.conv2)\n+            weight_norm(layer.res_unit3.conv1)\n+            weight_norm(layer.res_unit3.conv2)\n \n     def remove_weight_norm(self):\n         for layer in self.quantizer.quantizers:"
        },
        {
            "sha": "28ccb9513d63d8e607f374857b6d670b76457731",
            "filename": "src/transformers/models/encodec/modeling_encodec.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fencodec%2Fmodeling_encodec.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -103,8 +103,12 @@ def __init__(\n             )\n \n         self.conv = nn.Conv1d(in_channels, out_channels, kernel_size, stride, dilation=dilation)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n         if self.norm_type == \"weight_norm\":\n-            self.conv = nn.utils.weight_norm(self.conv)\n+            self.conv = weight_norm(self.conv)\n         elif self.norm_type == \"time_group_norm\":\n             self.norm = nn.GroupNorm(1, out_channels)\n \n@@ -186,8 +190,13 @@ def __init__(self, config, in_channels: int, out_channels: int, kernel_size: int\n             )\n \n         self.conv = nn.ConvTranspose1d(in_channels, out_channels, kernel_size, stride)\n+\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n         if config.norm_type == \"weight_norm\":\n-            self.conv = nn.utils.weight_norm(self.conv)\n+            self.conv = weight_norm(self.conv)\n         elif config.norm_type == \"time_group_norm\":\n             self.norm = nn.GroupNorm(1, out_channels)\n "
        },
        {
            "sha": "1e1900d38afdc35c4b139745800dcdc1c61c2601",
            "filename": "src/transformers/models/fastspeech2_conformer/modeling_fastspeech2_conformer.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffastspeech2_conformer%2Fmodeling_fastspeech2_conformer.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -1416,10 +1416,14 @@ def get_padding(self, kernel_size, dilation=1):\n         return (kernel_size * dilation - dilation) // 2\n \n     def apply_weight_norm(self):\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n         for layer in self.convs1:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n         for layer in self.convs2:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n \n     def remove_weight_norm(self):\n         for layer in self.convs1:\n@@ -1493,12 +1497,16 @@ def _init_weights(self, module):\n                 module.bias.data.zero_()\n \n     def apply_weight_norm(self):\n-        nn.utils.weight_norm(self.conv_pre)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n+        weight_norm(self.conv_pre)\n         for layer in self.upsampler:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n         for layer in self.resblocks:\n             layer.apply_weight_norm()\n-        nn.utils.weight_norm(self.conv_post)\n+        weight_norm(self.conv_post)\n \n     def remove_weight_norm(self):\n         nn.utils.remove_weight_norm(self.conv_pre)"
        },
        {
            "sha": "ba8230ec509df86e57c9ccd9609bfc5ee1dd22f5",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -2361,10 +2361,14 @@ def get_padding(self, kernel_size, dilation=1):\n         return (kernel_size * dilation - dilation) // 2\n \n     def apply_weight_norm(self):\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n         for layer in self.convs1:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n         for layer in self.convs2:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n \n     def remove_weight_norm(self):\n         for layer in self.convs1:\n@@ -2633,12 +2637,16 @@ def _init_weights(self, module):\n                 module.weight.data[module.padding_idx].zero_()\n \n     def apply_weight_norm(self):\n-        nn.utils.weight_norm(self.hifi_gan.conv_pre)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n+        weight_norm(self.hifi_gan.conv_pre)\n         for layer in self.hifi_gan.upsampler:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n         for layer in self.hifi_gan.resblocks:\n             layer.apply_weight_norm()\n-        nn.utils.weight_norm(self.hifi_gan.conv_post)\n+        weight_norm(self.hifi_gan.conv_post)\n \n     def remove_weight_norm(self):\n         nn.utils.remove_weight_norm(self.hifi_gan.conv_pre)"
        },
        {
            "sha": "2d1fde8eed69a0a4ef1bdbcfce889118e0ed52fc",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -2608,10 +2608,14 @@ def get_padding(self, kernel_size, dilation=1):\n         return (kernel_size * dilation - dilation) // 2\n \n     def apply_weight_norm(self):\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n         for layer in self.convs1:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n         for layer in self.convs2:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n \n     def remove_weight_norm(self):\n         for layer in self.convs1:\n@@ -2889,12 +2893,16 @@ def _init_weights(self, module):\n \n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.apply_weight_norm\n     def apply_weight_norm(self):\n-        nn.utils.weight_norm(self.hifi_gan.conv_pre)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n+        weight_norm(self.hifi_gan.conv_pre)\n         for layer in self.hifi_gan.upsampler:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n         for layer in self.hifi_gan.resblocks:\n             layer.apply_weight_norm()\n-        nn.utils.weight_norm(self.hifi_gan.conv_post)\n+        weight_norm(self.hifi_gan.conv_post)\n \n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TCodeHifiGan.remove_weight_norm\n     def remove_weight_norm(self):"
        },
        {
            "sha": "c9a3494b88b4866834d8c39084e63825dbd12e63",
            "filename": "src/transformers/models/sew/modeling_sew.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew%2Fmodeling_sew.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -274,11 +274,15 @@ def __init__(self, config):\n             stride=config.squeeze_factor,\n         )\n \n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n         if is_deepspeed_zero3_enabled():\n             import deepspeed\n \n             with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n-                self.conv = nn.utils.weight_norm(self.conv, name=\"weight\", dim=2)\n+                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n             if hasattr(self.conv, \"parametrizations\"):\n                 weight_g = self.conv.parametrizations.weight.original0\n                 weight_v = self.conv.parametrizations.weight.original1\n@@ -288,7 +292,7 @@ def __init__(self, config):\n             deepspeed.zero.register_external_parameter(self, weight_v)\n             deepspeed.zero.register_external_parameter(self, weight_g)\n         else:\n-            self.conv = nn.utils.weight_norm(self.conv, name=\"weight\", dim=2)\n+            self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n \n         self.padding = SEWSamePadLayer(config.num_conv_pos_embeddings)\n         self.activation = ACT2FN[config.feat_extract_activation]"
        },
        {
            "sha": "7f3db54defc1fd82aec5a4c1b6b2b7c731ca2361",
            "filename": "src/transformers/models/sew_d/modeling_sew_d.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsew_d%2Fmodeling_sew_d.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -349,11 +349,15 @@ def __init__(self, config):\n             stride=config.squeeze_factor,\n         )\n \n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n         if is_deepspeed_zero3_enabled():\n             import deepspeed\n \n             with deepspeed.zero.GatheredParameters(self.conv.weight, modifier_rank=0):\n-                self.conv = nn.utils.weight_norm(self.conv, name=\"weight\", dim=2)\n+                self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n             if hasattr(self.conv, \"parametrizations\"):\n                 weight_g = self.conv.parametrizations.weight.original0\n                 weight_v = self.conv.parametrizations.weight.original1\n@@ -363,7 +367,7 @@ def __init__(self, config):\n             deepspeed.zero.register_external_parameter(self, weight_v)\n             deepspeed.zero.register_external_parameter(self, weight_g)\n         else:\n-            self.conv = nn.utils.weight_norm(self.conv, name=\"weight\", dim=2)\n+            self.conv = weight_norm(self.conv, name=\"weight\", dim=2)\n \n         self.padding = SEWDSamePadLayer(config.num_conv_pos_embeddings)\n         self.activation = ACT2FN[config.feat_extract_activation]"
        },
        {
            "sha": "790e6a74a47135c15dee9aa94ec6286743e4a399",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 13,
            "deletions": 5,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -3234,10 +3234,14 @@ def get_padding(self, kernel_size, dilation=1):\n         return (kernel_size * dilation - dilation) // 2\n \n     def apply_weight_norm(self):\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n         for layer in self.convs1:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n         for layer in self.convs2:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n \n     def remove_weight_norm(self):\n         for layer in self.convs1:\n@@ -3310,12 +3314,16 @@ def _init_weights(self, module):\n                 module.bias.data.zero_()\n \n     def apply_weight_norm(self):\n-        nn.utils.weight_norm(self.conv_pre)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n+        weight_norm(self.conv_pre)\n         for layer in self.upsampler:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n         for layer in self.resblocks:\n             layer.apply_weight_norm()\n-        nn.utils.weight_norm(self.conv_post)\n+        weight_norm(self.conv_post)\n \n     def remove_weight_norm(self):\n         nn.utils.remove_weight_norm(self.conv_pre)"
        },
        {
            "sha": "a780e54538f213a708df23fd6cef60e5623c31e4",
            "filename": "src/transformers/models/univnet/modeling_univnet.py",
            "status": "modified",
            "additions": 29,
            "deletions": 9,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Funivnet%2Fmodeling_univnet.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -87,8 +87,12 @@ def forward(self, hidden_states: torch.FloatTensor):\n         return hidden_states + residual\n \n     def apply_weight_norm(self):\n-        nn.utils.weight_norm(self.conv1)\n-        nn.utils.weight_norm(self.conv2)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n+        weight_norm(self.conv1)\n+        weight_norm(self.conv2)\n \n     def remove_weight_norm(self):\n         nn.utils.remove_weight_norm(self.conv1)\n@@ -197,11 +201,15 @@ def forward(self, spectrogram: torch.FloatTensor):\n         return kernels, biases\n \n     def apply_weight_norm(self):\n-        nn.utils.weight_norm(self.input_conv)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n+        weight_norm(self.input_conv)\n         for layer in self.resblocks:\n             layer.apply_weight_norm()\n-        nn.utils.weight_norm(self.kernel_conv)\n-        nn.utils.weight_norm(self.bias_conv)\n+        weight_norm(self.kernel_conv)\n+        weight_norm(self.bias_conv)\n \n     def remove_weight_norm(self):\n         nn.utils.remove_weight_norm(self.input_conv)\n@@ -328,7 +336,11 @@ def location_variable_convolution(\n         return output_hidden_states\n \n     def apply_weight_norm(self):\n-        nn.utils.weight_norm(self.conv)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n+        weight_norm(self.conv)\n \n     def remove_weight_norm(self):\n         nn.utils.remove_weight_norm(self.conv)\n@@ -398,7 +410,11 @@ def forward(self, hidden_states: torch.FloatTensor, spectrogram: torch.FloatTens\n         return hidden_states\n \n     def apply_weight_norm(self):\n-        nn.utils.weight_norm(self.convt_pre)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n+        weight_norm(self.convt_pre)\n         self.kernel_predictor.apply_weight_norm()\n         for layer in self.resblocks:\n             layer.apply_weight_norm()\n@@ -619,10 +635,14 @@ def _init_weights(self, module):\n                 module.bias.data.zero_()\n \n     def apply_weight_norm(self):\n-        nn.utils.weight_norm(self.conv_pre)\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n+        weight_norm(self.conv_pre)\n         for layer in self.resblocks:\n             layer.apply_weight_norm()\n-        nn.utils.weight_norm(self.conv_post)\n+        weight_norm(self.conv_post)\n \n     def remove_weight_norm(self):\n         nn.utils.remove_weight_norm(self.conv_pre)"
        },
        {
            "sha": "23bc8a72f8ba374b364fe17a59bf47c37821ddbd",
            "filename": "src/transformers/models/vits/modeling_vits.py",
            "status": "modified",
            "additions": 11,
            "deletions": 3,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -461,10 +461,14 @@ def get_padding(self, kernel_size, dilation=1):\n         return (kernel_size * dilation - dilation) // 2\n \n     def apply_weight_norm(self):\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n         for layer in self.convs1:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n         for layer in self.convs2:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n \n     def remove_weight_norm(self):\n         for layer in self.convs1:\n@@ -521,8 +525,12 @@ def __init__(self, config: VitsConfig):\n             self.cond = nn.Conv1d(config.speaker_embedding_size, config.upsample_initial_channel, 1)\n \n     def apply_weight_norm(self):\n+        weight_norm = nn.utils.weight_norm\n+        if hasattr(nn.utils.parametrizations, \"weight_norm\"):\n+            weight_norm = nn.utils.parametrizations.weight_norm\n+\n         for layer in self.upsampler:\n-            nn.utils.weight_norm(layer)\n+            weight_norm(layer)\n         for layer in self.resblocks:\n             layer.apply_weight_norm()\n "
        },
        {
            "sha": "852f87c8f58a57aa36cd8c02230e41acfcae5faa",
            "filename": "tests/models/sew/test_modeling_sew.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew%2Ftest_modeling_sew.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -420,6 +420,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 uniform_init_parms = [\n+                    \"conv.parametrizations.weight\",\n                     \"conv.weight\",\n                     \"masked_spec_embed\",\n                     \"quantizer.weight_proj.weight\","
        },
        {
            "sha": "34374eb1e0e63b110ed88c6f32c3970e7c90b21d",
            "filename": "tests/models/sew_d/test_modeling_sew_d.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/18e1a9c7195543ec7b7314fcd995bc7aad559e66/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/18e1a9c7195543ec7b7314fcd995bc7aad559e66/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsew_d%2Ftest_modeling_sew_d.py?ref=18e1a9c7195543ec7b7314fcd995bc7aad559e66",
            "patch": "@@ -422,6 +422,7 @@ def test_initialization(self):\n             model = model_class(config=configs_no_init)\n             for name, param in model.named_parameters():\n                 uniform_init_parms = [\n+                    \"conv.parametrizations.weight\",\n                     \"conv.weight\",\n                     \"masked_spec_embed\",\n                     \"quantizer.weight_proj.weight\","
        }
    ],
    "stats": {
        "total": 232,
        "additions": 166,
        "deletions": 66
    }
}