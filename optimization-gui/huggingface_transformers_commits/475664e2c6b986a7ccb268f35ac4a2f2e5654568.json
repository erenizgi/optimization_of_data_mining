{
    "author": "faaany",
    "message": "[tests] remove cuda-only test marker in `AwqConfigTest`  (#37032)\n\n* enable on xpu\n\n* add xpu support\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "475664e2c6b986a7ccb268f35ac4a2f2e5654568",
    "files": [
        {
            "sha": "8b29cfec1a69e2317a7f8c23269512b227bf257c",
            "filename": "src/transformers/utils/quantization_config.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/475664e2c6b986a7ccb268f35ac4a2f2e5654568/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/475664e2c6b986a7ccb268f35ac4a2f2e5654568/src%2Ftransformers%2Futils%2Fquantization_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fquantization_config.py?ref=475664e2c6b986a7ccb268f35ac4a2f2e5654568",
            "patch": "@@ -893,12 +893,13 @@ def post_init(self):\n \n         if self.backend == AwqBackendPackingMethod.LLMAWQ:\n             # Only cuda device can run this function\n-            if not torch.cuda.is_available():\n-                raise ValueError(\"LLM-AWQ backend is only supported on CUDA\")\n-            compute_capability = torch.cuda.get_device_capability()\n-            major, minor = compute_capability\n-            if major < 8:\n-                raise ValueError(\"LLM-AWQ backend is only supported on GPUs with compute capability >= 8.0\")\n+            if not (torch.cuda.is_available() or torch.xpu.is_available()):\n+                raise ValueError(\"LLM-AWQ backend is only supported on CUDA and XPU\")\n+            if torch.cuda.is_available():\n+                compute_capability = torch.cuda.get_device_capability()\n+                major, minor = compute_capability\n+                if major < 8:\n+                    raise ValueError(\"LLM-AWQ backend is only supported on CUDA GPUs with compute capability >= 8.0\")\n \n         if self.do_fuse and self.fuse_max_seq_len is None:\n             raise ValueError("
        },
        {
            "sha": "913c6636b1028b7ee829fbdcdac27095b109197c",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/475664e2c6b986a7ccb268f35ac4a2f2e5654568/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/475664e2c6b986a7ccb268f35ac4a2f2e5654568/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=475664e2c6b986a7ccb268f35ac4a2f2e5654568",
            "patch": "@@ -41,7 +41,6 @@\n \n @require_torch_accelerator\n class AwqConfigTest(unittest.TestCase):\n-    @require_torch_gpu\n     def test_wrong_backend(self):\n         \"\"\"\n         Simple test that checks if a user passes a wrong backend an error is raised\n@@ -59,13 +58,15 @@ def test_wrong_backend(self):\n         with self.assertRaises(ValueError):\n             AwqConfig(bits=4, backend=\"unexisting-backend\")\n \n-        # Only cuda device can run this function\n+        # Only cuda and xpu devices can run this function\n         support_llm_awq = False\n         if torch.cuda.is_available():\n             compute_capability = torch.cuda.get_device_capability()\n             major, minor = compute_capability\n             if major >= 8:\n                 support_llm_awq = True\n+        elif torch.xpu.is_available():\n+            support_llm_awq = True\n \n         if support_llm_awq:\n             # LLMAWQ should work on an A100"
        }
    ],
    "stats": {
        "total": 18,
        "additions": 10,
        "deletions": 8
    }
}