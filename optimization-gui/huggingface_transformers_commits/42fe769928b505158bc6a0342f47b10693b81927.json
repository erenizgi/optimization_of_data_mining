{
    "author": "ahadnagy",
    "message": "SmolVLM test fixes (#40275)\n\n* Fix SmolVLM tests\n\n* Add the proper CUDA expectations as well\n\n* Split 'A10 and A100 expectations\n\n* Ruff\n\n---------\n\nCo-authored-by: Akos Hadnagy <akoshuggingface@mi325x8-123.atl1.do.cpe.ice.amd.com>",
    "sha": "42fe769928b505158bc6a0342f47b10693b81927",
    "files": [
        {
            "sha": "5abaee3eac5e553a00d0c1c265751b70d7130359",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/42fe769928b505158bc6a0342f47b10693b81927/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42fe769928b505158bc6a0342f47b10693b81927/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=42fe769928b505158bc6a0342f47b10693b81927",
            "patch": "@@ -128,11 +128,7 @@ def export_text_decoder(self):\n         \"\"\"Export the text decoder component.\"\"\"\n \n         # Create text decoder exportable wrapper\n-        self.exportable_text_decoder = TorchExportableModuleForDecoderOnlyLM(\n-            model=self.text_decoder,\n-            max_batch_size=self.max_batch_size,\n-            max_cache_len=self.max_cache_len,\n-        )\n+        self.exportable_text_decoder = TorchExportableModuleForDecoderOnlyLM(model=self.text_decoder)\n \n         # Use the existing text decoder exportable wrapper\n         seq_length = 3"
        },
        {
            "sha": "f732d7734a18e978bf101fa90ed26d9a5627dc51",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/42fe769928b505158bc6a0342f47b10693b81927/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42fe769928b505158bc6a0342f47b10693b81927/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=42fe769928b505158bc6a0342f47b10693b81927",
            "patch": "@@ -27,7 +27,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n-from ...generation import GenerationMixin\n+from ...generation import GenerationConfig, GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -844,6 +844,7 @@ def __init__(self, config):\n         self.image_token_id = self.config.image_token_id\n         self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n         self.vocab_size = config.text_config.vocab_size\n+        self.model.text_model.generation_config = GenerationConfig.from_model_config(config)\n \n         # Initialize weights and apply final processing\n         self.post_init()"
        },
        {
            "sha": "3e26dd68fee769309e4d4699261d08e3328fd48f",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/42fe769928b505158bc6a0342f47b10693b81927/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42fe769928b505158bc6a0342f47b10693b81927/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=42fe769928b505158bc6a0342f47b10693b81927",
            "patch": "@@ -20,6 +20,7 @@\n from torch import nn\n \n from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, logging\n@@ -341,6 +342,7 @@ class SmolVLMForConditionalGeneration(Idefics3ForConditionalGeneration):\n     def __init__(self, config):\n         super().__init__(config)\n         self.model = SmolVLMModel(config)\n+        self.model.text_model.generation_config = GenerationConfig.from_model_config(config)\n         self.lm_head = nn.Linear(config.text_config.hidden_size, config.text_config.vocab_size, bias=False)\n         self.post_init()\n "
        },
        {
            "sha": "16a2a02c33d9da0216d7aff77952d1353d6dc898",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 27,
            "deletions": 1,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/42fe769928b505158bc6a0342f47b10693b81927/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42fe769928b505158bc6a0342f47b10693b81927/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=42fe769928b505158bc6a0342f47b10693b81927",
            "patch": "@@ -27,6 +27,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    Expectations,\n     cleanup,\n     is_flaky,\n     require_torch,\n@@ -44,6 +45,7 @@\n     import torch\n \n     from transformers import (\n+        GenerationConfig,\n         SmolVLMConfig,\n         SmolVLMForConditionalGeneration,\n         SmolVLMModel,\n@@ -595,7 +597,19 @@ def test_integration_test_video(self):\n         generated_ids = model.generate(**inputs, max_new_tokens=20)\n         generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n \n-        expected_generated_text = 'User: You are provided the following series of nine frames from a 0:00:09 [H:MM:SS] video.\\n\\nFrame from 00:00:\\nFrame from 00:01:\\nFrame from 00:02:\\nFrame from 00:03:\\nFrame from 00:04:\\nFrame from 00:05:\\nFrame from 00:06:\\nFrame from 00:08:\\nFrame from 00:09:\\n\\nDescribe this video in detail\\nAssistant: The video depicts a large language model architecture, specifically a language model with a \"quick brown\" feature'  # fmt: skip\n+        expected_generated_strings = Expectations(\n+            {\n+                (None, None): 'User: You are provided the following series of nine frames from a 0:00:09 [H:MM:SS] video.\\n\\nFrame from 00:00:\\nFrame from 00:01:\\nFrame from 00:02:\\nFrame from 00:03:\\nFrame from 00:04:\\nFrame from 00:05:\\nFrame from 00:06:\\nFrame from 00:08:\\nFrame from 00:09:\\n\\nDescribe this video in detail\\nAssistant: The video depicts a large language model architecture, specifically a language model with a \"quick brown\" feature',  # fmt: skip\n+                (\"cuda\", (8, 0)): 'User: You are provided the following series of nine frames from a 0:00:09 [H:MM:SS] video.\\n\\nFrame from 00:00:\\nFrame from 00:01:\\nFrame from 00:02:\\nFrame from 00:03:\\nFrame from 00:04:\\nFrame from 00:05:\\nFrame from 00:06:\\nFrame from 00:08:\\nFrame from 00:09:\\n\\nDescribe this video in detail\\nAssistant: The video showcases a large language model architecture, specifically a \"Quick Brown\" model, which is designed',  # fmt: skip\n+                (\"cuda\", (8, 6)): 'User: You are provided the following series of nine frames from a 0:00:09 [H:MM:SS] video.\\n\\nFrame from 00:00:\\nFrame from 00:01:\\nFrame from 00:02:\\nFrame from 00:03:\\nFrame from 00:04:\\nFrame from 00:05:\\nFrame from 00:06:\\nFrame from 00:08:\\nFrame from 00:09:\\n\\nDescribe this video in detail\\nAssistant: The video showcases a large language model, specifically a neural network model, which is designed to learn and',  # fmt: skip\n+                (\"rocm\", None): 'User: You are provided the following series of nine frames from a 0:00:09 [H:MM:SS] video.\\n\\nFrame from 00:00:\\nFrame from 00:01:\\nFrame from 00:02:\\nFrame from 00:03:\\nFrame from 00:04:\\nFrame from 00:05:\\nFrame from 00:06:\\nFrame from 00:08:\\nFrame from 00:09:\\n\\nDescribe this video in detail\\nAssistant: The video showcases a large language model architecture, specifically a \"Quick Brown\" model, which is designed',  # fmt: skip\n+            }\n+        )  # fmt: skip\n+\n+        expected_generated_text = expected_generated_strings.get_expectation()\n+\n+        print(f\"Generated text: {generated_texts[0]}\")\n+\n         self.assertEqual(generated_texts[0], expected_generated_text)\n \n     @slow\n@@ -661,13 +675,25 @@ def test_export_smolvlm_text_decoder(self):\n         config.text_config.use_cache = True\n         config.text_config.attn_implementation = \"sdpa\"\n \n+        generation_config = GenerationConfig(\n+            use_cache=True,\n+            cache_implementation=\"static\",\n+            max_length=1234,\n+            cache_config={\n+                \"batch_size\": 1,\n+                \"max_cache_len\": 1234,\n+            },\n+        )\n+\n         # Load the model and extract the text decoder\n         model = SmolVLMForConditionalGeneration.from_pretrained(\n             model_id,\n             torch_dtype=torch.float32,\n             config=config,\n         )\n \n+        model.model.text_model.generation_config = generation_config\n+\n         text_decoder = model.model.text_model\n         text_decoder.eval()\n "
        }
    ],
    "stats": {
        "total": 39,
        "additions": 32,
        "deletions": 7
    }
}