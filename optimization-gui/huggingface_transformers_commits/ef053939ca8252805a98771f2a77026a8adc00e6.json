{
    "author": "remi-or",
    "message": "Fixes for continuous batching (#40828)\n\n* Fix for CB attn mask and refactor\n\n* Tests for CB (not all passing)\n\n* Passing tests and a logger fix\n\n* Fixed the KV metrics that were broken when we moved to hybrid alloc\n\n* Fix circular import and style\n\n* Added tests for FA\n\n* Unfolded test to have device expectations\n\n* Fixes for H100\n\n* more fixes for h100\n\n* H100 are good\n\n* Style\n\n* Adding some comments from #40831\n\n* Rename test\n\n* Avoid 1 letter variables\n\n* Dictonnary is only removed during kwargs\n\n* Test for supported sample\n\n* Fix a unvoluntary slice\n\n* Fixes for non-sliced inputs and small example improvments\n\n* Slice inputs is more understandabe\n\n* Style",
    "sha": "ef053939ca8252805a98771f2a77026a8adc00e6",
    "files": [
        {
            "sha": "2b0d506eb8957edeaf7626862d15a8f218b79ee2",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef053939ca8252805a98771f2a77026a8adc00e6/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef053939ca8252805a98771f2a77026a8adc00e6/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=ef053939ca8252805a98771f2a77026a8adc00e6",
            "patch": "@@ -187,18 +187,20 @@ def batch_generate(\n         \"--attn\", type=str, default=\"paged_attention|kernels-community/flash-attn\", help=\"Attention implementation\"\n     )\n     parser.add_argument(\"--matmul-precision\", \"-mp\", type=str, default=\"high\")  # set to \"none\" to disable\n-    parser.add_argument(\"--slice-inputs\", action=\"store_true\", default=False)\n-    parser.add_argument(\"--use-cuda-graph\", action=\"store_true\", default=False)\n-    parser.add_argument(\"--compile\", action=\"store_true\", default=False)\n+    parser.add_argument(\"--no-slice-inputs\", action=\"store_true\")  # slicing is enabled by default because much faster\n+    parser.add_argument(\"--use-cuda-graph\", \"-cg\", action=\"store_true\")\n+    parser.add_argument(\"--compile\", action=\"store_true\")\n \n     parser.add_argument(\"--samples\", type=int, default=500)\n     parser.add_argument(\"--displayed\", type=int, default=0, help=\"Number of samples to display\")\n     parser.add_argument(\"--output-file\", type=str, default=None)\n-    parser.add_argument(\"--compare\", action=\"store_true\", default=False)\n-    parser.add_argument(\"--metrics\", action=\"store_true\", default=False)\n+    parser.add_argument(\"--compare\", action=\"store_true\")\n+    parser.add_argument(\"--metrics\", action=\"store_true\")\n     parser.add_argument(\"--profile\", type=str, default=None)\n     args = parser.parse_args()\n \n+    args.slice_inputs = not args.no_slice_inputs\n+\n     # If turned on, we setup metrics\n     if args.metrics:\n         setup_metrics()"
        },
        {
            "sha": "05de093f661f2668c714e3d929cb9b80b71e341b",
            "filename": "src/transformers/generation/continuous_batching/cache.py",
            "status": "modified",
            "additions": 18,
            "deletions": 8,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache.py?ref=ef053939ca8252805a98771f2a77026a8adc00e6",
            "patch": "@@ -198,7 +198,7 @@ def __init__(\n         # Add the inferred attributes to the class\n         self.num_blocks = num_blocks\n         self.max_batch_tokens = max_batch_tokens\n-        logger.warning(\n+        logger.info(\n             f\"PagedAttentionCache initialized with {self.num_blocks = }, {self.block_size = }, {page_size = }, \"\n             f\"{self.max_batch_tokens = } {num_attention_masks = }\"\n         )\n@@ -253,7 +253,7 @@ def get_num_free_blocks(self) -> int:\n         return len(self._free_blocks)\n \n     @traced\n-    def get_read_indices(\n+    def extend_read_indices(\n         self, request_id: str, past_length: int, query_length: int, read_index: list[list[int]]\n     ) -> None:\n         \"\"\"Retrieve physical cache indices for reading KV states in the cache across all layer groups. This method\n@@ -264,7 +264,7 @@ def get_read_indices(\n             read_indices.extend(indices)\n \n     @traced\n-    def get_write_indices(\n+    def extend_write_indices(\n         self, request_id: str, past_length: int, query_length: int, write_index: list[list[int]]\n     ) -> None:\n         \"\"\"Retrieve physical cache indices for writing new KV states to the cache across all layer groups. This method\n@@ -274,6 +274,16 @@ def get_write_indices(\n             indices = cm.get_write_indices(request_id, past_length, query_length)\n             write_indices.extend(indices)\n \n+    @traced\n+    def get_seqlens_k(self, request_id: str, past_length: int, query_length: int) -> dict[str, int]:\n+        \"\"\"Retrieve the key sequence length for the given request_id across all layer types. Returns a dictionary of\n+        layer types to their corresponding key sequence lengths.\"\"\"\n+        seqlens_k = {}\n+        for cm in self.group_cache_managers:\n+            attn_type, seqlen_k = cm.get_seqlens_k(request_id, past_length, query_length)\n+            seqlens_k[attn_type] = seqlen_k\n+        return seqlens_k\n+\n     @traced\n     def update(\n         self,\n@@ -471,7 +481,7 @@ def compute_num_blocks_and_max_batch_tokens(\n         b = 2 * (self.group_size * self.page_size * cache_dtype.itemsize + 2 * self.num_groups)\n         b += m * (self.peak_activation_per_token * self._activation_dtype.itemsize + 28 + 4 * self.num_groups)\n         c = -cache_memory\n-        logger.info(f\"Coefficients of 2nd degree polynomial: {a = }, {b = }, {c = }\")\n+        logger.debug(f\"Coefficients of 2nd degree polynomial: {a = }, {b = }, {c = }\")\n \n         # Compute discriminant and greatest solution\n         discriminant = b**2 - 4 * a * c\n@@ -485,11 +495,11 @@ def compute_num_blocks_and_max_batch_tokens(\n         num_pages = floor(greatest_solution)\n         num_blocks = num_pages // self.block_size\n         if num_blocks > self._upper_bound_num_blocks:\n-            logger.warning(f\"{num_blocks = } is too large, setting to {self._upper_bound_num_blocks = }\")\n+            logger.info(f\"{num_blocks = } is too large, setting to {self._upper_bound_num_blocks = }\")\n             num_blocks = self._upper_bound_num_blocks\n         max_batch_tokens = int(greatest_solution * m)\n         if max_batch_tokens > self._upper_bound_max_batch_tokens:\n-            logger.warning(f\"{max_batch_tokens = } is too large, setting to {self._upper_bound_max_batch_tokens = }\")\n+            logger.info(f\"{max_batch_tokens = } is too large, setting to {self._upper_bound_max_batch_tokens = }\")\n             max_batch_tokens = self._upper_bound_max_batch_tokens\n         return num_blocks, max_batch_tokens\n \n@@ -517,7 +527,7 @@ def compute_max_batch_tokens(\n         # Compute max batch tokens and return\n         max_batch_tokens = floor(num / denum)\n         if max_batch_tokens > self._upper_bound_max_batch_tokens:\n-            logger.warning(f\"{max_batch_tokens = } is too large, setting to {self._upper_bound_max_batch_tokens = }\")\n+            logger.info(f\"{max_batch_tokens = } is too large, setting to {self._upper_bound_max_batch_tokens = }\")\n             max_batch_tokens = self._upper_bound_max_batch_tokens\n         return max_batch_tokens\n \n@@ -545,7 +555,7 @@ def compute_num_blocks(\n         num_pages = floor(num / denum)\n         num_blocks = num_pages // self.block_size\n         if num_blocks > self._upper_bound_num_blocks:\n-            logger.warning(f\"{num_blocks = } is too large, setting to {self._upper_bound_num_blocks = }\")\n+            logger.info(f\"{num_blocks = } is too large, setting to {self._upper_bound_num_blocks = }\")\n             num_blocks = self._upper_bound_num_blocks\n         return num_blocks\n "
        },
        {
            "sha": "7e2d4f2b553278f3dcbf3b7d4a46b6a0f2f9f0dd",
            "filename": "src/transformers/generation/continuous_batching/cache_manager.py",
            "status": "modified",
            "additions": 15,
            "deletions": 0,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcache_manager.py?ref=ef053939ca8252805a98771f2a77026a8adc00e6",
            "patch": "@@ -53,6 +53,11 @@ def get_write_indices(self, request_id: str, past_length: int, query_length: int\n         \"\"\"Returns the physical indices of where to write request_id's cache in the cache tensor.\"\"\"\n         pass\n \n+    @abstractmethod\n+    def get_seqlens_k(self, request_id: str, past_length: int, query_length: int) -> tuple[str, int]:\n+        \"\"\"Returns the attention type of the cache allocator and the key sequence length for the given request_id.\"\"\"\n+        pass\n+\n \n class FullAttentionCacheAllocator(CacheAllocator):\n     \"\"\"Cache manager for a group of full attention layers.\"\"\"\n@@ -108,6 +113,11 @@ def get_write_indices(self, request_id: str, past_length: int, query_length: int\n             physical_indices.append(physical_index)\n         return physical_indices\n \n+    def get_seqlens_k(self, request_id: str, past_length: int, query_length: int) -> tuple[str, int]:\n+        \"\"\"Returns the attention type of the cache allocator and the key sequence length for the given request_id.\"\"\"\n+        seqlens_k = past_length + query_length\n+        return \"full_attention\", seqlens_k\n+\n \n class SlidingAttentionCacheAllocator(CacheAllocator):\n     \"\"\"Cache manager for sliding window attention layers.\"\"\"\n@@ -191,6 +201,11 @@ def get_write_indices(self, request_id: str, past_length: int, query_length: int\n             physical_indices = [-1] * padding_length + physical_indices\n         return physical_indices\n \n+    def get_seqlens_k(self, request_id: str, past_length: int, query_length: int) -> tuple[str, int]:\n+        \"\"\"Returns the attention type of the cache allocator and the key sequence length for the given request_id.\"\"\"\n+        seqlens_k = query_length + min(past_length, self.sliding_window - 1)\n+        return \"sliding_attention\", seqlens_k\n+\n \n # TODO: test the impact of this\n # def get_read_indices(self, request_id: str, past_length: int) -> list[int]:"
        },
        {
            "sha": "b00c0a4825c368896aa3d288adaf7b15dc82d672",
            "filename": "src/transformers/generation/continuous_batching/continuous_api.py",
            "status": "modified",
            "additions": 181,
            "deletions": 136,
            "changes": 317,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Fcontinuous_api.py?ref=ef053939ca8252805a98771f2a77026a8adc00e6",
            "patch": "@@ -19,7 +19,7 @@\n from functools import partial\n from itertools import count\n from time import perf_counter\n-from typing import Optional\n+from typing import Optional, Union\n \n import torch\n from torch import nn\n@@ -102,11 +102,11 @@ def __init__(\n         streaming: bool = False,\n         manual_eviction: bool = False,\n         slice_inputs: bool = True,  # TODO: There should be an heuristic to decide on slicing, compile, cuda graphs...\n-    ):\n+    ) -> None:\n         \"\"\"Initialize the continuous batch processor.\n \n         Args:\n-            cache: The paged attention cache to use\n+            cache: A [`PagedAttentionCache`] object\n             config: The model configuration\n             generation_config: The generation configuration\n             input_queue: Queue for incoming requests\n@@ -147,97 +147,129 @@ def __init__(\n         self.total_batch_size = 0\n         self.setup_static_tensors(cache.num_groups)\n \n-    def return_attention_mask(self) -> bool:\n-        return self.config._attn_implementation != \"paged_attention\"  # we set `is_causal` to True in paged call\n-\n     @traced(standalone=True)\n-    def setup_static_tensors(self, num_groups: int):\n+    def setup_static_tensors(self, num_groups: int) -> None:\n         T = self.max_batch_tokens\n         num_pages = self.cache.num_blocks * self.cache.block_size\n-        tensor_metadata = {\"dtype\": torch.int32, \"device\": self.model_device}\n-        self.tensor_metadata = tensor_metadata\n-        self.input_ids = torch.empty((1, T), **tensor_metadata)\n-        self.position_ids = torch.empty((1, T), **tensor_metadata)\n-        self.cumulative_seqlens_q = torch.empty((T + 1,), **tensor_metadata)\n+        self.tensor_metadata = {\"dtype\": torch.int32, \"device\": self.model_device}\n+\n+        # Some tensors always have the same shape regardless of the model\n+        self.input_ids = torch.empty((1, T), **self.tensor_metadata)\n+        self.position_ids = torch.empty((1, T), **self.tensor_metadata)\n+        self.cumulative_seqlens_q = torch.empty((T + 1,), **self.tensor_metadata)\n+        self.max_seqlen_q = 0\n+        self.logits_indices = torch.empty((T,), **self.tensor_metadata)\n+        self.output_ids = torch.empty((1, T), **self.tensor_metadata)\n+\n+        # For some kwargs, we have a dict of tensors with as many items as there are attention types\n+        layer_types = getattr(self.config, \"layer_types\", None)\n+        if layer_types is None:\n+            sliding_window = getattr(self.config, \"sliding_window\", 1)\n+            layer_types = [\"full_attention\"] if sliding_window in [1, None] else [\"sliding_attention\"]\n+        layer_types = list(set(layer_types))\n+\n         self.cumulative_seqlens_k = {\n-            \"full_attention\": torch.empty((T + 1), **tensor_metadata),\n-            \"sliding_attention\": torch.empty((T + 1), **tensor_metadata),\n-            # TODO: can be generalized using layer types, for block-attn for instance\n+            layer_type: torch.empty((T + 1), **self.tensor_metadata) for layer_type in layer_types\n         }\n+        self.max_seqlen_k = dict.fromkeys(layer_types, 0)\n \n-        # There is one read and write index tensor per group\n-        self.write_index_tensors = [torch.empty((T,), **tensor_metadata) for _ in range(num_groups)]\n-        self.read_index_tensors = [torch.empty((num_pages + T), **tensor_metadata) for _ in range(num_groups)]\n-        # +T is because there are -1 for seqlen_q when model uses a sliding window\n-\n-        self.logits_indices = torch.empty((T,), **tensor_metadata)\n-        self.max_seqlen_q = 0\n-        self.max_seqlen_k = {\"full_attention\": 0, \"sliding_attention\": 0}\n-        self.output_ids = torch.empty((1, T), **tensor_metadata)\n-        # Since attenention_mask is not always needed, we only allocate it if it is\n         if self.return_attention_mask():\n-            # TODO: this could be 2 iff model is hybrid, and then we can also change memory handler to account for it\n-            size_0 = 1 if self.sliding_window == 1 else 2\n-            self.attention_mask = torch.empty(\n-                (size_0, 1, T, num_pages), dtype=self.model_dtype, device=self.model_device\n-            )\n+            attn_mask_kwargs = {\n+                \"size\": (1, 1, T, num_pages + T),\n+                \"dtype\": self.model_dtype,\n+                \"device\": self.model_device,\n+            }\n+            self.attention_mask = {layer_type: torch.empty(**attn_mask_kwargs) for layer_type in layer_types}\n         else:\n-            logger.warning(f\"Attention mask is not needed for {self.config._attn_implementation}\")\n             self.attention_mask = None\n+\n+        # For other kwargs, we need a list of tensors with as many tensors as there are groups\n+        self.write_index_storage = [torch.empty((T,), **self.tensor_metadata) for _ in range(num_groups)]\n+        self.read_index_storage = [torch.empty((num_pages + T), **self.tensor_metadata) for _ in range(num_groups)]\n+        # For read index, the +T is because there are -1 for seqlen_q when model uses a sliding window\n+\n+        # After allocating empty tensors, we reset them to the right value\n         self.reset_static_tensors(full_reset=True)\n \n+    def return_attention_mask(self) -> bool:\n+        return self.config._attn_implementation != \"paged_attention\"  # we set `is_causal` to True in paged call\n+\n     @traced\n     @torch.no_grad()\n     def reset_static_tensors(self, full_reset: bool = False):\n         \"\"\"Reset static tensors for the next batch. In between batches, reset only the parts that were used in the last\n         batch, but for initialisation, we can reset everything using the (full_reset) flag.\"\"\"\n         # Compute the slice to reset\n-        t = self.total_query_length if self.slice_inputs and not full_reset else self.write_index_tensors[0].size(-1)\n-        c = self.total_key_length if self.slice_inputs and not full_reset else self.read_index_tensors[0].size(-1)\n-        b = self.total_batch_size if self.slice_inputs and not full_reset else self.write_index_tensors[0].size(0)\n-        # Reset the tensors\n-        self.input_ids[:, :t].zero_()\n-        self.position_ids[:, :t].zero_()\n-        self.cumulative_seqlens_q[: b + 1].zero_()\n+        if full_reset or not self.slice_inputs:\n+            q_len = self.write_index_storage[0].size(-1)\n+            k_len = self.read_index_storage[0].size(-1)\n+            b_size = self.write_index_storage[0].size(0)\n+        else:\n+            q_len = self.total_query_length\n+            k_len = self.total_key_length\n+            b_size = self.total_batch_size\n+\n+        # Reset the attributes that always have the same shape\n+        self.input_ids[:, :q_len].zero_()\n+        self.position_ids[:, :q_len].zero_()\n+        self.cumulative_seqlens_q[: b_size + 1].zero_()\n+        self.max_seqlen_q = 0\n+        self.logits_indices[:q_len].fill_(-1)\n+        self.output_ids[:, :q_len].fill_(-1)\n+\n+        # Reset the attributes that are either tensors or dict of tensors\n         for layer_type in self.cumulative_seqlens_k:\n-            self.cumulative_seqlens_k[layer_type][: b + 1].zero_()\n+            self.cumulative_seqlens_k[layer_type][: b_size + 1].zero_()\n             self.max_seqlen_k[layer_type] = 0\n+            if self.attention_mask is not None:\n+                self.attention_mask[layer_type][:, :, :q_len, :k_len].fill_(torch.finfo(self.model_dtype).min)\n+\n+        # Reset the attributes that are lists of tensors\n         for i in range(self.cache.num_groups):\n-            self.write_index_tensors[i][:t].fill_(-1)\n-            self.read_index_tensors[i][: t + c].fill_(-1)\n-        self.logits_indices[:t].fill_(-1)\n-        self.max_seqlen_q = 0\n-        self.output_ids[:, :t].fill_(-1)\n-        if self.attention_mask is not None:\n-            self.attention_mask[:, :, :t, :c].fill_(torch.finfo(self.model_dtype).min)\n+            self.write_index_storage[i][:q_len].fill_(-1)\n+            self.read_index_storage[i][: q_len + k_len].fill_(-1)\n \n     def get_model_kwargs(self) -> PagedAttentionArgs:\n         \"\"\"Get model keyword arguments for the current batch.\"\"\"\n         # Compute the slice to return\n-        t = self.total_query_length if self.slice_inputs else self.write_index.size(-1)\n-        b = self.total_batch_size\n-        # Prepare the kwargs\n+        q_len = self.total_query_length if self.slice_inputs else self.write_index_storage[0].size(-1)\n+        b_size = self.total_batch_size if self.slice_inputs else self.cumulative_seqlens_q.size(-1) - 1\n+\n+        # Prepare the kwargs, the attributes that are either tensors or dict of tensors are initialized to empty dicts\n         kwargs = {\n-            \"input_ids\": self.input_ids[:, :t],\n-            \"position_ids\": self.position_ids[:, :t],\n-            \"cu_seq_lens_q\": self.cumulative_seqlens_q[: b + 1],\n+            \"input_ids\": self.input_ids[:, :q_len],\n+            \"position_ids\": self.position_ids[:, :q_len],\n+            \"cu_seq_lens_q\": self.cumulative_seqlens_q[: b_size + 1],\n+            \"max_seqlen_q\": self.max_seqlen_q,\n+            \"logits_indices\": self.logits_indices[:q_len],\n             \"cu_seq_lens_k\": {},\n+            \"max_seqlen_k\": {},\n+            \"attention_mask\": {},\n             \"read_index\": self.read_index,  # slicing is done during building\n             \"write_index\": self.write_index,  # slicing is done during building\n-            \"logits_indices\": self.logits_indices[:t],\n-            \"max_seqlen_q\": self.max_seqlen_q,\n-            \"max_seqlen_k\": self.max_seqlen_k,\n             \"cache\": self.cache,\n             \"use_cache\": False,\n         }\n-        for layer_type in self.cumulative_seqlens_k:\n-            kwargs[\"cu_seq_lens_k\"][layer_type] = self.cumulative_seqlens_k[layer_type][: b + 1]\n-        # If the attention mask is not None, we slice it as the others\n-        if self.attention_mask is not None:\n-            kwargs[\"attention_mask\"] = {}\n-            for layer_type, seqlens_k in kwargs[\"cu_seq_lens_k\"].items():\n-                kwargs[\"attention_mask\"][layer_type] = self.attention_mask[:1, :, :t, : seqlens_k[-1]]\n+\n+        # For the attributes that are dict of tensors, we replace the dict with a tensor if there is only one entry\n+        layer_types = list(self.cumulative_seqlens_k.keys())\n+        if len(layer_types) > 1:\n+            for layer_type, seqlens_k in self.cumulative_seqlens_k.items():\n+                kwargs[\"cu_seq_lens_k\"][layer_type] = seqlens_k[: b_size + 1]\n+                kwargs[\"max_seqlen_k\"][layer_type] = self.max_seqlen_k[layer_type]\n+                if self.attention_mask is not None:\n+                    k_len = seqlens_k[b_size] if self.slice_inputs else self.attention_mask[layer_type].size(-1)\n+                    kwargs[\"attention_mask\"][layer_type] = self.attention_mask[layer_type][..., :q_len, :k_len]\n         else:\n+            layer_type = layer_types[0]\n+            kwargs[\"cu_seq_lens_k\"] = self.cumulative_seqlens_k[layer_type][: b_size + 1]\n+            kwargs[\"max_seqlen_k\"] = self.max_seqlen_k[layer_type]\n+            if self.attention_mask is not None:\n+                k_len = self.cumulative_seqlens_k[layer_type][b_size]\n+                k_len = k_len if self.slice_inputs else self.attention_mask[layer_type].size(-1)\n+                kwargs[\"attention_mask\"] = self.attention_mask[layer_type][..., :q_len, :k_len]\n+\n+        if self.attention_mask is None:\n             kwargs[\"attention_mask\"] = None\n         return kwargs\n \n@@ -283,75 +315,75 @@ def _handle_request_error(self, error, state: RequestState):\n \n     @traced\n     def prepare_next_batch(self) -> bool:\n-        \"\"\"Prepare tensors and metadata for the next model forward pass.\"\"\"\n-        # Get new requests from the queue\n+        \"\"\"Prepare tensors and metadata for the next model forward pass. Returns True if there are requests to process,\n+        False otherwise.\"\"\"\n+\n+        # Get new requests from the queue, stop if there are no pending requests\n         self._get_new_requests()\n         self.scheduler.clear_cancelled_requests()\n         if not self.scheduler.has_pending_requests():\n             return False\n-\n         self.metrics.record_queue_metrics(len(self.scheduler.active_requests), len(self.scheduler.waiting_requests))\n \n+        # Schedule the next batch of requests, stop if there are no requests in the batch\n         self.requests_in_batch = self.scheduler.schedule_batch(self.max_batch_tokens)\n         if not self.requests_in_batch:\n             return False\n-\n-        # Get the request objects for this batch\n-        self.reset_static_tensors()  # TOOD: with slice_inputs, this might be unnecessary\n-        position_ids = []\n-        input_ids = []\n-        read_index = [[] for _ in range(self.cache.num_groups)]\n-        write_index = [[] for _ in range(self.cache.num_groups)]\n-        cumulative_seqlens_q = [0]\n-        cumulative_seqlens_k = {\"full_attention\": [0], \"sliding_attention\": [0]}\n-        logits_indices = []\n         self.metrics.record_batch_metrics(self.requests_in_batch)\n \n+        # Reset the static tensors used for storage\n+        self.reset_static_tensors()  # TODO: with slice_inputs, this might be unnecessary\n+\n+        # Prepare accumulators\n         self.total_query_length = 0\n         self.total_key_length = 0\n         self.total_batch_size = 0\n \n+        input_ids = []\n+        position_ids = []\n+        cumulative_seqlens_q = [0]\n+        logits_indices = []\n+\n+        if isinstance(self.cumulative_seqlens_k, dict):\n+            cumulative_seqlens_k = {layer_type: [0] for layer_type in self.cumulative_seqlens_k}\n+        else:\n+            cumulative_seqlens_k = [0]\n+\n+        read_index = [[] for _ in range(self.cache.num_groups)]\n+        write_index = [[] for _ in range(self.cache.num_groups)]\n+\n+        # Go through all the requests in the batch\n         for state in self.requests_in_batch:\n-            next_input_ids = state.prompt_ids\n-            input_ids.extend(next_input_ids)\n+            # First we retrieve the lengths related to the request\n             past_length = state.position_offset\n-            query_length = len(next_input_ids)\n-            key_length = query_length + past_length\n+            query_length = len(state.prompt_ids)\n+            seqlens_k = self.cache.get_seqlens_k(state.request_id, past_length, query_length)\n \n+            # Then we update the total lengths that are used for slicing\n             self.total_query_length += query_length\n-            self.total_key_length += key_length\n+            # total_key_length is used to slice the keys so we need to take the max of all the key lengths\n+            self.total_key_length += max(seqlens_k.values())\n             self.total_batch_size += 1\n+            # And the attribute tracking the position in the request object\n+            state.position_offset += query_length\n \n-            positions_to_add = list(range(past_length, key_length))\n-            self.cache.get_read_indices(state.request_id, past_length, query_length, read_index)\n-            self.cache.get_write_indices(state.request_id, past_length, query_length, write_index)\n-\n-            position_ids.extend(positions_to_add)\n+            # Then we accumulate for the object used in the kwargs\n+            input_ids.extend(state.prompt_ids)\n+            position_ids.extend(range(past_length, past_length + query_length))\n             cumulative_seqlens_q.append(cumulative_seqlens_q[-1] + query_length)\n+            self.max_seqlen_q = max(self.max_seqlen_q, query_length)\n \n-            cumulative_seqlens_k[\"full_attention\"].append(\n-                cumulative_seqlens_k[\"full_attention\"][-1] + query_length + past_length\n-            )\n-            cumulative_seqlens_k[\"sliding_attention\"].append(\n-                cumulative_seqlens_k[\"sliding_attention\"][-1]\n-                + query_length\n-                + min(past_length, self.sliding_window - 1)\n-            )\n-\n-            if len(state.remaining_prompt_ids) == 0:\n+            if not state.remaining_prompt_ids:\n                 logits_indices.append(cumulative_seqlens_q[-1] - 1)\n-            self.max_seqlen_q = max(self.max_seqlen_q, query_length)\n-            self.max_seqlen_k[\"full_attention\"] = max(self.max_seqlen_k[\"full_attention\"], query_length + past_length)\n-            self.max_seqlen_k[\"sliding_attention\"] = max(\n-                self.max_seqlen_k[\"sliding_attention\"], query_length + min(past_length, self.sliding_window - 1)\n-            )\n-            state.position_offset += query_length\n \n-        logger.debug(\n-            f\"Scheduled: {len(self.requests_in_batch)}, Waiting: {len(self.scheduler.waiting_requests)}, \"\n-            f\"Active: {len(self.scheduler.active_requests)}. cum Q: {cumulative_seqlens_q[-1]}. \"\n-            f\"cum KV: {max(ck[-1] for ck in cumulative_seqlens_k)}, free blocks: {self.cache.get_num_free_blocks()}\"\n-        )\n+            for layer_type, layer_type_seqlen_k in seqlens_k.items():\n+                cumulative_seqlens_k[layer_type].append(cumulative_seqlens_k[layer_type][-1] + layer_type_seqlen_k)\n+                self.max_seqlen_k[layer_type] = max(self.max_seqlen_k[layer_type], layer_type_seqlen_k)\n+\n+            self.cache.extend_read_indices(state.request_id, past_length, query_length, read_index)\n+            self.cache.extend_write_indices(state.request_id, past_length, query_length, write_index)\n+\n+        # When looping over request is done, we can build the actual tensors\n         self._build_tensors(\n             input_ids,\n             position_ids,\n@@ -361,54 +393,64 @@ def prepare_next_batch(self) -> bool:\n             cumulative_seqlens_k,\n             logits_indices,\n         )\n-\n         self.metrics.record_kv_cache_memory_metrics(self.cache)\n \n+        if logger.isEnabledFor(logging.DEBUG):\n+            if isinstance(self.cumulative_seqlens_k, dict):\n+                ck = max(cumulative_seqlens_k[layer_type][-1] for layer_type in self.cumulative_seqlens_k)\n+            else:\n+                ck = cumulative_seqlens_k[-1]\n+            logger.debug(\n+                f\"Scheduled: {len(self.requests_in_batch)}, Waiting: {len(self.scheduler.waiting_requests)}, \"\n+                f\"Active: {len(self.scheduler.active_requests)}. cum Q: {cumulative_seqlens_q[-1]}. \"\n+                f\"cum KV: {ck}, free blocks: {self.cache.get_num_free_blocks()}\"\n+            )\n         return True\n \n     @traced\n     def _build_tensors(\n         self,\n-        input_ids,\n-        position_ids,\n+        input_ids: list[int],\n+        position_ids: list[int],\n         read_index: list[list[int]],\n         write_index: list[list[int]],\n-        cumulative_seqlens_q,\n-        cumulative_seqlens_k,\n-        logits_indices,\n-    ):\n+        cumulative_seqlens_q: list[int],\n+        cumulative_seqlens_k: Union[list[int], dict[str, list[int]]],\n+        logits_indices: list[int],\n+    ) -> None:\n+        \"\"\"Builds the actual tensors for the current batch, by modifying the already allocated tensors in place.\"\"\"\n         to_tensor = partial(torch.tensor, **self.tensor_metadata)\n+\n+        # Those kwargs always have the same type regardless of the model\n         self.input_ids[:, : len(input_ids)] = to_tensor(input_ids)\n         self.position_ids[:, : len(position_ids)] = to_tensor(position_ids)\n+        self.cumulative_seqlens_q[: len(cumulative_seqlens_q)] = to_tensor(cumulative_seqlens_q)\n+        self.logits_indices[: len(logits_indices)] = to_tensor(logits_indices)\n \n+        # Those kwargs are either dict of tensors or tensors, so we need to handle both cases\n+        for layer_type, layer_type_seqlens_k in cumulative_seqlens_k.items():\n+            self.cumulative_seqlens_k[layer_type][: len(layer_type_seqlens_k)] = to_tensor(layer_type_seqlens_k)\n+            if self.attention_mask is not None:\n+                build_attention_mask(\n+                    attention_mask=self.attention_mask[layer_type],\n+                    cumulative_seqlens_q=cumulative_seqlens_q,\n+                    cumulative_seqlens_k=layer_type_seqlens_k,\n+                    sliding_window=self.sliding_window if layer_type == \"sliding_attention\" else 1,\n+                )\n+\n+        # The index only contain references to the storage tensors, so we update the storage and their references\n         self.read_index = []\n         self.write_index = []\n         for i, group_read_indices, group_write_indices in zip(count(), read_index, write_index):\n             # Write in the actual tensors\n-            self.read_index_tensors[i][: len(group_read_indices)] = to_tensor(group_read_indices)\n-            self.write_index_tensors[i][: len(group_write_indices)] = to_tensor(group_write_indices)\n+            self.read_index_storage[i][: len(group_read_indices)] = to_tensor(group_read_indices)\n+            self.write_index_storage[i][: len(group_write_indices)] = to_tensor(group_write_indices)\n             # Slice to the right size\n-            r = len(group_read_indices) if self.slice_inputs else self.read_index_tensors[i].size(-1)\n-            w = len(group_write_indices) if self.slice_inputs else self.write_index_tensors[i].size(-1)\n+            r = len(group_read_indices) if self.slice_inputs else self.read_index_storage[i].size(-1)\n+            w = len(group_write_indices) if self.slice_inputs else self.write_index_storage[i].size(-1)\n             # Add to the index\n-            self.read_index.append(self.read_index_tensors[i][:r])\n-            self.write_index.append(self.write_index_tensors[i][:w])\n-\n-        self.cumulative_seqlens_q[: len(cumulative_seqlens_q)] = to_tensor(cumulative_seqlens_q)\n-        for layer_type in self.cumulative_seqlens_k:\n-            l = len(cumulative_seqlens_k[layer_type])\n-            self.cumulative_seqlens_k[layer_type][:l] = to_tensor(cumulative_seqlens_k[layer_type])\n-        self.logits_indices[: len(logits_indices)] = to_tensor(logits_indices)\n-\n-        if self.attention_mask is not None:\n-            build_attention_mask(self.attention_mask[0], cumulative_seqlens_q, cumulative_seqlens_k[\"full_attention\"])\n-            if self.sliding_window != 1:\n-                build_attention_mask(\n-                    self.attention_mask[1],\n-                    cumulative_seqlens_q,\n-                    cumulative_seqlens_k[\"sliding_attention\"],\n-                    self.sliding_window,\n-                )\n+            self.read_index.append(self.read_index_storage[i][:r])\n+            self.write_index.append(self.write_index_storage[i][:w])\n \n     @traced\n     def _sync(self):\n@@ -526,12 +568,15 @@ def __init__(\n         self.model.generation_config.top_p = None\n         self.do_sample = getattr(generation_config, \"do_sample\", True)\n         self.logit_processor = self.model._get_logits_processor(generation_config)\n-        self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", True)\n+        self.use_cuda_graph = getattr(generation_config, \"use_cuda_graph\", False)  # TODO: same as do_sample\n         self.profile = getattr(generation_config, \"profile\", False)\n         self.manual_eviction = manual_eviction\n         self.batch_processor: Optional[ContinuousBatchProcessor] = None\n         self.slice_inputs = slice_inputs\n \n+        if self.use_cuda_graph:\n+            raise NotImplementedError(\"Cuda graphs are not supported yet\")\n+\n     @traced\n     def start(self):\n         \"\"\"Start the background generation thread.\"\"\""
        },
        {
            "sha": "a27cefd18fcf9fa41d80514ca438b77ba5b38f7d",
            "filename": "src/transformers/generation/continuous_batching/requests.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcontinuous_batching%2Frequests.py?ref=ef053939ca8252805a98771f2a77026a8adc00e6",
            "patch": "@@ -25,7 +25,7 @@\n \n # We centralize the logger here to coordinate between logging and progress bar\n logger = logging.getLogger(\"ContinuousBatchingLogger\")\n-logger.setLevel(logging.INFO)\n+# logger.setLevel(logging.INFO)\n \n \n @staticmethod"
        },
        {
            "sha": "8293bd049c800b8ac986fb69f0fab8fb94128e23",
            "filename": "src/transformers/integrations/eager_paged.py",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fintegrations%2Feager_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Feager_paged.py?ref=ef053939ca8252805a98771f2a77026a8adc00e6",
            "patch": "@@ -42,7 +42,7 @@ def eager_paged_attention_forward(\n     # Get the right causal mask for the current layer\n     if isinstance(attention_mask, dict):\n         sliding_window = getattr(module, \"sliding_window\", 1)\n-        layer_type = \"full_attention\" if sliding_window == 1 else \"sliding_attention\"\n+        layer_type = \"full_attention\" if sliding_window == 1 or sliding_window is None else \"sliding_attention\"\n         causal_mask = attention_mask[layer_type]\n     else:\n         causal_mask = attention_mask\n@@ -51,7 +51,19 @@ def eager_paged_attention_forward(\n     if causal_mask is not None:\n         attn_weights = attn_weights + causal_mask\n \n-    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    # Handle attention sinks if the model has them\n+    if hasattr(module, \"sinks\"):\n+        # Retrieve the sink and add it to the attention weights\n+        sinks = module.sinks.reshape(1, -1, 1, 1).expand(query.shape[0], -1, query.shape[-2], -1)\n+        attn_weights = torch.cat([attn_weights, sinks], dim=-1)\n+        # Normalize the attention weights for better numerical stability\n+        attn_weights = attn_weights - attn_weights.max(dim=-1, keepdim=True).values\n+        # Apply softmax and drop the sink. Not exactly the same code as eager w/ sink, but the same code does not produce the same results.\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+        attn_weights = attn_weights[..., :-1]\n+    else:\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n     attn_output = torch.matmul(attn_weights, value)\n     attn_output = attn_output.transpose(1, 2).contiguous()\n "
        },
        {
            "sha": "329fab4c93230806e54b0da3e7861d7358439042",
            "filename": "src/transformers/integrations/flash_paged.py",
            "status": "modified",
            "additions": 7,
            "deletions": 6,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fintegrations%2Fflash_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflash_paged.py?ref=ef053939ca8252805a98771f2a77026a8adc00e6",
            "patch": "@@ -56,18 +56,19 @@ def paged_attention_forward(\n     if cache is not None:\n         k, v = cache.update(k, v, module.layer_idx, **kwargs)\n \n-        # Check if we are in a sliding window context\n-        cu_seq_lens_k = cu_seq_lens_k[layer_type].clone()\n-        max_seqlen_k = max_seqlen_k[layer_type]\n-\n-    # If there is no cache, we assume this is full attention, and we check if cu_seq_lens_k is a list of tensors\n-    elif isinstance(cu_seq_lens_k, list):\n+    # Retrieve the cumulative sequence lengths for the current layer\n+    if isinstance(cu_seq_lens_k, dict):\n         cu_seq_lens_k = cu_seq_lens_k[layer_type].clone()\n         max_seqlen_k = max_seqlen_k[layer_type]\n+    else:\n+        cu_seq_lens_k = cu_seq_lens_k.clone()\n+        max_seqlen_k = max_seqlen_k\n \n     if implementation is not None and hasattr(implementation, \"flash_attn_varlen_func\"):\n         flash_attn_varlen_func = implementation.flash_attn_varlen_func\n+\n     custom_kwargs = {\"s_aux\": kwargs.get(\"s_aux\")} if \"s_aux\" in kwargs else {}\n+\n     attn_output = flash_attn_varlen_func(\n         q.transpose(1, 2).squeeze(0).contiguous(),\n         k.contiguous(),"
        },
        {
            "sha": "e6cbac418156e017a12b7742604ab58bb674b27b",
            "filename": "src/transformers/integrations/sdpa_paged.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fsdpa_paged.py?ref=ef053939ca8252805a98771f2a77026a8adc00e6",
            "patch": "@@ -42,7 +42,7 @@ def sdpa_attention_paged_forward(\n     # Get the right causal mask for the current layer\n     if isinstance(attention_mask, dict):\n         sliding_window = getattr(module, \"sliding_window\", 1)\n-        layer_type = \"full_attention\" if sliding_window == 1 else \"sliding_attention\"\n+        layer_type = \"full_attention\" if sliding_window == 1 or sliding_window is None else \"sliding_attention\"\n         causal_mask = attention_mask[layer_type]\n     else:\n         causal_mask = attention_mask"
        },
        {
            "sha": "afc0c3e6d794eb4d73dfb786c29f808f7413c0ac",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=ef053939ca8252805a98771f2a77026a8adc00e6",
            "patch": "@@ -596,12 +596,12 @@ def require_flash_attn(test_case):\n \n def require_kernels(test_case):\n     \"\"\"\n-    Decorator marking a test that requires Flash Attention.\n+    Decorator marking a test that requires the kernels library.\n \n-    These tests are skipped when Flash Attention isn't installed.\n+    These tests are skipped when the kernels library isn't installed.\n \n     \"\"\"\n-    return unittest.skipUnless(is_kernels_available(), \"test requires Flash Attention\")(test_case)\n+    return unittest.skipUnless(is_kernels_available(), \"test requires the kernels library\")(test_case)\n \n \n def require_flash_attn_3(test_case):"
        },
        {
            "sha": "62b41995a6d963fc1c13f12407542a22c97db68e",
            "filename": "src/transformers/utils/metrics.py",
            "status": "modified",
            "additions": 20,
            "deletions": 36,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Futils%2Fmetrics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef053939ca8252805a98771f2a77026a8adc00e6/src%2Ftransformers%2Futils%2Fmetrics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fmetrics.py?ref=ef053939ca8252805a98771f2a77026a8adc00e6",
            "patch": "@@ -4,8 +4,6 @@\n from enum import Enum\n from typing import Any, Callable, Optional, Union\n \n-import torch\n-\n \n class RequestStatus(Enum):\n     \"\"\"Status of a generation request through its lifecycle.\"\"\"\n@@ -337,42 +335,28 @@ def record_kv_cache_memory_metrics(self, cache) -> None:\n             return\n \n         try:\n-            # Calculate memory usage based on cache configuration\n-            num_used_blocks = cache.num_blocks - len(cache._free_blocks)\n-            num_layers = len(cache.key_cache)\n-\n-            # Each used block stores key and value states\n-            # Each with shape: (num_kv_heads, block_size, head_dim)\n-            bytes_per_parameter = 2 if cache.dtype in [torch.float16, torch.bfloat16] else 4  # Size in bytes\n-\n-            # Total bytes = num_layers * num_used_blocks * block_size *\n-            #               num_kv_heads * head_dim * 2 (both K and V) * bytes_per_parameter\n-            memory_bytes = (\n-                num_layers\n-                * num_used_blocks\n-                * cache.block_size\n-                * cache.num_key_value_heads\n-                * cache.head_dim\n-                * 2  # For both key and value caches\n-                * bytes_per_parameter\n-            )\n-\n-            free_memory_bytes = (\n-                num_layers\n-                * len(cache._free_blocks)\n-                * cache.block_size\n-                * cache.num_key_value_heads\n-                * cache.head_dim\n-                * 2  # For both key and value caches\n-                * bytes_per_parameter\n-            )\n-\n-            self.kv_cache_memory_gauge.set(memory_bytes)\n+            # Retrieve the memory footprint of the cache\n+            page_size = cache.head_dim * cache.num_key_value_heads\n+            page_mem_in_bytes = page_size * cache.dtype.itemsize\n+            # When a block is allocated, it is for both K and V, so we multiply by 2\n+            # It's also allocated accross all cache tensors, so we multiply by the nb of tensors: len(cache.key_cache)\n+            block_mem_in_bytes = 2 * len(cache.key_cache) * cache.block_size * page_mem_in_bytes\n+\n+            # Retrieve the number of used and free blocks\n+            free_blocks = cache.get_num_free_blocks()\n+            used_blocks = cache.num_blocks - free_blocks\n+\n+            # Convert that into used and free memory in bytes\n+            used_memory_bytes = used_blocks * block_mem_in_bytes\n+            free_memory_bytes = free_blocks * block_mem_in_bytes\n+\n+            # Update the telemetry gauges and add a message in the logs\n+            self.kv_cache_memory_gauge.set(used_memory_bytes)\n             self.kv_cache_free_memory_gauge.set(free_memory_bytes)\n             logger.debug(\n-                f\"KV Cache memory: {memory_bytes / (1024 * 1024):.2f}MB, \"\n-                f\"Used blocks: {num_used_blocks}/{cache.num_blocks} \"\n-                f\"({num_used_blocks / cache.num_blocks * 100:.1f}%)\"\n+                f\"KV Cache memory: {used_memory_bytes / (1024 * 1024):.2f}MB, \"\n+                f\"Used blocks: {used_blocks}/{cache.num_blocks} \"\n+                f\"({used_blocks / cache.num_blocks * 100:.1f}%)\"\n             )\n         except Exception as e:\n             logger.warning(f\"Failed to record KV cache memory metrics: {e}\")"
        },
        {
            "sha": "3179479bdb111a06bdc5f6cd70fff5d1a310b1d9",
            "filename": "tests/generation/test_continuous_batching.py",
            "status": "modified",
            "additions": 207,
            "deletions": 1,
            "changes": 208,
            "blob_url": "https://github.com/huggingface/transformers/blob/ef053939ca8252805a98771f2a77026a8adc00e6/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ef053939ca8252805a98771f2a77026a8adc00e6/tests%2Fgeneration%2Ftest_continuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_continuous_batching.py?ref=ef053939ca8252805a98771f2a77026a8adc00e6",
            "patch": "@@ -15,10 +15,15 @@\n import unittest\n from typing import Optional\n \n+import torch\n from parameterized import parameterized\n \n-from transformers import AutoConfig\n+from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer\n from transformers.generation.continuous_batching.cache import group_layers_by_attn_type\n+from transformers.testing_utils import Expectations, require_kernels, require_torch_gpu, slow\n+\n+\n+ALLOW_EXPECTED_OUTPUTS = True  # this is a debug flag when you want to measure deviation between CB and non-CB gen\n \n \n class ContinuousBatchingTest(unittest.TestCase):\n@@ -82,3 +87,204 @@ def test_group_layers(\n                     expected_group_type,\n                     f\"Test failed for: {layer_types_str = }, {sliding_window = }, {group_types = }\",\n                 )\n+\n+    def _continuous_batching_parity(\n+        self, model_id: str, attn_implementation: str, expected_outputs: dict[str, str]\n+    ) -> None:\n+        # Prepare common elements\n+        tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n+        prompts = [\n+            \"Janet's ducks lay 16 eggs per day. She eats three for breakfast every morning and bakes muffins for her \"\n+                \"friends every day with four. She sells the remainder at the farmers' market daily for $2 per fresh \"\n+                \"duck egg. How much in dollars does she make every day at the farmers' market? The answer is:\",\n+            \"A robe takes 2 bolts of blue fiber and half that much white fiber. How many bolts in total does it take? \"\n+                \"The answer is:\",\n+            \"Josh decides to try flipping a house. He buys a house for $80,000 and then puts in $50,000 in repairs. \"\n+                \"This increased the value of the house by 150%. How much profit did he make? The answer is:\",\n+        ]  # fmt: skip\n+        batched_inputs = [tokenizer.encode(prompt) for prompt in prompts]\n+\n+        # Generation with continuous batching\n+        model = AutoModelForCausalLM.from_pretrained(model_id, attn_implementation=attn_implementation, dtype=\"auto\")\n+        model = model.cuda().eval()\n+        model.generation_config.max_new_tokens = 40\n+        model.generation_config.do_sample = False\n+        model.generation_config.use_cuda_graph = False\n+\n+        cb_outputs = model.generate_batch(inputs=batched_inputs, generation_config=model.generation_config)\n+\n+        # Generation without continuous batching\n+        if attn_implementation == \"sdpa_paged\":\n+            non_cb_attn_implementation = \"sdpa\"\n+        elif attn_implementation == \"eager_paged\":\n+            non_cb_attn_implementation = \"eager\"\n+        elif attn_implementation == \"paged_attention|kernels-community/flash-attn\":\n+            non_cb_attn_implementation = \"eager\"\n+        else:\n+            raise ValueError(f\"Invalid attention implementation: {attn_implementation}\")\n+\n+        # We regenerate the model because just changing the attn_implementation does not work\n+        model = AutoModelForCausalLM.from_pretrained(\n+            model_id, attn_implementation=non_cb_attn_implementation, dtype=\"auto\"\n+        )\n+        model = model.cuda().eval()\n+        model.generation_config.max_new_tokens = 40\n+        model.generation_config.do_sample = False\n+        model.generation_config.use_cuda_graph = False\n+\n+        for request_id, request in cb_outputs.items():\n+            # Generate without continuous batching\n+            input_ids = torch.tensor([request.prompt_ids]).cuda()\n+            attention_mask = torch.ones_like(input_ids)\n+            outputs = model.generate(\n+                input_ids, attention_mask=attention_mask, generation_config=model.generation_config\n+            )\n+            generated_tokens = outputs[0][input_ids.shape[1] :]\n+            non_cb_decoded_output = tokenizer.decode(generated_tokens, skip_special_tokens=True)\n+            input_ids = input_ids.tolist()[0]\n+\n+            # Check that the generated output with and without CB match\n+            cb_decoded_output = tokenizer.decode(request.generated_tokens, skip_special_tokens=True)\n+            outputs_match = non_cb_decoded_output == cb_decoded_output\n+\n+            # If they dont, that might be expected: the outputs can differ slightly due to numerical differences\n+            # If that's the case, there is an expected output ready\n+            if not outputs_match:\n+                expected_output = expected_outputs.get(request_id) if ALLOW_EXPECTED_OUTPUTS else None\n+\n+                if expected_output is None:\n+                    self.fail(\n+                        f\"Test {request_id = } failed, no expected output was provided.\\nRef:\"\n+                        f\"{repr(non_cb_decoded_output)}\\nOut:{repr(cb_decoded_output)}\"\n+                    )\n+                else:\n+                    self.assertEqual(\n+                        expected_output,\n+                        cb_decoded_output,\n+                        msg=f\"Test {request_id = } failed, expected output did not match.\\n\"\n+                        f\"Exp:{repr(expected_output)}\\nOut:{repr(cb_decoded_output)}\",\n+                    )\n+\n+    # Eager tests\n+    @require_torch_gpu\n+    @slow\n+    def test_continuous_batching_parity_llama_eager(self) -> None:\n+        expected_outputs = Expectations({\n+            (\"rocm\", (9, 4)): {\n+                \"req_0\": \" $16. How did I get that answer? I used the following equation: 16 - 3 - 4 = 9. 9 x $2 = $18. $18 -\"\n+            },\n+            (\"cuda\", (9, 0)): {\n+                \"req_1\": \" 3 bolts of blue fiber and 1.5 bolts of white fiber. The total number of bolts is 4.5. The total number of bolts is 4.5. The total\",\n+                \"req_2\": \" $50,000. This is because the value of the house increased by 150%, which means that the value of the house increased by $50,000. This is because the value of the\"\n+            }\n+        }).get_expectation()  # fmt: skip\n+        self._continuous_batching_parity(\"meta-llama/Llama-3.1-8B\", \"eager_paged\", expected_outputs)\n+\n+    @require_torch_gpu\n+    @slow\n+    def test_continuous_batching_parity_gemma_eager(self) -> None:\n+        expected_outputs = Expectations({\n+            (\"rocm\", (9, 4)): {\n+                \"req_1\": \" \\n\\n**Answer:** 3 bolts\\n\\n**Solution:**\\n\\n* **White fiber:** The robe needs half as much white fiber as blue fiber, so it needs 2 bolts / 2 =\"\n+            },\n+            (\"cuda\", (9, 0)): {\n+                \"req_0\": \"\\n\\n**$12**\\n\\n**Here's how to solve it:**\\n\\n* **Eggs eaten:** 3\\n* **Eggs left:** 16 - 3 = 13\",\n+                \"req_1\": \" \\n \\n 2 + 1 = 3 bolts \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \"\n+            }\n+        }).get_expectation()  # fmt: skip\n+        self._continuous_batching_parity(\"google/gemma-2-2b-it\", \"eager_paged\", expected_outputs)\n+\n+    @require_torch_gpu\n+    @slow\n+    def test_continuous_batching_parity_qwen_eager(self) -> None:\n+        expected_outputs = {}\n+        self._continuous_batching_parity(\"Qwen/Qwen3-4B-Instruct-2507\", \"eager_paged\", expected_outputs)\n+\n+    @require_torch_gpu\n+    @slow\n+    def test_continuous_batching_parity_gpt_oss_eager(self) -> None:\n+        expected_outputs = Expectations({\n+            (\"cuda\", (9, 0)): {\n+                \"req_1\": \" 2.5 bolts. The question: \\\"What is the name of the puzzle that involves a robe taking 2 bolts of blue fiber and half that much white fiber?\\\" The answer: \\\"The\",\n+                \"req_2\": \" 50%.\\\"\\n\\nWe need to parse: He buys a house for $80,000. He puts in $50,000 in repairs. This increased the value of the house by 150%.\"\n+            }\n+        }).get_expectation()  # fmt: skip\n+        self._continuous_batching_parity(\"openai/gpt-oss-20b\", \"eager_paged\", expected_outputs)\n+\n+    # SDPA tests\n+    @require_torch_gpu\n+    @slow\n+    def test_continuous_batching_parity_llama_sdpa(self) -> None:\n+        expected_outputs = Expectations({\n+            (\"rocm\", (9, 4)): {\n+                \"req_2\": \" $50,000. This is because the value of the house increased by 150%, which means that the value of the house increased by $50,000. This is because the value of the\"\n+            }\n+        }).get_expectation()  # fmt: skip\n+        self._continuous_batching_parity(\"meta-llama/Llama-3.1-8B\", \"sdpa_paged\", expected_outputs)\n+\n+    @require_torch_gpu\n+    @slow\n+    def test_continuous_batching_parity_gemma_sdpa(self) -> None:\n+        expected_outputs = Expectations({\n+            (\"cuda\", (9, 0)): {\n+                \"req_1\": \" \\n\\n**Answer:** 3 bolts\\n\\n**Solution:**\\n\\n* **White fiber:** The robe needs half as much white fiber as blue fiber, so it needs 2 bolts / 2 =\",\n+            }\n+        }).get_expectation()  # fmt: skip\n+        self._continuous_batching_parity(\"google/gemma-2-2b-it\", \"sdpa_paged\", expected_outputs)\n+\n+    @require_torch_gpu\n+    @slow\n+    def test_continuous_batching_parity_qwen_sdpa(self) -> None:\n+        expected_outputs = {}\n+        self._continuous_batching_parity(\"Qwen/Qwen3-4B-Instruct-2507\", \"sdpa_paged\", expected_outputs)\n+\n+    # GPT-OSS is not compatible with SDPA because it has an attention sink. TODO: is this fixable?\n+\n+    # Flash attention test\n+    @require_torch_gpu\n+    @require_kernels\n+    @slow\n+    def test_continuous_batching_parity_llama_flash(self) -> None:\n+        expected_outputs = Expectations({\n+            (\"cuda\", (9, 0)): {\n+                \"req_1\": \" 3 bolts of blue fiber and 1.5 bolts of white fiber. The total number of bolts is 4.5 bolts. The total number of bolts is 4.5 bolts.\",\n+            }\n+        }).get_expectation()  # fmt: skip\n+        self._continuous_batching_parity(\n+            \"meta-llama/Llama-3.1-8B\", \"paged_attention|kernels-community/flash-attn\", expected_outputs\n+        )\n+\n+    @require_torch_gpu\n+    @require_kernels\n+    @slow\n+    def test_continuous_batching_parity_gemma_flash(self) -> None:\n+        expected_outputs = Expectations({\n+            (\"cuda\", (9, 0)): {\n+                \"req_1\": \" \\n \\n 2 + 1 = 3 bolts \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \\n \",\n+            }\n+        }).get_expectation()  # fmt: skip\n+        self._continuous_batching_parity(\n+            \"google/gemma-2-2b-it\", \"paged_attention|kernels-community/flash-attn\", expected_outputs\n+        )\n+\n+    @require_torch_gpu\n+    @require_kernels\n+    @slow\n+    def test_continuous_batching_parity_qwen_flash(self) -> None:\n+        expected_outputs = {}\n+        self._continuous_batching_parity(\n+            \"Qwen/Qwen3-4B-Instruct-2507\", \"paged_attention|kernels-community/flash-attn\", expected_outputs\n+        )\n+\n+    @require_torch_gpu\n+    @require_kernels\n+    @slow\n+    def test_continuous_batching_parity_gpt_oss_flash(self) -> None:\n+        expected_outputs = {}\n+        self._continuous_batching_parity(\n+            \"openai/gpt-oss-20b\", \"paged_attention|kernels-community/flash-attn\", expected_outputs\n+        )\n+\n+\n+# FIXME: the gemma test seem broken, there is a message about cuda graphs and the sdpa and flash expecteations are\n+# inverted on CUDA. On AMD they do fine."
        }
    ],
    "stats": {
        "total": 673,
        "additions": 474,
        "deletions": 199
    }
}