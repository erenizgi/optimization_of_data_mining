{
    "author": "HyunZ118",
    "message": "ğŸŒ [i18n-KO] Translated smolvlm.md to Korean (#40414)\n\n* fix: manual edits\n\n* Apply suggestions from code review\n\n* Update docs/source/ko/model_doc/smolvlm.md\n\n* Update docs/source/ko/model_doc/smolvlm.md\n\n* Update docs/source/ko/model_doc/smolvlm.md\n\n* Update docs/source/ko/model_doc/smolvlm.md\n\n* Update docs/source/ko/_toctree.yml\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "2141a5b7642f33df22548c5e0730b95efccdb96e",
    "files": [
        {
            "sha": "98d2e13810692e6f85aa6b7c6a5d5b5eb99bf3fb",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2141a5b7642f33df22548c5e0730b95efccdb96e/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/2141a5b7642f33df22548c5e0730b95efccdb96e/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=2141a5b7642f33df22548c5e0730b95efccdb96e",
            "patch": "@@ -1161,8 +1161,8 @@\n         title: SigLIP2\n       - local: in_translation\n         title: SmolLM3\n-      - local: in_translation\n-        title: SmolVLM\n+      - local: model_doc/smolvlm\n+        title: ì†Œí˜• ë¹„ì „ ì–¸ì–´ ëª¨ë¸\n       - local: in_translation\n         title: Speech Encoder Decoder Models\n       - local: in_translation"
        },
        {
            "sha": "1ebd50519c180421b729bce0c95a5b83554fcd8d",
            "filename": "docs/source/ko/model_doc/smolvlm.md",
            "status": "added",
            "additions": 210,
            "deletions": 0,
            "changes": 210,
            "blob_url": "https://github.com/huggingface/transformers/blob/2141a5b7642f33df22548c5e0730b95efccdb96e/docs%2Fsource%2Fko%2Fmodel_doc%2Fsmolvlm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2141a5b7642f33df22548c5e0730b95efccdb96e/docs%2Fsource%2Fko%2Fmodel_doc%2Fsmolvlm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fsmolvlm.md?ref=2141a5b7642f33df22548c5e0730b95efccdb96e",
            "patch": "@@ -0,0 +1,210 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*ì´ ëª¨ë¸ì€ 2025ë…„ 2ì›” 20ì¼ì— ì¶œì‹œë˜ì—ˆìœ¼ë©°, ë™ì‹œì— í—ˆê¹…í˜ì´ìŠ¤ `Transformer` ë¼ì´ë¸ŒëŸ¬ë¦¬ì— ì¶”ê°€ë˜ì—ˆìŠµë‹ˆë‹¤.*\n+\n+# ì†Œí˜• ë¹„ì „ ì–¸ì–´ ëª¨ë¸(SmolVLM)[[smolvlm]]\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## ê°œìš”[[overview]]\n+[SmolVLM2](https://huggingface.co/papers/2504.05299) ([ë¸”ë¡œê·¸ ê¸€](https://huggingface.co/blog/smolvlm2)) ì€ Idefics3 ëª¨ë¸ì„ ê°œì„ í•œ ë²„ì „ìœ¼ë¡œ, ë‘ ê°€ì§€ ì£¼ìš” ì°¨ì´ì ì´ ìˆìŠµë‹ˆë‹¤:\n+\n+- í…ìŠ¤íŠ¸ ëª¨ë¸ë¡œ SmolLM2ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+- í•œ ì¥ì˜ ì´ë¯¸ì§€ë¿ ì•„ë‹ˆë¼ ì—¬ëŸ¬ ì¥ì˜ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ ì…ë ¥ë„ ì§€ì›í•©ë‹ˆë‹¤.\n+\n+## ì‚¬ìš© íŒ[[usage-tips]]\n+\n+ì…ë ¥ëœ ì´ë¯¸ì§€ëŠ” ì„¤ì •ì— ë”°ë¼ ì›ë³¸ í•´ìƒë„ë¥¼ ìœ ì§€í•˜ê±°ë‚˜ í¬ê¸°ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ ì´ë¯¸ì§€ í¬ê¸° ì¡°ì ˆ ì—¬ë¶€ì™€ ë°©ì‹ì€ `do_resize`ì™€ `size` íŒŒë¼ë¯¸í„°ë¡œ ê²°ì •ë©ë‹ˆë‹¤.\n+\n+ë¹„ë””ì˜¤ì˜ ê²½ìš°ì—ëŠ” ì—…ìƒ˜í”Œë§ì„ í•˜ë©´ ì•ˆ ë©ë‹ˆë‹¤.\n+\n+ë§Œì•½ `do_resize`ê°€ `True`ì¼ ê²½ìš°, ëª¨ë¸ì€ ê¸°ë³¸ì ìœ¼ë¡œ ì´ë¯¸ì§€ì˜ ê°€ì¥ ê¸´ ë³€ì„ 4*512 í”½ì…€ì´ ë˜ë„ë¡ í¬ê¸°ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤.\n+ì´ ê¸°ë³¸ ë™ì‘ì€ `size` íŒŒë¼ë¯¸í„°ì— ë”•ì…”ë„ˆë¦¬ë¥¼ ì „ë‹¬í•˜ì—¬ ì›í•˜ëŠ” ê°’ìœ¼ë¡œ ì§ì ‘ ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ê¸°ë³¸ê°’ì€ `{\"longest_edge\": 4 * 512}` ì´ì—¬ë„ ì‚¬ìš©ì í•„ìš”ì— ë”°ë¼ ë‹¤ë¥¸ ê°’ìœ¼ë¡œ ë³€ê²½í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ë‹¤ìŒì€ ë¦¬ì‚¬ì´ì§•ì„ ì œì–´í•˜ê³  ì‚¬ìš©ì ì •ì˜ í¬ê¸°ë¡œ ë³€ê²½í•˜ëŠ” ë°©ë²•ì…ë‹ˆë‹¤:\n+```python\n+image_processor = SmolVLMImageProcessor(do_resize=True, size={\"longest_edge\": 2 * 512}, max_image_size=512)\n+```\n+\n+ë˜í•œ, `max_image_size` ë§¤ê°œë³€ìˆ˜ëŠ” ì´ë¯¸ì§€ë¥¼ ë¶„í• í•˜ëŠ” ì •ì‚¬ê°í˜• íŒ¨ì¹˜ì˜ í¬ê¸°ë¥¼ ì œì–´í•©ë‹ˆë‹¤. ì´ ê°’ì€ ê¸°ë³¸ì ìœ¼ë¡œ 512ë¡œ ì„¤ì •ë˜ì–´ ìˆìœ¼ë©° í•„ìš”ì— ë”°ë¼ ì¡°ì • ê°€ëŠ¥í•©ë‹ˆë‹¤. ì´ë¯¸ì§€ ì²˜ë¦¬ê¸°ëŠ” ë¦¬ì‚¬ì´ì§•ì„ ë§ˆì¹œ í›„, `max_image_size` ê°’ì„ ê¸°ì¤€ìœ¼ë¡œ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ê°œì˜ ì •ì‚¬ê°í˜• íŒ¨ì¹˜ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n+\n+ì´ ëª¨ë¸ì˜ ê¸°ì—¬ìëŠ” [orrzohar](https://huggingface.co/orrzohar) ì…ë‹ˆë‹¤.\n+\n+\n+\n+## ì‚¬ìš© ì˜ˆì‹œ[[usage-example]]\n+\n+### ë‹¨ì¼ ë¯¸ë””ì–´ ì¶”ë¡ [[single-media-inference]]\n+\n+ì´ ëª¨ë¸ì€ ì´ë¯¸ì§€ì™€ ë¹„ë””ì˜¤ë¥¼ ëª¨ë‘ ì…ë ¥ìœ¼ë¡œ ë°›ì„ ìˆ˜ ìˆì§€ë§Œ, í•œ ë²ˆì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ë¯¸ë””ì–´ëŠ” ë°˜ë“œì‹œ í•˜ë‚˜ì˜ ì¢…ë¥˜ì—¬ì•¼ í•©ë‹ˆë‹¤. ê´€ë ¨ ì˜ˆì‹œ ì½”ë“œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n+\n+```python\n+import torch\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n+\n+processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\")\n+model = AutoModelForImageTextToText.from_pretrained(\n+    \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\",\n+    dtype=torch.bfloat16,\n+    device_map=\"auto\"\n+)\n+\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\":[\n+            {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n+            {\"type\": \"text\", \"text\": \"ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"}\n+        ]\n+    }\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+).to(model.device, dtype=torch.bfloat16)\n+\n+output_ids = model.generate(**inputs, max_new_tokens=128)\n+generated_texts = processor.batch_decode(output_ids, skip_special_tokens=True)\n+print(generated_texts)\n+\n+\n+# Video\n+conversation = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"video\", \"path\": \"/path/to/video.mp4\"},\n+            {\"type\": \"text\", \"text\": \"ì´ ë¹„ë””ì˜¤ì— ëŒ€í•´ ìì„¸íˆ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"}\n+        ]\n+    },\n+]\n+\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+).to(model.device, dtype=torch.bfloat16)\n+\n+generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=100)\n+generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+print(generated_texts[0])\n+```\n+\n+### ë°°ì¹˜ ë‹¤ì¤‘ ë¯¸ë””ì–´ ì¶”ë¡ [[batch-mixed-media-inference]]\n+\n+ì´ ëª¨ë¸ì€ ì—¬ëŸ¬ ì´ë¯¸ì§€, ë¹„ë””ì˜¤, í…ìŠ¤íŠ¸ë¡œ êµ¬ì„±ëœ ì…ë ¥ì„ í•œ ë²ˆì— ë°°ì¹˜ í˜•íƒœë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê´€ë ¨ ì˜ˆì‹œëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤.\n+\n+```python\n+import torch\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n+\n+processor = AutoProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\")\n+model = AutoModelForImageTextToText.from_pretrained(\n+    \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\",\n+    dtype=torch.bfloat16,\n+    device_map=\"auto\"\n+)\n+\n+# ì²« ë²ˆì§¸ ì´ë¯¸ì§€ì— ëŒ€í•œ êµ¬ì„±\n+conversation1 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"path\": \"/path/to/image.jpg\"},\n+            {\"type\": \"text\", \"text\": \"ì´ ì´ë¯¸ì§€ì— ëŒ€í•´ ì„¤ëª…í•´ì£¼ì„¸ìš”.\"}\n+        ]\n+    }\n+]\n+\n+# ë‘ ì¥ì˜ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ êµ¬ì„±\n+conversation2 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\", \"path\": \"/path/to/image.jpg\"},\n+            {\"type\": \"image\", \"path\": \"/path/to/image.jpg\"},\n+            {\"type\": \"text\", \"text\": \"ê·¸ë¦¼ì— ë¬´ì—‡ì´ ì í˜€ìˆë‚˜ìš”?\"}\n+        ]\n+    }\n+]\n+\n+# í…ìŠ¤íŠ¸ë§Œ í¬í•¨í•˜ê³  ìˆëŠ” êµ¬ì„±\n+conversation3 = [\n+    {\"role\": \"user\",\"content\": \"ë‹¹ì‹ ì€ ëˆ„êµ¬ì¸ê°€ìš”?\"}\n+]\n+\n+\n+conversations = [conversation1, conversation2, conversation3]\n+inputs = processor.apply_chat_template(\n+    conversation,\n+    add_generation_prompt=True,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+).to(model.device, dtype=torch.bfloat16)\n+\n+generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=100)\n+generated_texts = processor.batch_decode(generated_ids, skip_special_tokens=True)\n+print(generated_texts[0])\n+```\n+\n+## SmolVLMConfig[[transformers.SmolVLMConfig]]\n+\n+[[autodoc]] SmolVLMConfig\n+\n+## SmolVLMVisionConfig[[transformers.SmolVLMVisionConfig]]\n+\n+[[autodoc]] SmolVLMVisionConfig\n+\n+## Idefics3VisionTransformer[[transformers.SmolVLMVisionTransformer]]\n+\n+[[autodoc]] SmolVLMVisionTransformer\n+\n+## SmolVLMModel[[transformers.SmolVLMModel]]\n+\n+[[autodoc]] SmolVLMModel\n+    - forward\n+\n+## SmolVLMForConditionalGeneration[[transformers.SmolVLMForConditionalGeneration]]\n+\n+[[autodoc]] SmolVLMForConditionalGeneration\n+    - forward\n+\n+## SmolVLMImageProcessor[[transformers.SmolVLMImageProcessor]]\n+[[autodoc]] SmolVLMImageProcessor\n+    - preprocess\n+\n+## SmolVLMImageProcessorFast[[transformers.SmolVLMImageProcessorFast]]\n+[[autodoc]] SmolVLMImageProcessorFast\n+    - preprocess\n+\n+## SmolVLMVideoProcessor[[transformers.SmolVLMVideoProcessor]]\n+[[autodoc]] SmolVLMVideoProcessor\n+    - preprocess\n+\n+## SmolVLMProcessor[[transformers.SmolVLMProcessor]]\n+[[autodoc]] SmolVLMProcessor\n+    - __call__"
        }
    ],
    "stats": {
        "total": 214,
        "additions": 212,
        "deletions": 2
    }
}