{
    "author": "xenova",
    "message": "ðŸš¨ Fix `torch.jit.trace` for `interpolate_pos_encoding` in all vision models (#33226)\n\n* Fix `torch.jit.tracing` for `interpolate_pos_encoding` in all vision models\r\n\r\n* Apply formatting\r\n\r\n* Add missing `self.config = config`\r\n\r\n* Fix copies\r\n\r\n* Fix hiera interpolation unit test\r\n\r\n* Formatting\r\n\r\n* Update `_import_structure`\r\n\r\n* make style\r\n\r\n* Fix docstring\r\n\r\n* Use `# Copied from` instead of utils\r\n\r\n* DeiT variable renaming (`class_and_dist_pos_embed`)\r\n\r\n* Fix Hiera `interpolate_pos_encoding`",
    "sha": "c6d2848a23aba42404784ba52e421ae7b8c68eda",
    "files": [
        {
            "sha": "f972e021f3e2b3efc35e1bd14f7b2b07f56746ed",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 23,
            "deletions": 17,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -41,6 +41,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_beit import BeitConfig\n@@ -150,41 +151,46 @@ def __init__(self, config: BeitConfig) -> None:\n             self.position_embeddings = None\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows the model to interpolate the pre-trained position encodings so that it can be used on\n-        higher resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n+\n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n \n-        class_pos_embed = self.position_embeddings[:, 0]\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        h = height // self.patch_size\n-        w = width // self.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h, w = h + 0.1, w + 0.1\n \n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h / math.sqrt(num_positions), w / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n-        if int(h) != patch_pos_embed.shape[-2] or int(w) != patch_pos_embed.shape[-1]:\n-            raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n \n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(\n         self,\n@@ -566,7 +572,7 @@ def forward(self, window_size, interpolate_pos_encoding: bool = False, dim_size=\n \n         old_sub_table = old_sub_table.reshape(1, old_width, old_height, -1).permute(0, 3, 1, 2)\n         new_sub_table = nn.functional.interpolate(\n-            old_sub_table, size=(int(new_height), int(new_width)), mode=\"bilinear\"\n+            old_sub_table, size=(torch_int(new_height), torch_int(new_width)), mode=\"bilinear\"\n         )\n         new_sub_table = new_sub_table.permute(0, 2, 3, 1).reshape(new_num_relative_distance - 3, -1)\n "
        },
        {
            "sha": "2392961037f2113c3dffb4f9f645afaeeaa09c36",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 26,
            "deletions": 18,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"PyTorch BLIP model.\"\"\"\n \n-import math\n import warnings\n from dataclasses import dataclass\n from typing import Any, Optional, Tuple, Union\n@@ -33,6 +32,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_blip import BlipConfig, BlipTextConfig, BlipVisionConfig\n from .modeling_blip_text import BlipTextLMHeadModel, BlipTextModel\n@@ -232,38 +232,46 @@ def __init__(self, config: BlipVisionConfig):\n \n         self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n+\n         num_patches = embeddings.shape[1] - 1\n-        num_positions = self.position_embedding.shape[1] - 1\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n \n-        if num_patches == num_positions and height == width:\n-            return self.position_embedding\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n \n-        class_pos_embed = self.position_embedding[:, 0, :]\n-        patch_pos_embed = self.position_embedding[:, 1:, :]\n         dim = embeddings.shape[-1]\n-        h0 = height // self.config.patch_size\n-        w0 = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n         batch_size, _, height, width = pixel_values.shape"
        },
        {
            "sha": "8c3b5254ea8b9058201ae5edc7c4f99bc7edfff1",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 26,
            "deletions": 17,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -38,6 +38,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from ..auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_blip_2 import Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n@@ -198,38 +199,46 @@ def __init__(self, config: Blip2VisionConfig):\n \n         self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n+\n         num_patches = embeddings.shape[1] - 1\n-        num_positions = self.position_embedding.shape[1] - 1\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n \n-        if num_patches == num_positions and height == width:\n-            return self.position_embedding\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n \n-        class_pos_embed = self.position_embedding[:, 0, :]\n-        patch_pos_embed = self.position_embedding[:, 1:, :]\n         dim = embeddings.shape[-1]\n-        h0 = height // self.config.patch_size\n-        w0 = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n         batch_size, _, height, width = pixel_values.shape"
        },
        {
            "sha": "4d252ce1f19db775c4221c8a1fde0abc677ba05a",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 23,
            "deletions": 17,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -39,6 +39,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_data2vec_vision import Data2VecVisionConfig\n \n@@ -149,41 +150,46 @@ def __init__(self, config: Data2VecVisionConfig) -> None:\n             self.position_embeddings = None\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows the model to interpolate the pre-trained position encodings so that it can be used on\n-        higher resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n+\n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n \n-        class_pos_embed = self.position_embeddings[:, 0]\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        h = height // self.patch_size\n-        w = width // self.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h, w = h + 0.1, w + 0.1\n \n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h / math.sqrt(num_positions), w / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n-        if int(h) != patch_pos_embed.shape[-2] or int(w) != patch_pos_embed.shape[-1]:\n-            raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n \n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(\n         self,\n@@ -575,7 +581,7 @@ def forward(self, window_size, interpolate_pos_encoding: bool = False, dim_size=\n \n         old_sub_table = old_sub_table.reshape(1, old_width, old_height, -1).permute(0, 3, 1, 2)\n         new_sub_table = nn.functional.interpolate(\n-            old_sub_table, size=(int(new_height), int(new_width)), mode=\"bilinear\"\n+            old_sub_table, size=(torch_int(new_height), torch_int(new_width)), mode=\"bilinear\"\n         )\n         new_sub_table = new_sub_table.permute(0, 2, 3, 1).reshape(new_num_relative_distance - 3, -1)\n "
        },
        {
            "sha": "03194c15d98f1c1ebf3b1804541b7978ab6a2fbb",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 22,
            "deletions": 17,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -40,6 +40,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_deit import DeiTConfig\n \n@@ -77,39 +78,43 @@ def __init__(self, config: DeiTConfig, use_mask_token: bool = False) -> None:\n \n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing and 2 class embeddings.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n-        # return self.position_embeddings\n         num_patches = embeddings.shape[1] - 2\n         num_positions = self.position_embeddings.shape[1] - 2\n \n-        if num_patches == num_positions and height == width:\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n \n-        class_pos_embed = self.position_embeddings[:, 0, :]\n-        dist_pos_embed = self.position_embeddings[:, 1, :]\n-        patch_pos_embed = self.position_embeddings[:, 2:, :]\n+        class_and_dist_pos_embed = self.position_embeddings[:, :2]\n+        patch_pos_embed = self.position_embeddings[:, 2:]\n+\n         dim = embeddings.shape[-1]\n-        h0 = height // self.patch_size\n-        w0 = width // self.patch_size\n-        # # we add a small number to avoid floating point error in the interpolation\n-        # # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n \n-        return torch.cat((class_pos_embed.unsqueeze(0), dist_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+        return torch.cat((class_and_dist_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(\n         self,"
        },
        {
            "sha": "dca17adf2b09bbc442953080a7ef312d1952c8b2",
            "filename": "src/transformers/models/deprecated/vit_hybrid/modeling_vit_hybrid.py",
            "status": "modified",
            "additions": 31,
            "deletions": 17,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fmodeling_vit_hybrid.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -27,7 +27,13 @@\n from ....modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ....modeling_utils import PreTrainedModel\n from ....pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n-from ....utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n+from ....utils import (\n+    add_code_sample_docstrings,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    torch_int,\n+)\n from ....utils.backbone_utils import load_backbone\n from .configuration_vit_hybrid import ViTHybridConfig\n \n@@ -60,41 +66,49 @@ def __init__(self, config: ViTHybridConfig, use_mask_token: bool = False) -> Non\n         num_patches = self.patch_embeddings.num_patches\n         self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config.hidden_size))\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n         self.config = config\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n-        class_pos_embed = self.position_embeddings[:, 0]\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        height = height // self.config.patch_size\n-        width = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        height, width = height + 0.1, width + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(height / math.sqrt(num_positions), width / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n-        if int(height) != patch_pos_embed.shape[-2] or int(width) != patch_pos_embed.shape[-1]:\n-            raise ValueError(f\"Invalid height or width: {height}, {width}\")\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(\n         self,"
        },
        {
            "sha": "160c5ae69f394a5ab0af0507a7997f063f9fdc7c",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 24,
            "deletions": 17,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -38,6 +38,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_dinov2 import Dinov2Config\n@@ -71,42 +72,48 @@ def __init__(self, config: Dinov2Config) -> None:\n         num_patches = self.patch_embeddings.num_patches\n         self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config.hidden_size))\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n         self.config = config\n \n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing and interpolation at torch.float32 precision.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n-        class_pos_embed = self.position_embeddings[:, 0]\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        height = height // self.config.patch_size\n-        width = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        height, width = height + 0.1, width + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n         target_dtype = patch_pos_embed.dtype\n         patch_pos_embed = nn.functional.interpolate(\n-            patch_pos_embed.to(dtype=torch.float32),\n-            scale_factor=(float(height / math.sqrt(num_positions)), float(width / math.sqrt(num_positions))),\n+            patch_pos_embed.to(torch.float32),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         ).to(dtype=target_dtype)\n-        if int(height) != patch_pos_embed.shape[-2] or int(width) != patch_pos_embed.shape[-1]:\n-            raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.Tensor] = None) -> torch.Tensor:\n         batch_size, _, height, width = pixel_values.shape"
        },
        {
            "sha": "8d639131b841caeb7a61328a636537c0a7dfa517",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 25,
            "deletions": 14,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -166,38 +166,49 @@ def __init__(self, config, use_mask_token=False):\n \n         self.norm = nn.LayerNorm(config.embed_dim)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n+        self.config = config\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n-        class_pos_embed = self.position_embeddings[:, 0]\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        h0 = height // self.config.patch_size\n-        w0 = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(\n         self,"
        },
        {
            "sha": "1587493643e99d7b39f1e635077501e184464c40",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -152,7 +152,7 @@ def _resize_pos_embed(self, posemb, grid_size_height, grid_size_width, start_ind\n         posemb_tok = posemb[:, :start_index]\n         posemb_grid = posemb[0, start_index:]\n \n-        old_grid_size = int(math.sqrt(len(posemb_grid)))\n+        old_grid_size = torch_int(len(posemb_grid) ** 0.5)\n \n         posemb_grid = posemb_grid.reshape(1, old_grid_size, old_grid_size, -1).permute(0, 3, 1, 2)\n         posemb_grid = nn.functional.interpolate(posemb_grid, size=(grid_size_height, grid_size_width), mode=\"bilinear\")\n@@ -626,7 +626,7 @@ def forward(self, hidden_states: List[torch.Tensor], patch_height=None, patch_wi\n                 if patch_height is not None and patch_width is not None:\n                     hidden_state = hidden_state.reshape(batch_size, patch_height, patch_width, num_channels)\n                 else:\n-                    size = int(math.sqrt(sequence_length))\n+                    size = torch_int(sequence_length**0.5)\n                     hidden_state = hidden_state.reshape(batch_size, size, size, num_channels)\n                 hidden_state = hidden_state.permute(0, 3, 1, 2).contiguous()\n "
        },
        {
            "sha": "589385dffecfb071ed91d4a53b73c72a883a1f89",
            "filename": "src/transformers/models/flava/modeling_flava.py",
            "status": "modified",
            "additions": 29,
            "deletions": 21,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fmodeling_flava.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -34,6 +34,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_flava import (\n     FlavaConfig,\n@@ -259,42 +260,49 @@ def __init__(self, config: FlavaImageConfig, use_mask_token: bool = False) -> No\n         num_patches = self.patch_embeddings.num_patches\n         self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n         self.config = config\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/image_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n-        npatch = embeddings.shape[1] - 1\n-        num_pos = self.position_embeddings.shape[1] - 1\n-        if npatch == num_pos and height == width:\n+        num_patches = embeddings.shape[1] - 1\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n-        class_pos_embed = self.position_embeddings[:, 0]\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        num_h_patches = height // self.config.patch_size\n-        num_w_patches = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        num_h_patches, num_w_patches = num_h_patches + 0.1, num_w_patches + 0.1\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n-            patch_pos_embed.reshape(1, int(math.sqrt(num_pos)), int(math.sqrt(num_pos)), dim).permute(0, 3, 1, 2),\n-            scale_factor=(num_h_patches / math.sqrt(num_pos), num_w_patches / math.sqrt(num_pos)),\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n-        if int(num_h_patches) != patch_pos_embed.shape[-2] or int(num_w_patches) != patch_pos_embed.shape[-1]:\n-            raise ValueError(\n-                f\"Number of patches for images ({int(num_h_patches), int(num_w_patches)}) don't match the \"\n-                f\"shape of position embedding ({patch_pos_embed.shape[-2], patch_pos_embed.shape[-1]})\"\n-            )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(\n         self,"
        },
        {
            "sha": "3a2ccab8429efa39f263ee6f127494fae3a2f1df",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 25,
            "deletions": 20,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch GroupViT model.\"\"\"\n \n import collections.abc\n-import math\n from dataclasses import dataclass\n from typing import Any, Optional, Tuple, Union\n \n@@ -34,6 +33,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_groupvit import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n \n@@ -365,39 +365,44 @@ def __init__(self, config: GroupViTVisionConfig):\n         self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches, config.hidden_size))\n         self.dropout = nn.Dropout(config.dropout)\n         self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.patch_size = config.patch_size\n         self.config = config\n \n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing and no class embeddings.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n-        npatch = embeddings.shape[1]\n-        if npatch == self.position_embeddings.shape[1] and height == width:\n+        num_patches = embeddings.shape[1]\n+        num_positions = self.position_embeddings.shape[1]\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n+\n         patch_pos_embed = self.position_embeddings\n-        num_original_pos_embed = patch_pos_embed.shape[1]\n+\n         dim = embeddings.shape[-1]\n-        feat_height = height // self.config.patch_size\n-        feat_width = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        feat_height, feat_width = feat_height + 0.1, feat_width + 0.1\n-        original_height = original_width = math.sqrt(num_original_pos_embed)\n-        reshaped_patch_pos_embed = patch_pos_embed.reshape(1, int(original_height), int(original_width), dim).permute(\n-            0, 3, 1, 2\n-        )\n-        scale_factor = (feat_height / original_height, feat_width / original_width)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n-            reshaped_patch_pos_embed,\n-            scale_factor=scale_factor,\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n         return patch_pos_embed\n "
        },
        {
            "sha": "de327eb91d2d7d519a043fc1d891325cd8c907d5",
            "filename": "src/transformers/models/hiera/modeling_hiera.py",
            "status": "modified",
            "additions": 21,
            "deletions": 18,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhiera%2Fmodeling_hiera.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -38,6 +38,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_hiera import HieraConfig\n@@ -320,46 +321,48 @@ def interpolate_pos_encoding(\n         self, embeddings: torch.Tensor, pos_embeds: torch.Tensor, height: int, width: int\n     ) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing, no class embeddings, and different patch strides.\n \n         Adapted from:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n         num_patches = embeddings.shape[1]\n         num_positions = pos_embeds.shape[1]\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return pos_embeds\n+\n         dim = embeddings.shape[-1]\n-        h0 = height // self.patch_stride[0]\n-        w0 = width // self.patch_stride[1]\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        pos_embeds = pos_embeds.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_stride[0]\n+        new_width = width // self.patch_stride[1]\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        pos_embeds = pos_embeds.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         pos_embeds = pos_embeds.permute(0, 3, 1, 2)\n+\n         pos_embeds = nn.functional.interpolate(\n             pos_embeds,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n-        if int(h0) != pos_embeds.shape[-2] or int(w0) != pos_embeds.shape[-1]:\n-            raise ValueError(\"The interpolated position encoding does not have the right size\")\n+\n         pos_embeds = pos_embeds.permute(0, 2, 3, 1).view(1, -1, dim)\n         return pos_embeds\n \n     def get_position_embedding(\n         self, embeddings: torch.Tensor, height: int, width: int, interpolate_pos_encoding: bool\n     ) -> torch.FloatTensor:\n-        position_embeddings = self.position_embeddings\n-        position_embeddings = (\n-            self.interpolate_pos_encoding(embeddings, position_embeddings, height, width)\n+        return (\n+            self.interpolate_pos_encoding(embeddings, self.position_embeddings, height, width)\n             if interpolate_pos_encoding\n-            else position_embeddings\n+            else self.position_embeddings\n         )\n-        return position_embeddings\n \n     def forward(\n         self,"
        },
        {
            "sha": "ba77afe9f7c211b846940dde48e8bdb5d59a7846",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 26,
            "deletions": 17,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -38,6 +38,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from ..auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_instructblip import InstructBlipConfig, InstructBlipQFormerConfig, InstructBlipVisionConfig\n@@ -102,38 +103,46 @@ def __init__(self, config: InstructBlipVisionConfig):\n \n         self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n+\n         num_patches = embeddings.shape[1] - 1\n-        num_positions = self.position_embedding.shape[1] - 1\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n \n-        if num_patches == num_positions and height == width:\n-            return self.position_embedding\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n \n-        class_pos_embed = self.position_embedding[:, 0, :]\n-        patch_pos_embed = self.position_embedding[:, 1:, :]\n         dim = embeddings.shape[-1]\n-        h0 = height // self.config.patch_size\n-        w0 = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n         batch_size, _, height, width = pixel_values.shape"
        },
        {
            "sha": "8cb813e0ac571c2043ddec6d13a41b982daeab7e",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 26,
            "deletions": 17,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -44,6 +44,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from ..auto import AutoModelForCausalLM, AutoModelForSeq2SeqLM\n from .configuration_instructblipvideo import (\n@@ -110,38 +111,46 @@ def __init__(self, config: InstructBlipVideoVisionConfig):\n \n         self.position_embedding = nn.Parameter(torch.randn(1, self.num_positions, self.embed_dim))\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n+\n         num_patches = embeddings.shape[1] - 1\n-        num_positions = self.position_embedding.shape[1] - 1\n+        num_positions = self.position_embeddings.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n \n-        if num_patches == num_positions and height == width:\n-            return self.position_embedding\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n \n-        class_pos_embed = self.position_embedding[:, 0, :]\n-        patch_pos_embed = self.position_embedding[:, 1:, :]\n         dim = embeddings.shape[-1]\n-        h0 = height // self.config.patch_size\n-        w0 = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding: bool = False) -> torch.Tensor:\n         batch_size, _, height, width = pixel_values.shape"
        },
        {
            "sha": "9a40e050459816d90927b9e69881188db1a9f7bf",
            "filename": "src/transformers/models/maskformer/modeling_maskformer_swin.py",
            "status": "modified",
            "additions": 25,
            "deletions": 14,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fmodeling_maskformer_swin.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -29,6 +29,7 @@\n from ...modeling_outputs import BackboneOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, meshgrid, prune_linear_layer\n+from ...utils import torch_int\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_maskformer_swin import MaskFormerSwinConfig\n \n@@ -162,38 +163,48 @@ def __init__(self, config):\n \n         self.norm = nn.LayerNorm(config.embed_dim)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n-        class_pos_embed = self.position_embeddings[:, 0]\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        h0 = height // self.config.patch_size\n-        w0 = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(self, pixel_values, interpolate_pos_encoding):\n         _, num_channels, height, width = pixel_values.shape"
        },
        {
            "sha": "b6c233c7611218f7787f90f6d6d2effb1566de4b",
            "filename": "src/transformers/models/perceiver/modeling_perceiver.py",
            "status": "modified",
            "additions": 12,
            "deletions": 6,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperceiver%2Fmodeling_perceiver.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -37,6 +37,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_perceiver import PerceiverConfig\n \n@@ -2767,13 +2768,19 @@ def output_size(self, *args, **kwargs) -> int:\n \n     def interpolate_pos_encoding(self, position_embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         num_positions = position_embeddings.shape[0]\n-        new_height = new_width = math.sqrt(num_positions)\n-        position_embeddings = position_embeddings.reshape(\n-            1, int(new_height), int(new_width), self._num_channels\n-        ).permute(0, 3, 1, 2)\n+        new_height = new_width = torch_int(num_positions**0.5)\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and height == new_height and width == new_width:\n+            return position_embeddings\n+\n+        position_embeddings = position_embeddings.reshape(1, new_height, new_width, self._num_channels).permute(\n+            0, 3, 1, 2\n+        )\n+\n         position_embeddings = nn.functional.interpolate(\n             position_embeddings,\n-            scale_factor=(height / new_height, width / new_width),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n@@ -2787,7 +2794,6 @@ def forward(\n \n         if interpolate_pos_encoding:\n             height, width = input_size\n-            height, width = height + 0.1, width + 0.1\n             position_embeddings = self.interpolate_pos_encoding(position_embeddings, height, width)\n \n         if batch_size is not None:"
        },
        {
            "sha": "7befa4dad021f685c1d435f42436d3c68fb2c587",
            "filename": "src/transformers/models/pvt/modeling_pvt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpvt%2Fmodeling_pvt.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -123,7 +123,9 @@ def __init__(\n \n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         num_patches = height * width\n-        if num_patches == self.config.image_size * self.config.image_size:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == self.config.image_size * self.config.image_size:\n             return self.position_embeddings\n         embeddings = embeddings.reshape(1, height, width, -1).permute(0, 3, 1, 2)\n         interpolated_embeddings = F.interpolate(embeddings, size=(height, width), mode=\"bilinear\")"
        },
        {
            "sha": "174aeaad00ae41fb619f4da430bda473e52ab6a9",
            "filename": "src/transformers/models/seggpt/modeling_seggpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseggpt%2Fmodeling_seggpt.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -15,7 +15,6 @@\n \"\"\"PyTorch SegGpt model.\"\"\"\n \n import collections.abc\n-import math\n from dataclasses import dataclass\n from typing import Dict, List, Optional, Tuple, Union\n \n@@ -32,6 +31,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_seggpt import SegGptConfig\n \n@@ -155,9 +155,10 @@ def __init__(self, config: SegGptConfig) -> None:\n     def interpolate_pos_encoding(self, height: int, width: int) -> torch.Tensor:\n         patch_pos_embed = self.position_embeddings[:, 1:]\n         num_patches = patch_pos_embed.shape[1]\n-        pretrain_patch_size = int(math.sqrt(num_patches))\n+        pretrain_patch_size = torch_int(num_patches**0.5)\n \n-        if pretrain_patch_size != height or pretrain_patch_size != width:\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if torch.jit.is_tracing() or pretrain_patch_size != height or pretrain_patch_size != width:\n             patch_pos_embed = F.interpolate(\n                 patch_pos_embed.reshape(1, pretrain_patch_size, pretrain_patch_size, -1).permute(0, 3, 1, 2),\n                 size=(height, width),"
        },
        {
            "sha": "1d35d1d44cfd97f7ba3ddf25577c2e0f4e3a13d6",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 22,
            "deletions": 21,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -38,6 +38,7 @@\n     is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n \n@@ -269,38 +270,38 @@ def __init__(self, config: SiglipVisionConfig):\n \n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method is an adapted method for SigLIP (due to SigLIP not having class embedding unlike other ViTs)\n-        that allows the model to interpolate the pre-trained position encodings such that it can be usable on\n-        higher resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing and no class embeddings.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n-        position_embeddings = self.position_embedding.weight.unsqueeze(0)\n+\n         num_patches = embeddings.shape[1]\n-        num_positions = position_embeddings.shape[1]\n-        if num_patches == num_positions and height == width:\n-            return position_embeddings\n+        num_positions = self.position_embeddings.shape[1]\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embeddings\n+\n+        patch_pos_embed = self.position_embeddings\n \n         dim = embeddings.shape[-1]\n-        height = height // self.patch_size\n-        width = width // self.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        height, width = height + 0.1, width + 0.1\n-\n-        patch_pos_embed = position_embeddings.reshape(\n-            1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim\n-        )\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(height / math.sqrt(num_positions), width / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n-        if int(height) != patch_pos_embed.shape[-2] or int(width) != patch_pos_embed.shape[-1]:\n-            raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n \n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n         return patch_pos_embed"
        },
        {
            "sha": "45383a36d9bea82828eef9eef4916942db568566",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 25,
            "deletions": 14,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -251,38 +251,49 @@ def __init__(self, config, use_mask_token=False):\n \n         self.norm = nn.LayerNorm(config.embed_dim)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n+        self.config = config\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n-        class_pos_embed = self.position_embeddings[:, 0]\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        h0 = height // self.config.patch_size\n-        w0 = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(\n         self,"
        },
        {
            "sha": "0c30e739a48f911787f1cbe3310f949ac87fbbdf",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 26,
            "deletions": 14,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -36,6 +36,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from ...utils.backbone_utils import BackboneMixin\n from .configuration_swinv2 import Swinv2Config\n@@ -293,38 +294,49 @@ def __init__(self, config, use_mask_token=False):\n \n         self.norm = nn.LayerNorm(config.embed_dim)\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n+        self.config = config\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n-        class_pos_embed = self.position_embeddings[:, 0]\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        h0 = height // self.config.patch_size\n-        w0 = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(\n         self,"
        },
        {
            "sha": "76ebd18ed32d7bbe80cf81192d386d8e4f6fbf79",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 24,
            "deletions": 15,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -38,6 +38,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_vit import ViTConfig\n \n@@ -70,40 +71,48 @@ def __init__(self, config: ViTConfig, use_mask_token: bool = False) -> None:\n         num_patches = self.patch_embeddings.num_patches\n         self.position_embeddings = nn.Parameter(torch.randn(1, num_patches + 1, config.hidden_size))\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n         self.config = config\n \n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n-        class_pos_embed = self.position_embeddings[:, 0]\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        h0 = height // self.config.patch_size\n-        w0 = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n-        assert int(h0) == patch_pos_embed.shape[-2] and int(w0) == patch_pos_embed.shape[-1]\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(\n         self,"
        },
        {
            "sha": "f6444999ac12370f515296f9b06c1b3077362de0",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 25,
            "deletions": 17,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -35,6 +35,7 @@\n     add_start_docstrings_to_model_forward,\n     logging,\n     replace_return_docstrings,\n+    torch_int,\n )\n from .configuration_vit_mae import ViTMAEConfig\n \n@@ -206,6 +207,7 @@ def __init__(self, config):\n         self.position_embeddings = nn.Parameter(\n             torch.zeros(1, self.num_patches + 1, config.hidden_size), requires_grad=False\n         )\n+        self.patch_size = config.patch_size\n         self.config = config\n         self.initialize_weights()\n \n@@ -223,40 +225,46 @@ def initialize_weights(self):\n         # timm's trunc_normal_(std=.02) is effectively normal_(std=0.02) as cutoff is too big (2.)\n         torch.nn.init.normal_(self.cls_token, std=self.config.initializer_range)\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n+\n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n \n-        if num_patches == num_positions and height == width:\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n \n-        class_pos_embed = self.position_embeddings[:, 0, :]\n-        patch_pos_embed = self.position_embeddings[:, 1:, :]\n+        class_pos_embed = self.position_embeddings[:, :1]\n+        patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        h0 = height // self.config.patch_size\n-        w0 = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n-        if int(h0) != patch_pos_embed.shape[-2] or int(w0) != patch_pos_embed.shape[-1]:\n-            raise ValueError(\"Width or height does not match with the interpolated position embeddings\")\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def random_masking(self, sequence, noise=None):\n         \"\"\""
        },
        {
            "sha": "b962ac597dabb8b6556ac5e08dfdff4496ebdb4b",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 31,
            "deletions": 18,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -27,7 +27,13 @@\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+    torch_int,\n+)\n from .configuration_vit_msn import ViTMSNConfig\n \n \n@@ -52,42 +58,49 @@ def __init__(self, config: ViTMSNConfig, use_mask_token: bool = False) -> None:\n         num_patches = self.patch_embeddings.num_patches\n         self.position_embeddings = nn.Parameter(torch.zeros(1, num_patches + 1, config.hidden_size))\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n+        self.patch_size = config.patch_size\n         self.config = config\n \n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n     def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n-        class_pos_embed = self.position_embeddings[:, 0]\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        patch_window_height = height // self.config.patch_size\n-        patch_window_width = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        patch_window_height, patch_window_width = patch_window_height + 0.1, patch_window_width + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(\n-                patch_window_height / math.sqrt(num_positions),\n-                patch_window_width / math.sqrt(num_positions),\n-            ),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(\n         self,"
        },
        {
            "sha": "972040264fec314ebaedef41042d4a34689eb832",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 31,
            "deletions": 16,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -26,7 +26,13 @@\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n from ...modeling_utils import PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n-from ...utils import add_start_docstrings, add_start_docstrings_to_model_forward, logging, replace_return_docstrings\n+from ...utils import (\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    logging,\n+    replace_return_docstrings,\n+    torch_int,\n+)\n from .configuration_vivit import VivitConfig\n \n \n@@ -100,37 +106,46 @@ def __init__(self, config):\n         self.dropout = nn.Dropout(config.hidden_dropout_prob)\n         self.config = config\n \n-    def interpolate_pos_encoding(self, embeddings, height, width):\n+    # Copied from transformers.models.vit.modeling_vit.ViTEmbeddings.interpolate_pos_encoding\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n         \"\"\"\n-        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher\n-        resolution images.\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n \n-        Source:\n-        https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n         \"\"\"\n \n         num_patches = embeddings.shape[1] - 1\n         num_positions = self.position_embeddings.shape[1] - 1\n-        if num_patches == num_positions and height == width:\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n             return self.position_embeddings\n-        class_pos_embed = self.position_embeddings[:, 0]\n+\n+        class_pos_embed = self.position_embeddings[:, :1]\n         patch_pos_embed = self.position_embeddings[:, 1:]\n+\n         dim = embeddings.shape[-1]\n-        h0 = height // self.config.patch_size\n-        w0 = width // self.config.patch_size\n-        # we add a small number to avoid floating point error in the interpolation\n-        # see discussion at https://github.com/facebookresearch/dino/issues/8\n-        h0, w0 = h0 + 0.1, w0 + 0.1\n-        patch_pos_embed = patch_pos_embed.reshape(1, int(math.sqrt(num_positions)), int(math.sqrt(num_positions)), dim)\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n         patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n         patch_pos_embed = nn.functional.interpolate(\n             patch_pos_embed,\n-            scale_factor=(h0 / math.sqrt(num_positions), w0 / math.sqrt(num_positions)),\n+            size=(new_height, new_width),\n             mode=\"bicubic\",\n             align_corners=False,\n         )\n+\n         patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n-        return torch.cat((class_pos_embed.unsqueeze(0), patch_pos_embed), dim=1)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n \n     def forward(self, pixel_values, interpolate_pos_encoding: bool = False):\n         batch_size, num_frames, num_channels, height, width = pixel_values.shape"
        },
        {
            "sha": "b118d6db5af61aacc131d597046d6c2d71c76885",
            "filename": "tests/models/hiera/test_modeling_hiera.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6d2848a23aba42404784ba52e421ae7b8c68eda/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6d2848a23aba42404784ba52e421ae7b8c68eda/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhiera%2Ftest_modeling_hiera.py?ref=c6d2848a23aba42404784ba52e421ae7b8c68eda",
            "patch": "@@ -578,7 +578,7 @@ def test_inference_interpolate_pos_encoding(self):\n         self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n \n         expected_slice = torch.tensor(\n-            [[1.8522, 0.1532, 0.3849], [2.7352, -0.1941, 0.1848], [1.5859, -0.0773, 0.0168]]\n+            [[1.7853, 0.0690, 0.3177], [2.6853, -0.2334, 0.0889], [1.5445, -0.1515, -0.0300]]\n         ).to(torch_device)\n \n         self.assertTrue(torch.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))"
        }
    ],
    "stats": {
        "total": 927,
        "additions": 558,
        "deletions": 369
    }
}