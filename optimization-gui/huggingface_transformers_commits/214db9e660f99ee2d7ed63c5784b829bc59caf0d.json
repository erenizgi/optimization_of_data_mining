{
    "author": "chengchengpei",
    "message": "add back self.max_position_embeddings = config.max_position_embeddings (#33550)\n\n* add back self.max_position_embeddings = config.max_position_embeddings\r\n\r\n* fix-copies",
    "sha": "214db9e660f99ee2d7ed63c5784b829bc59caf0d",
    "files": [
        {
            "sha": "1e79115d34701b0678491a9dbce021e593f3087d",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/214db9e660f99ee2d7ed63c5784b829bc59caf0d/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214db9e660f99ee2d7ed63c5784b829bc59caf0d/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=214db9e660f99ee2d7ed63c5784b829bc59caf0d",
            "patch": "@@ -310,6 +310,7 @@ def __init__(self, config: Qwen2Config, layer_idx: Optional[int] = None):\n         self.head_dim = self.hidden_size // self.num_heads\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.max_position_embeddings = config.max_position_embeddings\n         self.rope_theta = config.rope_theta\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout"
        },
        {
            "sha": "c9ee7b5f57a1f2bd7c2d9c113efe65b4b116cd42",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/214db9e660f99ee2d7ed63c5784b829bc59caf0d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/214db9e660f99ee2d7ed63c5784b829bc59caf0d/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=214db9e660f99ee2d7ed63c5784b829bc59caf0d",
            "patch": "@@ -388,6 +388,7 @@ def __init__(self, config: Qwen2MoeConfig, layer_idx: Optional[int] = None):\n         self.head_dim = self.hidden_size // self.num_heads\n         self.num_key_value_heads = config.num_key_value_heads\n         self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n+        self.max_position_embeddings = config.max_position_embeddings\n         self.rope_theta = config.rope_theta\n         self.is_causal = True\n         self.attention_dropout = config.attention_dropout"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}