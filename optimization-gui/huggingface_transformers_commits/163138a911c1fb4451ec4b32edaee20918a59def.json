{
    "author": "Cyrilvallez",
    "message": "ðŸš¨ðŸš¨[core] Completely rewrite the masking logic for all attentions (#37866)\n\n* start\n\n* start having a clean 4d mask primitive\n\n* Update mask_utils.py\n\n* Update mask_utils.py\n\n* switch name\n\n* Update masking_utils.py\n\n* add a new AttentionMask tensor class\n\n* fix import\n\n* nits\n\n* fixes\n\n* use full and quandrants\n\n* general sdpa mask for all caches\n\n* style\n\n* start some tests\n\n* tests with sliding, chunked\n\n* add styling\n\n* test hybrid\n\n* Update masking_utils.py\n\n* small temp fixes\n\n* Update modeling_gemma2.py\n\n* compile compatible\n\n* Update masking_utils.py\n\n* improve\n\n* start making it more general\n\n* Update masking_utils.py\n\n* generate\n\n* make it work with flex style primitives!\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* improve\n\n* Update cache_utils.py\n\n* Update masking_utils.py\n\n* simplify - starting to look good!\n\n* Update masking_utils.py\n\n* name\n\n* Update masking_utils.py\n\n* style\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* small fix for flex\n\n* flex compile\n\n* FA2\n\n* Update masking_utils.py\n\n* Escape for TGI/vLLM!\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* General case without cache\n\n* rename\n\n* full test on llama4\n\n* small fix for FA2 guard with chunk\n\n* Update modeling_gemma2.py\n\n* post rebase cleanup\n\n* FA2 supports static cache!\n\n* Update modeling_flash_attention_utils.py\n\n* Update flex_attention.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* Update utils.py\n\n* override for export\n\n* Update executorch.py\n\n* Update executorch.py\n\n* Update executorch.py\n\n* Update executorch.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* output attentions\n\n* style\n\n* Update masking_utils.py\n\n* Update executorch.py\n\n* Add doicstring\n\n* Add license and put mask visualizer at the end\n\n* Update test_modeling_common.py\n\n* fix broken test\n\n* Update test_modeling_gemma.py\n\n* Update test_modeling_gemma2.py\n\n* Use fullgraph=False with FA2\n\n* Update utils.py\n\n* change name\n\n* Update masking_utils.py\n\n* improve doc\n\n* change name\n\n* Update modeling_attn_mask_utils.py\n\n* more explicit logic based on model's property\n\n* pattern in config\n\n* extend\n\n* fixes\n\n* make it better\n\n* generalize to other test models\n\n* fix\n\n* Update masking_utils.py\n\n* fix\n\n* do not check mask equivalence if layer types are different\n\n* executorch\n\n* Update modeling_gemma2.py\n\n* Update masking_utils.py\n\n* use layer_idx instead\n\n* adjust\n\n* Update masking_utils.py\n\n* test\n\n* fix imports\n\n* Update modeling_gemma2.py\n\n* other test models\n\n* Update modeling_llama4.py\n\n* Update masking_utils.py\n\n* improve\n\n* simplify\n\n* Update masking_utils.py\n\n* typos\n\n* typo\n\n* fix\n\n* Update masking_utils.py\n\n* default DynamicCache\n\n* remove default cache\n\n* simplify\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* simplify\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* export\n\n* Update executorch.py\n\n* Update executorch.py\n\n* Update flex_attention.py\n\n* Update executorch.py\n\n* upstream to modular gemma 1 & 2\n\n* Update modular_mistral.py\n\n* switch names\n\n* use dict\n\n* put it in the Layer directly\n\n* update copy model source for mask functions\n\n* apply so many modular (hopefully 1 shot)\n\n* use explicite dicts for make style happy\n\n* protect import\n\n* check docstring\n\n* better default in hybrid caches\n\n* qwens\n\n* Update modular_qwen2.py\n\n* simplify core logic!\n\n* Update executorch.py\n\n* qwen3 moe\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* simplify a lot sdpa causal skip\n\n* Update masking_utils.py\n\n* post-rebase\n\n* gemma3 finally\n\n* style\n\n* check it before\n\n* gemma3\n\n* More general with newer torch\n\n* align gemma3\n\n* Update utils.py\n\n* Update utils.py\n\n* Update masking_utils.py\n\n* Update test_modeling_common.py\n\n* Update flex_attention.py\n\n* Update flex_attention.py\n\n* Update flex_attention.py\n\n* test\n\n* executorch\n\n* Update test_modeling_common.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* Update masking_utils.py\n\n* Update executorch.py\n\n* Update test_modeling_common.py\n\n* fix copies\n\n* device\n\n* sdpa can be used without mask -> pass the torchscript tests in this case\n\n* Use enum for check\n\n* revert enum and add check instead\n\n* remove broken test\n\n* cohere2\n\n* some doc & reorganize the Interface\n\n* Update tensor_parallel.py\n\n* Update tensor_parallel.py\n\n* doc and dummy\n\n* Update test_modeling_paligemma2.py\n\n* Update modeling_falcon_h1.py\n\n* Update masking_utils.py\n\n* executorch patch\n\n* style\n\n* CIs\n\n* use register in executorch\n\n* final comments!\n\n---------\n\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>",
    "sha": "163138a911c1fb4451ec4b32edaee20918a59def",
    "files": [
        {
            "sha": "76f264d83ee1979e4484c4563eec799098074001",
            "filename": "docs/source/en/attention_interface.md",
            "status": "modified",
            "additions": 41,
            "deletions": 1,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/docs%2Fsource%2Fen%2Fattention_interface.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/docs%2Fsource%2Fen%2Fattention_interface.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fattention_interface.md?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -125,4 +125,44 @@ would expect from a usual Python dictionary:\n \n # You can also globally `register` a new function directly on it\n >>> ALL_ATTENTION_FUNCTIONS.register(\"new_func\", new_func)\n-```\n\\ No newline at end of file\n+```\n+\n+## Attention Mask Interface\n+\n+Having a new attention function may mean that you need a new format of attention mask to decide what key and value tokens\n+the query tokens should attend to. This is now possible with the `AttentionMaskInterface`! It works in the same way as\n+the `AttentionInterface`:\n+\n+```python\n+from transformers import AttentionMaskInterface\n+from transformers.masking_utils import sdpa_mask\n+import torch\n+\n+def my_new_sdpa_mask(*args, **kwargs):\n+    print(\"I just entered the attention mask computation\")\n+    return sdpa_mask(*args, **kwargs)\n+\n+AttentionMaskInterface.register(\"my_new_sdpa_mask\", my_new_sdpa_mask)\n+```\n+\n+The reason you have to register it is because we need to automatically correct your mask format based on the attention implementation (for example, flex attention uses a BlockMask format, while sdpa uses a 4D tensor).\n+By default, if you do not register an attention mask function along with your attention function, mask creation will be skipped\n+and `attention_mask=None` will be passed along to the Attention layers.\n+\n+The default signature of the attention mask functions is the following:\n+\n+```python\n+def custom_attention_mask(\n+    batch_size: int,  # required arg\n+    cache_position: torch.Tensor,  # required arg\n+    kv_length: int,  # required arg\n+    kv_offset: int = 0,  # required arg\n+    mask_function: Callable = causal_mask_function,  # required arg\n+    attention_mask: Optional[torch.Tensor] = None,  # required arg\n+    **kwargs,  # a few additional args may be passed as kwargs, especially the model's config is always passed\n+) -> Optional[torch.Tensor]:\n+```\n+\n+It mostly works thanks to the `mask_function`, which is a `Callable` in the form of [torch's mask_mod functions](https://pytorch.org/blog/flexattention/), taking 4 indices as input and returning a boolean to indicate if this position should take part in the attention computation.\n+\n+If you cannot use the `mask_function` to create your mask for some reason, you can try to work around it by doing something similar to our [torch export workaround](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/executorch.py).\n\\ No newline at end of file"
        },
        {
            "sha": "11f13de081b2055172f8a94a338feb392581976f",
            "filename": "docs/source/en/internal/modeling_utils.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodeling_utils.md?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -29,6 +29,11 @@ Most of those are only useful if you are studying the code of the models in the\n [[autodoc]] AttentionInterface\n     - register\n \n+## Attention Mask Functions\n+\n+[[autodoc]] AttentionMaskInterface\n+    - register\n+\n ## Rotary Position Embedding Functions\n \n [[autodoc]] dynamic_rope_update"
        },
        {
            "sha": "155d0fd6d39cf661286d5f9ac7803cbf91ff552f",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -445,6 +445,7 @@\n     _import_structure[\"modeling_outputs\"] = []\n     _import_structure[\"modeling_rope_utils\"] = [\"ROPE_INIT_FUNCTIONS\", \"dynamic_rope_update\"]\n     _import_structure[\"modeling_utils\"] = [\"PreTrainedModel\", \"AttentionInterface\"]\n+    _import_structure[\"masking_utils\"] = [\"AttentionMaskInterface\"]\n     _import_structure[\"optimization\"] = [\n         \"Adafactor\",\n         \"get_constant_schedule\",\n@@ -914,6 +915,7 @@\n             TorchExportableModuleWithStaticCache,\n             convert_and_export_with_cache,\n         )\n+        from .masking_utils import AttentionMaskInterface\n         from .model_debugging_utils import (\n             model_addition_debugger_context,\n         )"
        },
        {
            "sha": "c0bd42f2e394e1a258aa92ca208bdeed2db95e18",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 103,
            "deletions": 16,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -196,6 +196,18 @@ def seen_tokens(self):\n         else:\n             return None\n \n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n+        for each layer.\n+        \"\"\"\n+        query_length = cache_position.shape[0]\n+        past_seen_tokens = self.get_seq_length()\n+        kv_length = query_length + past_seen_tokens\n+        return kv_length, 0\n+\n \n @dataclass\n class CacheConfig:\n@@ -1084,8 +1096,6 @@ class SinkCache(Cache):\n         ```\n     \"\"\"\n \n-    is_sliding = True\n-\n     def __init__(self, window_length: int, num_sink_tokens: int) -> None:\n         super().__init__()\n         self.key_cache: List[torch.Tensor] = []\n@@ -1390,6 +1400,16 @@ def reset(self):\n             self.key_cache[layer_idx].zero_()\n             self.value_cache[layer_idx].zero_()\n \n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n+        for each layer.\n+        \"\"\"\n+        kv_length = self.get_max_cache_shape()\n+        return kv_length, 0\n+\n \n class SlidingWindowCache(StaticCache):\n     \"\"\"\n@@ -1446,7 +1466,6 @@ class SlidingWindowCache(StaticCache):\n         ```\n     \"\"\"\n \n-    is_sliding = True\n     is_compileable = True\n \n     def __init__(\n@@ -1465,6 +1484,7 @@ def __init__(\n                 \"config and it's not set to None.\"\n             )\n         max_cache_len = min(config.sliding_window, max_cache_len)\n+        self.sliding_window = config.sliding_window\n         super().__init__(\n             config=config,\n             max_batch_size=max_batch_size,\n@@ -1509,6 +1529,21 @@ def reset(self):\n             self.key_cache[layer_idx].zero_()\n             self.value_cache[layer_idx].zero_()\n \n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n+        for each layer.\n+        \"\"\"\n+        query_length = cache_position.shape[0]\n+        first_cache_position = cache_position[0]\n+        # torch.clamp() is equivalent to max() but should be compile-friendly/exportable as first_cache_position is a Tensor\n+        kv_offset = torch.clamp(first_cache_position - self.sliding_window + 1, min=0)\n+        # This is not general (see HybridChunkedCache for the whole general case), but it's what the cache returns\n+        kv_length = max(query_length, self.get_max_cache_shape())\n+        return kv_length, kv_offset\n+\n \n class EncoderDecoderCache(Cache):\n     \"\"\"\n@@ -1761,12 +1796,17 @@ def __init__(\n             else config.num_key_value_heads\n         )\n \n-        layer_switch = config.sliding_window_pattern if hasattr(config, \"sliding_window_pattern\") else 2  # 2 is for BC\n-        self.is_sliding_list = [bool((i + 1) % layer_switch) for i in range(config.num_hidden_layers)]\n+        # If the attribute does not exist in the config, fallback to a simple StaticCache\n+        if hasattr(config, \"layer_types\"):\n+            self.is_sliding = [layer_type != \"full_attention\" for layer_type in config.layer_types]\n+        else:\n+            self.is_sliding = [False] * config.num_hidden_layers\n+\n         self.key_cache: List[torch.Tensor] = []\n         self.value_cache: List[torch.Tensor] = []\n         global_cache_shape = (self.max_batch_size, self.num_key_value_heads, self.max_cache_len, self.head_dim)\n         sliding_cache_shape = (self.max_batch_size, self.num_key_value_heads, self.sliding_window_len, self.head_dim)\n+        self.sliding_window = min(config.sliding_window, max_cache_len)\n         device = torch.device(device) if device is not None else None\n         for i in range(config.num_hidden_layers):\n             if layer_device_map is not None:\n@@ -1775,7 +1815,7 @@ def __init__(\n                 layer_device = device\n             # Note: `mark_static_address` is used to tag the cache as an fixed data pointer, preventing cuda graph\n             # breaks when updating the cache.\n-            cache_shape = sliding_cache_shape if self.is_sliding_list[i] else global_cache_shape\n+            cache_shape = sliding_cache_shape if self.is_sliding[i] else global_cache_shape\n             new_layer_key_cache = torch.zeros(cache_shape, dtype=self._dtype, device=layer_device)\n             new_layer_value_cache = torch.zeros(cache_shape, dtype=self._dtype, device=layer_device)\n             torch._dynamo.mark_static_address(new_layer_key_cache)\n@@ -1796,7 +1836,7 @@ def update(\n         if cache_position is None:\n             raise ValueError(\"`cache_position` must be provided for HybridCache.\")\n \n-        is_sliding_layer = self.is_sliding_list[layer_idx]\n+        is_sliding_layer = self.is_sliding[layer_idx]\n \n         # These two `if` blocks are only reached in multigpu and if `layer_device_map` is not passed. They are used\n         # when the cache is initialized in the forward pass (e.g. Gemma2)\n@@ -1843,6 +1883,26 @@ def reset(self):\n             self.key_cache[layer_idx].zero_()\n             self.value_cache[layer_idx].zero_()\n \n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n+        for each layer.\n+        \"\"\"\n+        if self.is_sliding[layer_idx]:\n+            query_length = cache_position.shape[0]\n+            first_cache_position = cache_position[0]\n+\n+            local_mask_kv_offset = torch.clamp(first_cache_position - self.sliding_window + 1, min=0)\n+            # This is not general (see HybridChunkedCache for the whole general case), but it's what the cache returns\n+            local_mask_kv_length = max(query_length, self.sliding_window)\n+            return local_mask_kv_length, local_mask_kv_offset\n+\n+        full_mask_kv_offset = 0\n+        full_mask_kv_length = self.get_max_cache_shape()\n+        return full_mask_kv_length, full_mask_kv_offset\n+\n \n class HybridChunkedCache(Cache):\n     \"\"\"\n@@ -1912,11 +1972,11 @@ def __init__(\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n         self._dtype = dtype\n \n-        if hasattr(config.get_text_config(), \"no_rope_layers\"):\n-            self.is_sliding = config.no_rope_layers\n+        # If the attribute does not exist in the config, fallback to a simple StaticCache\n+        if hasattr(config, \"layer_types\"):\n+            self.is_sliding = [layer_type != \"full_attention\" for layer_type in config.layer_types]\n         else:\n-            layer_switch = getattr(config, \"sliding_window_pattern\", 2)\n-            self.is_sliding = [bool((i + 1) % layer_switch) for i in range(config.num_hidden_layers)]\n+            self.is_sliding = [False] * config.num_hidden_layers\n \n         self.key_cache: List[torch.Tensor] = []\n         self.value_cache: List[torch.Tensor] = []\n@@ -1999,11 +2059,7 @@ def update(\n         key_states = key_states.to(k_out.dtype)\n         value_states = value_states.to(v_out.dtype)\n \n-        if self.is_sliding[layer_idx]:\n-            update_fn = self._sliding_update\n-        else:\n-            update_fn = self._static_update\n-\n+        update_fn = self._sliding_update if self.is_sliding[layer_idx] else self._static_update\n         return update_fn(\n             cache_position,\n             layer_idx,\n@@ -2038,6 +2094,37 @@ def reset(self):\n             self.value_cache[layer_idx].zero_()\n         self.cumulative_length = [0 for _ in range(len(self.cumulative_length))]\n \n+    def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n+        \"\"\"\n+        Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n+        the given layer at `layer_idx`.\n+        The masks are then prepared according to the given lengths (kv_length, kv_offset) and patterns (i.e. sliding_window, chunk_size),\n+        for each layer.\n+        \"\"\"\n+        if self.is_sliding[layer_idx]:\n+            query_length = cache_position.shape[0]\n+            first_cache_position = cache_position[0]\n+\n+            local_mask_kv_offset = torch.clamp(first_cache_position - self.sliding_window + 1, min=0)\n+            # This is the true general case for any Cache using local attention (sliding or chunked)\n+            if first_cache_position >= self.sliding_window:\n+                # Here the Cache is already full\n+                local_mask_kv_length = self.sliding_window + query_length - 1\n+            elif (\n+                first_cache_position < self.sliding_window\n+                and first_cache_position + query_length > self.sliding_window\n+            ):\n+                # Here the Cache becomes full with the new input\n+                local_mask_kv_length = first_cache_position + query_length\n+            else:\n+                # Here the Cache is still smaller than the local size, but we return the local size as it's static\n+                local_mask_kv_length = self.sliding_window\n+            return local_mask_kv_length, local_mask_kv_offset\n+\n+        full_mask_kv_offset = 0\n+        full_mask_kv_length = self.get_max_cache_shape()\n+        return full_mask_kv_length, full_mask_kv_offset\n+\n \n class OffloadedHybridCache(HybridChunkedCache):\n     def __init__("
        },
        {
            "sha": "6e75fbfb54aea84326d797a607a056e812a9711f",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1209,3 +1209,16 @@ def recursive_diff_dict(dict_a, dict_b, config_obj=None):\n     PretrainedConfig.push_to_hub.__doc__ = PretrainedConfig.push_to_hub.__doc__.format(\n         object=\"config\", object_class=\"AutoConfig\", object_files=\"configuration file\"\n     )\n+\n+\n+ALLOWED_LAYER_TYPES = (\n+    \"full_attention\",\n+    \"sliding_attention\",\n+    \"chunked_attention\",\n+)\n+\n+\n+def layer_type_validation(layer_types: list[str]):\n+    \"\"\"Check that each entry in `layer_types` are allowed.\"\"\"\n+    if not all(layer_type in ALLOWED_LAYER_TYPES for layer_type in layer_types):\n+        raise ValueError(f\"The `layer_types` entries must be in {ALLOWED_LAYER_TYPES}\")"
        },
        {
            "sha": "49dc4b8df7297b0c07bbc893d46d48a0416e2040",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 30,
            "deletions": 5,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -46,6 +46,7 @@\n )\n from ..integrations.deepspeed import is_deepspeed_zero3_enabled\n from ..integrations.fsdp import is_fsdp_managed_module\n+from ..masking_utils import create_masks_for_generate\n from ..modeling_outputs import CausalLMOutputWithPast, Seq2SeqLMOutput\n from ..pytorch_utils import isin_mps_friendly\n from ..tokenization_utils import ExtensionsTrie\n@@ -74,6 +75,7 @@\n from .configuration_utils import (\n     NEED_SETUP_CACHE_CLASSES_MAPPING,\n     QUANT_BACKEND_CLASSES_MAPPING,\n+    CompileConfig,\n     GenerationConfig,\n     GenerationMode,\n )\n@@ -649,12 +651,22 @@ def prepare_inputs_for_generation(\n                 causal_mask_creation_function = getattr(\n                     decoder, \"_prepare_4d_causal_attention_mask_with_cache_position\", None\n                 )\n+\n+            # If it's not defined, it means the model uses the new general mask API\n             if causal_mask_creation_function is None:  # can't be found\n-                logger.warning_once(\n-                    f\"{self.__class__.__name__} has no `_prepare_4d_causal_attention_mask_with_cache_position` method \"\n-                    \"defined in its base modeling class. Compiled forward passes will be sub-optimal. If you're \"\n-                    \"writing code, see Llama for an example implementation. If you're a user, please report this \"\n-                    \"issue on GitHub.\"\n+                output_attentions = kwargs.get(\"output_attentions\", False)\n+                token_type_ids = getattr(model_input, \"token_type_ids\", None)\n+                # Some models may overwrite the general one\n+                causal_mask_creation_function = getattr(self, \"create_masks_for_generate\", create_masks_for_generate)\n+                attention_mask = causal_mask_creation_function(\n+                    config=self.config,\n+                    # we only need batch size, seq_length and dtype here - we don't care about the values of the embeddings\n+                    input_embeds=torch.empty((batch_size, sequence_length), dtype=self.dtype),\n+                    attention_mask=attention_mask,\n+                    cache_position=cache_position,\n+                    past_key_values=past_key_values,\n+                    output_attentions=output_attentions,\n+                    token_type_ids=token_type_ids,\n                 )\n             else:\n                 attention_mask = causal_mask_creation_function(\n@@ -3533,6 +3545,19 @@ def _sample(\n         compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n         if compile_forward:\n             os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n+            # If we use FA2 and a static cache, we cannot compile with fullgraph\n+            if self.config._attn_implementation == \"flash_attention_2\" and getattr(\n+                model_kwargs.get(\"past_key_values\"), \"is_compileable\", False\n+            ):\n+                if generation_config.compile_config is None:\n+                    generation_config.compile_config = CompileConfig(fullgraph=False)\n+                # only raise warning if the user passed an explicit compile-config (otherwise, simply change the default without confusing the user)\n+                elif generation_config.compile_config.fullgraph:\n+                    logger.warning_once(\n+                        \"When using Flash Attention 2 and a static cache, you cannot use the option `CompileConfig(fullgraph=True)` as \"\n+                        \"FA2 introduces graph breaks. We overrode the option with `fullgraph=False`.\"\n+                    )\n+                    generation_config.compile_config.fullgraph = False\n             model_forward = self.get_compiled_call(generation_config.compile_config)\n \n         if generation_config.prefill_chunk_size is not None:"
        },
        {
            "sha": "eb17dab55af7fe68893e272381c809acf99fc8b1",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 216,
            "deletions": 72,
            "changes": 288,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -11,18 +11,21 @@\n # specific language governing permissions and limitations under the License.\n \n import logging\n-from typing import Optional\n+from contextlib import contextmanager\n+from typing import Callable, Optional\n \n import torch\n \n-from transformers.generation.configuration_utils import GenerationConfig\n-\n-from ..utils.import_utils import is_torch_available\n-\n-\n-if is_torch_available():\n-    from transformers import HybridCache, PreTrainedModel, StaticCache\n-    from transformers.pytorch_utils import is_torch_greater_or_equal, is_torch_greater_or_equal_than_2_3\n+from ..cache_utils import DynamicCache, HybridCache, StaticCache\n+from ..generation.configuration_utils import GenerationConfig\n+from ..masking_utils import (\n+    ALL_MASK_ATTENTION_FUNCTIONS,\n+    _ignore_causal_mask_sdpa,\n+    _is_torch_greater_or_equal_than_2_5,\n+    prepare_padding_mask,\n+)\n+from ..modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ..pytorch_utils import is_torch_greater_or_equal, is_torch_greater_or_equal_than_2_3\n \n \n class TorchExportableModuleForDecoderOnlyLM(torch.nn.Module):\n@@ -54,19 +57,13 @@ def __init__(\n         if not hasattr(model.config, \"use_cache\") or model.config.use_cache is False:\n             raise ValueError(\"The model must have caching enabled to be performant.\")\n \n-        if not hasattr(model.config, \"cache_implementation\"):\n-            # If `cache_implementation` is not specified explicitly in the config, `DynamicCache` will\n-            # be used by default, so export will use `StaticCache` by default.\n-            logging.info(\"Using `StaticCache` for export as `cache_implementation` is not specified in the config.\")\n+        if not hasattr(model.config, \"layer_types\"):\n+            # If `layer_types` is not specified explicitly in the config, there is only 1 type of layers, so\n+            # export will use `StaticCache` by default.\n+            logging.info(\"Using `StaticCache` for export as `layer_types` is not specified in the config.\")\n             self.model = TorchExportableModuleWithStaticCache(model)\n         else:\n-            if model.config.cache_implementation == \"hybrid\":\n-                self.model = TorchExportableModuleWithHybridCache(model, max_batch_size, max_cache_len)\n-            else:\n-                raise ValueError(\n-                    f\"Unsupported cache implementation: {model.config.cache_implementation}. \"\n-                    \"Please use `hybrid` or `static`.\"\n-                )\n+            self.model = TorchExportableModuleWithHybridCache(model, max_batch_size, max_cache_len)\n \n     def forward(\n         self,\n@@ -105,16 +102,23 @@ def export(\n             strict(`Optional[bool]`):\n                 Flag to instruct `torch.export` to use `torchdynamo`.\n         \"\"\"\n+        # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n+        ALL_MASK_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_mask_without_vmap)\n+        ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n+        self.model.model.config._attn_implementation = \"sdpa_without_vmap\"\n+\n         example_input_ids = input_ids if input_ids is not None else torch.tensor([[1]], dtype=torch.long)\n         example_cache_position = cache_position if cache_position is not None else torch.tensor([0], dtype=torch.long)\n \n-        return torch.export.export(\n-            self.model,\n-            args=(example_input_ids, example_cache_position),\n-            kwargs={},\n-            dynamic_shapes=dynamic_shapes,\n-            strict=strict if strict is not None else True,\n-        )\n+        with patch_mask_interface():\n+            exported_program = torch.export.export(\n+                self.model,\n+                args=(example_input_ids, example_cache_position),\n+                kwargs={},\n+                dynamic_shapes=dynamic_shapes,\n+                strict=strict if strict is not None else True,\n+            )\n+        return exported_program\n \n     @staticmethod\n     def generate(\n@@ -281,17 +285,6 @@ def __init__(self, model: PreTrainedModel):\n             self.register_buffer(f\"key_cache_{i}\", self.static_cache.key_cache[i], persistent=False)\n             self.register_buffer(f\"value_cache_{i}\", self.static_cache.value_cache[i], persistent=False)\n \n-        self.is_causal = any(\"CausalLM\" in arch for arch in self.model.config.architectures)\n-        if self.is_causal:\n-            causal_mask = torch.tril(\n-                torch.ones(\n-                    self.static_cache.max_cache_len,\n-                    self.static_cache.max_cache_len,\n-                    dtype=torch.bool,\n-                )\n-            )\n-            self.register_buffer(\"mask\", causal_mask, persistent=False)\n-\n     def forward(self, input_ids: torch.Tensor, cache_position: torch.Tensor):\n         \"\"\"\n         Forward pass of the module, which is compatible with the ExecuTorch runtime.\n@@ -314,13 +307,12 @@ def forward(self, input_ids: torch.Tensor, cache_position: torch.Tensor):\n             ensuring that the exported model can be executed in `ExecuTorch` out-of-the-box.\n         \"\"\"\n         _, seqlen = input_ids.shape\n-        attn_mask = self.mask[cache_position, :seqlen] if self.is_causal else None\n         position_ids = cache_position.unsqueeze(0)\n         past_key_values = self.static_cache\n \n         outs = self.model(\n             input_ids=input_ids,\n-            attention_mask=attn_mask,\n+            attention_mask=None,\n             position_ids=position_ids,\n             cache_position=cache_position,\n             past_key_values=past_key_values,\n@@ -445,18 +437,15 @@ def forward(\n         Returns:\n             torch.Tensor: Logits output from the model.\n         \"\"\"\n-        batch_size, seq_len = input_ids.shape\n+        batch_size = input_ids.shape[0]\n \n         # Generate position_ids from cache_position\n         position_ids = cache_position.unsqueeze(0).expand(batch_size, -1)\n \n-        # Create attention mask (always ones for token-by-token generation)\n-        attention_mask = torch.ones((batch_size, seq_len), dtype=torch.long, device=input_ids.device)\n-\n         # Forward pass with the model\n         outputs = self.model(\n             input_ids=input_ids,\n-            attention_mask=attention_mask,\n+            attention_mask=None,\n             position_ids=position_ids,\n             past_key_values=self.cache,\n             use_cache=True,\n@@ -467,6 +456,24 @@ def forward(\n         return outputs.logits\n \n \n+@contextmanager\n+def patch_mask_interface():\n+    \"\"\"\n+    Context manager to locally use a simple dict instead of `AttentionMaskInterface`, as otherwise export will fail\n+    with `strict=True` due to dynamo skip rules, i.e. `torch._dynamo.exc.Unsupported: 'inline in skipfiles:\n+    Mapping.__contains__ | __contains__, skipped according trace_rules.lookup SKIP_DIRS'`.\n+    Note that this seem to be an issue only for python<3.11.\n+    \"\"\"\n+    import transformers\n+\n+    original = transformers.masking_utils.ALL_MASK_ATTENTION_FUNCTIONS\n+    transformers.masking_utils.ALL_MASK_ATTENTION_FUNCTIONS = ALL_MASK_ATTENTION_FUNCTIONS._global_mapping\n+    try:\n+        yield\n+    finally:\n+        transformers.masking_utils.ALL_MASK_ATTENTION_FUNCTIONS = original\n+\n+\n def convert_and_export_with_cache(\n     model: PreTrainedModel,\n     example_input_ids: Optional[torch.Tensor] = None,\n@@ -493,6 +500,11 @@ def convert_and_export_with_cache(\n \n     import torch.export._trace\n \n+    # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n+    ALL_MASK_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_mask_without_vmap)\n+    ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n+    model.config._attn_implementation = \"sdpa_without_vmap\"\n+\n     with torch.no_grad():\n         # TODO: The default inputs only work for text models. We need to add support for vision/audio models.\n         example_input_ids = (\n@@ -503,13 +515,14 @@ def convert_and_export_with_cache(\n         )\n \n         if is_torch_greater_or_equal(\"2.6.0\"):\n-            exported_program = torch.export.export(\n-                TorchExportableModuleWithStaticCache(model),\n-                args=(example_input_ids, example_cache_position),\n-                kwargs={},\n-                dynamic_shapes=dynamic_shapes,\n-                strict=strict if strict is not None else True,\n-            )\n+            with patch_mask_interface():\n+                exported_program = torch.export.export(\n+                    TorchExportableModuleWithStaticCache(model),\n+                    args=(example_input_ids, example_cache_position),\n+                    kwargs={},\n+                    dynamic_shapes=dynamic_shapes,\n+                    strict=strict if strict is not None else True,\n+                )\n         else:\n             if dynamic_shapes is not None:\n                 logging.warning(\n@@ -521,13 +534,14 @@ def convert_and_export_with_cache(\n             #\n             # Due to issue https://github.com/pytorch/pytorch/issues/128394, we need to switch to use an internal\n             # export API and pre_dispatch=False. Switch to use the public API once the issue is included in 2.5 release.\n-            exported_program = torch.export._trace._export(\n-                TorchExportableModuleWithStaticCache(model),\n-                args=(example_input_ids,),\n-                kwargs={\"cache_position\": example_cache_position},\n-                pre_dispatch=False,\n-                strict=True,\n-            )\n+            with patch_mask_interface():\n+                exported_program = torch.export._trace._export(\n+                    TorchExportableModuleWithStaticCache(model),\n+                    args=(example_input_ids,),\n+                    kwargs={\"cache_position\": example_cache_position},\n+                    pre_dispatch=False,\n+                    strict=True,\n+                )\n         return exported_program\n \n \n@@ -620,9 +634,10 @@ def _export_encoder(self, encoder_input_ids):\n \n         # Export the encoder\n         with torch.no_grad():\n-            exported_encoder = torch.export.export(\n-                wrapped_encoder, (encoder_input_ids,), dynamic_shapes={\"input_ids\": {1: seq_len_dim}}, strict=True\n-            )\n+            with patch_mask_interface():\n+                exported_encoder = torch.export.export(\n+                    wrapped_encoder, (encoder_input_ids,), dynamic_shapes={\"input_ids\": {1: seq_len_dim}}, strict=True\n+                )\n \n         return exported_encoder\n \n@@ -642,16 +657,17 @@ def _export_decoder(self, decoder_input_ids, encoder_hidden_states, cache_positi\n \n         # Export the decoder\n         with torch.no_grad():\n-            exported_decoder = torch.export.export(\n-                wrapped_decoder,\n-                (decoder_input_ids, encoder_hidden_states, cache_position),\n-                dynamic_shapes={\n-                    \"decoder_input_ids\": None,\n-                    \"encoder_hidden_states\": {1: encoder_seq_len_dim},\n-                    \"cache_position\": None,\n-                },\n-                strict=True,\n-            )\n+            with patch_mask_interface():\n+                exported_decoder = torch.export.export(\n+                    wrapped_decoder,\n+                    (decoder_input_ids, encoder_hidden_states, cache_position),\n+                    dynamic_shapes={\n+                        \"decoder_input_ids\": None,\n+                        \"encoder_hidden_states\": {1: encoder_seq_len_dim},\n+                        \"cache_position\": None,\n+                    },\n+                    strict=True,\n+                )\n \n         return exported_decoder\n \n@@ -706,3 +722,131 @@ def generate(self, prompt_token_ids, max_new_tokens):\n                     break\n \n             return generated_ids\n+\n+\n+def export_with_dynamic_cache(\n+    model: PreTrainedModel,\n+    example_input_ids: Optional[torch.Tensor] = None,\n+    example_attention_mask: Optional[torch.Tensor] = None,\n+):\n+    \"\"\"\n+    Export a model with DynamicCache using `torch.export`, ensuring the exported model is compatible with `ExecuTorch`.\n+\n+    Args:\n+        model (`PreTrainedModel`): The pretrained model to be exported.\n+        example_input_ids (`Optional[torch.Tensor]`): Example input token id used by `torch.export`.\n+        example_attention_mask (`Optional[torch.Tensor]`): Example attention mask used by `torch.export`.\n+\n+    Returns:\n+        Exported program (`torch.export.ExportedProgram`): The exported program generated via `torch.export`.\n+    \"\"\"\n+    if not is_torch_greater_or_equal_than_2_3:\n+        raise ImportError(\"torch >= 2.3 is required.\")\n+\n+    # This is the same as sdpa, but mask creation does not use `vmap` which is not exportable\n+    ALL_MASK_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", sdpa_mask_without_vmap)\n+    ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n+    model.config._attn_implementation = \"sdpa_without_vmap\"\n+\n+    with torch.no_grad():\n+        exported_program = torch.export.export(\n+            model,\n+            (),\n+            {\n+                \"input_ids\": example_input_ids,\n+                \"attention_mask\": example_attention_mask,\n+                \"past_key_values\": DynamicCache(),\n+                \"use_cache\": True,\n+            },\n+            strict=False,\n+        )\n+        return exported_program\n+\n+\n+def sdpa_mask_without_vmap(\n+    batch_size: int,\n+    cache_position: torch.Tensor,\n+    kv_length: int,\n+    kv_offset: int = 0,\n+    mask_function: Optional[Callable] = None,\n+    attention_mask: Optional[torch.Tensor] = None,\n+    local_size: Optional[int] = None,\n+    allow_is_causal_skip: bool = True,\n+    allow_torch_fix: bool = True,\n+    **kwargs,\n+) -> Optional[torch.Tensor]:\n+    \"\"\"\n+    Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that\n+    the element should take part in the attention computation, and False that it should not.\n+\n+    This is similar to `masking_utils.sdpa_mask` but does not use `vmap` which is incompatible with export.\n+\n+    Args:\n+        batch_size (`int`):\n+            The batch size of the input sequence.\n+        cache_position (`torch.Tensor`):\n+            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n+        kv_length (`int`):\n+            The size that the key and value states will have during the attention computation.\n+        kv_offset (`int`, optional):\n+            An optional offset to indicate at which first position the key and values states will refer to.\n+        mask_function (`Callable`):\n+            The mask factory function describing the mask pattern.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n+        local_size (`int`, optional):\n+            The size of the local attention, if we do not use full attention. This is used only if `allow_is_causal_skip=True`\n+            to try to skip mask creation if possible.\n+        allow_is_causal_skip (`bool`, optional):\n+            Whether to allow to return `None` for the mask under conditions where we can use the `is_causal` argument in\n+            `torch.sdpa` instead. Default to `True`.\n+        allow_torch_fix (`bool`, optional):\n+            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n+            versions. We need an arg to skip it when using eager. By default `True`.\n+\n+    \"\"\"\n+\n+    q_length = cache_position.shape[0]\n+    # Potentially pad the 2D mask, and slice it correctly\n+    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset)\n+\n+    #  Under specific conditions, we can avoid materializing the mask, instead relying on the `is_causal` argument\n+    if allow_is_causal_skip and _ignore_causal_mask_sdpa(padding_mask, q_length, kv_length, local_size):\n+        return None\n+\n+    # Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`\n+    # but without data-dependent slicing (i.e. torch.compile friendly)\n+    kv_arange = torch.arange(kv_length, device=cache_position.device)\n+    kv_arange += kv_offset\n+    reshaped_cache_position = cache_position.view(-1, 1)\n+\n+    # This is a bit hacky to know what pattern we are using, but all mask creation function actually forward\n+    # the config through kwargs anyway, so it allows to rely on it\n+    # Usually, the `mask_function` is the only entry-point to define the pattern - we could do for loops over it,\n+    # but this is more efficient\n+    sliding_window = getattr(kwargs[\"config\"], \"sliding_window\", None)\n+    chunk_size = getattr(kwargs[\"config\"], \"attention_chunk_size\", None)\n+\n+    if sliding_window is not None and chunk_size is not None:\n+        raise ValueError(\"Cannot use both `sliding_window` and `attention_chunk_size`\")\n+\n+    # Simplest and most efficient way to obtain a causal mask\n+    causal_mask = kv_arange <= reshaped_cache_position\n+    # If using sliding window, add the sliding mask\n+    if sliding_window is not None:\n+        sliding_mask_overlay = kv_arange > reshaped_cache_position - sliding_window\n+        causal_mask *= sliding_mask_overlay\n+    # If using chunk attention, add the chunked mask\n+    elif chunk_size is not None:\n+        chunked_mask_overlay = kv_arange // chunk_size == reshaped_cache_position // chunk_size\n+        causal_mask *= chunked_mask_overlay\n+\n+    causal_mask = causal_mask[None, None, :, :].expand(batch_size, -1, -1, -1)\n+    if padding_mask is not None:\n+        causal_mask = causal_mask * padding_mask[:, None, None, :]\n+\n+    # Due to a bug in some older torch version, we need to update the mask in case a query is not attending to any\n+    # tokens (due to padding). See details in https://github.com/pytorch/pytorch/issues/110213\n+    if not _is_torch_greater_or_equal_than_2_5 and allow_torch_fix:\n+        causal_mask |= torch.all(~causal_mask, dim=-1, keepdim=True)\n+    return causal_mask"
        },
        {
            "sha": "afdaba5199dea0a31486e759170e1903fd2f2a42",
            "filename": "src/transformers/integrations/flex_attention.py",
            "status": "modified",
            "additions": 24,
            "deletions": 23,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fintegrations%2Fflex_attention.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fflex_attention.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -32,14 +32,12 @@\n from packaging import version\n \n from ..utils import is_torch_flex_attn_available\n-from ..utils.import_utils import _torch_version\n+from ..utils.import_utils import _torch_version, is_torchdynamo_compiling\n \n \n if is_torch_flex_attn_available():\n     from torch.nn.attention.flex_attention import BlockMask, flex_attention\n-    from torch.nn.attention.flex_attention import (\n-        create_block_mask as create_block_causal_mask_flex,\n-    )\n+    from torch.nn.attention.flex_attention import create_block_mask as create_block_causal_mask_flex\n \n \n class WrappedFlexAttention:\n@@ -79,6 +77,24 @@ def __call__(self):\n         return self._compiled_flex_attention\n \n \n+def compile_friendly_flex_attention(\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    training=False,\n+    **kwargs,\n+) -> torch.Tensor:\n+    # First call initialise singleton wrapper object, second call invokes the object method to return compiled flex attention\n+    # Do not use compiled version if already compiling forward (it raises issues)\n+    flex_attention_compiled = WrappedFlexAttention(training)() if not is_torchdynamo_compiling() else flex_attention\n+    return flex_attention_compiled(\n+        query,\n+        key,\n+        value,\n+        **kwargs,\n+    )\n+\n+\n Offset = Union[torch.Tensor, int]\n \n \n@@ -90,6 +106,10 @@ def make_flex_block_causal_mask(\n     offsets: Optional[Tuple[Offset, Offset]] = None,\n ) -> \"BlockMask\":\n     \"\"\"\n+    IMPORTANT NOTICE: This function is deprecated in favor of using the mask primitives in `masking_utils.py`,\n+    and will be removed in a future version without warnings. New code should not use it. It is only kept here\n+    for BC for now, while models using it are being patched accordingly.\n+\n     Create a block causal document mask for a batch of sequences, both packed and unpacked.\n     Create Block causal logic and passing it into :func:`torch.nn.attention.flex_attention.create_block_mask`.\n     The resultant BlockMask is a compressed representation of the full block causal\n@@ -133,7 +153,6 @@ def causal_mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n         \"\"\"\n         Defines the logic of a block causal mask by combining both a standard causal mask\n         and a block diagonal document mask.\n-\n         See :func:`~torchtune.modules.attention_utils.create_block_causal_mask`\n         for an illustration.\n         \"\"\"\n@@ -174,24 +193,6 @@ def mask_mod(batch_idx, head_idx, q_idx, kv_idx):\n     )\n \n \n-@torch.compiler.disable(recursive=False)\n-def compile_friendly_flex_attention(\n-    query: torch.Tensor,\n-    key: torch.Tensor,\n-    value: torch.Tensor,\n-    training=False,\n-    **kwargs,\n-) -> torch.Tensor:\n-    # First call initialise singleton wrapper object, second call invokes the object method to return compiled flex attention\n-    flex_attention_compiled = WrappedFlexAttention(training)()\n-    return flex_attention_compiled(\n-        query,\n-        key,\n-        value,\n-        **kwargs,\n-    )\n-\n-\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,"
        },
        {
            "sha": "8312891941bc5a8025fe2496e6351844d3f96f5f",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 10,
            "deletions": 48,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -16,15 +16,15 @@\n import operator\n import os\n import re\n-from collections.abc import MutableMapping\n from functools import partial, reduce\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import List, Optional, Tuple, Union\n \n import torch\n import torch.distributed as dist\n from torch import nn\n \n from ..utils import is_torch_greater_or_equal, logging\n+from ..utils.generic import GeneralInterface\n \n \n ALL_LAYERNORM_LAYERS = [nn.LayerNorm]\n@@ -720,20 +720,11 @@ def partition_tensor(self, param, empty_param, param_type, param_casting_dtype,\n         return nn.Parameter(parameter, requires_grad=parameter.is_floating_point())\n \n \n-class ParallelInterface(MutableMapping):\n-    \"\"\"\n-    Dict-like object keeping track of allowed attention functions. You can easily add a new attention function\n-    with a call to `register()`. If a model needs to locally overwrite an existing attention function, say `sdpa`,\n-    it needs to declare a new instance of this class inside the `modeling_<model>.py`, and declare it on that instance.\n-    \"\"\"\n-\n+class ParallelInterface(GeneralInterface):\n     # Class instance object, so that a call to `register` can be reflected into all other files correctly, even if\n-    # a new instance is created (in order to locally override a given function)\n-\n-    def __init__(self):\n-        self._local_mapping = {}\n-\n-        ParallelInterface._global_mapping = {\n+    # a new instance is created (in order to locally override a given entry)\n+    _global_mapping = (\n+        {\n             \"colwise\": ColwiseParallel(),\n             \"rowwise\": RowwiseParallel(),\n             \"colwise_rep\": ColwiseParallel(output_layouts=Replicate()),\n@@ -746,41 +737,12 @@ def __init__(self):\n             \"sequence_parallel\": SequenceParallel(),\n             \"replicate\": ReplicateParallel(),\n         }\n+        if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available\n+        else {}\n+    )\n \n-    def __getitem__(self, key):\n-        # First check if instance has a local override\n-        if key in self._local_mapping:\n-            return self._local_mapping[key]\n-        return self._global_mapping[key]\n-\n-    def __setitem__(self, key, value):\n-        # Allow local update of the default functions without impacting other instances\n-        self._local_mapping.update({key: value})\n-\n-    def __delitem__(self, key):\n-        del self._local_mapping[key]\n-\n-    def __iter__(self):\n-        # Ensure we use all keys, with the overwritten ones on top\n-        return iter({**self._global_mapping, **self._local_mapping})\n-\n-    def __len__(self):\n-        return len(self._global_mapping.keys() | self._local_mapping.keys())\n-\n-    @classmethod\n-    def register(cls, key: str, value: Callable):\n-        cls._global_mapping.update({key: value})\n-\n-    def valid_keys(self) -> List[str]:\n-        return list(self.keys())\n-\n-\n-# Global AttentionInterface shared by all models which do not need to overwrite any of the existing ones\n \n-if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n-    ALL_PARALLEL_STYLES: ParallelInterface = ParallelInterface()\n-else:\n-    ALL_PARALLEL_STYLES = None\n+ALL_PARALLEL_STYLES: ParallelInterface = ParallelInterface()\n \n \n def convert_local_tensor_to_dtensor("
        },
        {
            "sha": "36538882af570ad747a70654f3688c68344dc859",
            "filename": "src/transformers/masking_utils.py",
            "status": "added",
            "additions": 1129,
            "deletions": 0,
            "changes": 1129,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmasking_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmasking_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmasking_utils.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -0,0 +1,1129 @@\n+# coding=utf-8\n+# Copyright 2025 HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import itertools\n+from typing import Callable, Optional, Union\n+\n+import torch\n+import torch.nn.functional as F\n+\n+from .cache_utils import Cache\n+from .configuration_utils import PretrainedConfig\n+from .utils.generic import GeneralInterface\n+from .utils.import_utils import is_torch_flex_attn_available, is_torch_greater_or_equal, is_torchdynamo_compiling\n+\n+\n+if is_torch_flex_attn_available():\n+    from torch._dynamo._trace_wrapped_higher_order_op import TransformGetItemToIndex\n+    from torch.nn.attention.flex_attention import BlockMask, create_block_mask\n+\n+\n+_is_torch_greater_or_equal_than_2_5 = is_torch_greater_or_equal(\"2.5\", accept_dev=True)\n+\n+\n+def and_masks(*mask_functions: list[Callable]) -> Callable:\n+    \"\"\"Returns a mask function that is the intersection of provided mask functions\"\"\"\n+    if not all(callable(arg) for arg in mask_functions):\n+        raise RuntimeError(f\"All inputs should be callable mask_functions: {mask_functions}\")\n+\n+    def and_mask(batch_idx, head_idx, q_idx, kv_idx):\n+        result = q_idx.new_ones((), dtype=torch.bool)\n+        for mask in mask_functions:\n+            result = result & mask(batch_idx, head_idx, q_idx, kv_idx)\n+        return result\n+\n+    return and_mask\n+\n+\n+def or_masks(*mask_functions: list[Callable]) -> Callable:\n+    \"\"\"Returns a mask function that is the union of provided mask functions\"\"\"\n+    if not all(callable(arg) for arg in mask_functions):\n+        raise RuntimeError(f\"All inputs should be callable mask_functions: {mask_functions}\")\n+\n+    def or_mask(batch_idx, head_idx, q_idx, kv_idx):\n+        result = q_idx.new_zeros((), dtype=torch.bool)\n+        for mask in mask_functions:\n+            result = result | mask(batch_idx, head_idx, q_idx, kv_idx)\n+        return result\n+\n+    return or_mask\n+\n+\n+def causal_mask_function(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+    \"\"\"\n+    This creates a basic lower-diagonal causal mask.\n+    \"\"\"\n+    return kv_idx <= q_idx\n+\n+\n+def sliding_window_overlay(sliding_window: int) -> Callable:\n+    \"\"\"\n+    This is an overlay depicting a sliding window pattern. Add it on top of a causal mask for a proper sliding\n+    window mask.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        return kv_idx > q_idx - sliding_window\n+\n+    return inner_mask\n+\n+\n+def chunked_overlay(chunk_size: int) -> Callable:\n+    \"\"\"\n+    This is an overlay depicting a chuned attention pattern. Add it on top of a causal mask for a proper chunked\n+    attention mask.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        return kv_idx // chunk_size == q_idx // chunk_size\n+\n+    return inner_mask\n+\n+\n+def sliding_window_causal_mask_function(sliding_window: int) -> Callable:\n+    \"\"\"\n+    This return the mask_function function to create a sliding window mask.\n+    \"\"\"\n+    return and_masks(sliding_window_overlay(sliding_window), causal_mask_function)\n+\n+\n+def chunked_causal_mask_function(chunk_size: int) -> Callable:\n+    \"\"\"\n+    This return the mask_function function to create a chunked attention mask.\n+    \"\"\"\n+    return and_masks(chunked_overlay(chunk_size), causal_mask_function)\n+\n+\n+def padding_mask_function(padding_mask: torch.Tensor) -> Callable:\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        # Note that here the mask should ALWAYS be at least of the max `kv_index` size in the dimension 1. This is because\n+        # we cannot pad it here in the mask_function as we don't know the final size, and we cannot try/except, as it is not\n+        # vectorizable on accelerator devices\n+        return padding_mask[batch_idx, kv_idx]\n+\n+    return inner_mask\n+\n+\n+def add_offsets_to_mask_function(mask_function: Callable, q_offset: int, kv_offset: int) -> Callable:\n+    \"\"\"\n+    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n+    not start and end indices.\n+    \"\"\"\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        return mask_function(batch_idx, head_idx, q_idx + q_offset, kv_idx + kv_offset)\n+\n+    return inner_mask\n+\n+\n+def _vmap_for_bhqkv(mask_function: Callable, bh_indices: bool = True) -> Callable:\n+    \"\"\"\n+    Used to vmap our mask_functions over the q_idx and kv_idx dimensions of the inputs. Optionally, vmap over\n+    the batch and head indices as well if `bh_indices=True`.\n+    Using vmap here allows us to keep the performance of vectorized ops, while having a single set of primitive\n+    functions between attention interfaces (i.e. between flex and sdpa/eager, FA2 being a bit different).\n+\n+    Args:\n+        mask_function (`Callable`):\n+            The mask_function to vmap.\n+        bh_indices (`bool`, optional):\n+            Whether to vmap over the batch and head indices as well, or only q and kv indices.\n+\n+    Returns:\n+        Callable: The vmapped function.\n+    \"\"\"\n+    # We vmap the function 2 times, broadcasting the [q_idx, kv_idx] dimensions\n+    dimensions = [(None, None, None, 0), (None, None, 0, None)]\n+    if bh_indices:\n+        # We extend broadcasting over the [batch_idx, head_idx] dimensions\n+        dimensions.extend([(None, 0, None, None), (0, None, None, None)])\n+\n+    for dims in dimensions:\n+        mask_function = torch.vmap(mask_function, in_dims=dims, out_dims=0)\n+    return mask_function\n+\n+\n+def prepare_padding_mask(\n+    attention_mask: Optional[torch.Tensor], kv_length: int, kv_offset: int, _slice: bool = True\n+) -> Optional[torch.Tensor]:\n+    \"\"\"\n+    From the 2D attention mask, prepare the correct padding mask to use by potentially padding it, and slicing\n+    according to the `kv_offset` if `_slice` is `True`.\n+    \"\"\"\n+    local_padding_mask = attention_mask\n+    if attention_mask is not None:\n+        # Pad it if necesary\n+        if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n+            local_padding_mask = torch.nn.functional.pad(attention_mask, (0, padding_length))\n+        # For flex, we should not slice them, only use an offset\n+        if _slice:\n+            # Equivalent to: `local_padding_mask = attention_mask[:, kv_offset : kv_offset + kv_length]`,\n+            # but without data-dependent slicing (i.e. torch.compile friendly)\n+            mask_indices = torch.arange(kv_length, device=local_padding_mask.device)\n+            mask_indices += kv_offset\n+            local_padding_mask = local_padding_mask[:, mask_indices]\n+    return local_padding_mask\n+\n+\n+def _ignore_causal_mask_sdpa(\n+    padding_mask: Optional[torch.Tensor],\n+    query_length: int,\n+    kv_length: int,\n+    kv_offset: int,\n+    local_attention_size: Optional[int] = None,\n+) -> bool:\n+    \"\"\"\n+    Detects whether the causal mask can be ignored in case PyTorch's SDPA is used, rather relying on SDPA's `is_causal` argument.\n+\n+    In case no token is masked in the 2D `padding_mask` argument, if `query_length == 1` or\n+    `key_value_length == query_length`, we rather rely on SDPA `is_causal` argument to use causal/non-causal masks,\n+    allowing to dispatch to the flash attention kernel (that can otherwise not be used if a custom `attn_mask` is\n+    passed).\n+    \"\"\"\n+    is_tracing = torch.jit.is_tracing() or isinstance(padding_mask, torch.fx.Proxy) or is_torchdynamo_compiling()\n+    if padding_mask is not None and padding_mask.shape[-1] > kv_length:\n+        mask_indices = torch.arange(kv_length, device=padding_mask.device)\n+        mask_indices += kv_offset\n+        padding_mask = padding_mask[:, mask_indices]\n+\n+    # When using `torch.export` or `torch.onnx.dynamo_export`, we must pass an example input, and `is_causal` behavior is\n+    # hard-coded to the forward. If a user exports a model with query_length > 1, the exported model will hard-code `is_causal=True`\n+    # which is in general wrong (see https://github.com/pytorch/pytorch/issues/108108). Thus, we only set\n+    # `ignore_causal_mask = True` if we are not tracing\n+    if (\n+        not is_tracing\n+        # only cases when lower and upper diags are the same, see https://github.com/pytorch/pytorch/issues/108108\n+        and (query_length == 1 or kv_length == query_length)\n+        # in this case we need to add special patterns to the mask so cannot be skipped otherwise\n+        and (local_attention_size is None or kv_length < local_attention_size)\n+        # In this case, we need to add padding to the mask, so cannot be skipped otherwise\n+        and (padding_mask is None or padding_mask.all())\n+    ):\n+        return True\n+\n+    return False\n+\n+\n+def sdpa_mask_recent_torch(\n+    batch_size: int,\n+    cache_position: torch.Tensor,\n+    kv_length: int,\n+    kv_offset: int = 0,\n+    mask_function: Callable = causal_mask_function,\n+    attention_mask: Optional[torch.Tensor] = None,\n+    local_size: Optional[int] = None,\n+    allow_is_causal_skip: bool = True,\n+    **kwargs,\n+) -> Optional[torch.Tensor]:\n+    \"\"\"\n+    Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that\n+    the element should take part in the attention computation, and False that it should not.\n+    This function can only be used with torch>=2.5, as the context manager is otherwise not available.\n+\n+    Args:\n+        batch_size (`int`):\n+            The batch size of the input sequence.\n+        cache_position (`torch.Tensor`):\n+            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n+        kv_length (`int`):\n+            The size that the key and value states will have during the attention computation.\n+        kv_offset (`int`, optional):\n+            An optional offset to indicate at which first position the key and values states will refer to.\n+        mask_function (`Callable`):\n+            The mask factory function describing the mask pattern.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n+        local_size (`int`, optional):\n+            The size of the local attention, if we do not use full attention. This is used only if `allow_is_causal_skip=True`\n+            to try to skip mask creation if possible.\n+        allow_is_causal_skip (`bool`, optional):\n+            Whether to allow to return `None` for the mask under conditions where we can use the `is_causal` argument in\n+            `torch.sdpa` instead. Default to `True`.\n+        allow_torch_fix (`bool`, optional):\n+            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n+            versions. We need an arg to skip it when using eager. By default `True`.\n+\n+\n+    ## Creating a simple causal mask:\n+\n+    To create the following causal mask:\n+\n+        0 â–  â¬š â¬š â¬š â¬š\n+        1 â–  â–  â¬š â¬š â¬š\n+        2 â–  â–  â–  â¬š â¬š\n+        3 â–  â–  â–  â–  â¬š\n+        4 â–  â–  â–  â–  â– \n+\n+    You can do\n+\n+    ```python\n+    >>> create_4d_causal_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5)\n+    >>> tensor([[[[ True, False, False, False, False],\n+                  [ True,  True, False, False, False],\n+                  [ True,  True,  True, False, False],\n+                  [ True,  True,  True,  True, False],\n+                  [ True,  True,  True,  True,  True]]]])\n+    ```\n+\n+    ## Creating a sliding window mask:\n+\n+    To create the following sliding window mask (`sliding_window=3`):\n+\n+        0 â–  â¬š â¬š â¬š â¬š\n+        1 â–  â–  â¬š â¬š â¬š\n+        2 â–  â–  â–  â¬š â¬š\n+        3 â¬š â–  â–  â–  â¬š\n+        4 â¬š â¬š â–  â–  â– \n+\n+    You can do\n+\n+    ```python\n+    >>> create_4d_causal_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5, mask_function=sliding_window_causal_mask_function(3))\n+    >>> tensor([[[[ True, False, False, False, False],\n+                  [ True,  True, False, False, False],\n+                  [ True,  True,  True, False, False],\n+                  [False,  True,  True,  True, False],\n+                  [False, False,  True,  True,  True]]]])\n+    ```\n+\n+    ## Creating a chunked attention mask\n+\n+    To create the following chunked attention mask (`chunk_size=3`):\n+\n+        0 â–  â¬š â¬š â¬š â¬š\n+        1 â–  â–  â¬š â¬š â¬š\n+        2 â–  â–  â–  â¬š â¬š\n+        3 â¬š â¬š â¬š â–  â¬š\n+        4 â¬š â¬š â¬š â–  â– \n+\n+    You can do\n+\n+    ```python\n+    >>> create_4d_causal_mask(batch_size=1, cache_position=torch.arange(5), kv_length=5, mask_function=chunked_causal_mask_function(3))\n+    >>> tensor([[[[ True, False, False, False, False],\n+                [ True,  True, False, False, False],\n+                [ True,  True,  True, False, False],\n+                [False, False, False,  True, False],\n+                [False, False, False,  True,  True]]]])\n+    ```\n+\n+    \"\"\"\n+    q_length = cache_position.shape[0]\n+    # Potentially pad the 2D mask, and slice it correctly\n+    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n+\n+    # Under specific conditions, we can avoid materializing the mask, instead relying on the `is_causal` argument\n+    if allow_is_causal_skip and _ignore_causal_mask_sdpa(padding_mask, q_length, kv_length, kv_offset, local_size):\n+        return None\n+\n+    # Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`\n+    # but without data-dependent slicing (i.e. torch.compile friendly)\n+    kv_arange = torch.arange(kv_length, device=cache_position.device)\n+    kv_arange += kv_offset\n+\n+    # Potentially add the padding 2D mask\n+    if padding_mask is not None:\n+        mask_function = and_masks(mask_function, padding_mask_function(padding_mask))\n+\n+    batch_arange = torch.arange(batch_size, device=cache_position.device)\n+    head_arange = torch.arange(1, device=cache_position.device)\n+    # This creates the 4D mask easily. Note that we need this context manager as vmap cannot handle slicing a tensor from\n+    # scalar tensor (it internally calls `.item()` which vmap does not allow, but this context works around it\n+    # We don't need to add an offset to the mask_function either, as we vmap directly the correct indices for k and kv indices\n+    with TransformGetItemToIndex():\n+        causal_mask = _vmap_for_bhqkv(mask_function)(batch_arange, head_arange, cache_position, kv_arange)\n+\n+    return causal_mask\n+\n+\n+def sdpa_mask_older_torch(\n+    batch_size: int,\n+    cache_position: torch.Tensor,\n+    kv_length: int,\n+    kv_offset: int = 0,\n+    mask_function: Callable = causal_mask_function,\n+    attention_mask: Optional[torch.Tensor] = None,\n+    local_size: Optional[int] = None,\n+    allow_is_causal_skip: bool = True,\n+    allow_torch_fix: bool = True,\n+    **kwargs,\n+) -> Optional[torch.Tensor]:\n+    \"\"\"\n+    NOTE: This function is only used when torch version is torch<2.5 - see `sdpa_mask_recent_torch` otherwise.\n+\n+    Create a 4D boolean mask of shape `(batch_size, 1, query_length, kv_length)` where a value of True indicates that\n+    the element should take part in the attention computation, and False that it should not.\n+    If `allow_torch_fix=True` (the default), rows corresponding to query tokens that do not attend\n+    to any other tokens (due to padding) will be fully attended to instead, in order to avoid `nan` propagation (this does\n+    not change the final result).\n+\n+    Args:\n+        batch_size (`int`):\n+            The batch size of the input sequence.\n+        cache_position (`torch.Tensor`):\n+            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n+        kv_length (`int`):\n+            The size that the key and value states will have during the attention computation.\n+        kv_offset (`int`, optional):\n+            An optional offset to indicate at which first position the key and values states will refer to.\n+        mask_function (`Callable`):\n+            The mask factory function describing the mask pattern.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n+        local_size (`int`, optional):\n+            The size of the local attention, if we do not use full attention. This is used only if `allow_is_causal_skip=True`\n+            to try to skip mask creation if possible.\n+        allow_is_causal_skip (`bool`, optional):\n+            Whether to allow to return `None` for the mask under conditions where we can use the `is_causal` argument in\n+            `torch.sdpa` instead. Default to `True`.\n+        allow_torch_fix (`bool`, optional):\n+            Whether to update the mask in case a query is not attending to any tokens, to solve a bug in torch's older\n+            versions. We need an arg to skip it when using eager. By default `True`.\n+    \"\"\"\n+    q_length = cache_position.shape[0]\n+    # Potentially pad the 2D mask, and slice it correctly\n+    padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset)\n+\n+    # Under specific conditions, we can avoid materializing the mask, instead relying on the `is_causal` argument\n+    if allow_is_causal_skip and _ignore_causal_mask_sdpa(padding_mask, q_length, kv_length, kv_offset, local_size):\n+        return None\n+\n+    # Similar to `kv_arange = torch.arange(start=kv_offset, end=kv_offset + kv_length, device=cache_position.device)`\n+    # but without data-dependent slicing (i.e. torch.compile friendly)\n+    kv_arange = torch.arange(kv_length, device=cache_position.device)\n+    kv_arange += kv_offset\n+\n+    # This creates the 4D mask easily. Note that we do not include vmap over the batch_idx dimension as well,\n+    # as vmap cannot handle slicing a tensor from scalar tensor (it internally calls `.item()` which vmap does not allow\n+    # However, in more recent version of Pytorch, a trick was introduced to handle it - which is the reason we have\n+    # `sdpa_mask_recent_torch`, as it allows more general `mask_function`\n+    causal_mask = _vmap_for_bhqkv(mask_function, bh_indices=False)(None, None, cache_position, kv_arange)\n+    causal_mask = causal_mask[None, None, :, :].expand(batch_size, -1, -1, -1)\n+    if padding_mask is not None:\n+        causal_mask = causal_mask * padding_mask[:, None, None, :]\n+\n+    # Due to a bug in versions of torch<2.5, we need to update the mask in case a query is not attending to any\n+    # tokens (due to padding). See details in https://github.com/pytorch/pytorch/issues/110213\n+    if allow_torch_fix:\n+        causal_mask |= torch.all(~causal_mask, dim=-1, keepdim=True)\n+    return causal_mask\n+\n+\n+# We use the version with newer torch whenever possible, as it is more general and can handle arbitrary mask functions\n+# (especially mask_function indexing a tensor, such as the padding mask function)\n+sdpa_mask = sdpa_mask_recent_torch if is_torch_flex_attn_available() else sdpa_mask_older_torch\n+\n+\n+def eager_mask(\n+    batch_size: int,\n+    cache_position: torch.Tensor,\n+    kv_length: int,\n+    kv_offset: int = 0,\n+    mask_function: Callable = causal_mask_function,\n+    attention_mask: Optional[torch.Tensor] = None,\n+    dtype: torch.dtype = torch.float32,\n+    **kwargs,\n+) -> torch.Tensor:\n+    \"\"\"\n+    Create a 4D float mask of shape `(batch_size, 1, query_length, kv_length)` where a value of 0 indicates that\n+    the element should take part in the attention computation, and -inf (minimum value for the given `dtype`) that\n+    it should not.\n+\n+    Args:\n+        batch_size (`int`):\n+            The batch size of the input sequence.\n+        cache_position (`torch.Tensor`):\n+            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n+        kv_length (`int`):\n+            The size that the key and value states will have during the attention computation.\n+        kv_offset (`int`, optional):\n+            An optional offset to indicate at which first position the key and values states will refer to.\n+        mask_function (`Callable`):\n+            The mask factory function describing the mask pattern.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n+        dtype (`torch.dtype`, optional):\n+            The dtype to use for the mask. By default, `torch.float32`.\n+    \"\"\"\n+    # The masks for eager attention are simply boolean mask from sdpa, casted to 0 and -inf\n+    _ = kwargs.pop(\"allow_is_causal_skip\", None)\n+    mask = sdpa_mask(\n+        batch_size=batch_size,\n+        cache_position=cache_position,\n+        kv_length=kv_length,\n+        kv_offset=kv_offset,\n+        mask_function=mask_function,\n+        attention_mask=attention_mask,\n+        allow_is_causal_skip=False,\n+        allow_torch_fix=False,\n+        **kwargs,\n+    )\n+    min_dtype = torch.finfo(dtype).min\n+    # we need 0s where the tokens should be taken into account, and -inf otherwise (mask is already of boolean type)\n+    mask = torch.where(mask, torch.tensor(0.0, device=mask.device, dtype=dtype), min_dtype)\n+    return mask\n+\n+\n+def flash_attention_mask(\n+    batch_size: int,\n+    cache_position: torch.Tensor,\n+    kv_length: int,\n+    kv_offset: int = 0,\n+    mask_function: Callable = causal_mask_function,\n+    attention_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+):\n+    \"\"\"\n+    Create the attention mask necesary to use FA2. Since FA2 is un-padded by definition, here we simply return\n+    `None` if the mask is fully causal, or we return the 2D mask which will then be used to extract the seq_lens.\n+    We just slice it in case of sliding window.\n+\n+    Args:\n+        batch_size (`int`):\n+            The batch size of the input sequence.\n+        cache_position (`torch.Tensor`):\n+            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n+        kv_length (`int`):\n+            The size that the key and value states will have during the attention computation.\n+        kv_offset (`int`, optional):\n+            An optional offset to indicate at which first position the key and values states will refer to.\n+        mask_function (`Callable`):\n+            The mask factory function describing the mask pattern.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n+    \"\"\"\n+    if attention_mask is not None:\n+        # Here we need to slice from the right if using sliding or chunked (for full attention, this is equivalent to doing nothing)\n+        attention_mask = attention_mask[:, -kv_length:]\n+        # We only return an actual mask if there is at least 1 padding token, otherwise we return `None` and use `is_causal` in FA2\n+        # (note that the attention_mask is a boolean dtype here)\n+        if attention_mask.all():\n+            attention_mask = None\n+\n+    return attention_mask\n+\n+\n+def flex_attention_mask(\n+    batch_size: int,\n+    cache_position: torch.Tensor,\n+    kv_length: int,\n+    kv_offset: int = 0,\n+    mask_function: Callable = causal_mask_function,\n+    attention_mask: Optional[torch.Tensor] = None,\n+    **kwargs,\n+) -> \"BlockMask\":\n+    \"\"\"\n+    Create a 4D block mask which is a compressed representation of the full 4D block causal mask. BlockMask is essential\n+    for performant computation of flex attention. See: https://pytorch.org/blog/flexattention/\n+\n+    Args:\n+        batch_size (`int`):\n+            The batch size of the input sequence.\n+        cache_position (`torch.Tensor`):\n+            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n+        kv_length (`int`):\n+            The size that the key and value states will have during the attention computation.\n+        kv_offset (`int`, optional):\n+            An optional offset to indicate at which first position the key and values states will refer to.\n+        mask_function (`Callable`):\n+            The mask factory function describing the mask pattern.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length)\n+    \"\"\"\n+    q_length, q_offset = cache_position.shape[0], cache_position[0]\n+\n+    # Potentially add the padding 2D mask\n+    if attention_mask is not None:\n+        padding_mask = prepare_padding_mask(attention_mask, kv_length, kv_offset, _slice=False)\n+        mask_function = and_masks(mask_function, padding_mask_function(padding_mask))\n+\n+    # Add the offsets on top (because flex interface only allows length, not start and end indices)\n+    mask_function = add_offsets_to_mask_function(mask_function, q_offset, kv_offset)\n+\n+    # Finally create the block mask\n+    block_mask = create_block_mask(\n+        mask_mod=mask_function,\n+        B=batch_size,\n+        H=None,\n+        Q_LEN=q_length,\n+        KV_LEN=kv_length,\n+        device=cache_position.device,\n+        _compile=True,\n+    )\n+    return block_mask\n+\n+\n+class AttentionMaskInterface(GeneralInterface):\n+    # Class instance object, so that a call to `register` can be reflected into all other files correctly, even if\n+    # a new instance is created (in order to locally override a given function)\n+    _global_mapping = {\n+        \"sdpa\": sdpa_mask,\n+        \"eager\": eager_mask,\n+        \"flash_attention_2\": flash_attention_mask,\n+        \"flex_attention\": flex_attention_mask,\n+    }\n+\n+\n+# Global AttentionMaskInterface shared by all models which do not need to overwrite any of the existing ones\n+ALL_MASK_ATTENTION_FUNCTIONS: AttentionMaskInterface = AttentionMaskInterface()\n+\n+\n+def _preprocess_mask_arguments(\n+    config: PretrainedConfig,\n+    input_embeds: torch.Tensor,\n+    attention_mask: Optional[Union[torch.Tensor, BlockMask]],\n+    cache_position: torch.Tensor,\n+    past_key_values: Optional[Cache],\n+    layer_idx: Optional[int],\n+) -> tuple[bool, Optional[Union[torch.Tensor, BlockMask]], int, int]:\n+    \"\"\"\n+    Perform some common pre-processing of the mask arguments we get from the modeling code. Mostly determine the\n+    key-value length and offsets, and if we should early exit or not.\n+\n+    Args:\n+        config (`PretrainedConfig`):\n+            The model config.\n+        input_embeds (`torch.Tensor`):\n+            The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n+            batch size, query length and dtype.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length).\n+            It can also be an already prepared 4D mask, in which case it is returned as-is.\n+        cache_position (`torch.Tensor`):\n+            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n+        past_key_values (`Cache`, optional):\n+            The past key values, if we use a cache.\n+        layer_idx (`int`, optional):\n+            If `past_key_values` is not None, this is the layer index of the cache from which to get the key-value\n+            length and offset. Indeed, for hybrid caches, different layers may return different lengths.\n+\n+    Returns:\n+        early_exit (`bool`):\n+            Whether we should early exit mask creation, and return the mask as-is.\n+        attention_mask (`torch.Tensor` or `BlockMask` or `None`):\n+            The attention mask to either return immediately, or to use in downstream mask creation.\n+        kv_length (`int`):\n+            The size that the key and value states will have during the attention computation.\n+        kv_offset (`int`):\n+            An offset to indicate at which first position the key and values states will refer to.\n+    \"\"\"\n+    # If the mask is already 4D, simply return as-is (it was already prepared, or it is custom)\n+    if isinstance(attention_mask, (torch.Tensor, BlockMask)) and len(attention_mask.shape) == 4:\n+        return True, attention_mask, None, None\n+\n+    # For TGI/vLLM backends, or other custom attention without equivalent mask creation: we don't need a mask!\n+    if config._attn_implementation not in ALL_MASK_ATTENTION_FUNCTIONS:\n+        return True, None, None, None\n+\n+    # Move the mask to correct device, and potentially switch dtype for efficiency\n+    if attention_mask is not None and attention_mask.ndim == 2:\n+        attention_mask = attention_mask.to(device=cache_position.device, dtype=torch.bool)\n+\n+    # If using a cache, it can give all informations about mask sizes based on seen tokens\n+    if past_key_values is not None:\n+        kv_length, kv_offset = past_key_values.get_mask_sizes(cache_position, layer_idx)\n+    # Otherwise, the sizes are simply the input sizes\n+    else:\n+        kv_length, kv_offset = input_embeds.shape[1], 0\n+\n+    return False, attention_mask, kv_length, kv_offset\n+\n+\n+def _get_mask_interface(config: PretrainedConfig, output_attentions: bool = False) -> Callable:\n+    \"\"\"\n+    Return the mask interface (a function) to be used, based on the type of attention found in the config.\n+\n+    Args:\n+        config (`PretrainedConfig`):\n+            The model config.\n+        output_attentions (`bool`, optional):\n+            Whether we return the attention scores or not. By default `False`.\n+    \"\"\"\n+    mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[config._attn_implementation]\n+    # Sdpa fallbacks to eager in the Attention modules if `output_attentions=True`\n+    if config._attn_implementation == \"sdpa\" and output_attentions:\n+        mask_interface = ALL_MASK_ATTENTION_FUNCTIONS[\"eager\"]\n+    return mask_interface\n+\n+\n+def create_causal_mask(\n+    config: PretrainedConfig,\n+    input_embeds: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    cache_position: torch.Tensor,\n+    past_key_values: Optional[Cache],\n+    output_attentions: bool = False,\n+    or_mask_function: Optional[Callable] = None,\n+    and_mask_function: Optional[Callable] = None,\n+) -> Optional[Union[torch.Tensor, \"BlockMask\"]]:\n+    \"\"\"\n+    Create a standard causal mask based on the attention implementation used (stored in the config). If `past_key_values`\n+    has an HybridCache structure, this function will return the mask corresponding to one of the \"full_attention\" layers (to align\n+    to what is needed in the `modeling_xxx.py` files).\n+\n+    Args:\n+        config (`PretrainedConfig`):\n+            The model config.\n+        input_embeds (`torch.Tensor`):\n+            The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n+            batch size, query length and dtype.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length).\n+            It can also be an already prepared 4D mask, in which case it is returned as-is.\n+        cache_position (`torch.Tensor`):\n+            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n+        past_key_values (`Cache`, optional):\n+            The past key values, if we use a cache.\n+        output_attentions (`bool`, optional):\n+            Whether we return the attention scores or not. By default `False`.\n+        or_mask_function (`Callable`, optional):\n+            An optional mask function to combine with the causal mask function (by doing the union of both). This is\n+            useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n+        and_mask_function (`Callable`, optional):\n+            An optional mask function to combine with the causal mask function (by doing the intersection of both). This is\n+            useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n+    \"\"\"\n+    # If we have an HybridCache structure, here we want to create the mask for the full layers\n+    try:\n+        layer_idx = past_key_values.is_sliding.index(False)\n+    except (ValueError, AttributeError):\n+        layer_idx = 0\n+\n+    early_exit, attention_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n+        config, input_embeds, attention_mask, cache_position, past_key_values, layer_idx\n+    )\n+    if early_exit:\n+        return attention_mask\n+\n+    batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n+    mask_factory_function = causal_mask_function\n+    mask_interface = _get_mask_interface(config, output_attentions)\n+\n+    # Do not allow skip if we are compiling (this is to match BC)\n+    # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n+    allow_is_causal_skip = not past_key_values.is_compileable if past_key_values is not None else True\n+\n+    # Allow slight deviations from causal mask\n+    if or_mask_function is not None:\n+        if not _is_torch_greater_or_equal_than_2_5:\n+            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n+        mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n+        allow_is_causal_skip = False\n+    if and_mask_function is not None:\n+        if not _is_torch_greater_or_equal_than_2_5:\n+            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n+        mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n+        allow_is_causal_skip = False\n+\n+    # We now create the mask\n+    causal_mask = mask_interface(\n+        batch_size=batch_size,\n+        cache_position=cache_position,\n+        kv_length=kv_length,\n+        kv_offset=kv_offset,\n+        mask_function=mask_factory_function,\n+        attention_mask=attention_mask,\n+        allow_is_causal_skip=allow_is_causal_skip,  # additional kwarg for sdpa\n+        dtype=dtype,  # Additional kwarg for eager\n+        config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n+    )\n+    return causal_mask\n+\n+\n+def create_sliding_window_causal_mask(\n+    config: PretrainedConfig,\n+    input_embeds: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    cache_position: torch.Tensor,\n+    past_key_values: Optional[Cache],\n+    output_attentions: bool = False,\n+    or_mask_function: Optional[Callable] = None,\n+    and_mask_function: Optional[Callable] = None,\n+) -> Optional[Union[torch.Tensor, \"BlockMask\"]]:\n+    \"\"\"\n+    Create a sliding window causal mask based on the attention implementation used (stored in the config). This type\n+    of attention pattern was mostly democratized by Mistral. If `past_key_values` has an HybridCache structure, this\n+    function will return the mask corresponding to one of the \"sliding_attention\" layers (to align to what is needed in the\n+    `modeling_xxx.py` files).\n+\n+    Args:\n+        config (`PretrainedConfig`):\n+            The model config.\n+        input_embeds (`torch.Tensor`):\n+            The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n+            batch size, query length and dtype.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length).\n+            It can also be an already prepared 4D mask, in which case it is returned as-is.\n+        cache_position (`torch.Tensor`):\n+            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n+        past_key_values (`Cache`, optional):\n+            The past key values, if we use a cache.\n+        output_attentions (`bool`, optional):\n+            Whether we return the attention scores or not. By default `False`.\n+        or_mask_function (`Callable`, optional):\n+            An optional mask function to combine with the sliding causal mask function (by doing the union of both). This is\n+            useful to easily overlay another mask on top of the sliding causal one, for example for image tokens handling.\n+        and_mask_function (`Callable`, optional):\n+            An optional mask function to combine with the sliding causal mask function (by doing the intersection of both). This is\n+            useful to easily overlay another mask on top of the sliding causal one, for example for image tokens handling.\n+    \"\"\"\n+    # If we have an HybridCache structure, here we want to create the mask for the sliding layers\n+    try:\n+        layer_idx = past_key_values.is_sliding.index(True)\n+    except (ValueError, AttributeError):\n+        layer_idx = 0\n+\n+    early_exit, attention_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n+        config, input_embeds, attention_mask, cache_position, past_key_values, layer_idx\n+    )\n+    if early_exit:\n+        return attention_mask\n+\n+    sliding_window = getattr(config, \"sliding_window\", None)\n+    if sliding_window is None:\n+        raise ValueError(\"Could not find a `sliding_window` argument in the config, or it is not set\")\n+\n+    batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n+    mask_factory_function = sliding_window_causal_mask_function(sliding_window)\n+    mask_interface = _get_mask_interface(config, output_attentions)\n+\n+    # Do not allow skip if we are compiling (this is to match BC)\n+    # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n+    allow_is_causal_skip = not past_key_values.is_compileable if past_key_values is not None else True\n+\n+    # Allow slight deviations from sliding causal mask\n+    if or_mask_function is not None:\n+        if not _is_torch_greater_or_equal_than_2_5:\n+            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n+        mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n+        allow_is_causal_skip = False\n+    if and_mask_function is not None:\n+        if not _is_torch_greater_or_equal_than_2_5:\n+            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n+        mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n+        allow_is_causal_skip = False\n+\n+    # We now create the mask\n+    causal_mask = mask_interface(\n+        batch_size=batch_size,\n+        cache_position=cache_position,\n+        kv_length=kv_length,\n+        kv_offset=kv_offset,\n+        mask_function=mask_factory_function,\n+        attention_mask=attention_mask,\n+        allow_is_causal_skip=allow_is_causal_skip,  # additional kwarg for sdpa\n+        local_size=sliding_window,  # Additional kwarg for sdpa\n+        dtype=dtype,  # Additional kwarg for eager\n+        config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n+    )\n+    return causal_mask\n+\n+\n+def create_chunked_causal_mask(\n+    config: PretrainedConfig,\n+    input_embeds: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    cache_position: torch.Tensor,\n+    past_key_values: Optional[Cache],\n+    output_attentions: bool = False,\n+    or_mask_function: Optional[Callable] = None,\n+    and_mask_function: Optional[Callable] = None,\n+) -> Optional[Union[torch.Tensor, \"BlockMask\"]]:\n+    \"\"\"\n+    Create a chunked attention causal mask based on the attention implementation used (stored in the config). This type\n+    of attention pattern was mostly democratized by Llama4. If `past_key_values` has an HybridCache structure, this\n+    function will return the mask corresponding to one of the \"chunked_attention\" layers (to align to what is needed in the\n+    `modeling_xxx.py` files).\n+\n+    Args:\n+        config (`PretrainedConfig`):\n+            The model config.\n+        input_embeds (`torch.Tensor`):\n+            The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n+            batch size, query length and dtype.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length).\n+            It can also be an already prepared 4D mask, in which case it is returned as-is.\n+        cache_position (`torch.Tensor`):\n+            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n+        past_key_values (`Cache`, optional):\n+            The past key values, if we use a cache.\n+        output_attentions (`bool`, optional):\n+            Whether we return the attention scores or not. By default `False`.\n+        or_mask_function (`Callable`, optional):\n+            An optional mask function to combine with the chunked causal mask function (by doing the union of both). This is\n+            useful to easily overlay another mask on top of the chunked causal one, for example for image tokens handling.\n+        and_mask_function (`Callable`, optional):\n+            An optional mask function to combine with the chunked causal mask function (by doing the intersection of both). This is\n+            useful to easily overlay another mask on top of the chunked causal one, for example for image tokens handling.\n+    \"\"\"\n+    # If we have an HybridCache structure, here we want to create the mask for the sliding layers\n+    try:\n+        layer_idx = past_key_values.is_sliding.index(True)\n+    except (ValueError, AttributeError):\n+        layer_idx = 0\n+\n+    early_exit, attention_mask, kv_length, kv_offset = _preprocess_mask_arguments(\n+        config, input_embeds, attention_mask, cache_position, past_key_values, layer_idx\n+    )\n+    if early_exit:\n+        return attention_mask\n+\n+    chunk_size = getattr(config, \"attention_chunk_size\", None)\n+    if chunk_size is None:\n+        raise ValueError(\"Could not find an `attention_chunk_size` argument in the config, or it is not set\")\n+\n+    # Raise if using chunked attention on context too large with FA2\n+    if config._attn_implementation == \"flash_attention_2\" and kv_length + kv_offset > chunk_size:\n+        raise ValueError(\n+            \"Flash attention 2 cannot handle chunked attention, and the key-value length is larger than the chunk size so the \"\n+            \"chunked pattern cannot be respected. You should use another `attn_implementation` when instantiating the model\"\n+        )\n+\n+    batch_size, dtype = input_embeds.shape[0], input_embeds.dtype\n+    mask_factory_function = chunked_causal_mask_function(chunk_size)\n+    mask_interface = _get_mask_interface(config, output_attentions)\n+\n+    # Do not allow skip if we are compiling (this is to match BC)\n+    # TODO: cyril -> probably revisit and remove this, but a lot of tests rely on it\n+    allow_is_causal_skip = not past_key_values.is_compileable if past_key_values is not None else True\n+\n+    # Allow slight deviations from chunked causal mask\n+    if or_mask_function is not None:\n+        if not _is_torch_greater_or_equal_than_2_5:\n+            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n+        mask_factory_function = or_masks(mask_factory_function, or_mask_function)\n+        allow_is_causal_skip = False\n+    if and_mask_function is not None:\n+        if not _is_torch_greater_or_equal_than_2_5:\n+            raise ValueError(\"Using `or_mask_function` or `and_mask_function` arguments require torch>=2.5\")\n+        mask_factory_function = and_masks(mask_factory_function, and_mask_function)\n+        allow_is_causal_skip = False\n+\n+    # We now create the mask\n+    causal_mask = mask_interface(\n+        batch_size=batch_size,\n+        cache_position=cache_position,\n+        kv_length=kv_length,\n+        kv_offset=kv_offset,\n+        mask_function=mask_factory_function,\n+        attention_mask=attention_mask,\n+        allow_is_causal_skip=allow_is_causal_skip,  # additional kwarg for sdpa\n+        local_size=chunk_size,  # Additional kwarg for sdpa\n+        dtype=dtype,  # Additional kwarg for eager\n+        config=config,  # Pass the config as well, in case someone wants to easily have their own mask_interface\n+    )\n+    return causal_mask\n+\n+\n+LAYER_PATTERN_TO_MASK_FUNCTION_MAPPING = {\n+    \"full_attention\": create_causal_mask,\n+    \"sliding_attention\": create_sliding_window_causal_mask,\n+    \"chunked_attention\": create_chunked_causal_mask,\n+}\n+\n+\n+def create_masks_for_generate(\n+    config: PretrainedConfig,\n+    input_embeds: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    cache_position: torch.Tensor,\n+    past_key_values: Optional[Cache],\n+    output_attentions: bool = False,\n+    or_mask_function: Optional[Callable] = None,\n+    and_mask_function: Optional[Callable] = None,\n+    **kwargs,\n+):\n+    \"\"\"\n+    This function mimics how we create the masks in the `modeling_xxx.py` files, and is used in `generate` in order\n+    to easily create the masks in advance, when we compile the forwards with Static caches.\n+\n+    Args:\n+        config (`PretrainedConfig`):\n+            The model config.\n+        input_embeds (`torch.Tensor`):\n+            The input embeddings of shape (batch_size, query_length, hidden_dim). This is used only to infer the\n+            batch size, query length and dtype.\n+        attention_mask (`torch.Tensor`, optional):\n+            The 2D attention mask corresponding to padded tokens of shape (batch_size, number_of_seen_tokens+q_length).\n+            It can also be an already prepared 4D mask, in which case it is returned as-is.\n+        cache_position (`torch.Tensor`):\n+            A tensor of shape (query_length,) indicating the current indices of the input sequence elements.\n+        past_key_values (`Cache`, optional):\n+            The past key values, if we use a cache.\n+        output_attentions (`bool`, optional):\n+            Whether we return the attention scores or not. By default `False`.\n+        or_mask_function (`Callable`, optional):\n+            An optional mask function to combine with the other mask function (by doing the union of both). This is\n+            useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n+        and_mask_function (`Callable`, optional):\n+            An optional mask function to combine with the other mask function (by doing the intersection of both). This is\n+            useful to easily overlay another mask on top of the causal one, for example for image tokens handling.\n+    \"\"\"\n+    # The attribute reside in the text config for composite models\n+    effective_config = config.get_text_config()\n+    # Prepare the mask args\n+    mask_kwargs = {\n+        \"config\": effective_config,\n+        \"input_embeds\": input_embeds,\n+        \"attention_mask\": attention_mask,\n+        \"cache_position\": cache_position,\n+        \"past_key_values\": past_key_values,\n+        \"output_attentions\": output_attentions,\n+        \"or_mask_function\": or_mask_function,\n+        \"and_mask_function\": and_mask_function,\n+    }\n+\n+    # If the attribute exist, we need several masks\n+    if hasattr(effective_config, \"layer_types\"):\n+        causal_masks = {}\n+        for layer_pattern in set(effective_config.layer_types):\n+            causal_masks[layer_pattern] = LAYER_PATTERN_TO_MASK_FUNCTION_MAPPING[layer_pattern](**mask_kwargs)\n+        return causal_masks\n+    # In this case, all layers are sliding\n+    elif getattr(effective_config, \"sliding_window\", None) is not None:\n+        return create_sliding_window_causal_mask(**mask_kwargs)\n+    # In this case, all layers are chunked\n+    elif getattr(effective_config, \"attention_chunk_size\", None) is not None:\n+        return create_chunked_causal_mask(**mask_kwargs)\n+    # All layers use standard causal attention\n+    return create_causal_mask(**mask_kwargs)\n+\n+\n+# Below are utilities to pretty-print the different masks\n+# Print the matrix with words as row labels\n+GREEN = \"\\033[92m\"\n+YELLOW = \"\\033[93m\"\n+RESET = \"\\033[0m\"\n+BLACK_SQUARE = \"â– \"\n+WHITE_SQUARE = \"â¬š\"\n+GREY_SQUARE = \"âˆ™\"\n+LOW_TRIANGLE = \"â¬•\"\n+UPPER_TRIANGLE = \"â¬”\"\n+\n+\n+def get_style(style):\n+    if style == \"majong\":\n+        BLACK_SQUARE = \"ðŸ€ž\"  # Full block (represents \"on\" or active)\n+        BLACK_SQUARE = \"ðŸ€™\"  # Full block (represents \"on\" or active)\n+        WHITE_SQUARE = \"ðŸ€†\"  # \"â–’\"  # Light shade (represents \"off\" or inactive)\n+        LOW_TRIANGLE = \"ðŸ€›\"  # Lower left triangle (stylized indication)\n+        UPPER_TRIANGLE = \"ðŸ€›\"  # Upper left triangle (stylized indication)\n+    else:\n+        BLACK_SQUARE = \"â–ˆ\"  # Full block (represents \"on\" or active)\n+        WHITE_SQUARE = \"â–‘\"  # \"â–’\"  # Light shade (represents \"off\" or inactive)\n+        LOW_TRIANGLE = \"â–™\"  # Lower left triangle (stylized indication))\n+        UPPER_TRIANGLE = \"â–œ\"  # Upper left triangle (stylized indication)\n+\n+    return BLACK_SQUARE, WHITE_SQUARE, LOW_TRIANGLE, UPPER_TRIANGLE\n+\n+\n+# LOW_TRIANGLE = UPPER_TRIANGLE = \"âŸ\"   # Upper right triangle (stylized indication)\n+\n+YELLOW_SQUARE = f\"{YELLOW}{BLACK_SQUARE}{RESET}\"\n+GREEN_SQUARE = f\"{GREEN}{BLACK_SQUARE}{RESET}\"\n+\n+\n+def tensor_to_mask_visual(original_tensor: torch.Tensor, grid_size=(20, 40), style=\"majong\") -> str:\n+    BLACK_SQUARE, WHITE_SQUARE, LOW_TRIANGLE, UPPER_TRIANGLE = get_style(style)\n+    h, w = original_tensor.shape\n+    max_h, max_w = grid_size\n+    if not (h < max_h and w < max_w):\n+        # Preserve aspect ratio within max grid size\n+        aspect_ratio = 2 * w / h\n+        if aspect_ratio > 1:\n+            w = max_w\n+            h = min(max_h, max(1, round(max_w / aspect_ratio)))\n+        else:\n+            h = max_h\n+            w = max(1, round(max_h * aspect_ratio))\n+\n+        # Step 1: Rescale tensor by average pooling\n+        tensor = original_tensor.unsqueeze(0).unsqueeze(0)  # Add batch and channel dimensions\n+        tensor = F.adaptive_avg_pool2d(tensor, output_size=(h, w))[0, 0]  # Remove extra dims\n+    else:\n+        tensor = original_tensor\n+\n+    # Step 3: Build the string representation\n+    result = []\n+    for i in range(h):\n+        row = \"\"\n+        for j in range(w):\n+            if tensor[i, j] == 1:\n+                row += BLACK_SQUARE\n+            elif tensor[i, j] == 0:\n+                row += WHITE_SQUARE\n+            else:\n+                if j > 0:\n+                    if tensor[i, j - 1] == 1:\n+                        row += LOW_TRIANGLE\n+                    elif tensor[i, j - 1] == 0:\n+                        row += UPPER_TRIANGLE\n+                    else:\n+                        row += BLACK_SQUARE if tensor[i, j] == 1 else WHITE_SQUARE\n+                else:\n+                    row += (\n+                        BLACK_SQUARE\n+                        if tensor[i, j] == 1\n+                        else (\n+                            WHITE_SQUARE\n+                            if tensor[i, j] == 0\n+                            else (UPPER_TRIANGLE if tensor[i, j + 1] == 1 else LOW_TRIANGLE)\n+                        )\n+                    )\n+        result.append(row)\n+\n+    return \"\\n\".join(result)\n+\n+\n+class AttentionMask(torch.Tensor):\n+    def __new__(cls, data, style=None):\n+        # Create a new instance of AttentionMask as a Tensor\n+        cls.style = style\n+        return torch.Tensor._make_subclass(cls, data, require_grad=False)\n+\n+    def __init__(self, data):\n+        # You can initialize any additional metadata here if needed\n+        pass\n+\n+    def to_string(self, grid_size=(20, 40), limit=4):\n+        \"\"\"Returns a string representation of the block mask.\"\"\"\n+        dense_mask = self\n+        *batch_dims, num_rows, num_cols = dense_mask.shape\n+        total_vis = []\n+\n+        for idx, batch_idx in enumerate(itertools.product(*[range(i) for i in batch_dims])):\n+            if idx == limit:\n+                total_vis.append(\"...\")\n+                total_vis.append(\"To print out more, set AttentionMask.to_string(limit=N)\")\n+                total_vis.append(\"You can also index (AttentionMask[batch, head]) to choose a specific batch or head\")\n+                break\n+            block_vis = tensor_to_mask_visual(dense_mask[batch_idx], grid_size=grid_size, style=self.style)\n+            total_vis.append(block_vis)\n+\n+        total_vis.append(f\"torch.Tensor(shape={tuple(self.shape)}, dtype={self.dtype})\")\n+        return \"\\n\".join(total_vis)\n+\n+    def __repr__(self):\n+        return self.to_string()\n+\n+    def __str__(self):\n+        return self.to_string()\n+\n+    @classmethod\n+    def from_tensor(cls, tensor: torch.Tensor, style: Optional[str] = None) -> \"AttentionMask\":\n+        res = cls(tensor)\n+        res.style = style\n+        return res"
        },
        {
            "sha": "df85a307aae323d9e2daf6d6137e0a2a611eb033",
            "filename": "src/transformers/modeling_attn_mask_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodeling_attn_mask_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodeling_attn_mask_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_attn_mask_utils.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -11,6 +11,12 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+\"\"\"\n+IMPORTANT NOTICE: Every class and function in this file is deprecated in favor of using the much more general\n+`masking_utils.py` primitives. New code should not rely on it, it is only kept for backward compatibility for now,\n+and will be removed in the future.\n+\"\"\"\n+\n from dataclasses import dataclass\n from typing import Optional, Union\n "
        },
        {
            "sha": "678ee983da5333d7ce974a3f06d232d7dca9d379",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -148,6 +148,12 @@ def _upad_input(\n             Maximum sequence length in batch (`max_seqlen_in_batch_q` for the target sequence i.e. query, `max_seqlen_in_batch_k` for the source sequence i.e. key/value).\n     \"\"\"\n     indices_k, cu_seqlens_k, max_seqlen_in_batch_k = _get_unpad_data(attention_mask)\n+\n+    # With static caches, the k/v states may be larger than the mask -> we need to slice them to avoid generating garbage\n+    # It's a bit of an anti-pattern, but otherwise we silently compute wrong attentions scores\n+    if key_layer.shape[1] > (seq_len := attention_mask.shape[-1]):\n+        key_layer, value_layer = key_layer[:, :seq_len, :, :], value_layer[:, :seq_len, :, :]\n+\n     batch_size, kv_seq_len, num_key_value_heads, head_dim = key_layer.shape\n \n     key_layer = index_first_axis(key_layer.reshape(batch_size * kv_seq_len, num_key_value_heads, head_dim), indices_k)"
        },
        {
            "sha": "97e95b4161b078f62ab04b2d6f21559bcb37882c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 32,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -27,7 +27,6 @@\n import tempfile\n import warnings\n from collections import defaultdict\n-from collections.abc import MutableMapping\n from contextlib import contextmanager\n from dataclasses import dataclass\n from enum import Enum\n@@ -124,6 +123,7 @@\n     replace_return_docstrings,\n     strtobool,\n )\n+from .utils.generic import GeneralInterface\n from .utils.hub import create_and_tag_model_card, get_checkpoint_shard_files\n from .utils.import_utils import (\n     ENV_VARS_TRUE_VALUES,\n@@ -6076,7 +6076,7 @@ def get_disk_only_shard_files(device_map, weight_map):\n     return [fname for fname, devices in files_content.items() if set(devices) == {\"disk\"}]\n \n \n-class AttentionInterface(MutableMapping):\n+class AttentionInterface(GeneralInterface):\n     \"\"\"\n     Dict-like object keeping track of allowed attention functions. You can easily add a new attention function\n     with a call to `register()`. If a model needs to locally overwrite an existing attention function, say `sdpa`,\n@@ -6091,36 +6091,6 @@ class AttentionInterface(MutableMapping):\n         \"sdpa\": sdpa_attention_forward,\n     }\n \n-    def __init__(self):\n-        self._local_mapping = {}\n-\n-    def __getitem__(self, key):\n-        # First check if instance has a local override\n-        if key in self._local_mapping:\n-            return self._local_mapping[key]\n-        return self._global_mapping[key]\n-\n-    def __setitem__(self, key, value):\n-        # Allow local update of the default functions without impacting other instances\n-        self._local_mapping.update({key: value})\n-\n-    def __delitem__(self, key):\n-        del self._local_mapping[key]\n-\n-    def __iter__(self):\n-        # Ensure we use all keys, with the overwritten ones on top\n-        return iter({**self._global_mapping, **self._local_mapping})\n-\n-    def __len__(self):\n-        return len(self._global_mapping.keys() | self._local_mapping.keys())\n-\n-    @classmethod\n-    def register(cls, key: str, value: Callable):\n-        cls._global_mapping.update({key: value})\n-\n-    def valid_keys(self) -> List[str]:\n-        return list(self.keys())\n-\n \n # Global AttentionInterface shared by all models which do not need to overwrite any of the existing ones\n ALL_ATTENTION_FUNCTIONS: AttentionInterface = AttentionInterface()"
        },
        {
            "sha": "cd7948462757e77537471542425f8c7a0269e269",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 9,
            "deletions": 194,
            "changes": 203,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -25,20 +25,14 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    LossKwargs,\n-    auto_docstring,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    logging,\n-)\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.import_utils import is_torch_available\n from ..auto import AutoModel\n from .configuration_aria import AriaConfig, AriaTextConfig\n@@ -49,12 +43,6 @@\n     from torch import nn\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -818,8 +806,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -865,129 +858,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n@@ -1521,61 +1391,6 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\n     \"AriaForConditionalGeneration\","
        },
        {
            "sha": "a851d4d0a0f27da76d095e28229bc3daf08a4d76",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -542,60 +542,5 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\"AyaVisionForConditionalGeneration\", \"AyaVisionPreTrainedModel\", \"AyaVisionModel\"]"
        },
        {
            "sha": "01f7f19a79e5b029f7493c1891c08b7e3ff06ea2",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -757,7 +757,7 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -827,7 +827,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "4ff34b9ef259e3a7a0b1702189545a5418b72e14",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1602,7 +1602,7 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1672,7 +1672,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "d93b6f6ae2d838fc814dbf13478be86422946608",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -482,7 +482,7 @@ def _init_weights(self, module):\n             module.bias.data.zero_()\n             module.weight.data.fill_(1.0)\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -552,7 +552,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "e98f9ed116264cb6350e701fc79586ab4c994057",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -27,23 +27,17 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_bitnet import BitNetConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -425,8 +419,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -472,129 +471,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "8eb282ac6faf8d6aba2495967aefa1d07fee321b",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -493,7 +493,7 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -563,7 +563,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "2f778d7293958ff653ca2429e87aaef78d2bec89",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -482,7 +482,7 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -552,7 +552,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "bdba37a73b01dd7ae7583028fc6eb426940c22ea",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -658,7 +658,7 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -728,7 +728,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "e7a46b43cf3356cd7f75c7c1002d7841dfaaf9a2",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1057,7 +1057,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1127,7 +1127,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "a1bef381ce703a07efd61395c531ce91784fdc24",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -491,7 +491,7 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -561,7 +561,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "37f698a86ec9e118d2639c74b561adaeee1d139d",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -35,23 +35,17 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_cohere import CohereConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -462,8 +456,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -509,129 +508,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "e407fb83dfd81bbbc8960eeee650daf3e7d2ddb5",
            "filename": "src/transformers/models/cohere2/configuration_cohere2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 8,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fconfiguration_cohere2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -19,7 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n \n \n@@ -119,9 +119,8 @@ class Cohere2Config(PretrainedConfig):\n             The dropout ratio for the attention probabilities.\n         sliding_window (`int`, *optional*, defaults to 4096):\n             Size of the sliding window attention context.\n-        sliding_window_pattern (`int`, *optional*, defaults to 4):\n-            Pattern for the sliding window attention.\n-        cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n \n     ```python\n     >>> from transformers import Cohere2Model, Cohere2Config\n@@ -177,8 +176,7 @@ def __init__(\n         attention_bias=False,\n         attention_dropout=0.0,\n         sliding_window=4096,\n-        sliding_window_pattern=4,\n-        cache_implementation=\"hybrid\",\n+        layer_types=None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -203,10 +201,9 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.sliding_window = sliding_window\n-        self.sliding_window_pattern = sliding_window_pattern\n+        self.layer_types = layer_types\n         # Need to specify head_dim in the config so it can be used in the attention forward functions\n         self.head_dim = hidden_size // num_attention_heads\n-        self.cache_implementation = cache_implementation\n \n         # Validate the correctness of rotary position embeddings parameters\n         rope_config_validation(self)\n@@ -219,5 +216,14 @@ def __init__(\n             **kwargs,\n         )\n \n+        if self.layer_types is None:\n+            # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+            sliding_window_pattern = getattr(self, \"sliding_window_pattern\", 4)\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % sliding_window_pattern) else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n \n __all__ = [\"Cohere2Config\"]"
        },
        {
            "sha": "144667f1e3dac064b936c07907af54b5937c4307",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 27,
            "deletions": 246,
            "changes": 273,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -25,25 +25,20 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, HybridCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_cohere2 import Cohere2Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -186,6 +181,7 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n         self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n         self.is_causal = True\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n         self.q_proj = nn.Linear(\n             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n@@ -199,9 +195,6 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.sliding_window = (\n-            config.sliding_window if (self.layer_idx + 1) % self.config.sliding_window_pattern != 0 else None\n-        )\n \n     def forward(\n         self,\n@@ -224,19 +217,9 @@ def forward(\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"sliding_window\": self.sliding_window,\n-                \"cache_position\": cache_position,\n-            }\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-            # Here we need to slice as we use a static cache by default, but FA2 does not support it\n-            if attention_mask is not None and self.config._attn_implementation == \"flash_attention_2\":\n-                seq_len = attention_mask.shape[-1]\n-                key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n-\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -284,12 +267,10 @@ class Cohere2DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Cohere2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-        self.self_attn = Cohere2Attention(config, layer_idx)\n+        self.self_attn = Cohere2Attention(config=config, layer_idx=layer_idx)\n         self.mlp = Cohere2MLP(config)\n         self.input_layernorm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n-        self.config = config\n-        self.is_sliding = (layer_idx + 1) % self.config.sliding_window_pattern != 0\n-        self.sliding_window = config.sliding_window\n+        self.attention_type = config.layer_types[layer_idx]\n \n     @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n@@ -322,34 +303,6 @@ def forward(\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n         \"\"\"\n-\n-        if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n-            # In prefill, we may be larger than sliding window\n-            effective_seq_len = max(cache_position.shape[0], self.sliding_window)\n-            # For FA2, the mask is 2D and is of shape [bs, processed_tokens] (not [bs, max_cache_len]),\n-            # thus we must slice from the right (at most `effective_seq_len` elements)\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                attention_mask = attention_mask[:, -effective_seq_len:]\n-            # Otherwise, the mask is 4D of shape [bs, 1, query_len, max_cache_len] thus we must slice\n-            # from the left, with an offset if we are beyond the sliding window\n-            else:\n-                min_dtype = torch.finfo(hidden_states.dtype).min\n-                sliding_window_mask = torch.tril(\n-                    torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n-                )\n-                attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n-                # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                offset = cache_position[-1] - effective_seq_len + 1\n-                # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = torch.clamp(offset, min=0)\n-                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n-                # but without data-dependent slicing (i.e. torch.compile friendly)\n-                mask_indexes = torch.arange(\n-                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n-                )\n-                mask_indexes += offset\n-                attention_mask = attention_mask[:, :, :, mask_indexes]\n-\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n@@ -440,7 +393,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridCache] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -467,15 +420,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            batch_size, seq_len, _ = inputs_embeds.shape\n-            # NOTE: ideally, `HybridCache` should be initialized outside the model with `layer_device_map`\n-            past_key_values = HybridCache(\n-                self.config,\n-                max_batch_size=batch_size,\n-                max_cache_len=seq_len,\n-                dtype=inputs_embeds.dtype,\n-                device=self.device,\n-            )\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -485,9 +430,22 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n \n         hidden_states = inputs_embeds\n \n@@ -505,7 +463,7 @@ def forward(\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 position_embeddings=position_embeddings,\n-                attention_mask=causal_mask,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -531,100 +489,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    @torch.no_grad()\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: HybridCache,\n-        output_attentions: bool = False,\n-    ):\n-        # Flash Attention currently doesn't support static cache but Cohere2 work only with static cache.\n-        # So we will pass in attention mask as is in any case, not only when ther's padding. Then we'll use its shape\n-        # to cut out keys/values trailing 0 used in static cache. This workaround should be compile compatible\n-        # as it doesn't cause dynamic control issues.\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            return attention_mask\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        dtype, device = input_tensor.dtype, input_tensor.device\n-        sequence_length = input_tensor.shape[1]\n-        if isinstance(past_key_values, (HybridCache, StaticCache)):\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            device=device,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n@@ -635,7 +499,7 @@ class Cohere2ForCausalLM(Cohere2PreTrainedModel, GenerationMixin):\n     _tp_plan = {\"lm_head\": \"colwise_rep\"}\n     _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n \n-    def __init__(self, config: Cohere2Config):\n+    def __init__(self, config):\n         super().__init__(config)\n         self.model = Cohere2Model(config)\n         self.vocab_size = config.vocab_size\n@@ -740,88 +604,5 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        position_ids=None,\n-        use_cache=True,\n-        logits_to_keep=None,\n-        **kwargs,\n-    ):\n-        # Overwritten: has a special cache type, `HybridCache`\n-\n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        #              (we can't check exception 3 while compiling)\n-        if past_key_values is not None:\n-            if (\n-                inputs_embeds is not None  # Exception 1\n-                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n-            ):\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s\n-                # `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride\n-                # during the decoding. Here, simply using `.contiguous()` is not sufficient as in the\n-                # batch size = 1 case, `position_ids` is already contiguous but with varying stride\n-                # which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # The clone here is for the same reason as for `position_ids`.\n-            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n-\n-        if (\n-            isinstance(past_key_values, HybridCache)\n-            and attention_mask.ndim == 2\n-            and not self.config._attn_implementation == \"flash_attention_2\"\n-        ):\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-            )\n-\n-        if logits_to_keep is not None:\n-            model_inputs[\"logits_to_keep\"] = logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-            }\n-        )\n-        return model_inputs\n-\n \n __all__ = [\"Cohere2ForCausalLM\", \"Cohere2Model\", \"Cohere2PreTrainedModel\"]"
        },
        {
            "sha": "792d278cc0ac7ebad0f5d4ceb2eedfbe69436795",
            "filename": "src/transformers/models/cohere2/modular_cohere2.py",
            "status": "modified",
            "additions": 39,
            "deletions": 154,
            "changes": 193,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodular_cohere2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -19,8 +19,9 @@\n import torch.nn as nn\n import torch.utils.checkpoint\n \n-from ...cache_utils import Cache, HybridCache\n-from ...configuration_utils import PretrainedConfig\n+from ...cache_utils import Cache, DynamicCache\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import rope_config_validation\n@@ -140,9 +141,8 @@ class Cohere2Config(PretrainedConfig):\n             The dropout ratio for the attention probabilities.\n         sliding_window (`int`, *optional*, defaults to 4096):\n             Size of the sliding window attention context.\n-        sliding_window_pattern (`int`, *optional*, defaults to 4):\n-            Pattern for the sliding window attention.\n-        cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n \n     ```python\n     >>> from transformers import Cohere2Model, Cohere2Config\n@@ -198,8 +198,7 @@ def __init__(\n         attention_bias=False,\n         attention_dropout=0.0,\n         sliding_window=4096,\n-        sliding_window_pattern=4,\n-        cache_implementation=\"hybrid\",\n+        layer_types=None,\n         **kwargs,\n     ):\n         self.vocab_size = vocab_size\n@@ -224,10 +223,9 @@ def __init__(\n         self.attention_bias = attention_bias\n         self.attention_dropout = attention_dropout\n         self.sliding_window = sliding_window\n-        self.sliding_window_pattern = sliding_window_pattern\n+        self.layer_types = layer_types\n         # Need to specify head_dim in the config so it can be used in the attention forward functions\n         self.head_dim = hidden_size // num_attention_heads\n-        self.cache_implementation = cache_implementation\n \n         # Validate the correctness of rotary position embeddings parameters\n         rope_config_validation(self)\n@@ -240,6 +238,15 @@ def __init__(\n             **kwargs,\n         )\n \n+        if self.layer_types is None:\n+            # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+            sliding_window_pattern = getattr(self, \"sliding_window_pattern\", 4)\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % sliding_window_pattern) else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n \n class Cohere2RotaryEmbedding(CohereRotaryEmbedding):\n     pass\n@@ -261,6 +268,7 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n         self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n         self.is_causal = True\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n         self.q_proj = nn.Linear(\n             config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n@@ -274,9 +282,6 @@ def __init__(self, config: Cohere2Config, layer_idx: Optional[int] = None):\n         self.o_proj = nn.Linear(\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n-        self.sliding_window = (\n-            config.sliding_window if (self.layer_idx + 1) % self.config.sliding_window_pattern != 0 else None\n-        )\n \n     def forward(\n         self,\n@@ -299,19 +304,9 @@ def forward(\n             query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"sliding_window\": self.sliding_window,\n-                \"cache_position\": cache_position,\n-            }\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-            # Here we need to slice as we use a static cache by default, but FA2 does not support it\n-            if attention_mask is not None and self.config._attn_implementation == \"flash_attention_2\":\n-                seq_len = attention_mask.shape[-1]\n-                key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n-\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -342,10 +337,7 @@ def forward(\n class Cohere2DecoderLayer(CohereDecoderLayer):\n     def __init__(self, config: Cohere2Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n-        self.self_attn = Cohere2Attention(config, layer_idx)\n-        self.config = config\n-        self.is_sliding = (layer_idx + 1) % self.config.sliding_window_pattern != 0\n-        self.sliding_window = config.sliding_window\n+        self.attention_type = config.layer_types[layer_idx]\n \n     @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n@@ -378,34 +370,6 @@ def forward(\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n         \"\"\"\n-\n-        if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n-            # In prefill, we may be larger than sliding window\n-            effective_seq_len = max(cache_position.shape[0], self.sliding_window)\n-            # For FA2, the mask is 2D and is of shape [bs, processed_tokens] (not [bs, max_cache_len]),\n-            # thus we must slice from the right (at most `effective_seq_len` elements)\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                attention_mask = attention_mask[:, -effective_seq_len:]\n-            # Otherwise, the mask is 4D of shape [bs, 1, query_len, max_cache_len] thus we must slice\n-            # from the left, with an offset if we are beyond the sliding window\n-            else:\n-                min_dtype = torch.finfo(hidden_states.dtype).min\n-                sliding_window_mask = torch.tril(\n-                    torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n-                )\n-                attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n-                # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                offset = cache_position[-1] - effective_seq_len + 1\n-                # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = torch.clamp(offset, min=0)\n-                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n-                # but without data-dependent slicing (i.e. torch.compile friendly)\n-                mask_indexes = torch.arange(\n-                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n-                )\n-                mask_indexes += offset\n-                attention_mask = attention_mask[:, :, :, mask_indexes]\n-\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n@@ -451,7 +415,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridCache] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -478,15 +442,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            batch_size, seq_len, _ = inputs_embeds.shape\n-            # NOTE: ideally, `HybridCache` should be initialized outside the model with `layer_device_map`\n-            past_key_values = HybridCache(\n-                self.config,\n-                max_batch_size=batch_size,\n-                max_cache_len=seq_len,\n-                dtype=inputs_embeds.dtype,\n-                device=self.device,\n-            )\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -496,9 +452,22 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n \n         hidden_states = inputs_embeds\n \n@@ -516,7 +485,7 @@ def forward(\n             layer_outputs = decoder_layer(\n                 hidden_states,\n                 position_embeddings=position_embeddings,\n-                attention_mask=causal_mask,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n                 use_cache=use_cache,\n@@ -544,91 +513,7 @@ def forward(\n \n \n class Cohere2ForCausalLM(CohereForCausalLM):\n-    def __init__(self, config: Cohere2Config):\n-        super().__init__(config)\n-\n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        position_ids=None,\n-        use_cache=True,\n-        logits_to_keep=None,\n-        **kwargs,\n-    ):\n-        # Overwritten: has a special cache type, `HybridCache`\n-\n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        #              (we can't check exception 3 while compiling)\n-        if past_key_values is not None:\n-            if (\n-                inputs_embeds is not None  # Exception 1\n-                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n-            ):\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-        if attention_mask is not None and position_ids is None:\n-            # create position_ids on the fly for batch generation\n-            position_ids = attention_mask.long().cumsum(-1) - 1\n-            position_ids.masked_fill_(attention_mask == 0, 1)\n-            if past_key_values:\n-                position_ids = position_ids[:, -input_ids.shape[1] :]\n-                # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s\n-                # `mode=\"reduce-overhead`, as otherwise the input `position_ids` would have various stride\n-                # during the decoding. Here, simply using `.contiguous()` is not sufficient as in the\n-                # batch size = 1 case, `position_ids` is already contiguous but with varying stride\n-                # which retriggers a capture.\n-                position_ids = position_ids.clone(memory_format=torch.contiguous_format)\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # The clone here is for the same reason as for `position_ids`.\n-            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n-\n-        if (\n-            isinstance(past_key_values, HybridCache)\n-            and attention_mask.ndim == 2\n-            and not self.config._attn_implementation == \"flash_attention_2\"\n-        ):\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-            )\n-\n-        if logits_to_keep is not None:\n-            model_inputs[\"logits_to_keep\"] = logits_to_keep\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-            }\n-        )\n-        return model_inputs\n+    pass\n \n \n __all__ = [\"Cohere2Config\", \"Cohere2ForCausalLM\", \"Cohere2Model\", \"Cohere2PreTrainedModel\"]"
        },
        {
            "sha": "6f8fd7a487f641faddc40735ad9f06b885353617",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 16,
            "deletions": 313,
            "changes": 329,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -29,25 +29,19 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, ModelOutput, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, ModelOutput, auto_docstring, can_return_tuple, logging\n from ..auto import AutoModel\n from .configuration_csm import CsmConfig, CsmDepthDecoderConfig\n from .generation_csm import CsmGenerationMixin\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -516,8 +510,13 @@ def forward(\n \n         inputs_embeds = self.inputs_embeds_projector(inputs_embeds)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -564,129 +563,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class CsmCodebooksHead(nn.Module):\n     def __init__(self, hidden_size, num_codebooks, vocab_size):\n@@ -946,8 +822,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -993,129 +874,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\"\n@@ -1477,61 +1235,6 @@ def forward(\n             depth_decoder_attentions=depth_decoder_outputs.attentions if depth_decoder_outputs is not None else None,\n         )\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\n     \"CsmPreTrainedModel\","
        },
        {
            "sha": "35fdf127fcd38e3feed6741aa60dac780edaec35",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 8,
            "deletions": 57,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -21,6 +21,7 @@\n \n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -240,8 +241,13 @@ def forward(\n \n         inputs_embeds = self.inputs_embeds_projector(inputs_embeds)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -835,61 +841,6 @@ def forward(\n             depth_decoder_attentions=depth_decoder_outputs.attentions if depth_decoder_outputs is not None else None,\n         )\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\n     \"CsmPreTrainedModel\","
        },
        {
            "sha": "0a530e87ae1b8ffc6b848eab40cb17ef37c261a6",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1001,7 +1001,7 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1071,7 +1071,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "b15301e2884ec37cb44ef7fb98f22c79dbe78be2",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -15,23 +15,17 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_deepseek_v3 import DeepseekV3Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -608,8 +602,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -655,129 +654,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "84df7b4d41f2aafa6dd27b1848b2f2ff3af18b37",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -31,7 +31,7 @@\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import (\n     FlashAttentionKwargs,\n     _flash_attention_forward,\n@@ -48,16 +48,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_diffllama import DiffLlamaConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -708,8 +702,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -755,129 +754,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "3b570fd1f264fc97ebaa2c99237af7f43c9cb5c0",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 9,
            "deletions": 189,
            "changes": 198,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -32,23 +32,17 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_emu3 import Emu3Config, Emu3TextConfig, Emu3VQVAEConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -1279,8 +1273,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -1326,129 +1325,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n@@ -1857,62 +1733,6 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\n     \"Emu3ForConditionalGeneration\","
        },
        {
            "sha": "bf2e6a5efa7f928acc7a7389792f4d1a1dcfb32f",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 56,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1212,62 +1212,6 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\n     \"Emu3ForConditionalGeneration\","
        },
        {
            "sha": "df87d36242e0c9574bffc9871031d221e4d04591",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -976,7 +976,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "3a2e20e7cc187f1b9cf793b9d3b7b9239932f552",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -45,12 +45,7 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    auto_docstring,\n-    is_torchdynamo_compiling,\n-    logging,\n-    replace_return_docstrings,\n-)\n+from ...utils import auto_docstring, is_torchdynamo_compiling, logging, replace_return_docstrings\n from ...utils.deprecation import deprecate_kwarg\n from ...utils.import_utils import is_causal_conv1d_available, is_mamba_2_ssm_available\n from .configuration_falcon_h1 import FalconH1Config"
        },
        {
            "sha": "897f329e56c6d1541224cc5594b61a07a0984840",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 11,
            "deletions": 135,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -19,15 +19,15 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import Callable, List, Optional, Tuple, Union\n+from typing import Callable, Optional, Tuple, Union\n \n import torch\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -39,16 +39,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_gemma import GemmaConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -384,7 +378,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -422,8 +416,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         # embed positions\n@@ -475,129 +474,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "1a1e8cc1c63848361bf1656bb50d37b24140fc13",
            "filename": "src/transformers/models/gemma/modular_gemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodular_gemma.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -13,7 +13,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from typing import TYPE_CHECKING, Any, Dict, List, Optional, Union\n+from typing import TYPE_CHECKING, Any, Dict, List, Optional\n \n import sentencepiece as spm\n import torch\n@@ -22,6 +22,7 @@\n \n from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PretrainedConfig\n+from ...masking_utils import create_causal_mask\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...tokenization_utils import AddedToken, PreTrainedTokenizer\n from ...utils import logging\n@@ -371,7 +372,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -409,8 +410,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         # embed positions"
        },
        {
            "sha": "810c10cc928dfecac330f6554d0e3b9f791ae1a4",
            "filename": "src/transformers/models/gemma2/configuration_gemma2.py",
            "status": "modified",
            "additions": 19,
            "deletions": 9,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fconfiguration_gemma2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -19,7 +19,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n \n \n class Gemma2Config(PretrainedConfig):\n@@ -78,12 +78,16 @@ class Gemma2Config(PretrainedConfig):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        query_pre_attn_scalar (`float`, *optional*, defaults to 256): scaling factor used on the attention scores\n-        sliding_window (`int`, *optional*, defaults to 4096): in Gemma2, every other layer uses sliding window attention. This is the\n-            size of the sliding window.\n-        final_logit_softcapping (`float`, *optional*, defaults to 30.0): scaling factor when applying tanh softcapping on the logits.\n-        attn_logit_softcapping (`float`, *optional*, defaults to 50.0): scaling factor when applying tanh softcapping on the attention scores.\n-        cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n+            scaling factor used on the attention scores\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            in Gemma2, every other layer uses sliding window attention. This is the size of the sliding window.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        final_logit_softcapping (`float`, *optional*, defaults to 30.0):\n+            scaling factor when applying tanh softcapping on the logits.\n+        attn_logit_softcapping (`float`, *optional*, defaults to 50.0):\n+            scaling factor when applying tanh softcapping on the attention scores.\n \n     ```python\n     >>> from transformers import Gemma2Model, Gemma2Config\n@@ -135,9 +139,9 @@ def __init__(\n         attention_dropout=0.0,\n         query_pre_attn_scalar=256,\n         sliding_window=4096,\n+        layer_types=None,\n         final_logit_softcapping=30.0,\n         attn_logit_softcapping=50.0,\n-        cache_implementation=\"hybrid\",\n         **kwargs,\n     ):\n         super().__init__(\n@@ -166,7 +170,13 @@ def __init__(\n         self.sliding_window = sliding_window\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n-        self.cache_implementation = cache_implementation\n+        self.layer_types = layer_types\n+\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n \n \n __all__ = [\"Gemma2Config\"]"
        },
        {
            "sha": "fe5576ae1c8c51c38cbbea41f9181057deea7eda",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 27,
            "deletions": 213,
            "changes": 240,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -26,8 +26,9 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, HybridCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -38,17 +39,11 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import auto_docstring, can_return_tuple, logging\n from ...utils.deprecation import deprecate_kwarg\n from .configuration_gemma2 import Gemma2Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -195,7 +190,7 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n             config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n         )\n         self.attn_logit_softcapping = self.config.attn_logit_softcapping\n-        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n     def forward(\n         self,\n@@ -218,19 +213,9 @@ def forward(\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"cache_position\": cache_position,\n-                \"sliding_window\": self.sliding_window,\n-            }\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-            # Here we need to slice as we use a static cache by default, but FA2 does not support it\n-            if attention_mask is not None and self.config._attn_implementation == \"flash_attention_2\":\n-                seq_len = attention_mask.shape[-1]\n-                key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n-\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -264,15 +249,14 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.config = config\n-        self.is_sliding = not bool(layer_idx % 2)\n+        self.attention_type = config.layer_types[layer_idx]\n         self.self_attn = Gemma2Attention(config=config, layer_idx=layer_idx)\n         self.mlp = Gemma2MLP(config)\n         self.input_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.sliding_window = config.sliding_window\n \n     @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n@@ -287,33 +271,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n-            # In prefill, we may be larger than sliding window\n-            effective_seq_len = max(cache_position.shape[0], self.sliding_window)\n-            # For FA2, the mask is 2D and is of shape [bs, processed_tokens] (not [bs, max_cache_len]),\n-            # thus we must slice from the right (at most `effective_seq_len` elements)\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                attention_mask = attention_mask[:, -effective_seq_len:]\n-            # Otherwise, the mask is 4D of shape [bs, 1, query_len, max_cache_len] thus we must slice\n-            # from the left, with an offset if we are beyond the sliding window\n-            else:\n-                min_dtype = torch.finfo(attention_mask.dtype).min\n-                sliding_window_mask = torch.tril(\n-                    torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n-                )\n-                attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n-                # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                offset = cache_position[-1] - effective_seq_len + 1\n-                # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = torch.clamp(offset, min=0)\n-                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n-                # but without data-dependent slicing (i.e. torch.compile friendly)\n-                mask_indexes = torch.arange(\n-                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n-                )\n-                mask_indexes += offset\n-                attention_mask = attention_mask[:, :, :, mask_indexes]\n-\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n@@ -441,7 +398,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridCache] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -468,15 +425,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            batch_size, seq_len, _ = inputs_embeds.shape\n-            # NOTE: ideally, `HybridCache` should be initialized outside the model with `layer_device_map`\n-            past_key_values = HybridCache(\n-                self.config,\n-                max_batch_size=batch_size,\n-                max_cache_len=seq_len,\n-                dtype=inputs_embeds.dtype,\n-                device=self.device,\n-            )\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -487,9 +436,22 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n \n         # embed positions\n         hidden_states = inputs_embeds\n@@ -516,7 +478,7 @@ def forward(\n                     partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     position_embeddings,\n-                    causal_mask,\n+                    causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids,\n                     past_key_values,\n                     output_attentions,\n@@ -527,7 +489,7 @@ def forward(\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n                     position_embeddings=position_embeddings,\n-                    attention_mask=causal_mask,\n+                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n@@ -553,100 +515,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    @torch.no_grad()\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: HybridCache,\n-        output_attentions: bool = False,\n-    ):\n-        # Flash Attention currently doesn't support static cache but Gemma2 work only with static cache.\n-        # So we will pass in attention mask as is in any case, not only when ther's padding. Then we'll use its shape\n-        # to cut out keys/values trailing 0 used in static cache. This workaround should be compile compatible\n-        # as it doesn't cause dynamic control issues.\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            return attention_mask\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        dtype, device = input_tensor.dtype, input_tensor.device\n-        sequence_length = input_tensor.shape[1]\n-        if isinstance(past_key_values, (HybridCache, StaticCache)):\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            device=device,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @auto_docstring\n class Gemma2ForCausalLM(Gemma2PreTrainedModel, GenerationMixin):\n@@ -688,7 +556,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridCache] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -765,60 +633,6 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        position_ids=None,\n-        use_cache=True,\n-        logits_to_keep=None,\n-        **kwargs,\n-    ):\n-        # Overwritten: has a special cache type, `HybridCache`\n-\n-        model_inputs = super().prepare_inputs_for_generation(\n-            input_ids,\n-            past_key_values=past_key_values,\n-            attention_mask=attention_mask,\n-            inputs_embeds=inputs_embeds,\n-            cache_position=cache_position,\n-            position_ids=position_ids,\n-            use_cache=use_cache,\n-            logits_to_keep=logits_to_keep,\n-            **kwargs,\n-        )\n-\n-        if logits_to_keep is None:\n-            _ = model_inputs.pop(\"logits_to_keep\", None)\n-\n-        if (\n-            isinstance(past_key_values, HybridCache)\n-            and attention_mask.ndim == 2\n-            and not self.config._attn_implementation == \"flash_attention_2\"\n-        ):\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-            )\n-            model_inputs[\"attention_mask\"] = attention_mask\n-\n-        return model_inputs\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "7d0b721d8093863bdf0765814d047b740828e161",
            "filename": "src/transformers/models/gemma2/modular_gemma2.py",
            "status": "modified",
            "additions": 46,
            "deletions": 167,
            "changes": 213,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodular_gemma2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -21,13 +21,14 @@\n import torch.utils.checkpoint\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, HybridCache, StaticCache\n-from ...configuration_utils import PretrainedConfig\n+from ...cache_utils import Cache, DynamicCache\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import is_torch_flex_attn_available, logging\n+from ...utils import logging\n from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import (\n     GemmaAttention,\n@@ -42,12 +43,6 @@\n )\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -107,12 +102,16 @@ class Gemma2Config(PretrainedConfig):\n             Whether to use a bias in the query, key, value and output projection layers during self-attention.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n-        query_pre_attn_scalar (`float`, *optional*, defaults to 256): scaling factor used on the attention scores\n-        sliding_window (`int`, *optional*, defaults to 4096): in Gemma2, every other layer uses sliding window attention. This is the\n-            size of the sliding window.\n-        final_logit_softcapping (`float`, *optional*, defaults to 30.0): scaling factor when applying tanh softcapping on the logits.\n-        attn_logit_softcapping (`float`, *optional*, defaults to 50.0): scaling factor when applying tanh softcapping on the attention scores.\n-        cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n+        query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n+            scaling factor used on the attention scores\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            in Gemma2, every other layer uses sliding window attention. This is the size of the sliding window.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n+        final_logit_softcapping (`float`, *optional*, defaults to 30.0):\n+            scaling factor when applying tanh softcapping on the logits.\n+        attn_logit_softcapping (`float`, *optional*, defaults to 50.0):\n+            scaling factor when applying tanh softcapping on the attention scores.\n \n     ```python\n     >>> from transformers import Gemma2Model, Gemma2Config\n@@ -164,9 +163,9 @@ def __init__(\n         attention_dropout=0.0,\n         query_pre_attn_scalar=256,\n         sliding_window=4096,\n+        layer_types=None,\n         final_logit_softcapping=30.0,\n         attn_logit_softcapping=50.0,\n-        cache_implementation=\"hybrid\",\n         **kwargs,\n     ):\n         super().__init__(\n@@ -195,7 +194,13 @@ def __init__(\n         self.sliding_window = sliding_window\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n-        self.cache_implementation = cache_implementation\n+        self.layer_types = layer_types\n+\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % 2) else \"full_attention\" for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n \n \n class Gemma2RMSNorm(GemmaRMSNorm):\n@@ -250,7 +255,7 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         self.attention_dropout = self.config.attention_dropout\n         self.is_causal = True\n         self.scaling = config.query_pre_attn_scalar**-0.5\n-        self.sliding_window = config.sliding_window if not bool(layer_idx % 2) else None\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n     def forward(\n         self,\n@@ -273,19 +278,9 @@ def forward(\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"cache_position\": cache_position,\n-                \"sliding_window\": self.sliding_window,\n-            }\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-            # Here we need to slice as we use a static cache by default, but FA2 does not support it\n-            if attention_mask is not None and self.config._attn_implementation == \"flash_attention_2\":\n-                seq_len = attention_mask.shape[-1]\n-                key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n-\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -319,15 +314,14 @@ def __init__(self, config: Gemma2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.config = config\n-        self.is_sliding = not bool(layer_idx % 2)\n+        self.attention_type = config.layer_types[layer_idx]\n         self.self_attn = Gemma2Attention(config=config, layer_idx=layer_idx)\n         self.mlp = Gemma2MLP(config)\n         self.input_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n         self.pre_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        self.sliding_window = config.sliding_window\n \n     @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n@@ -342,33 +336,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n-            # In prefill, we may be larger than sliding window\n-            effective_seq_len = max(cache_position.shape[0], self.sliding_window)\n-            # For FA2, the mask is 2D and is of shape [bs, processed_tokens] (not [bs, max_cache_len]),\n-            # thus we must slice from the right (at most `effective_seq_len` elements)\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                attention_mask = attention_mask[:, -effective_seq_len:]\n-            # Otherwise, the mask is 4D of shape [bs, 1, query_len, max_cache_len] thus we must slice\n-            # from the left, with an offset if we are beyond the sliding window\n-            else:\n-                min_dtype = torch.finfo(attention_mask.dtype).min\n-                sliding_window_mask = torch.tril(\n-                    torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n-                )\n-                attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n-                # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                offset = cache_position[-1] - effective_seq_len + 1\n-                # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = torch.clamp(offset, min=0)\n-                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n-                # but without data-dependent slicing (i.e. torch.compile friendly)\n-                mask_indexes = torch.arange(\n-                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n-                )\n-                mask_indexes += offset\n-                attention_mask = attention_mask[:, :, :, mask_indexes]\n-\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n@@ -414,7 +381,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridCache] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -441,15 +408,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            batch_size, seq_len, _ = inputs_embeds.shape\n-            # NOTE: ideally, `HybridCache` should be initialized outside the model with `layer_device_map`\n-            past_key_values = HybridCache(\n-                self.config,\n-                max_batch_size=batch_size,\n-                max_cache_len=seq_len,\n-                dtype=inputs_embeds.dtype,\n-                device=self.device,\n-            )\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -460,9 +419,22 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n \n         # embed positions\n         hidden_states = inputs_embeds\n@@ -489,7 +461,7 @@ def forward(\n                     partial(decoder_layer.__call__, **flash_attn_kwargs),\n                     hidden_states,\n                     position_embeddings,\n-                    causal_mask,\n+                    causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids,\n                     past_key_values,\n                     output_attentions,\n@@ -500,7 +472,7 @@ def forward(\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n                     position_embeddings=position_embeddings,\n-                    attention_mask=causal_mask,\n+                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n@@ -526,45 +498,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    @torch.no_grad()\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: HybridCache,\n-        output_attentions: bool = False,\n-    ):\n-        # Flash Attention currently doesn't support static cache but Gemma2 work only with static cache.\n-        # So we will pass in attention mask as is in any case, not only when ther's padding. Then we'll use its shape\n-        # to cut out keys/values trailing 0 used in static cache. This workaround should be compile compatible\n-        # as it doesn't cause dynamic control issues.\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            return attention_mask\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        dtype, device = input_tensor.dtype, input_tensor.device\n-        sequence_length = input_tensor.shape[1]\n-        if isinstance(past_key_values, (HybridCache, StaticCache)):\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            device=device,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-        return causal_mask\n-\n \n class Gemma2ForCausalLM(GemmaForCausalLM):\n     def __init__(self, config):\n@@ -577,7 +510,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridCache] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -649,60 +582,6 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        position_ids=None,\n-        use_cache=True,\n-        logits_to_keep=None,\n-        **kwargs,\n-    ):\n-        # Overwritten: has a special cache type, `HybridCache`\n-\n-        model_inputs = super().prepare_inputs_for_generation(\n-            input_ids,\n-            past_key_values=past_key_values,\n-            attention_mask=attention_mask,\n-            inputs_embeds=inputs_embeds,\n-            cache_position=cache_position,\n-            position_ids=position_ids,\n-            use_cache=use_cache,\n-            logits_to_keep=logits_to_keep,\n-            **kwargs,\n-        )\n-\n-        if logits_to_keep is None:\n-            _ = model_inputs.pop(\"logits_to_keep\", None)\n-\n-        if (\n-            isinstance(past_key_values, HybridCache)\n-            and attention_mask.ndim == 2\n-            and not self.config._attn_implementation == \"flash_attention_2\"\n-        ):\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-            )\n-            model_inputs[\"attention_mask\"] = attention_mask\n-\n-        return model_inputs\n-\n \n class Gemma2ForSequenceClassification(GemmaForSequenceClassification):\n     def __init__(self, config):"
        },
        {
            "sha": "db2749644cca88ab714b2689309e8f3318bed839",
            "filename": "src/transformers/models/gemma3/configuration_gemma3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 11,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fconfiguration_gemma3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -21,7 +21,7 @@\n # limitations under the License.\n from typing import Any, Dict, Optional, Union\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n from ..siglip import SiglipVisionConfig\n@@ -88,13 +88,14 @@ class Gemma3TextConfig(PretrainedConfig):\n             The dropout ratio for the attention probabilities.\n         query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n             Scaling factor used on the attention scores\n-        sliding_window (`int`, *optional*, defaults to 4096): in Gemma3Text, every other layer uses sliding window attention. This is the\n-            size of the sliding window.\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            In Gemma3Text, every other layer uses sliding window attention. This is the size of the sliding window.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n         final_logit_softcapping (`float`, *optional*):\n             Scaling factor when applying tanh softcapping on the logits.\n         attn_logit_softcapping (`float`, *optional*):\n             Scaling factor when applying tanh softcapping on the attention scores.\n-        cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n         rope_scaling (`Dict`, *optional*):\n             Dictionary containing the scaling configuration for the RoPE embeddings used in global attention. NOTE: if you apply new rope type\n             and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n@@ -134,8 +135,6 @@ class Gemma3TextConfig(PretrainedConfig):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         rope_local_base_freq (float, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings for local attention.\n-        sliding_window_pattern (`int`, *optional*, defaults to 6):\n-            Pattern for the sliding window attention.\n \n     ```python\n     >>> from transformers import Gemma3TextModel, Gemma3TextConfig\n@@ -192,12 +191,11 @@ def __init__(\n         attention_dropout=0.0,\n         query_pre_attn_scalar=256,\n         sliding_window=4096,\n+        layer_types=None,\n         final_logit_softcapping=None,\n         attn_logit_softcapping=None,\n-        cache_implementation=\"hybrid\",\n         rope_scaling=None,\n         rope_local_base_freq=10_000.0,\n-        sliding_window_pattern=6,\n         **kwargs,\n     ):\n         super().__init__(\n@@ -226,14 +224,21 @@ def __init__(\n         self.sliding_window = sliding_window\n         self.final_logit_softcapping = final_logit_softcapping\n         self.attn_logit_softcapping = attn_logit_softcapping\n-        self.cache_implementation = cache_implementation\n+        self.layer_types = layer_types\n \n         self.rope_local_base_freq = rope_local_base_freq\n-        # For configuring HybridCache to work with 5:1 attention pattern\n-        self.sliding_window_pattern = sliding_window_pattern\n         self.rope_scaling = rope_scaling\n         rope_config_validation(self)\n \n+        if self.layer_types is None:\n+            # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+            sliding_window_pattern = getattr(self, \"sliding_window_pattern\", 6)\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % sliding_window_pattern) else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n \n class Gemma3Config(PretrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "122d16aafce5a4e697c69a61ad5f9543e8344d5a",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 92,
            "deletions": 369,
            "changes": 461,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -29,32 +29,21 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, HybridCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n+from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    ModelOutput,\n-    auto_docstring,\n-    can_return_tuple,\n-    is_torch_flex_attn_available,\n-    is_torchdynamo_compiling,\n-    logging,\n-)\n+from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n from ...utils.deprecation import deprecate_kwarg\n from ..auto import AutoModel\n from .configuration_gemma3 import Gemma3Config, Gemma3TextConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -300,7 +289,7 @@ class Gemma3Attention(nn.Module):\n \n     def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         super().__init__()\n-        self.is_sliding = bool((layer_idx + 1) % config.sliding_window_pattern)\n+        self.is_sliding = config.layer_types[layer_idx] == \"sliding_attention\"\n         self.config = config\n         self.layer_idx = layer_idx\n         self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n@@ -351,19 +340,9 @@ def forward(\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"cache_position\": cache_position,\n-                \"sliding_window\": self.sliding_window,\n-            }\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-            # Here we need to slice as we use a static cache by default, but FA2 does not support it\n-            if attention_mask is not None and self.config._attn_implementation == \"flash_attention_2\":\n-                seq_len = attention_mask.shape[-1]\n-                key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n-\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -374,9 +353,7 @@ def forward(\n                 )\n             else:\n                 attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n-        if attention_mask is not None:\n-            # backwards compatibility\n-            attention_mask = attention_mask.to(query_states)\n+\n         attn_output, attn_weights = attention_interface(\n             self,\n             query_states,\n@@ -400,14 +377,13 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.config = config\n         self.hidden_size = config.hidden_size\n         self.layer_idx = layer_idx\n+        self.attention_type = config.layer_types[layer_idx]\n         self.self_attn = Gemma3Attention(config=config, layer_idx=layer_idx)\n         self.mlp = Gemma3MLP(config)\n         self.input_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.pre_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n-        self.is_sliding = self.self_attn.is_sliding\n-        self.sliding_window = config.sliding_window\n \n     @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n@@ -423,33 +399,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n-            # In prefill, we may be larger than sliding window\n-            effective_seq_len = max(cache_position.shape[0], self.sliding_window)\n-            # For FA2, the mask is 2D and is of shape [bs, processed_tokens] (not [bs, max_cache_len]),\n-            # thus we must slice from the right (at most `effective_seq_len` elements)\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                attention_mask = attention_mask[:, -effective_seq_len:]\n-            # Otherwise, the mask is 4D of shape [bs, 1, query_len, max_cache_len] thus we must slice\n-            # from the left, with an offset if we are beyond the sliding window\n-            else:\n-                min_dtype = torch.finfo(attention_mask.dtype).min\n-                sliding_window_mask = torch.tril(\n-                    torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n-                )\n-                attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n-                # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                offset = cache_position[-1] - effective_seq_len + 1\n-                # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = torch.clamp(offset, min=0)\n-                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n-                # but without data-dependent slicing (i.e. torch.compile friendly)\n-                mask_indexes = torch.arange(\n-                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n-                )\n-                mask_indexes += offset\n-                attention_mask = attention_mask[:, :, :, mask_indexes]\n-\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n@@ -568,7 +517,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridCache] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -595,13 +544,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            batch_size, seq_len, _ = inputs_embeds.shape\n-            past_key_values = HybridCache(\n-                self.config,\n-                max_batch_size=batch_size,\n-                max_cache_len=seq_len,\n-                dtype=inputs_embeds.dtype,\n-            )\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -614,13 +557,22 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            past_key_values,\n-            output_attentions,\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n \n         # embed positions\n         hidden_states = inputs_embeds\n@@ -643,7 +595,7 @@ def forward(\n                     hidden_states,\n                     position_embeddings_global,\n                     position_embeddings_local,\n-                    causal_mask,\n+                    causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids,\n                     past_key_values,\n                     output_attentions,\n@@ -655,7 +607,7 @@ def forward(\n                     hidden_states,\n                     position_embeddings_global=position_embeddings_global,\n                     position_embeddings_local=position_embeddings_local,\n-                    attention_mask=causal_mask,\n+                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n@@ -681,100 +633,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    @torch.no_grad()\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: HybridCache,\n-        output_attentions: bool = False,\n-    ):\n-        # Flash Attention currently doesn't support static cache but Gemma3Text work only with static cache.\n-        # So we will pass in attention mask as is in any case, not only when ther's padding. Then we'll use its shape\n-        # to cut out keys/values trailing 0 used in static cache. This workaround should be compile compatible\n-        # as it doesn't cause dynamic control issues.\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            return attention_mask\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        dtype, device = input_tensor.dtype, input_tensor.device\n-        sequence_length = input_tensor.shape[1]\n-        if isinstance(past_key_values, (HybridCache, StaticCache)):\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = attention_mask.shape[-1] if attention_mask is not None else input_tensor.shape[1]\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            device=device,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @auto_docstring\n class Gemma3ForCausalLM(Gemma3PreTrainedModel, GenerationMixin):\n@@ -818,7 +676,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridCache] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -895,60 +753,6 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    def prepare_inputs_for_generation(\n-        self,\n-        input_ids,\n-        past_key_values=None,\n-        attention_mask=None,\n-        inputs_embeds=None,\n-        cache_position=None,\n-        position_ids=None,\n-        use_cache=True,\n-        logits_to_keep=None,\n-        **kwargs,\n-    ):\n-        # Overwritten: has a special cache type, `HybridCache`\n-\n-        model_inputs = super().prepare_inputs_for_generation(\n-            input_ids,\n-            past_key_values=past_key_values,\n-            attention_mask=attention_mask,\n-            inputs_embeds=inputs_embeds,\n-            cache_position=cache_position,\n-            position_ids=position_ids,\n-            use_cache=use_cache,\n-            logits_to_keep=logits_to_keep,\n-            **kwargs,\n-        )\n-\n-        if logits_to_keep is None:\n-            _ = model_inputs.pop(\"logits_to_keep\", None)\n-\n-        if (\n-            isinstance(past_key_values, HybridCache)\n-            and attention_mask.ndim == 2\n-            and not self.config._attn_implementation == \"flash_attention_2\"\n-        ):\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = model_inputs[\"inputs_embeds\"].shape\n-                device = model_inputs[\"inputs_embeds\"].device\n-            else:\n-                batch_size, sequence_length = model_inputs[\"input_ids\"].shape\n-                device = model_inputs[\"input_ids\"].device\n-\n-            attention_mask = self.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-            )\n-            model_inputs[\"attention_mask\"] = attention_mask\n-\n-        return model_inputs\n-\n \n class Gemma3MultiModalProjector(nn.Module):\n     def __init__(self, config: Gemma3Config):\n@@ -986,6 +790,22 @@ def forward(self, vision_outputs: torch.Tensor):\n         return projected_vision_outputs.type_as(vision_outputs)\n \n \n+def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor]) -> Optional[Callable]:\n+    \"\"\"\n+    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n+    not start and end indices.\n+    \"\"\"\n+    # Do not return an additional mask in this case\n+    if token_type_ids is None:\n+        return None\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        # If it's 1, we need to unmask it\n+        return token_type_ids[batch_idx, kv_idx] == 1\n+\n+    return inner_mask\n+\n+\n @auto_docstring(\n     custom_intro=\"\"\"\n     The Base Gemma3 model which consists of a vision backbone and a language model withou language modeling head.,\n@@ -1012,86 +832,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.language_model.set_input_embeddings(value)\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask,\n-        token_type_ids,\n-        past_key_values,\n-        cache_position,\n-        input_tensor,\n-        is_training: bool = False,\n-    ):\n-        if self.config.text_config._attn_implementation == \"flash_attention_2\":\n-            return attention_mask\n-\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted\n-            # form and requires no inversion or slicing.\n-            return attention_mask\n-\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        min_dtype = torch.finfo(self.dtype).min\n-        inputs_lead_dim, sequence_length = input_tensor.shape[:2]\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        elif isinstance(past_key_values, HybridCache):\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else cache_position[0] + sequence_length + 1\n-            )\n-\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            return attention_mask\n-\n-        causal_mask = torch.full(\n-            (sequence_length, target_length), fill_value=min_dtype, dtype=self.dtype, device=cache_position.device\n-        )\n-\n-        # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n-        if sequence_length != 1:\n-            causal_mask = torch.triu(causal_mask, diagonal=1)\n-\n-        causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(inputs_lead_dim, 1, -1, -1)\n-\n-        # Apply bidirectional mask on images if token type ids are provided\n-        if token_type_ids is not None and sequence_length != 1:\n-            token_type_mask = token_type_ids.unsqueeze(1) == token_type_ids.unsqueeze(2)\n-            token_type_mask[token_type_ids == 0] = False  # if text token do not change anything\n-\n-            # Find where a new image block starts: 1 if image and previous not image\n-            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n-            is_image = token_type_ids == 1\n-            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n-            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n-\n-            same_image_mask = image_group_ids.unsqueeze(1) == image_group_ids.unsqueeze(2)\n-            same_image_mask[image_group_ids == -1] = False  # remove non-image\n-            image_mask = (token_type_mask & same_image_mask).unsqueeze(1).to(causal_mask.device, dtype=torch.bool)\n-\n-            causal_mask = causal_mask.clone()\n-            causal_mask[:, :, :, :sequence_length] = causal_mask[:, :, :, :sequence_length].masked_fill(\n-                image_mask, 0.0\n-            )\n-\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-\n-            # Then apply padding mask (will mask pad tokens)\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(causal_mask.device)\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-\n-        return causal_mask\n-\n     def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Projects the last hidden state from the vision model into language model space.\n@@ -1161,8 +901,6 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        is_training = token_type_ids is not None and labels is not None\n-\n         # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n@@ -1202,11 +940,31 @@ def forward(\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config.get_text_config(),\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n+                # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n+                mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n+                    token_type_ids.to(cache_position.device)\n+                )\n+\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n         outputs = self.language_model(\n-            attention_mask=causal_mask,\n+            attention_mask=causal_mask_mapping,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -1432,70 +1190,35 @@ def prepare_inputs_for_generation(\n         # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always\n         if cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n-        is_training = token_type_ids is not None and labels is not None\n-        if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n-            input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n-            causal_mask = self.model._update_causal_mask(\n-                attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training\n-            )\n-            model_inputs[\"attention_mask\"] = causal_mask\n \n         return model_inputs\n \n     @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n+    def create_masks_for_generate(\n+        config: PretrainedConfig,\n+        input_embeds: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor],\n         cache_position: torch.Tensor,\n-        batch_size: int,\n+        past_key_values: Optional[Cache],\n+        output_attentions: bool = False,\n+        token_type_ids: Optional[torch.Tensor] = None,\n         **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n+    ) -> dict:\n+        # Prepare mask arguments\n+        mask_kwargs = {\n+            \"config\": config.get_text_config(),\n+            \"input_embeds\": input_embeds,\n+            \"attention_mask\": attention_mask,\n+            \"cache_position\": cache_position,\n+            \"past_key_values\": past_key_values,\n+            \"output_attentions\": output_attentions,\n+        }\n+        # Add the token type ids mask for generate as well\n+        if token_type_ids is not None and input_embeds.shape[1] != 1:\n+            # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n+            mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(token_type_ids.to(cache_position.device))\n+\n+        return create_masks_for_generate(**mask_kwargs)\n \n \n __all__ = ["
        },
        {
            "sha": "f0761d863d117aa170061b7e6a8afa0c572b1733",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 163,
            "deletions": 169,
            "changes": 332,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -23,8 +23,9 @@\n import torch.nn as nn\n import torch.utils.checkpoint\n \n-from ...cache_utils import Cache, HybridCache, StaticCache\n-from ...configuration_utils import PretrainedConfig\n+from ...cache_utils import Cache, DynamicCache\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...masking_utils import create_causal_mask, create_masks_for_generate, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_rope_utils import rope_config_validation\n@@ -56,7 +57,7 @@\n logger = logging.get_logger(__name__)\n \n \n-class Gemma3TextConfig(Gemma2Config):\n+class Gemma3TextConfig(Gemma2Config, PretrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Gemma3TextModel`]. It is used to instantiate an Gemma3Text\n     model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n@@ -114,13 +115,14 @@ class Gemma3TextConfig(Gemma2Config):\n             The dropout ratio for the attention probabilities.\n         query_pre_attn_scalar (`float`, *optional*, defaults to 256):\n             Scaling factor used on the attention scores\n-        sliding_window (`int`, *optional*, defaults to 4096): in Gemma3Text, every other layer uses sliding window attention. This is the\n-            size of the sliding window.\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            In Gemma3Text, every other layer uses sliding window attention. This is the size of the sliding window.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n         final_logit_softcapping (`float`, *optional*):\n             Scaling factor when applying tanh softcapping on the logits.\n         attn_logit_softcapping (`float`, *optional*):\n             Scaling factor when applying tanh softcapping on the attention scores.\n-        cache_implementation (`str`, *optional*, defaults to `\"hybrid\"`): the cache type to be used with `generate`.\n         rope_scaling (`Dict`, *optional*):\n             Dictionary containing the scaling configuration for the RoPE embeddings used in global attention. NOTE: if you apply new rope type\n             and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n@@ -160,8 +162,6 @@ class Gemma3TextConfig(Gemma2Config):\n                     Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n         rope_local_base_freq (float, *optional*, defaults to 10000.0):\n             The base period of the RoPE embeddings for local attention.\n-        sliding_window_pattern (`int`, *optional*, defaults to 6):\n-            Pattern for the sliding window attention.\n \n     ```python\n     >>> from transformers import Gemma3TextModel, Gemma3TextConfig\n@@ -183,23 +183,74 @@ class Gemma3TextConfig(Gemma2Config):\n     def __init__(\n         self,\n         vocab_size=262_208,\n-        rope_theta=1_000_000.0,\n-        rope_scaling=None,\n-        rope_local_base_freq=10_000.0,\n-        sliding_window_pattern=6,\n+        hidden_size=2304,\n+        intermediate_size=9216,\n+        num_hidden_layers=26,\n+        num_attention_heads=8,\n+        num_key_value_heads=4,\n+        head_dim=256,\n+        hidden_activation=\"gelu_pytorch_tanh\",\n         max_position_embeddings=131_072,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        pad_token_id=0,\n+        eos_token_id=1,\n+        bos_token_id=2,\n+        tie_word_embeddings=True,\n+        rope_theta=1_000_000.0,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        query_pre_attn_scalar=256,\n+        sliding_window=4096,\n+        layer_types=None,\n         final_logit_softcapping=None,\n         attn_logit_softcapping=None,\n-        **super_kwargs,\n+        rope_scaling=None,\n+        rope_local_base_freq=10_000.0,\n+        **kwargs,\n     ):\n-        super().__init__(self, **super_kwargs)\n+        PretrainedConfig.__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.head_dim = head_dim\n+        self.num_key_value_heads = num_key_value_heads\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+        self.hidden_activation = hidden_activation\n+        self.query_pre_attn_scalar = query_pre_attn_scalar\n+        self.sliding_window = sliding_window\n+        self.final_logit_softcapping = final_logit_softcapping\n+        self.attn_logit_softcapping = attn_logit_softcapping\n+        self.layer_types = layer_types\n \n         self.rope_local_base_freq = rope_local_base_freq\n-        # For configuring HybridCache to work with 5:1 attention pattern\n-        self.sliding_window_pattern = sliding_window_pattern\n         self.rope_scaling = rope_scaling\n         rope_config_validation(self)\n \n+        if self.layer_types is None:\n+            # BC -> the pattern used to be a simple int, and it's still present in configs on the Hub\n+            sliding_window_pattern = getattr(self, \"sliding_window_pattern\", 6)\n+            self.layer_types = [\n+                \"sliding_attention\" if bool((i + 1) % sliding_window_pattern) else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n \n class Gemma3Config(PretrainedConfig):\n     r\"\"\"\n@@ -336,7 +387,7 @@ def __init__(self, config: Gemma3TextConfig, device=None):\n # Weird way to inherit but otherwise the sliding window gets defined first and can't access `is_sliding`\n class Gemma3Attention(Gemma2Attention):\n     def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n-        self.is_sliding = bool((layer_idx + 1) % config.sliding_window_pattern)\n+        self.is_sliding = config.layer_types[layer_idx] == \"sliding_attention\"\n \n         super().__init__()\n         self.sliding_window = config.sliding_window if self.is_sliding else None\n@@ -368,19 +419,9 @@ def forward(\n \n         if past_key_value is not None:\n             # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\n-                \"sin\": sin,\n-                \"cos\": cos,\n-                \"cache_position\": cache_position,\n-                \"sliding_window\": self.sliding_window,\n-            }\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-            # Here we need to slice as we use a static cache by default, but FA2 does not support it\n-            if attention_mask is not None and self.config._attn_implementation == \"flash_attention_2\":\n-                seq_len = attention_mask.shape[-1]\n-                key_states, value_states = key_states[:, :, :seq_len, :], value_states[:, :, :seq_len, :]\n-\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -391,9 +432,7 @@ def forward(\n                 )\n             else:\n                 attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n-        if attention_mask is not None:\n-            # backwards compatibility\n-            attention_mask = attention_mask.to(query_states)\n+\n         attn_output, attn_weights = attention_interface(\n             self,\n             query_states,\n@@ -417,14 +456,13 @@ def __init__(self, config: Gemma3TextConfig, layer_idx: int):\n         self.config = config\n         self.hidden_size = config.hidden_size\n         self.layer_idx = layer_idx\n+        self.attention_type = config.layer_types[layer_idx]\n         self.self_attn = Gemma3Attention(config=config, layer_idx=layer_idx)\n         self.mlp = Gemma3MLP(config)\n         self.input_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.pre_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n         self.post_feedforward_layernorm = Gemma3RMSNorm(self.hidden_size, eps=config.rms_norm_eps)\n-        self.is_sliding = self.self_attn.is_sliding\n-        self.sliding_window = config.sliding_window\n \n     @deprecate_kwarg(\"last_cache_position\", version=\"4.53.0\")\n     def forward(\n@@ -440,33 +478,6 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:\n-        if self.is_sliding and attention_mask is not None:  # efficient SDPA and no padding\n-            # In prefill, we may be larger than sliding window\n-            effective_seq_len = max(cache_position.shape[0], self.sliding_window)\n-            # For FA2, the mask is 2D and is of shape [bs, processed_tokens] (not [bs, max_cache_len]),\n-            # thus we must slice from the right (at most `effective_seq_len` elements)\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                attention_mask = attention_mask[:, -effective_seq_len:]\n-            # Otherwise, the mask is 4D of shape [bs, 1, query_len, max_cache_len] thus we must slice\n-            # from the left, with an offset if we are beyond the sliding window\n-            else:\n-                min_dtype = torch.finfo(attention_mask.dtype).min\n-                sliding_window_mask = torch.tril(\n-                    torch.ones_like(attention_mask, dtype=torch.bool), diagonal=-self.sliding_window\n-                )\n-                attention_mask = torch.where(sliding_window_mask, min_dtype, attention_mask)\n-                # In case we are beyond the sliding window, we need to correctly offset the mask slicing\n-                offset = cache_position[-1] - effective_seq_len + 1\n-                # Should only be used when beyond the sliding window (i.e. offset > 0)\n-                offset = torch.clamp(offset, min=0)\n-                # equivalent to: `attention_mask = attention_mask[:, :, :, offset : offset + effective_seq_len]`,\n-                # but without data-dependent slicing (i.e. torch.compile friendly)\n-                mask_indexes = torch.arange(\n-                    min(effective_seq_len, attention_mask.shape[-1]), device=attention_mask.device\n-                )\n-                mask_indexes += offset\n-                attention_mask = attention_mask[:, :, :, mask_indexes]\n-\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n@@ -557,7 +568,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[HybridCache] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -584,13 +595,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids)\n \n         if use_cache and past_key_values is None and not self.training:\n-            batch_size, seq_len, _ = inputs_embeds.shape\n-            past_key_values = HybridCache(\n-                self.config,\n-                max_batch_size=batch_size,\n-                max_cache_len=seq_len,\n-                dtype=inputs_embeds.dtype,\n-            )\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -603,13 +608,22 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask,\n-            inputs_embeds,\n-            cache_position,\n-            past_key_values,\n-            output_attentions,\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n \n         # embed positions\n         hidden_states = inputs_embeds\n@@ -632,7 +646,7 @@ def forward(\n                     hidden_states,\n                     position_embeddings_global,\n                     position_embeddings_local,\n-                    causal_mask,\n+                    causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids,\n                     past_key_values,\n                     output_attentions,\n@@ -644,7 +658,7 @@ def forward(\n                     hidden_states,\n                     position_embeddings_global=position_embeddings_global,\n                     position_embeddings_local=position_embeddings_local,\n-                    attention_mask=causal_mask,\n+                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n@@ -716,6 +730,22 @@ def forward(self, vision_outputs: torch.Tensor):\n         return projected_vision_outputs.type_as(vision_outputs)\n \n \n+def token_type_ids_mask_function(token_type_ids: Optional[torch.Tensor]) -> Optional[Callable]:\n+    \"\"\"\n+    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n+    not start and end indices.\n+    \"\"\"\n+    # Do not return an additional mask in this case\n+    if token_type_ids is None:\n+        return None\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        # If it's 1, we need to unmask it\n+        return token_type_ids[batch_idx, kv_idx] == 1\n+\n+    return inner_mask\n+\n+\n class Gemma3Model(PaliGemmaModel):\n     def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n@@ -731,85 +761,8 @@ def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         image_features = self.multi_modal_projector(vision_outputs)\n         return image_features\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask,\n-        token_type_ids,\n-        past_key_values,\n-        cache_position,\n-        input_tensor,\n-        is_training: bool = False,\n-    ):\n-        if self.config.text_config._attn_implementation == \"flash_attention_2\":\n-            return attention_mask\n-\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted\n-            # form and requires no inversion or slicing.\n-            return attention_mask\n-\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        min_dtype = torch.finfo(self.dtype).min\n-        inputs_lead_dim, sequence_length = input_tensor.shape[:2]\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        elif isinstance(past_key_values, HybridCache):\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else cache_position[0] + sequence_length + 1\n-            )\n-\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            return attention_mask\n-\n-        causal_mask = torch.full(\n-            (sequence_length, target_length), fill_value=min_dtype, dtype=self.dtype, device=cache_position.device\n-        )\n-\n-        # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n-        if sequence_length != 1:\n-            causal_mask = torch.triu(causal_mask, diagonal=1)\n-\n-        causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-        causal_mask = causal_mask[None, None, :, :].expand(inputs_lead_dim, 1, -1, -1)\n-\n-        # Apply bidirectional mask on images if token type ids are provided\n-        if token_type_ids is not None and sequence_length != 1:\n-            token_type_mask = token_type_ids.unsqueeze(1) == token_type_ids.unsqueeze(2)\n-            token_type_mask[token_type_ids == 0] = False  # if text token do not change anything\n-\n-            # Find where a new image block starts: 1 if image and previous not image\n-            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n-            is_image = token_type_ids == 1\n-            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n-            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n-            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n-\n-            same_image_mask = image_group_ids.unsqueeze(1) == image_group_ids.unsqueeze(2)\n-            same_image_mask[image_group_ids == -1] = False  # remove non-image\n-            image_mask = (token_type_mask & same_image_mask).unsqueeze(1).to(causal_mask.device, dtype=torch.bool)\n-\n-            causal_mask = causal_mask.clone()\n-            causal_mask[:, :, :, :sequence_length] = causal_mask[:, :, :, :sequence_length].masked_fill(\n-                image_mask, 0.0\n-            )\n-\n-        if attention_mask is not None:\n-            causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-            mask_length = attention_mask.shape[-1]\n-\n-            # Then apply padding mask (will mask pad tokens)\n-            padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(causal_mask.device)\n-            padding_mask = padding_mask == 0\n-            causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                padding_mask, min_dtype\n-            )\n-\n-        return causal_mask\n+    def _update_causal_mask(self, **super_kwargs):\n+        raise AttributeError(\"We don't want to inherit it\")\n \n     @can_return_tuple\n     @auto_docstring\n@@ -839,8 +792,6 @@ def forward(\n         )\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        is_training = token_type_ids is not None and labels is not None\n-\n         # Replace image id woth PAD if the image token if OOV, to avoid index-errors\n         if input_ids is not None and self.config.image_token_id >= self.vocab_size:\n             special_image_mask = input_ids == self.config.image_token_id\n@@ -880,11 +831,31 @@ def forward(\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, token_type_ids, past_key_values, cache_position, inputs_embeds, is_training\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config.get_text_config(),\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            if token_type_ids is not None and inputs_embeds.shape[1] != 1:\n+                # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n+                mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n+                    token_type_ids.to(cache_position.device)\n+                )\n+\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n         outputs = self.language_model(\n-            attention_mask=causal_mask,\n+            attention_mask=causal_mask_mapping,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n@@ -1066,16 +1037,39 @@ def prepare_inputs_for_generation(\n         # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always\n         if cache_position[0] == 0:\n             model_inputs[\"pixel_values\"] = pixel_values\n-        is_training = token_type_ids is not None and labels is not None\n-        if cache_position[0] == 0 and isinstance(past_key_values, HybridCache):\n-            input_tensor = inputs_embeds if inputs_embeds is not None else input_ids\n-            causal_mask = self.model._update_causal_mask(\n-                attention_mask, token_type_ids, past_key_values, cache_position, input_tensor, is_training\n-            )\n-            model_inputs[\"attention_mask\"] = causal_mask\n \n         return model_inputs\n \n+    def _prepare_4d_causal_attention_mask_with_cache_position(self, **super_kwargs):\n+        raise AttributeError(\"We don't want to inherit it\")\n+\n+    @staticmethod\n+    def create_masks_for_generate(\n+        config: PretrainedConfig,\n+        input_embeds: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor],\n+        cache_position: torch.Tensor,\n+        past_key_values: Optional[Cache],\n+        output_attentions: bool = False,\n+        token_type_ids: Optional[torch.Tensor] = None,\n+        **kwargs,\n+    ) -> dict:\n+        # Prepare mask arguments\n+        mask_kwargs = {\n+            \"config\": config.get_text_config(),\n+            \"input_embeds\": input_embeds,\n+            \"attention_mask\": attention_mask,\n+            \"cache_position\": cache_position,\n+            \"past_key_values\": past_key_values,\n+            \"output_attentions\": output_attentions,\n+        }\n+        # Add the token type ids mask for generate as well\n+        if token_type_ids is not None and input_embeds.shape[1] != 1:\n+            # We need to pass an additional mask function to account for token type ids, and it needs to be an `or`\n+            mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(token_type_ids.to(cache_position.device))\n+\n+        return create_masks_for_generate(**mask_kwargs)\n+\n \n __all__ = [\n     \"Gemma3Config\","
        },
        {
            "sha": "f3ac600e22b2297a8b72fac4047abd6150938e53",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -28,7 +28,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -40,16 +40,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_glm import GlmConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -443,8 +437,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -490,129 +489,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "4525ba15018788d6b593239a554cc08c5b2fc5c1",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -28,7 +28,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -40,16 +40,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_glm4 import Glm4Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -451,8 +445,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -498,129 +497,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @auto_docstring\n class Glm4ForCausalLM(Glm4PreTrainedModel, GenerationMixin):"
        },
        {
            "sha": "6da4405fad5d5de5df585d8cf59f138994f11f5b",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -893,60 +893,5 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\"GotOcr2PreTrainedModel\", \"GotOcr2Model\", \"GotOcr2ForConditionalGeneration\"]"
        },
        {
            "sha": "95de9e82d5ecf5eec0aac02444cf20574fee8868",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -682,7 +682,7 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -752,7 +752,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "9c32acdb06ace564d28ce8893a4dc814d8aaa112",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -12,7 +12,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -24,16 +24,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_gpt_neox import GPTNeoXConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -409,8 +403,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         # Prepare head mask if needed\n@@ -479,129 +478,6 @@ def forward(\n             attentions=all_attentions,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "70bee31b280bff01f535b1e4620dd83c4c8845eb",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -7,6 +7,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -349,8 +350,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         # Prepare head mask if needed"
        },
        {
            "sha": "f4a073cc4a7f132c281ef508f24211c02a1c1060",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -540,7 +540,7 @@ def forward(\n             attentions=all_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -610,7 +610,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "093daaef193fc6acad99ef7ce14f6c07f763f68a",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -793,7 +793,6 @@ def forward(\n             attentions=all_self_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -863,7 +862,6 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "fdba3f4c0ebd5eef5df1a745346440484a02c233",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -28,23 +28,17 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_granite import GraniteConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -446,8 +440,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -493,129 +492,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "424a0cc3fa25b8240ca8e321f7af4501e3e5a391",
            "filename": "src/transformers/models/granite/modular_granite.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodular_granite.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -20,6 +20,7 @@\n from torch import nn\n \n from ...cache_utils import Cache, DynamicCache\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...processing_utils import Unpack\n@@ -174,8 +175,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "fdd7addc450e44b52d4c180e24adc87e6830cf8d",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -773,7 +773,7 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -843,7 +843,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "5d58ca59458907d8dd98a940e5f1ca3ed1f01187",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -28,7 +28,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -40,16 +40,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_helium import HeliumConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -428,8 +422,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -475,129 +474,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "9aabc686795cc0d881b3b26afac67d11d0d3d92e",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1316,7 +1316,7 @@ def vblock(\n             image_hidden_states=image_hidden_states,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1386,7 +1386,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "26463c200910be6b5c0eda2cd6901015d4543659",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1035,61 +1035,6 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\n     \"InternVLVisionPreTrainedModel\","
        },
        {
            "sha": "788b2066b5dc54fc174917b366c76cc83219d704",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1014,7 +1014,7 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1084,7 +1084,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "1718c587d943bff73cba39c2b191bb0f838d24eb",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 10,
            "deletions": 135,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -26,7 +26,8 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -40,18 +41,10 @@\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_llama import LlamaConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-from ...integrations import use_kernel_forward_from_hub\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -433,8 +426,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -480,129 +478,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "42f9442d6783d6d12047d8806167650ca332031f",
            "filename": "src/transformers/models/llama4/configuration_llama4.py",
            "status": "modified",
            "additions": 11,
            "deletions": 4,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fconfiguration_llama4.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -15,7 +15,7 @@\n # limitations under the License.\n \n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...utils import logging\n \n \n@@ -233,12 +233,13 @@ class Llama4TextConfig(PretrainedConfig):\n             `no_rope_layer_interval` layers.\n         attention_chunk_size (`int`, *optional*, defaults to 8192):\n             <TODO>\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n         attn_temperature_tuning (`bool`, *optional*, defaults to `True`):\n             Whether to dynamically scale the attention temperature for each query token based on sequence length.\n             Recommended for long sequences (e.g., >32k tokens) to maintain stable output results.\n         floor_scale (`int`, *optional*, defaults to 8192): TODO\n         attn_scale (`int`, *optional*, defaults to 0.1): TODO\n-        cache_implementation (`<fill_type>`, *optional*, defaults to `\"hybrid\"`): <fill_docstring>\n \n     Example:\n     \"\"\"\n@@ -298,10 +299,10 @@ def __init__(\n         no_rope_layers=None,\n         no_rope_layer_interval=4,\n         attention_chunk_size=8192,\n+        layer_types=None,\n         attn_temperature_tuning=True,\n         floor_scale=8192,\n         attn_scale=0.1,\n-        cache_implementation=\"hybrid\",\n         **kwargs,\n     ):\n         super().__init__(\n@@ -323,7 +324,6 @@ def __init__(\n         self.num_attention_heads = num_attention_heads\n         self.rope_scaling = rope_scaling\n         self.attention_bias = False\n-        self.cache_implementation = cache_implementation\n         # for backward compatibility\n         if num_key_value_heads is None:\n             num_key_value_heads = num_attention_heads\n@@ -363,6 +363,13 @@ def __init__(\n         )\n         self.attention_chunk_size = attention_chunk_size\n \n+        self.layer_types = layer_types\n+        if layer_types is None:\n+            self.layer_types = [\n+                \"chunked_attention\" if no_rope else \"full_attention\" for no_rope in self.no_rope_layers\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n \n class Llama4Config(PretrainedConfig):\n     r\"\"\""
        },
        {
            "sha": "82caab17dba6707306acfdb9af31a161b0f5573e",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 24,
            "deletions": 292,
            "changes": 316,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -24,24 +24,19 @@\n from transformers.models.llama4.configuration_llama4 import Llama4VisionConfig\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, HybridChunkedCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations.hub_kernels import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask, create_chunked_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_llama4 import Llama4Config, Llama4TextConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -388,8 +383,9 @@ class Llama4TextDecoderLayer(nn.Module):\n     def __init__(self, config, layer_idx):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n+        self.layer_idx = layer_idx\n+        self.attention_type = config.layer_types[layer_idx]\n         self.self_attn = Llama4TextAttention(config, layer_idx)\n-        self.use_chunked_attention = config.attention_chunk_size is not None and bool(config.no_rope_layers[layer_idx])\n         self.is_moe_layer = layer_idx in config.moe_layers\n         if self.is_moe_layer:  # the 128E model interleaves dense / sparse\n             self.feed_forward = Llama4TextMoe(config)\n@@ -399,13 +395,10 @@ def __init__(self, config, layer_idx):\n         self.input_layernorm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Llama4TextRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n-        self.layer_idx = layer_idx\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n-        chunk_causal_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n         past_key_value: Optional[Tuple[torch.Tensor]] = None,\n         output_attentions: Optional[bool] = False,\n@@ -419,10 +412,6 @@ def forward(\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n-        # use local attention mask for ROPE layers\n-        if self.use_chunked_attention and chunk_causal_mask is not None:\n-            attention_mask = chunk_causal_mask\n-\n         # Self Attention\n         attention_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n@@ -561,10 +550,7 @@ def forward(\n             inputs_embeds = self.embed_tokens(input_ids.to(self.embed_tokens.weight.device))\n \n         if use_cache and past_key_values is None:\n-            if self.config.get_text_config().attention_chunk_size is not None:\n-                past_key_values = HybridChunkedCache(self.config, inputs_embeds.shape[0], inputs_embeds.shape[1])\n-            else:\n-                past_key_values = DynamicCache()\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -575,9 +561,22 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask, chunk_causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions, use_cache=use_cache\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"chunked_attention\": create_chunked_causal_mask(**mask_kwargs),\n+            }\n \n         hidden_states = inputs_embeds\n \n@@ -596,8 +595,7 @@ def forward(\n                 layer_outputs = self._gradient_checkpointing_func(\n                     decoder_layer.__call__,\n                     hidden_states,\n-                    causal_mask,\n-                    chunk_causal_mask,\n+                    causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids,\n                     past_key_values,\n                     output_attentions,\n@@ -609,8 +607,7 @@ def forward(\n             else:\n                 layer_outputs = decoder_layer(\n                     hidden_states,\n-                    attention_mask=causal_mask,\n-                    chunk_causal_mask=chunk_causal_mask,\n+                    attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                     position_ids=position_ids,\n                     past_key_value=past_key_values,\n                     output_attentions=output_attentions,\n@@ -638,216 +635,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    @torch.compiler.disable(recursive=False)  # the operations in this method are not compilable\n-    def _update_causal_mask(\n-        self,\n-        attention_mask: torch.Tensor,\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-        chunked_attention_mask=None,\n-        use_cache=True,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask, attention_mask  # flash does not support chunked attn TODO support flash\n-            return None, None\n-\n-        if self.config._attn_implementation not in [\"sdpa\", \"flex_attention\", \"eager\"]:\n-            return None, None\n-\n-        sequence_length = input_tensor.shape[1]\n-        cache_position = cache_position.to(self.device)\n-        attention_chunk_size = self.config.attention_chunk_size\n-        using_chunked_attention = attention_chunk_size is not None\n-\n-        first_cache_position = cache_position[0]\n-\n-        if past_key_values is not None:\n-            full_cache_length = past_key_values.get_max_cache_shape() or sequence_length\n-        else:\n-            full_cache_length = attention_mask.shape[-1] if attention_mask is not None else sequence_length\n-\n-        if using_chunked_attention:\n-            cond1 = first_cache_position >= attention_chunk_size\n-            cond2 = (first_cache_position < attention_chunk_size) & (\n-                first_cache_position + sequence_length > attention_chunk_size\n-            )\n-            key_length = (\n-                torch.where(\n-                    cond1,\n-                    attention_chunk_size + sequence_length - 1,\n-                    torch.where(cond2, first_cache_position + sequence_length, attention_chunk_size),\n-                )\n-                if use_cache\n-                else full_cache_length\n-            )\n-\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                if using_chunked_attention:\n-                    offsets = (first_cache_position, max(first_cache_position - attention_chunk_size + 1, 0))\n-                    chunked_attention_mask = make_flex_block_causal_mask(\n-                        attention_mask, attention_chunk_size, sequence_length, key_length, offsets=offsets\n-                    )\n-                attention_mask = make_flex_block_causal_mask(\n-                    attention_mask,\n-                    query_length=sequence_length,\n-                    key_length=full_cache_length,\n-                    offsets=(first_cache_position, 0),\n-                )\n-                return attention_mask, chunked_attention_mask\n-            if isinstance(attention_mask, BlockMask):\n-                return attention_mask, chunked_attention_mask\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        dtype, device = input_tensor.dtype, input_tensor.device\n-        target_length = max(full_cache_length, attention_chunk_size) if using_chunked_attention else full_cache_length\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-        if using_chunked_attention and full_cache_length > attention_chunk_size:\n-            start_idx = max(first_cache_position - attention_chunk_size + 1, 0)\n-            end_idx = start_idx + key_length\n-            chunked_attention_mask = self.create_chunked_attention_mask(\n-                self.config.attention_chunk_size,\n-                start=start_idx,  # same offset as with flex\n-                end=end_idx,\n-                device=device,\n-            )\n-\n-            local_attention_mask = attention_mask[:, start_idx:end_idx]  # offset here as well\n-            # It may be smaller than attention_chunk_size -> pad it\n-            requires_padding = local_attention_mask.shape[-1] < attention_chunk_size\n-            if requires_padding:\n-                local_attention_mask = nn.functional.pad(\n-                    local_attention_mask, (0, attention_chunk_size - local_attention_mask.shape[-1])\n-                )\n-            # Depending on the padding, take the query tokens from the end or the cache_position\n-            if not requires_padding:\n-                chunked_attention_mask = chunked_attention_mask[None, None, -sequence_length:, :]\n-            else:\n-                chunked_attention_mask = chunked_attention_mask[None, None, cache_position, :]\n-\n-            chunked_attention_mask = chunked_attention_mask.expand(input_tensor.shape[0], -1, -1, -1)\n-            chunked_attention_mask = chunked_attention_mask * local_attention_mask[:, None, None, :]\n-            if self.config._attn_implementation == \"eager\":\n-                min_dtype = torch.finfo(dtype).min\n-                chunked_attention_mask = torch.where(chunked_attention_mask == 0, min_dtype, 0.0).to(dtype)\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and attention_mask.ndim == 4\n-            and not output_attentions  # Only unmask for 4d masks\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and chunked_attention_mask is not None:\n-            chunked_attention_mask = chunked_attention_mask.bool()\n-            causal_mask = causal_mask != torch.finfo(dtype).min\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=first_cache_position,\n-                is_training=self.training,\n-            ):\n-                causal_mask = None\n-        return causal_mask, chunked_attention_mask\n-\n-    def create_chunked_attention_mask(\n-        self, attention_chunk_size: int, start: int, end: int, device: torch.device\n-    ) -> torch.Tensor:\n-        \"\"\"\n-        Generate the following:\n-\n-        'What'      :  0 â–  â¬š â¬š â¬š â¬š â¬š    |\n-        'â–is'       :  1 â–  â–  â¬š â¬š â¬š â¬š     |\n-        'â–ch'       :  2 â–  â–  â–  â¬š â¬š â¬š     |\n-        'unked'     :  3 â¬š â¬š â¬š â–  â¬š â¬š    |\n-        'â–attention':  4 â¬š â¬š â¬š â–  â–  â¬š    |\n-        '?'         :  5 â¬š â¬š â¬š â–  â–  â–      |\n-\n-        If the chunk size is 3.\n-        This can just be applied over the already created attention mask\n-        \"\"\"\n-        arange_vector = torch.arange(start, end, device=device)\n-        block_pos = torch.abs(\n-            arange_vector.unsqueeze(0) // attention_chunk_size - arange_vector.unsqueeze(1) // attention_chunk_size\n-        )\n-        token_pos = arange_vector.unsqueeze(0) - arange_vector.unsqueeze(1)\n-        mask = (block_pos == 0) & (token_pos <= 0)\n-        return mask.to(device)\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            device (`torch.device`):\n-                The device to place the 4D attention mask on.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    cache_position.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n@@ -1726,61 +1513,6 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\n     \"Llama4PreTrainedModel\","
        },
        {
            "sha": "448879ec06f7b69580833119176faf1b5f2a18de",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 56,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -503,61 +503,5 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\"LlavaForConditionalGeneration\", \"LlavaPreTrainedModel\", \"LlavaModel\"]"
        },
        {
            "sha": "496049e31231f01001dbc7d01f0a9ef9cfa79aca",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -719,7 +719,7 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "62e696e8111dd8bbae603f623ed16f6baf1b3e0d",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1589,7 +1589,7 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1659,7 +1659,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "757a393a0bc021a32ab102aa085105feac90af95",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -790,7 +790,7 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -860,7 +860,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "a9a3fd353ecfb8f41c064cdf3cf0a686f90ba274",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -484,7 +484,7 @@ def _init_weights(self, module: Union[nn.Linear, nn.Embedding, MarianSinusoidalP\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -554,7 +554,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "3fbb3e8b5be45489398c096a567f3e765ca30885",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -762,7 +762,7 @@ def dummy_inputs(self):\n         }\n         return dummy_inputs\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -832,7 +832,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "4d7b92979ab1a0236cf830cfef99aa07e34fc85e",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1060,7 +1060,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Mimi\n+    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._update_causal_mask with Phimoe->Mimi\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1148,7 +1148,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Mimi\n+    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._prepare_4d_causal_attention_mask_with_cache_position with Phimoe->Mimi\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "92ef09fc7390dbd6acd599f574ce1aae29de6abc",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 11,
            "deletions": 166,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -10,10 +10,10 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -26,16 +26,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_mistral import MistralConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -403,8 +397,14 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -450,161 +450,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: MistralConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`MistralConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "9dd2e051b56d6c03b5c6eac88054a6a39fcba051",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 95,
            "deletions": 160,
            "changes": 255,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -4,13 +4,13 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ...cache_utils import Cache, SlidingWindowCache, StaticCache\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...cache_utils import Cache, DynamicCache\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, QuestionAnsweringModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import is_torch_flex_attn_available, logging\n+from ...utils import auto_docstring, can_return_tuple, logging\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -27,12 +27,6 @@\n from .configuration_mistral import MistralConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n _CHECKPOINT_FOR_DOC = \"mistralai/Mistral-7B-v0.1\"\n@@ -118,166 +112,107 @@ class MistralPreTrainedModel(LlamaPreTrainedModel):\n \n \n class MistralModel(LlamaModel):\n-    def __init__(self, config: MistralConfig):\n-        super().__init__(config)\n-        self.layers = nn.ModuleList(\n-            [MistralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Mistral. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n             )\n+            use_cache = False\n \n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n             config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n             past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: MistralConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`MistralConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask,\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n             )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n \n \n class MistralForCausalLM(LlamaForCausalLM):"
        },
        {
            "sha": "625e1c3185ed8d943e4723223e91615bb04b20ba",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 61,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -32,12 +32,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import (\n-    LossKwargs,\n-    auto_docstring,\n-    can_return_tuple,\n-    is_torchdynamo_compiling,\n-)\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling\n from ..auto import AutoModel\n from .configuration_mistral3 import Mistral3Config\n \n@@ -538,60 +533,5 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\"Mistral3Model\", \"Mistral3PreTrainedModel\", \"Mistral3ForConditionalGeneration\"]"
        },
        {
            "sha": "9f176a35d8985447f5311f5f8358549b62012a51",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 11,
            "deletions": 166,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -32,10 +32,10 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -48,16 +48,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_mixtral import MixtralConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -524,8 +518,14 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -591,161 +591,6 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Mixtral. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: MixtralConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`MixtralConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "95d8defdddeb9e00fae4b247f9445ecc95b5c020",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 9,
            "deletions": 8,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -29,6 +29,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import DynamicCache\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...processing_utils import Unpack\n@@ -315,12 +316,6 @@ class MixtralPreTrainedModel(MistralPreTrainedModel):\n \n \n class MixtralModel(MistralModel):\n-    def __init__(self, config: MixtralConfig):\n-        super().__init__(config)\n-        self.layers = nn.ModuleList(\n-            [MixtralDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n-        )\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -368,8 +363,14 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "fcccd2b9ea63a0cf84ac1a9d1b63338b5c0479b4",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -893,7 +893,7 @@ def _init_weights(self, module):\n             if module.is_gated:\n                 module.gate.data.zero_()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -963,7 +963,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "edc70eafa0827eae83ae7f994a5dec7794223a56",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 10,
            "deletions": 137,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -27,11 +27,8 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import (\n-    AttentionMaskConverter,\n-    _prepare_4d_attention_mask,\n-    _prepare_4d_attention_mask_for_sdpa,\n-)\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -44,16 +41,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import auto_docstring, can_return_tuple, logging\n from .configuration_moonshine import MoonshineConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -752,8 +743,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -826,129 +822,6 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n def _compute_mask_indices(\n     shape: Tuple[int, int],"
        },
        {
            "sha": "c22198843c48c440de2ccb5c204d249c2cc869b9",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -21,6 +21,7 @@\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n+from ...masking_utils import create_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask, _prepare_4d_attention_mask_for_sdpa\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -748,8 +749,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "7e71eb2ce20055a6fdd342cd2c5db6b636980571",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1084,7 +1084,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Moshi\n+    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._update_causal_mask with Phimoe->Moshi\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1172,7 +1172,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->MoshiDepth\n+    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._prepare_4d_causal_attention_mask_with_cache_position with Phimoe->MoshiDepth\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1391,7 +1391,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Moshi\n+    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._update_causal_mask with Phimoe->Moshi\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1479,7 +1479,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Moshi\n+    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._prepare_4d_causal_attention_mask_with_cache_position with Phimoe->Moshi\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "6b488b66d22e68f478e019c4503719abdef9129a",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1180,7 +1180,7 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1250,7 +1250,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "ea0f0de04569ff947f7b5aec930d856b2ef3c8e8",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -739,7 +739,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -809,7 +809,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "fe4e081a3e419b3fed480228c9770f1ab7fc806c",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -13,23 +13,17 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_olmo import OlmoConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -406,8 +400,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -453,129 +452,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "dd2fcdb17f54a48673916a6e1c6181801cf2d8d6",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -13,23 +13,17 @@\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_olmo2 import Olmo2Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -412,8 +406,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -459,129 +458,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "480687ae7f03e2235f414349abc8a26201a87f5f",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -384,7 +384,7 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embed_tokens = value\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -454,7 +454,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "15180b91b960b8755ce07e7efbcbc9afaebca288",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -566,7 +566,7 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "3f59a8c9186b52da2d3aece8c5a1f43f28ef8c71",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -481,7 +481,7 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -551,7 +551,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "04cf37a7622ec6a67b0ad286785fa10eabfbc783",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -773,7 +773,7 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -843,7 +843,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "3f8983083e2fe781715f0e28cf06193977526299",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -539,7 +539,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -609,7 +609,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "4dade5e49e2061a2cc6abc2c0cd5afdef69d9ffe",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 9,
            "deletions": 133,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -13,7 +13,7 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -24,16 +24,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_phi import PhiConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -400,8 +394,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         inputs_embeds = self.embed_dropout(inputs_embeds)  # diff with Llama\n@@ -461,129 +460,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and (attention_mask == 0.0).any():\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_compilable_cache = past_key_values.is_compileable if past_key_values is not None else False\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_compilable_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_compilable_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "6f9edaba9415513525431cfd0a4830c047ac28c4",
            "filename": "src/transformers/models/phi/modular_phi.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodular_phi.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -5,6 +5,7 @@\n import torch.nn as nn\n \n from ...cache_utils import Cache, DynamicCache\n+from ...masking_utils import create_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -244,8 +245,13 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         inputs_embeds = self.embed_dropout(inputs_embeds)  # diff with Llama"
        },
        {
            "sha": "52cf0ef96d3c533cd5242843a10dded0e01edacd",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 166,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -26,10 +26,10 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -41,16 +41,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_phi3 import Phi3Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -458,8 +452,14 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -505,161 +505,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Phi3. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: Phi3Config,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`Phi3Config`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "6b6cef7df3e509a7d9cff492a1a78edbd69b6888",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 12,
            "deletions": 168,
            "changes": 180,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -28,13 +28,12 @@\n from torch import nn\n from torch.nn.init import _calculate_fan_in_and_fan_out\n \n-from transformers.modeling_attn_mask_utils import _prepare_4d_attention_mask\n-\n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -46,16 +45,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging, torch_int\n+from ...utils import auto_docstring, can_return_tuple, logging, torch_int\n from .configuration_phi4_multimodal import Phi4MultimodalAudioConfig, Phi4MultimodalConfig, Phi4MultimodalVisionConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -1766,8 +1759,14 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -1813,161 +1812,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Phi4Multimodal. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: Phi4MultimodalConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`Phi4MultimodalConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n-\n \n @auto_docstring\n class Phi4MultimodalForCausalLM(Phi4MultimodalPreTrainedModel, GenerationMixin):"
        },
        {
            "sha": "344e6c1776eb5f3c88bea188cde9480c3d5cc324",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 10,
            "deletions": 4,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -21,11 +21,11 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from transformers.modeling_attn_mask_utils import _prepare_4d_attention_mask\n-\n from ...activations import ACT2FN\n from ...cache_utils import DynamicCache\n from ...configuration_utils import PretrainedConfig\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_outputs import (\n     BaseModelOutput,\n     BaseModelOutputWithPast,\n@@ -1570,8 +1570,14 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "e81d38e2d88dada39a6d4b3a036affb1bb20fd96",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1068,7 +1068,6 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Phimoe\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1156,7 +1155,6 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Phimoe\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "f9a5b00218df9f956c2ba3926f04862a1270c9f7",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1345,7 +1345,7 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1415,7 +1415,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "614baee8bf32adfce4fa9412488664b641826e98",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -520,7 +520,7 @@ def _init_weights(self, module):\n             module.weight.data.fill_(1.0)\n             module.bias.data.zero_()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -590,7 +590,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "c63b4df774b7043dffee16ea3cd5ac3d33dc0643",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -900,7 +900,7 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -970,7 +970,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "f89be07bf13f805ebf8d151fc17677125db72d44",
            "filename": "src/transformers/models/qwen2/configuration_qwen2.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fconfiguration_qwen2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Qwen2 model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n@@ -110,6 +110,8 @@ class Qwen2Config(PretrainedConfig):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 28):\n             The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n \n@@ -164,6 +166,7 @@ def __init__(\n         use_sliding_window=False,\n         sliding_window=4096,\n         max_window_layers=28,\n+        layer_types=None,\n         attention_dropout=0.0,\n         **kwargs,\n     ):\n@@ -174,7 +177,7 @@ def __init__(\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window  # we check `use_sliding_window` in the modeling code\n+        self.sliding_window = sliding_window if self.use_sliding_window else None\n         self.max_window_layers = max_window_layers\n \n         # for backward compatibility\n@@ -195,6 +198,16 @@ def __init__(\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n \n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\"\n+                if self.sliding_window is not None and i >= self.max_window_layers\n+                else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,"
        },
        {
            "sha": "9e9b0641f0d854462607d3fee96eb317c982ca7c",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 28,
            "deletions": 182,
            "changes": 210,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -10,10 +10,10 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -26,16 +26,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_qwen2 import Qwen2Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -143,6 +137,7 @@ def __init__(self, config: Qwen2Config, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n     def forward(\n         self,\n@@ -168,14 +163,6 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        sliding_window = None\n-        if (\n-            self.config.use_sliding_window\n-            and getattr(self.config, \"sliding_window\", None) is not None\n-            and self.layer_idx >= self.config.max_window_layers\n-        ):\n-            sliding_window = self.config.sliding_window\n-\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -194,7 +181,7 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            sliding_window=sliding_window,  # main diff with Llama\n+            sliding_window=self.sliding_window,  # main diff with Llama\n             **kwargs,\n         )\n \n@@ -228,15 +215,13 @@ class Qwen2DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n+\n         self.self_attn = Qwen2Attention(config=config, layer_idx=layer_idx)\n+\n         self.mlp = Qwen2MLP(config)\n         self.input_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        if config.use_sliding_window and config._attn_implementation != \"flash_attention_2\":\n-            logger.warning_once(\n-                f\"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; \"\n-                \"unexpected results may be encountered.\"\n-            )\n+        self.attention_type = config.layer_types[layer_idx]\n \n     def forward(\n         self,\n@@ -357,6 +342,7 @@ def __init__(self, config: Qwen2Config):\n         self.norm = Qwen2RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = Qwen2RotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n+        self.has_sliding_layers = \"sliding_attention\" in self.config.layer_types\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -416,9 +402,24 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+            }\n+            # The sliding window alternating layers are not always activated depending on the config\n+            if self.has_sliding_layers:\n+                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n \n@@ -435,7 +436,7 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n@@ -463,161 +464,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen2. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: Qwen2Config,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`Qwen2Config`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "5a24b425f0c7e071ced5c43155ab75ba94cefeab",
            "filename": "src/transformers/models/qwen2/modular_qwen2.py",
            "status": "modified",
            "additions": 123,
            "deletions": 19,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodular_qwen2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -4,11 +4,15 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ...cache_utils import Cache\n+from ...cache_utils import Cache, DynamicCache\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import (\n+    BaseModelOutputWithPast,\n+)\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n-from ...utils import logging\n+from ...utils import auto_docstring, can_return_tuple, logging\n from ..llama.modeling_llama import (\n     LlamaAttention,\n     LlamaDecoderLayer,\n@@ -43,6 +47,7 @@ def __init__(self, config: Qwen2Config, layer_idx: int):\n         self.k_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.v_proj = nn.Linear(config.hidden_size, config.num_key_value_heads * self.head_dim, bias=True)\n         self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n     def forward(\n         self,\n@@ -68,14 +73,6 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        sliding_window = None\n-        if (\n-            self.config.use_sliding_window\n-            and getattr(self.config, \"sliding_window\", None) is not None\n-            and self.layer_idx >= self.config.max_window_layers\n-        ):\n-            sliding_window = self.config.sliding_window\n-\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n             if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n@@ -94,7 +91,7 @@ def forward(\n             attention_mask,\n             dropout=0.0 if not self.training else self.attention_dropout,\n             scaling=self.scaling,\n-            sliding_window=sliding_window,  # main diff with Llama\n+            sliding_window=self.sliding_window,  # main diff with Llama\n             **kwargs,\n         )\n \n@@ -106,21 +103,128 @@ def forward(\n class Qwen2DecoderLayer(LlamaDecoderLayer):\n     def __init__(self, config: Qwen2Config, layer_idx: int):\n         super().__init__()\n-        self.self_attn = Qwen2Attention(config=config, layer_idx=layer_idx)\n-        self.mlp = Qwen2MLP(config)\n-        if config.use_sliding_window and config._attn_implementation != \"flash_attention_2\":\n-            logger.warning_once(\n-                f\"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; \"\n-                \"unexpected results may be encountered.\"\n-            )\n+        self.attention_type = config.layer_types[layer_idx]\n \n \n class Qwen2PreTrainedModel(LlamaPreTrainedModel):\n     pass\n \n \n class Qwen2Model(MistralModel):\n-    pass\n+    def __init__(self, config: Qwen2Config):\n+        super().__init__(config)\n+        self.has_sliding_layers = \"sliding_attention\" in self.config.layer_types\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        use_cache = use_cache if use_cache is not None else self.config.use_cache\n+\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n+\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+            }\n+            # The sliding window alternating layers are not always activated depending on the config\n+            if self.has_sliding_layers:\n+                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n+\n+        hidden_states = inputs_embeds\n+\n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n+        # decoder layers\n+        all_hidden_states = () if output_hidden_states else None\n+        all_self_attns = () if output_attentions else None\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            if output_hidden_states:\n+                all_hidden_states += (hidden_states,)\n+\n+            layer_outputs = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n+                position_ids=position_ids,\n+                past_key_value=past_key_values,\n+                output_attentions=output_attentions,\n+                use_cache=use_cache,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings,\n+                **flash_attn_kwargs,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_self_attns += (layer_outputs[1],)\n+\n+        hidden_states = self.norm(hidden_states)\n+\n+        # add hidden states from the last decoder layer\n+        if output_hidden_states:\n+            all_hidden_states += (hidden_states,)\n+\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values if use_cache else None,\n+            hidden_states=all_hidden_states,\n+            attentions=all_self_attns,\n+        )\n \n \n class Qwen2ForCausalLM(LlamaForCausalLM):"
        },
        {
            "sha": "06db2e83f1ff12056a1949e6369b8242484c6683",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -372,7 +372,7 @@ def __init__(\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window  # we check `use_sliding_window` in the modeling code\n+        self.sliding_window = sliding_window if self.use_sliding_window else None\n         self.max_window_layers = max_window_layers\n \n         # for backward compatibility\n@@ -392,6 +392,7 @@ def __init__(\n         if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n+\n         if self.rope_scaling is None:\n             self.rope_scaling = {\"mrope_section\": [16, 24, 24], \"rope_type\": \"default\", \"type\": \"default\"}\n "
        },
        {
            "sha": "84e553465337323c0e388eb34cc78494b1abc141",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -68,16 +68,15 @@\n     apply_rotary_emb = None\n \n \n+if is_flash_attn_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward\n+\n if is_torch_flex_attn_available():\n     from torch.nn.attention.flex_attention import BlockMask\n \n     from ...integrations.flex_attention import make_flex_block_causal_mask\n \n \n-if is_flash_attn_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "a75c0fafa406a845c589c1049535af7e177faa7e",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 62,
            "deletions": 4,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -27,7 +27,6 @@\n from torch.nn import Parameter\n \n from transformers.models.llama.modeling_llama import rotate_half\n-from transformers.models.qwen2.configuration_qwen2 import Qwen2Config\n from transformers.models.qwen2_5_vl.configuration_qwen2_5_vl import Qwen2_5_VLVisionConfig\n from transformers.models.qwen2_5_vl.modeling_qwen2_5_vl import (\n     Qwen2_5_VisionTransformerPretrainedModel,\n@@ -45,6 +44,7 @@\n from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n+from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...utils import (\n     auto_docstring,\n@@ -251,7 +251,7 @@ def __init__(\n         del self.encoder_layerdrop\n \n \n-class Qwen2_5OmniTextConfig(Qwen2Config):\n+class Qwen2_5OmniTextConfig(PretrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Qwen2_5OmniThinkerForConditionalGeneration`]. It is used to instantiate an\n     Qwen2.5-Omni-Thinker model according to the specified arguments, defining the model architecture. Instantiating a configuration\n@@ -362,6 +362,23 @@ class Qwen2_5OmniTextConfig(Qwen2Config):\n     ```\"\"\"\n \n     model_type = \"qwen2_5_omni_text\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+\n+    # Default tensor parallel plan for base model `Qwen25OmniText`\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise\",\n+        \"layers.*.self_attn.k_proj\": \"colwise\",\n+        \"layers.*.self_attn.v_proj\": \"colwise\",\n+        \"layers.*.self_attn.o_proj\": \"rowwise\",\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n \n     def __init__(\n         self,\n@@ -371,11 +388,52 @@ def __init__(\n         num_hidden_layers=28,\n         num_attention_heads=28,\n         num_key_value_heads=4,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=32768,\n+        initializer_range=0.02,\n+        rms_norm_eps=1e-6,\n+        use_cache=True,\n+        tie_word_embeddings=False,\n         rope_theta=1000000.0,\n+        rope_scaling=None,\n+        use_sliding_window=False,\n         sliding_window=32768,\n-        **super_kwargs,\n+        max_window_layers=28,\n+        attention_dropout=0.0,\n+        **kwargs,\n     ):\n-        super().__init__(**super_kwargs)\n+        super().__init__(\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.use_sliding_window = use_sliding_window\n+        self.sliding_window = sliding_window if self.use_sliding_window else None\n+        self.max_window_layers = max_window_layers\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.rms_norm_eps = rms_norm_eps\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self.attention_dropout = attention_dropout\n+        # Validate the correctness of rotary position embeddings parameters\n+        # BC: if there is a 'type' field, move it to 'rope_type'.\n+        if self.rope_scaling is not None and \"type\" in self.rope_scaling:\n+            self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n+        rope_config_validation(self)\n+\n         if self.rope_scaling is None:\n             self.rope_scaling = {\"mrope_section\": [16, 24, 24], \"rope_type\": \"default\", \"type\": \"default\"}\n "
        },
        {
            "sha": "a529dcdb559c89251e03023d9be1747052968666",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -915,7 +915,7 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Qwen2Moe\n+    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._update_causal_mask with Phimoe->Qwen2Moe\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1003,7 +1003,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Qwen2Moe\n+    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._prepare_4d_causal_attention_mask_with_cache_position with Phimoe->Qwen2Moe\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "faa0f7aabb7e3e1759f558e32a62f9a3fe7a3b90",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1182,7 +1182,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Qwen2VL\n+    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._update_causal_mask with Phimoe->Qwen2VL\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1270,7 +1270,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Qwen2VL\n+    # Copied from transformers.models.phimoe.modeling_phimoe.PhimoeModel._prepare_4d_causal_attention_mask_with_cache_position with Phimoe->Qwen2VL\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1675,7 +1675,7 @@ def forward(\n         return output if return_dict else output.to_tuple()\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "8335c798d1612eed42c4b64a53278c59d8847945",
            "filename": "src/transformers/models/qwen3/configuration_qwen3.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fconfiguration_qwen3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -14,7 +14,7 @@\n # limitations under the License.\n \"\"\"Qwen3 model configuration\"\"\"\n \n-from ...configuration_utils import PretrainedConfig\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging\n \n@@ -114,6 +114,8 @@ class Qwen3Config(PretrainedConfig):\n             Sliding window attention (SWA) window size. If not specified, will default to `4096`.\n         max_window_layers (`int`, *optional*, defaults to 28):\n             The number of layers that use SWA (Sliding Window Attention). The bottom layers use SWA while the top use full attention.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer.\n         attention_dropout (`float`, *optional*, defaults to 0.0):\n             The dropout ratio for the attention probabilities.\n \n@@ -170,6 +172,7 @@ def __init__(\n         use_sliding_window=False,\n         sliding_window=4096,\n         max_window_layers=28,\n+        layer_types=None,\n         attention_dropout=0.0,\n         **kwargs,\n     ):\n@@ -180,7 +183,7 @@ def __init__(\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n         self.use_sliding_window = use_sliding_window\n-        self.sliding_window = sliding_window  # we check `use_sliding_window` in the modeling code\n+        self.sliding_window = sliding_window if self.use_sliding_window else None\n         self.max_window_layers = max_window_layers\n \n         # for backward compatibility\n@@ -203,6 +206,16 @@ def __init__(\n             self.rope_scaling[\"rope_type\"] = self.rope_scaling[\"type\"]\n         rope_config_validation(self)\n \n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\"\n+                if self.sliding_window is not None and i >= self.max_window_layers\n+                else \"full_attention\"\n+                for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n         super().__init__(\n             tie_word_embeddings=tie_word_embeddings,\n             **kwargs,"
        },
        {
            "sha": "48eb9489be2800175871f8d2632d19b4e8a511bb",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 27,
            "deletions": 182,
            "changes": 209,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -25,10 +25,10 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -41,16 +41,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_qwen3 import Qwen3Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -190,13 +184,7 @@ def __init__(self, config: Qwen3Config, layer_idx: int):\n         )\n         self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n-        self.sliding_window = config.sliding_window\n-        if not (\n-            self.config.use_sliding_window\n-            and getattr(self.config, \"sliding_window\", None) is not None\n-            and self.layer_idx >= self.config.max_window_layers\n-        ):\n-            self.sliding_window = None\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n     def forward(\n         self,\n@@ -253,17 +241,13 @@ class Qwen3DecoderLayer(GradientCheckpointingLayer):\n     def __init__(self, config: Qwen3Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n+\n         self.self_attn = Qwen3Attention(config=config, layer_idx=layer_idx)\n+\n         self.mlp = Qwen3MLP(config)\n         self.input_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_attention_layernorm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-        if (\n-            config.sliding_window and config._attn_implementation != \"flash_attention_2\"\n-        ):  # diff with Llama is this warning\n-            logger.warning_once(\n-                f\"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; \"\n-                \"unexpected results may be encountered.\"\n-            )\n+        self.attention_type = config.layer_types[layer_idx]\n \n     def forward(\n         self,\n@@ -384,6 +368,7 @@ def __init__(self, config: Qwen3Config):\n         self.norm = Qwen3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = Qwen3RotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n+        self.has_sliding_layers = \"sliding_attention\" in self.config.layer_types\n \n         # Initialize weights and apply final processing\n         self.post_init()\n@@ -443,9 +428,24 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n-        )\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"output_attentions\": output_attentions,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+            }\n+            # The sliding window alternating layers are not always activated depending on the config\n+            if self.has_sliding_layers:\n+                causal_mask_mapping[\"sliding_attention\"] = create_sliding_window_causal_mask(**mask_kwargs)\n \n         hidden_states = inputs_embeds\n \n@@ -462,7 +462,7 @@ def forward(\n \n             layer_outputs = decoder_layer(\n                 hidden_states,\n-                attention_mask=causal_mask,\n+                attention_mask=causal_mask_mapping[decoder_layer.attention_type],\n                 position_ids=position_ids,\n                 past_key_value=past_key_values,\n                 output_attentions=output_attentions,\n@@ -490,161 +490,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen3. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: Qwen3Config,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`Qwen3Config`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "096a0e5b9c64c32d4ecca35488aecc8352ff7b2a",
            "filename": "src/transformers/models/qwen3/modular_qwen3.py",
            "status": "modified",
            "additions": 20,
            "deletions": 34,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodular_qwen3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -28,16 +28,18 @@\n from ..gemma.modeling_gemma import GemmaMLP\n from ..llama.modeling_llama import (\n     LlamaAttention,\n-    LlamaDecoderLayer,\n-    LlamaForCausalLM,\n-    LlamaForQuestionAnswering,\n-    LlamaForSequenceClassification,\n-    LlamaForTokenClassification,\n-    LlamaRMSNorm,\n+)\n+from ..qwen2.modeling_qwen2 import (\n+    Qwen2DecoderLayer,\n+    Qwen2ForCausalLM,\n+    Qwen2ForQuestionAnswering,\n+    Qwen2ForSequenceClassification,\n+    Qwen2ForTokenClassification,\n+    Qwen2Model,\n+    Qwen2RMSNorm,\n     apply_rotary_pos_emb,\n     eager_attention_forward,\n )\n-from ..mistral.modeling_mistral import MistralModel\n from .configuration_qwen3 import Qwen3Config\n \n \n@@ -46,7 +48,7 @@\n _CHECKPOINT_FOR_DOC = \"Qwen/Qwen3-8B\"\n \n \n-class Qwen3RMSNorm(LlamaRMSNorm):\n+class Qwen3RMSNorm(Qwen2RMSNorm):\n     pass\n \n \n@@ -59,13 +61,7 @@ def __init__(self, config: Qwen3Config, layer_idx: int):\n         super().__init__(config, layer_idx)\n         self.q_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3RMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n-        self.sliding_window = config.sliding_window\n-        if not (\n-            self.config.use_sliding_window\n-            and getattr(self.config, \"sliding_window\", None) is not None\n-            and self.layer_idx >= self.config.max_window_layers\n-        ):\n-            self.sliding_window = None\n+        self.sliding_window = config.sliding_window if config.layer_types[layer_idx] == \"sliding_attention\" else None\n \n     def forward(\n         self,\n@@ -118,28 +114,18 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class Qwen3DecoderLayer(LlamaDecoderLayer):\n-    def __init__(self, config: Qwen3Config, layer_idx: int):\n-        super().__init__()\n-        self.self_attn = Qwen3Attention(config=config, layer_idx=layer_idx)\n-        self.mlp = Qwen3MLP(config)\n-        if (\n-            config.sliding_window and config._attn_implementation != \"flash_attention_2\"\n-        ):  # diff with Llama is this warning\n-            logger.warning_once(\n-                f\"Sliding Window Attention is enabled but not implemented for `{config._attn_implementation}`; \"\n-                \"unexpected results may be encountered.\"\n-            )\n-\n-\n-class Qwen3Model(MistralModel):  # mistral model creates sliding window\n+class Qwen3DecoderLayer(Qwen2DecoderLayer):\n+    pass\n+\n+\n+class Qwen3Model(Qwen2Model):\n     pass\n \n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n \n \n-class Qwen3ForCausalLM(LlamaForCausalLM):\n+class Qwen3ForCausalLM(Qwen2ForCausalLM):\n     def forward(\n         self,\n         **super_kwargs: Unpack[KwargsForCausalLM],\n@@ -169,15 +155,15 @@ def forward(\n         return super().forward(**super_kwargs)\n \n \n-class Qwen3ForSequenceClassification(LlamaForSequenceClassification):\n+class Qwen3ForSequenceClassification(Qwen2ForSequenceClassification):\n     pass\n \n \n-class Qwen3ForTokenClassification(LlamaForTokenClassification):\n+class Qwen3ForTokenClassification(Qwen2ForTokenClassification):\n     pass\n \n \n-class Qwen3ForQuestionAnswering(LlamaForQuestionAnswering):\n+class Qwen3ForQuestionAnswering(Qwen2ForQuestionAnswering):\n     pass\n \n "
        },
        {
            "sha": "f349f2f3d6dd797b23179a3ed0b910ece6c271a7",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 173,
            "changes": 185,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -27,10 +27,10 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n@@ -43,16 +43,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_qwen3_moe import Qwen3MoeConfig\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -155,13 +149,7 @@ def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n         )\n         self.q_norm = Qwen3MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # unlike olmo, only on the head dim!\n         self.k_norm = Qwen3MoeRMSNorm(self.head_dim, eps=config.rms_norm_eps)  # thus post q_norm does not need reshape\n-        self.sliding_window = config.sliding_window\n-        if not (\n-            self.config.use_sliding_window\n-            and getattr(self.config, \"sliding_window\", None) is not None\n-            and self.layer_idx >= self.config.max_window_layers\n-        ):\n-            self.sliding_window = None\n+        self.sliding_window = getattr(config, \"sliding_window\", None)\n \n     def forward(\n         self,\n@@ -535,8 +523,14 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -602,161 +596,6 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen3Moe. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: Qwen3MoeConfig,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`Qwen3MoeConfig`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "7170164f515ed989ee532faf4d8287452c51b3af",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -42,7 +42,9 @@\n \n \n class Qwen3MoeAttention(Qwen3Attention):  # This is the main diff with qwen2Moe!\n-    pass\n+    def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.sliding_window = getattr(config, \"sliding_window\", None)\n \n \n class Qwen3MoeMLP(nn.Module):"
        },
        {
            "sha": "be9af304d51d7494019f17596ef9c40b41e4f360",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -793,7 +793,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -863,7 +863,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "6d79e0f0f741d97cf8766a75255b3b2e242e2167",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 166,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -30,9 +30,9 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -44,16 +44,10 @@\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...utils import LossKwargs, auto_docstring, can_return_tuple, is_torch_flex_attn_available, logging\n+from ...utils import LossKwargs, auto_docstring, can_return_tuple, logging\n from .configuration_starcoder2 import Starcoder2Config\n \n \n-if is_torch_flex_attn_available():\n-    from torch.nn.attention.flex_attention import BlockMask\n-\n-    from ...integrations.flex_attention import make_flex_block_causal_mask\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -404,8 +398,14 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds\n@@ -454,161 +454,6 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: Union[torch.Tensor, \"BlockMask\"],\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool = False,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and past_key_values is not None:\n-                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n-                if is_padding_right:\n-                    raise ValueError(\n-                        \"You are attempting to perform batched generation with padding_side='right'\"\n-                        \" this may lead to unexpected behaviour for Flash Attention version of Starcoder2. Make sure to \"\n-                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n-                    )\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-        if self.config._attn_implementation == \"flex_attention\":\n-            if isinstance(attention_mask, torch.Tensor):\n-                attention_mask = make_flex_block_causal_mask(attention_mask)\n-            return attention_mask\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-        using_sliding_window_cache = isinstance(past_key_values, SlidingWindowCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and not (using_static_cache or using_sliding_window_cache)\n-            and not output_attentions\n-        ):\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                sliding_window=self.config.sliding_window,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        min_dtype = torch.finfo(dtype).min\n-        sequence_length = input_tensor.shape[1]\n-        # SlidingWindowCache or StaticCache\n-        if using_sliding_window_cache or using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        # DynamicCache or no cache\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-            config=self.config,\n-            past_key_values=past_key_values,\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type in [\"cuda\", \"xpu\", \"npu\"]\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        config: Starcoder2Config,\n-        past_key_values: Cache,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache, to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-            config (`Starcoder2Config`):\n-                The model's configuration class\n-            past_key_values (`Cache`):\n-                The cache class that is being used currently to generate\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            diagonal_attend_mask = torch.arange(target_length, device=cache_position.device) > cache_position.reshape(\n-                -1, 1\n-            )\n-            text_config = config.get_text_config()\n-            if getattr(text_config, \"use_sliding_window\", True) and text_config.sliding_window is not None:\n-                # if we have sliding window, we should not attend to tokens beyond sliding window length, so we mask them out also\n-                # the check is needed to verify is current checkpoint was trained with sliding window or not\n-                if not isinstance(past_key_values, SlidingWindowCache) or sequence_length > target_length:\n-                    sliding_attend_mask = torch.arange(target_length, device=cache_position.device) <= (\n-                        cache_position.reshape(-1, 1) - text_config.sliding_window\n-                    )\n-                    diagonal_attend_mask.bitwise_or_(sliding_attend_mask)\n-            causal_mask *= diagonal_attend_mask\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                if attention_mask.shape[-1] > target_length:\n-                    attention_mask = attention_mask[:, :target_length]\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-        return causal_mask\n-\n \n class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n "
        },
        {
            "sha": "fd5840e40a7c0995226f2bf77f509b68c9fe096d",
            "filename": "src/transformers/models/starcoder2/modular_starcoder2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 2,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodular_starcoder2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -27,6 +27,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n@@ -212,8 +213,14 @@ def forward(\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+        mask_function = create_causal_mask if self.config.sliding_window is None else create_sliding_window_causal_mask\n+        causal_mask = mask_function(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            output_attentions=output_attentions,\n         )\n \n         hidden_states = inputs_embeds"
        },
        {
            "sha": "d2db7781d2fc3631629fcf3f91a8b63b7d5afe8e",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1123,7 +1123,7 @@ def forward(\n             router_probs=all_router_probs,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1193,7 +1193,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "466b725bce2a5ece3ae8c52698cad88df3a1dab7",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1195,7 +1195,7 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1265,7 +1265,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "478baddc505cbe9bc00b82d0f3631f9508770822",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1360,7 +1360,7 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1430,7 +1430,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "e45d63aba7dba00f295788aa120d19a595dacf57",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -836,7 +836,7 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -906,7 +906,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "937d44a7817fbe5570fd94e3d505221a15b3f27b",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -648,7 +648,7 @@ def prepare_inputs_for_generation(\n         return model_inputs\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "375278b5d85644014c325a57699bc26d817df930",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -460,60 +460,5 @@ def prepare_inputs_for_generation(\n \n         return model_inputs\n \n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :].to(\n-                    causal_mask.device\n-                )\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n __all__ = [\"VipLlavaModel\", \"VipLlavaForConditionalGeneration\", \"VipLlavaPreTrainedModel\"]"
        },
        {
            "sha": "e42c7ce306057d05f144efe683126f7db8be6b65",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1230,7 +1230,7 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._update_causal_mask\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n         attention_mask: Union[torch.Tensor, \"BlockMask\"],\n@@ -1300,7 +1300,7 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel._prepare_4d_causal_attention_mask_with_cache_position\n+    # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._prepare_4d_causal_attention_mask_with_cache_position\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,"
        },
        {
            "sha": "e865237485a6228a825ce81f46816e8473617f71",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -538,6 +538,13 @@ def convert_and_export_with_cache(*args, **kwargs):\n     requires_backends(convert_and_export_with_cache, [\"torch\"])\n \n \n+class AttentionMaskInterface(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n def model_addition_debugger_context(*args, **kwargs):\n     requires_backends(model_addition_debugger_context, [\"torch\"])\n "
        },
        {
            "sha": "8fc2a857dd0d1f52957ed82a93478fe7c9f56e60",
            "filename": "src/transformers/utils/generic.py",
            "status": "modified",
            "additions": 42,
            "deletions": 1,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Futils%2Fgeneric.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/src%2Ftransformers%2Futils%2Fgeneric.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fgeneric.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -26,7 +26,7 @@\n from dataclasses import fields, is_dataclass\n from enum import Enum\n from functools import partial, wraps\n-from typing import Any, ContextManager, Optional, TypedDict\n+from typing import Any, Callable, ContextManager, List, Optional, TypedDict\n \n import numpy as np\n from packaging import version\n@@ -977,3 +977,44 @@ def wrapper(self, *args, **kwargs):\n         return output\n \n     return wrapper\n+\n+\n+class GeneralInterface(MutableMapping):\n+    \"\"\"\n+    Dict-like object keeping track of a class-wide mapping, as well as a local one. Allows to have library-wide\n+    modifications though the class mapping, as well as local modifications in a single file with the local mapping.\n+    \"\"\"\n+\n+    # Class instance object, so that a call to `register` can be reflected into all other files correctly, even if\n+    # a new instance is created (in order to locally override a given function)\n+    _global_mapping = {}\n+\n+    def __init__(self):\n+        self._local_mapping = {}\n+\n+    def __getitem__(self, key):\n+        # First check if instance has a local override\n+        if key in self._local_mapping:\n+            return self._local_mapping[key]\n+        return self._global_mapping[key]\n+\n+    def __setitem__(self, key, value):\n+        # Allow local update of the default functions without impacting other instances\n+        self._local_mapping.update({key: value})\n+\n+    def __delitem__(self, key):\n+        del self._local_mapping[key]\n+\n+    def __iter__(self):\n+        # Ensure we use all keys, with the overwritten ones on top\n+        return iter({**self._global_mapping, **self._local_mapping})\n+\n+    def __len__(self):\n+        return len(self._global_mapping.keys() | self._local_mapping.keys())\n+\n+    @classmethod\n+    def register(cls, key: str, value: Callable):\n+        cls._global_mapping.update({key: value})\n+\n+    def valid_keys(self) -> List[str]:\n+        return list(self.keys())"
        },
        {
            "sha": "373b3ffbe222244d5a08625e8fdfadf53b3ddbd1",
            "filename": "tests/models/gemma/test_modeling_gemma.py",
            "status": "modified",
            "additions": 27,
            "deletions": 23,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_gemma.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -13,7 +13,6 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Gemma model.\"\"\"\n \n-import tempfile\n import unittest\n \n import pytest\n@@ -23,7 +22,6 @@\n from transformers.generation.configuration_utils import GenerationConfig\n from transformers.testing_utils import (\n     cleanup,\n-    is_flaky,\n     require_bitsandbytes,\n     require_flash_attn,\n     require_read_token,\n@@ -303,39 +301,45 @@ def test_Gemma_token_classification_model(self):\n     def test_flash_attn_2_inference_equivalence_right_padding(self):\n         self.skipTest(reason=\"Gemma flash attention does not support right padding\")\n \n+    @require_torch_sdpa\n+    @require_torch_accelerator\n+    def test_sdpa_equivalence(self):\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_sdpa:\n+                self.skipTest(reason=\"Model does not support SDPA\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config).to(torch_device)\n+            dummy_input = inputs_dict[model_class.main_input_name].to(torch_device)\n+\n+            model.config._attn_implementation = \"sdpa\"\n+            states_sdpa = model(dummy_input, output_hidden_states=True).hidden_states[-1]\n+\n+            model.config._attn_implementation = \"eager\"\n+            states_eager = model(dummy_input, output_hidden_states=True).hidden_states[-1]\n+\n+            torch.testing.assert_close(states_sdpa, states_eager, atol=1e-5, rtol=1e-5)\n+\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test\n-    @is_flaky()\n-    @slow\n     def test_flash_attn_2_equivalence(self):\n         for model_class in self.all_model_classes:\n             if not model_class._supports_flash_attn_2:\n                 self.skipTest(reason=\"Model does not support Flash Attention 2\")\n \n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            model = model_class(config)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname)\n-                model_fa = model_class.from_pretrained(\n-                    tmpdirname, torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\"\n-                )\n-                model_fa.to(torch_device)\n-\n-                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, attn_implementation=\"eager\")\n-                model.to(torch_device)\n+            model = model_class(config).to(device=torch_device, dtype=torch.float16)\n+            dummy_input = inputs_dict[model_class.main_input_name].to(torch_device)\n \n-                dummy_input = inputs_dict[model_class.main_input_name]\n-                dummy_input = dummy_input.to(torch_device)\n-                outputs = model(dummy_input, output_hidden_states=True)\n-                outputs_fa = model_fa(dummy_input, output_hidden_states=True)\n+            model.config._attn_implementation = \"flash_attention_2\"\n+            states_sdpa = model(dummy_input, output_hidden_states=True).hidden_states[1]\n \n-                logits = outputs.hidden_states[-1]\n-                logits_fa = outputs_fa.hidden_states[-1]\n+            model.config._attn_implementation = \"eager\"\n+            states_eager = model(dummy_input, output_hidden_states=True).hidden_states[1]\n \n-                # gemma flash attention 2 needs a high tolerance\n-                assert torch.allclose(logits_fa, logits, atol=3e-3)\n+            # Here we use higher tolerance and the output of the 2nd layer because otherwise small diffs add-up\n+            torch.testing.assert_close(states_sdpa, states_eager, atol=1e-3, rtol=1e-3)\n \n \n @slow"
        },
        {
            "sha": "2561875f387253991dae64759f09cfa42bac93a9",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -154,6 +154,10 @@ def test_multi_gpu_data_parallel_forward(self):\n     def test_eager_matches_fa2_generate(self):\n         pass\n \n+    @unittest.skip(\"Gemma2 eager/FA2 attention outputs are expected to be different\")\n+    def test_flash_attn_2_equivalence(self):\n+        pass\n+\n \n @slow\n @require_torch_accelerator"
        },
        {
            "sha": "87350c048951ab084268621ed0616653790f29be",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -25,7 +25,6 @@\n     AutoTokenizer,\n     Gemma3Config,\n     Gemma3TextConfig,\n-    GenerationConfig,\n     is_torch_available,\n )\n from transformers.testing_utils import (\n@@ -635,46 +634,6 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         EXPECTED_COMPLETIONS = [\" and I'm going to take a walk.\\n\\nI really enjoy the scenery, and I'\", \", green, yellow, orange, purple, brown, black, white, gray.\\n\\nI'\"]  # fmt: skip\n         self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n \n-    def test_generation_beyond_sliding_window_with_generation_config(self):\n-        \"\"\"\n-        Similar to `test_generation_beyond_sliding_window`, but passing a GenerationConfig. Regression test for #36684\n-        -- ensures `cache_implementation='hybrid'` is correctly inherited from the base `model.generation_config`.\n-        \"\"\"\n-        model_id = \"google/gemma-3-1b-it\"\n-        attn_implementation = \"sdpa\"\n-\n-        input_text = [\n-            \"This is a nice place. \" * 800 + \"I really enjoy the scenery,\",  # This is larger than 4096 tokens\n-            \"A list of colors: red, blue\",  # This will almost all be padding tokens\n-        ]\n-        tokenizer = AutoTokenizer.from_pretrained(model_id, padding=\"left\")\n-        inputs = tokenizer(input_text, padding=True, return_tensors=\"pt\").to(torch_device)\n-\n-        model = AutoModelForCausalLM.from_pretrained(\n-            model_id, attn_implementation=attn_implementation, torch_dtype=torch.float16\n-        ).to(torch_device)\n-\n-        # Make sure prefill is larger than sliding window\n-        input_size = inputs.input_ids.shape[-1]\n-        self.assertGreater(input_size, model.config.sliding_window)\n-\n-        generation_config = GenerationConfig(max_new_tokens=5, min_new_tokens=5)\n-        out = model.generate(**inputs, generation_config=generation_config)\n-\n-        out = model.generate(**inputs, generation_config=generation_config, do_sample=False)[:, input_size:]\n-        output_text = tokenizer.batch_decode(out)\n-        EXPECTED_COMPLETIONS = [\" and I'm going to take a walk.\\n\\nI really enjoy the scenery, and I'\", \", green, yellow, orange, purple, brown, black, white, gray.\\n\\nI'\"]  # fmt: skip\n-        self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n-\n-        # Generation works beyond sliding window\n-        self.assertGreater(out.shape[1], model.config.sliding_window)\n-        self.assertEqual(out.shape[1], input_size + 5)\n-\n-        # Note: Auto-inheritance only works for models saved starting from 4.50.0\n-        model.generation_config.transformers_version = \"4.49.0\"\n-        with self.assertRaises(RuntimeError):  # errors out because it is not using hybrid cache\n-            out = model.generate(**inputs, generation_config=generation_config)\n-\n     def test_export_text_only_with_hybrid_cache(self):\n         if not is_torch_greater_or_equal(\"2.6.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.6 to run.\")"
        },
        {
            "sha": "95cb5d2785b0d0c020efda7bb5fcb491b811c351",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -26,6 +26,7 @@\n     is_vision_available,\n )\n from transformers.testing_utils import (\n+    is_flaky,\n     require_torch,\n     torch_device,\n )\n@@ -381,3 +382,8 @@ def test_contrastive_generate_low_memory(self):\n     @unittest.skip(\"Gemma2 has HybridCache and doesn't support StaticCache\")\n     def test_generate_with_static_cache(self):\n         pass\n+\n+    @pytest.mark.generate\n+    @is_flaky\n+    def test_generate_compile_model_forward(self):\n+        super().test_generate_compile_model_forward()"
        },
        {
            "sha": "2f16a86d80b5d04614defc5d0712b8a2c875e536",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 29,
            "deletions": 26,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -1172,25 +1172,10 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n                         traced_model = torch.jit.trace(model, example_inputs, check_trace=False)\n                     else:\n                         main_input = inputs[main_input_name]\n-\n-                        if model.config._attn_implementation == \"sdpa\":\n-                            trace_input = {main_input_name: main_input}\n-\n-                            if \"attention_mask\" in inputs:\n-                                trace_input[\"attention_mask\"] = inputs[\"attention_mask\"]\n-                            else:\n-                                self.skipTest(reason=\"testing SDPA without attention_mask is not supported\")\n-\n-                            outputs = model(main_input, attention_mask=inputs[\"attention_mask\"])\n-                            if any(isinstance(x, Cache) for x in outputs):\n-                                continue\n-                            # example_kwarg_inputs was introduced in torch==2.0, but it is fine here since SDPA has a requirement on torch>=2.1.\n-                            traced_model = torch.jit.trace(model, example_kwarg_inputs=trace_input)\n-                        else:\n-                            outputs = model(main_input)\n-                            if any(isinstance(x, Cache) for x in outputs):\n-                                continue\n-                            traced_model = torch.jit.trace(model, (main_input,))\n+                        outputs = model(main_input)\n+                        if any(isinstance(x, Cache) for x in outputs):\n+                            continue\n+                        traced_model = torch.jit.trace(model, (main_input,))\n                 except RuntimeError:\n                     self.fail(\"Couldn't trace module.\")\n \n@@ -3907,6 +3892,11 @@ def test_sdpa_can_compile_dynamic(self):\n                 self.skipTest(\n                     \"DBRX (transformers==4.40) requires a modification to support dynamic shapes with compile.\"\n                 )\n+            if getattr(config, \"cache_implementation\", None) == \"hybrid\":\n+                self.skipTest(\n+                    \"Cannot compile forward without an existing cache with Hybrid, as `torch._dynamo.mark_static_address` \"\n+                    \"is a forbidden call.\"\n+                )\n             model = model_class(config)\n \n             with tempfile.TemporaryDirectory() as tmpdirname:\n@@ -4346,18 +4336,31 @@ def test_sliding_window_mask(self):\n         config.sliding_window = sliding_window\n         inputs[\"attention_mask\"] = torch.ones(batch_size, seq_len).to(torch.int64).to(torch_device)\n         for model_class in self.all_model_classes:\n-            model = model_class(config).to(torch_device)\n-            model.eval()\n-\n             # Set sliding window to `True` and check that all tokens beyond window size are masked\n-            model.config.use_sliding_window = True\n+            config.use_sliding_window = True\n+            config_dict = config.to_diff_dict()\n+            if hasattr(config, \"layer_types\"):\n+                del config_dict[\"layer_types\"]\n+            new_config = config.__class__(**config_dict)\n+            model = model_class(new_config).to(torch_device)\n+            model.eval()\n+            layer_types = getattr(model.config, \"layer_types\", [\"sliding_attention\"] * config.num_hidden_layers)\n             attentions = model(**inputs, output_attentions=True).attentions\n-            for layer_attention in attentions:\n-                self.assertTrue((layer_attention[:, :, ~sliding_mask] == 0).all().item())\n+            for layer_attention, layer_type in zip(attentions, layer_types):\n+                if layer_type == \"sliding_attention\":\n+                    self.assertTrue((layer_attention[:, :, ~sliding_mask] == 0).all().item())\n+                else:\n+                    self.assertFalse((layer_attention[:, :, ~sliding_mask] == 0).all().item())\n \n             # Set sliding window to `False` while keeping `sliding_window=3`\n             # Check that all tokens beyond window size are not masked\n-            model.config.use_sliding_window = False\n+            config.use_sliding_window = False\n+            config_dict = config.to_diff_dict()\n+            if hasattr(config, \"layer_types\"):\n+                del config_dict[\"layer_types\"]\n+            new_config = config.__class__(**config_dict)\n+            model = model_class(new_config).to(torch_device)\n+            model.eval()\n             attentions_not_sliding = model(**inputs, output_attentions=True).attentions\n             for layer_attention in attentions_not_sliding:\n                 self.assertFalse((layer_attention[:, :, ~sliding_mask] == 0).all().item())"
        },
        {
            "sha": "3d1fa7a44741a317a152267c87ae3dfe3c091999",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -55,6 +55,7 @@\n         convert_and_export_with_cache,\n         pipeline,\n     )\n+    from transformers.integrations.executorch import export_with_dynamic_cache\n \n \n TEST_CACHE_IMPLEMENTATIONS = [\n@@ -593,22 +594,11 @@ def test_dynamic_cache_exportability(self):\n         attention_mask = inputs.attention_mask\n         input_ids = inputs.input_ids\n \n-        past_key_values = DynamicCache()\n-        ep = torch.export.export(\n-            model,\n-            (),\n-            {\n-                \"input_ids\": input_ids,\n-                \"attention_mask\": attention_mask,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": True,\n-            },\n-            strict=False,\n-        )\n+        ep = export_with_dynamic_cache(model, input_ids, attention_mask)\n         res = ep.module()(\n             input_ids=input_ids,\n             attention_mask=attention_mask,\n-            past_key_values=past_key_values,\n+            past_key_values=DynamicCache(),\n             use_cache=True,\n         )\n         self.assertTrue(len(res.past_key_values.key_cache) == model.config.num_hidden_layers)"
        },
        {
            "sha": "238da721bf68d821b56cc2771123fbd19835bba1",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/163138a911c1fb4451ec4b32edaee20918a59def/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/163138a911c1fb4451ec4b32edaee20918a59def/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=163138a911c1fb4451ec4b32edaee20918a59def",
            "patch": "@@ -44,9 +44,11 @@\n         \"expert_layer_offset\",\n         \"expert_layer_period\",\n     ],\n-    \"Qwen2Config\": [\"use_sliding_window\"],\n+    \"Qwen2Config\": [\"use_sliding_window\", \"max_window_layers\"],\n     \"Qwen2MoeConfig\": [\"use_sliding_window\"],\n     \"Qwen2VLConfig\": [\"use_sliding_window\"],\n+    \"Qwen3Config\": [\"max_window_layers\", \"use_sliding_window\"],  # now use `layer_types` instead\n+    \"Qwen3MoeConfig\": [\"max_window_layers\", \"use_sliding_window\"],\n     # `cache_implementation` should be in the default generation config, but we don't yet support per-model\n     # generation configs (TODO joao)\n     \"Gemma2Config\": [\"tie_word_embeddings\", \"cache_implementation\"],\n@@ -263,6 +265,7 @@\n         \"router_aux_loss_coef\",\n         \"router_jitter_noise\",\n         \"cache_implementation\",\n+        \"attention_chunk_size\",\n     ],\n     \"Llama4VisionConfig\": [\"multi_modal_projector_bias\", \"norm_eps\"],\n }"
        }
    ],
    "stats": {
        "total": 9792,
        "additions": 2984,
        "deletions": 6808
    }
}