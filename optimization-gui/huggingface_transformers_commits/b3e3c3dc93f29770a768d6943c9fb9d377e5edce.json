{
    "author": "HollowMan6",
    "message": "[Qwen3VL] fix device mismatch error for FSDP2 training (#41536)\n\nFor FSDP2, parameters might be on a meta device, and the weight.device attribute may\nnot accurately reflect where the actual computation will happen during forward passes.\n\n```log\n  File \"transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py\", line 776, in forward\n    pos_embeds = self.fast_pos_embed_interpolate(grid_thw)\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py\", line 745, in fast_pos_embed_interpolate\n    pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n                 ^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"torch/nn/modules/module.py\", line 1773, in _wrapped_call_impl\n    return self._call_impl(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"torch/nn/modules/module.py\", line 1879, in _call_impl\n    return inner()\n           ^^^^^^^\n  File \"torch/nn/modules/module.py\", line 1827, in inner\n    result = forward_call(*args, **kwargs)\n             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"torch/nn/modules/sparse.py\", line 192, in forward\n    return F.embedding(\n           ^^^^^^^^^^^^\n  File \"torch/nn/functional.py\", line 2546, in embedding\n    return torch.embedding(weight, input, padding_idx, scale_grad_by_freq, sparse)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\nRuntimeError: Expected all tensors to be on the same device, but got index is on cpu, different from other tensors on cuda:0 (when checking argument in method wrapper_CUDA__index_select)\n```\nhttps://github.com/volcengine/verl/pull/3686#issuecomment-3380981817\n\nSigned-off-by: Hollow Man <hollowman@opensuse.org>",
    "sha": "b3e3c3dc93f29770a768d6943c9fb9d377e5edce",
    "files": [
        {
            "sha": "2665cb7f36a70f926c514acd57c06d9cf408ed0e",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3e3c3dc93f29770a768d6943c9fb9d377e5edce/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3e3c3dc93f29770a768d6943c9fb9d377e5edce/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=b3e3c3dc93f29770a768d6943c9fb9d377e5edce",
            "patch": "@@ -1099,6 +1099,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -1136,11 +1137,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
        },
        {
            "sha": "7d9a00d9cf31f689a5544619260ebc836fee5f96",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3e3c3dc93f29770a768d6943c9fb9d377e5edce/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3e3c3dc93f29770a768d6943c9fb9d377e5edce/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=b3e3c3dc93f29770a768d6943c9fb9d377e5edce",
            "patch": "@@ -639,6 +639,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -676,11 +677,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
        },
        {
            "sha": "1aa9774f223bb7c61088969be5f1a4437775f332",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3e3c3dc93f29770a768d6943c9fb9d377e5edce/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3e3c3dc93f29770a768d6943c9fb9d377e5edce/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=b3e3c3dc93f29770a768d6943c9fb9d377e5edce",
            "patch": "@@ -615,6 +615,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -652,11 +653,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
        },
        {
            "sha": "a09d2d938f27f3538bc088a5b54fea139aed9d2e",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/b3e3c3dc93f29770a768d6943c9fb9d377e5edce/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b3e3c3dc93f29770a768d6943c9fb9d377e5edce/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=b3e3c3dc93f29770a768d6943c9fb9d377e5edce",
            "patch": "@@ -661,6 +661,7 @@ def rot_pos_emb(self, grid_thw: torch.Tensor) -> torch.Tensor:\n \n     def fast_pos_embed_interpolate(self, grid_thw):\n         grid_ts, grid_hs, grid_ws = grid_thw[:, 0], grid_thw[:, 1], grid_thw[:, 2]\n+        device = grid_thw.device\n \n         idx_list = [[] for _ in range(4)]\n         weight_list = [[] for _ in range(4)]\n@@ -698,11 +699,9 @@ def fast_pos_embed_interpolate(self, grid_thw):\n                 idx_list[i].extend(indices[i].tolist())\n                 weight_list[i].extend(weights[i].tolist())\n \n-        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=self.pos_embed.weight.device)\n-        weight_tensor = torch.tensor(\n-            weight_list, dtype=self.pos_embed.weight.dtype, device=self.pos_embed.weight.device\n-        )\n-        pos_embeds = self.pos_embed(idx_tensor) * weight_tensor[:, :, None]\n+        idx_tensor = torch.tensor(idx_list, dtype=torch.long, device=device)\n+        weight_tensor = torch.tensor(weight_list, dtype=self.pos_embed.weight.dtype, device=device)\n+        pos_embeds = self.pos_embed(idx_tensor).to(device) * weight_tensor[:, :, None]\n         patch_pos_embeds = pos_embeds[0] + pos_embeds[1] + pos_embeds[2] + pos_embeds[3]\n \n         patch_pos_embeds = patch_pos_embeds.split([h * w for h, w in zip(grid_hs, grid_ws)])"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 16,
        "deletions": 20
    }
}