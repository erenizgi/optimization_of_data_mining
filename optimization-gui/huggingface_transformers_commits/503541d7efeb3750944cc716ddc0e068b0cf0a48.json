{
    "author": "garrett361",
    "message": "add FlashAttentionKwargs and seq_idx to flat collator (#36456)\n\n* add flash attn kwargs to flattening collator\n\n* add return_seq_idx option\n\n* doc string edits\n\n* cleaner max len updates\n\n* various fixes\n\n* temp testing code\n\n* return int32 seq_idx and FlashAttnKwargs\n\n* DataCollatorIntegrationTest impl\n\n* fix batch dims and dtypes\n\n* fill out remaining collator tests\n\n* test name change and fmt\n\n* rm unused var\n\n* fmt\n\n* minor change\n\n* fmt\n\n* add missing pos_ids check\n\n* consistent {np,pt,tf} tests\n\n* split pt tests into 3, like np/tf tests\n\n* mv comment, rename fa test\n\n* remove batch dim comment\n\n* simply wrapping\n\n* compute cu_seq_len/max_length once\n\n* fmt\n\n* remove tf code\n\n* rm warning\n\n* move separator_id back to 2nd pos\n\n* use cleaner lists in tests\n\n* ret -> batch\n\n* fmt\n\n* attr ordering\n\n* use py ints for max_length_{k,q}",
    "sha": "503541d7efeb3750944cc716ddc0e068b0cf0a48",
    "files": [
        {
            "sha": "55aed55a13fba1a5175e8696b4a588237308addb",
            "filename": "src/transformers/data/data_collator.py",
            "status": "modified",
            "additions": 63,
            "deletions": 11,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/503541d7efeb3750944cc716ddc0e068b0cf0a48/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/503541d7efeb3750944cc716ddc0e068b0cf0a48/src%2Ftransformers%2Fdata%2Fdata_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fdata_collator.py?ref=503541d7efeb3750944cc716ddc0e068b0cf0a48",
            "patch": "@@ -1974,9 +1974,11 @@ class DataCollatorWithFlattening(DefaultDataCollator):\n     \"\"\"\n     Data collator used for padding free approach. Does the following:\n \n-    - concatate the entire mini batch into single long sequence [1, total_tokens]\n+    - concatenates the entire mini batch into single long sequence of shape [1, total_tokens]\n     - uses `separator_id` to separate sequences within the concatenated `labels`, default value is -100\n-    - no padding will be added, returns `input_ids`, `labels` and `position_ids`\n+    - no padding will be added, returns `input_ids`, `labels` and `position_ids` by default\n+    - optionally returns the kwargs contained in FlashAttentionKwargs\n+    - optionally returns seq_idx indicating which sequence each token belongs to\n \n     <Tip warning={true}>\n \n@@ -1986,26 +1988,76 @@ class DataCollatorWithFlattening(DefaultDataCollator):\n     </Tip>\n     \"\"\"\n \n-    def __init__(self, *args, return_position_ids=True, separator_id=-100, **kwargs):\n+    def __init__(\n+        self,\n+        *args,\n+        return_position_ids=True,\n+        separator_id=-100,\n+        return_flash_attn_kwargs=False,\n+        return_seq_idx=False,\n+        **kwargs,\n+    ):\n         super().__init__(*args, **kwargs)\n         self.return_position_ids = return_position_ids\n         self.separator_id = separator_id\n+        self.return_flash_attn_kwargs = return_flash_attn_kwargs\n+        self.return_seq_idx = return_seq_idx\n+        self._int_64_keys = {\"labels\", \"position_ids\", \"input_ids\"}\n+        self._batch_dim_keys = {\"labels\", \"position_ids\", \"input_ids\", \"seq_idx\"}\n+        self._py_int_keys = {\"max_length_q\", \"max_length_k\"}\n \n     def __call__(self, features, return_tensors=None, separator_id=None):\n         if return_tensors is None:\n             return_tensors = self.return_tensors\n         if separator_id is None:\n             separator_id = self.separator_id\n         is_labels_provided = \"labels\" in features[0]\n-        ret = {\"input_ids\": [], \"labels\": []}\n+        batch = {\"input_ids\": [], \"labels\": []}\n         if self.return_position_ids:\n-            ret.update({\"position_ids\": []})\n-        for idx in range(0, len(features)):\n-            ret[\"input_ids\"] += features[idx][\"input_ids\"]\n+            batch.update({\"position_ids\": []})\n+        if self.return_seq_idx:\n+            batch.update({\"seq_idx\": []})\n+        if self.return_flash_attn_kwargs:\n+            cu_seq_lens = [0]\n+            max_length = 0\n+        for seq_idx, sample in enumerate(features):\n+            input_ids = sample[\"input_ids\"]\n+            batch[\"input_ids\"] += input_ids\n             if is_labels_provided:\n-                ret[\"labels\"] += [separator_id] + features[idx][\"labels\"][1:]\n+                batch[\"labels\"] += [separator_id] + sample[\"labels\"][1:]\n             else:\n-                ret[\"labels\"] += [separator_id] + features[idx][\"input_ids\"][1:]\n+                batch[\"labels\"] += [separator_id] + input_ids[1:]\n             if self.return_position_ids:\n-                ret[\"position_ids\"] += list(range(len(features[idx][\"input_ids\"])))\n-        return default_data_collator([ret], return_tensors)\n+                batch[\"position_ids\"] += list(range(len(input_ids)))\n+            if self.return_seq_idx:\n+                batch[\"seq_idx\"] += [seq_idx for _ in range(len(input_ids))]\n+            if self.return_flash_attn_kwargs:\n+                cu_seq_lens.append(cu_seq_lens[-1] + len(input_ids))\n+                max_length = max(max_length, len(input_ids))\n+\n+        if self.return_flash_attn_kwargs:\n+            batch[\"cu_seq_lens_q\"] = batch[\"cu_seq_lens_k\"] = cu_seq_lens\n+            batch[\"max_length_q\"] = batch[\"max_length_k\"] = max_length\n+\n+        # FlashAttentionKwargs and seq_idx are expected to be int32s.\n+        if return_tensors == \"pt\":\n+            import torch\n+\n+            data_cls = torch.tensor\n+            dtype_64 = torch.int64\n+            dtype_32 = torch.int32\n+        elif return_tensors == \"np\":\n+            data_cls = np.array\n+            dtype_64 = np.int64\n+            dtype_32 = np.int32\n+        else:\n+            raise ValueError(f'return_tensors must be one of (\"pt\", \"np\"), {return_tensors=} not suported')\n+\n+        for k, v in batch.items():\n+            if k in self._batch_dim_keys:\n+                v = [v]\n+            # Flash attention max_len_{q,k} are python ints\n+            if k not in self._py_int_keys:\n+                batch[k] = data_cls(v, dtype=dtype_64 if k in self._int_64_keys else dtype_32)\n+\n+        return batch"
        },
        {
            "sha": "00fb2a77d81a4e75180125e8e6e7be0197b6495a",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 73,
            "deletions": 0,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/503541d7efeb3750944cc716ddc0e068b0cf0a48/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/503541d7efeb3750944cc716ddc0e068b0cf0a48/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=503541d7efeb3750944cc716ddc0e068b0cf0a48",
            "patch": "@@ -34,6 +34,7 @@\n     AutoModel,\n     AutoModelForCausalLM,\n     AutoModelForSequenceClassification,\n+    DataCollatorWithFlattening,\n     PretrainedConfig,\n     PreTrainedModel,\n     is_torch_available,\n@@ -4170,6 +4171,78 @@ def test_flash_attention_2_padding_matches_padding_free_with_position_ids(self):\n                 tol = torch.finfo(torch.float16).eps\n                 torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attention_2_padding_matches_padding_free_with_position_ids_and_fa_kwargs(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        max_new_tokens = 30\n+\n+        for model_class in self.all_generative_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            if 0 not in inputs_dict.get(\"attention_mask\", []) or \"attention_mask\" not in inputs_dict:\n+                self.skipTest(\"Model dummy inputs should contain padding in their attention mask\")\n+\n+            dummy_input = inputs_dict[model_class.main_input_name]\n+            if dummy_input.dtype in [torch.float32, torch.bfloat16]:\n+                dummy_input = dummy_input.to(torch.float16)\n+\n+            # make sure that all models have enough positions for generation\n+            if hasattr(config, \"max_position_embeddings\"):\n+                config.max_position_embeddings = max_new_tokens + dummy_input.shape[1] + 1\n+\n+            model = model_class(config)\n+            if \"position_ids\" not in inspect.signature(model.forward).parameters:\n+                self.skipTest(\"Model does not support position_ids\")\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # ensure left padding, to adapt for some models\n+                if 0 in inputs_dict[\"attention_mask\"][:, -1]:\n+                    inputs_dict[\"attention_mask\"] = inputs_dict[\"attention_mask\"].flip(1)\n+                dummy_attention_mask = inputs_dict[\"attention_mask\"]\n+                inputs_dict[\"input_ids\"][~dummy_attention_mask.bool()] = config.get_text_config().pad_token_id\n+\n+                model = (\n+                    model_class.from_pretrained(\n+                        tmpdirname,\n+                        torch_dtype=torch.float16,\n+                        attn_implementation=\"flash_attention_2\",\n+                        low_cpu_mem_usage=True,\n+                    )\n+                    .to(torch_device)\n+                    .eval()\n+                )\n+\n+                # flatten\n+                features = [\n+                    {\"input_ids\": i[a.bool()].tolist()}\n+                    for i, a in zip(inputs_dict[\"input_ids\"], inputs_dict[\"attention_mask\"])\n+                ]\n+\n+                # add position_ids + fa_kwargs\n+                data_collator = DataCollatorWithFlattening(return_tensors=\"pt\", return_flash_attn_kwargs=True)\n+                batch = data_collator(features)\n+                batch_cuda = {k: t.cuda() if torch.is_tensor(t) else t for k, t in batch.items()}\n+\n+                res_padded = model(**inputs_dict)\n+                res_padfree = model(**batch_cuda)\n+\n+                logits_padded = res_padded.logits[inputs_dict[\"attention_mask\"].bool()]\n+                logits_padfree = res_padfree.logits[0]\n+\n+                torch.testing.assert_close(logits_padded.argmax(-1), logits_padfree.argmax(-1), rtol=0, atol=0)\n+                # acceptable numerical instability\n+                tol = torch.finfo(torch.float16).eps\n+                torch.testing.assert_close(logits_padded, logits_padfree, rtol=tol, atol=tol)\n+\n     @require_flash_attn\n     @require_torch_gpu\n     @mark.flash_attn_test"
        },
        {
            "sha": "d4360c32c90333c78e911f6c87d75097e97ab3b5",
            "filename": "tests/trainer/test_data_collator.py",
            "status": "modified",
            "additions": 182,
            "deletions": 2,
            "changes": 184,
            "blob_url": "https://github.com/huggingface/transformers/blob/503541d7efeb3750944cc716ddc0e068b0cf0a48/tests%2Ftrainer%2Ftest_data_collator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/503541d7efeb3750944cc716ddc0e068b0cf0a48/tests%2Ftrainer%2Ftest_data_collator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_data_collator.py?ref=503541d7efeb3750944cc716ddc0e068b0cf0a48",
            "patch": "@@ -126,6 +126,104 @@ def test_data_collator_with_padding(self):\n         batch = data_collator(features)\n         self.assertEqual(batch[\"input_ids\"].shape, torch.Size([2, 8]))\n \n+    def test_data_collator_with_flattening(self):\n+        features = [\n+            {\"input_ids\": [10, 11, 12]},\n+            {\"input_ids\": [20, 21, 22, 23, 24, 25]},\n+            {\"input_ids\": [30, 31, 32, 33, 34, 35, 36]},\n+        ]\n+\n+        data_collator = DataCollatorWithFlattening(return_tensors=\"pt\")\n+        batch = data_collator(features)\n+\n+        for unexpected_key in [\n+            \"attention_mask\",\n+            \"cu_seq_lens_k\",\n+            \"cu_seq_lens_q\",\n+            \"max_length_k\",\n+            \"max_length_q\",\n+            \"seq_idx\",\n+        ]:\n+            self.assertNotIn(unexpected_key, batch)\n+        self.assertIn(\"position_ids\", batch)\n+\n+        self.assertEqual(batch[\"input_ids\"].shape, torch.Size([1, 16]))\n+        self.assertEqual(\n+            batch[\"input_ids\"][0].tolist(), [10, 11, 12, 20, 21, 22, 23, 24, 25, 30, 31, 32, 33, 34, 35, 36]\n+        )\n+        self.assertEqual(batch[\"position_ids\"].shape, torch.Size([1, 16]))\n+        self.assertEqual(batch[\"position_ids\"][0].tolist(), [0, 1, 2, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6])\n+\n+    def test_data_collator_with_flattening_flash_attn_kwargs(self):\n+        features = [\n+            {\"input_ids\": [10, 11, 12]},\n+            {\"input_ids\": [20, 21, 22, 23, 24, 25]},\n+            {\"input_ids\": [30, 31, 32, 33, 34, 35, 36]},\n+        ]\n+        data_collator = DataCollatorWithFlattening(return_tensors=\"pt\", return_flash_attn_kwargs=True)\n+        batch = data_collator(features)\n+\n+        for unexpected_key in [\n+            \"attention_mask\",\n+            \"seq_idx\",\n+        ]:\n+            self.assertNotIn(unexpected_key, batch)\n+        for expected_key in [\n+            \"position_ids\",\n+            \"cu_seq_lens_k\",\n+            \"cu_seq_lens_q\",\n+            \"max_length_k\",\n+            \"max_length_q\",\n+        ]:\n+            self.assertIn(expected_key, batch)\n+\n+        self.assertEqual(batch[\"input_ids\"].shape, torch.Size([1, 16]))\n+        self.assertEqual(\n+            batch[\"input_ids\"][0].tolist(), [10, 11, 12, 20, 21, 22, 23, 24, 25, 30, 31, 32, 33, 34, 35, 36]\n+        )\n+        self.assertEqual(batch[\"position_ids\"].shape, torch.Size([1, 16]))\n+        self.assertEqual(batch[\"position_ids\"][0].tolist(), [0, 1, 2, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6])\n+\n+        self.assertEqual(batch[\"cu_seq_lens_k\"].shape, torch.Size([4]))\n+        self.assertEqual(batch[\"cu_seq_lens_k\"].tolist(), [0, 3, 9, 16])\n+        self.assertEqual(batch[\"cu_seq_lens_q\"].shape, torch.Size([4]))\n+        self.assertEqual(batch[\"cu_seq_lens_q\"].tolist(), [0, 3, 9, 16])\n+        # The flash attn max_length_{k,q} are simple python ints\n+        self.assertEqual(batch[\"max_length_k\"], 7)\n+        self.assertEqual(batch[\"max_length_q\"], 7)\n+\n+    def test_data_collator_with_flattening_seq_idx(self):\n+        features = [\n+            {\"input_ids\": [10, 11, 12]},\n+            {\"input_ids\": [20, 21, 22, 23, 24, 25]},\n+            {\"input_ids\": [30, 31, 32, 33, 34, 35, 36]},\n+        ]\n+        data_collator = DataCollatorWithFlattening(return_tensors=\"pt\", return_seq_idx=True)\n+        batch = data_collator(features)\n+\n+        for unexpected_key in [\n+            \"attention_mask\",\n+            \"cu_seq_lens_k\",\n+            \"cu_seq_lens_q\",\n+            \"max_length_k\",\n+            \"max_length_q\",\n+        ]:\n+            self.assertNotIn(unexpected_key, batch)\n+        for expected_key in [\n+            \"position_ids\",\n+            \"seq_idx\",\n+        ]:\n+            self.assertIn(expected_key, batch)\n+\n+        self.assertEqual(batch[\"input_ids\"].shape, torch.Size([1, 16]))\n+        self.assertEqual(\n+            batch[\"input_ids\"][0].tolist(), [10, 11, 12, 20, 21, 22, 23, 24, 25, 30, 31, 32, 33, 34, 35, 36]\n+        )\n+        self.assertEqual(batch[\"position_ids\"].shape, torch.Size([1, 16]))\n+        self.assertEqual(batch[\"position_ids\"][0].tolist(), [0, 1, 2, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6])\n+        self.assertEqual(batch[\"seq_idx\"].shape, batch[\"input_ids\"].shape)\n+        self.assertEqual(batch[\"seq_idx\"][0].tolist(), [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2])\n+\n     def test_data_collator_for_token_classification(self):\n         tokenizer = BertTokenizer(self.vocab_file)\n         features = [\n@@ -1803,14 +1901,96 @@ def test_data_collator_with_flattening(self):\n \n         data_collator = DataCollatorWithFlattening(return_tensors=\"np\")\n         batch = data_collator(features)\n+\n+        for unexpected_key in [\n+            \"attention_mask\",\n+            \"cu_seq_lens_k\",\n+            \"cu_seq_lens_q\",\n+            \"max_length_k\",\n+            \"max_length_q\",\n+            \"seq_idx\",\n+        ]:\n+            self.assertNotIn(unexpected_key, batch)\n+        self.assertIn(\"position_ids\", batch)\n+\n+        self.assertEqual(batch[\"input_ids\"].shape, (1, 16))\n+        self.assertEqual(\n+            batch[\"input_ids\"][0].tolist(), [10, 11, 12, 20, 21, 22, 23, 24, 25, 30, 31, 32, 33, 34, 35, 36]\n+        )\n+        self.assertEqual(batch[\"position_ids\"].shape, (1, 16))\n+        self.assertEqual(batch[\"position_ids\"][0].tolist(), [0, 1, 2, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6])\n+\n+    def test_data_collator_with_flattening_flash_attn_kwargs(self):\n+        features = [\n+            {\"input_ids\": [10, 11, 12]},\n+            {\"input_ids\": [20, 21, 22, 23, 24, 25]},\n+            {\"input_ids\": [30, 31, 32, 33, 34, 35, 36]},\n+        ]\n+\n+        data_collator = DataCollatorWithFlattening(return_tensors=\"np\", return_flash_attn_kwargs=True)\n+        batch = data_collator(features)\n+\n+        for unexpected_key in [\n+            \"attention_mask\",\n+            \"seq_idx\",\n+        ]:\n+            self.assertNotIn(unexpected_key, batch)\n+        for expected_key in [\n+            \"position_ids\",\n+            \"cu_seq_lens_k\",\n+            \"cu_seq_lens_q\",\n+            \"max_length_k\",\n+            \"max_length_q\",\n+        ]:\n+            self.assertIn(expected_key, batch)\n+\n+        self.assertEqual(batch[\"input_ids\"].shape, (1, 16))\n+        self.assertEqual(\n+            batch[\"input_ids\"][0].tolist(), [10, 11, 12, 20, 21, 22, 23, 24, 25, 30, 31, 32, 33, 34, 35, 36]\n+        )\n+        self.assertEqual(batch[\"position_ids\"].shape, (1, 16))\n+        self.assertEqual(batch[\"position_ids\"][0].tolist(), [0, 1, 2, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6])\n+\n+        self.assertEqual(batch[\"cu_seq_lens_k\"].shape, (4,))\n+        self.assertEqual(batch[\"cu_seq_lens_k\"].tolist(), [0, 3, 9, 16])\n+        self.assertEqual(batch[\"cu_seq_lens_q\"].shape, (4,))\n+        self.assertEqual(batch[\"cu_seq_lens_q\"].tolist(), [0, 3, 9, 16])\n+        # The flash attn max_length_{k,q} are simple python ints\n+        self.assertEqual(batch[\"max_length_k\"], 7)\n+        self.assertEqual(batch[\"max_length_q\"], 7)\n+\n+    def test_data_collator_with_flattening_seq_idx(self):\n+        features = [\n+            {\"input_ids\": [10, 11, 12]},\n+            {\"input_ids\": [20, 21, 22, 23, 24, 25]},\n+            {\"input_ids\": [30, 31, 32, 33, 34, 35, 36]},\n+        ]\n+\n+        data_collator = DataCollatorWithFlattening(return_tensors=\"np\", return_seq_idx=True)\n+        batch = data_collator(features)\n+\n+        for unexpected_key in [\n+            \"attention_mask\",\n+            \"cu_seq_lens_k\",\n+            \"cu_seq_lens_q\",\n+            \"max_length_k\",\n+            \"max_length_q\",\n+        ]:\n+            self.assertNotIn(unexpected_key, batch)\n+        for expected_key in [\n+            \"position_ids\",\n+            \"seq_idx\",\n+        ]:\n+            self.assertIn(expected_key, batch)\n+\n         self.assertEqual(batch[\"input_ids\"].shape, (1, 16))\n         self.assertEqual(\n             batch[\"input_ids\"][0].tolist(), [10, 11, 12, 20, 21, 22, 23, 24, 25, 30, 31, 32, 33, 34, 35, 36]\n         )\n-        self.assertNotIn(\"attention_mask\", batch)\n-        self.assertIn(\"position_ids\", batch)\n         self.assertEqual(batch[\"position_ids\"].shape, (1, 16))\n         self.assertEqual(batch[\"position_ids\"][0].tolist(), [0, 1, 2, 0, 1, 2, 3, 4, 5, 0, 1, 2, 3, 4, 5, 6])\n+        self.assertEqual(batch[\"seq_idx\"].shape, batch[\"input_ids\"].shape)\n+        self.assertEqual(batch[\"seq_idx\"][0].tolist(), [0, 0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2])\n \n     def test_data_collator_for_token_classification(self):\n         tokenizer = BertTokenizer(self.vocab_file)"
        }
    ],
    "stats": {
        "total": 331,
        "additions": 318,
        "deletions": 13
    }
}