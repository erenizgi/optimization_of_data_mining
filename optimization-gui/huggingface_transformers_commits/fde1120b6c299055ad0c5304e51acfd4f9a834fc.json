{
    "author": "cyyever",
    "message": "Remove deprecated use_flash_attention_2 parameter (#37131)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "fde1120b6c299055ad0c5304e51acfd4f9a834fc",
    "files": [
        {
            "sha": "c3c4c75750c3c3503b118b5a9f59542f4a0d965d",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 23,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/fde1120b6c299055ad0c5304e51acfd4f9a834fc/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fde1120b6c299055ad0c5304e51acfd4f9a834fc/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=fde1120b6c299055ad0c5304e51acfd4f9a834fc",
            "patch": "@@ -2157,8 +2157,6 @@ def _from_config(cls, config, **kwargs):\n         if isinstance(torch_dtype, str):\n             torch_dtype = getattr(torch, torch_dtype)\n \n-        use_flash_attention_2 = kwargs.pop(\"use_flash_attention_2\", False)\n-\n         # override default dtype if needed\n         dtype_orig = None\n         if torch_dtype is not None:\n@@ -2177,7 +2175,6 @@ def _from_config(cls, config, **kwargs):\n         if not getattr(config, \"_attn_implementation_autoset\", False):\n             config = cls._autoset_attn_implementation(\n                 config,\n-                use_flash_attention_2=use_flash_attention_2,\n                 check_device_map=False,\n                 torch_dtype=torch_dtype,\n             )\n@@ -2205,29 +2202,21 @@ def _from_config(cls, config, **kwargs):\n     def _autoset_attn_implementation(\n         cls,\n         config,\n-        use_flash_attention_2: bool = False,\n         torch_dtype: Optional[torch.dtype] = None,\n         device_map: Optional[Union[str, Dict[str, int]]] = None,\n         check_device_map: bool = True,\n     ):\n         \"\"\"\n         Automatically checks and dispatches to a default attention implementation. In order of priority:\n             1. An implementation specified in `config._attn_implementation` (due for example to the argument attn_implementation=\"sdpa\" in from_pretrained).\n-            2. DEPRECATED: if use_flash_attention_2 is set to `True` and `flash_attn` is available, flash attention. (`LlamaFlashAttention` for example)\n-            3. SDPA implementation, if available and supported by the model type. (`LlamaSdpaAttention` for example)\n-            4. The default model's implementation otherwise (`LlamaAttention` for example) .\n+            2. SDPA implementation, if available and supported by the model type. (`LlamaSdpaAttention` for example)\n+            3. The default model's implementation otherwise (`LlamaAttention` for example) .\n         \"\"\"\n         # Here we use config._attn_implementation_internal to check whether the attention implementation was explicitly set by the user.\n         # The property `PretrainedConfig._attn_implementation` is never `None`, for backward compatibility (always fall back on \"eager\").\n         # The `hasattr` here is used as some Transformers tests for some reason do not call PretrainedConfig __init__ (e.g. test_no_super_init_config_and_model)\n         requested_attn_implementation = None\n         if hasattr(config, \"_attn_implementation_internal\") and config._attn_implementation_internal is not None:\n-            if config._attn_implementation != \"flash_attention_2\" and use_flash_attention_2:\n-                raise ValueError(\n-                    f'Both attn_implementation=\"{config._attn_implementation}\" and `use_flash_attention_2=True` were used when loading the model, which are not compatible.'\n-                    ' We recommend to just use `attn_implementation=\"flash_attention_2\"` when loading the model.'\n-                )\n-\n             if isinstance(config._attn_implementation, str) and re.match(\n                 r\"^[^/:]+/[^/:]+:[^/:]+$\", config._attn_implementation\n             ):\n@@ -2292,12 +2281,6 @@ def _autoset_attn_implementation(\n             if sub_config is not None:\n                 sub_config._attn_implementation_internal = curr_attn_implementation\n \n-        if use_flash_attention_2:\n-            logger.warning_once(\n-                'The model was loaded with use_flash_attention_2=True, which is deprecated and may be removed in a future release. Please use `attn_implementation=\"flash_attention_2\"` instead.'\n-            )\n-            config._attn_implementation = \"flash_attention_2\"\n-\n         if config._attn_implementation == \"flash_attention_2\":\n             cls._check_and_enable_flash_attn_2(\n                 config,\n@@ -2309,10 +2292,10 @@ def _autoset_attn_implementation(\n         elif requested_attn_implementation == \"flex_attention\":\n             config = cls._check_and_enable_flex_attn(config, hard_check_only=True)\n         elif requested_attn_implementation in [None, \"sdpa\"] and not is_torch_xla_available():\n-            # use_flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\n+            # flash_attention_2 takes priority over SDPA, hence SDPA treated in this elif.\n             config = cls._check_and_enable_sdpa(\n                 config,\n-                hard_check_only=False if requested_attn_implementation is None else True,\n+                hard_check_only=requested_attn_implementation is not None,\n             )\n \n             if (\n@@ -4256,7 +4239,6 @@ def from_pretrained(\n         variant = kwargs.pop(\"variant\", None)\n         adapter_kwargs = kwargs.pop(\"adapter_kwargs\", {})\n         adapter_name = kwargs.pop(\"adapter_name\", \"default\")\n-        use_flash_attention_2 = kwargs.pop(\"use_flash_attention_2\", False)\n         generation_config = kwargs.pop(\"generation_config\", None)\n         gguf_file = kwargs.pop(\"gguf_file\", None)\n         tp_plan = kwargs.pop(\"tp_plan\", None)\n@@ -4618,7 +4600,6 @@ def from_pretrained(\n         if not getattr(config, \"_attn_implementation_autoset\", False):\n             config = cls._autoset_attn_implementation(\n                 config,\n-                use_flash_attention_2=use_flash_attention_2,\n                 torch_dtype=torch_dtype,\n                 device_map=device_map,\n             )"
        },
        {
            "sha": "0b254bd73a2ba699e463365b8c3bb3edbd298129",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fde1120b6c299055ad0c5304e51acfd4f9a834fc/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fde1120b6c299055ad0c5304e51acfd4f9a834fc/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=fde1120b6c299055ad0c5304e51acfd4f9a834fc",
            "patch": "@@ -615,7 +615,6 @@ def init_weight(module: nn.Module, std: float):\n     def _autoset_attn_implementation(\n         cls,\n         config,\n-        use_flash_attention_2: bool = False,\n         torch_dtype: Optional[torch.dtype] = None,\n         device_map: Optional[Union[str, Dict[str, int]]] = None,\n         check_device_map: bool = True,\n@@ -638,8 +637,7 @@ def _autoset_attn_implementation(\n                 config._attn_implementation_internal = None\n         return super()._autoset_attn_implementation(\n             config,\n-            use_flash_attention_2=use_flash_attention_2,\n-            torch_dtype=torch.float16,\n+            torch_dtype=torch_dtype,\n             device_map=device_map,\n             check_device_map=check_device_map,\n         )"
        },
        {
            "sha": "18f2bb8beb73237a3023c2680eca2d2dca376eb9",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fde1120b6c299055ad0c5304e51acfd4f9a834fc/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fde1120b6c299055ad0c5304e51acfd4f9a834fc/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=fde1120b6c299055ad0c5304e51acfd4f9a834fc",
            "patch": "@@ -817,7 +817,6 @@ def init_weight(module: nn.Module, std: float):\n     def _autoset_attn_implementation(\n         cls,\n         config,\n-        use_flash_attention_2: bool = False,\n         torch_dtype: Optional[torch.dtype] = None,\n         device_map: Optional[Union[str, Dict[str, int]]] = None,\n         check_device_map: bool = True,\n@@ -840,8 +839,7 @@ def _autoset_attn_implementation(\n                 config._attn_implementation_internal = None\n         return super()._autoset_attn_implementation(\n             config,\n-            use_flash_attention_2=use_flash_attention_2,\n-            torch_dtype=torch.float16,\n+            torch_dtype=torch_dtype,\n             device_map=device_map,\n             check_device_map=check_device_map,\n         )"
        },
        {
            "sha": "0b3ef078905ffbd2b382c51dd8533472bb77e144",
            "filename": "tests/models/diffllama/test_modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fde1120b6c299055ad0c5304e51acfd4f9a834fc/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fde1120b6c299055ad0c5304e51acfd4f9a834fc/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdiffllama%2Ftest_modeling_diffllama.py?ref=fde1120b6c299055ad0c5304e51acfd4f9a834fc",
            "patch": "@@ -488,7 +488,7 @@ def test_use_flash_attention_2_true(self):\n                 model.save_pretrained(tmp_dir)\n \n                 new_model = DiffLlamaForCausalLM.from_pretrained(\n-                    tmp_dir, use_flash_attention_2=True, torch_dtype=torch.float16\n+                    tmp_dir, attn_implementation=\"flash_attention_2\", torch_dtype=torch.float16\n                 ).to(\"cuda\")\n \n                 self.assertTrue(new_model.config._attn_implementation == \"flash_attention_2\")"
        }
    ],
    "stats": {
        "total": 37,
        "additions": 7,
        "deletions": 30
    }
}