{
    "author": "zucchini-nlp",
    "message": "Update dynamic attnt setter for multimodals (#39908)\n\n* update\n\n* fix the test for DepthPro\n\n* PR comments\n\n* wait, I didn't delete this in prev commit?\n\n* fix\n\n* better way\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>",
    "sha": "52c6c1bb6e27ca87c4faede34a4c2a7404c17c4d",
    "files": [
        {
            "sha": "4829157d04bd5d2745022312bc4c9c9ba7981fe2",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 93,
            "deletions": 76,
            "changes": 169,
            "blob_url": "https://github.com/huggingface/transformers/blob/52c6c1bb6e27ca87c4faede34a4c2a7404c17c4d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52c6c1bb6e27ca87c4faede34a4c2a7404c17c4d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=52c6c1bb6e27ca87c4faede34a4c2a7404c17c4d",
            "patch": "@@ -121,7 +121,6 @@\n     is_torch_greater_or_equal,\n     is_torch_mlu_available,\n     is_torch_npu_available,\n-    is_torch_sdpa_available,\n     is_torch_xla_available,\n     is_torch_xpu_available,\n     logging,\n@@ -2641,14 +2640,12 @@ def _sdpa_can_dispatch(self, is_init_check: bool = False) -> bool:\n                 BetterTransformer, which are only available later after __init__. This allows to raise proper exceptions early\n                 before instantiating the full models if we know that the model does not support the requested attention.\n         \"\"\"\n-        if not self._supports_sdpa and not is_init_check:\n+        if not self._supports_sdpa:\n             raise ValueError(\n                 f\"{self.__class__.__name__} does not support an attention implementation through torch.nn.functional.scaled_dot_product_attention yet.\"\n                 \" Please request the support for this architecture: https://github.com/huggingface/transformers/issues/28005. If you believe\"\n                 ' this error is a bug, please open an issue in Transformers GitHub repository and load your model with the argument `attn_implementation=\"eager\"` meanwhile. Example: `model = AutoModel.from_pretrained(\"openai/whisper-tiny\", attn_implementation=\"eager\")`'\n             )\n-        if not is_torch_sdpa_available():\n-            raise ImportError(\"PyTorch SDPA requirements in Transformers are not met. Please install torch>=2.1.1.\")\n \n         if (\n             torch.version.hip is not None\n@@ -2721,22 +2718,25 @@ def _check_and_adjust_attn_implementation(\n             `str`: The final attention implementation to use, including potential fallbacks from sdpa to eager, or from\n             None to sdpa (to potentially eager).\n         \"\"\"\n-        applicable_attn_implementation = \"sdpa\" if attn_implementation is None else attn_implementation\n-        if re.match(r\"^[^/:]+/[^/:]+(?:@[^/:]+)?(?::[^/:]+)?$\", applicable_attn_implementation):\n+        # Register kernel if relevant\n+        if attn_implementation is not None and re.match(\n+            r\"^[^/:]+/[^/:]+(?:@[^/:]+)?(?::[^/:]+)?$\", attn_implementation\n+        ):\n             if not is_kernels_available():\n                 raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n             attention_wrapper = None\n             # FIXME: @ArthurZucker this is dirty, did not want to do a lof of extra work\n-            if \"|\" in applicable_attn_implementation:\n-                attention_wrapper, applicable_attn_implementation = applicable_attn_implementation.split(\"|\")\n+            actual_attn_name = attn_implementation\n+            if \"|\" in attn_implementation:\n+                attention_wrapper, actual_attn_name = attn_implementation.split(\"|\")\n                 # `transformers` has wrapper for sdpa, paged, flash, flex etc.\n                 attention_wrapper = ALL_ATTENTION_FUNCTIONS.get(attention_wrapper)\n             # Extract repo_id and kernel_name from the string\n-            if \":\" in applicable_attn_implementation:\n-                repo_id, kernel_name = attn_implementation.split(\":\")\n+            if \":\" in actual_attn_name:\n+                repo_id, kernel_name = actual_attn_name.split(\":\")\n                 kernel_name = kernel_name.strip()\n             else:\n-                repo_id = applicable_attn_implementation\n+                repo_id = actual_attn_name\n                 kernel_name = None\n             repo_id = repo_id.strip()\n             # extract the rev after the @ if it exists\n@@ -2761,56 +2761,53 @@ def _check_and_adjust_attn_implementation(\n                     f\"Could not find a kernel repository '{repo_id}' compatible with your device in the hub: {e}. Using \"\n                     \"default attention implementation instead (sdpa if available, eager otherwise).\"\n                 )\n-\n-                attn_implementation = \"sdpa\"  # Try to fallback to sdpa in this case\n-            return attn_implementation\n+                try:\n+                    self._sdpa_can_dispatch(is_init_check)\n+                    attn_implementation = \"sdpa\"\n+                except (ValueError, ImportError) as e:\n+                    attn_implementation = \"eager\"\n         else:\n-            attn_implementation = self.get_correct_attn_implementation(applicable_attn_implementation, is_init_check)\n-\n+            attn_implementation = self.get_correct_attn_implementation(attn_implementation, is_init_check)\n             # preload flash attention here to allow compile with fullgraph\n-            if applicable_attn_implementation.startswith(\"flash_attention\"):\n-                lazy_import_flash_attention(applicable_attn_implementation)\n+            if attn_implementation.startswith(\"flash_attention\"):\n+                lazy_import_flash_attention(attn_implementation)\n \n-            return attn_implementation\n+        return attn_implementation\n \n-    def get_correct_attn_implementation(self, _requested_attention: str, is_init_check: bool = False) -> str:\n-        requested_attention = \"sdpa\" if _requested_attention is None else _requested_attention\n-        if is_init_check and requested_attention == \"sdpa\":\n-            if not self._supports_sdpa:\n-                requested_attention = \"eager\"\n-        if requested_attention not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n+    def get_correct_attn_implementation(self, requested_attention: Optional[str], is_init_check: bool = False) -> str:\n+        applicable_attention = \"sdpa\" if requested_attention is None else requested_attention\n+\n+        if applicable_attention not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n             message = (\n-                f'Specified `attn_implementation=\"{requested_attention}\"` is not supported. The only possible arguments are '\n-                '`attn_implementation=\"eager\"` (manual attention implementation)'\n+                f'Specified `attn_implementation=\"{applicable_attention}\"` is not supported. The only possible arguments are '\n+                '`attn_implementation=\"eager\"`'\n             )\n             # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n             if self._supports_flash_attn or getattr(self, \"_supports_flash_attn_2\", False):\n-                message += (\n-                    ', `\"attn_implementation=flash_attention_3\"` (implementation using flash attention 3)'\n-                    ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n-                )\n+                message += ', `\"attn_implementation=flash_attention_3\"`, `\"attn_implementation=flash_attention_2\"`'\n             if self._supports_sdpa:\n-                message += ', `\"attn_implementation=sdpa\"` (implementation using torch.nn.functional.scaled_dot_product_attention)'\n+                message += ', `\"attn_implementation=sdpa\"'\n             if self._supports_flex_attn:\n-                message += ', `\"attn_implementation=flex_attention\"` (implementation using torch\\'s flex_attention)'\n+                message += ', `\"attn_implementation=flex_attention\"`'\n             raise ValueError(message + \".\")\n \n         # Perform relevant checks\n-        if requested_attention == \"flash_attention_2\":\n+        if applicable_attention == \"flash_attention_2\":\n             self._flash_attn_2_can_dispatch(is_init_check)\n-        elif requested_attention == \"flash_attention_3\":\n+        elif applicable_attention == \"flash_attention_3\":\n             self._flash_attn_3_can_dispatch(is_init_check)\n-        elif requested_attention == \"flex_attention\":\n+        elif applicable_attention == \"flex_attention\":\n             self._flex_attn_can_dispatch(is_init_check)\n-        elif requested_attention == \"sdpa\":\n+        elif applicable_attention == \"sdpa\":\n             # Sdpa is the default, so we try it and fallback to eager otherwise when not possible\n             try:\n                 self._sdpa_can_dispatch(is_init_check)\n             except (ValueError, ImportError) as e:\n-                if _requested_attention == \"sdpa\":\n+                if requested_attention == \"sdpa\":\n                     raise e\n-                requested_attention = \"eager\"\n-        return requested_attention\n+                applicable_attention = \"eager\"\n+\n+        return applicable_attention\n \n     @classmethod\n     def _can_set_attn_implementation(cls) -> bool:\n@@ -2821,9 +2818,14 @@ def _can_set_attn_implementation(cls) -> bool:\n         with open(class_file, \"r\") as f:\n             code = f.read()\n         # heuristic -> if we find those patterns, the model uses the correct interface\n-        return (\n-            \"eager_attention_forward\" in code and \"ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\" in code\n-        )\n+        if re.search(r\"class \\w+Attention\\(nn.Module\\)\", code):\n+            return (\n+                \"eager_attention_forward\" in code\n+                and \"ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\" in code\n+            )\n+        else:\n+            # If no attention layer, assume `True`. Most probably a multimodal model or inherits from existing models\n+            return True\n \n     def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n         \"\"\"\n@@ -2852,18 +2854,12 @@ def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n                     \"(see https://huggingface.co/docs/transformers/en/attention_interface)\"\n                 )\n             else:\n-                try:\n-                    applicable_attn_implementation = self._check_and_adjust_attn_implementation(\n-                        requested_implementation, is_init_check=False\n-                    )\n-                    # Apply the change (on the internal attr, to avoid setting it recursively)\n-                    self.config._attn_implementation_internal = applicable_attn_implementation\n-                except Exception as e:\n-                    logger.warning(\n-                        f\"Impossible to set the requested `attn_implementation`. The following error was captured: {str(e)}\"\n-                    )\n+                requested_implementation = self._check_and_adjust_attn_implementation(\n+                    requested_implementation, is_init_check=False\n+                )\n+                # Apply the change (on the internal attr, to avoid setting it recursively)\n+                self.config._attn_implementation_internal = requested_implementation\n \n-        subconfigs_changed = set()\n         # Apply it to all submodels as well\n         for submodule in self.modules():\n             # We found a submodel (which is not self) with a different config (otherwise, it may be the same \"actual model\",\n@@ -2872,44 +2868,65 @@ def set_attn_implementation(self, attn_implementation: Union[str, dict]):\n                 submodule is not self\n                 and isinstance(submodule, PreTrainedModel)\n                 and submodule.config.__class__ != self.config.__class__\n+                # If it was already changed, no need to do it again\n+                and not hasattr(submodule.config, \"_attn_was_changed\")\n             ):\n-                sub_implementation = attn_implementation\n-                if isinstance(attn_implementation, dict):\n-                    for subconfig_key in self.config.sub_configs:\n-                        # We need to check for exact object match here, with `is`\n-                        if getattr(self.config, subconfig_key) is submodule.config:\n-                            sub_implementation = attn_implementation.get(\n-                                subconfig_key, submodule.config._attn_implementation\n-                            )\n-                            break\n-                # check the module can use correctly, otherwise we silently set the config without the model using it\n-                try:\n+                # In this case, warn and skip\n+                if not submodule._can_set_attn_implementation():\n+                    logger.warning(\n+                        f\"{submodule.__class__.__name__} does not support setting its attention implementation dynamically, because it \"\n+                        \"does not follow the functional approach based on AttentionInterface \"\n+                        \"(see https://huggingface.co/docs/transformers/en/attention_interface)\"\n+                    )\n+                # Set the attn on the submodule\n+                else:\n+                    sub_implementation = requested_implementation\n+                    if isinstance(attn_implementation, dict):\n+                        for subconfig_key in self.config.sub_configs:\n+                            # We need to check for exact object match here, with `is`\n+                            if getattr(self.config, subconfig_key) is submodule.config:\n+                                sub_implementation = attn_implementation.get(\n+                                    subconfig_key, submodule.config._attn_implementation\n+                                )\n+                                break\n+                    # Check the module can use correctly, otherwise we raise an error if requested attention can't be set for submodule\n                     sub_implementation = submodule.get_correct_attn_implementation(sub_implementation)\n-                    submodule.config._attn_implementation = sub_implementation\n-                    subconfigs_changed.add(submodule.config.__class__)\n-                except Exception:\n-                    pass\n+                    submodule.config._attn_implementation_internal = sub_implementation\n+\n+                # Still add it as \"changed\" even if it was skipped, as we would otherwise try to set it in the dark afterwards\n+                # We need to set it on the config itself, to differentiate 2 subconfigs of the same __class__ potentially\n+                submodule.config._attn_was_changed = True\n \n         # We need this as some old and badly designed models use subconfigs without declaring the corresponding modules as PreTrainedModel\n         for subconfig_key in self.config.sub_configs:\n             subconfig = getattr(self.config, subconfig_key)\n-            requested_implementation = (\n-                attn_implementation\n+            sub_implementation = (\n+                requested_implementation\n                 if not isinstance(attn_implementation, dict)\n                 else attn_implementation.get(subconfig_key, subconfig._attn_implementation)\n             )\n             # This means we did not perform any check above for this particular subconfig -> set it in the dark if it is registered\n             if (\n-                subconfig.__class__ not in subconfigs_changed\n-                and requested_implementation != subconfig._attn_implementation\n-                and requested_implementation in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys()\n+                not hasattr(subconfig, \"_attn_was_changed\")\n+                # If it's already the same, then no need to enter here and raise warnings\n+                and sub_implementation != subconfig._attn_implementation\n             ):\n-                subconfig._attn_implementation_internal = requested_implementation\n+                if sub_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n+                    raise ValueError(\n+                        f'Specified `attn_implementation=\"{sub_implementation}\"` is not supported for {subconfig_key}. '\n+                        'The only possible arguments are \"eager\" (manual attention implementation)'\n+                        f\"or one of the following: {list(ALL_ATTENTION_FUNCTIONS.valid_keys())}\"\n+                    )\n+                subconfig._attn_implementation_internal = sub_implementation\n                 logger.warning(\n-                    f\"We set the attention implementation for the sub-config `{subconfig_key}` to `{requested_implementation}` \"\n+                    f\"We set the attention implementation for the sub-config `{subconfig_key}` to `{sub_implementation}` \"\n                     \"without finding the associated sub-model. For this reason we could not check if the model supports it. \"\n                     \"You may encounter undefined behavior.\"\n                 )\n+            # Unset the attribute in this case, to avoid issues in the future\n+            else:\n+                if hasattr(subconfig, \"_attn_was_changed\"):\n+                    del subconfig._attn_was_changed\n \n     def enable_input_require_grads(self):\n         \"\"\""
        },
        {
            "sha": "678f5173f669cf6df14ec26fc11782f546b65a6f",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 9,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/52c6c1bb6e27ca87c4faede34a4c2a7404c17c4d/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/52c6c1bb6e27ca87c4faede34a4c2a7404c17c4d/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=52c6c1bb6e27ca87c4faede34a4c2a7404c17c4d",
            "patch": "@@ -5023,16 +5023,9 @@ def test_can_set_attention_dynamically(self):\n             for subconfig_key in model.config.sub_configs:\n                 self.assertTrue(getattr(model.config, subconfig_key)._attn_implementation == \"sdpa\")\n \n-            # Check we cannot set it to random values, and it raises a warning (but no crash)\n-            with self.assertLogs(\"transformers.modeling_utils\", level=\"WARNING\") as cm:\n+            # Check we cannot set it to random values, and it raises an error\n+            with self.assertRaisesRegex(ValueError, 'Specified `attn_implementation=\"foo\"` is not supported'):\n                 model.set_attn_implementation(\"foo\")\n-                self.assertTrue(\n-                    any(\n-                        \"Impossible to set the requested `attn_implementation`. The following error was captured:\"\n-                        in warning\n-                        for warning in cm.output\n-                    )\n-                )\n \n             # Should still be sdpa everywhere\n             self.assertTrue(model.config._attn_implementation == \"sdpa\")"
        }
    ],
    "stats": {
        "total": 180,
        "additions": 95,
        "deletions": 85
    }
}