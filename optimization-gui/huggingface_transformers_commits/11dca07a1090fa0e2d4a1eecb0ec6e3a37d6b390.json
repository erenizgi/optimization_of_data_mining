{
    "author": "Fiona-Waters",
    "message": "Fix retrieve function signature and remove faiss requirement (#38624)\n\nSigned-off-by: Fiona Waters <fiwaters6@gmail.com>",
    "sha": "11dca07a1090fa0e2d4a1eecb0ec6e3a37d6b390",
    "files": [
        {
            "sha": "d1fde00995a39b06088e248ea86934fe7283149c",
            "filename": "src/transformers/models/rag/retrieval_rag.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/11dca07a1090fa0e2d4a1eecb0ec6e3a37d6b390/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/11dca07a1090fa0e2d4a1eecb0ec6e3a37d6b390/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fretrieval_rag.py?ref=11dca07a1090fa0e2d4a1eecb0ec6e3a37d6b390",
            "patch": "@@ -104,6 +104,7 @@ class LegacyIndex(Index):\n     PASSAGE_FILENAME = \"psgs_w100.tsv.pkl\"\n \n     def __init__(self, vector_size, index_path):\n+        requires_backends(self, [\"faiss\"])\n         self.index_id_to_db_id = []\n         self.index_path = index_path\n         self.passages = self._load_passages()\n@@ -197,6 +198,7 @@ def get_top_docs(self, question_hidden_states: np.ndarray, n_docs=5) -> Tuple[np\n \n class HFIndexBase(Index):\n     def __init__(self, vector_size, dataset, index_initialized=False):\n+        requires_backends(self, [\"faiss\"])\n         self.vector_size = vector_size\n         self.dataset = dataset\n         self._index_initialized = index_initialized\n@@ -269,6 +271,7 @@ def __init__(\n         use_dummy_dataset=False,\n         dataset_revision=None,\n     ):\n+        requires_backends(self, [\"faiss\"])\n         if int(index_path is None) + int(index_name is None) != 1:\n             raise ValueError(\"Please provide `index_name` or `index_path`.\")\n         self.dataset_name = dataset_name\n@@ -321,6 +324,7 @@ class CustomHFIndex(HFIndexBase):\n     \"\"\"\n \n     def __init__(self, vector_size: int, dataset, index_path=None):\n+        requires_backends(self, [\"faiss\"])\n         super().__init__(vector_size, dataset, index_initialized=index_path is None)\n         self.index_path = index_path\n \n@@ -375,14 +379,14 @@ class RagRetriever:\n \n     >>> dataset = (\n     ...     ...\n-    ... )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a faiss index\n+    ... )  # dataset must be a datasets.Datasets object with columns \"title\", \"text\" and \"embeddings\", and it must have a supported index (e.g., Faiss or other index types depending on your setup)\n     >>> retriever = RagRetriever.from_pretrained(\"facebook/dpr-ctx_encoder-single-nq-base\", indexed_dataset=dataset)\n \n     >>> # To load your own indexed dataset built with the datasets library that was saved on disk. More info in examples/rag/use_own_knowledge_dataset.py\n     >>> from transformers import RagRetriever\n \n     >>> dataset_path = \"path/to/my/dataset\"  # dataset saved via *dataset.save_to_disk(...)*\n-    >>> index_path = \"path/to/my/index.faiss\"  # faiss index saved via *dataset.get_index(\"embeddings\").save(...)*\n+    >>> index_path = \"path/to/my/index\"  # index saved via *dataset.get_index(\"embeddings\").save(...)*\n     >>> retriever = RagRetriever.from_pretrained(\n     ...     \"facebook/dpr-ctx_encoder-single-nq-base\",\n     ...     index_name=\"custom\",\n@@ -398,7 +402,7 @@ class RagRetriever:\n \n     def __init__(self, config, question_encoder_tokenizer, generator_tokenizer, index=None, init_retrieval=True):\n         self._init_retrieval = init_retrieval\n-        requires_backends(self, [\"datasets\", \"faiss\"])\n+        requires_backends(self, [\"datasets\"])\n         super().__init__()\n         self.index = index or self._build_index(config)\n         self.generator_tokenizer = generator_tokenizer\n@@ -440,7 +444,7 @@ def _build_index(config):\n \n     @classmethod\n     def from_pretrained(cls, retriever_name_or_path, indexed_dataset=None, **kwargs):\n-        requires_backends(cls, [\"datasets\", \"faiss\"])\n+        requires_backends(cls, [\"datasets\"])\n         config = kwargs.pop(\"config\", None) or RagConfig.from_pretrained(retriever_name_or_path, **kwargs)\n         rag_tokenizer = RagTokenizer.from_pretrained(retriever_name_or_path, config=config)\n         question_encoder_tokenizer = rag_tokenizer.question_encoder\n@@ -557,7 +561,7 @@ def _main_retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tup\n             np.array(vectors_batched),\n         )  # shapes (batch_size, n_docs) and (batch_size, n_docs, d)\n \n-    def retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, List[dict]]:\n+    def retrieve(self, question_hidden_states: np.ndarray, n_docs: int) -> Tuple[np.ndarray, np.ndarray, List[dict]]:\n         \"\"\"\n         Retrieves documents for specified `question_hidden_states`.\n "
        }
    ],
    "stats": {
        "total": 14,
        "additions": 9,
        "deletions": 5
    }
}