{
    "author": "keyboardAnt",
    "message": "Speculative decoding: Test the target distribution (to prevent issues like #32867) (#34553)\n\n* Update test_utils.py\r\n\r\n* formatting\r\n\r\n* Update test_utils.py\r\n\r\n* formatting\r\n\r\n* formatting\r\n\r\n* Update test_utils.py\r\n\r\n* formatting\r\n\r\n* Update test_utils.py\r\n\r\n* formatting\r\n\r\n* format\r\n\r\n* comments at standard positions",
    "sha": "42b36d73958d326b2e0cc8fdd46c34d56402ba98",
    "files": [
        {
            "sha": "a31def2f9a6ea8b8c3fd147ad8148aa50f2953e1",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 53,
            "deletions": 0,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/42b36d73958d326b2e0cc8fdd46c34d56402ba98/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/42b36d73958d326b2e0cc8fdd46c34d56402ba98/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=42b36d73958d326b2e0cc8fdd46c34d56402ba98",
            "patch": "@@ -14,6 +14,7 @@\n # limitations under the License.\n \n \n+import collections\n import copy\n import gc\n import inspect\n@@ -2450,6 +2451,58 @@ def test_speculative_sampling(self):\n         self.assertTrue(n_matches.item() == 2)\n         self.assertTrue(validated_tokens.tolist()[0] == [1, 4, 8])\n \n+    def test_speculative_sampling_target_distribution(self):\n+        \"\"\"\n+        Asserts that the target distribution is preserved.\n+        Should help with catching issues like #32867.\n+        \"\"\"\n+        # assume vocab size 10, input length 5 + 3 generated candidates\n+        candidate_input_ids = torch.tensor([[8, 0, 3, 9, 8, 1, 4, 5]])  # input tokens\n+        candidate_logits = torch.tensor(\n+            [\n+                [\n+                    [-10.0, 10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0, -10.0],  # generated 1\n+                    [-10.0, -10.0, -10.0, -10.0, 10.0, -10.0, -10.0, -10.0, -10.0, -10.0],  # generated 4\n+                    [-10.0, -10.0, -10.0, -10.0, -10.0, 10.0, -10.0, -10.0, -10.0, -10.0],  # generated 5\n+                ]\n+            ]\n+        )\n+        candidate_length = 3\n+        inf = float(\"inf\")\n+        new_logits = torch.tensor(\n+            [\n+                [\n+                    # accepts 1:\n+                    [-inf, 10.0, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n+                    # accepts 4:\n+                    [-inf, -inf, -inf, -inf, 10.0, -inf, -inf, -inf, -inf, -inf],\n+                    # most likely to be 1 or 8, less likely to be 3, then 7, and should never be any other value:\n+                    [-inf, 2.0, -inf, 1.0, -inf, -inf, -inf, -0.01, 2.0, -inf],\n+                    # N/A:\n+                    [-inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf, -inf],\n+                ]\n+            ]\n+        )\n+        last_assistant_token_is_eos = False\n+        last_validated_token = []\n+        for _ in range(10_000):\n+            validated_tokens, n_matches = _speculative_sampling(\n+                candidate_input_ids,\n+                candidate_logits,\n+                candidate_length,\n+                new_logits,\n+                last_assistant_token_is_eos,\n+            )\n+            self.assertTrue(n_matches.item() == 2)\n+            self.assertTrue(validated_tokens.tolist()[0][0] == 1)\n+            self.assertTrue(validated_tokens.tolist()[0][1] == 4)\n+            self.assertTrue(validated_tokens.tolist()[0][2] in [1, 3, 7, 8])\n+            last_validated_token.append(validated_tokens.tolist()[0][2])\n+        # check that the most likely tokens are selected more often than the less likely ones\n+        last_token_counts = collections.Counter(last_validated_token)\n+        self.assertTrue(last_token_counts[1] > last_token_counts[3] > last_token_counts[7] > 0)\n+        self.assertTrue(last_token_counts[8] > last_token_counts[3])\n+\n \n @pytest.mark.generate\n @require_torch"
        }
    ],
    "stats": {
        "total": 53,
        "additions": 53,
        "deletions": 0
    }
}