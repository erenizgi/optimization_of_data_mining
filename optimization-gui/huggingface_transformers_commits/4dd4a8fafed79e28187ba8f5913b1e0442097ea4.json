{
    "author": "Abdennacer-Badaoui",
    "message": "Fix GPT-2 Flash Attention 2 generation with left-padding (#41966)\n\n* Fix GPT-2 Flash Attention 2 generation with left-padding\n\n* repo consistency\n\n* define is_causal in init\n\n* fix",
    "sha": "4dd4a8fafed79e28187ba8f5913b1e0442097ea4",
    "files": [
        {
            "sha": "51190a978440598ab76b1afdfe64ee60658cb3c5",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4dd4a8fafed79e28187ba8f5913b1e0442097ea4/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4dd4a8fafed79e28187ba8f5913b1e0442097ea4/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=4dd4a8fafed79e28187ba8f5913b1e0442097ea4",
            "patch": "@@ -121,7 +121,7 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n \n         self.attn_dropout = nn.Dropout(config.attn_pdrop)\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n-        self.is_causal = True\n+        self.is_causal = not is_cross_attention\n \n     def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n         # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n@@ -234,8 +234,6 @@ def forward(\n             if is_cross_attention:\n                 past_key_values.is_updated[self.layer_idx] = True\n \n-        is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n-\n         using_eager = self.config._attn_implementation == \"eager\"\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -253,7 +251,6 @@ def forward(\n                 value_states,\n                 attention_mask,\n                 dropout=self.attn_dropout.p if self.training else 0.0,\n-                is_causal=is_causal,\n                 **kwargs,\n             )\n "
        },
        {
            "sha": "2be82afedd7b3c13fbdda99826e0e28bf2015089",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4dd4a8fafed79e28187ba8f5913b1e0442097ea4/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4dd4a8fafed79e28187ba8f5913b1e0442097ea4/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=4dd4a8fafed79e28187ba8f5913b1e0442097ea4",
            "patch": "@@ -130,7 +130,7 @@ def __init__(self, config, is_cross_attention=False, layer_idx=None):\n \n         self.attn_dropout = nn.Dropout(config.attn_pdrop)\n         self.resid_dropout = nn.Dropout(config.resid_pdrop)\n-        self.is_causal = True\n+        self.is_causal = not is_cross_attention\n \n     def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None):\n         # Use `torch.baddbmm` (a bit more efficient w/ alpha param for scaling -- from Megatron-LM)\n@@ -243,8 +243,6 @@ def forward(\n             if is_cross_attention:\n                 past_key_values.is_updated[self.layer_idx] = True\n \n-        is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n-\n         using_eager = self.config._attn_implementation == \"eager\"\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n@@ -262,7 +260,6 @@ def forward(\n                 value_states,\n                 attention_mask,\n                 dropout=self.attn_dropout.p if self.training else 0.0,\n-                is_causal=is_causal,\n                 **kwargs,\n             )\n "
        }
    ],
    "stats": {
        "total": 10,
        "additions": 2,
        "deletions": 8
    }
}