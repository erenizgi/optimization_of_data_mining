{
    "author": "cyyever",
    "message": "Remove doc of tf and flax (#41029)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "226667ec2f98cff3a5dd949b2c68c02b112fb10b",
    "files": [
        {
            "sha": "6585cf206350750966cf8f86f9cc5ecb2a4238c1",
            "filename": "docs/source/ar/autoclass_tutorial.md",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Fautoclass_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Fautoclass_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fautoclass_tutorial.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -145,23 +145,4 @@\n Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…ØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙØ¦Ø© `AutoTokenizer` ÙˆÙØ¦Ø© `AutoModelFor` Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø«ÙŠÙ„Ø§Øª Ù…ÙØ¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Ø§Ù„Ù†Ù…Ø§Ø°Ø¬. Ø³ÙŠØ³Ø§Ø¹Ø¯Ùƒ Ù‡Ø°Ø§ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØµØ­ÙŠØ­Ø© ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©. ÙÙŠ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„ØªØ§Ù„ÙŠØŒ ØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù„ØºÙˆÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬ Ø§Ù„ØµÙˆØ± ÙˆÙ…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ Ø­Ø¯ÙŠØ«Ù‹Ø§ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚.\n </pt>\n \n-<tf>\n-Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ ØªØ³Ù…Ø­ Ù„Ùƒ ÙØ¦Ø§Øª `TFAutoModelFor` Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù…ÙØ¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù„Ù…Ù‡Ù…Ø© Ù…Ø¹ÙŠÙ†Ø© (Ø±Ø§Ø¬Ø¹ [Ù‡Ù†Ø§](model_doc/auto) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù‚Ø§Ø¦Ù…Ø© ÙƒØ§Ù…Ù„Ø© Ø¨Ø§Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…ØªØ§Ø­Ø©). Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªØ³Ù„Ø³Ù„ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForSequenceClassification.from_pretrained`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Ø£Ø¹Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†ÙØ³ Ù†Ù‚Ø·Ø© Ø§Ù„ØªÙØªÙŠØ´ Ù„ØªØ­Ù…ÙŠÙ„ Ø¨Ù†ÙŠØ© Ù„Ù…Ù‡Ù…Ø© Ù…Ø®ØªÙ„ÙØ©:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù…ØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙØ¦Ø© `AutoTokenizer` ÙˆÙØ¦Ø© `TFAutoModelFor` Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ø³Ø® Ù„Ù†Ù…Ø§Ø°Ø¬ Ù…ÙØ¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§. Ø³ÙŠØ³Ø§Ø¹Ø¯Ùƒ Ù‡Ø°Ø§ ÙÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨Ù†ÙŠØ© Ø§Ù„ØµØ­ÙŠØ­Ø© ÙÙŠ ÙƒÙ„ Ù…Ø±Ø©. ÙÙŠ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„ØªØ§Ù„ÙŠØŒ Ø³ØªØªØ¹Ø±Ù Ø¹Ù„Ù‰ ÙƒÙŠÙÙŠØ© Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙØ¬Ø²Ù‘Ø¦ Ø§Ù„Ù„ØºÙˆÙŠ ÙˆÙ…Ø¹Ø§Ù„Ø¬ Ø§Ù„ØµÙˆØ± ÙˆÙ…Ø³ØªØ®Ø±Ø¬ Ø§Ù„Ù…ÙŠØ²Ø§Øª ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø°ÙŠ ØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡ Ø­Ø¯ÙŠØ«Ù‹Ø§ Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚.\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "f681d13aa9efb32ca34a351478ff85f0e51aace7",
            "filename": "docs/source/ar/create_a_model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fcreate_a_model.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -106,30 +106,6 @@ DistilBertConfig {\n >>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\"ØŒ config=my_config)\n ```\n </pt>\n-<tf>\n-Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø®ØµØ§Ø¦Øµ Ø§Ù„ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù…ÙØ®ØµØµØ© Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙÙŠ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n-\n-```py\n->>> from transformers import TFDistilBertModel\n-\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")\n->>> tf_model = TFDistilBertModel(my_config)\n-```\n-\n-Ù‡Ø°Ø§ ÙŠÙ†Ø´Ø¦ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ø¨Ù‚ÙŠÙ… Ø¹Ø´ÙˆØ§Ø¦ÙŠØ© Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ÙØ¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§. Ù„Ù† ÙŠÙƒÙˆÙ† Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…ÙÙŠØ¯Ù‹Ø§ Ø­ØªÙ‰ ÙŠØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡. ØªÙØ¹Ø¯ Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…ÙƒÙ„ÙØ© ÙˆØªØ³ØªØºØ±Ù‚ ÙˆÙ‚ØªÙ‹Ø§ Ø·ÙˆÙŠÙ„Ø§Ù‹. Ù…Ù† Ø§Ù„Ø£ÙØ¶Ù„ Ø¨Ø´ÙƒÙ„ Ø¹Ø§Ù… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ù…ÙØ¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù†ØªØ§Ø¦Ø¬ Ø£ÙØ¶Ù„ Ø¨Ø´ÙƒÙ„ Ø£Ø³Ø±Ø¹ØŒ Ù…Ø¹ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¬Ø²Ø¡ Ø¨Ø³ÙŠØ· ÙÙ‚Ø· Ù…Ù† Ø§Ù„Ù…ÙˆØ§Ø±Ø¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨.\n-\n-Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…ÙØ¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~TFPreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Ø¹Ù†Ø¯Ù…Ø§ ØªÙ‚ÙˆÙ… Ø¨ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø£ÙˆØ²Ø§Ù† Ø§Ù„Ù…ÙØ¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ØŒÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— Transformers. ÙˆÙ…Ø¹ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ Ø§Ø³ØªØ¨Ø¯Ø§Ù„ - Ø¨Ø¹Ø¶ Ø£Ùˆ ÙƒÙ„ - Ø¥Ø¹Ø¯Ø§Ø¯Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬  Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø¨Ø¥Ø¹Ø¯Ø§Ø¯Ø§ØªÙƒ Ø§Ù„Ø®Ø§ØµØ©:\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\"ØŒ config=my_config)\n-```\n-</tf>\n </frameworkcontent>\n \n ### Ø±Ø¤ÙˆØ³ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬\n@@ -154,23 +130,6 @@ DistilBertConfig {\n >>> model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n ```\n </pt>\n-<tf>\n-Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ [`TFDistilBertForSequenceClassification`] Ù‡Ùˆ Ù†Ù…ÙˆØ°Ø¬ DistilBERT Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ Ø¨Ø±Ø£Ø³ ØªØµÙ†ÙŠÙ ØªØ³Ù„Ø³Ù„. Ø±Ø£Ø³ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªØ³Ù„Ø³Ù„ÙŠ Ù‡Ùˆ Ø·Ø¨Ù‚Ø© Ø®Ø·ÙŠØ© Ø£Ø¹Ù„Ù‰ Ø§Ù„Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ø¹Ø©.\n-\n-```py\n->>> from transformers import TFDistilBertForSequenceClassification\n-\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Ø£Ø¹Ø¯ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ø§ Ù†Ù‚Ø·Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù„Ù…Ù‡Ù…Ø© Ø£Ø®Ø±Ù‰ Ø¹Ù† Ø·Ø±ÙŠÙ‚ Ø§Ù„ØªØ¨Ø¯ÙŠÙ„ Ø¥Ù„Ù‰ Ø±Ø£Ø³ Ù†Ù…ÙˆØ°Ø¬ Ù…Ø®ØªÙ„Ù. Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©ØŒ Ø³ØªØ³ØªØ®Ø¯Ù… Ø±Ø£Ø³ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ [`TFDistilBertForQuestionAnswering`]. Ø±Ø£Ø³ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø© Ù…Ø´Ø§Ø¨Ù‡ Ù„Ø±Ø£Ø³ Ø§Ù„ØªØµÙ†ÙŠÙ Ø§Ù„ØªØ³Ù„Ø³Ù„ÙŠ Ø¨Ø§Ø³ØªØ«Ù†Ø§Ø¡ Ø£Ù†Ù‡ Ø·Ø¨Ù‚Ø© Ø®Ø·ÙŠØ© Ø£Ø¹Ù„Ù‰ Ø­Ø§Ù„Ø§Øª Ø§Ù„Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„Ù…Ø®ÙÙŠØ©.\n-\n-```py\n->>> from transformers import TFDistilBertForQuestionAnswering\n-\n->>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ù…Ø¬Ø²Ø¦ Ø§Ù„Ù†ØµÙˆØµ"
        },
        {
            "sha": "c50c2cf40fbeabd96460ffa5cc56199fcbfbed8b",
            "filename": "docs/source/ar/model_sharing.md",
            "status": "modified",
            "additions": 1,
            "deletions": 44,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Fmodel_sharing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Fmodel_sharing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fmodel_sharing.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -74,28 +74,6 @@ pip install huggingface_hub\n >>> pt_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n ```\n </pt>\n-<tf>\n-Ø­Ø¯Ø¯ `from_pt=True` Ù„ØªØ­ÙˆÙŠÙ„ Ù†Ù‚Ø·Ø© ØªØ­Ù‚Ù‚ Ù…Ù† PyTorch Ø¥Ù„Ù‰ TensorFlow:\n-\n-```py\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\n-```\n-\n-Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ TensorFlow Ø§Ù„Ø¬Ø¯ÙŠØ¯ Ø¨Ù†Ù‚Ø·Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ø¬Ø¯ÙŠØ¯Ø©:\n-\n-```py\n->>> tf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n-```\n-</tf>\n-<jax>\n-Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…ØªØ§Ø­Ù‹Ø§ ÙÙŠ FlaxØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ­ÙˆÙŠÙ„ Ù†Ù‚Ø·Ø© ØªØ­Ù‚Ù‚ Ù…Ù† PyTorch Ø¥Ù„Ù‰ Flax:\n-\n-```py\n->>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\n-...     \"path/to/awesome-name-you-picked\", from_pt=True\n-... )\n-```\n-</jax>\n </frameworkcontent>\n \n ## Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨\n@@ -128,27 +106,6 @@ pip install huggingface_hub\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ù‹Ø§ Ø¹Ù„Ù‰ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`PushToHubCallback`]. ÙÙŠ Ø¯Ø§Ù„Ø© [`PushToHubCallback`], Ø£Ø¶Ù:\n-\n-- Ø¯Ù„ÙŠÙ„ Ø¥Ø®Ø±Ø§Ø¬ Ù„Ù†Ù…ÙˆØ°Ø¬Ùƒ.\n-- Ù…ÙØ¬Ø²Ù‘Ø¦ Ø§Ù„Ù„ØºÙˆÙŠ.\n-- `hub_model_id`ØŒ ÙˆØ§Ù„Ø°ÙŠ Ù‡Ùˆ Ø§Ø³Ù… Ù…Ø³ØªØ®Ø¯Ù… Hub ÙˆØ§Ø³Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ.\n-\n-```py\n->>> from transformers import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\n-... )\n-```\n-\n-Ø£Ø¶Ù Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ Ø¥Ù„Ù‰ [`fit`](https://keras.io/api/models/model_training_apis/)ØŒ ÙˆØ³ÙŠÙ‚ÙˆÙ… ğŸ¤— Transformers Ø¨Ø¯ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ø¥Ù„Ù‰ Hub:\n-\n-```py\n->>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© `push_to_hub`\n@@ -220,4 +177,4 @@ pip install huggingface_hub\n * Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù…Ù„Ù `README.md` ÙˆØªØ­Ù…ÙŠÙ„Ù‡ ÙŠØ¯ÙˆÙŠÙ‹Ø§.\n * Ø§Ù†Ù‚Ø± ÙÙˆÙ‚ Ø§Ù„Ø²Ø± **Edit model card** ÙÙŠ Ù…Ø³ØªÙˆØ¯Ø¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ.\n \n-Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø¨Ø·Ø§Ù‚Ø© [DistilBert](https://huggingface.co/distilbert/distilbert-base-uncased) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø¬ÙŠØ¯ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ¶Ù…Ù†Ù‡Ø§ Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠÙ‡Ø§ ÙÙŠ Ù…Ù„Ù `README.md` Ù…Ø«Ù„ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„ÙƒØ±Ø¨ÙˆÙ†ÙŠØ© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ùˆ Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø£Ø¯Ø§Ø©ØŒ Ø±Ø§Ø¬Ø¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ [Ù‡Ù†Ø§](https://huggingface.co/docs/hub/models-cards).\n\\ No newline at end of file\n+Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø¨Ø·Ø§Ù‚Ø© [DistilBert](https://huggingface.co/distilbert/distilbert-base-uncased) Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø¬ÙŠØ¯ Ø¹Ù„Ù‰ Ù†ÙˆØ¹ Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªØªØ¶Ù…Ù†Ù‡Ø§ Ø¨Ø·Ø§Ù‚Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ù„Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„Ø£Ø®Ø±Ù‰ Ø§Ù„ØªÙŠ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„ØªØ­ÙƒÙ… ÙÙŠÙ‡Ø§ ÙÙŠ Ù…Ù„Ù `README.md` Ù…Ø«Ù„ Ø§Ù„Ø¨ØµÙ…Ø© Ø§Ù„ÙƒØ±Ø¨ÙˆÙ†ÙŠØ© Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø£Ùˆ Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ø£Ø¯Ø§Ø©ØŒ Ø±Ø§Ø¬Ø¹ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ [Ù‡Ù†Ø§](https://huggingface.co/docs/hub/models-cards)."
        },
        {
            "sha": "18ab522e436cbad31ad5e0b3ee77a5ecfe09a1b3",
            "filename": "docs/source/ar/preprocessing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Fpreprocessing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Fpreprocessing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fpreprocessing.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -174,31 +174,6 @@ pip install datasets\n                            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n ```\n </pt>\n-<tf>\n- \n-```py\n->>> batch_sentences = [\n-...     \"But what about second breakfast?\",\n-...     \"Don't think he knows about second breakfast, Pip.\",\n-...     \"What about elevensies?\",\n-... ]\n->>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n->>> print(encoded_input)\n-{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n-       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n-       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n-      dtype=int32)>,\n- 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n- 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n-       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n-       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>"
        },
        {
            "sha": "aebcc847ae584296c3da98799e2f3affe67cf0e0",
            "filename": "docs/source/ar/quicktour.md",
            "status": "modified",
            "additions": 0,
            "deletions": 92,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Fquicktour.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -19,12 +19,6 @@\n pip install torch\n ```\n </pt>\n-<tf>\n-\n-```bash\n-pip install tensorflow\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ø®Ø· Ø§Ù„Ø£Ù†Ø§Ø¨ÙŠØ¨\n@@ -133,16 +127,6 @@ label: NEGATIVE, with score: 0.5309\n >>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n ```\n </pt>\n-<tf>\n-Ø§Ø³ØªØ®Ø¯Ù… [`TFAutoModelForSequenceClassification`] Ùˆ [`AutoTokenizer`] Ù„ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…ÙØ¯Ø±Ø¨ Ù…Ø³Ø¨Ù‚Ù‹Ø§ ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡ Ø§Ù„Ù…Ø±ØªØ¨Ø· Ø¨Ù‡ (Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ `TFAutoClass` ÙÙŠ Ø§Ù„Ù‚Ø³Ù… Ø§Ù„ØªØ§Ù„ÙŠ):\n-\n-```py\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n-```\n-</tf>\n </frameworkcontent>\n \n Ø­Ø¯Ø¯ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ù…Ø¹Ø§Ù„Ø¬ ÙÙŠ [`pipeline`]. Ø§Ù„Ø¢Ù† ÙŠÙ…ÙƒÙ†Ùƒ ØªØ·Ø¨ÙŠÙ‚ `classifier` Ø¹Ù„Ù‰ Ø§Ù„Ù†Øµ Ø§Ù„ÙØ±Ù†Ø³ÙŠ:\n@@ -205,18 +189,6 @@ label: NEGATIVE, with score: 0.5309\n ... )\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> tf_batch = tokenizer(\n-...     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n-...     padding=True,\n-...     truncation=True,\n-...     max_length=512,\n-...     return_tensors=\"tf\",\n-... )\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -265,37 +237,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n         [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n ```\n </pt>\n-<tf>\n-ÙŠÙˆÙØ± ğŸ¤— Transformers Ø·Ø±ÙŠÙ‚Ø© Ø¨Ø³ÙŠØ·Ø© ÙˆÙ…ÙˆØ­Ø¯Ø© Ù„ØªØ­Ù…ÙŠÙ„ Ù…Ø«ÙŠÙ„Ø§Øª Ù…ÙØ¯Ø±Ø¨Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§. ÙˆÙ‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ [`TFAutoModel`] Ù…Ø«Ù„ ØªØ­Ù…ÙŠÙ„ [`AutoTokenizer`]. ÙˆØ§Ù„ÙØ±Ù‚ Ø§Ù„ÙˆØ­ÙŠØ¯ Ù‡Ùˆ ØªØ­Ø¯ÙŠØ¯ [`TFAutoModel`] Ø§Ù„ØµØ­ÙŠØ­ Ù„Ù„Ù…Ù‡Ù…Ø©. Ù„Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙŠ (Ø£Ùˆ Ø§Ù„ØªØ³Ù„Ø³Ù„ÙŠ)ØŒ ÙŠØ¬Ø¨ ØªØ­Ù…ÙŠÙ„ [`TFAutoModelForSequenceClassification`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n-```\n-\n-<Tip>\n-\n-Ø±Ø§Ø¬Ø¹ [Ù…Ù„Ø®Øµ Ø§Ù„Ù…Ù‡Ø§Ù…](./task_summary) Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„Ù…Ø¯Ø¹ÙˆÙ…Ø© Ø¨ÙˆØ§Ø³Ø·Ø© ÙØ¦Ø© [`AutoModel`].\n-\n-</Tip>\n-\n-Ø§Ù„Ø¢Ù†ØŒ Ù…Ø±Ø± Ø¯ÙØ¹Ø© Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. ÙŠÙ…ÙƒÙ†Ùƒ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…ØµÙÙˆÙØ§Øª ÙƒÙ…Ø§ Ù‡ÙŠ:\n-\n-```py\n->>> tf_outputs = tf_model(tf_batch)\n-```\n-\n-ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø¥Ø®Ø±Ø§Ø¬ Ø§Ù„ØªÙ†Ø´ÙŠØ·Ø§Øª Ø§Ù„Ù†Ù‡Ø§Ø¦ÙŠØ© ÙÙŠ Ø³Ù…Ø© `logits`. Ø·Ø¨Ù‚ Ø¯Ø§Ù„Ø© softmax Ø¹Ù„Ù‰ `logits` Ù„Ø§Ø³ØªØ±Ø¯Ø§Ø¯ Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„Ø§Øª:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n->>> tf_predictions  # doctest: +IGNORE_RESULT\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -322,21 +263,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")\n ```\n </pt>\n-<tf>\n-Ø¨Ù…Ø¬Ø±Ø¯ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø­ÙØ¸Ù‡ Ù…Ø¹ Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ±Ù…ÙŠØ² Ø§Ù„Ø®Ø§Øµ Ø¨Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFPreTrainedModel.save_pretrained`]:\n-\n-```py\n->>> tf_save_directory = \"./tf_save_pretrained\"\n->>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT\n->>> tf_model.save_pretrained(tf_save_directory)\n-```\n-\n-Ø¹Ù†Ø¯Ù…Ø§ ØªÙƒÙˆÙ† Ù…Ø³ØªØ¹Ø¯Ù‹Ø§ Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ØŒ Ø£Ø¹Ø¯ ØªØ­Ù…ÙŠÙ„Ù‡ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFPreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n-```\n-</tf>\n </frameworkcontent>\n \n Ù…Ù† Ø§Ù„Ù…ÙŠØ²Ø§Øª Ø§Ù„Ø±Ø§Ø¦Ø¹Ø© ÙÙŠ ğŸ¤— Transformers Ø§Ù„Ù‚Ø¯Ø±Ø© Ø¹Ù„Ù‰ Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ø¹Ø§Ø¯Ø© ØªØ­Ù…ÙŠÙ„Ù‡ ÙƒÙ†Ù…ÙˆØ°Ø¬ PyTorch Ø£Ùˆ TensorFlow. ÙŠÙ…ÙƒÙ† Ø£Ù† ÙŠØ­ÙˆÙ„ Ù…Ø¹Ø§Ù…Ù„ `from_pt` Ø£Ùˆ `from_tf` Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† Ø¥Ø·Ø§Ø± Ø¹Ù…Ù„ Ø¥Ù„Ù‰ Ø¢Ø®Ø±:\n@@ -351,15 +277,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n-```\n-</tf>\n </frameworkcontent>\n \n \n@@ -385,15 +302,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n >>> my_model = AutoModel.from_config(my_config)\n ```\n </pt>\n-<tf>\n-Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ù†Ù…ÙˆØ°Ø¬ Ù…Ù† ØªÙƒÙˆÙŠÙ†Ùƒ Ø§Ù„Ù…Ø®ØµØµ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModel.from_config`]:\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> my_model = TFAutoModel.from_config(my_config)\n-```\n-</tf>\n </frameworkcontent>\n \n Ø§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø¯Ù„ÙŠÙ„ [Ø¥Ù†Ø´Ø§Ø¡ Ø¨Ù†ÙŠØ© Ù…Ø®ØµØµØ©](./create_a_model) Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø­ÙˆÙ„ Ø¨Ù†Ø§Ø¡ Ø§Ù„ØªÙƒÙˆÙŠÙ†Ø§Øª Ø§Ù„Ù…Ø®ØµØµØ©."
        },
        {
            "sha": "784703a4bbfcb4420f41fc99889d49a7b3dac5f9",
            "filename": "docs/source/ar/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Frun_scripts.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -99,26 +99,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-    \n-- ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ø§Ù„ØªÙˆØ¶ÙŠØ­ÙŠ Ø¨ØªÙ†Ø²ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ÙˆÙ…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ù…Ø³Ø¨Ù‚Ù‹Ø§ Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/).\n-- Ø«Ù… ÙŠÙ‚ÙˆÙ… Ø§Ù„Ù†Øµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠ Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨ÙŠØ§Ù†Ø§Øª Ø¯Ù‚ÙŠÙ‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Keras Ø¹Ù„Ù‰ Ø¨Ù†ÙŠØ© ØªØ¯Ø¹Ù… Ø§Ù„Ù…Ù„Ø®Øµ.\n-- ÙŠÙˆØ¶Ø­ Ø§Ù„Ù…Ø«Ø§Ù„ Ø§Ù„ØªØ§Ù„ÙŠ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ [T5-small](https://huggingface.co/google-t5/t5-small) Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail).\n-- ÙŠØªØ·Ù„Ø¨ Ù†Ù…ÙˆØ°Ø¬ T5 Ù…Ø§Ø¹Ù…Ù„ `source_prefix` Ø¥Ø¶Ø§ÙÙŠØ© Ø¨Ø³Ø¨Ø¨ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙŠ ØªÙ… ØªØ¯Ø±ÙŠØ¨Ù‡ Ø¨Ù‡Ø§. ÙŠØªÙŠØ­ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø© Ù„Ù€ T5 Ù…Ø¹Ø±ÙØ© Ø£Ù† Ù‡Ø°Ù‡ Ù…Ù‡Ù…Ø© Ø§Ù„ØªÙ„Ø®ÙŠØµ.\n-\n-```bash\n-python examples/tensorflow/summarization/run_summarization.py  \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù…ÙˆØ²Ø¹ ÙˆØ§Ù„Ø¯Ù‚Ø© Ø§Ù„Ù…Ø®ØªÙ„Ø·Ø©\n@@ -170,23 +150,6 @@ python xla_spawn.py --num_cores 8 \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-    \n-ØªÙØ¹Ø¯ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ÙØ§Ø¦Ù‚Ø© (TPUs) Ù…ØµÙ…Ù…Ø© Ø®ØµÙŠØµÙ‹Ø§ Ù„ØªØ³Ø±ÙŠØ¹ Ø§Ù„Ø£Ø¯Ø§Ø¡. ØªØ³ØªØ®Ø¯Ù… Ù†ØµÙˆØµ TensorFlow Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ© [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy) Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ ÙˆØ­Ø¯Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ÙØ§Ø¦Ù‚Ø© (TPUs). Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ÙØ§Ø¦Ù‚Ø© (TPU)ØŒ Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ø³Ù… Ù…ÙˆØ±Ø¯ ÙˆØ­Ø¯Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¯Ù‚Ø© Ø§Ù„ÙØ§Ø¦Ù‚Ø© (TPU) Ø¥Ù„Ù‰ Ø­Ø¬Ø© `tpu`.\n-```bash\n-python run_summarization.py  \\\n-    --tpu name_of_tpu_resource \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## ØªØ´ØºÙŠÙ„ Ù†Øµ Ø¨Ø±Ù…Ø¬ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ğŸ¤— Accelerate"
        },
        {
            "sha": "c0788a47a6b5b7a90d5b6b8ed98127c7fd9dafd4",
            "filename": "docs/source/ar/tasks/language_modeling.md",
            "status": "modified",
            "additions": 1,
            "deletions": 104,
            "changes": 105,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Flanguage_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Flanguage_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Flanguage_modeling.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -194,16 +194,6 @@ pip install transformers datasets evaluate\n ```\n \n </pt>\n-<tf>\n-Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…Ø² Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ±Ù…Ø² Ù„Ù„Ø­Ø´ÙˆØŒ ÙˆØ­Ø¯Ø¯ `mlm_probability` Ù„Ø­Ø¬Ø¨ Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø´ÙƒÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¹Ù†Ø¯ ÙƒÙ„ ØªÙƒØ±Ø§Ø± Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n-\n-```py\n->>> from transformers import DataCollatorForLanguageModeling\n-\n->>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")\n-```\n-\n-</tf>\n </frameworkcontent>\n \n ## Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Train)\n@@ -268,73 +258,6 @@ Perplexity: 49.61\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ [Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†ØŒ ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…ØŒ ÙˆØ¨Ø¹Ø¶ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ DistilGPT2 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForCausalLM`]:\n-\n-```py\n->>> from transformers import TFAutoModelForCausalLM\n-\n->>> model = TFAutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n-```\n-\n-Ø­ÙˆÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Ù‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers Ù„Ø¯ÙŠÙ‡Ø§ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ø¬Ø© Ù„Ù„Ø®Ø³Ø§Ø±Ø©!\n-```\n-\n-ÙŠÙ…ÙƒÙ† Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ø¯ÙŠØ¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø¬Ù…Ù‘Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_eli5_clm-model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Ø£Ø®ÙŠØ±Ø§Ù‹ØŒ Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ±ØŒ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n-```\n-\n-Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -393,30 +316,4 @@ Perplexity: 49.61\n [\"Somatic hypermutation allows the immune system to react to drugs with the ability to adapt to a different environmental situation. In other words, a system of 'hypermutation' can help the immune system to adapt to a different environmental situation or in some cases even a single life. In contrast, researchers at the University of Massachusetts-Boston have found that 'hypermutation' is much stronger in mice than in humans but can be found in humans, and that it's not completely unknown to the immune system. A study on how the immune system\"]\n ```\n </pt>\n-<tf>\n-Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒÙ€ TensorFlow tensors:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n->>> inputs = tokenizer(prompt, return_tensors=\"tf\").input_ids\n-```\n-\n-Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ø®Øµ. Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ù„Ø¨Ø§Ø±Ø§Ù…ØªØ±Ø§Øª Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ØŒ Ø±Ø§Ø¬Ø¹ ØµÙØ­Ø© [Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ](../generation_strategies).\n-\n-```py\n->>> from transformers import TFAutoModelForCausalLM\n-\n->>> model = TFAutoModelForCausalLM.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n->>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n-```\n-\n-ÙÙƒ ØªØ±Ù…ÙŠØ²  Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ù†Øµ:\n-\n-```py\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Somatic hypermutation allows the immune system to detect the presence of other viruses as they become more prevalent. Therefore, researchers have identified a high proportion of human viruses. The proportion of virus-associated viruses in our study increases with age. Therefore, we propose a simple algorithm to detect the presence of these new viruses in our samples as a sign of improved immunity. A first study based on this algorithm, which will be published in Science on Friday, aims to show that this finding could translate into the development of a better vaccine that is more effective for']\n-```\n-</tf>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "34c3913224c036aea550eb2f7dbfbbc0add38bb4",
            "filename": "docs/source/ar/tasks/masked_language_modeling.md",
            "status": "modified",
            "additions": 1,
            "deletions": 111,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Fmasked_language_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Fmasked_language_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Fmasked_language_modeling.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -188,16 +188,6 @@ pip install transformers datasets evaluate\n >>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n ```\n </pt>\n-<tf>\n-\n-Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…Ø² Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ±Ù…Ø² Ø§Ù„Ø­Ø´Ùˆ ÙˆØ­Ø¯Ø¯ `mlm_probability` Ù„Ø­Ø¬Ø¨ Ø§Ù„Ø±Ù…ÙˆØ² Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ ÙƒÙ„ Ù…Ø±Ø© ØªÙƒØ±Ø± ÙÙŠÙ‡Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n-\n-```py\n->>> from transformers import DataCollatorForLanguageModeling\n-\n->>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Train)\n@@ -264,73 +254,6 @@ Perplexity: 8.76\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-Ù„ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…Ø­Ø³Ù†ØŒ ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…ØŒ ÙˆØ¨Ø¹Ø¶ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ DistilRoBERTa Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForMaskedLM`]:\n-\n-```py\n->>> from transformers import TFAutoModelForMaskedLM\n-\n->>> model = TFAutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n-```\n-\n-Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Ù‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ Transformers Ù„Ø¯ÙŠÙ‡Ø§ Ø¬Ù…ÙŠØ¹Ù‡Ø§ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªÙƒÙ† ØªØ±ÙŠØ¯ Ø°Ù„Ùƒ:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­Ø¬Ø© Ù„Ù„Ø®Ø³Ø§Ø±Ø©!\n-```\n-\n-ÙŠÙ…ÙƒÙ† Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ø¯ÙŠØ¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_eli5_mlm_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Ø£Ø®ÙŠØ±Ø§Ù‹ØŒ Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ±ØŒ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n-```\n-\n-Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -406,37 +329,4 @@ The Milky Way is a massive galaxy.\n The Milky Way is a small galaxy.\n ```\n </pt>\n-<tf>\n-Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒÙ€ TensorFlow tensors. Ø³ØªØ­ØªØ§Ø¬ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¶Ø¹ Ø±Ù…Ø² `<mask>`:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\")\n->>> mask_token_index = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n-```\n-\n-Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `logits` Ù„Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ù‚Ù†Ø¹:\n-\n-```py\n->>> from transformers import TFAutoModelForMaskedLM\n-\n->>> model = TFAutoModelForMaskedLM.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n->>> logits = model(**inputs).logits\n->>> mask_token_logits = logits[0, mask_token_index, :]\n-```\n-\n-Ø«Ù… Ù‚Ù… Ø¨Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰ ÙˆØ·Ø¨Ø§Ø¹ØªÙ‡Ø§:\n-\n-```py\n->>> top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()\n-\n->>> for token in top_3_tokens:\n-...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))\n-The Milky Way is a spiral galaxy.\n-The Milky Way is a massive galaxy.\n-The Milky Way is a small galaxy.\n-```\n-</tf>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "3a849251c9929621bfbc9ebd667e196655e1dec6",
            "filename": "docs/source/ar/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 0,
            "deletions": 156,
            "changes": 156,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Fmultiple_choice.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -159,48 +159,6 @@ tokenized_swag = swag.map(preprocess_function, batched=True)\n ...         return batch\n ```\n </pt>\n-<tf>\n- \n-```py\n->>> from dataclasses import dataclass\n->>> from transformers.tokenization_utils_base import PreTrainedTokenizerBase, PaddingStrategy\n->>> from typing import Optional, Union\n->>> import tensorflow as tf\n-\n->>> @dataclass\n-... class DataCollatorForMultipleChoice:\n-...     \"\"\"\n-...     Data collator that will dynamically pad the inputs for multiple choice received.\n-...     \"\"\"\n-\n-...     tokenizer: PreTrainedTokenizerBase\n-...     padding: Union[bool, str, PaddingStrategy] = True\n-...     max_length: Optional[int] = None\n-...     pad_to_multiple_of: Optional[int] = None\n-\n-...     def __call__(self, features):\n-...         label_name = \"label\" if \"label\" in features[0].keys() else \"labels\"\n-...         labels = [feature.pop(label_name) for feature in features]\n-...         batch_size = len(features)\n-...         num_choices = len(features[0][\"input_ids\"])\n-...         flattened_features = [\n-...             [{k: v[i] for k, v in feature.items()} for i in range(num_choices)] for feature in features\n-...         ]\n-...         flattened_features = sum(flattened_features, [])\n-\n-...         batch = self.tokenizer.pad(\n-...             flattened_features,\n-...             padding=self.padding,\n-...             max_length=self.max_length,\n-...             pad_to_multiple_of=self.pad_to_multiple_of,\n-...             return_tensors=\"tf\",\n-...         )\n-\n-...         batch = {k: tf.reshape(v, (batch_size, num_choices, -1)) for k, v in batch.items()}\n-...         batch[\"labels\"] = tf.convert_to_tensor(labels, dtype=tf.int64)\n-...         return batch\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Evaluate)\n@@ -284,91 +242,6 @@ tokenized_swag = swag.map(preprocess_function, batched=True)\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ ÙØ±Ø§Ø¬Ø¹ Ø§Ù„Ø¯Ø±Ø³ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…ÙØ­Ø³ÙÙ‘Ù† ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ¨Ø¹Ø¶ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_train_epochs = 2\n->>> total_train_steps = (len(tokenized_swag[\"train\"]) // batch_size) * num_train_epochs\n->>> optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n-```\n-\n-Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ BERT Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForMultipleChoice`]:\n-\n-```py\n->>> from transformers import TFAutoModelForMultipleChoice\n-\n->>> model = TFAutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")\n-```\n-\n-Ø­ÙˆÙ‘Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_swag[\"train\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_swag[\"validation\"],\n-...     shuffle=False,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Ù‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù„Ù…Ù‡Ù…Ø© Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:\n-\n-```py\n->>> model.compile(optimizer=optimizer)  # Ù„Ø§ ØªÙˆØ¬Ø¯ ÙˆØ³ÙŠØ·Ø© Ø®Ø³Ø§Ø±Ø©!\n-```\n-\n-Ø§Ù„Ø®Ø·ÙˆØªØ§Ù† Ø§Ù„Ø£Ø®ÙŠØ±ØªØ§Ù† Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡Ù…Ø§: Ø­Ø³Ø§Ø¨ Ø¯Ù‚Ø© Ø§Ù„ØªÙ†Ø¨Ø¤Ø§ØªØŒ ÙˆØªÙˆÙÙŠØ± Ø·Ø±ÙŠÙ‚Ø© Ù„Ø±ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub. ÙˆÙŠÙ…ÙƒÙ† ØªØ­Ù‚ÙŠÙ‚ Ø°Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Keras](../main_classes/keras_callbacks)\n-\n-Ù…Ø±Ø± Ø¯Ø§Ù„ØªÙƒ `compute_metrics` Ø¥Ù„Ù‰ [`~transformers.KerasMetricCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-Ø­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ùƒ ÙÙŠ [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Ø«Ù… Ù‚Ù… Ø¨ØªØ¶Ù…ÙŠÙ† Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ù…Ø¹Ù‹Ø§:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ø§Ø³ØªØ¯Ø¹Ù[`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø© ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨ ÙˆØ§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2, callbacks=callbacks)\n-```\n-\n-Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -420,33 +293,4 @@ tokenized_swag = swag.map(preprocess_function, batched=True)\n 0\n ```\n </pt>\n-<tf>\n-Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ ÙƒÙ„ Ù…Ø·Ø§Ù„Ø¨Ø© ÙˆØ²ÙˆØ¬ Ø¥Ø¬Ø§Ø¨Ø© Ù…Ø±Ø´Ø­ ÙˆØ£Ø¹Ø¯ Ù…ÙˆØªØ±Ø§Øª TensorFlow:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_swag_model\")\n->>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"tf\", padding=True)\n-```\n-\n-Ù…Ø±Ø± Ù…Ø¯Ø®Ù„Ø§ØªÙƒ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ£Ø¹Ø¯ Ø§Ù„Ù‚ÙŠÙ… logits:\n-\n-```py\n->>> from transformers import TFAutoModelForMultipleChoice\n-\n->>> model = TFAutoModelForMultipleChoice.from_pretrained(\"username/my_awesome_swag_model\")\n->>> inputs = {k: tf.expand_dims(v, 0) for k, v in inputs.items()}\n->>> outputs = model(inputs)\n->>> logits = outputs.logits\n-```\n-\n-Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙØ¦Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£ÙƒØ¨Ø±:\n-\n-```py\n->>> predicted_class = int(tf.math.argmax(logits, axis=-1)[0])\n->>> predicted_class\n-0\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "d86816a13c841eded0ca7592284f42c456a51426",
            "filename": "docs/source/ar/tasks/question_answering.md",
            "status": "modified",
            "additions": 0,
            "deletions": 116,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Fquestion_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Fquestion_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Fquestion_answering.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -176,14 +176,6 @@ pip install transformers datasets evaluate\n >>> data_collator = DefaultDataCollator()\n ```\n </pt>\n-<tf>\n- \n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Train)\n@@ -241,80 +233,6 @@ pip install transformers datasets evaluate\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n- \n-<Tip>\n-\n-Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ ÙØ£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…ÙØ­Ø³ÙÙ‘Ù†ØŒ ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…ØŒ ÙˆØ¨Ø¹Ø¶ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_epochs = 2\n->>> total_train_steps = (len(tokenized_squad[\"train\"]) // batch_size) * num_epochs\n->>> optimizer, schedule = create_optimizer(\n-...     init_lr=2e-5,\n-...     num_warmup_steps=0,\n-...     num_train_steps=total_train_steps,\n-... )\n-```\n-\n-Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ DistilBERT Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForQuestionAnswering`]:\n-\n-```py\n->>> from transformers import TFAutoModelForQuestionAnswering\n-\n->>> model = TFAutoModelForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Ø­ÙˆÙ‘Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_squad[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_squad[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Ù‚Ù… Ø¨ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-Ø¢Ø®Ø± Ø´ÙŠØ¡ ÙŠØ¬Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯Ù‡ Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡Ùˆ ØªÙˆÙÙŠØ± Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Hub. ÙŠÙ…ÙƒÙ† Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ø¯ÙŠØ¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ùƒ Ø§Ù„Ù…Ø¹Ø¬Ù…ÙŠ ÙÙŠ [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_qa_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ø§ØªØµÙ„ Ø¨Ù€ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹Ù‡ÙˆØ¯ØŒ ÙˆÙ…Ø¹Ø§ÙˆØ¯Ø© Ø§Ù„Ø§ØªØµØ§Ù„ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=[callback])\n-```\n-Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n-</tf>\n </frameworkcontent>\n \n \n@@ -395,38 +313,4 @@ pip install transformers datasets evaluate\n '176 billion parameters and can generate text in 46 languages natural languages and 13'\n ```\n </pt>\n-<tf>\n-Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø¹Ø¬Ù…ÙŠ ÙˆØ£Ø¹Ø¯ Ù…ÙˆØªØ±Ø§Øª TensorFlow:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n->>> inputs = tokenizer(question, context, return_tensors=\"tf\")\n-```\n-\n-Ù…Ø±Ø± Ù…Ø¯Ø®Ù„Ø§ØªÙƒ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ£Ø¹Ø¯ `logits`:\n-\n-```py\n->>> from transformers import TFAutoModelForQuestionAnswering\n-\n->>> model = TFAutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\")\n->>> outputs = model(**inputs)\n-```\n-\n-Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø£Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„ Ù…Ù† Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…ÙˆØ¶Ø¹ÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ©:\n-\n-```py\n->>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n->>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n-```\n-\n-Ø§Ø³ØªØ®Ù„Ø§Øµ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©:\n-\n-```py\n->>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n->>> tokenizer.decode(predict_answer_tokens)\n-'176 billion parameters and can generate text in 46 languages natural languages and 13'\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "f73dc634489f88e67a217eebf45d06c6d58d168e",
            "filename": "docs/source/ar/tasks/sequence_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 125,
            "changes": 125,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Fsequence_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Fsequence_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Fsequence_classification.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -101,14 +101,6 @@ tokenized_imdb = imdb.map(preprocess_function, batched=True)\n >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import DataCollatorWithPadding\n-\n->>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ø§Ù„ØªÙ‚ÙŠÙŠÙ…(Evaluate)\n@@ -206,96 +198,6 @@ tokenized_imdb = imdb.map(preprocess_function, batched=True)\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ Ù‚Ù… Ø¨Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†ØŒ ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…ØŒ ÙˆØ¨Ø¹Ø¶ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n-\n-```py\n->>> from transformers import create_optimizer\n->>> import tensorflow as tf\n-\n->>> batch_size = 16\n->>> num_epochs = 5\n->>> batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n->>> total_train_steps = int(batches_per_epoch * num_epochs)\n->>> optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n-```\n-\n-Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ DistilBERT Ù…Ø¹ [`TFAutoModelForSequenceClassification`] Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©ØŒ ÙˆØªØ¹ÙŠÙŠÙ†Ø§Øª Ø§Ù„ØªØ³Ù…ÙŠØ§Øª:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\n-...     \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n-... )\n-```\n-\n-Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_imdb[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_imdb[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Ù‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers Ù„Ø¯ÙŠÙ‡Ø§ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-Ø¢Ø®Ø± Ø£Ù…Ø±ÙŠÙ† ÙŠØ¬Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯Ù‡Ù…Ø§ Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡Ùˆ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø© Ù…Ù† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§ØªØŒ ÙˆØªÙˆÙÙŠØ± Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Hub. ÙŠØªÙ… Ø°Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Keras callbacks](../main_classes/keras_callbacks).\n-\n-Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø¯Ø§Ù„Ø© `compute_metrics` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ [`~transformers.KerasMetricCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-Ø­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆØ§Ù„Ù…Ø¬Ø²Ø¦ Ø§Ù„Ù„ØºÙˆÙŠ ÙÙŠ [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Ø«Ù… Ø§Ø¬Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ù…Ø¹Ù‹Ø§:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨Ø§ØªØŒ ÙˆØ§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§ØªÙƒ Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n-```\n-\n-Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -357,31 +259,4 @@ tokenized_imdb = imdb.map(preprocess_function, batched=True)\n 'POSITIVE'\n ```\n </pt>\n-<tf>\n-Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ ØªÙ†Ø³ÙŠÙ‚Ø§Øª TensorFlow:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\")\n-```\n-\n-Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ù…Ø¯Ø®Ù„Ø§ØªÙƒ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `logits`:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n->>> logits = model(**inputs).logits\n-```\n-\n-Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙØ¦Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… `id2label` Ù„ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ù†ØµÙŠ:\n-\n-```py\n->>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n->>> model.config.id2label[predicted_class_id]\n-'POSITIVE'\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "45c99767483fc08ce5c3b4d9f56b370f92e58687",
            "filename": "docs/source/ar/tasks/summarization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 118,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Fsummarization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Fsummarization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Fsummarization.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -127,14 +127,6 @@ pip install transformers datasets evaluate rouge_score\n >>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import DataCollatorForSeq2Seq\n-\n->>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Evaluate)\n@@ -227,89 +219,6 @@ pip install transformers datasets evaluate rouge_score\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ ÙØ£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…ÙØ­Ø³ÙÙ‘Ù† ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù… ÙˆØ¨Ø¹Ø¶ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ T5 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForSeq2SeqLM`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n-```\n-\n-Ø­ÙˆÙ‘Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_billsum[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     tokenized_billsum[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Ù‚Ù… Ø¨ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers Ù„Ø¯ÙŠÙ‡Ø§ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠÙ‹Ø§ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø³Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªÙƒÙ† ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-Ø¢Ø®Ø± Ø´ÙŠØ¦ÙŠÙ† ÙŠØ¬Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯Ù‡Ù…Ø§ Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡Ù…Ø§ Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø© ROUGE Ù…Ù† Ø§Ù„ØªÙ†Ø¨Ø¤Ø§ØªØŒ ÙˆØªÙˆÙÙŠØ± Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Hub. ÙŠØªÙ… ÙƒÙ„Ø§Ù‡Ù…Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Keras](../main_classes/keras_callbacks).\n-\n-Ù…Ø±Ø± Ø¯Ø§Ù„Ø© `compute_metrics` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ [`~transformers.KerasMetricCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)\n-```\n-\n-Ø­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…ÙØ­Ù„ÙÙ‘Ù„Ùƒ Ø§Ù„Ù„ØºÙˆÙŠ ÙÙŠ [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_billsum_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Ø«Ù… Ø§Ø¬Ù…Ø¹ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§ØªÙƒ Ù…Ø¹Ù‹Ø§:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ø§ØªØµÙ„ Ø¨Ù€ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø© ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨ ÙˆØ§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§ØªÙƒ Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n-```\n-\n-Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -368,30 +277,4 @@ pip install transformers datasets evaluate rouge_score\n 'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'\n ```\n </pt>\n-<tf>\n-Ù‚Ø³Ù… Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø¹ `input_ids` ÙƒØªÙ†Ø³ÙˆØ±Ø§Øª TensorFlow:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_billsum_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n-```\n-\n-Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªÙ„Ø®ÙŠØµ. Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ØŒ Ø±Ø§Ø¬Ø¹ ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª [ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ](../main_classes/text_generation).\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_billsum_model\")\n->>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n-```\n-\n-ÙÙƒ ØªØ´ÙÙŠØ± Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ù†Øµ:\n-\n-```py\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'\n-```\n-</tf>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "fe8ff5116adb378b074297a0b3e5b8e72284a5bb",
            "filename": "docs/source/ar/tasks/token_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 144,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Ftoken_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Ftoken_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Ftoken_classification.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -159,13 +159,6 @@ pip install transformers datasets evaluate seqeval\n >>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n ```\n </pt>\n-<tf>\n-```py\n->>> from transformers import DataCollatorForTokenClassification\n-\n->>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ø§Ù„ØªÙ‚ÙŠÙŠÙ…(Evaluate)\n@@ -303,99 +296,6 @@ pip install transformers datasets evaluate seqeval\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-Ù„Ù„ØªØ¹Ø¯ÙŠÙ„ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…Ø­Ø³Ù†ØŒ ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…ØŒ ÙˆØ¨Ø¹Ø¶ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_train_epochs = 3\n->>> num_train_steps = (len(tokenized_wnut[\"train\"]) // batch_size) * num_train_epochs\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=2e-5,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=0.01,\n-...     num_warmup_steps=0,\n-... )\n-```\n-\n-Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ DistilBERT Ù…Ø¹ [`TFAutoModelForTokenClassification`] Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ Ø¹Ø¯Ø¯ Ø§Ù„ØªØ³Ù…ÙŠØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©ØŒ ÙˆØªØ®Ø·ÙŠØ·Ø§Øª Ø§Ù„ØªØ³Ù…ÙŠØ§Øª:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\n-...     \"distilbert/distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n-... )\n-```\n-\n-Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ù…Ø¹ [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_wnut[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_wnut[\"validation\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Ù‡ÙŠÙ‘Ø¦ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ Transformers ØªØªØ¶Ù…Ù† Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ù…Ø±ØªØ¨Ø·Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-Ø¢Ø®Ø± Ø£Ù…Ø±ÙŠÙ† ÙŠØ¬Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯Ù‡Ù…Ø§ Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡Ùˆ Ø­Ø³Ø§Ø¨ Ø¯Ø±Ø¬Ø§Øª seqeval Ù…Ù† Ø§Ù„ØªÙ†Ø¨Ø¤Ø§ØªØŒ ÙˆØªÙˆÙÙŠØ± Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Hub. ÙŠØªÙ… Ø°Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Keras callbacks](../main_classes/keras_callbacks).\n-\n-Ù…Ø±Ø± Ø¯Ø§Ù„Ø© `compute_metrics` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ [`~transformers.KerasMetricCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-Ø­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆØ§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù„ØºÙˆÙŠ ÙÙŠ [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_wnut_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Ø«Ù… Ø¬Ù…Ù‘Ø¹ callbacks Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù…Ø¹Ù‹Ø§:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ø§Ù„Ø¢Ù† Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨Ø§ØªØŒ Ùˆcallbacks Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n-```\n-\n-Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -503,48 +403,4 @@ pip install transformers datasets evaluate seqeval\n  'O']\n ```\n </pt>\n-<tf>\n-Ù‚Ø³Ù‘Ù… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ÙˆØ£Ø±Ø¬Ø¹ Ø§Ù„Ù…ÙÙˆØªÙ‘Ø±Ø§Øª Ø¨ TensorFlow:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\")\n-```\n-\n-Ù…Ø±Ø± Ù…Ø¯Ø®Ù„Ø§ØªÙƒ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø­ØµÙ„ Ø¹Ù„Ù‰ `logits`:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n->>> logits = model(**inputs).logits\n-```\n-\n-Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙØ¦Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø¬Ø¯ÙˆÙ„ `id2label` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ ØªØ³Ù…ÙŠØ© Ù†ØµÙŠØ©:\n-\n-```py\n->>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)\n->>> predicted_token_class = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]\n->>> predicted_token_class\n-['O',\n- 'O',\n- 'B-location',\n- 'I-location',\n- 'B-group',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'B-location',\n- 'B-location',\n- 'O',\n- 'O']\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "3198d4c36871fe62dcf6100753df0930d8e2ba1c",
            "filename": "docs/source/ar/tasks/translation.md",
            "status": "modified",
            "additions": 1,
            "deletions": 118,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Ftranslation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftasks%2Ftranslation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Ftranslation.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -122,14 +122,6 @@ pip install transformers datasets evaluate sacrebleu\n >>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import DataCollatorForSeq2Seq\n-\n->>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ø§Ù„ØªÙ‚ÙŠÙŠÙ… (Evaluate)\n@@ -234,89 +226,6 @@ pip install transformers datasets evaluate sacrebleu\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ù…Ø¹ØªØ§Ø¯Ù‹Ø§ Ø¹Ù„Ù‰ Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ ÙØ£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!\n-\n-</Tip>\n-Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…ÙØ­Ø³ÙÙ‘Ù† ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ ØªØ¹Ù„Ù… ÙˆØ¨Ø¹Ø¶ Ø§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ÙØ§Ø¦Ù‚Ø© Ù„Ù„ØªØ¯Ø±ÙŠØ¨:\n-\n-```py\n->>> from transformers import AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ T5 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForSeq2SeqLM`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n-```\n-\n-Ø­ÙˆÙ‘Ù„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_books[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     tokenized_books[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-Ù‚Ù… Ø¨ØªÙƒÙˆÙŠÙ† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers ØªØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-Ø¢Ø®Ø± Ø´ÙŠØ¦ÙŠÙ† ÙŠØ¬Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯Ù‡Ù…Ø§ Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡Ù…Ø§ Ø­Ø³Ø§Ø¨ Ù…Ù‚ÙŠØ§Ø³ SacreBLEU Ù…Ù† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§ØªØŒ ÙˆØªÙˆÙÙŠØ± Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Hub. ÙŠØªÙ… ÙƒÙ„Ø§Ù‡Ù…Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Keras](../main_classes/keras_callbacks).\n-\n-Ù…Ø±Ø± Ø¯Ø§Ù„Ø© `compute_metrics` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ [`~transformers.KerasMetricCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_test_set)\n-```\n-\n-Ø­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ùƒ Ø§Ù„Ù„ØºÙˆÙŠ ÙÙŠ [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_opus_books_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-Ø«Ù… Ø§Ø¬Ù…Ø¹ Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§ØªÙƒ Ù…Ø¹Ù‹Ø§:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ø§ØªØµÙ„ Ø¨Ù€ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø© ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨ ÙˆØ§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§ØªÙƒ Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n-```\n-\n-Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -378,30 +287,4 @@ pip install transformers datasets evaluate sacrebleu\n 'Les lignÃ©es partagent des ressources avec des bactÃ©ries enfixant l'azote.'\n ```\n </pt>\n-<tf>\n-Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒÙ…ÙˆØªØ±Ø§Øª TensorFlow:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_opus_books_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n-```\n-\n-Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„ØªØ±Ø¬Ù…Ø©. Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ù„Ù…Ø¹Ù„Ù…Ø§Øª Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ØŒ ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª [ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†ØµÙˆØµ](../main_classes/text_generation).\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"username/my_awesome_opus_books_model\")\n->>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n-```\n-\n-ÙÙƒ ØªØ´ÙÙŠØ± Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ù†Øµ:\n-\n-```py\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'Les lugumes partagent les ressources avec des bactÃ©ries fixatrices d'azote.'\n-```\n-</tf>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "dee48ac7822cdc3934f8e8fea58a66f7b90de90f",
            "filename": "docs/source/ar/training.md",
            "status": "modified",
            "additions": 1,
            "deletions": 111,
            "changes": 112,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftraining.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Far%2Ftraining.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftraining.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -140,116 +140,6 @@\n >>> trainer.train()\n ```\n </pt>\n-<tf>\n-<a id='keras'></a>\n-\n-<Youtube id=\"rnTGBy2ax1c\"/>\n-\n-## ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ TensorFlow Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Keras\n-\n-ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…Ø§Ø°Ø¬ ğŸ¤— Transformers ÙÙŠ TensorFlow Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª Keras!\n-\n-### ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù„Ù€ Keras\n-\n-Ø¹Ù†Ø¯Ù…Ø§ ØªØ±ÙŠØ¯ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ğŸ¤— Transformers Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙˆØ§Ø¬Ù‡Ø© Ø¨Ø±Ù…Ø¬Ø© ØªØ·Ø¨ÙŠÙ‚Ø§Øª KerasØŒ ÙØ£Ù†Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ ÙŠÙÙ‡Ù…Ù‡\n-Keras. Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ØµØºÙŠØ±Ø©ØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ Ø¨Ø¨Ø³Ø§Ø·Ø© ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ§Øª NumPy ÙˆØ¥Ø±Ø³Ø§Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Keras.\n-Ø¯Ø¹ÙˆÙ†Ø§ Ù†Ø¬Ø±Ø¨ Ø°Ù„Ùƒ Ø£ÙˆÙ„Ø§Ù‹ Ù‚Ø¨Ù„ Ø£Ù† Ù†Ù‚ÙˆÙ… Ø¨Ø£ÙŠ Ø´ÙŠØ¡ Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ù‹Ø§.\n-\n-Ø£ÙˆÙ„Ø§Ù‹ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª. Ø³Ù†Ø³ØªØ®Ø¯Ù… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª CoLA Ù…Ù† Ù…Ø¹ÙŠØ§Ø± [GLUE benchmark](https://huggingface.co/datasets/glue)ØŒ\n-Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù†Ù‡ Ù…Ù‡Ù…Ø© ØªØµÙ†ÙŠÙ Ù†Øµ Ø«Ù†Ø§Ø¦ÙŠ Ø¨Ø³ÙŠØ·Ø©ØŒ ÙˆØ³Ù†Ø£Ø®Ø° ÙÙ‚Ø· Ù‚Ø³Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¢Ù†.\n-\n-```py\n-from datasets import load_dataset\n-\n-dataset = load_dataset(\"glue\"ØŒ \"cola\")\n-dataset = dataset [\"train\"] # Ø®Ø° ÙÙ‚Ø· Ù‚Ø³Ù… Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø¢Ù†\n-```\n-\n-Ø¨Ø¹Ø¯ Ø°Ù„ÙƒØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ø£Ø¯Ø§Ø© Ø§Ù„Ù…ÙØ¬Ø²Ù‘Ø¦ Ø§Ù„Ù„ØºÙˆÙŠ ÙˆÙ‚Ù… Ø¨ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ…ØµÙÙˆÙØ§Øª NumPy. Ù„Ø§Ø­Ø¸ Ø£Ù† Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ù‡ÙŠ Ø¨Ø§Ù„ÙØ¹Ù„ Ù‚Ø§Ø¦Ù…Ø© Ù…Ù† 0 Ùˆ 1ØŒ\n-Ù„Ø°Ø§ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¨Ø¨Ø³Ø§Ø·Ø© ØªØ­ÙˆÙŠÙ„ Ø°Ù„Ùƒ Ù…Ø¨Ø§Ø´Ø±Ø© Ø¥Ù„Ù‰ Ù…ØµÙÙˆÙØ© NumPy Ø¨Ø¯ÙˆÙ† ØªØ±Ù…ÙŠØ²!\n-\n-```py\n-from transformers import AutoTokenizer\n-import numpy as np\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-tokenized_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n-# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n-tokenized_data = dict(tokenized_data)\n-\n-labels = np.array(dataset[\"label\"])  # Label is already an array of 0 and 1\n-```\n-\n-Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù…ÙŠØ¹ ÙˆØªÙ†Ø§Ø³Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. Ù„Ø§Ø­Ø¸ Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ Transformers ØªØ­ØªÙˆÙŠ Ø¬Ù…ÙŠØ¹Ù‡Ø§ Ø¹Ù„Ù‰ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ Ù„Ø°Ø§ ÙØ£Ù†Øª Ù„Ø³Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:\n-\n-```py\n-from transformers import TFAutoModelForSequenceClassification\n-from tensorflow.keras.optimizers import Adam\n-\n-# ØªØ­Ù…ÙŠÙ„ ÙˆØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø®Ø§Øµ Ø¨Ù†Ø§\n-model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n-# Ù…Ø¹Ø¯Ù„Ø§Øª Ø§Ù„ØªØ¹Ù„Ù… Ø§Ù„Ù…Ù†Ø®ÙØ¶Ø© Ø£ÙØ¶Ù„ ØºØ§Ù„Ø¨Ù‹Ø§ Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ø¯Ù‚ÙŠÙ‚Ø©\n-model.compile(optimizer=Adam(3e-5)) # Ù„Ø§ ØªÙˆØ¬Ø¯ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø©!\n-\n-model.fit(tokenized_data, labels)\n-```\n-\n-<Tip>\n-\n-Ø£Ù†Øª Ù„Ø³Øª Ù…Ø¶Ø·Ø±Ù‹Ø§ Ù„ØªÙ…Ø±ÙŠØ± Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø¥Ù„Ù‰ Ù†Ù…Ø§Ø°Ø¬Ùƒ Ø¹Ù†Ø¯ ØªØ¬Ù…ÙŠØ¹Ù‡Ø§! ØªØ®ØªØ§Ø± Ù†Ù…Ø§Ø°Ø¬ Hugging Face ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§\n-Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ù…Ù†Ø§Ø³Ø¨Ø© Ù„Ù…Ù‡Ù…ØªÙ‡Ø§ ÙˆÙ‡Ù†Ø¯Ø³Ø© Ù†Ù…ÙˆØ°Ø¬Ù‡Ø§ Ø¥Ø°Ø§ ØªÙØ±ÙƒØª Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø¬Ø© ÙØ§Ø±ØºØ©. ÙŠÙ…ÙƒÙ†Ùƒ Ø¯Ø§Ø¦Ù…Ù‹Ø§\n-ØªØ¬Ø§ÙˆØ² Ø°Ù„Ùƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ø¯ÙŠØ¯ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø¨Ù†ÙØ³Ùƒ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ Ø°Ù„Ùƒ!\n-\n-</Tip>\n-\n-ÙŠØ¹Ù…Ù„ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‡Ø¬ Ø¨Ø´ÙƒÙ„ Ø±Ø§Ø¦Ø¹ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØµØºÙŠØ±Ø©ØŒ ÙˆÙ„ÙƒÙ† Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø£ÙƒØ¨Ø±ØŒ ÙÙ‚Ø¯ ØªØ¬Ø¯ Ø£Ù†Ù‡ ÙŠØµØ¨Ø­ Ù…Ø´ÙƒÙ„Ø©. Ù„Ù…Ø§Ø°Ø§ØŸ\n-Ù„Ø£Ù† Ø§Ù„Ù…ØµÙÙˆÙØ© Ø§Ù„Ù…Ø±Ù…Ø²Ø© ÙˆØ§Ù„ØªØµÙ†ÙŠÙØ§Øª ÙŠØ¬Ø¨ Ø£Ù† ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„Ù‡Ø§ Ø¨Ø§Ù„ÙƒØ§Ù…Ù„ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©ØŒ ÙˆÙ„Ø£Ù† NumPy Ù„Ø§ ÙŠØªØ¹Ø§Ù…Ù„ Ù…Ø¹\n-Ø§Ù„Ù…ØµÙÙˆÙØ§Øª\"ØºÙŠØ± Ø§Ù„Ù…Ù†ØªØ¸Ù…Ø©\"ØŒ Ù„Ø°Ø§ Ø­Ø´Ùˆ ÙƒÙ„ Ø¹ÙŠÙ†Ø©  Ø¥Ù„Ù‰ Ø·ÙˆÙ„ Ø£Ø·ÙˆÙ„ Ø¹ÙŠÙ†Ø© ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§. Ø³ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ Ø²ÙŠØ§Ø¯Ø© Ø­Ø¬Ù… Ø§Ù„Ù…ØµÙÙˆÙØ© Ù„Ø¯ÙŠÙƒØŒ ÙˆØ³ØªØ¨Ø·Ø¦ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø²Ø§Ø¦Ø¯Ù‡ Ù…Ù† Ø¹Ù…Ù„ÙŠØ© Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø£ÙŠØ¶Ù‹Ø§!\n-\n-### ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙƒÙ€ tf.data.Dataset\n-\n-Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ ØªØ¬Ù†Ø¨ Ø¥Ø¨Ø·Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§ØªÙƒ ÙƒÙ€ `tf.data.Dataset` Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„Ùƒ. Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù†Ù‡ ÙŠÙ…ÙƒÙ†Ùƒ ÙƒØªØ§Ø¨Ø© Ø®Ø· Ø£Ù†Ø§Ø¨ÙŠØ¨ `tf.data` Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø¥Ø°Ø§ ÙƒÙ†Øª ØªØ±ÙŠØ¯ØŒ Ø¥Ù„Ø§ Ø£Ù† Ù„Ø¯ÙŠÙ†Ø§ Ø·Ø±ÙŠÙ‚ØªÙŠÙ† Ù…Ø®ØªØµØ±ØªÙŠÙ† Ù„Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ:\n-- [`~TFPreTrainedModel.prepare_tf_dataset`]: Ù‡Ø°Ù‡ Ù‡ÙŠ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø§Ù„ØªÙŠ Ù†ÙˆØµÙŠ Ø¨Ù‡Ø§ ÙÙŠ Ù…Ø¹Ø¸Ù… Ø§Ù„Ø­Ø§Ù„Ø§Øª. Ù†Ø¸Ø±Ù‹Ø§ Ù„Ø£Ù†Ù‡ Ø·Ø±ÙŠÙ‚Ø©\n-Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙÙŠÙ…ÙƒÙ†Ù‡ ÙØ­Øµ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØ­Ø¯ÙŠØ¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ù‚Ø§Ø¨Ù„Ø© Ù„Ù„Ø§Ø³ØªØ®Ø¯Ø§Ù… ÙƒÙ…Ø¯Ø®Ù„Ø§Øª Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ØŒ\n-ÙˆØ§Ø³ØªØ¨Ø¹Ø§Ø¯ Ø§Ù„Ø£Ø¹Ù…Ø¯Ø© Ø§Ù„Ø£Ø®Ø±Ù‰ Ù„Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª Ø£Ø¨Ø³Ø· ÙˆØ£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø©.\n-- [`~datasets.Dataset.to_tf_dataset`]: Ù‡Ø°Ù‡ Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© Ø£ÙƒØ«Ø± Ø£Ø³Ø§Ø³ÙŠØ©ØŒ ÙˆÙ‡ÙŠ Ù…ÙÙŠØ¯Ø© Ø¹Ù†Ø¯Ù…Ø§ ØªØ±ÙŠØ¯ Ø§Ù„ØªØ­ÙƒÙ… Ø¨Ø¯Ù‚Ø© ÙÙŠ ÙƒÙŠÙÙŠØ©\n-Ø¥Ù†Ø´Ø§Ø¡ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨ÙƒØŒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ø¯ÙŠØ¯ Ø£Ø¹Ù…Ø¯Ø© `columns` Ùˆ `label_cols` Ø§Ù„Ù…Ø­Ø¯Ø¯Ø© Ø§Ù„ØªÙŠ Ø³ÙŠØªÙ… ØªØ¶Ù…ÙŠÙ†Ù‡Ø§.\n-\n-Ù‚Ø¨Ù„ Ø£Ù† ØªØªÙ…ÙƒÙ† Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~TFPreTrainedModel.prepare_tf_dataset`]ØŒ Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø¥Ø¶Ø§ÙØ© Ù…Ø®Ø±Ø¬Ø§Øª Ø§Ù„Ù…ÙØ¬Ø²Ø¦ Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙƒØ£Ø¹Ù…Ø¯Ø©ØŒ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ ÙÙŠ\n-Ø¹ÙŠÙ†Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø§Ù„ØªØ§Ù„ÙŠØ©:\n-\n-```py\n-def tokenize_dataset (data):\n-# Ø³ØªØªÙ… Ø¥Ø¶Ø§ÙØ© Ù…ÙØ§ØªÙŠØ­ Ø§Ù„Ù‚Ø§Ù…ÙˆØ³ Ø§Ù„Ø°ÙŠ ØªÙ…Øª Ø¥Ø¹Ø§Ø¯ØªÙ‡ ÙƒØ£Ø¹Ù…Ø¯Ø© Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª\n-return tokenizer(data[\"text\"])\n-\n-\n-dataset = dataset.map(tokenize_dataset)\n-```\n-\n-ØªØ°ÙƒØ± Ø£Ù† Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Hugging Face ÙŠØªÙ… ØªØ®Ø²ÙŠÙ†Ù‡Ø§ Ø¹Ù„Ù‰ Ø§Ù„Ù‚Ø±Øµ Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ Ù„Ø°Ø§ ÙÙ„Ù† ÙŠØ¤Ø¯ÙŠ Ø°Ù„Ùƒ Ø¥Ù„Ù‰ ØªØ¶Ø®ÙŠÙ… Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ù„Ø¯ÙŠÙƒ! Ø¨Ù…Ø¬Ø±Ø¯ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø£Ø¹Ù…Ø¯Ø©ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø¨Ø« Ø§Ù„Ø¯ÙØ¹Ø§Øª Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙˆØ¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¥Ù„Ù‰ ÙƒÙ„ Ø¯ÙØ¹Ø©ØŒ Ù…Ù…Ø§ ÙŠÙ‚Ù„Ù„ Ø¨Ø´ÙƒÙ„ ÙƒØ¨ÙŠØ± Ù…Ù† Ø¹Ø¯Ø¯ Ø±Ù…ÙˆØ² Ø§Ù„ØªØ±Ù‚ÙŠÙ… Ù…Ù‚Ø§Ø±Ù†Ø© Ø¨ØªØ±Ù…ÙŠØ² Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§.\n-\n-\n-```py\n->>> tf_dataset = model.prepare_tf_dataset(dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer)\n-```\n-\n-Ù„Ø§Ø­Ø¸ Ø£Ù†Ù‡ ÙÙŠ Ø¹ÙŠÙ†Ø© Ø§Ù„ØªØ¹Ù„ÙŠÙ…Ø§Øª Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ø£Ø¹Ù„Ø§Ù‡ØŒ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…ÙØ¬Ø²Ø¦ Ø§Ù„Ù„ØºÙˆÙŠ Ø¥Ù„Ù‰ `prepare_tf_dataset` Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† Ø­Ø´Ùˆ Ø§Ù„Ø¯ÙÙØ¹Ø§Øª Ø¨Ø´ÙƒÙ„ ØµØ­ÙŠØ­ Ø£Ø«Ù†Ø§Ø¡ ØªØ­Ù…ÙŠÙ„Ù‡Ø§.\n-Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹ÙŠÙ†Ø§Øª ÙÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¨Ù†ÙØ³ Ø§Ù„Ø·ÙˆÙ„ ÙˆÙ„Ù… ÙŠÙƒÙ† Ø§Ù„ØªØ±Ù…ÙŠØ² Ø¶Ø±ÙˆØ±ÙŠÙ‹Ø§ØŒ ÙÙŠÙ…ÙƒÙ†Ùƒ ØªØ®Ø·ÙŠ Ù‡Ø°Ø§ Ø§Ù„Ù…Ø¹Ø§Ù…Ù„.\n-Ø¥Ø°Ø§ ÙƒÙ†Øª Ø¨Ø­Ø§Ø¬Ø© Ø¥Ù„Ù‰ Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø´ÙŠØ¡ Ø£ÙƒØ«Ø± ØªØ¹Ù‚ÙŠØ¯Ù‹Ø§ Ù…Ù† Ù…Ø¬Ø±Ø¯ ØªØ±Ù…ÙŠØ² Ø§Ù„Ø¹ÙŠÙ†Ø§Øª (Ø¹Ù„Ù‰ Ø³Ø¨ÙŠÙ„ Ø§Ù„Ù…Ø«Ø§Ù„ØŒ Ø¥ÙØ³Ø§Ø¯ Ø§Ù„Ø±Ù…ÙˆØ² Ù„Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ù…ÙÙ‚Ù†Ø¹Ø©)ØŒ\n-ÙÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ø§Ù…Ù„ `collate_fn` Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„Ùƒ Ù„ØªÙ…Ø±ÙŠØ± Ø¯Ø§Ù„Ø© ÙŠØªÙ… Ø§Ø³ØªØ¯Ø¹Ø§Ø¤Ù‡Ø§ Ù„ØªØ­ÙˆÙŠÙ„\n-Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¹ÙŠÙ†Ø§Øª Ø¥Ù„Ù‰ Ø¯ÙØ¹Ø© ÙˆØªØ·Ø¨ÙŠÙ‚ Ø£ÙŠ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© ØªØ±ÙŠØ¯Ù‡Ø§. Ø±Ø§Ø¬Ø¹ Ø£Ù…Ø«Ù„Ø©Ù†Ø§ [examples](https://github.com/huggingface/transformers/tree/main/examples) Ø£Ùˆ\n-[Ø¯ÙØ§ØªØ± Ø§Ù„Ù…Ù„Ø§Ø­Ø¸Ø§Øª](https://huggingface.co/docs/transformers/notebooks) Ù„Ø±Ø¤ÙŠØ© Ù‡Ø°Ø§ Ø§Ù„Ù†Ù‡Ø¬ ÙÙŠ Ø§Ù„Ø¹Ù…Ù„.\n-\n-Ø¨Ù…Ø¬Ø±Ø¯ Ø¥Ù†Ø´Ø§Ø¡ `tf.data.Dataset`ØŒ ÙŠÙ…ÙƒÙ†Ùƒ ØªØ¬Ù…ÙŠØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØªÙ†Ø§Ø³Ø¨Ù‡ ÙƒÙ…Ø§ Ù‡Ùˆ Ø§Ù„Ø­Ø§Ù„ Ù…Ù† Ù‚Ø¨Ù„:\n-\n-```py\n-model.compile(optimizer=Adam(3e-5))  # No loss argument!\n-\n-model.fit(tf_dataset)\n-```\n-\n-</tf>\n </frameworkcontent>\n \n <a id='pytorch_native'></a>\n@@ -409,4 +299,4 @@ torch.cuda.empty_cache()\n - [ğŸ¤— Ø£Ù…Ø«Ù„Ø© Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª](https://github.com/huggingface/transformers/tree/main/examples) ØªØªØ¶Ù…Ù†\n   Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„Ø¨Ø±Ù…Ø¬ÙŠØ© Ù„ØªØ¯Ø±ÙŠØ¨ Ù…Ù‡Ø§Ù… NLP Ø§Ù„Ø´Ø§Ø¦Ø¹Ø© ÙÙŠ PyTorch ÙˆTensorFlow.\n \n-- [ğŸ¤— Ø¯ÙØ§ØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª](notebooks) ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¯ÙØ§ØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ù‡Ù…Ø© Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ PyTorch ÙˆTensorFlow.\n\\ No newline at end of file\n+- [ğŸ¤— Ø¯ÙØ§ØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ø§Ù„Ù…Ø­ÙˆÙ„Ø§Øª](notebooks) ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¯ÙØ§ØªØ± Ù…Ù„Ø§Ø­Ø¸Ø§Øª Ù…Ø®ØªÙ„ÙØ© Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ù„Ù…Ù‡Ù…Ø© Ù…Ø­Ø¯Ø¯Ø© ÙÙŠ PyTorch ÙˆTensorFlow."
        },
        {
            "sha": "178267049a4b62ecc72e58097966834156d081f9",
            "filename": "docs/source/de/autoclass_tutorial.md",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Fautoclass_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Fautoclass_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fautoclass_tutorial.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -109,23 +109,4 @@ TensorFlow- und Flax-Checkpoints sind nicht betroffen und kÃ¶nnen in PyTorch-Arc\n \n Im Allgemeinen empfehlen wir die Verwendung der Klasse \"AutoTokenizer\" und der Klasse \"AutoModelFor\", um trainierte Instanzen von Modellen zu laden. Dadurch wird sichergestellt, dass Sie jedes Mal die richtige Architektur laden. Im nÃ¤chsten [Tutorial] (Vorverarbeitung) erfahren Sie, wie Sie Ihren neu geladenen Tokenizer, Feature Extractor und Prozessor verwenden, um einen Datensatz fÃ¼r die Feinabstimmung vorzuverarbeiten.\n </pt>\n-<tf>\n-Mit den Klassen `TFAutoModelFor` schlieÃŸlich kÃ¶nnen Sie ein vortrainiertes Modell fÃ¼r eine bestimmte Aufgabe laden (siehe [hier](model_doc/auto) fÃ¼r eine vollstÃ¤ndige Liste der verfÃ¼gbaren Aufgaben). Laden Sie zum Beispiel ein Modell fÃ¼r die Sequenzklassifikation mit [`TFAutoModelForSequenceClassification.from_pretrained`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Sie kÃ¶nnen denselben PrÃ¼fpunkt problemlos wiederverwenden, um eine Architektur fÃ¼r eine andere Aufgabe zu laden:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Im Allgemeinen empfehlen wir, die Klasse \"AutoTokenizer\" und die Klasse \"TFAutoModelFor\" zu verwenden, um vortrainierte Instanzen von Modellen zu laden. Dadurch wird sichergestellt, dass Sie jedes Mal die richtige Architektur laden. Im nÃ¤chsten [Tutorial] (Vorverarbeitung) erfahren Sie, wie Sie Ihren neu geladenen Tokenizer, Feature Extractor und Prozessor verwenden, um einen Datensatz fÃ¼r die Feinabstimmung vorzuverarbeiten.\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "dfa2c7f785bc3016a2ea84c3718db70ce19593b0",
            "filename": "docs/source/de/model_sharing.md",
            "status": "modified",
            "additions": 1,
            "deletions": 44,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Fmodel_sharing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Fmodel_sharing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fmodel_sharing.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -88,28 +88,6 @@ Geben Sie `from_tf=True` an, um einen PrÃ¼fpunkt von TensorFlow nach PyTorch zu\n >>> pt_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n ```\n </pt>\n-<tf>\n-Geben Sie `from_pt=True` an, um einen PrÃ¼fpunkt von PyTorch nach TensorFlow zu konvertieren:\n-\n-```py\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\n-```\n-\n-Dann kÃ¶nnen Sie Ihr neues TensorFlow-Modell mit seinem neuen Checkpoint speichern:\n-\n-```py\n->>> tf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n-```\n-</tf>\n-<jax>\n-Wenn ein Modell in Flax verfÃ¼gbar ist, kÃ¶nnen Sie auch einen Kontrollpunkt von PyTorch nach Flax konvertieren:\n-\n-```py\n->>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\n-...     \"path/to/awesome-name-you-picked\", from_pt=True\n-... )\n-```\n-</jax>\n </frameworkcontent>\n \n ## Ein Modell wÃ¤hrend des Trainings hochladen\n@@ -142,27 +120,6 @@ Nach der Feinabstimmung Ihres Modells rufen Sie [`~transformers.Trainer.push_to_\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-Geben Sie ein Modell mit [`PushToHubCallback`] an den Hub weiter. In der [`PushToHubCallback`] Funktion, fÃ¼gen Sie hinzu:\n-\n-- Ein Ausgabeverzeichnis fÃ¼r Ihr Modell.\n-- Einen Tokenizer.\n-- Die `hub_model_id`, die Ihr Hub-Benutzername und Modellname ist.\n-\n-```py\n->>> from transformers import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\n-... )\n-```\n-\n-FÃ¼gen Sie den Callback zu [`fit`](https://keras.io/api/models/model_training_apis/) hinzu, und ğŸ¤— Transformers wird das trainierte Modell an den Hub weiterleiten:\n-\n-```py\n->>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\n-```\n-</tf>\n </frameworkcontent>\n \n ## Verwenden Sie die Funktion `push_to_hub`.\n@@ -229,4 +186,4 @@ Um sicherzustellen, dass die Benutzer die FÃ¤higkeiten, Grenzen, mÃ¶glichen Verz\n * Manuelles Erstellen und Hochladen einer \"README.md\"-Datei.\n * Klicken Sie auf die SchaltflÃ¤che **Modellkarte bearbeiten** in Ihrem Modell-Repository.\n \n-Werfen Sie einen Blick auf die DistilBert [model card](https://huggingface.co/distilbert/distilbert-base-uncased) als gutes Beispiel fÃ¼r die Art von Informationen, die eine Modellkarte enthalten sollte. Weitere Details Ã¼ber andere Optionen, die Sie in der Datei \"README.md\" einstellen kÃ¶nnen, wie z.B. den Kohlenstoff-FuÃŸabdruck eines Modells oder Beispiele fÃ¼r Widgets, finden Sie in der Dokumentation [hier](https://huggingface.co/docs/hub/models-cards).\n\\ No newline at end of file\n+Werfen Sie einen Blick auf die DistilBert [model card](https://huggingface.co/distilbert/distilbert-base-uncased) als gutes Beispiel fÃ¼r die Art von Informationen, die eine Modellkarte enthalten sollte. Weitere Details Ã¼ber andere Optionen, die Sie in der Datei \"README.md\" einstellen kÃ¶nnen, wie z.B. den Kohlenstoff-FuÃŸabdruck eines Modells oder Beispiele fÃ¼r Widgets, finden Sie in der Dokumentation [hier](https://huggingface.co/docs/hub/models-cards)."
        },
        {
            "sha": "8da34e8162208057766e51bf682d7cb74126aa13",
            "filename": "docs/source/de/preprocessing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Fpreprocessing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Fpreprocessing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fpreprocessing.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -175,30 +175,6 @@ Setzen Sie den Parameter `return_tensors` entweder auf `pt` fÃ¼r PyTorch, oder `\n                            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n ```\n </pt>\n-<tf>\n-```py\n->>> batch_sentences = [\n-...     \"But what about second breakfast?\",\n-...     \"Don't think he knows about second breakfast, Pip.\",\n-...     \"What about elevensies?\",\n-... ]\n->>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n->>> print(encoded_input)\n-{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n-       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n-       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n-      dtype=int32)>, \n- 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, \n- 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n-       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n-       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n-```\n-</tf>\n </frameworkcontent>\n \n ## Audio"
        },
        {
            "sha": "5f05a4441e845633920013189db9d5806624ee8f",
            "filename": "docs/source/de/quicktour.md",
            "status": "modified",
            "additions": 0,
            "deletions": 92,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Fquicktour.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -73,12 +73,6 @@ Installieren Sie die folgenden AbhÃ¤ngigkeiten, falls Sie dies nicht bereits get\n pip install torch\n ```\n </pt>\n-<tf>\n-\n-```bash\n-pip install tensorflow\n-```\n-</tf>\n </frameworkcontent>\n \n Importieren sie die [`pipeline`] und spezifizieren sie die Aufgabe, welche sie lÃ¶sen mÃ¶chten:\n@@ -165,16 +159,6 @@ Use the [`AutoModelForSequenceClassification`] and [`AutoTokenizer`] to load the\n >>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n ```\n </pt>\n-<tf>\n-Use the [`TFAutoModelForSequenceClassification`] and [`AutoTokenizer`] to load the pretrained model and its associated tokenizer (more on an `TFAutoClass` below):\n-\n-```py\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n-```\n-</tf>\n </frameworkcontent>\n \n Dann kÃ¶nnen Sie das Modell und den Tokenizer in der [`pipeline`] angeben und den `Klassifikator` auf Ihren Zieltext anwenden:\n@@ -239,18 +223,6 @@ Genau wie die [`pipeline`] akzeptiert der Tokenizer eine Liste von Eingaben. Dar\n ... )\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> tf_batch = tokenizer(\n-...     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n-...     padding=True,\n-...     truncation=True,\n-...     max_length=512,\n-...     return_tensors=\"tf\",\n-... )\n-```\n-</tf>\n </frameworkcontent>\n \n Lesen Sie das Tutorial [preprocessing](./preprocessing) fÃ¼r weitere Details zur Tokenisierung.\n@@ -291,37 +263,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n         [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n ```\n </pt>\n-<tf>\n-ğŸ¤— Transformers bietet eine einfache und einheitliche Methode zum Laden von vortrainierten Instanzen. Das bedeutet, dass Sie ein [`TFAutoModel`] genauso laden kÃ¶nnen, wie Sie einen [`AutoTokenizer`] laden wÃ¼rden. Der einzige Unterschied ist die Auswahl des richtigen [`TFAutoModel`] fÃ¼r die Aufgabe. Da Sie Text - oder Sequenz - Klassifizierung machen, laden Sie [`TFAutoModelForSequenceClassification`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n-```\n-\n-<Tip>\n-\n-In der [Aufgabenzusammenfassung](./task_summary) steht, welche [AutoModel]-Klasse fÃ¼r welche Aufgabe zu verwenden ist.\n-\n-</Tip>\n-\n-Jetzt kÃ¶nnen Sie Ihren vorverarbeiteten Stapel von Eingaben direkt an das Modell Ã¼bergeben, indem Sie die WÃ¶rterbuchschlÃ¼ssel direkt an die Tensoren Ã¼bergeben:\n-\n-```py\n->>> tf_outputs = tf_model(tf_batch)\n-```\n-\n-Das Modell gibt die endgÃ¼ltigen Aktivierungen in dem Attribut \"logits\" aus. Wenden Sie die Softmax-Funktion auf die \"logits\" an, um die Wahrscheinlichkeiten zu erhalten:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n->>> tf_predictions  # doctest: +IGNORE_RESULT\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -358,21 +299,6 @@ Wenn Sie bereit sind, das Modell erneut zu verwenden, laden Sie es mit [`PreTrai\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")\n ```\n </pt>\n-<tf>\n-Sobald Ihr Modell feinabgestimmt ist, kÃ¶nnen Sie es mit seinem Tokenizer unter Verwendung von [`TFPreTrainedModel.save_pretrained`] speichern:\n-\n-```py\n->>> tf_save_directory = \"./tf_save_pretrained\"\n->>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT\n->>> tf_model.save_pretrained(tf_save_directory)\n-```\n-\n-Wenn Sie bereit sind, das Modell wieder zu verwenden, laden Sie es mit [`TFPreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n-```\n-</tf>\n </frameworkcontent>\n \n Ein besonders cooles ğŸ¤— Transformers-Feature ist die MÃ¶glichkeit, ein Modell zu speichern und es entweder als PyTorch- oder TensorFlow-Modell wieder zu laden. Der Parameter \"from_pt\" oder \"from_tf\" kann das Modell von einem Framework in das andere konvertieren:\n@@ -387,15 +313,6 @@ Ein besonders cooles ğŸ¤— Transformers-Feature ist die MÃ¶glichkeit, ein Modell\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n-```\n-</tf>\n </frameworkcontent>\n \n ## Custom model builds\n@@ -420,15 +337,6 @@ Create a model from your custom configuration with [`AutoModel.from_config`]:\n >>> my_model = AutoModel.from_config(my_config)\n ```\n </pt>\n-<tf>\n-Create a model from your custom configuration with [`TFAutoModel.from_config`]:\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> my_model = TFAutoModel.from_config(my_config)\n-```\n-</tf>\n </frameworkcontent>\n \n Weitere Informationen zur Erstellung von benutzerdefinierten Konfigurationen finden Sie in der Anleitung [Erstellen einer benutzerdefinierten Architektur](./create_a_model)."
        },
        {
            "sha": "10485a5de2a367786cef4c0b1c6f370953df88e8",
            "filename": "docs/source/de/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 34,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Frun_scripts.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -104,22 +104,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-Das Beispielskript lÃ¤dt einen Datensatz aus der ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/) Bibliothek herunter und verarbeitet ihn vor. AnschlieÃŸend nimmt das Skript die Feinabstimmung eines Datensatzes mit Keras auf einer Architektur vor, die die Zusammenfassung unterstÃ¼tzt. Das folgende Beispiel zeigt, wie die Feinabstimmung von [T5-small](https://huggingface.co/google-t5/t5-small) auf dem [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) Datensatz durchgefÃ¼hrt wird. Das T5-Modell benÃ¶tigt aufgrund der Art und Weise, wie es trainiert wurde, ein zusÃ¤tzliches Argument `source_prefix`. Mit dieser Eingabeaufforderung weiÃŸ T5, dass es sich um eine Zusammenfassungsaufgabe handelt.\n-\n-```bash\n-python examples/tensorflow/summarization/run_summarization.py  \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## Verteiltes Training und gemischte PrÃ¤zision\n@@ -170,23 +154,6 @@ python xla_spawn.py --num_cores 8 \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-Tensor Processing Units (TPUs) sind speziell fÃ¼r die Beschleunigung der Leistung konzipiert. TensorFlow Skripte verwenden eine [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy) fÃ¼r das Training auf TPUs. Um eine TPU zu verwenden, Ã¼bergeben Sie den Namen der TPU-Ressource an das Argument `tpu`.\n-\n-```bash\n-python run_summarization.py  \\\n-    --tpu name_of_tpu_resource \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## FÃ¼hren Sie ein Skript mit ğŸ¤— Accelerate aus.\n@@ -348,4 +315,4 @@ python examples/pytorch/summarization/run_summarization.py\n     --per_device_eval_batch_size=4 \\\n     --overwrite_output_dir \\\n     --predict_with_generate\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "fb5cb5695b9f0c8ab6a6d45a6e7633a8ae49b9d4",
            "filename": "docs/source/de/training.md",
            "status": "modified",
            "additions": 1,
            "deletions": 114,
            "changes": 115,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Ftraining.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fde%2Ftraining.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fde%2Ftraining.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -156,119 +156,6 @@ AnschlieÃŸend kÃ¶nnen Sie Ihr Modell durch den Aufruf von [`~transformers.Traine\n >>> trainer.train()\n ```\n </pt>\n-<tf>\n-<a id='keras'></a>\n-\n-<Youtube id=\"rnTGBy2ax1c\"/>\n-\n-## Trainieren Sie ein TensorFlow-Modell mit Keras\n-\n-Sie kÃ¶nnen auch ğŸ¤— Transformers Modelle in TensorFlow mit der Keras API trainieren!\n-\n-### Laden von Daten fÃ¼r Keras\n-\n-Wenn Sie ein ğŸ¤— Transformers Modell mit der Keras API trainieren wollen, mÃ¼ssen Sie Ihren Datensatz in ein Format konvertieren, das\n-Keras versteht. Wenn Ihr Datensatz klein ist, kÃ¶nnen Sie das Ganze einfach in NumPy-Arrays konvertieren und an Keras Ã¼bergeben.\n-Probieren wir das zuerst aus, bevor wir etwas Komplizierteres tun.\n-\n-Laden Sie zunÃ¤chst ein Dataset. Wir werden den CoLA-Datensatz aus dem [GLUE-Benchmark](https://huggingface.co/datasets/glue) verwenden,\n-da es sich um eine einfache Aufgabe zur Klassifizierung von binÃ¤rem Text handelt, und nehmen vorerst nur den Trainingssplit.\n-\n-```py\n-from datasets import load_dataset\n-\n-dataset = load_dataset(\"glue\", \"cola\")\n-dataset = dataset[\"train\"]  # Just take the training split for now\n-```\n-\n-Als nÃ¤chstes laden Sie einen Tokenizer und tokenisieren die Daten als NumPy-Arrays. Beachten Sie, dass die Beschriftungen bereits eine Liste von 0 und 1en sind,\n-Wir kÃ¶nnen sie also ohne Tokenisierung direkt in ein NumPy-Array konvertieren!\n-\n-```py\n-from transformers import AutoTokenizer\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-tokenized_data = tokenizer(dataset[\"text\"], return_tensors=\"np\", padding=True)\n-# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n-tokenized_data = dict(tokenized_data)\n-\n-labels = np.array(dataset[\"label\"])  # Label is already an array of 0 and 1\n-```\n-\n-SchlieÃŸlich laden, [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) und [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Sie das Modell:\n-\n-```py\n-from transformers import TFAutoModelForSequenceClassification\n-from tensorflow.keras.optimizers import Adam\n-\n-# Load and compile our model\n-model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n-# Lower learning rates are often better for fine-tuning transformers\n-model.compile(optimizer=Adam(3e-5))\n-\n-model.fit(tokenized_data, labels)\n-```\n-\n-<Tip>\n-\n-Sie mÃ¼ssen Ihren Modellen kein Verlustargument Ã¼bergeben, wenn Sie sie `compile()`! Hugging-Face-Modelle wÃ¤hlen automatisch\n-einen Loss, der fÃ¼r ihre Aufgabe und Modellarchitektur geeignet ist, wenn dieses Argument leer gelassen wird. Sie kÃ¶nnen jederzeit auÃŸer Kraft setzen, indem Sie selbst einen Loss angeben, wenn Sie das mÃ¶chten!\n-\n-</Tip>\n-\n-Dieser Ansatz eignet sich hervorragend fÃ¼r kleinere DatensÃ¤tze, aber bei grÃ¶ÃŸeren DatensÃ¤tzen kann er zu einem Problem werden. Warum?\n-Weil das tokenisierte Array und die Beschriftungen vollstÃ¤ndig in den Speicher geladen werden mÃ¼ssten, und weil NumPy nicht mit\n-\"gezackte\" Arrays nicht verarbeiten kann, so dass jedes tokenisierte Sample auf die LÃ¤nge des lÃ¤ngsten Samples im gesamten Datensatz aufgefÃ¼llt werden mÃ¼sste.\n-Datensatzes aufgefÃ¼llt werden. Dadurch wird das Array noch grÃ¶ÃŸer, und all die aufgefÃ¼llten Token verlangsamen auch das Training!\n-\n-### Laden von Daten als tf.data.Dataset\n-\n-Wenn Sie eine Verlangsamung des Trainings vermeiden wollen, kÃ¶nnen Sie Ihre Daten stattdessen als `tf.data.Dataset` laden. Sie kÃ¶nnen zwar Ihre eigene\n-tf.data\"-Pipeline schreiben kÃ¶nnen, wenn Sie wollen, haben wir zwei bequeme Methoden, um dies zu tun:\n-\n-- [`~TFPreTrainedModel.prepare_tf_dataset`]: Dies ist die Methode, die wir in den meisten FÃ¤llen empfehlen. Da es sich um eine Methode\n-Ihres Modells ist, kann sie das Modell inspizieren, um automatisch herauszufinden, welche Spalten als Modelleingaben verwendet werden kÃ¶nnen, und\n-verwirft die anderen, um einen einfacheren, leistungsfÃ¤higeren Datensatz zu erstellen.\n-- [`~datasets.Dataset.to_tf_dataset`]: Diese Methode ist eher auf niedriger Ebene angesiedelt und ist nÃ¼tzlich, wenn Sie genau kontrollieren wollen, wie\n-Dataset erstellt wird, indem man genau angibt, welche `columns` und `label_cols` einbezogen werden sollen.\n-\n-Bevor Sie [`~TFPreTrainedModel.prepare_tf_dataset`] verwenden kÃ¶nnen, mÃ¼ssen Sie die Tokenizer-Ausgaben als Spalten zu Ihrem Datensatz hinzufÃ¼gen, wie in\n-dem folgenden Codebeispiel:\n-\n-```py\n-def tokenize_dataset(data):\n-    # Keys of the returned dictionary will be added to the dataset as columns\n-    return tokenizer(data[\"text\"])\n-\n-\n-dataset = dataset.map(tokenize_dataset)\n-```\n-\n-Denken Sie daran, dass Hugging Face-DatensÃ¤tze standardmÃ¤ÃŸig auf der Festplatte gespeichert werden, so dass dies nicht zu einem erhÃ¶hten Arbeitsspeicherbedarf fÃ¼hren wird! Sobald die\n-Spalten hinzugefÃ¼gt wurden, kÃ¶nnen Sie Batches aus dem Datensatz streamen und zu jedem Batch AuffÃ¼llungen hinzufÃ¼gen, was die Anzahl der AuffÃ¼llungs-Token im Vergleich zum AuffÃ¼llen des gesamten Datensatzes reduziert.\n-\n-\n-```py\n->>> tf_dataset = model.prepare_tf_dataset(dataset, batch_size=16, shuffle=True, tokenizer=tokenizer)\n-```\n-\n-Beachten Sie, dass Sie im obigen Codebeispiel den Tokenizer an `prepare_tf_dataset` Ã¼bergeben mÃ¼ssen, damit die Stapel beim Laden korrekt aufgefÃ¼llt werden kÃ¶nnen.\n-Wenn alle Stichproben in Ihrem Datensatz die gleiche LÃ¤nge haben und kein AuffÃ¼llen erforderlich ist, kÃ¶nnen Sie dieses Argument weglassen.\n-Wenn Sie etwas Komplexeres als nur das AuffÃ¼llen von Stichproben benÃ¶tigen (z. B. das Korrumpieren von Token fÃ¼r die maskierte Sprachmodellierung), kÃ¶nnen Sie das Argument\n-Modellierung), kÃ¶nnen Sie stattdessen das Argument `collate_fn` verwenden, um eine Funktion zu Ã¼bergeben, die aufgerufen wird, um die\n-Liste von Stichproben in einen Stapel umwandelt und alle gewÃ¼nschten Vorverarbeitungen vornimmt. Siehe unsere\n-[examples](https://github.com/huggingface/transformers/tree/main/examples) oder\n-[notebooks](https://huggingface.co/docs/transformers/notebooks), um diesen Ansatz in Aktion zu sehen.\n-\n-Sobald Sie einen `tf.data.Dataset` erstellt haben, kÃ¶nnen Sie das Modell wie zuvor kompilieren und anpassen:\n-\n-```py\n-model.compile(optimizer=Adam(3e-5))\n-\n-model.fit(tf_dataset)\n-```\n-\n-</tf>\n </frameworkcontent>\n \n <a id='pytorch_native'></a>\n@@ -430,4 +317,4 @@ Weitere Beispiele fÃ¼r die Feinabstimmung finden Sie unter:\n - [ğŸ¤— Transformers Examples](https://github.com/huggingface/transformers/tree/main/examples) enthÃ¤lt Skripte\n   um gÃ¤ngige NLP-Aufgaben in PyTorch und TensorFlow zu trainieren.\n \n-- [ğŸ¤— Transformers Notebooks](notebooks) enthÃ¤lt verschiedene Notebooks zur Feinabstimmung eines Modells fÃ¼r bestimmte Aufgaben in PyTorch und TensorFlow.\n\\ No newline at end of file\n+- [ğŸ¤— Transformers Notebooks](notebooks) enthÃ¤lt verschiedene Notebooks zur Feinabstimmung eines Modells fÃ¼r bestimmte Aufgaben in PyTorch und TensorFlow."
        },
        {
            "sha": "77554c85b02abd1ee73966820caff73b34a7ed14",
            "filename": "docs/source/en/internal/import_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fimport_utils.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -51,12 +51,7 @@ Let's see how to specify specific object dependencies.\n \n All objects under a given filename have an automatic dependency to the tool linked to the filename\n \n-**TensorFlow**: All files starting with `modeling_tf_` have an automatic TensorFlow dependency.\n-\n-**Flax**: All files starting with `modeling_flax_` have an automatic Flax dependency\n-\n-**PyTorch**: All files starting with `modeling_` and not valid with the above (TensorFlow and Flax) have an automatic \n-PyTorch dependency\n+**PyTorch**: All files starting with `modeling_` have an automatic PyTorch dependency\n \n **Tokenizers**: All files starting with `tokenization_` and ending with `_fast` have an automatic `tokenizers` dependency\n "
        },
        {
            "sha": "7866f1e627e61b4526bd02c0ae98b5ca8e7dedad",
            "filename": "docs/source/es/autoclass_tutorial.md",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Fautoclass_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Fautoclass_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fautoclass_tutorial.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -101,23 +101,4 @@ Reutiliza fÃ¡cilmente el mismo checkpoint para cargar una aquitectura para algun\n \n Generalmente recomendamos utilizar las clases `AutoTokenizer` y `AutoModelFor` para cargar instancias pre-entrenadas de modelos. Ã‰sto asegurarÃ¡ que cargues la arquitectura correcta en cada ocasiÃ³n. En el siguiente [tutorial](preprocessing), aprende a usar tu tokenizador reciÃ©n cargado, el extractor de caracterÃ­sticas y el procesador para preprocesar un dataset para fine-tuning.\n </pt>\n-<tf>\n-Finalmente, la clase `TFAutoModelFor` te permite cargar tu modelo pre-entrenado para una tarea dada (revisa [aquÃ­](model_doc/auto) para conocer la lista completa de tareas disponibles). Por ejemplo, carga un modelo para clasificaciÃ³n de secuencias con [`TFAutoModelForSequenceClassification.from_pretrained`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Reutiliza fÃ¡cilmente el mismo checkpoint para cargar una aquitectura para alguna tarea diferente:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Generalmente recomendamos utilizar las clases `AutoTokenizer` y `TFAutoModelFor` para cargar instancias de modelos pre-entrenados. Ã‰sto asegurarÃ¡ que cargues la arquitectura correcta cada vez. En el siguiente [tutorial](preprocessing), aprende a usar tu tokenizador reciÃ©n cargado, el extractor de caracterÃ­sticas y el procesador para preprocesar un dataset para fine-tuning.\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "2cb16267af2286dddf65740a213014bb3b565b1d",
            "filename": "docs/source/es/create_a_model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fcreate_a_model.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -137,31 +137,6 @@ Cuando cargues tus pesos del preentrenamiento, el modelo por defecto se carga au\n >>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n ```\n </pt>\n-<tf>\n-  \n-Carga los atributos de tu configuraciÃ³n personalizada en el modelo de la siguiente forma:\n-\n-```py\n->>> from transformers import TFDistilBertModel\n-\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")\n->>> tf_model = TFDistilBertModel(my_config)\n-```\n-\n-Esto crea un modelo con valores aleatorios, en lugar de crearlo con los pesos del preentrenamiento, por lo que no serÃ¡s capaz de usar este modelo para nada Ãºtil hasta que no lo entrenes. El entrenamiento es un proceso costoso, tanto en cuestiÃ³n de recursos como de tiempo, por lo que generalmente es mejor usar un modelo preentrenado para obtener mejores resultados mÃ¡s rÃ¡pido, consumiendo solo una fracciÃ³n de los recursos que un entrenamiento completo hubiera requerido. \n-\n-Puedes crear un modelo preentrenado con [`~TFPreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Cuando cargues tus pesos del preentrenamiento, el modelo por defecto se carga automÃ¡ticamente si este nos lo proporciona ğŸ¤— Transformers. Sin embargo, siempre puedes reemplazar (todos o algunos de) los atributos del modelo por defecto por los tuyos:\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n-```\n-</tf>\n </frameworkcontent>\n \n ### Cabezas de modelo \n@@ -189,25 +164,6 @@ Puedes reutilizar este punto de guardado o *checkpoint* para otra tarea fÃ¡cilme\n >>> model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n ```\n </pt>\n-<tf>\n-\n-Por ejemplo,  [`TFDistilBertForSequenceClassification`] es un modelo DistilBERT base con una cabeza de clasificaciÃ³n de secuencias. La cabeza de clasificaciÃ³n de secuencias es una capa superior que precede a la recolecciÃ³n de las salidas.\n-\n-```py\n->>> from transformers import TFDistilBertForSequenceClassification\n-\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Puedes reutilizar este punto de guardado o *checkpoint* para otra tarea fÃ¡cilmente cambiando a una cabeza de un modelo diferente. Para una tarea de respuesta a preguntas, puedes usar la cabeza del modelo [`TFDistilBertForQuestionAnswering`]. La cabeza de respuesta a preguntas es similar a la de clasificaciÃ³n de secuencias, excepto porque consta de una capa lineal delante de la salida de los *hidden states*. \n-\n-\n-```py\n->>> from transformers import TFDistilBertForQuestionAnswering\n-\n->>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Tokenizer"
        },
        {
            "sha": "a9433d0951320cceaf358d4fce744fb4f90a6c5b",
            "filename": "docs/source/es/quicktour.md",
            "status": "modified",
            "additions": 0,
            "deletions": 86,
            "changes": 86,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fquicktour.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -73,12 +73,6 @@ Instala las siguientes dependencias si aÃºn no lo has hecho:\n pip install torch\n ```\n </pt>\n-<tf>\n-\n-```bash\n-pip install tensorflow\n-```\n-</tf>\n </frameworkcontent>\n \n Importa [`pipeline`] y especifica la tarea que deseas completar:\n@@ -161,17 +155,6 @@ Usa [`AutoModelForSequenceClassification`] y ['AutoTokenizer'] para cargar un mo\n \n </pt>\n \n-<tf>\n-Usa [`TFAutoModelForSequenceClassification`] y ['AutoTokenizer'] para cargar un modelo preentrenado y un tokenizador asociado (mÃ¡s en un `TFAutoClass` debajo):\n-\n-```py\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n-```\n-\n-</tf>\n </frameworkcontent>\n \n DespuÃ©s puedes especificar el modelo y el tokenizador en el [`pipeline`], y aplicar el `classifier` en tu texto objetivo:\n@@ -237,18 +220,6 @@ Como con el [`pipeline`], el tokenizador aceptarÃ¡ una lista de inputs. AdemÃ¡s,\n ... )\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> tf_batch = tokenizer(\n-...     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n-...     padding=True,\n-...     truncation=True,\n-...     max_length=512,\n-...     return_tensors=\"tf\",\n-... )\n-```\n-</tf>\n </frameworkcontent>\n \n Lee el tutorial de [preprocessing](./preprocessing) para mÃ¡s detalles acerca de la tokenizaciÃ³n.\n@@ -289,39 +260,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n         [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n ```\n </pt>\n-<tf>\n-ğŸ¤— Transformers provee una forma simple y unificada de cargar tus instancias preentrenadas. Esto significa que puedes cargar un [`TFAutoModel`] como cargarÃ­as un [`AutoTokenizer`]. La Ãºnica diferencia es seleccionar el [`TFAutoModel`] correcto para la tarea. Ya que estÃ¡s clasificando texto, o secuencias, carga [`TFAutoModelForSequenceClassification`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n-```\n-\n-<Tip>\n-  Ve el [task summary](./task_summary) para revisar quÃ© clase del [`AutoModel`]\n-  deberÃ­as usar para cada tarea.\n-</Tip>\n-\n-Ahora puedes pasar tu lote preprocesado de inputs directamente al modelo pasando las llaves del diccionario directamente a los tensores:\n-\n-```py\n->>> tf_outputs = tf_model(tf_batch)\n-```\n-\n-El modelo producirÃ¡ las activaciones finales en el atributo `logits`. Aplica la funciÃ³n softmax a `logits` para obtener las probabilidades:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n->>> print(tf.math.round(tf_predictions * 10**4) / 10**4)\n-tf.Tensor(\n-[[0.0021 0.0018 0.0116 0.2121 0.7725]\n- [0.2084 0.1826 0.1969 0.1755  0.2365]], shape=(2, 5), dtype=float32)\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -360,21 +298,6 @@ Cuando quieras usar el modelo otra vez cÃ¡rgalo con [`PreTrainedModel.from_pretr\n \n </pt>\n \n-<tf>\n-Una vez que se haya hecho fine-tuning a tu modelo puedes guardarlo con tu tokenizador usando [`TFPreTrainedModel.save_pretrained`]:\n-\n-```py\n->>> tf_save_directory = \"./tf_save_pretrained\"\n->>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT\n->>> tf_model.save_pretrained(tf_save_directory)\n-```\n-\n-Cuando quieras usar el modelo otra vez cÃ¡rgalo con [`TFPreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n-```\n-</tf>\n </frameworkcontent>\n \n Una caracterÃ­stica particularmente interesante de ğŸ¤— Transformers es la habilidad de guardar el modelo y cargarlo como un modelo de PyTorch o TensorFlow. El parÃ¡metro `from_pt` o `from_tf` puede convertir el modelo de un framework al otro:\n@@ -389,13 +312,4 @@ Una caracterÃ­stica particularmente interesante de ğŸ¤— Transformers es la habil\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "eb43a0f84d2b6c3a6f0745428293545a879630cf",
            "filename": "docs/source/es/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Frun_scripts.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -104,22 +104,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-El script de ejemplo descarga y preprocesa un conjunto de datos de la biblioteca ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/). Luego, el script ajusta un conjunto de datos utilizando Keras en una arquitectura que soporta la tarea de resumir. El siguiente ejemplo muestra cÃ³mo ajustar un [T5-small](https://huggingface.co/google-t5/t5-small) en el conjunto de datos [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail). El modelo T5 requiere un argumento adicional `source_prefix` debido a cÃ³mo fue entrenado. Este aviso le permite a T5 saber que se trata de una tarea de resumir.\n-\n-```bash\n-python examples/tensorflow/summarization/run_summarization.py  \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## Entrenamiento distribuido y de precisiÃ³n mixta\n@@ -170,23 +154,6 @@ python xla_spawn.py --num_cores 8 \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-Las Unidades de Procesamiento de Tensor (TPUs) estÃ¡n diseÃ±adas especÃ­ficamente para acelerar el rendimiento. TensorFlow utiliza [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy) para entrenar en TPUs. Para usar una TPU, pasa el nombre del recurso de la TPU al argumento `tpu`\n-\n-```bash\n-python run_summarization.py  \\\n-    --tpu name_of_tpu_resource \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## Ejecutar un script con ğŸ¤— Accelerate"
        },
        {
            "sha": "dce3b7239a3986e83967f5212b99647e3c0e3f89",
            "filename": "docs/source/es/serialization.md",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Fserialization.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -215,25 +215,6 @@ del paquete `transformers.onnx` al directorio deseado:\n python -m transformers.onnx --model=local-pt-checkpoint onnx/\n ```\n </pt>\n-<tf>\n-```python\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> # Load tokenizer and TensorFlow weights from the Hub\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> # Save to disk\n->>> tokenizer.save_pretrained(\"local-tf-checkpoint\")\n->>> tf_model.save_pretrained(\"local-tf-checkpoint\")\n-```\n-\n-Una vez que se guarda el checkpoint, podemos exportarlo a ONNX usando el argumento `--model` \n-del paquete `transformers.onnx` al directorio deseado:\n-\n-```bash\n-python -m transformers.onnx --model=local-tf-checkpoint onnx/\n-```\n-</tf>\n </frameworkcontent>\n \n ### Seleccionar caracterÃ­sticas para diferentes topologÃ­as de un modelo"
        },
        {
            "sha": "8d23fc199af9b669eafc1495c9d5049d6d27c5d4",
            "filename": "docs/source/es/tasks/language_modeling.md",
            "status": "modified",
            "additions": 1,
            "deletions": 132,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Ftasks%2Flanguage_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Ftasks%2Flanguage_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Flanguage_modeling.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -180,23 +180,6 @@ Para modelados de lenguaje por enmascaramiento usa el mismo [`DataCollatorForLan\n >>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n ```\n </pt>\n-<tf>\n-Puedes usar el token de final de secuencia como el token de relleno y asignar `mlm=False`. Esto usarÃ¡ los inputs como etiquetas movidas un elemento hacia la derecha:\n-\n-```py\n->>> from transformers import DataCollatorForLanguageModeling\n-\n->>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")\n-```\n-\n-Para modelados de lenguajes por enmascaramiento usa el mismo [`DataCollatorForLanguageModeling`] excepto que deberÃ¡s especificar `mlm_probability` para enmascarar tokens aleatoriamente cada vez que iteras sobre los datos.\n-\n-```py\n->>> from transformers import DataCollatorForLanguageModeling\n-\n->>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Modelado de lenguaje causal\n@@ -246,63 +229,6 @@ A este punto, solo faltan tres pasos:\n >>> trainer.train()\n ```\n </pt>\n-<tf>\n-Para realizar el fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato `tf.data.Dataset` con [`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset). Especifica los inputs y etiquetas en `columns`, ya sea para mezclar el dataset, tamaÃ±o de lote, y el data collator:\n-\n-```py\n->>> tf_train_set = lm_dataset[\"train\"].to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n-...     dummy_labels=True,\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = lm_dataset[\"test\"].to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n-...     dummy_labels=True,\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-<Tip>\n-\n-Si no estÃ¡s familiarizado con realizar fine-tuning de tus modelos con Keras, considera el tutorial bÃ¡sico [aquÃ­](training#finetune-with-keras)!\n-\n-</Tip>\n-\n-Crea la funciÃ³n optimizadora, la tasa de aprendizaje, y algunos hiperparÃ¡metros de entrenamiento:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-Carga DistilGPT2 con [`TFAutoModelForCausalLM`]:\n-\n-```py\n->>> from transformers import TFAutoModelForCausalLM\n-\n->>> model = TFAutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n-```\n-\n-Configura el modelo para entrenamiento con [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-Llama a [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) para realizar el fine-tuning del modelo:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)\n-```\n-</tf>\n </frameworkcontent>\n \n ## Modelado de lenguaje por enmascaramiento\n@@ -353,63 +279,6 @@ A este punto, solo faltan tres pasos:\n >>> trainer.train()\n ```\n </pt>\n-<tf>\n-Para realizar el fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato `tf.data.Dataset` con [`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset). Especifica los inputs y etiquetas en `columns`, ya sea para mezclar el dataset, tamaÃ±o de lote, y el data collator:\n-\n-```py\n->>> tf_train_set = lm_dataset[\"train\"].to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n-...     dummy_labels=True,\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = lm_dataset[\"test\"].to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n-...     dummy_labels=True,\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-<Tip>\n-\n-Si no estÃ¡s familiarizado con realizar fine-tuning de tus modelos con Keras, considera el tutorial bÃ¡sico [aquÃ­](training#finetune-with-keras)!\n-\n-</Tip>\n-\n-Crea la funciÃ³n optimizadora, la tasa de aprendizaje, y algunos hiperparÃ¡metros de entrenamiento:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-Carga DistilRoBERTa con [`TFAutoModelForMaskedLM`]:\n-\n-```py\n->>> from transformers import TFAutoModelForMaskedLM\n-\n->>> model = TFAutoModelForCausalLM.from_pretrained(\"distilbert/distilroberta-base\")\n-```\n-\n-Configura el modelo para entrenamiento con [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-Llama a [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) para realizar el fine-tuning del modelo:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -418,4 +287,4 @@ Para un ejemplo mÃ¡s profundo sobre cÃ³mo realizar el fine-tuning sobre un model\n [PyTorch notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\n o [TensorFlow notebook](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n \n-</Tip>\n\\ No newline at end of file\n+</Tip>"
        },
        {
            "sha": "fb4d988a00ff6886fee819e8f55c77c1bad31277",
            "filename": "docs/source/es/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 0,
            "deletions": 57,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Fmultiple_choice.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -147,61 +147,4 @@ En este punto, solo quedan tres pasos:\n >>> trainer.train()\n ```\n </pt>\n-<tf>\n-Para realizar el fine-tuning de un modelo en TensorFlow, primero convierte tus datasets al formato `tf.data.Dataset` con el mÃ©todo [`~TFPreTrainedModel.prepare_tf_dataset`].\n-\n-```py\n->>> data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_swag[\"train\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_swag[\"validation\"],\n-...     shuffle=False,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-<Tip>\n-\n-Para familiarizarte con el fine-tuning con Keras, Â¡mira el tutorial bÃ¡sico [aquÃ­](training#finetune-with-keras)!\n-\n-</Tip>\n-\n-Prepara una funciÃ³n de optimizaciÃ³n, un programa para la tasa de aprendizaje y algunos hiperparÃ¡metros de entrenamiento:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_train_epochs = 2\n->>> total_train_steps = (len(tokenized_swag[\"train\"]) // batch_size) * num_train_epochs\n->>> optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n-```\n-\n-Carga el modelo BERT con [`TFAutoModelForMultipleChoice`]:\n-\n-```py\n->>> from transformers import TFAutoModelForMultipleChoice\n-\n->>> model = TFAutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")\n-```\n-\n-Configura el modelo para entrenarlo con [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):\n-\n-```py\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-Invoca el mÃ©todo [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) para realizar el fine-tuning del modelo:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2)\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "0e1bd9b1b497b26caee71089fb9faab58c71fab2",
            "filename": "docs/source/es/tasks/question_answering.md",
            "status": "modified",
            "additions": 0,
            "deletions": 69,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Ftasks%2Fquestion_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Ftasks%2Fquestion_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Fquestion_answering.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -146,13 +146,6 @@ Usa el [`DefaultDataCollator`] para crear un lote de ejemplos. A diferencia de l\n >>> data_collator = DefaultDataCollator()\n ```\n </pt>\n-<tf>\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Entrenamiento\n@@ -202,68 +195,6 @@ En este punto, solo quedan tres pasos:\n >>> trainer.train()\n ```\n </pt>\n-<tf>\n-Para realizar el fine-tuning de un modelo en TensorFlow, primero convierte tus datasets al formato `tf.data.Dataset` con el mÃ©todo [`~TFPreTrainedModel.prepare_tf_dataset`].\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_squad[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_squad[\"validation\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-<Tip>\n-\n-Para familiarizarte con el fine-tuning con Keras, Â¡mira el tutorial bÃ¡sico [aquÃ­](training#finetune-with-keras)!\n-\n-</Tip>\n-\n-Prepara una funciÃ³n de optimizaciÃ³n, un programa para la tasa de aprendizaje y algunos hiperparÃ¡metros de entrenamiento:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_epochs = 2\n->>> total_train_steps = (len(tokenized_squad[\"train\"]) // batch_size) * num_epochs\n->>> optimizer, schedule = create_optimizer(\n-...     init_lr=2e-5,\n-...     num_warmup_steps=0,\n-...     num_train_steps=total_train_steps,\n-... )\n-```\n-\n-Carga el modelo DistilBERT con [`TFAutoModelForQuestionAnswering`]:\n-\n-```py\n->>> from transformers import TFAutoModelForQuestionAnswering\n-\n->>> model = TFAutoModelForQuestionAnswering(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Configura el modelo para entrenarlo con [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-Invoca el mÃ©todo [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) para realizar el fine-tuning del modelo:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>"
        },
        {
            "sha": "024568c4443ae8316799edeb8a135e2fadfdd0e4",
            "filename": "docs/source/es/tasks/summarization.md",
            "status": "modified",
            "additions": 0,
            "deletions": 60,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Ftasks%2Fsummarization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fes%2Ftasks%2Fsummarization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fes%2Ftasks%2Fsummarization.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -104,13 +104,6 @@ Usa [`DataCollatorForSeq2Seq`] para crear un lote de ejemplos. Esto tambiÃ©n *re\n >>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n ```\n </pt>\n-<tf>\n-```py\n->>> from transformers import DataCollatorForSeq2Seq\n-\n->>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Entrenamiento\n@@ -162,59 +155,6 @@ En este punto, solo faltan tres pasos:\n >>> trainer.train()\n ```\n </pt>\n-<tf>\n-Para hacer fine-tuning de un modelo en TensorFlow, comienza por convertir tus datasets al formato `tf.data.Dataset` con [`~datasets.Dataset.to_tf_dataset`]. Especifica los inputs y etiquetas en `columns`, el tamaÃ±o de lote, el data collator, y si es necesario mezclar el dataset:\n-\n-```py\n->>> tf_train_set = tokenized_billsum[\"train\"].to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = tokenized_billsum[\"test\"].to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-<Tip>\n-\n-Para familiarizarte con el fine-tuning con Keras, Â¡mira el tutorial bÃ¡sico [aquÃ­](training#finetune-with-keras)!\n-\n-</Tip>\n-\n-Crea la funciÃ³n optimizadora, establece la tasa de aprendizaje y algunos hiperparÃ¡metros de entrenamiento:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-Carga T5 con [`TFAutoModelForSeq2SeqLM`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"google-t5/t5-small\")\n-```\n-\n-Configura el modelo para entrenamiento con [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):\n-\n-```py\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-Llama a [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) para realizar el fine-tuning del modelo:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3)\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>"
        },
        {
            "sha": "6dafd37b6d3ee76358e84cd9541e0f2e30e515d0",
            "filename": "docs/source/fr/autoclass_tutorial.md",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Ffr%2Fautoclass_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Ffr%2Fautoclass_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Fautoclass_tutorial.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -164,23 +164,4 @@ Les points de contrÃ´le TensorFlow et Flax ne sont pas concernÃ©s, et peuvent Ãª\n \n En gÃ©nÃ©ral, nous recommandons d'utiliser les classes `AutoTokenizer` et `AutoModelFor` pour charger des instances prÃ©-entraÃ®nÃ©es de tokenizers et modÃ¨les respectivement. Cela vous permettra de charger la bonne architecture Ã  chaque fois. Dans le prochain [tutoriel](preprocessing), vous apprenez Ã  utiliser un tokenizer, processeur d'image, extracteur de caractÃ©ristiques et processeur pour prÃ©-traiter un jeu de donnÃ©es pour le fine-tuning.\n </pt>\n-<tf>\n-Enfin, les classes `TFAutoModelFor` vous permettent de charger un modÃ¨le prÃ©-entraÃ®nÃ© pour une tÃ¢che donnÃ©e (voir [ici](model_doc/auto) pour une liste complÃ¨te des tÃ¢ches disponibles). Par exemple, chargez un modÃ¨le pour la classification de sÃ©quence avec [`TFAutoModelForSequenceClassification.from_pretrained`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-RÃ©utilisez facilement le mÃªme ensemble de poids pour charger une architecture pour une tÃ¢che diffÃ©rente :\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-En gÃ©nÃ©ral, nous recommandons d'utiliser les classes `AutoTokenizer` et `TFAutoModelFor` pour charger des instances prÃ©-entraÃ®nÃ©es de tokenizers et modÃ¨les respectivement. Cela vous permettra de charger la bonne architecture Ã  chaque fois. Dans le prochain [tutoriel](preprocessing), vous apprenez Ã  utiliser un tokenizer, processeur d'image, extracteur de caractÃ©ristiques et processeur pour prÃ©-traiter un jeu de donnÃ©es pour le fine-tuning.\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "b2c35cffd566713b0f25d04d4da98095b95e4b9b",
            "filename": "docs/source/fr/quicktour.md",
            "status": "modified",
            "additions": 0,
            "deletions": 92,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Ffr%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Ffr%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Fquicktour.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -35,12 +35,6 @@ Vous aurez aussi besoin d'installer votre bibliothÃ¨que d'apprentissage profond\n pip install torch\n ```\n </pt>\n-<tf>\n-\n-```bash\n-pip install tensorflow\n-```\n-</tf>\n </frameworkcontent>\n \n ## Pipeline\n@@ -143,16 +137,6 @@ Utilisez [`AutoModelForSequenceClassification`] et [`AutoTokenizer`] pour charge\n >>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n ```\n </pt>\n-<tf>\n-Utilisez [`TFAutoModelForSequenceClassification`] et [`AutoTokenizer`] pour charger le modÃ¨le prÃ©-entraÃ®nÃ© et le tokenizer adaptÃ© (plus de dÃ©tails sur une `TFAutoClass` dans la section suivante) :\n-\n-```py\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n-```\n-</tf>\n </frameworkcontent>\n \n SpÃ©cifiez le modÃ¨le et le tokenizer dans le [`pipeline`], et utilisez le `classifier` sur le texte en franÃ§ais :\n@@ -216,18 +200,6 @@ Un tokenizer peut Ã©galement accepter une liste de textes, et remplir et tronque\n ... )\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> tf_batch = tokenizer(\n-...     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n-...     padding=True,\n-...     truncation=True,\n-...     max_length=512,\n-...     return_tensors=\"tf\",\n-... )\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -272,37 +244,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n         [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n ```\n </pt>\n-<tf>\n-ğŸ¤— Transformers fournit un moyen simple et unifiÃ© de charger des instances prÃ©-entraÃ®nÃ©s. Cela signifie que vous pouvez charger un [`TFAutoModel`] comme vous chargeriez un [`AutoTokenizer`]. La seule diffÃ©rence est de sÃ©lectionner le [`TFAutoModel`] appropriÃ© pour la tÃ¢che. Pour une classification de texte (ou de sÃ©quence de textes), vous devez charger [`TFAutoModelForSequenceClassification`] :\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n-```\n-\n-<Tip>\n-\n-Voir le [rÃ©sumÃ© de la tÃ¢che](./task_summary) pour vÃ©rifier si elle est prise en charge par une classe [`AutoModel`].\n-\n-</Tip>\n-\n-Passez maintenant votre Ã©chantillon d'entrÃ©es prÃ©traitÃ©es directement au modÃ¨le en passant les clÃ©s du dictionnaire directement aux tensors :\n-\n-```py\n->>> tf_outputs = tf_model(tf_batch)\n-```\n-\n-Le modÃ¨le produit les activations finales dans l'attribut `logits`. Appliquez la fonction softmax aux `logits` pour rÃ©cupÃ©rer les probabilitÃ©s :\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n->>> tf_predictions  # doctest: +IGNORE_RESULT\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -329,21 +270,6 @@ Lorsque vous voulez rÃ©utiliser le modÃ¨le, rechargez-le avec [`PreTrainedModel.\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")\n ```\n </pt>\n-<tf>\n-Une fois que votre modÃ¨le est finetunÃ©, vous pouvez le sauvegarder avec son tokenizer en utilisant [`TFPreTrainedModel.save_pretrained`] :\n-\n-```py\n->>> tf_save_directory = \"./tf_save_pretrained\"\n->>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT\n->>> tf_model.save_pretrained(tf_save_directory)\n-```\n-\n-Lorsque vous voulez rÃ©utiliser le modÃ¨le, rechargez-le avec [`TFPreTrainedModel.from_pretrained`] :\n-\n-```py\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n-```\n-</tf>\n </frameworkcontent>\n \n Une fonctionnalitÃ© particuliÃ¨rement cool ğŸ¤— Transformers est la possibilitÃ© d'enregistrer un modÃ¨le et de le recharger en tant que modÃ¨le PyTorch ou TensorFlow. Le paramÃ¨tre `from_pt` ou `from_tf` permet de convertir le modÃ¨le d'un framework Ã  l'autre :\n@@ -358,15 +284,6 @@ Une fonctionnalitÃ© particuliÃ¨rement cool ğŸ¤— Transformers est la possibilitÃ©\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n-```\n-</tf>\n </frameworkcontent>\n \n ## Constructions de modÃ¨les personnalisÃ©s\n@@ -391,15 +308,6 @@ CrÃ©ez un modÃ¨le personnalisÃ© Ã  partir de votre configuration avec [`AutoMode\n >>> my_model = AutoModel.from_config(my_config)\n ```\n </pt>\n-<tf>\n-CrÃ©ez un modÃ¨le personnalisÃ© Ã  partir de votre configuration avec [`TFAutoModel.from_config`] :\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> my_model = TFAutoModel.from_config(my_config)\n-```\n-</tf>\n </frameworkcontent>\n \n Consultez le guide [CrÃ©er une architecture personnalisÃ©e](./create_a_model) pour plus d'informations sur la crÃ©ation de configurations personnalisÃ©es."
        },
        {
            "sha": "671467e52d706b7f1d820a2736411a2606eaf1ca",
            "filename": "docs/source/fr/run_scripts_fr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 35,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Ffr%2Frun_scripts_fr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Ffr%2Frun_scripts_fr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Ffr%2Frun_scripts_fr.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -106,23 +106,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-\n-Le script d'exemple tÃ©lÃ©charge et prÃ©traite un jeu de donnÃ©es Ã  partir de la bibliothÃ¨que  ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/). Ensuite, le script ajuste un modÃ¨le Ã  l'aide de Keras sur une architecture qui prend en charge la tÃ¢che de rÃ©sumÃ©. L'exemple suivant montre comment ajuster le modÃ¨le [T5-small](https://huggingface.co/google-t5/t5-small) sur le jeu de donnÃ©es [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail). Le modÃ¨le T5 nÃ©cessite un argument supplÃ©mentaire source_prefix en raison de la faÃ§on dont il a Ã©tÃ© entraÃ®nÃ©. Cette invite permet Ã  T5 de savoir qu'il s'agit d'une tÃ¢che de rÃ©sumÃ©.\n-\n-```bash\n-python examples/tensorflow/summarization/run_summarization.py  \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## EntraÃ®nement distribuÃ© et prÃ©cision mixte\n@@ -174,23 +157,6 @@ python xla_spawn.py --num_cores 8 \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-Les scripts TensorFlow utilisent une [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy) pour l'entraÃ®nement sur TPU. Pour utiliser un TPU, passez le nom de la ressource TPU Ã  l'argument tpu.\n-\n-```bash\n-python run_summarization.py  \\\n-    --tpu name_of_tpu_resource \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## ExÃ©cuter un script avec ğŸ¤— Accelerate \n@@ -352,4 +318,4 @@ python examples/pytorch/summarization/run_summarization.py\n     --per_device_eval_batch_size=4 \\\n     --overwrite_output_dir \\\n     --predict_with_generate\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "e823fd5f5cb6b11bedd2bc333b3de92ee9513c60",
            "filename": "docs/source/it/autoclass_tutorial.md",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Fautoclass_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Fautoclass_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fautoclass_tutorial.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -101,23 +101,4 @@ Semplicemente utilizza lo stesso checkpoint per caricare un'architettura per un\n Generalmente, raccomandiamo di utilizzare la classe `AutoTokenizer` e la classe `AutoModelFor` per caricare istanze pre-allenate dei modelli. Questo ti assicurerÃ  di aver caricato la corretta architettura ogni volta. Nel prossimo [tutorial](preprocessing), imparerai come utilizzare il tokenizer, il feature extractor e il processore per elaborare un dataset per il fine-tuning.\n \n </pt>\n-<tf>\n-Infine, le classi `TFAutoModelFor` ti permettono di caricare un modello pre-allenato per un determinato compito (guarda [qui](model_doc/auto) per una lista completa di compiti presenti). Per esempio, carica un modello per la classificazione di sequenze con [`TFAutoModelForSequenceClassification.from_pretrained`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Semplicemente utilizza lo stesso checkpoint per caricare un'architettura per un task differente:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Generalmente, raccomandiamo di utilizzare la classe `AutoTokenizer` e la classe `TFAutoModelFor` per caricare istanze pre-allenate dei modelli. Questo ti assicurerÃ  di aver caricato la corretta architettura ogni volta. Nel prossimo [tutorial](preprocessing), imparerai come utilizzare il tokenizer, il feature extractor e il processore per elaborare un dataset per il fine-tuning.\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "b5c594ae03cd3f16a3794822a97bb0e9f4c943f1",
            "filename": "docs/source/it/create_a_model.md",
            "status": "modified",
            "additions": 1,
            "deletions": 44,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fcreate_a_model.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -136,32 +136,6 @@ Quando carichi pesi pre-allenati, la configurazione del modello predefinito Ã¨ a\n >>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n ```\n </pt>\n-<tf>\n-Carica gli attributi di configurazione personalizzati nel modello:\n-\n-```py\n->>> from transformers import TFDistilBertModel\n-\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")\n->>> tf_model = TFDistilBertModel(my_config)\n-```\n-\n-\n-Questo crea modelli con valori casuali invece di pesi pre-allenati. Non sarai in grado di usare questo modello per niente di utile finchÃ© non lo alleni. L'allenamento Ã¨ un processo costoso e che richiede tempo . Generalmente Ã¨ meglio usare un modello pre-allenato per ottenere risultati migliori velocemente, utilizzando solo una frazione delle risorse neccesarie per l'allenamento.\n-\n-Crea un modello pre-allenoto con [`~TFPreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Quando carichi pesi pre-allenati, la configurazione del modello predefinito Ã¨ automaticamente caricato se il modello Ã¨ fornito da ğŸ¤— Transformers. Tuttavia, puoi ancora sostituire gli attributi - alcuni o tutti - di configurazione del modello predefinito con i tuoi se lo desideri:\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n-```\n-\n-</tf>\n </frameworkcontent>\n \n ### Model head\n@@ -186,23 +160,6 @@ Riutilizza facilmente questo checkpoint per un'altra attivitÃ  passando ad un mo\n >>> model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n ```\n </pt>\n-<tf>\n-Per esempio, [`TFDistilBertForSequenceClassification`] Ã¨ un modello DistilBERT base con classificazione di sequenza head. La classificazione di sequenza head Ã¨ uno strato lineare sopra gli output raggruppati.\n-\n-```py\n->>> from transformers import TFDistilBertForSequenceClassification\n-\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Riutilizza facilmente questo checkpoint per un altra attivitÃ  passando ad un modello head diverso. Per un attivitÃ  di risposta alle domande, utilizzerai il model head [`TFDistilBertForQuestionAnswering`]. Il head di risposta alle domande Ã¨ simile alla sequenza di classificazione head tranne per il fatto che Ã¨ uno strato lineare sopra l'output degli stati nascosti (hidden states in inglese)\n-\n-```py\n->>> from transformers import TFDistilBertForQuestionAnswering\n-\n->>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Tokenizer\n@@ -358,4 +315,4 @@ Combinare l'estrattore di caratteristiche e il tokenizer in [`Wav2Vec2Processor`\n >>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n ```\n \n-Con due classi di base - configurazione e modello - e una classe di preelaborazione aggiuntiva (tokenizer, estrattore di caratteristiche o processore), puoi creare qualsiasi modello supportato da ğŸ¤— Transformers. Ognuna di queste classi base Ã¨ configurabile, consentendoti di utilizzare gli attributi specifici che desideri. Ãˆ possibile impostare facilmente un modello per l'addestramento o modificare un modello preallenato esistente per la messa a punto.\n\\ No newline at end of file\n+Con due classi di base - configurazione e modello - e una classe di preelaborazione aggiuntiva (tokenizer, estrattore di caratteristiche o processore), puoi creare qualsiasi modello supportato da ğŸ¤— Transformers. Ognuna di queste classi base Ã¨ configurabile, consentendoti di utilizzare gli attributi specifici che desideri. Ãˆ possibile impostare facilmente un modello per l'addestramento o modificare un modello preallenato esistente per la messa a punto."
        },
        {
            "sha": "7c527d5cd7710117f962224e3f4e67200d48d982",
            "filename": "docs/source/it/model_sharing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Fmodel_sharing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Fmodel_sharing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fmodel_sharing.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -90,30 +90,6 @@ Specifica `from_tf=True` per convertire un checkpoint da TensorFlow a PyTorch:\n >>> pt_model.save_pretrained(\"path/verso/il-nome-magnifico-che-hai-scelto\")\n ```\n </pt>\n-<tf>\n-Specifica `from_pt=True` per convertire un checkpoint da PyTorch a TensorFlow:\n-\n-```py\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\n-...     \"path/verso/il-nome-magnifico-che-hai-scelto\", from_pt=True\n-... )\n-```\n-\n-Poi puoi salvare il tuo nuovo modello in TensorFlow con il suo nuovo checkpoint:\n-\n-```py\n->>> tf_model.save_pretrained(\"path/verso/il-nome-magnifico-che-hai-scelto\")\n-```\n-</tf>\n-<jax>\n-Se un modello Ã¨ disponibile in Flax, puoi anche convertire un checkpoint da PyTorch a Flax:\n-\n-```py\n->>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\n-...     \"path/verso/il-nome-magnifico-che-hai-scelto\", from_pt=True\n-... )\n-```\n-</jax>\n </frameworkcontent>\n \n ## Condividi un modello durante il training\n@@ -146,29 +122,6 @@ Dopo aver effettuato il fine-tuning del tuo modello, chiama [`~transformers.Trai\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-Condividi un modello nell'Hub con [`PushToHubCallback`]. Nella funzione [`PushToHubCallback`], aggiungi:\n-\n-- Una directory di output per il tuo modello.\n-- Un tokenizer.\n-- L'`hub_model_id`, che Ã¨ il tuo username sull'Hub e il nome del modello.\n-\n-```py\n->>> from transformers import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"./il_path_dove_salvare_il_tuo_modello\",\n-...     tokenizer=tokenizer,\n-...     hub_model_id=\"il-tuo-username/il-mio-bellissimo-modello\",\n-... )\n-```\n-\n-Aggiungi il callback a [`fit`](https://keras.io/api/models/model_training_apis/), e ğŸ¤— Transformers caricherÃ  il modello allenato nell'Hub:\n-\n-```py\n->>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\n-```\n-</tf>\n </frameworkcontent>\n \n ## Utilizzare la funzione `push_to_hub`"
        },
        {
            "sha": "dda825c801e5461539930c7e99744d9f49c26f10",
            "filename": "docs/source/it/quicktour.md",
            "status": "modified",
            "additions": 0,
            "deletions": 81,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fquicktour.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -73,12 +73,6 @@ Installa le seguenti dipendenze se non lo hai giÃ  fatto:\n pip install torch\n ```\n </pt>\n-<tf>\n-\n-```bash\n-pip install tensorflow\n-```\n-</tf>\n </frameworkcontent>\n \n Importa [`pipeline`] e specifica il compito che vuoi completare:\n@@ -169,16 +163,6 @@ Usa [`AutoModelForSequenceClassification`] e [`AutoTokenizer`] per caricare il m\n >>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n ```\n </pt>\n-<tf>\n-Usa [`TFAutoModelForSequenceClassification`] e [`AutoTokenizer`] per caricare il modello pre-allenato e il suo tokenizer associato (maggiori informazioni su una `TFAutoClass` in seguito):\n-\n-```py\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n-```\n-</tf>\n </frameworkcontent>\n \n Poi puoi specificare il modello e il tokenizer nella [`pipeline`], e applicare il `classifier` sul tuo testo obiettivo:\n@@ -243,17 +227,6 @@ Come con la [`pipeline`], il tokenizer accetterÃ  una lista di input. In piÃ¹, i\n ... )\n ```\n </pt>\n-<tf>\n-```py\n->>> tf_batch = tokenizer(\n-...     [\"Siamo molto felici di mostrarti la libreria ğŸ¤— Transformers.\", \"Speriamo te non la odierai.\"],\n-...     padding=True,\n-...     truncation=True,\n-...     max_length=512,\n-...     return_tensors=\"tf\",\n-... )\n-```\n-</tf>\n </frameworkcontent>\n \n Leggi il tutorial sul [preprocessing](./preprocessing) per maggiori dettagli sulla tokenizzazione.\n@@ -294,36 +267,6 @@ tensor([[0.0041, 0.0037, 0.0203, 0.2005, 0.7713],\n         [0.3766, 0.3292, 0.1832, 0.0558, 0.0552]], grad_fn=<SoftmaxBackward0>)\n ```\n </pt>\n-<tf>\n-ğŸ¤— Transformers fornisce un metodo semplice e unificato per caricare istanze pre-allenate. Questo significa che puoi caricare un [`TFAutoModel`] come caricheresti un [`AutoTokenizer`]. L'unica differenza Ã¨ selezionare il [`TFAutoModel`] corretto per il compito di interesse. Dato che stai facendo classificazione di testi, o sequenze, carica [`TFAutoModelForSequenceClassification`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> nome_del_modello = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(nome_del_modello)\n-```\n-\n-<Tip>\n-\n-Guarda il [task summary](./task_summary) per sapere quale classe di [`AutoModel`] utilizzare per quale compito.\n-\n-</Tip>\n-\n-Ora puoi passare il tuo lotto di input pre-processati direttamente al modello passando le chiavi del dizionario al tensore:\n-\n-```py\n->>> tf_outputs = tf_model(tf_batch)\n-```\n-\n-Il modello produrrÃ  le attivazioni finali nell'attributo `logits`. Applica la funzione softmax a `logits` per ottenere le probabilitÃ :\n-```py\n->>> import tensorflow as tf\n-\n->>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n->>> tf_predictions  # doctest: +IGNORE_RESULT\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -360,21 +303,6 @@ Quando desideri utilizzare il tuo modello nuovamente, puoi ri-caricarlo con [`Pr\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")\n ```\n </pt>\n-<tf>\n-Una volta completato il fine-tuning del tuo modello, puoi salvarlo con il suo tokenizer utilizzando [`TFPreTrainedModel.save_pretrained`]:\n-\n-```py\n->>> tf_save_directory = \"./tf_save_pretrained\"\n->>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT\n->>> tf_model.save_pretrained(tf_save_directory)\n-```\n-\n-Quando desideri utilizzare il tuo modello nuovamente, puoi ri-caricarlo con [`TFPreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n-```\n-</tf>\n </frameworkcontent>\n \n Una caratteristica particolarmente interessante di ğŸ¤— Transformers Ã¨ la sua abilitÃ  di salvare un modello e ri-caricarlo sia come modello di PyTorch che di TensorFlow. I parametri `from_pt` o `from_tf` possono convertire un modello da un framework all'altro:\n@@ -389,13 +317,4 @@ Una caratteristica particolarmente interessante di ğŸ¤— Transformers Ã¨ la sua a\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "0d3f2d32351dd747bc4de76b6c51cd00d6054abb",
            "filename": "docs/source/it/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Frun_scripts.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -104,22 +104,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-Lo script di esempio scarica e pre-processa un dataset dalla libreria ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/). Successivamente, lo script esegue il fine-tuning su un dataset usando Keras su un'architettura che supporta la summarization. Il seguente esempio mostra come eseguire il fine-tuning di [T5-small](https://huggingface.co/google-t5/t5-small) sul dataset [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail). Il modello T5 richiede un parametro addizionale `source_prefix` a causa del modo in cui Ã¨ stato addestrato. Questo prefisso permette a T5 di sapere che si tratta di un task di summarization.\n-\n-```bash\n-python examples/tensorflow/summarization/run_summarization.py  \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## Addestramento distribuito e precisione mista\n@@ -170,23 +154,6 @@ python xla_spawn.py --num_cores 8 \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-Le Tensor Processing Units (TPU) sono state progettate per migliorare le prestazioni. Gli script TensorFlow utilizzano una [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy) per eseguire l'addestramento su TPU. Per usare una TPU, passa il nome della risorsa TPU all'argomento `tpu`.\n-\n-```bash\n-python run_summarization.py  \\\n-    --tpu name_of_tpu_resource \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## Esegui uno script con ğŸ¤— Accelerate"
        },
        {
            "sha": "2edd837533f2e4b2cdb302d6386d9ef902528325",
            "filename": "docs/source/it/serialization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Fserialization.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -201,25 +201,6 @@ del pacchetto `transformers.onnx` nella directory desiderata:\n python -m transformers.onnx --model=local-pt-checkpoint onnx/\n ```\n </pt>\n-<tf>\n-```python\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> # Load tokenizer and TensorFlow weights from the Hub\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n->>> # Save to disk\n->>> tokenizer.save_pretrained(\"local-tf-checkpoint\")\n->>> tf_model.save_pretrained(\"local-tf-checkpoint\")\n-```\n-\n-Once the checkpoint is saved, we can export it to ONNX by pointing the `--model`\n-argument of the `transformers.onnx` package to the desired directory:\n-\n-```bash\n-python -m transformers.onnx --model=local-tf-checkpoint onnx/\n-```\n-</tf>\n </frameworkcontent>\n \n ### Selezione delle caratteristiche per diverse topologie di modello\n@@ -673,4 +654,4 @@ torch.neuron.trace(model, [token_tensor, segments_tensors])\n Questa modifica consente a Neuron SDK di tracciare il modello e ottimizzarlo per l'esecuzione nelle istanze Inf1.\n \n Per ulteriori informazioni sulle funzionalitÃ , gli strumenti, i tutorial di esempi e gli ultimi aggiornamenti di AWS Neuron SDK,\n-consultare la [documentazione AWS NeuronSDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html).\n\\ No newline at end of file\n+consultare la [documentazione AWS NeuronSDK](https://awsdocs-neuron.readthedocs-hosted.com/en/latest/index.html)."
        },
        {
            "sha": "9772ff4a5bbc8ca01dd659ed0abf15136376eb4b",
            "filename": "docs/source/it/training.md",
            "status": "modified",
            "additions": 0,
            "deletions": 66,
            "changes": 66,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Ftraining.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fit%2Ftraining.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fit%2Ftraining.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -149,72 +149,6 @@ Poi metti a punto il modello richiamando [`~transformers.Trainer.train`]:\n >>> trainer.train()\n ```\n </pt>\n-<tf>\n-<a id='keras'></a>\n-\n-<Youtube id=\"rnTGBy2ax1c\"/>\n-\n-I modelli ğŸ¤— Transformers supportano anche l'addestramento in TensorFlow usando l'API di Keras.\n-\n-### Convertire dataset nel formato per TensorFlow\n-\n-Il [`DefaultDataCollator`] assembla tensori in lotti su cui il modello si addestrerÃ . Assicurati di specificare di restituire tensori per TensorFlow in `return_tensors`:\n-\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-```\n-\n-<Tip>\n-\n-[`Trainer`] usa [`DataCollatorWithPadding`] in maniera predefinita in modo da non dover specificare esplicitamente un collettore di dati.\n-\n-</Tip>\n-\n-Successivamente, converti i datasets tokenizzati in TensorFlow datasets con il metodo [`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes.html#datasets.Dataset.to_tf_dataset). Specifica il tuo input in `columns` e le tue etichette in `label_cols`:\n-\n-```py\n->>> tf_train_dataset = small_train_dataset.to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n-...     label_cols=[\"labels\"],\n-...     shuffle=True,\n-...     collate_fn=data_collator,\n-...     batch_size=8,\n-... )\n-\n->>> tf_validation_dataset = small_eval_dataset.to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"token_type_ids\"],\n-...     label_cols=[\"labels\"],\n-...     shuffle=False,\n-...     collate_fn=data_collator,\n-...     batch_size=8,\n-... )\n-```\n-\n-### Compilazione e addestramento\n-\n-Carica un modello TensorFlow col numero atteso di etichette:\n-\n-```py\n->>> import tensorflow as tf\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\", num_labels=5)\n-```\n-\n-Poi compila e fai il fine-tuning del tuo modello usando [`fit`](https://keras.io/api/models/model_training_apis/) come faresti con qualsiasi altro modello di Keras:\n-\n-```py\n->>> model.compile(\n-...     optimizer=tf.keras.optimizers.Adam(learning_rate=5e-5),\n-...     loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n-...     metrics=tf.metrics.SparseCategoricalAccuracy(),\n-... )\n-\n->>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3)\n-```\n-</tf>\n </frameworkcontent>\n \n <a id='pytorch_native'></a>"
        },
        {
            "sha": "f28a2b042b192ee2b599d9e98584e0c31099fa4c",
            "filename": "docs/source/ja/autoclass_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Fautoclass_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Fautoclass_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fautoclass_tutorial.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -136,26 +136,4 @@ TensorFlowãŠã‚ˆã³Flaxã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã«ã¯å½±éŸ¿ãŒãªãã€`from_\n ã“ã‚Œã«ã‚ˆã‚Šã€å¸¸ã«æ­£ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n æ¬¡ã®[tutorial](preprocessing)ã§ã¯ã€æ–°ã—ããƒ­ãƒ¼ãƒ‰ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã€ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã€ç‰¹å¾´é‡æŠ½å‡ºå™¨ã€ãŠã‚ˆã³ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä½¿ç”¨ã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‰å‡¦ç†ã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚\n </pt>\n-<tf>\n-æœ€å¾Œã«ã€`TFAutoModelFor`ã‚¯ãƒ©ã‚¹ã¯ç‰¹å®šã®ã‚¿ã‚¹ã‚¯ã«å¯¾ã—ã¦äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼ˆä½¿ç”¨å¯èƒ½ãªã‚¿ã‚¹ã‚¯ã®å®Œå…¨ãªä¸€è¦§ã«ã¤ã„ã¦ã¯ã“ã¡ã‚‰ã‚’å‚ç…§ï¼‰ã€‚\n-ãŸã¨ãˆã°ã€[`TFAutoModelForSequenceClassification.from_pretrained`]ã‚’ä½¿ç”¨ã—ã¦ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†é¡ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼š\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-åŒã˜ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å†åˆ©ç”¨ã—ã¦ç•°ãªã‚‹ã‚¿ã‚¹ã‚¯ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ï¼š\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-ä¸€èˆ¬çš„ã«ã¯ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã®ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã«`AutoTokenizer`ã‚¯ãƒ©ã‚¹ã¨`TFAutoModelFor`ã‚¯ãƒ©ã‚¹ã®ä½¿ç”¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n-ã“ã‚Œã«ã‚ˆã‚Šã€å¸¸ã«æ­£ã—ã„ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n-æ¬¡ã®[tutorial](preproccesing)ã§ã¯ã€æ–°ã—ããƒ­ãƒ¼ãƒ‰ã—ãŸãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã€ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã€ç‰¹å¾´é‡æŠ½å‡ºå™¨ã€ãŠã‚ˆã³ãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ä½¿ç”¨ã—ã¦ã€ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’å‰å‡¦ç†ã™ã‚‹æ–¹æ³•ã‚’å­¦ã³ã¾ã™ã€‚\n-</tf>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "913e992d5a9f7085b41782ae187e84d871ee7c4d",
            "filename": "docs/source/ja/create_a_model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fcreate_a_model.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -145,33 +145,6 @@ Once you are satisfied with your model configuration, you can save it with [`Pre\n >>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n ```\n </pt>\n-<tf>\n-ãƒ¢ãƒ‡ãƒ«ã«ã‚«ã‚¹ã‚¿ãƒ è¨­å®šå±æ€§ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ãã ã•ã„ï¼š\n-\n-```py\n->>> from transformers import TFDistilBertModel\n-\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")\n->>> tf_model = TFDistilBertModel(my_config)\n-```\n-\n-ã“ã‚Œã«ã‚ˆã‚Šã€äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã§ã¯ãªããƒ©ãƒ³ãƒ€ãƒ ãªå€¤ã‚’æŒã¤ãƒ¢ãƒ‡ãƒ«ãŒä½œæˆã•ã‚Œã¾ã™ã€‚\n-ã“ã®ãƒ¢ãƒ‡ãƒ«ã‚’æœ‰ç”¨ãªç›®çš„ã«ã¯ã¾ã ä½¿ç”¨ã™ã‚‹ã“ã¨ã¯ã§ãã¾ã›ã‚“ã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã¯ã‚³ã‚¹ãƒˆãŒã‹ã‹ã‚Šã€æ™‚é–“ãŒã‹ã‹ã‚‹ãƒ—ãƒ­ã‚»ã‚¹ã§ã™ã€‚\n-ä¸€èˆ¬çš„ã«ã¯ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«å¿…è¦ãªãƒªã‚½ãƒ¼ã‚¹ã®ä¸€éƒ¨ã—ã‹ä½¿ç”¨ã›ãšã«ã€ã‚ˆã‚Šé€Ÿãå„ªã‚ŒãŸçµæœã‚’å¾—ã‚‹ãŸã‚ã«äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ãŒè‰¯ã„ã§ã—ã‚‡ã†ã€‚\n-\n-[`~TFPreTrainedModel.from_pretrained`]ã‚’ä½¿ç”¨ã—ã¦äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã—ã¾ã™ï¼š\n-\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-äº‹å‰å­¦ç¿’æ¸ˆã¿ã®é‡ã¿ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹éš›ã€ãƒ¢ãƒ‡ãƒ«ãŒğŸ¤— Transformersã«ã‚ˆã£ã¦æä¾›ã•ã‚Œã¦ã„ã‚‹å ´åˆã€ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«æ§‹æˆãŒè‡ªå‹•çš„ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã¾ã™ã€‚ãŸã ã—ã€å¿…è¦ã§ã‚ã‚Œã°ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¢ãƒ‡ãƒ«æ§‹æˆå±æ€§ã®ä¸€éƒ¨ã¾ãŸã¯ã™ã¹ã¦ã‚’ç‹¬è‡ªã®ã‚‚ã®ã§ç½®ãæ›ãˆã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼š\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n-```\n-</tf>\n </frameworkcontent>\n \n \n@@ -200,26 +173,6 @@ Once you are satisfied with your model configuration, you can save it with [`Pre\n ```\n \n </pt>\n-<tf>\n-ä¾‹ãˆã°ã€[`TFDistilBertForSequenceClassification`]ã¯ã€ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†é¡ãƒ˜ãƒƒãƒ‰ã‚’æŒã¤ãƒ™ãƒ¼ã‚¹ã®DistilBERTãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†é¡ãƒ˜ãƒƒãƒ‰ã¯ã€ãƒ—ãƒ¼ãƒ«ã•ã‚ŒãŸå‡ºåŠ›ã®ä¸Šã«ã‚ã‚‹ç·šå½¢å±¤ã§ã™ã€‚\n-\n-```py\n->>> from transformers import TFDistilBertForSequenceClassification\n-\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-åˆ¥ã®ã‚¿ã‚¹ã‚¯ã«ã“ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç°¡å˜ã«å†åˆ©ç”¨ã™ã‚‹ã“ã¨ãŒã§ãã€ç•°ãªã‚‹ãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã«åˆ‡ã‚Šæ›¿ãˆã‚‹ã ã‘ã§ã™ã€‚\n-è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã®å ´åˆã€[`TFDistilBertForQuestionAnswering`]ãƒ¢ãƒ‡ãƒ«ãƒ˜ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n-è³ªå•å¿œç­”ãƒ˜ãƒƒãƒ‰ã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹åˆ†é¡ãƒ˜ãƒƒãƒ‰ã¨ä¼¼ã¦ã„ã¾ã™ãŒã€éš ã‚ŒçŠ¶æ…‹ã®å‡ºåŠ›ã®ä¸Šã«ç·šå½¢å±¤ãŒã‚ã‚‹ã ã‘ã§ã™ã€‚\n-\n-\n-```py\n->>> from transformers import TFDistilBertForQuestionAnswering\n-\n->>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Tokenizer"
        },
        {
            "sha": "f602208f04e57c49986e9749abe303c45842f223",
            "filename": "docs/source/ja/model_sharing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 46,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Fmodel_sharing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Fmodel_sharing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_sharing.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -94,30 +94,6 @@ TensorFlowã‹ã‚‰PyTorchã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å¤‰æ›ã™ã‚‹ã«ã¯ã€`from_\n >>> pt_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n ```\n </pt>\n-<tf>\n-\n-æŒ‡å®šã—ã¦ã€PyTorchã‹ã‚‰TensorFlowã«ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’å¤‰æ›ã™ã‚‹ã«ã¯ `from_pt=True` ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š\n-\n-```python\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\n-```\n-\n-æ–°ã—ã„TensorFlowãƒ¢ãƒ‡ãƒ«ã¨ãã®æ–°ã—ã„ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ä¿å­˜ã§ãã¾ã™ï¼š\n-\n-```python\n->>> tf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n-```\n-</tf>\n-<tf>\n-<jax>\n-Flaxã§ãƒ¢ãƒ‡ãƒ«ãŒåˆ©ç”¨å¯èƒ½ãªå ´åˆã€PyTorchã‹ã‚‰Flaxã¸ã®ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã®å¤‰æ›ã‚‚è¡Œã†ã“ã¨ãŒã§ãã¾ã™ï¼š\n-\n-```py\n->>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\n-...     \"path/to/awesome-name-you-picked\", from_pt=True\n-... )\n-```\n-</jax>\n </frameworkcontent>\n \n ## Push a model during traning\n@@ -165,28 +141,6 @@ Pass your training arguments as usual to [`Trainer`]:\n ```\n \n </pt>\n-<tf>\n-\n-[`PushToHubCallback`]ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’Hubã«å…±æœ‰ã—ã¾ã™ã€‚[`PushToHubCallback`]é–¢æ•°ã«ã¯ã€æ¬¡ã®ã‚‚ã®ã‚’è¿½åŠ ã—ã¾ã™ï¼š\n-\n-- ãƒ¢ãƒ‡ãƒ«ã®å‡ºåŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã€‚\n-- ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã€‚\n-- `hub_model_id`ã€ã¤ã¾ã‚ŠHubã®ãƒ¦ãƒ¼ã‚¶ãƒ¼åã¨ãƒ¢ãƒ‡ãƒ«åã€‚\n-\n-```python\n->>> from transformers import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\n-... )\n-```\n-\n-ğŸ¤— Transformersã¯[`fit`](https://keras.io/api/models/model_training_apis/)ã«ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ ã—ã€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã‚’Hubã«ãƒ—ãƒƒã‚·ãƒ¥ã—ã¾ã™ï¼š\n-\n-```py\n->>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\n-```\n-</tf>\n </frameworkcontent>\n \n ## `push_to_hub` é–¢æ•°ã‚’ä½¿ç”¨ã™ã‚‹"
        },
        {
            "sha": "9f61595e7c33cdd7f15cf4b625702838634db866",
            "filename": "docs/source/ja/preprocessing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Fpreprocessing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Fpreprocessing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fpreprocessing.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -196,30 +196,6 @@ pip install datasets\n                            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n ```\n </pt>\n-<tf>\n-```py\n->>> batch_sentences = [\n-...     \"But what about second breakfast?\",\n-...     \"Don't think he knows about second breakfast, Pip.\",\n-...     \"What about elevensies?\",\n-... ]\n->>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n->>> print(encoded_input)\n-{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n-       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n-       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n-      dtype=int32)>,\n- 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n- 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n-       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n-       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n-```\n-</tf>\n </frameworkcontent>\n \n ## Audio"
        },
        {
            "sha": "e077d512df4fa9b6bf73f068ca57b97f52afff2f",
            "filename": "docs/source/ja/quicktour.md",
            "status": "modified",
            "additions": 0,
            "deletions": 98,
            "changes": 98,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fquicktour.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -38,12 +38,6 @@ specific language governing permissions and limitations under the License.\n pip install torch\n ```\n </pt>\n-<tf>\n-\n-```bash\n-pip install tensorflow\n-```\n-</tf>\n </frameworkcontent>\n \n ## Pipeline\n@@ -155,16 +149,6 @@ label: NEGATIVE, ã‚¹ã‚³ã‚¢: 0.5309\n ```\n \n </pt>\n-<tf>\n-ä»¥ä¸‹ã®ã‚³ãƒ¼ãƒ‰ã¯ã€[`TFAutoModelForSequenceClassification`]ãŠã‚ˆã³[`AutoTokenizer`]ã‚’ä½¿ç”¨ã—ã¦ã€äº‹å‰å­¦ç¿’æ¸ˆã¿ãƒ¢ãƒ‡ãƒ«ã¨ãã®é–¢é€£ã™ã‚‹ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¦ã„ã¾ã™ï¼ˆ`TFAutoClass`ã«ã¤ã„ã¦ã¯æ¬¡ã®ã‚»ã‚¯ã‚·ãƒ§ãƒ³ã§è©³ã—ãèª¬æ˜ã—ã¾ã™ï¼‰ï¼š\n-\n-```python\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n-```\n-</tf>\n </frameworkcontent>\n \n æŒ‡å®šã—ãŸãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’[`pipeline`]ã«è¨­å®šã—ã€ä»Šåº¦ã¯ãƒ•ãƒ©ãƒ³ã‚¹èªã®ãƒ†ã‚­ã‚¹ãƒˆã«`classifier`ã‚’é©ç”¨ã§ãã¾ã™ï¼š\n@@ -235,18 +219,6 @@ Pass your text to the tokenizer:\n ... )\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> tf_batch = tokenizer(\n-...     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n-...     padding=True,\n-...     truncation=True,\n-...     max_length=512,\n-...     return_tensors=\"tf\",\n-... )\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -295,41 +267,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ```\n \n </pt>\n-<tf>\n-ğŸ¤— Transformersã¯äº‹å‰å­¦ç¿’æ¸ˆã¿ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ãŸã‚ã®ã‚·ãƒ³ãƒ—ãƒ«ã§çµ±ä¸€ã•ã‚ŒãŸæ–¹æ³•ã‚’æä¾›ã—ã¾ã™ã€‚\n-ã“ã‚Œã¯ã€[`TFAutoModel`]ã‚’[`AutoTokenizer`]ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹ã®ã¨åŒã˜ã‚ˆã†ã«ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã“ã¨ã‚’æ„å‘³ã—ã¾ã™ã€‚\n-å”¯ä¸€ã®é•ã„ã¯ã€ã‚¿ã‚¹ã‚¯ã«é©ã—ãŸ[`TFAutoModel`]ã‚’é¸æŠã™ã‚‹ã“ã¨ã§ã™ã€‚\n-ãƒ†ã‚­ã‚¹ãƒˆï¼ˆã¾ãŸã¯ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ï¼‰åˆ†é¡ã®å ´åˆã€[`TFAutoModelForSequenceClassification`]ã‚’ãƒ­ãƒ¼ãƒ‰ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n-```\n-\n-<Tip>\n-\n-è©³ç´°ã«ã¤ã„ã¦ã¯ã€[`AutoModel`]ã‚¯ãƒ©ã‚¹ã§ã‚µãƒãƒ¼ãƒˆã•ã‚Œã¦ã„ã‚‹ã‚¿ã‚¹ã‚¯ã«é–¢ã™ã‚‹æƒ…å ±ã¯ã€[ã‚¿ã‚¹ã‚¯ã®æ¦‚è¦](./task_summary)ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n-\n-</Tip>\n-\n-æ¬¡ã«ã€å‰å‡¦ç†æ¸ˆã¿ã®ãƒãƒƒãƒã‚’ç›´æ¥ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã¾ã™ã€‚ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãã®ã¾ã¾æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ï¼š\n-\n-```python\n->>> tf_outputs = tf_model(tf_batch)\n-```\n-\n-ãƒ¢ãƒ‡ãƒ«ã¯`logits`å±æ€§ã«æœ€çµ‚çš„ãªã‚¢ã‚¯ãƒ†ã‚£ãƒ™ãƒ¼ã‚·ãƒ§ãƒ³ã‚’å‡ºåŠ›ã—ã¾ã™ã€‚`logits`ã«ã‚½ãƒ•ãƒˆãƒãƒƒã‚¯ã‚¹é–¢æ•°ã‚’é©ç”¨ã—ã¦ç¢ºç‡ã‚’å–å¾—ã—ã¾ã™ï¼š\n-\n-```python\n->>> import tensorflow as tf\n-\n->>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n->>> tf_predictions  # doctest: +IGNORE_RESULT\n-```\n-\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -360,22 +297,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ```\n \n </pt>\n-<tf>\n-ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ãŸã‚‰ã€ãã®ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã§ãã¾ã™ã€‚[`TFPreTrainedModel.save_pretrained`]ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š\n-\n-```py\n->>> tf_save_directory = \"./tf_save_pretrained\"\n->>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT\n->>> tf_model.save_pretrained(tf_save_directory)\n-```\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’å†åº¦ä½¿ç”¨ã™ã‚‹æº–å‚™ãŒã§ããŸã‚‰ã€[`TFPreTrainedModel.from_pretrained`]ã‚’ä½¿ç”¨ã—ã¦å†åº¦ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ï¼š\n-\n-```py\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n-```\n-\n-</tf>\n </frameworkcontent>\n \n ğŸ¤— Transformersã®ç‰¹ã«ç´ æ™´ã‚‰ã—ã„æ©Ÿèƒ½ã®ä¸€ã¤ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’ä¿å­˜ã—ã€ãã‚Œã‚’PyTorchãƒ¢ãƒ‡ãƒ«ã¾ãŸã¯TensorFlowãƒ¢ãƒ‡ãƒ«ã¨ã—ã¦å†ãƒ­ãƒ¼ãƒ‰ã§ãã‚‹ã“ã¨ã§ã™ã€‚ `from_pt`ã¾ãŸã¯`from_tf`ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ•ãƒ¬ãƒ¼ãƒ ãƒ¯ãƒ¼ã‚¯é–“ã§å¤‰æ›ã§ãã¾ã™ï¼š\n@@ -391,15 +312,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ```\n \n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n-```\n-</tf>\n </frameworkcontent>\n \n ## Custom model builds\n@@ -425,16 +337,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n ```\n \n </pt>\n-<tf>\n-ã‚«ã‚¹ã‚¿ãƒ æ§‹æˆã‹ã‚‰ãƒ¢ãƒ‡ãƒ«ã‚’ä½œæˆã™ã‚‹ã«ã¯ã€[`TFAutoModel.from_config`]ã‚’ä½¿ç”¨ã—ã¾ã™ï¼š\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> my_model = TFAutoModel.from_config(my_config)\n-```\n-\n-</tf>\n </frameworkcontent>\n \n [ã‚«ã‚¹ã‚¿ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ä½œæˆ](./create_a_model)ã‚¬ã‚¤ãƒ‰ã‚’å‚ç…§ã—ã¦ã€ã‚«ã‚¹ã‚¿ãƒ æ§‹æˆã®è©³ç´°æƒ…å ±ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚"
        },
        {
            "sha": "af0c1fdb1a507ca053c9bb2d828942a0b772ae5d",
            "filename": "docs/source/ja/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Frun_scripts.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -111,23 +111,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n ```\n \n </pt>\n-<tf>\n-ã“ã®ä¾‹ã®ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ€ã‚¦ãƒ³ãƒ­ãƒ¼ãƒ‰ã—ã¦å‰å‡¦ç†ã—ã¾ã™ã€‚ãã®å¾Œã€ã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯è¦ç´„ã‚’ã‚µãƒãƒ¼ãƒˆã™ã‚‹ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ä¸Šã§ Keras ã‚’ä½¿ç”¨ã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã—ã¾ã™ã€‚ä»¥ä¸‹ã®ä¾‹ã§ã¯ã€[T5-small](https://huggingface.co/google-t5/t5-small) ã‚’ [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã§ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¦ã„ã¾ã™ã€‚T5 ãƒ¢ãƒ‡ãƒ«ã¯ã€ãã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°æ–¹æ³•ã«èµ·å› ã—ã¦è¿½åŠ ã® `source_prefix` å¼•æ•°ãŒå¿…è¦ã§ã™ã€‚ã“ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¯ã€T5 ã«ã“ã‚ŒãŒè¦ç´„ã‚¿ã‚¹ã‚¯ã§ã‚ã‚‹ã“ã¨ã‚’çŸ¥ã‚‰ã›ã¾ã™ã€‚\n-\n-\n-```bash\n-python examples/tensorflow/summarization/run_summarization.py  \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## Distributed training and mixed precision\n@@ -180,23 +163,6 @@ python xla_spawn.py --num_cores 8 \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-ã‚‚ã¡ã‚ã‚“ã€Tensor Processing Unitsï¼ˆTPUsï¼‰ã¯æ€§èƒ½ã‚’é«˜é€ŸåŒ–ã™ã‚‹ãŸã‚ã«ç‰¹åˆ¥ã«è¨­è¨ˆã•ã‚Œã¦ã„ã¾ã™ã€‚TensorFlowã‚¹ã‚¯ãƒªãƒ—ãƒˆã¯ã€TPUsã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ãŸã‚ã«[`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy)ã‚’åˆ©ç”¨ã—ã¾ã™ã€‚TPUã‚’ä½¿ç”¨ã™ã‚‹ã«ã¯ã€TPUãƒªã‚½ãƒ¼ã‚¹ã®åå‰ã‚’`tpu`å¼•æ•°ã«æ¸¡ã—ã¾ã™ã€‚\n-\n-```bash\n-python run_summarization.py  \\\n-    --tpu name_of_tpu_resource \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## Run a script with ğŸ¤— Accelerate"
        },
        {
            "sha": "3a048e396effc4d72b4631cab587ba681bef0f35",
            "filename": "docs/source/ja/tasks/image_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 232,
            "changes": 232,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fimage_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fimage_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fimage_classification.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -156,94 +156,6 @@ Datasetsã€ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ Food-101 ãƒ‡ãƒ¼ã‚¿ã‚»\n </pt>\n </frameworkcontent>\n \n-\n-<frameworkcontent>\n-<tf>\n-\n-éå‰°é©åˆã‚’å›é¿ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ã‚ˆã‚Šå …ç‰¢ã«ã™ã‚‹ãŸã‚ã«ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°éƒ¨åˆ†ã«ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’è¿½åŠ ã—ã¾ã™ã€‚\n-ã“ã“ã§ã¯ã€Keras å‰å‡¦ç†ãƒ¬ã‚¤ãƒ¤ãƒ¼ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ› (ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’å«ã‚€) ã‚’å®šç¾©ã—ã¾ã™ã€‚\n-æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã®å¤‰æ› (ä¸­å¤®ã®ãƒˆãƒªãƒŸãƒ³ã‚°ã€ã‚µã‚¤ã‚ºå¤‰æ›´ã€æ­£è¦åŒ–ã®ã¿)ã€‚ `tf.image` ã¾ãŸã¯\n-ä»–ã®ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã§ã‚‚æ§‹ã„ã¾ã›ã‚“ã€‚\n-\n-\n-```py\n->>> from tensorflow import keras\n->>> from tensorflow.keras import layers\n-\n->>> size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n-\n->>> train_data_augmentation = keras.Sequential(\n-...     [\n-...         layers.RandomCrop(size[0], size[1]),\n-...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n-...         layers.RandomFlip(\"horizontal\"),\n-...         layers.RandomRotation(factor=0.02),\n-...         layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n-...     ],\n-...     name=\"train_data_augmentation\",\n-... )\n-\n->>> val_data_augmentation = keras.Sequential(\n-...     [\n-...         layers.CenterCrop(size[0], size[1]),\n-...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n-...     ],\n-...     name=\"val_data_augmentation\",\n-... )\n-```\n-\n-æ¬¡ã«ã€ä¸€åº¦ã« 1 ã¤ã®ç”»åƒã§ã¯ãªãã€ç”»åƒã®ãƒãƒƒãƒã«é©åˆ‡ãªå¤‰æ›ã‚’é©ç”¨ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚\n-\n-```py\n->>> import numpy as np\n->>> import tensorflow as tf\n->>> from PIL import Image\n-\n-\n->>> def convert_to_tf_tensor(image: Image):\n-...     np_image = np.array(image)\n-...     tf_image = tf.convert_to_tensor(np_image)\n-...     # `expand_dims()` is used to add a batch dimension since\n-...     # the TF augmentation layers operates on batched inputs.\n-...     return tf.expand_dims(tf_image, 0)\n-\n-\n->>> def preprocess_train(example_batch):\n-...     \"\"\"Apply train_transforms across a batch.\"\"\"\n-...     images = [\n-...         train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n-...     ]\n-...     example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n-...     return example_batch\n-\n-\n-... def preprocess_val(example_batch):\n-...     \"\"\"Apply val_transforms across a batch.\"\"\"\n-...     images = [\n-...         val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n-...     ]\n-...     example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n-...     return example_batch\n-```\n-\n-ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ [`~datasets.Dataset.set_transform`] ã‚’ä½¿ç”¨ã—ã¦ã€ãã®å ´ã§å¤‰æ›ã‚’é©ç”¨ã—ã¾ã™ã€‚\n-\n-```py\n-food[\"train\"].set_transform(preprocess_train)\n-food[\"test\"].set_transform(preprocess_val)\n-```\n-\n-æœ€å¾Œã®å‰å‡¦ç†ã‚¹ãƒ†ãƒƒãƒ—ã¨ã—ã¦ã€`DefaultDataCollatâ€‹â€‹or`ã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒƒãƒã‚’ä½œæˆã—ã¾ã™ã€‚ ğŸ¤— Transformers ã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ç…§åˆæ©Ÿèƒ½ã¨ã¯ç•°ãªã‚Šã€\n-`DefaultDataCollatâ€‹â€‹or` ã¯ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãªã©ã®è¿½åŠ ã®å‰å‡¦ç†ã‚’é©ç”¨ã—ã¾ã›ã‚“ã€‚\n-\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-```\n-</tf>\n-</frameworkcontent>\n-\n ## Evaluate\n \n ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚ã‚‹ã¨ã€å¤šãã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ã™ãã«ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™\n@@ -338,117 +250,6 @@ food[\"test\"].set_transform(preprocess_val)\n </pt>\n </frameworkcontent>\n \n-<frameworkcontent>\n-<tf>\n-\n-<Tip>\n-\n-\n-Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ã¾ãš [åŸºæœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](./training#train-a-tensorflow-model-with-keras) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n-\n-</Tip>\n-\n-\n-TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€æ¬¡ã®æ‰‹é †ã«å¾“ã„ã¾ã™ã€‚\n-1. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚\n-2. äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚\n-3. ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` ã«å¤‰æ›ã—ã¾ã™ã€‚\n-4. ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚\n-5. ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ ã—ã€`fit()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n-6. ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã—ã¾ã™ã€‚\n-\n-ã¾ãšã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_epochs = 5\n->>> num_train_steps = len(food[\"train\"]) * num_epochs\n->>> learning_rate = 3e-5\n->>> weight_decay_rate = 0.01\n-\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=learning_rate,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=weight_decay_rate,\n-...     num_warmup_steps=0,\n-... )\n-```\n-\n-æ¬¡ã«ã€ãƒ©ãƒ™ãƒ« ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ã¨ã‚‚ã« [`TFAutoModelForImageClassification`] ã‚’ä½¿ç”¨ã—ã¦ ViT ã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForImageClassification\n-\n->>> model = TFAutoModelForImageClassification.from_pretrained(\n-...     checkpoint,\n-...     id2label=id2label,\n-...     label2id=label2id,\n-... )\n-```\n-\n-Convert your datasets to the `tf.data.Dataset` format using the [`~datasets.Dataset.to_tf_dataset`] and your `data_collator`:\n-\n-```py\n->>> # converting our train dataset to tf.data.Dataset\n->>> tf_train_dataset = food[\"train\"].to_tf_dataset(\n-...     columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n-... )\n-\n->>> # converting our test dataset to tf.data.Dataset\n->>> tf_eval_dataset = food[\"test\"].to_tf_dataset(\n-...     columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n-... )\n-```\n-\n-`compile()` ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã«ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚\n-\n-```py\n->>> from tensorflow.keras.losses import SparseCategoricalCrossentropy\n-\n->>> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n->>> model.compile(optimizer=optimizer, loss=loss)\n-```\n-\n-äºˆæ¸¬ã‹ã‚‰ç²¾åº¦ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã«ã¯ã€[Keras callbacks](../main_classes/keras_callbacks) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n-`compute_metrics` é–¢æ•°ã‚’ [KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback) ã«æ¸¡ã—ã¾ã™ã€‚\n-[PushToHubCallback](../main_classes/keras_callbacks#transformers.PushToHubCallback) ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"food_classifier\",\n-...     tokenizer=image_processor,\n-...     save_strategy=\"no\",\n-... )\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€\n-ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯:\n-\n-```py\n->>> model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)\n-Epoch 1/5\n-250/250 [==============================] - 313s 1s/step - loss: 2.5623 - val_loss: 1.4161 - accuracy: 0.9290\n-Epoch 2/5\n-250/250 [==============================] - 265s 1s/step - loss: 0.9181 - val_loss: 0.6808 - accuracy: 0.9690\n-Epoch 3/5\n-250/250 [==============================] - 252s 1s/step - loss: 0.3910 - val_loss: 0.4303 - accuracy: 0.9820\n-Epoch 4/5\n-250/250 [==============================] - 251s 1s/step - loss: 0.2028 - val_loss: 0.3191 - accuracy: 0.9900\n-Epoch 5/5\n-250/250 [==============================] - 238s 949ms/step - loss: 0.1232 - val_loss: 0.3259 - accuracy: 0.9890\n-```\n-\n-ãŠã‚ã§ã¨ã†ï¼ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã€ğŸ¤— Hub ã§å…±æœ‰ã—ã¾ã—ãŸã€‚ã“ã‚Œã§æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n-</tf>\n-</frameworkcontent>\n-\n-\n <Tip>\n \n ç”»åƒåˆ†é¡ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹æ–¹æ³•ã®è©³ç´°ãªä¾‹ã«ã¤ã„ã¦ã¯ã€å¯¾å¿œã™ã‚‹ [PyTorch ãƒãƒ¼ãƒˆãƒ–ãƒƒã‚¯](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/image_classification.ipynb)\n@@ -518,36 +319,3 @@ Epoch 5/5\n ```\n </pt>\n </frameworkcontent>\n-\n-<frameworkcontent>\n-<tf>\n-\n-ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ç”»åƒã‚’å‰å‡¦ç†ã—ã€`input`ã‚’ TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import AutoImageProcessor\n-\n->>> image_processor = AutoImageProcessor.from_pretrained(\"MariaK/food_classifier\")\n->>> inputs = image_processor(image, return_tensors=\"tf\")\n-```\n-\n-å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForImageClassification\n-\n->>> model = TFAutoModelForImageClassification.from_pretrained(\"MariaK/food_classifier\")\n->>> logits = model(**inputs).logits\n-```\n-\n-æœ€ã‚‚é«˜ã„ç¢ºç‡ã§äºˆæ¸¬ã•ã‚ŒãŸãƒ©ãƒ™ãƒ«ã‚’å–å¾—ã—ã€ãƒ¢ãƒ‡ãƒ«ã® `id2label` ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ãƒ©ãƒ™ãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n->>> model.config.id2label[predicted_class_id]\n-'beignets'\n-```\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "36662317a9be514a546d0de2b2bef482f2d74bfe",
            "filename": "docs/source/ja/tasks/language_modeling.md",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Flanguage_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Flanguage_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Flanguage_modeling.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -198,16 +198,6 @@ Apply the `group_texts` function over the entire dataset:\n ```\n \n </pt>\n-<tf>\n-ã‚·ãƒ¼ã‚±ãƒ³ã‚¹çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ä½¿ç”¨ã—ã€`mlm=False` ã‚’è¨­å®šã—ã¾ã™ã€‚ã“ã‚Œã¯ã€å…¥åŠ›ã‚’ 1 è¦ç´ åˆ†å³ã«ã‚·ãƒ•ãƒˆã—ãŸãƒ©ãƒ™ãƒ«ã¨ã—ã¦ä½¿ç”¨ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import DataCollatorForLanguageModeling\n-\n->>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")\n-```\n-\n-</tf>\n </frameworkcontent>\n \n \n@@ -272,78 +262,6 @@ Perplexity: 49.61\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[åŸºæœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](../training#train-a-tensorflow-model-with-keras) ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-</Tip>\n-TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãŠã‚ˆã³ã„ãã¤ã‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-æ¬¡ã«ã€[`TFAutoModelForCausalLM`] ã‚’ä½¿ç”¨ã—ã¦ DistilGPT2 ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForCausalLM\n-\n->>> model = TFAutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚ Transformers ãƒ¢ãƒ‡ãƒ«ã«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯é–¢é€£ã®æå¤±é–¢æ•°ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®å ´åˆã‚’é™¤ãã€æå¤±é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ [`~transformers.PushToHubCallback`] ã§ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´æ‰€ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§å®Ÿè¡Œã§ãã¾ã™ã€‚\n-\n-\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_eli5_clm-model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ã‚’å‘¼ã³å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n-\n-\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€èª°ã§ã‚‚ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n-\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -406,32 +324,4 @@ TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°\n ```\n \n </pt>\n-<tf>\n-\n-ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€`input_ids`ã‚’ TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_eli5_clm-model\")\n->>> inputs = tokenizer(prompt, return_tensors=\"tf\").input_ids\n-```\n-\n-[`~transformers.generation_tf_utils.TFGenerationMixin.generate`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦è¦ç´„ã‚’ä½œæˆã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæˆ¦ç•¥ã¨ç”Ÿæˆã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[ãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæˆ¦ç•¥](../generation_strategies) ãƒšãƒ¼ã‚¸ã‚’å‚ç…§ã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForCausalLM\n-\n->>> model = TFAutoModelForCausalLM.from_pretrained(\"my_awesome_eli5_clm-model\")\n->>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n-```\n-\n-ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ ID ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«æˆ»ã—ã¾ã™ã€‚\n-\n-```py\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Somatic hypermutation allows the immune system to detect the presence of other viruses as they become more prevalent. Therefore, researchers have identified a high proportion of human viruses. The proportion of virus-associated viruses in our study increases with age. Therefore, we propose a simple algorithm to detect the presence of these new viruses in our samples as a sign of improved immunity. A first study based on this algorithm, which will be published in Science on Friday, aims to show that this finding could translate into the development of a better vaccine that is more effective for']\n-```\n-\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "90b39c69534924aa7dcdb5a5074e932336a9eb52",
            "filename": "docs/source/ja/tasks/masked_language_modeling.md",
            "status": "modified",
            "additions": 0,
            "deletions": 119,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fmasked_language_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fmasked_language_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fmasked_language_modeling.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -185,17 +185,6 @@ pip install transformers datasets evaluate\n >>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n ```\n </pt>\n-<tf>\n-\n-ã‚·ãƒ¼ã‚±ãƒ³ã‚¹çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚° ãƒˆãƒ¼ã‚¯ãƒ³ã¨ã—ã¦ä½¿ç”¨ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚’åå¾©ã™ã‚‹ãŸã³ã«ãƒ©ãƒ³ãƒ€ãƒ ã«ãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒã‚¹ã‚¯ã™ã‚‹ãŸã‚ã« `mlm_probability` ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> from transformers import DataCollatorForLanguageModeling\n-\n->>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Train\n@@ -261,78 +250,6 @@ Perplexity: 8.76\n ```\n \n </pt>\n-<tf>\n-<Tip>\n-\n-Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰](../training#train-a-tensorflow-model-with-keras) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-</Tip>\n-\n-TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãŠã‚ˆã³ã„ãã¤ã‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-æ¬¡ã«ã€[`TFAutoModelForMaskedLM`] ã‚’ä½¿ç”¨ã—ã¦ DistilRoBERTa ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForMaskedLM\n-\n->>> model = TFAutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚ Transformers ãƒ¢ãƒ‡ãƒ«ã«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯é–¢é€£ã®æå¤±é–¢æ•°ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®å ´åˆã‚’é™¤ãã€æå¤±é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n-\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-This can be done by specifying where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_eli5_mlm_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ã‚’å‘¼ã³å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n-\n-\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€èª°ã§ã‚‚ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n-\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -410,40 +327,4 @@ The Milky Way is a small galaxy.\n ```\n \n </pt>\n-<tf>\n-\n-ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€`input_ids`ã‚’ TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚ `<mask>` ãƒˆãƒ¼ã‚¯ãƒ³ã®ä½ç½®ã‚‚æŒ‡å®šã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_eli5_mlm_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\")\n->>> mask_token_index = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n-```\n-\n-å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã®`logits`ã‚’è¿”ã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> from transformers import TFAutoModelForMaskedLM\n-\n->>> model = TFAutoModelForMaskedLM.from_pretrained(\"stevhliu/my_awesome_eli5_mlm_model\")\n->>> logits = model(**inputs).logits\n->>> mask_token_logits = logits[0, mask_token_index, :]\n-```\n-\n-æ¬¡ã«ã€ãƒã‚¹ã‚¯ã•ã‚ŒãŸ 3 ã¤ã®ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æœ€ã‚‚é«˜ã„ç¢ºç‡ã§è¿”ã—ã€å‡ºåŠ›ã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()\n-\n->>> for token in top_3_tokens:\n-...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))\n-The Milky Way is a spiral galaxy.\n-The Milky Way is a massive galaxy.\n-The Milky Way is a small galaxy.\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "b0f623e29ab1adbc4e671246c68689ca652014fc",
            "filename": "docs/source/ja/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 0,
            "deletions": 116,
            "changes": 116,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fmultiple_choice.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -200,92 +200,6 @@ tokenized_swag = swag.map(preprocess_function, batched=True)\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰](../training#train-a-tensorflow-model-with-keras) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-</Tip>\n-TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãŠã‚ˆã³ã„ãã¤ã‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_train_epochs = 2\n->>> total_train_steps = (len(tokenized_swag[\"train\"]) // batch_size) * num_train_epochs\n->>> optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n-```\n-\n-æ¬¡ã«ã€[`TFAutoModelForMultipleChoice`] ã‚’ä½¿ç”¨ã—ã¦ BERT ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForMultipleChoice\n-\n->>> model = TFAutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n-\n-```py\n->>> data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_swag[\"train\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_swag[\"validation\"],\n-...     shuffle=False,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚ Transformers ãƒ¢ãƒ‡ãƒ«ã«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯é–¢é€£ã®æå¤±é–¢æ•°ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®å ´åˆã‚’é™¤ãã€æå¤±é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹å‰ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹æœ€å¾Œã® 2 ã¤ã®ã“ã¨ã¯ã€äºˆæ¸¬ã‹ã‚‰ç²¾åº¦ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã¨ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹æ–¹æ³•ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã™ã€‚ã©ã¡ã‚‰ã‚‚ [Keras ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯](../main_classes/keras_callbacks) ã‚’ä½¿ç”¨ã—ã¦è¡Œã‚ã‚Œã¾ã™ã€‚\n-\n-`compute_metrics` é–¢æ•°ã‚’ [`~transformers.KerasMetricCallback`] ã«æ¸¡ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-[`~transformers.PushToHubCallback`] ã§ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´æ‰€ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-æ¬¡ã«ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ã¾ã¨ã‚ã¦ãƒãƒ³ãƒ‰ãƒ«ã—ã¾ã™ã€‚\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ã‚’å‘¼ã³å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2, callbacks=callbacks)\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€èª°ã§ã‚‚ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n-\n-</tf>\n </frameworkcontent>\n \n \n@@ -341,34 +255,4 @@ TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°\n '0'\n ```\n </pt>\n-<tf>\n-\n-å„ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã¨å›ç­”å€™è£œã®ãƒšã‚¢ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_swag_model\")\n->>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"tf\", padding=True)\n-```\n-\n-å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits`ã‚’è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForMultipleChoice\n-\n->>> model = TFAutoModelForMultipleChoice.from_pretrained(\"my_awesome_swag_model\")\n->>> inputs = {k: tf.expand_dims(v, 0) for k, v in inputs.items()}\n->>> outputs = model(inputs)\n->>> logits = outputs.logits\n-```\n-\n-æœ€ã‚‚é«˜ã„ç¢ºç‡ã§ã‚¯ãƒ©ã‚¹ã‚’å–å¾—ã—ã¾ã™ã€‚\n-\n-```py\n->>> predicted_class = int(tf.math.argmax(logits, axis=-1)[0])\n->>> predicted_class\n-'0'\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "302a794c8c02f817f3e9cca2e43f0edeff573313",
            "filename": "docs/source/ja/tasks/question_answering.md",
            "status": "modified",
            "additions": 0,
            "deletions": 310,
            "changes": 310,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fquestion_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fquestion_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fquestion_answering.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -1,200 +1,3 @@\n-<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n-\n-Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n-the License. You may obtain a copy of the License at\n-\n-http://www.apache.org/licenses/LICENSE-2.0\n-\n-Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n-an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n-specific language governing permissions and limitations under the License.\n-\n-âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n-rendered properly in your Markdown viewer.\n-\n--->\n-\n-# Question answering\n-\n-[[open-in-colab]]\n-\n-<Youtube id=\"ajPx5LwJD-I\"/>\n-\n-è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã¯ã€è³ªå•ã«å¯¾ã—ã¦å›ç­”ã‚’è¿”ã—ã¾ã™ã€‚ Alexaã€Siriã€Google ãªã©ã®ä»®æƒ³ã‚¢ã‚·ã‚¹ã‚¿ãƒ³ãƒˆã«å¤©æ°—ã‚’å°‹ã­ãŸã“ã¨ãŒã‚ã‚‹ãªã‚‰ã€è³ªå•å¿œç­”ãƒ¢ãƒ‡ãƒ«ã‚’ä½¿ç”¨ã—ãŸã“ã¨ãŒã‚ã‚‹ã¯ãšã§ã™ã€‚è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã«ã¯ä¸€èˆ¬çš„ã« 2 ã¤ã®ã‚¿ã‚¤ãƒ—ãŒã‚ã‚Šã¾ã™ã€‚\n-\n-- æŠ½å‡º: ä¸ãˆã‚‰ã‚ŒãŸã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å›ç­”ã‚’æŠ½å‡ºã—ã¾ã™ã€‚\n-- æŠ½è±¡çš„: è³ªå•ã«æ­£ã—ãç­”ãˆã‚‹ã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰å›ç­”ã‚’ç”Ÿæˆã—ã¾ã™ã€‚\n-\n-ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€æ¬¡ã®æ–¹æ³•ã‚’èª¬æ˜ã—ã¾ã™ã€‚\n-\n-1. æŠ½å‡ºçš„è³ªå•å¿œç­”ç”¨ã« [SQuAD](https://huggingface.co/datasets/squad) ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä¸Šã® [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n-2. å¾®èª¿æ•´ã—ãŸãƒ¢ãƒ‡ãƒ«ã‚’æ¨è«–ã«ä½¿ç”¨ã—ã¾ã™ã€‚\n-\n-<Tip>\n-\n-ã“ã®ã‚¿ã‚¹ã‚¯ã¨äº’æ›æ€§ã®ã‚ã‚‹ã™ã¹ã¦ã®ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã¨ãƒã‚§ãƒƒã‚¯ãƒã‚¤ãƒ³ãƒˆã‚’ç¢ºèªã™ã‚‹ã«ã¯ã€[ã‚¿ã‚¹ã‚¯ãƒšãƒ¼ã‚¸](https://huggingface.co/tasks/question-answering) ã‚’ç¢ºèªã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚\n-\n-</Tip>\n-\n-å§‹ã‚ã‚‹å‰ã«ã€å¿…è¦ãªãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒã™ã¹ã¦ã‚¤ãƒ³ã‚¹ãƒˆãƒ¼ãƒ«ã•ã‚Œã¦ã„ã‚‹ã“ã¨ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n-\n-```bash\n-pip install transformers datasets evaluate\n-```\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒŸãƒ¥ãƒ‹ãƒ†ã‚£ã¨å…±æœ‰ã§ãã‚‹ã‚ˆã†ã«ã€Hugging Face ã‚¢ã‚«ã‚¦ãƒ³ãƒˆã«ãƒ­ã‚°ã‚¤ãƒ³ã™ã‚‹ã“ã¨ã‚’ãŠå‹§ã‚ã—ã¾ã™ã€‚ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆãŒè¡¨ç¤ºã•ã‚ŒãŸã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…¥åŠ›ã—ã¦ãƒ­ã‚°ã‚¤ãƒ³ã—ã¾ã™ã€‚\n-\n-```py\n->>> from huggingface_hub import notebook_login\n-\n->>> notebook_login()\n-```\n-\n-## Load SQuAD dataset\n-\n-ã¾ãšã€ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‹ã‚‰ SQuAD ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®å°ã•ã„ã‚µãƒ–ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€å®Œå…¨ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã•ã‚‰ã«æ™‚é–“ã‚’è²»ã‚„ã™å‰ã«ã€å®Ÿé¨“ã—ã¦ã™ã¹ã¦ãŒæ©Ÿèƒ½ã™ã‚‹ã“ã¨ã‚’ç¢ºèªã™ã‚‹æ©Ÿä¼šãŒå¾—ã‚‰ã‚Œã¾ã™ã€‚\n-\n-\n-```py\n->>> from datasets import load_dataset\n-\n->>> squad = load_dataset(\"squad\", split=\"train[:5000]\")\n-```\n-\n-[`~datasets.Dataset.train_test_split`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã® `train` åˆ†å‰²ã‚’ãƒˆãƒ¬ã‚¤ãƒ³ ã‚»ãƒƒãƒˆã¨ãƒ†ã‚¹ãƒˆ ã‚»ãƒƒãƒˆã«åˆ†å‰²ã—ã¾ã™ã€‚\n-\n-```py\n->>> squad = squad.train_test_split(test_size=0.2)\n-```\n-\n-æ¬¡ã«ã€ä¾‹ã‚’è¦‹ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n-\n-```py\n->>> squad[\"train\"][0]\n-{'answers': {'answer_start': [515], 'text': ['Saint Bernadette Soubirous']},\n- 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.',\n- 'id': '5733be284776f41900661182',\n- 'question': 'To whom did the Virgin Mary allegedly appear in 1858 in Lourdes France?',\n- 'title': 'University_of_Notre_Dame'\n-}\n-```\n-\n-ã“ã“ã«ã¯ã„ãã¤ã‹ã®é‡è¦ãªãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ãŒã‚ã‚Šã¾ã™ã€‚\n-\n-- `answers`: å›ç­”ãƒˆãƒ¼ã‚¯ãƒ³ã¨å›ç­”ãƒ†ã‚­ã‚¹ãƒˆã®é–‹å§‹ä½ç½®ã€‚\n-- `context`: ãƒ¢ãƒ‡ãƒ«ãŒç­”ãˆã‚’æŠ½å‡ºã™ã‚‹ãŸã‚ã«å¿…è¦ãªèƒŒæ™¯æƒ…å ±ã€‚\n-- `question`: ãƒ¢ãƒ‡ãƒ«ãŒç­”ãˆã‚‹å¿…è¦ãŒã‚ã‚‹è³ªå•ã€‚\n-\n-## Preprocess\n-\n-<Youtube id=\"qgaM0weJHpA\"/>\n-\n-æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—ã§ã¯ã€DistilBERT ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦`question`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã¨`context`ãƒ•ã‚£ãƒ¼ãƒ«ãƒ‰ã‚’å‡¦ç†ã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-è³ªå•å¿œç­”ã‚¿ã‚¹ã‚¯ã«ç‰¹æœ‰ã®ã€æ³¨æ„ã™ã¹ãå‰å‡¦ç†æ‰‹é †ãŒã„ãã¤ã‹ã‚ã‚Šã¾ã™ã€‚\n-\n-1. ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã®ä¸€éƒ¨ã®ä¾‹ã«ã¯ã€ãƒ¢ãƒ‡ãƒ«ã®æœ€å¤§å…¥åŠ›é•·ã‚’è¶…ãˆã‚‹éå¸¸ã«é•·ã„ã€Œã‚³ãƒ³ãƒ†ã‚­ã‚¹ãƒˆã€ãŒå«ã¾ã‚Œã‚‹å ´åˆãŒã‚ã‚Šã¾ã™ã€‚ã‚ˆã‚Šé•·ã„ã‚·ãƒ¼ã‚±ãƒ³ã‚¹ã‚’å‡¦ç†ã™ã‚‹ã«ã¯ã€`truncation=\"only_second\"` ã‚’è¨­å®šã—ã¦ `context` ã®ã¿ã‚’åˆ‡ã‚Šæ¨ã¦ã¾ã™ã€‚\n-2. æ¬¡ã«ã€è¨­å®šã«ã‚ˆã£ã¦ã€å›ç­”ã®é–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã‚’å…ƒã® `context`ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã—ã¾ã™ã€‚\n-   ã€Œ`return_offset_mapping=True`ã€ã€‚\n-3. ãƒãƒƒãƒ”ãƒ³ã‚°ãŒæ‰‹å…ƒã«ã‚ã‚‹ã®ã§ã€ç­”ãˆã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¦‹ã¤ã‘ã‚‹ã“ã¨ãŒã§ãã¾ã™ã€‚ [`~tokenizers.Encoding.sequence_ids`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ã€\n-   ã‚ªãƒ•ã‚»ãƒƒãƒˆã®ã©ã®éƒ¨åˆ†ãŒ`question`ã«å¯¾å¿œã—ã€ã©ã®éƒ¨åˆ†ãŒ`context`ã«å¯¾å¿œã™ã‚‹ã‹ã‚’è¦‹ã¤ã‘ã¾ã™ã€‚\n-\n-ä»¥ä¸‹ã«ã€`answer`ã®é–‹å§‹ãƒˆãƒ¼ã‚¯ãƒ³ã¨çµ‚äº†ãƒˆãƒ¼ã‚¯ãƒ³ã‚’åˆ‡ã‚Šè©°ã‚ã¦`context`ã«ãƒãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹é–¢æ•°ã‚’ä½œæˆã™ã‚‹æ–¹æ³•ã‚’ç¤ºã—ã¾ã™ã€‚\n-\n-```py\n->>> def preprocess_function(examples):\n-...     questions = [q.strip() for q in examples[\"question\"]]\n-...     inputs = tokenizer(\n-...         questions,\n-...         examples[\"context\"],\n-...         max_length=384,\n-...         truncation=\"only_second\",\n-...         return_offsets_mapping=True,\n-...         padding=\"max_length\",\n-...     )\n-\n-...     offset_mapping = inputs.pop(\"offset_mapping\")\n-...     answers = examples[\"answers\"]\n-...     start_positions = []\n-...     end_positions = []\n-\n-...     for i, offset in enumerate(offset_mapping):\n-...         answer = answers[i]\n-...         start_char = answer[\"answer_start\"][0]\n-...         end_char = answer[\"answer_start\"][0] + len(answer[\"text\"][0])\n-...         sequence_ids = inputs.sequence_ids(i)\n-\n-...         # Find the start and end of the context\n-...         idx = 0\n-...         while sequence_ids[idx] != 1:\n-...             idx += 1\n-...         context_start = idx\n-...         while sequence_ids[idx] == 1:\n-...             idx += 1\n-...         context_end = idx - 1\n-\n-...         # If the answer is not fully inside the context, label it (0, 0)\n-...         if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n-...             start_positions.append(0)\n-...             end_positions.append(0)\n-...         else:\n-...             # Otherwise it's the start and end token positions\n-...             idx = context_start\n-...             while idx <= context_end and offset[idx][0] <= start_char:\n-...                 idx += 1\n-...             start_positions.append(idx - 1)\n-\n-...             idx = context_end\n-...             while idx >= context_start and offset[idx][1] >= end_char:\n-...                 idx -= 1\n-...             end_positions.append(idx + 1)\n-\n-...     inputs[\"start_positions\"] = start_positions\n-...     inputs[\"end_positions\"] = end_positions\n-...     return inputs\n-```\n-\n-ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«å‰å‡¦ç†é–¢æ•°ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ğŸ¤— Datasets [`~datasets.Dataset.map`] é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚ `batched=True` ã‚’è¨­å®šã—ã¦ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®è¤‡æ•°ã®è¦ç´ ã‚’ä¸€åº¦ã«å‡¦ç†ã™ã‚‹ã“ã¨ã§ã€`map` é–¢æ•°ã‚’é«˜é€ŸåŒ–ã§ãã¾ã™ã€‚ä¸è¦ãªåˆ—ã‚’å‰Šé™¤ã—ã¾ã™ã€‚\n-\n-```py\n->>> tokenized_squad = squad.map(preprocess_function, batched=True, remove_columns=squad[\"train\"].column_names)\n-```\n-\n-æ¬¡ã«ã€[`DefaultDataCollatâ€‹â€‹or`] ã‚’ä½¿ç”¨ã—ã¦ã‚µãƒ³ãƒ—ãƒ«ã®ãƒãƒƒãƒã‚’ä½œæˆã—ã¾ã™ã€‚ ğŸ¤— Transformers ã®ä»–ã®ãƒ‡ãƒ¼ã‚¿ç…§åˆå™¨ã¨ã¯ç•°ãªã‚Šã€[`DefaultDataCollatâ€‹â€‹or`] ã¯ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãªã©ã®è¿½åŠ ã®å‰å‡¦ç†ã‚’é©ç”¨ã—ã¾ã›ã‚“ã€‚\n-\n-<frameworkcontent>\n-<pt>\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator()\n-```\n-</pt>\n-<tf>\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-```\n-</tf>\n-</frameworkcontent>\n-\n-## Train\n-\n-<frameworkcontent>\n-<pt>\n-<Tip>\n-\n-[`Trainer`] ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã“](../training#train-with-pytorch-trainer) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-</Tip>\n \n ã“ã‚Œã§ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ [`AutoModelForQuestionAnswering`] ã‚’ä½¿ç”¨ã—ã¦ DitilBERT ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n \n@@ -241,82 +44,6 @@ pip install transformers datasets evaluate\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰](../training#train-a-tensorflow-model-with-keras) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-</Tip>\n-\n-</ãƒ’ãƒ³ãƒˆ>\n-TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãŠã‚ˆã³ã„ãã¤ã‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_epochs = 2\n->>> total_train_steps = (len(tokenized_squad[\"train\"]) // batch_size) * num_epochs\n->>> optimizer, schedule = create_optimizer(\n-...     init_lr=2e-5,\n-...     num_warmup_steps=0,\n-...     num_train_steps=total_train_steps,\n-... )\n-```\n-\n-æ¬¡ã«ã€[`TFAutoModelForQuestionAnswering`] ã‚’ä½¿ç”¨ã—ã¦ DistilBERT ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForQuestionAnswering\n-\n->>> model = TFAutoModelForQuestionAnswering(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_squad[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_squad[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹å‰ã«æœ€å¾Œã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã¯ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹æ–¹æ³•ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã¯ã€ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ [`~transformers.PushToHubCallback`] ã§ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´æ‰€ã‚’æŒ‡å®šã™ã‚‹ã“ã¨ã§å®Ÿè¡Œã§ãã¾ã™ã€‚\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_qa_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ã‚’å‘¼ã³å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=[callback])\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€èª°ã§ã‚‚ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -398,41 +125,4 @@ TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°\n '176 billion parameters and can generate text in 46 languages natural languages and 13'\n ```\n </pt>\n-<tf>\n-\n-ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n->>> inputs = tokenizer(question, text, return_tensors=\"tf\")\n-```\n-\n-å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits`ã‚’è¿”ã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> from transformers import TFAutoModelForQuestionAnswering\n-\n->>> model = TFAutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\")\n->>> outputs = model(**inputs)\n-```\n-\n-ãƒ¢ãƒ‡ãƒ«å‡ºåŠ›ã‹ã‚‰é–‹å§‹ä½ç½®ã¨çµ‚äº†ä½ç½®ã®æœ€ã‚‚é«˜ã„ç¢ºç‡ã‚’å–å¾—ã—ã¾ã™ã€‚\n-\n-```py\n->>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n->>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n-```\n-\n-äºˆæ¸¬ã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ç­”ãˆã‚’å–å¾—ã—ã¾ã™ã€‚\n-\n-```py\n->>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n->>> tokenizer.decode(predict_answer_tokens)\n-'176 billion parameters and can generate text in 46 languages natural languages and 13'\n-```\n-\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "95953e841f538ccd5ef014d9a1ade433d761a9ef",
            "filename": "docs/source/ja/tasks/semantic_segmentation.md",
            "status": "modified",
            "additions": 0,
            "deletions": 234,
            "changes": 234,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fsemantic_segmentation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fsemantic_segmentation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fsemantic_segmentation.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -143,65 +143,6 @@ pip install -q datasets transformers evaluate\n </pt>\n </frameworkcontent>\n \n-<frameworkcontent>\n-<tf>\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’éå­¦ç¿’ã«å¯¾ã—ã¦ã‚ˆã‚Šå …ç‰¢ã«ã™ã‚‹ãŸã‚ã«ã€ç”»åƒãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã„ãã¤ã‹ã®ãƒ‡ãƒ¼ã‚¿æ‹¡å¼µã‚’é©ç”¨ã™ã‚‹ã®ãŒä¸€èˆ¬çš„ã§ã™ã€‚\n-ã“ã®ã‚¬ã‚¤ãƒ‰ã§ã¯ã€[`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image) ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã®è‰²ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«å¤‰æ›´ã—ã¾ã™ãŒã€ä»»æ„ã®ãƒ—ãƒ­ãƒ‘ãƒ†ã‚£ã‚’ä½¿ç”¨ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ã€‚ç”»åƒ\n-å¥½ããªå›³æ›¸é¤¨ã€‚\n-2 ã¤ã®åˆ¥ã€…ã®å¤‰æ›é–¢æ•°ã‚’å®šç¾©ã—ã¾ã™ã€‚\n-- ç”»åƒæ‹¡å¼µã‚’å«ã‚€ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒ‡ãƒ¼ã‚¿å¤‰æ›\n-- ğŸ¤— Transformers ã®ã‚³ãƒ³ãƒ”ãƒ¥ãƒ¼ã‚¿ãƒ¼ ãƒ“ã‚¸ãƒ§ãƒ³ ãƒ¢ãƒ‡ãƒ«ã¯ãƒãƒ£ãƒãƒ«å„ªå…ˆã®ãƒ¬ã‚¤ã‚¢ã‚¦ãƒˆã‚’æƒ³å®šã—ã¦ã„ã‚‹ãŸã‚ã€ç”»åƒã‚’è»¢ç½®ã™ã‚‹ã ã‘ã®æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿å¤‰æ›\n-\n-```py\n->>> import tensorflow as tf\n-\n-\n->>> def aug_transforms(image):\n-...     image = tf.keras.utils.img_to_array(image)\n-...     image = tf.image.random_brightness(image, 0.25)\n-...     image = tf.image.random_contrast(image, 0.5, 2.0)\n-...     image = tf.image.random_saturation(image, 0.75, 1.25)\n-...     image = tf.image.random_hue(image, 0.1)\n-...     image = tf.transpose(image, (2, 0, 1))\n-...     return image\n-\n-\n->>> def transforms(image):\n-...     image = tf.keras.utils.img_to_array(image)\n-...     image = tf.transpose(image, (2, 0, 1))\n-...     return image\n-```\n-\n-æ¬¡ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ç”»åƒã¨æ³¨é‡ˆã®ãƒãƒƒãƒã‚’æº–å‚™ã™ã‚‹ 2 ã¤ã®å‰å‡¦ç†é–¢æ•°ã‚’ä½œæˆã—ã¾ã™ã€‚ã“ã‚Œã‚‰ã®æ©Ÿèƒ½ãŒé©ç”¨ã•ã‚Œã¾ã™\n-ç”»åƒå¤‰æ›ã‚’è¡Œã„ã€ä»¥å‰ã«ãƒ­ãƒ¼ãƒ‰ã•ã‚ŒãŸ `image_processor` ã‚’ä½¿ç”¨ã—ã¦ç”»åƒã‚’ `pixel_values` ã«å¤‰æ›ã—ã€\n-`labels`ã¸ã®æ³¨é‡ˆã€‚ `ImageProcessor` ã¯ã€ç”»åƒã®ã‚µã‚¤ã‚ºå¤‰æ›´ã¨æ­£è¦åŒ–ã‚‚å‡¦ç†ã—ã¾ã™ã€‚\n-\n-```py\n->>> def train_transforms(example_batch):\n-...     images = [aug_transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n-...     labels = [x for x in example_batch[\"annotation\"]]\n-...     inputs = image_processor(images, labels)\n-...     return inputs\n-\n-\n->>> def val_transforms(example_batch):\n-...     images = [transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n-...     labels = [x for x in example_batch[\"annotation\"]]\n-...     inputs = image_processor(images, labels)\n-...     return inputs\n-```\n-\n-ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«å‰å‡¦ç†å¤‰æ›ã‚’é©ç”¨ã™ã‚‹ã«ã¯ã€ğŸ¤— Datasets [`~datasets.Dataset.set_transform`] é–¢æ•°ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n-å¤‰æ›ã¯ã‚ªãƒ³ã‚¶ãƒ•ãƒ©ã‚¤ã§é©ç”¨ã•ã‚Œã‚‹ãŸã‚ã€é«˜é€Ÿã§æ¶ˆè²»ã™ã‚‹ãƒ‡ã‚£ã‚¹ã‚¯å®¹é‡ãŒå°‘ãªããªã‚Šã¾ã™ã€‚\n-\n-```py\n->>> train_ds.set_transform(train_transforms)\n->>> test_ds.set_transform(val_transforms)\n-```\n-</tf>\n-</frameworkcontent>\n-\n ## Evaluate\n \n ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ä¸­ã«ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’å«ã‚ã‚‹ã¨ã€å¤šãã®å ´åˆã€ãƒ¢ãƒ‡ãƒ«ã®ãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹ã‚’è©•ä¾¡ã™ã‚‹ã®ã«å½¹ç«‹ã¡ã¾ã™ã€‚ ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ãƒ©ã‚¤ãƒ–ãƒ©ãƒªã‚’ä½¿ç”¨ã—ã¦ã€è©•ä¾¡ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ã™ã°ã‚„ããƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚ã“ã®ã‚¿ã‚¹ã‚¯ã§ã¯ã€[Mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) ãƒ¡ãƒˆãƒªãƒƒã‚¯ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ (ğŸ¤— Evaluate [ã‚¯ã‚¤ãƒƒã‚¯ ãƒ„ã‚¢ãƒ¼](https://huggingface.co/docs/evaluate/a_quick_tour) ã‚’å‚ç…§ã—ã¦ã€ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦è¨ˆç®—ã™ã‚‹æ–¹æ³•ã®è©³ç´°ã‚’ç¢ºèªã—ã¦ãã ã•ã„)ã€‚\n@@ -252,39 +193,6 @@ pip install -q datasets transformers evaluate\n </frameworkcontent>\n \n \n-<frameworkcontent>\n-<tf>\n-\n-```py\n->>> def compute_metrics(eval_pred):\n-...     logits, labels = eval_pred\n-...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])\n-...     logits_resized = tf.image.resize(\n-...         logits,\n-...         size=tf.shape(labels)[1:],\n-...         method=\"bilinear\",\n-...     )\n-\n-...     pred_labels = tf.argmax(logits_resized, axis=-1)\n-...     metrics = metric.compute(\n-...         predictions=pred_labels,\n-...         references=labels,\n-...         num_labels=num_labels,\n-...         ignore_index=-1,\n-...         reduce_labels=image_processor.do_reduce_labels,\n-...     )\n-\n-...     per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n-...     per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n-\n-...     metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n-...     metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n-...     return {\"val_\" + k: v for k, v in metrics.items()}\n-```\n-\n-</tf>\n-</frameworkcontent>\n-\n ã“ã‚Œã§`compute_metrics`é–¢æ•°ã®æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã¨ãã«ã“ã®é–¢æ•°ã«æˆ»ã‚Šã¾ã™ã€‚\n \n ## Train\n@@ -347,110 +255,6 @@ pip install -q datasets transformers evaluate\n </pt>\n </frameworkcontent>\n \n-<frameworkcontent>\n-<tf>\n-<Tip>\n-\n-Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€ã¾ãš [åŸºæœ¬ãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«](./training#train-a-tensorflow-model-with-keras) ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n-\n-</Tip>\n-\n-TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€æ¬¡ã®æ‰‹é †ã«å¾“ã„ã¾ã™ã€‚\n-1. ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã®ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’å®šç¾©ã—ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã¨å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚\n-2. äº‹å‰ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã•ã‚ŒãŸãƒ¢ãƒ‡ãƒ«ã‚’ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹åŒ–ã—ã¾ã™ã€‚\n-3. ğŸ¤— ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` ã«å¤‰æ›ã—ã¾ã™ã€‚\n-4. ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚\n-5. ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’è¿½åŠ ã—ã¦ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— Hub ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™\n-6. `fit()` ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n-\n-ã¾ãšã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã‚’å®šç¾©ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 2\n->>> num_epochs = 50\n->>> num_train_steps = len(train_ds) * num_epochs\n->>> learning_rate = 6e-5\n->>> weight_decay_rate = 0.01\n-\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=learning_rate,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=weight_decay_rate,\n-...     num_warmup_steps=0,\n-... )\n-```\n-\n-æ¬¡ã«ã€ãƒ©ãƒ™ãƒ« ãƒãƒƒãƒ”ãƒ³ã‚°ã¨ã¨ã‚‚ã« [`TFAutoModelForSemanticSegmentation`] ã‚’ä½¿ç”¨ã—ã¦ SegFormer ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãã‚Œã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã¾ã™ã€‚\n-ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ã€‚ Transformers ãƒ¢ãƒ‡ãƒ«ã«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯é–¢é€£ã®æå¤±é–¢æ•°ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®å ´åˆã‚’é™¤ãã€æå¤±é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForSemanticSegmentation\n-\n->>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\n-...     checkpoint,\n-...     id2label=id2label,\n-...     label2id=label2id,\n-... )\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-[`~datasets.Dataset.to_tf_dataset`] ã¨ [`DefaultDataCollatâ€‹â€‹or`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-\n->>> tf_train_dataset = train_ds.to_tf_dataset(\n-...     columns=[\"pixel_values\", \"label\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_eval_dataset = test_ds.to_tf_dataset(\n-...     columns=[\"pixel_values\", \"label\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-äºˆæ¸¬ã‹ã‚‰ç²¾åº¦ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ ğŸ¤— ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹ã«ã¯ã€[Keras callbacks](../main_classes/keras_callbacks) ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n-`compute_metrics` é–¢æ•°ã‚’ [`KerasMetricCallback`] ã«æ¸¡ã—ã¾ã™ã€‚\n-ãã—ã¦ [`PushToHubCallback`] ã‚’ä½¿ç”¨ã—ã¦ãƒ¢ãƒ‡ãƒ«ã‚’ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n-\n->>> metric_callback = KerasMetricCallback(\n-...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=[\"labels\"]\n-... )\n-\n->>> push_to_hub_callback = PushToHubCallback(output_dir=\"scene_segmentation\", tokenizer=image_processor)\n-\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€\n-ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ãŸã‚ã®ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯:\n-\n-\n-```py\n->>> model.fit(\n-...     tf_train_dataset,\n-...     validation_data=tf_eval_dataset,\n-...     callbacks=callbacks,\n-...     epochs=num_epochs,\n-... )\n-```\n-\n-ãŠã‚ã§ã¨ã†ï¼ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã€ğŸ¤— Hub ã§å…±æœ‰ã—ã¾ã—ãŸã€‚ã“ã‚Œã§æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n-</tf>\n-</frameworkcontent>\n-\n ## Inference\n \n ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ãŸã®ã§ã€ãã‚Œã‚’æ¨è«–ã«ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã—ãŸã€‚\n@@ -537,44 +341,6 @@ TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€æ¬¡ã®æ‰‹é †ã«å¾“ã„ã¾ã™ã€‚\n </pt>\n </frameworkcontent>\n \n-<frameworkcontent>\n-<tf>\n-\n-ç”»åƒãƒ—ãƒ­ã‚»ãƒƒã‚µã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ç”»åƒã‚’å‰å‡¦ç†ã—ã€å…¥åŠ›ã‚’ TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import AutoImageProcessor\n-\n->>> image_processor = AutoImageProcessor.from_pretrained(\"MariaK/scene_segmentation\")\n->>> inputs = image_processor(image, return_tensors=\"tf\")\n-```\n-\n-å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits`ã‚’è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForSemanticSegmentation\n-\n->>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\"MariaK/scene_segmentation\")\n->>> logits = model(**inputs).logits\n-```\n-\n-æ¬¡ã«ã€ãƒ­ã‚¸ãƒƒãƒˆã‚’å…ƒã®ç”»åƒã‚µã‚¤ã‚ºã«å†ã‚¹ã‚±ãƒ¼ãƒ«ã—ã€ã‚¯ãƒ©ã‚¹æ¬¡å…ƒã« argmax ã‚’é©ç”¨ã—ã¾ã™ã€‚\n-\n-```py\n->>> logits = tf.transpose(logits, [0, 2, 3, 1])\n-\n->>> upsampled_logits = tf.image.resize(\n-...     logits,\n-...     # We reverse the shape of `image` because `image.size` returns width and height.\n-...     image.size[::-1],\n-... )\n-\n->>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]\n-```\n-\n-</tf>\n-</frameworkcontent>\n-\n çµæœã‚’è¦–è¦šåŒ–ã™ã‚‹ã«ã¯ã€[ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆ ã‚«ãƒ©ãƒ¼ ãƒ‘ãƒ¬ãƒƒãƒˆ](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51) ã‚’ã€ãã‚Œãã‚Œã‚’ãƒãƒƒãƒ—ã™ã‚‹ `ade_palette()` ã¨ã—ã¦ãƒ­ãƒ¼ãƒ‰ã—ã¾ã™ã€‚ã‚¯ãƒ©ã‚¹ã‚’ RGB å€¤ã«å¤‰æ›ã—ã¾ã™ã€‚æ¬¡ã«ã€ç”»åƒã¨äºˆæ¸¬ã•ã‚ŒãŸã‚»ã‚°ãƒ¡ãƒ³ãƒ†ãƒ¼ã‚·ãƒ§ãƒ³ ãƒãƒƒãƒ—ã‚’çµ„ã¿åˆã‚ã›ã¦ãƒ—ãƒ­ãƒƒãƒˆã§ãã¾ã™ã€‚\n \n ```py"
        },
        {
            "sha": "182221610ba2f468deaf4ea279509f24c2ba9564",
            "filename": "docs/source/ja/tasks/summarization.md",
            "status": "modified",
            "additions": 0,
            "deletions": 120,
            "changes": 120,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fsummarization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Fsummarization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Fsummarization.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -128,14 +128,6 @@ pip install transformers datasets evaluate rouge_score\n >>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import DataCollatorForSeq2Seq\n-\n->>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Evaluate\n@@ -230,91 +222,6 @@ pip install transformers datasets evaluate rouge_score\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰](../training#train-a-tensorflow-model-with-keras) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-</Tip>\n-TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãŠã‚ˆã³ã„ãã¤ã‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-æ¬¡ã«ã€[`TFAutoModelForSeq2SeqLM`] ã‚’ä½¿ç”¨ã—ã¦ T5 ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n-\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_billsum[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     tokenized_billsum[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚ Transformers ãƒ¢ãƒ‡ãƒ«ã«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯é–¢é€£ã®æå¤±é–¢æ•°ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®å ´åˆã‚’é™¤ãã€æå¤±é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹å‰ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹æœ€å¾Œã® 2 ã¤ã®ã“ã¨ã¯ã€äºˆæ¸¬ã‹ã‚‰ ROUGE ã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹æ–¹æ³•ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã™ã€‚ã©ã¡ã‚‰ã‚‚ [Keras ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯](../main_classes/keras_callbacks) ã‚’ä½¿ç”¨ã—ã¦è¡Œã‚ã‚Œã¾ã™ã€‚\n-\n-`compute_metrics` é–¢æ•°ã‚’ [`~transformers.KerasMetricCallback`] ã«æ¸¡ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-Specify where to push your model and tokenizer in the [`~transformers.PushToHubCallback`]:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_billsum_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-æ¬¡ã«ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ã¾ã¨ã‚ã¦ãƒãƒ³ãƒ‰ãƒ«ã—ã¾ã™ã€‚\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ã‚’å‘¼ã³å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€èª°ã§ã‚‚ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n-\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -374,31 +281,4 @@ Tokenize the text and return the `input_ids` as PyTorch tensors:\n 'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'\n ```\n </pt>\n-<tf>\n-\n-ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€`input_ids`ã‚’ TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n-```\n-\n-[`~transformers.generation_tf_utils.TFGenerationMixin.generate`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦è¦ç´„ã‚’ä½œæˆã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæˆ¦ç•¥ã¨ç”Ÿæˆã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[Text Generation](../main_classes/text_generation) API ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n->>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n-```\n-\n-ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ ID ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«æˆ»ã—ã¾ã™ã€‚\n-\n-```py\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "9642a425ff1725e0776b4562f325a49b2b84f4d2",
            "filename": "docs/source/ja/tasks/token_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 148,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Ftoken_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Ftoken_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Ftoken_classification.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -164,14 +164,6 @@ pip install transformers datasets evaluate seqeval\n >>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import DataCollatorForTokenClassification\n-\n->>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Evaluate\n@@ -309,100 +301,6 @@ pip install transformers datasets evaluate seqeval\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰](../training#train-a-tensorflow-model-with-keras) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-</Tip>\n-TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãŠã‚ˆã³ã„ãã¤ã‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_train_epochs = 3\n->>> num_train_steps = (len(tokenized_wnut[\"train\"]) // batch_size) * num_train_epochs\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=2e-5,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=0.01,\n-...     num_warmup_steps=0,\n-... )\n-```\n-æ¬¡ã«ã€[`TFAutoModelForTokenClassification`] ã‚’ä½¿ç”¨ã—ã¦ã€äºˆæœŸã•ã‚Œã‚‹ãƒ©ãƒ™ãƒ«ã®æ•°ã¨ãƒ©ãƒ™ãƒ« ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’æŒ‡å®šã—ã¦ DistilBERT ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\n-...     \"distilbert/distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n-... )\n-```\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_wnut[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_wnut[\"validation\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚ Transformers ãƒ¢ãƒ‡ãƒ«ã«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯é–¢é€£ã®æå¤±é–¢æ•°ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®å ´åˆã‚’é™¤ãã€æå¤±é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹å‰ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹æœ€å¾Œã® 2 ã¤ã®ã“ã¨ã¯ã€äºˆæ¸¬ã‹ã‚‰é€£ç¶šã‚¹ã‚³ã‚¢ã‚’è¨ˆç®—ã™ã‚‹ã“ã¨ã¨ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹æ–¹æ³•ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã™ã€‚ã©ã¡ã‚‰ã‚‚ [Keras ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯](../main_classes/keras_callbacks) ã‚’ä½¿ç”¨ã—ã¦è¡Œã‚ã‚Œã¾ã™ã€‚\n-\n-`compute_metrics` é–¢æ•°ã‚’ [`~transformers.KerasMetricCallback`] ã«æ¸¡ã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-[`~transformers.PushToHubCallback`] ã§ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´æ‰€ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_wnut_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-æ¬¡ã«ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ã¾ã¨ã‚ã¦ãƒãƒ³ãƒ‰ãƒ«ã—ã¾ã™ã€‚\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ã‚’å‘¼ã³å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€èª°ã§ã‚‚ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n-\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -512,50 +410,4 @@ TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°\n ```\n \n </pt>\n-<tf>\n-\n-ãƒ†ã‚­ã‚¹ãƒˆã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã€TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã‚’è¿”ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\")\n-```\n-\n-å…¥åŠ›ã‚’ãƒ¢ãƒ‡ãƒ«ã«æ¸¡ã—ã€`logits`ã‚’è¿”ã—ã¾ã™ã€‚\n-\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n->>> logits = model(**inputs).logits\n-```\n-\n-æœ€ã‚‚é«˜ã„ç¢ºç‡ã§ã‚¯ãƒ©ã‚¹ã‚’å–å¾—ã—ã€ãƒ¢ãƒ‡ãƒ«ã® `id2label` ãƒãƒƒãƒ”ãƒ³ã‚°ã‚’ä½¿ç”¨ã—ã¦ãã‚Œã‚’ãƒ†ã‚­ã‚¹ãƒˆ ãƒ©ãƒ™ãƒ«ã«å¤‰æ›ã—ã¾ã™ã€‚\n-\n-```py\n->>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)\n->>> predicted_token_class = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]\n->>> predicted_token_class\n-['O',\n- 'O',\n- 'B-location',\n- 'I-location',\n- 'B-group',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'B-location',\n- 'B-location',\n- 'O',\n- 'O']\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "82df32e082a9634c983e2a3972f31f87392204d4",
            "filename": "docs/source/ja/tasks/translation.md",
            "status": "modified",
            "additions": 0,
            "deletions": 119,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Ftranslation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftasks%2Ftranslation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftasks%2Ftranslation.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -122,14 +122,6 @@ pip install transformers datasets evaluate sacrebleu\n >>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import DataCollatorForSeq2Seq\n-\n->>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Evaluate\n@@ -234,90 +226,6 @@ pip install transformers datasets evaluate sacrebleu\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Keras ã‚’ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã®å¾®èª¿æ•´ã«æ…£ã‚Œã¦ã„ãªã„å ´åˆã¯ã€[ã“ã¡ã‚‰](../training#train-a-tensorflow-model-with-keras) ã®åŸºæœ¬çš„ãªãƒãƒ¥ãƒ¼ãƒˆãƒªã‚¢ãƒ«ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-</Tip>\n-TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°ã€å­¦ç¿’ç‡ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«ã€ãŠã‚ˆã³ã„ãã¤ã‹ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚° ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã‚’ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹ã“ã¨ã‹ã‚‰å§‹ã‚ã¾ã™ã€‚\n-\n-\n-```py\n->>> from transformers import AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-æ¬¡ã«ã€[`TFAutoModelForSeq2SeqLM`] ã‚’ä½¿ç”¨ã—ã¦ T5 ã‚’ãƒ­ãƒ¼ãƒ‰ã§ãã¾ã™ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`] ã‚’ä½¿ç”¨ã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ `tf.data.Dataset` å½¢å¼ã«å¤‰æ›ã—ã¾ã™ã€‚\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_books[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     tokenized_books[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ç”¨ã®ãƒ¢ãƒ‡ãƒ«ã‚’è¨­å®šã—ã¾ã™ã€‚ Transformers ãƒ¢ãƒ‡ãƒ«ã«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ã‚¿ã‚¹ã‚¯é–¢é€£ã®æå¤±é–¢æ•°ãŒã‚ã‚‹ãŸã‚ã€æ¬¡ã®å ´åˆã‚’é™¤ãã€æå¤±é–¢æ•°ã‚’æŒ‡å®šã™ã‚‹å¿…è¦ã¯ãªã„ã“ã¨ã«æ³¨æ„ã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # No loss argument!\n-```\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹å‰ã«ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—ã™ã‚‹æœ€å¾Œã® 2 ã¤ã®ã“ã¨ã¯ã€äºˆæ¸¬ã‹ã‚‰ SacreBLEU ãƒ¡ãƒˆãƒªã‚¯ã‚¹ã‚’è¨ˆç®—ã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒãƒ–ã«ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹æ–¹æ³•ã‚’æä¾›ã™ã‚‹ã“ã¨ã§ã™ã€‚ã©ã¡ã‚‰ã‚‚ [Keras ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯](../main_classes/keras_callbacks) ã‚’ä½¿ç”¨ã—ã¦è¡Œã‚ã‚Œã¾ã™ã€‚\n-\n-`compute_metrics` é–¢æ•°ã‚’ [`~transformers.KerasMetricCallback`] ã«æ¸¡ã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-[`~transformers.PushToHubCallback`] ã§ãƒ¢ãƒ‡ãƒ«ã¨ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã‚’ãƒ—ãƒƒã‚·ãƒ¥ã™ã‚‹å ´æ‰€ã‚’æŒ‡å®šã—ã¾ã™ã€‚\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_opus_books_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-æ¬¡ã«ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’ã¾ã¨ã‚ã¦ãƒãƒ³ãƒ‰ãƒ«ã—ã¾ã™ã€‚\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ã¤ã„ã«ã€ãƒ¢ãƒ‡ãƒ«ã®ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é–‹å§‹ã™ã‚‹æº–å‚™ãŒæ•´ã„ã¾ã—ãŸã€‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŠã‚ˆã³æ¤œè¨¼ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã€ã‚¨ãƒãƒƒã‚¯æ•°ã€ã‚³ãƒ¼ãƒ«ãƒãƒƒã‚¯ã‚’æŒ‡å®šã—ã¦ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ã‚’å‘¼ã³å‡ºã—ã€ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã—ã¾ã™ã€‚\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n-```\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ãŒå®Œäº†ã™ã‚‹ã¨ã€ãƒ¢ãƒ‡ãƒ«ã¯è‡ªå‹•çš„ã«ãƒãƒ–ã«ã‚¢ãƒƒãƒ—ãƒ­ãƒ¼ãƒ‰ã•ã‚Œã€èª°ã§ã‚‚ä½¿ç”¨ã§ãã‚‹ã‚ˆã†ã«ãªã‚Šã¾ã™ã€‚\n-\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -385,31 +293,4 @@ TensorFlow ã§ãƒ¢ãƒ‡ãƒ«ã‚’å¾®èª¿æ•´ã™ã‚‹ã«ã¯ã€ã‚ªãƒ—ãƒ†ã‚£ãƒã‚¤ã‚¶ãƒ¼é–¢æ•°\n ```\n \n </pt>\n-<tf>\n-\n-`input_ids`ã‚’ TensorFlow ãƒ†ãƒ³ã‚½ãƒ«ã¨ã—ã¦è¿”ã—ã¾ã™ã€‚ tensors:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_opus_books_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n-```\n-\n-[`~transformers.generation_tf_utils.TFGenerationMixin.generate`] ãƒ¡ã‚½ãƒƒãƒ‰ã‚’ä½¿ç”¨ã—ã¦ç¿»è¨³ã‚’ä½œæˆã—ã¾ã™ã€‚ã•ã¾ã–ã¾ãªãƒ†ã‚­ã‚¹ãƒˆç”Ÿæˆæˆ¦ç•¥ã¨ç”Ÿæˆã‚’åˆ¶å¾¡ã™ã‚‹ãŸã‚ã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒ¼ã®è©³ç´°ã«ã¤ã„ã¦ã¯ã€[Text Generation](../main_classes/text_generation) API ã‚’ç¢ºèªã—ã¦ãã ã•ã„ã€‚\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"my_awesome_opus_books_model\")\n->>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n-```\n-\n-ç”Ÿæˆã•ã‚ŒãŸãƒˆãƒ¼ã‚¯ãƒ³ ID ã‚’ãƒ‡ã‚³ãƒ¼ãƒ‰ã—ã¦ãƒ†ã‚­ã‚¹ãƒˆã«æˆ»ã—ã¾ã™ã€‚\n-\n-```py\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'Les lugumes partagent les ressources avec des bactÃ©ries fixatrices d'azote.'\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "ff70ed8e31ef0e9d8e9f327b9a3a27bf6b78aed3",
            "filename": "docs/source/ja/training.md",
            "status": "modified",
            "additions": 0,
            "deletions": 106,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftraining.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fja%2Ftraining.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Ftraining.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -164,112 +164,6 @@ BERTãƒ¢ãƒ‡ãƒ«ã®äº‹å‰å­¦ç¿’æ¸ˆã¿ã®ãƒ˜ãƒƒãƒ‰ã¯ç ´æ£„ã•ã‚Œã€ãƒ©ãƒ³ãƒ€ãƒ ã«\n ```\n \n </pt>\n-<tf>\n-<a id='keras'></a>\n-\n-<Youtube id=\"rnTGBy2ax1c\"/>\n-\n-## Kerasã‚’ä½¿ç”¨ã—ã¦TensorFlowãƒ¢ãƒ‡ãƒ«ã‚’ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹\n-\n-Keras APIã‚’ä½¿ç”¨ã—ã¦ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’TensorFlowã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼\n-\n-### Loading Data from Keras\n-\n-ğŸ¤— Transformersãƒ¢ãƒ‡ãƒ«ã‚’Keras APIã§ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã™ã‚‹å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’KerasãŒç†è§£ã§ãã‚‹å½¢å¼ã«å¤‰æ›ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n-ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒå°ã•ã„å ´åˆã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã‚’NumPyé…åˆ—ã«å¤‰æ›ã—ã¦Kerasã«æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n-è¤‡é›‘ãªã“ã¨ã‚’ã™ã‚‹å‰ã«ã€ã¾ãšãã‚Œã‚’è©¦ã—ã¦ã¿ã¾ã—ã‚‡ã†ã€‚\n-\n-ã¾ãšã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’èª­ã¿è¾¼ã¿ã¾ã™ã€‚GLUEãƒ™ãƒ³ãƒãƒãƒ¼ã‚¯ã‹ã‚‰CoLAãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½¿ç”¨ã—ã¾ã™\n-([GLUE Banchmark](https://huggingface.co/datasets/glue))ã€ã“ã‚Œã¯å˜ç´”ãªãƒã‚¤ãƒŠãƒªãƒ†ã‚­ã‚¹ãƒˆåˆ†é¡ã‚¿ã‚¹ã‚¯ã§ã™ã€‚ä»Šã®ã¨ã“ã‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åˆ†å‰²ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™ã€‚\n-\n-```py\n-from datasets import load_dataset\n-\n-dataset = load_dataset(\"glue\", \"cola\")\n-dataset = dataset[\"train\"]  # ä»Šã®ã¨ã“ã‚ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°åˆ†å‰²ã®ã¿ã‚’ä½¿ç”¨ã—ã¾ã™\n-```\n-\n-æ¬¡ã«ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€ãƒ‡ãƒ¼ã‚¿ã‚’NumPyé…åˆ—ã¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã—ã¾ã™ã€‚ãƒ©ãƒ™ãƒ«ã¯æ—¢ã«`0`ã¨`1`ã®ãƒªã‚¹ãƒˆã§ã‚ã‚‹ãŸã‚ã€ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ã›ãšã«ç›´æ¥NumPyé…åˆ—ã«å¤‰æ›ã§ãã¾ã™ï¼\n-\n-```python\n-from transformers import AutoTokenizer\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-tokenized_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n-# ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã¯BatchEncodingã‚’è¿”ã—ã¾ã™ãŒã€ãã‚Œã‚’Kerasç”¨ã«è¾æ›¸ã«å¤‰æ›ã—ã¾ã™\n-tokenized_data = dict(tokenized_data)\n-\n-labels = np.array(dataset[\"label\"])  # ãƒ©ãƒ™ãƒ«ã¯ã™ã§ã«0ã¨1ã®é…åˆ—ã§ã™\n-```\n-\n-æœ€å¾Œã«ã€ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã€[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ã¨ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ãƒ¡ã‚½ãƒƒãƒ‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚\n-æ³¨æ„ç‚¹ã¨ã—ã¦ã€Transformersãƒ¢ãƒ‡ãƒ«ã¯ã™ã¹ã¦ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ã‚¿ã‚¹ã‚¯ã«é–¢é€£ã—ãŸæå¤±é–¢æ•°ã‚’æŒã£ã¦ã„ã‚‹ãŸã‚ã€æŒ‡å®šã—ãªãã¦ã‚‚æ§‹ã„ã¾ã›ã‚“ï¼ˆæŒ‡å®šã™ã‚‹å ´åˆã‚’é™¤ãï¼‰ï¼š\n-\n-```python\n-from transformers import TFAutoModelForSequenceClassification\n-from tensorflow.keras.optimizers import Adam\n-\n-# ãƒ¢ãƒ‡ãƒ«ã‚’ãƒ­ãƒ¼ãƒ‰ã—ã¦ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã™ã‚‹\n-model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n-# ãƒ•ã‚¡ã‚¤ãƒ³ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°ã«ã¯é€šå¸¸ã€å­¦ç¿’ç‡ã‚’ä¸‹ã’ã‚‹ã¨è‰¯ã„ã§ã™\n-model.compile(optimizer=Adam(3e-5))  # æå¤±é–¢æ•°ã®æŒ‡å®šã¯ä¸è¦ã§ã™ï¼\n-\n-model.fit(tokenized_data, labels)\n-```\n-\n-<Tip>\n-\n-ãƒ¢ãƒ‡ãƒ«ã‚’`compile()`ã™ã‚‹éš›ã«`loss`å¼•æ•°ã‚’æ¸¡ã™å¿…è¦ã¯ã‚ã‚Šã¾ã›ã‚“ï¼Hugging Faceãƒ¢ãƒ‡ãƒ«ã¯ã€ã“ã®å¼•æ•°ã‚’ç©ºç™½ã®ã¾ã¾ã«ã—ã¦ãŠãã¨ã€ã‚¿ã‚¹ã‚¯ã¨ãƒ¢ãƒ‡ãƒ«ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã«é©ã—ãŸæå¤±ã‚’è‡ªå‹•çš„ã«é¸æŠã—ã¾ã™ã€‚\n-å¿…è¦ã«å¿œã˜ã¦è‡ªåˆ†ã§æå¤±ã‚’æŒ‡å®šã—ã¦ã‚ªãƒ¼ãƒãƒ¼ãƒ©ã‚¤ãƒ‰ã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ï¼\n-\n-</Tip>\n-\n-ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã¯ã€å°è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«ã¯é©ã—ã¦ã„ã¾ã™ãŒã€å¤§è¦æ¨¡ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å¯¾ã—ã¦ã¯å•é¡Œã«ãªã‚‹ã“ã¨ãŒã‚ã‚Šã¾ã™ã€‚ãªãœãªã‚‰ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã•ã‚ŒãŸé…åˆ—ã¨ãƒ©ãƒ™ãƒ«ã¯ãƒ¡ãƒ¢ãƒªã«å®Œå…¨ã«èª­ã¿è¾¼ã¾ã‚Œã‚‹å¿…è¦ãŒã‚ã‚Šã€ã¾ãŸNumPyã¯ã€Œã‚¸ãƒ£ã‚®ãƒ¼ã€ãªé…åˆ—ã‚’å‡¦ç†ã—ãªã„ãŸã‚ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚ºã•ã‚ŒãŸå„ã‚µãƒ³ãƒ—ãƒ«ã‚’å…¨ä½“ã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå†…ã§æœ€ã‚‚é•·ã„ã‚µãƒ³ãƒ—ãƒ«ã®é•·ã•ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n-ã“ã‚Œã«ã‚ˆã‚Šã€é…åˆ—ãŒã•ã‚‰ã«å¤§ãããªã‚Šã€ã™ã¹ã¦ã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ãŒãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é…ãã™ã‚‹åŸå› ã«ãªã‚Šã¾ã™ï¼\n-\n-### Loading data as a tf.data.Dataset\n-\n-ãƒˆãƒ¬ãƒ¼ãƒ‹ãƒ³ã‚°ã‚’é…ãã›ãšã«ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã‚€ã«ã¯ã€ãƒ‡ãƒ¼ã‚¿ã‚’`tf.data.Dataset`ã¨ã—ã¦èª­ã¿è¾¼ã‚€ã“ã¨ãŒã§ãã¾ã™ã€‚ç‹¬è‡ªã®`tf.data`ãƒ‘ã‚¤ãƒ—ãƒ©ã‚¤ãƒ³ã‚’ä½œæˆã™ã‚‹ã“ã¨ã‚‚ã§ãã¾ã™ãŒã€ã“ã‚Œã‚’è¡Œã†ãŸã‚ã®ä¾¿åˆ©ãªæ–¹æ³•ãŒ2ã¤ã‚ã‚Šã¾ã™ï¼š\n-\n-- [`~TFPreTrainedModel.prepare_tf_dataset`]: ã“ã‚Œã¯ã»ã¨ã‚“ã©ã®å ´åˆã§æ¨å¥¨ã™ã‚‹æ–¹æ³•ã§ã™ã€‚ãƒ¢ãƒ‡ãƒ«ä¸Šã®ãƒ¡ã‚½ãƒƒãƒ‰ãªã®ã§ã€ãƒ¢ãƒ‡ãƒ«ã‚’æ¤œæŸ»ã—ã¦ãƒ¢ãƒ‡ãƒ«å…¥åŠ›ã¨ã—ã¦ä½¿ç”¨å¯èƒ½ãªåˆ—ã‚’è‡ªå‹•çš„ã«æŠŠæ¡ã—ã€ä»–ã®åˆ—ã‚’ç ´æ£„ã—ã¦ã‚ˆã‚Šå˜ç´”ã§é«˜æ€§èƒ½ãªãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä½œæˆã§ãã¾ã™ã€‚\n-- [`~datasets.Dataset.to_tf_dataset`]: ã“ã®ãƒ¡ã‚½ãƒƒãƒ‰ã¯ã‚ˆã‚Šä½ãƒ¬ãƒ™ãƒ«ã§ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãŒã©ã®ã‚ˆã†ã«ä½œæˆã•ã‚Œã‚‹ã‹ã‚’æ­£ç¢ºã«åˆ¶å¾¡ã™ã‚‹å ´åˆã«ä¾¿åˆ©ã§ã™ã€‚`columns`ã¨`label_cols`ã‚’æŒ‡å®šã—ã¦ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«å«ã‚ã‚‹åˆ—ã‚’æ­£ç¢ºã«æŒ‡å®šã§ãã¾ã™ã€‚\n-\n-[`~TFPreTrainedModel.prepare_tf_dataset`]ã‚’ä½¿ç”¨ã™ã‚‹å‰ã«ã€æ¬¡ã®ã‚³ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«ã«ç¤ºã™ã‚ˆã†ã«ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã®å‡ºåŠ›ã‚’ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åˆ—ã¨ã—ã¦è¿½åŠ ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ï¼š\n-\n-```py\n-def tokenize_dataset(data):\n-    # è¿”ã•ã‚ŒãŸè¾æ›¸ã®ã‚­ãƒ¼ã¯ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã«åˆ—ã¨ã—ã¦è¿½åŠ ã•ã‚Œã¾ã™\n-    return tokenizer(data[\"text\"])\n-\n-\n-dataset = dataset.map(tokenize_dataset)\n-```\n-\n-Hugging Faceã®ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã¯ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã§ãƒ‡ã‚£ã‚¹ã‚¯ã«ä¿å­˜ã•ã‚Œã‚‹ãŸã‚ã€ã“ã‚Œã«ã‚ˆã‚Šãƒ¡ãƒ¢ãƒªã®ä½¿ç”¨é‡ãŒå¢—ãˆã‚‹ã“ã¨ã¯ã‚ã‚Šã¾ã›ã‚“ï¼\n-åˆ—ãŒè¿½åŠ ã•ã‚ŒãŸã‚‰ã€ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‹ã‚‰ãƒãƒƒãƒã‚’ã‚¹ãƒˆãƒªãƒ¼ãƒ ã—ã€å„ãƒãƒƒãƒã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ ã§ãã¾ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Šã€\n-ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆå…¨ä½“ã«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã‚’è¿½åŠ ã™ã‚‹å ´åˆã¨æ¯”ã¹ã¦ã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãƒˆãƒ¼ã‚¯ãƒ³ã®æ•°ãŒå¤§å¹…ã«å‰Šæ¸›ã•ã‚Œã¾ã™ã€‚\n-\n-```python\n->>> tf_dataset = model.prepare_tf_dataset(dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer)\n-```\n-\n-ä¸Šè¨˜ã®ã‚³ãƒ¼ãƒ‰ã‚µãƒ³ãƒ—ãƒ«ã§ã¯ã€ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ã‚’`prepare_tf_dataset`ã«æ¸¡ã—ã¦ã€ãƒãƒƒãƒã‚’æ­£ã—ãèª­ã¿è¾¼ã‚€éš›ã«æ­£ã—ããƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã§ãã‚‹ã‚ˆã†ã«ã™ã‚‹å¿…è¦ãŒã‚ã‚Šã¾ã™ã€‚\n-ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®ã™ã¹ã¦ã®ã‚µãƒ³ãƒ—ãƒ«ãŒåŒã˜é•·ã•ã§ã‚ã‚Šã€ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ãŒä¸è¦ãªå ´åˆã¯ã€ã“ã®å¼•æ•°ã‚’ã‚¹ã‚­ãƒƒãƒ—ã§ãã¾ã™ã€‚\n-ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ä»¥å¤–ã®è¤‡é›‘ãªå‡¦ç†ã‚’è¡Œã†å¿…è¦ãŒã‚ã‚‹å ´åˆï¼ˆä¾‹ï¼šãƒã‚¹ã‚¯è¨€èªãƒ¢ãƒ‡ãƒªãƒ³ã‚°ã®ãŸã‚ã®ãƒˆãƒ¼ã‚¯ãƒ³ã®ç ´æãªã©ï¼‰ã€\n-ä»£ã‚ã‚Šã«`collate_fn`å¼•æ•°ã‚’ä½¿ç”¨ã—ã¦ã€ã‚µãƒ³ãƒ—ãƒ«ã®ãƒªã‚¹ãƒˆã‚’ãƒãƒƒãƒã«å¤‰æ›ã—ã€å¿…è¦ãªå‰å‡¦ç†ã‚’é©ç”¨ã™ã‚‹é–¢æ•°ã‚’æ¸¡ã™ã“ã¨ãŒã§ãã¾ã™ã€‚\n-ã“ã®ã‚¢ãƒ—ãƒ­ãƒ¼ãƒã‚’å®Ÿéš›ã«ä½¿ç”¨ã—ãŸä¾‹ã«ã¤ã„ã¦ã¯ã€\n-[examples](https://github.com/huggingface/transformers/tree/main/examples)ã‚„\n-[notebooks](https://huggingface.co/docs/transformers/notebooks)ã‚’ã”è¦§ãã ã•ã„ã€‚\n-\n-`tf.data.Dataset`ã‚’ä½œæˆã—ãŸã‚‰ã€ä»¥å‰ã¨åŒæ§˜ã«ãƒ¢ãƒ‡ãƒ«ã‚’ã‚³ãƒ³ãƒ‘ã‚¤ãƒ«ã—ã€é©åˆã•ã›ã‚‹ã“ã¨ãŒã§ãã¾ã™ï¼š\n-\n-```python\n-model.compile(optimizer=Adam(3e-5))  # æå¤±å¼•æ•°ã¯ä¸è¦ã§ã™ï¼\n-\n-model.fit(tf_dataset)\n-```\n-\n-</tf>\n </frameworkcontent>\n \n <a id='pytorch_native'></a>"
        },
        {
            "sha": "c2bf04365a7977a09bc59e705f76a52c59fab1b0",
            "filename": "docs/source/ko/model_sharing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Fmodel_sharing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Fmodel_sharing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_sharing.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -88,28 +88,6 @@ pip install huggingface_hub\n >>> pt_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n ```\n </pt>\n-<tf>\n-ì²´í¬í¬ì¸íŠ¸ë¥¼ PyTorchì—ì„œ TensorFlowë¡œ ë³€í™˜í•˜ë ¤ë©´ `from_pt=True`ë¥¼ ì§€ì •í•˜ì„¸ìš”:\n-\n-```py\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ ìƒˆë¡œìš´ ì²´í¬í¬ì¸íŠ¸ì™€ í•¨ê»˜ ìƒˆë¡œìš´ TensorFlow ëª¨ë¸ì„ ì €ì¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n->>> tf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n-```\n-</tf>\n-<jax>\n-Flaxì—ì„œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, PyTorchì—ì„œ Flaxë¡œ ì²´í¬í¬ì¸íŠ¸ë¥¼ ë³€í™˜í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n->>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\n-...     \"path/to/awesome-name-you-picked\", from_pt=True\n-... )\n-```\n-</jax>\n </frameworkcontent>\n \n ## í›ˆë ¨ ì¤‘ ëª¨ë¸ í‘¸ì‹œí•˜ê¸°[[push-a-model-during-training]]\n@@ -142,27 +120,6 @@ Flaxì—ì„œ ëª¨ë¸ì„ ì‚¬ìš©í•˜ëŠ” ê²½ìš°, PyTorchì—ì„œ Flaxë¡œ ì²´í¬í¬ì¸íŠ¸\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-[`PushToHubCallback`]ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í—ˆë¸Œì— ê³µìœ í•˜ë ¤ë©´, [`PushToHubCallback`]ì— ë‹¤ìŒ ì¸ìˆ˜ë¥¼ ì •ì˜í•˜ì„¸ìš”:\n-\n-- ì¶œë ¥ëœ ëª¨ë¸ì˜ íŒŒì¼ ê²½ë¡œ\n-- í† í¬ë‚˜ì´ì €\n-- `{Hub ì‚¬ìš©ì ì´ë¦„}/{ëª¨ë¸ ì´ë¦„}` í˜•ì‹ì˜ `hub_model_id`\n-\n-```py\n->>> from transformers import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\n-... )\n-```\n-\n-[`fit`](https://keras.io/api/models/model_training_apis/)ì— ì½œë°±ì„ ì¶”ê°€í•˜ë©´, ğŸ¤— Transformersê°€ í›ˆë ¨ëœ ëª¨ë¸ì„ í—ˆë¸Œë¡œ í‘¸ì‹œí•©ë‹ˆë‹¤:\n-\n-```py\n->>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\n-```\n-</tf>\n </frameworkcontent>\n \n ## `push_to_hub` í•¨ìˆ˜ ì‚¬ìš©í•˜ê¸°[[use-the-pushtohub-function]]"
        },
        {
            "sha": "133b04206c9e6af492f98d47b922f75d6dd808e8",
            "filename": "docs/source/ko/quicktour.md",
            "status": "modified",
            "additions": 0,
            "deletions": 92,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fquicktour.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -35,12 +35,6 @@ rendered properly in your Markdown viewer.\n pip install torch\n ```\n </pt>\n-<tf>\n-\n-```bash\n-pip install tensorflow\n-```\n-</tf>\n </frameworkcontent>\n \n ## íŒŒì´í”„ë¼ì¸ [[pipeline]]\n@@ -150,16 +144,6 @@ label: NEGATIVE, with score: 0.5309\n >>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n ```\n </pt>\n-<tf>\n-[`TFAutoModelForSequenceClassification`]ê³¼ [`AutoTokenizer`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ê³¼ ê´€ë ¨ëœ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ì„¸ìš” (ë‹¤ìŒ ì„¹ì…˜ì—ì„œ [`TFAutoClass`]ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤):\n-\n-```py\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n-```\n-</tf>\n </frameworkcontent>\n \n [`pipeline`]ì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì§€ì •í•˜ë©´, ì´ì œ `classifier`ë¥¼ í”„ë‘ìŠ¤ì–´ í…ìŠ¤íŠ¸ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n@@ -223,18 +207,6 @@ label: NEGATIVE, with score: 0.5309\n ... )\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> tf_batch = tokenizer(\n-...     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n-...     padding=True,\n-...     truncation=True,\n-...     max_length=512,\n-...     return_tensors=\"tf\",\n-... )\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -279,37 +251,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n         [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n ```\n </pt>\n-<tf>\n-ğŸ¤— TransformersëŠ” ì‚¬ì „ í›ˆë ¨ëœ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ê°„ë‹¨í•˜ê³  í†µí•©ëœ ë°©ë²•ìœ¼ë¡œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, [`AutoTokenizer`]ì²˜ëŸ¼ [`TFAutoModel`]ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ ì¼í•œ ì°¨ì´ì ì€ ê³¼ì—…ì— ì•Œë§ì€ [`TFAutoModel`]ì„ ì„ íƒí•´ì•¼ í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤. í…ìŠ¤íŠ¸ (ë˜ëŠ” ì‹œí€€ìŠ¤) ë¶„ë¥˜ì˜ ê²½ìš° [`TFAutoModelForSequenceClassification`]ì„ ë¡œë“œí•´ì•¼ í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n-```\n-\n-<Tip>\n-\n-[`AutoModel`] í´ë˜ìŠ¤ì—ì„œ ì§€ì›í•˜ëŠ” ê³¼ì—…ì— ëŒ€í•´ì„œëŠ” [ê³¼ì—… ìš”ì•½](./task_summary)ì„ ì°¸ì¡°í•˜ì„¸ìš”.\n-\n-</Tip>\n-\n-ì´ì œ ì „ì²˜ë¦¬ëœ ì…ë ¥ ë¬¶ìŒì„ ì§ì ‘ ëª¨ë¸ì— ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤. ì•„ë˜ì²˜ëŸ¼ ê·¸ëŒ€ë¡œ í…ì„œë¥¼ ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤:\n-\n-```py\n->>> tf_outputs = tf_model(tf_batch)\n-```\n-\n-ëª¨ë¸ì˜ ìµœì¢… í™œì„±í™” í•¨ìˆ˜ ì¶œë ¥ì€ `logits` ì†ì„±ì— ë‹´ê²¨ìˆìŠµë‹ˆë‹¤. `logits`ì— softmax í•¨ìˆ˜ë¥¼ ì ìš©í•˜ì—¬ í™•ë¥ ì„ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n->>> tf_predictions  # doctest: +IGNORE_RESULT\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -336,21 +277,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")\n ```\n </pt>\n-<tf>\n-ë¯¸ì„¸ì¡°ì •ëœ ëª¨ë¸ì„ í† í¬ë‚˜ì´ì €ì™€ í•¨ê»˜ ì €ì¥í•˜ë ¤ë©´ [`TFPreTrainedModel.save_pretrained`]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:\n-\n-```py\n->>> tf_save_directory = \"./tf_save_pretrained\"\n->>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT\n->>> tf_model.save_pretrained(tf_save_directory)\n-```\n-\n-ëª¨ë¸ì„ ë‹¤ì‹œ ì‚¬ìš©í•˜ë ¤ë©´ [`TFPreTrainedModel.from_pretrained`]ë¡œ ëª¨ë¸ì„ ë‹¤ì‹œ ë¡œë“œí•˜ì„¸ìš”:\n-\n-```py\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n-```\n-</tf>\n </frameworkcontent>\n \n ğŸ¤— Transformersì˜ ë©‹ì§„ ê¸°ëŠ¥ ì¤‘ í•˜ë‚˜ëŠ” ëª¨ë¸ì„ PyTorch ë˜ëŠ” TensorFlow ëª¨ë¸ë¡œ ì €ì¥í•´ë’€ë‹¤ê°€ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ë¡œ ë‹¤ì‹œ ë¡œë“œí•  ìˆ˜ ìˆëŠ” ì ì…ë‹ˆë‹¤. `from_pt` ë˜ëŠ” `from_tf` ë§¤ê°œë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•œ í”„ë ˆì„ì›Œí¬ì—ì„œ ë‹¤ë¥¸ í”„ë ˆì„ì›Œí¬ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n@@ -365,15 +291,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n-```\n-</tf>\n </frameworkcontent>\n \n ## ì»¤ìŠ¤í…€ ëª¨ë¸ êµ¬ì¶•í•˜ê¸° [[custom-model-builds]]\n@@ -398,15 +315,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n >>> my_model = AutoModel.from_config(my_config)\n ```\n </pt>\n-<tf>\n-[`TFAutoModel.from_config`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ë°”ê¾¼ êµ¬ì„±ëŒ€ë¡œ ëª¨ë¸ì„ ìƒì„±í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> my_model = TFAutoModel.from_config(my_config)\n-```\n-</tf>\n </frameworkcontent>\n \n ì»¤ìŠ¤í…€ êµ¬ì„±ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [ì»¤ìŠ¤í…€ ì•„í‚¤í…ì²˜ ë§Œë“¤ê¸°](./create_a_model) ê°€ì´ë“œë¥¼ í™•ì¸í•˜ì„¸ìš”."
        },
        {
            "sha": "70ff270c04a47cf8e04ee783fd2ffba4f6814b14",
            "filename": "docs/source/ko/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 38,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Frun_scripts.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -112,24 +112,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸ëŠ” ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/) ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë‹¤ìš´ë¡œë“œí•˜ê³  ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.\n-ê·¸ëŸ° ë‹¤ìŒ ìŠ¤í¬ë¦½íŠ¸ëŠ” ìš”ì•½ ê¸°ëŠ¥ì„ ì§€ì›í•˜ëŠ” ì•„í‚¤í…ì²˜ì—ì„œ Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤. \n-ë‹¤ìŒ ì˜ˆëŠ” [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) ë°ì´í„° ì„¸íŠ¸ì—ì„œ [T5-small](https://huggingface.co/google-t5/t5-small)ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤.\n-T5 ëª¨ë¸ì€ í›ˆë ¨ ë°©ì‹ì— ë”°ë¼ ì¶”ê°€ `source_prefix` ì¸ìˆ˜ê°€ í•„ìš”í•˜ë©°, ì´ í”„ë¡¬í”„íŠ¸ëŠ” ìš”ì•½ ì‘ì—…ì„ì„ T5ì— ì•Œë ¤ì¤ë‹ˆë‹¤.\n-```bash\n-python examples/tensorflow/summarization/run_summarization.py  \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## í˜¼í•© ì •ë°€ë„(mixed precision)ë¡œ ë¶„ì‚° í›ˆë ¨í•˜ê¸°[[distributed-training-and-mixed-precision]]\n@@ -184,25 +166,6 @@ python xla_spawn.py --num_cores 8 \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-Tensor Processing Units (TPUs)ëŠ” ì„±ëŠ¥ì„ ê°€ì†í™”í•˜ê¸° ìœ„í•´ íŠ¹ë³„íˆ ì„¤ê³„ë˜ì—ˆìŠµë‹ˆë‹¤.\n-TensorFlow ìŠ¤í¬ë¦½íŠ¸ëŠ” TPUë¥¼ í›ˆë ¨ì— ì‚¬ìš©í•˜ê¸° ìœ„í•´ [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy)ë¥¼ í™œìš©í•©ë‹ˆë‹¤.\n-TPUë¥¼ ì‚¬ìš©í•˜ë ¤ë©´ TPU ë¦¬ì†ŒìŠ¤ì˜ ì´ë¦„ì„ `tpu` ì¸ìˆ˜ì— ì „ë‹¬í•©ë‹ˆë‹¤.\n-\n-```bash\n-python run_summarization.py  \\\n-    --tpu name_of_tpu_resource \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## ğŸ¤— Accelerateë¡œ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰í•˜ê¸°[[run-a-script-with-accelerate]]\n@@ -372,4 +335,4 @@ python examples/pytorch/summarization/run_summarization.py\n     --per_device_eval_batch_size=4 \\\n     --overwrite_output_dir \\\n     --predict_with_generate\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "48ac6742431a28667f207443214e4268198b915b",
            "filename": "docs/source/ko/tasks/image_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 225,
            "changes": 225,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fimage_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fimage_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fimage_classification.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -152,92 +152,6 @@ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì—\n </frameworkcontent>\n \n \n-<frameworkcontent>\n-<tf>\n-\n-ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì„ ë³´ë‹¤ ê²¬ê³ í•˜ê²Œ ë§Œë“¤ê¸° ìœ„í•´ ë°ì´í„° ì„¸íŠ¸ì˜ í›ˆë ¨ ë¶€ë¶„ì— ë°ì´í„° ì¦ê°•ì„ ì¶”ê°€í•©ë‹ˆë‹¤.\n-ì—¬ê¸°ì„œ Keras ì „ì²˜ë¦¬ ë ˆì´ì–´ë¡œ í›ˆë ¨ ë°ì´í„°ì— ëŒ€í•œ ë³€í™˜(ë°ì´í„° ì¦ê°• í¬í•¨)ê³¼\n-ê²€ì¦ ë°ì´í„°ì— ëŒ€í•œ ë³€í™˜(ì¤‘ì•™ í¬ë¡œí•‘, í¬ê¸° ì¡°ì •, ì •ê·œí™”ë§Œ)ì„ ì •ì˜í•©ë‹ˆë‹¤.\n-`tf.image` ë˜ëŠ” ë‹¤ë¥¸ ì›í•˜ëŠ” ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-```py\n->>> from tensorflow import keras\n->>> from tensorflow.keras import layers\n-\n->>> size = (image_processor.size[\"height\"], image_processor.size[\"width\"])\n-\n->>> train_data_augmentation = keras.Sequential(\n-...     [\n-...         layers.RandomCrop(size[0], size[1]),\n-...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n-...         layers.RandomFlip(\"horizontal\"),\n-...         layers.RandomRotation(factor=0.02),\n-...         layers.RandomZoom(height_factor=0.2, width_factor=0.2),\n-...     ],\n-...     name=\"train_data_augmentation\",\n-... )\n-\n->>> val_data_augmentation = keras.Sequential(\n-...     [\n-...         layers.CenterCrop(size[0], size[1]),\n-...         layers.Rescaling(scale=1.0 / 127.5, offset=-1),\n-...     ],\n-...     name=\"val_data_augmentation\",\n-... )\n-```\n-\n-ë‹¤ìŒìœ¼ë¡œ í•œ ë²ˆì— í•˜ë‚˜ì˜ ì´ë¯¸ì§€ê°€ ì•„ë‹ˆë¼ ì´ë¯¸ì§€ ë°°ì¹˜ì— ì ì ˆí•œ ë³€í™˜ì„ ì ìš©í•˜ëŠ” í•¨ìˆ˜ë¥¼ ë§Œë“­ë‹ˆë‹¤.\n-\n-```py\n->>> import numpy as np\n->>> import tensorflow as tf\n->>> from PIL import Image\n-\n-\n->>> def convert_to_tf_tensor(image: Image):\n-...     np_image = np.array(image)\n-...     tf_image = tf.convert_to_tensor(np_image)\n-...     # `expand_dims()` is used to add a batch dimension since\n-...     # the TF augmentation layers operates on batched inputs.\n-...     return tf.expand_dims(tf_image, 0)\n-\n-\n->>> def preprocess_train(example_batch):\n-...     \"\"\"Apply train_transforms across a batch.\"\"\"\n-...     images = [\n-...         train_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n-...     ]\n-...     example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n-...     return example_batch\n-\n-\n-... def preprocess_val(example_batch):\n-...     \"\"\"Apply val_transforms across a batch.\"\"\"\n-...     images = [\n-...         val_data_augmentation(convert_to_tf_tensor(image.convert(\"RGB\"))) for image in example_batch[\"image\"]\n-...     ]\n-...     example_batch[\"pixel_values\"] = [tf.transpose(tf.squeeze(image)) for image in images]\n-...     return example_batch\n-```\n-\n-ğŸ¤— Datasets [`~datasets.Dataset.set_transform`]ë¥¼ ì‚¬ìš©í•˜ì—¬ ì¦‰ì‹œ ë³€í™˜ì„ ì ìš©í•˜ì„¸ìš”:\n-\n-```py\n-food[\"train\"].set_transform(preprocess_train)\n-food[\"test\"].set_transform(preprocess_val)\n-```\n-\n-ìµœì¢… ì „ì²˜ë¦¬ ë‹¨ê³„ë¡œ `DefaultDataCollator`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì˜ˆì œ ë°°ì¹˜ë¥¼ ë§Œë“­ë‹ˆë‹¤. ğŸ¤— Transformersì˜ ë‹¤ë¥¸ ë°ì´í„° ì½œë ˆì´í„°ì™€ ë‹¬ë¦¬\n-`DefaultDataCollator`ëŠ” íŒ¨ë”©ê³¼ ê°™ì€ ì¶”ê°€ ì „ì²˜ë¦¬ë¥¼ ì ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤.\n-\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-```\n-</tf>\n-</frameworkcontent>\n-\n ## í‰ê°€[[evaluate]]\n \n í›ˆë ¨ ì¤‘ì— í‰ê°€ ì§€í‘œë¥¼ í¬í•¨í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤.\n@@ -332,114 +246,6 @@ food[\"test\"].set_transform(preprocess_val)\n </pt>\n </frameworkcontent>\n \n-<frameworkcontent>\n-<tf>\n-\n-<Tip>\n-\n-Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šì€ ê²½ìš°, ë¨¼ì € [ê¸°ë³¸ íŠœí† ë¦¬ì–¼](./training#train-a-tensorflow-model-with-keras)ì„ í™•ì¸í•˜ì„¸ìš”!\n-\n-</Tip>\n-\n-TensorFlowì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:\n-1. í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ê³  ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ì„ ì„¤ì •í•©ë‹ˆë‹¤.\n-2. ì‚¬ì „ í›ˆë ¨ëœ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•©ë‹ˆë‹¤.\n-3. ğŸ¤— Datasetì„ `tf.data.Dataset`ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤.\n-4. ëª¨ë¸ì„ ì»´íŒŒì¼í•©ë‹ˆë‹¤.\n-5. ì½œë°±ì„ ì¶”ê°€í•˜ê³  í›ˆë ¨ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ `fit()` ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\n-6. ì»¤ë®¤ë‹ˆí‹°ì™€ ê³µìœ í•˜ê¸° ìœ„í•´ ëª¨ë¸ì„ ğŸ¤— Hubì— ì—…ë¡œë“œí•©ë‹ˆë‹¤.\n-\n-í•˜ì´í¼íŒŒë¼ë¯¸í„°, ì˜µí‹°ë§ˆì´ì € ë° í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ì„ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_epochs = 5\n->>> num_train_steps = len(food[\"train\"]) * num_epochs\n->>> learning_rate = 3e-5\n->>> weight_decay_rate = 0.01\n-\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=learning_rate,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=weight_decay_rate,\n-...     num_warmup_steps=0,\n-... )\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ ë ˆì´ë¸” ë§¤í•‘ê³¼ í•¨ê»˜ [`TFAuto ModelForImageClassification`]ìœ¼ë¡œ ViTë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForImageClassification\n-\n->>> model = TFAutoModelForImageClassification.from_pretrained(\n-...     checkpoint,\n-...     id2label=id2label,\n-...     label2id=label2id,\n-... )\n-```\n-\n-ë°ì´í„° ì„¸íŠ¸ë¥¼ [`~datasets.Dataset.to_tf_dataset`]ì™€ `data_collator`ë¥¼ ì‚¬ìš©í•˜ì—¬ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:\n-\n-```py\n->>> # converting our train dataset to tf.data.Dataset\n->>> tf_train_dataset = food[\"train\"].to_tf_dataset(\n-...     columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n-... )\n-\n->>> # converting our test dataset to tf.data.Dataset\n->>> tf_eval_dataset = food[\"test\"].to_tf_dataset(\n-...     columns=\"pixel_values\", label_cols=\"label\", shuffle=True, batch_size=batch_size, collate_fn=data_collator\n-... )\n-```\n-\n-`compile()`ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ëª¨ë¸ì„ êµ¬ì„±í•˜ì„¸ìš”:\n-\n-```py\n->>> from tensorflow.keras.losses import SparseCategoricalCrossentropy\n-\n->>> loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n->>> model.compile(optimizer=optimizer, loss=loss)\n-```\n-\n-ì˜ˆì¸¡ì—ì„œ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³  ëª¨ë¸ì„ ğŸ¤— Hubë¡œ í‘¸ì‹œí•˜ë ¤ë©´ [Keras callbacks](../main_classes/keras_callbacks)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n-`compute_metrics` í•¨ìˆ˜ë¥¼ [KerasMetricCallback](../main_classes/keras_callbacks#transformers.KerasMetricCallback)ì— ì „ë‹¬í•˜ê³ ,\n-[PushToHubCallback](../main_classes/keras_callbacks#transformers.PushToHubCallback)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_eval_dataset)\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"food_classifier\",\n-...     tokenizer=image_processor,\n-...     save_strategy=\"no\",\n-... )\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ì´ì œ ëª¨ë¸ì„ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ì„¸íŠ¸, ì—í­ ìˆ˜ì™€ í•¨ê»˜ `fit()`ì„ í˜¸ì¶œí•˜ê³ ,\n-ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤:\n-\n-```py\n->>> model.fit(tf_train_dataset, validation_data=tf_eval_dataset, epochs=num_epochs, callbacks=callbacks)\n-Epoch 1/5\n-250/250 [==============================] - 313s 1s/step - loss: 2.5623 - val_loss: 1.4161 - accuracy: 0.9290\n-Epoch 2/5\n-250/250 [==============================] - 265s 1s/step - loss: 0.9181 - val_loss: 0.6808 - accuracy: 0.9690\n-Epoch 3/5\n-250/250 [==============================] - 252s 1s/step - loss: 0.3910 - val_loss: 0.4303 - accuracy: 0.9820\n-Epoch 4/5\n-250/250 [==============================] - 251s 1s/step - loss: 0.2028 - val_loss: 0.3191 - accuracy: 0.9900\n-Epoch 5/5\n-250/250 [==============================] - 238s 949ms/step - loss: 0.1232 - val_loss: 0.3259 - accuracy: 0.9890\n-```\n-\n-ì¶•í•˜í•©ë‹ˆë‹¤! ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê³  ğŸ¤— Hubì— ê³µìœ í–ˆìŠµë‹ˆë‹¤. ì´ì œ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n-</tf>\n-</frameworkcontent>\n-\n \n <Tip>\n \n@@ -509,34 +315,3 @@ Epoch 5/5\n ```\n </pt>\n </frameworkcontent>\n-\n-<frameworkcontent>\n-<tf>\n-ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ê°€ì ¸ì˜¤ê³  `input`ì„ TensorFlow í…ì„œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import AutoImageProcessor\n-\n->>> image_processor = AutoImageProcessor.from_pretrained(\"MariaK/food_classifier\")\n->>> inputs = image_processor(image, return_tensors=\"tf\")\n-```\n-\n-ì…ë ¥ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ê³  logitsì„ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForImageClassification\n-\n->>> model = TFAutoModelForImageClassification.from_pretrained(\"MariaK/food_classifier\")\n->>> logits = model(**inputs).logits\n-```\n-\n-í™•ë¥ ì´ ê°€ì¥ ë†’ì€ ì˜ˆì¸¡ ë ˆì´ë¸”ì„ ê°€ì ¸ì˜¤ê³ , ëª¨ë¸ì˜ `id2label` ë§¤í•‘ì„ ì‚¬ìš©í•˜ì—¬ ë ˆì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n->>> model.config.id2label[predicted_class_id]\n-'beignets'\n-```\n-\n-</tf>\n-</frameworkcontent>"
        },
        {
            "sha": "d444a15ee6ddb58934182a2eda64ca77ede96878",
            "filename": "docs/source/ko/tasks/language_modeling.md",
            "status": "modified",
            "additions": 0,
            "deletions": 103,
            "changes": 103,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Flanguage_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Flanguage_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Flanguage_modeling.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -187,16 +187,6 @@ pip install transformers datasets evaluate\n ```\n \n </pt>\n-<tf>\n-íŒ¨ë”© í† í°ìœ¼ë¡œ ì¢…ê²° í† í°ì„ ì‚¬ìš©í•˜ê³  `mlm=False`ë¡œ ì„¤ì •í•˜ì„¸ìš”. ì´ë ‡ê²Œ í•˜ë©´ ì…ë ¥ì„ ì˜¤ë¥¸ìª½ìœ¼ë¡œ í•œ ì¹¸ì”© ì‹œí”„íŠ¸í•œ ê°’ì„ ë ˆì´ë¸”ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import DataCollatorForLanguageModeling\n-\n->>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")\n-```\n-\n-</tf>\n </frameworkcontent>\n \n \n@@ -260,73 +250,6 @@ Perplexity: 49.61\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë°©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´ [ê¸°ë³¸ íŠœí† ë¦¬ì–¼](../training#train-a-tensorflow-model-with-keras)ì„ í™•ì¸í•´ë³´ì„¸ìš”!\n-\n-</Tip>\n-TensorFlowì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´, ë¨¼ì € ì˜µí‹°ë§ˆì´ì € í•¨ìˆ˜, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ë° ì¼ë¶€ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ [`TFAutoModelForCausalLM`]ë¥¼ ì‚¬ìš©í•˜ì—¬ DistilGPT2ë¥¼ ë¶ˆëŸ¬ì˜µë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForCausalLM\n-\n->>> model = TFAutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•´ êµ¬ì„±í•˜ì„¸ìš”. Transformers ëª¨ë¸ì€ ëª¨ë‘ ê¸°ë³¸ì ì¸ ì‘ì—… ê´€ë ¨ ì†ì‹¤ í•¨ìˆ˜ë¥¼ ê°€ì§€ê³  ìˆìœ¼ë¯€ë¡œ, ì›í•œë‹¤ë©´ ë³„ë„ë¡œ ì§€ì •í•˜ì§€ ì•Šì•„ë„ ë©ë‹ˆë‹¤:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)  # ë³„ë„ë¡œ loss ì¸ìë¥¼ ë„£ì§€ ì•Šì•˜ì–´ìš”!\n-```\n-\n-[`~transformers.PushToHubCallback`]ì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì—…ë¡œë“œí•  ìœ„ì¹˜ë¥¼ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_eli5_clm-model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-ë§ˆì§€ë§‰ìœ¼ë¡œ, ëª¨ë¸ì„ í›ˆë ¨í•˜ê¸° ìœ„í•´ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)ì„ í˜¸ì¶œí•˜ì„¸ìš”. í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸, ê²€ì¦ ë°ì´í„° ì„¸íŠ¸, ì—í­ ìˆ˜ ë° ì½œë°±ì„ ì „ë‹¬í•˜ì„¸ìš”:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n-```\n-\n-í›ˆë ¨ì´ ì™„ë£Œë˜ë©´ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ í—ˆë¸Œì— ì—…ë¡œë“œë˜ì–´ ëª¨ë‘ê°€ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -382,30 +305,4 @@ TensorFlowì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´, ë¨¼ì € ì˜µí‹°ë§ˆì´ì € í•¨ìˆ˜\n [\"Somatic hypermutation allows the immune system to react to drugs with the ability to adapt to a different environmental situation. In other words, a system of 'hypermutation' can help the immune system to adapt to a different environmental situation or in some cases even a single life. In contrast, researchers at the University of Massachusetts-Boston have found that 'hypermutation' is much stronger in mice than in humans but can be found in humans, and that it's not completely unknown to the immune system. A study on how the immune system\"]\n ```\n </pt>\n-<tf>\n-í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  `input_ids`ë¥¼ TensorFlow í…ì„œë¡œ ë°˜í™˜í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_eli5_clm-model\")\n->>> inputs = tokenizer(prompt, return_tensors=\"tf\").input_ids\n-```\n-\n-[`~transformers.generation_tf_utils.TFGenerationMixin.generate`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ìš”ì•½ì„ ìƒì„±í•˜ì„¸ìš”. ìƒì„±ì„ ì œì–´í•˜ëŠ” ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìƒì„± ì „ëµê³¼ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [í…ìŠ¤íŠ¸ ìƒì„± ì „ëµ](../generation_strategies) í˜ì´ì§€ë¥¼ í™•ì¸í•˜ì„¸ìš”.\n-\n-```py\n->>> from transformers import TFAutoModelForCausalLM\n-\n->>> model = TFAutoModelForCausalLM.from_pretrained(\"my_awesome_eli5_clm-model\")\n->>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n-```\n-\n-ìƒì„±ëœ í† í° IDë¥¼ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•˜ì„¸ìš”:\n-\n-```py\n->>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-['Somatic hypermutation allows the immune system to detect the presence of other viruses as they become more prevalent. Therefore, researchers have identified a high proportion of human viruses. The proportion of virus-associated viruses in our study increases with age. Therefore, we propose a simple algorithm to detect the presence of these new viruses in our samples as a sign of improved immunity. A first study based on this algorithm, which will be published in Science on Friday, aims to show that this finding could translate into the development of a better vaccine that is more effective for']\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "cb9216b1e6bcb4a580092a70b1730ab2256d1343",
            "filename": "docs/source/ko/tasks/masked_language_modeling.md",
            "status": "modified",
            "additions": 0,
            "deletions": 110,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fmasked_language_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fmasked_language_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fmasked_language_modeling.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -191,16 +191,6 @@ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì™€\n >>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n ```\n </pt>\n-<tf>\n-\n-ì‹œí€€ìŠ¤ ë í† í°ì„ íŒ¨ë”© í† í°ìœ¼ë¡œ ì‚¬ìš©í•˜ê³  ë°ì´í„°ë¥¼ ë°˜ë³µí•  ë•Œë§ˆë‹¤ í† í°ì„ ë¬´ì‘ìœ„ë¡œ ë§ˆìŠ¤í‚¹í•˜ë„ë¡ `mlm_-probability`ë¥¼ ì§€ì •í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import DataCollatorForLanguageModeling\n-\n->>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## í›ˆë ¨[[train]]\n@@ -263,74 +253,6 @@ Perplexity: 8.76\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Kerasë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë° ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ [ì—¬ê¸°](../training#train-a-tensorflow-model-with-keras)ë¥¼ ì‚´í´ë³´ì„¸ìš”!\n-\n-</Tip>\n-TensorFlowë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê¸° ìœ„í•´ì„œëŠ” ì˜µí‹°ë§ˆì´ì €(optimizer) í•¨ìˆ˜ ì„¤ì •, í•™ìŠµë¥ (learning rate) ìŠ¤ì¼€ì¥´ë§, í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •ë¶€í„° ì‹œì‘í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-ë‹¤ìŒìœ¼ë¡œ [`TFAutoModelForMaskedLM`]ë¥¼ ì‚¬ìš©í•´ DistilRoBERTa ëª¨ë¸ì„ ê°€ì ¸ì˜µë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForMaskedLM\n-\n->>> model = TFAutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•´ ë°ì´í„° ì„¸íŠ¸ë¥¼ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     lm_dataset[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ë©”ì†Œë“œë¥¼ í†µí•´ ëª¨ë¸ í›ˆë ¨ì„ êµ¬ì„±í•©ë‹ˆë‹¤:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-ì´ëŠ” ì—…ë¡œë“œí•  ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ì˜ ìœ„ì¹˜ë¥¼ [`~transformers.PushToHubCallback`]ì— ì§€ì •í•˜ì—¬ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_eli5_mlm_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-ë“œë””ì–´ ëª¨ë¸ì„ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!\n-ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•  ë•Œ í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ì„¸íŠ¸, ì—í¬í¬ ìˆ˜, ì½œë°±ì´ í¬í•¨ëœ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)ì„ í˜¸ì¶œí•©ë‹ˆë‹¤:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n-```\n-\n-í›ˆë ¨ì´ ì™„ë£Œë˜ë©´, ìë™ìœ¼ë¡œ Hubë¡œ ì—…ë¡œë“œë˜ì–´ ëˆ„êµ¬ë‚˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -406,36 +328,4 @@ The Milky Way is a massive galaxy.\n The Milky Way is a small galaxy.\n ```\n </pt>\n-<tf>\n-í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  `input_ids`ë¥¼ TensorFlow í…ì„œ í˜•íƒœë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.\n-ë˜í•œ, `<mask>` í† í°ì˜ ìœ„ì¹˜ë¥¼ ì§€ì •í•´ì•¼ í•©ë‹ˆë‹¤:\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_eli5_mlm_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\")\n->>> mask_token_index = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n-```\n-\n-ëª¨ë¸ì— `inputs`ë¥¼ ì…ë ¥í•˜ê³ , ë§ˆìŠ¤í‚¹ëœ í† í°ì˜ `logits`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForMaskedLM\n-\n->>> model = TFAutoModelForMaskedLM.from_pretrained(\"stevhliu/my_awesome_eli5_mlm_model\")\n->>> logits = model(**inputs).logits\n->>> mask_token_logits = logits[0, mask_token_index, :]\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ ê°€ì¥ ë†’ì€ í™•ë¥ ì€ ê°€ì§„ ë§ˆìŠ¤í¬ í† í° 3ê°œë¥¼ ë°˜í™˜í•˜ê³ , ì¶œë ¥í•©ë‹ˆë‹¤:\n-```py\n->>> top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()\n-\n->>> for token in top_3_tokens:\n-...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))\n-The Milky Way is a spiral galaxy.\n-The Milky Way is a massive galaxy.\n-The Milky Way is a small galaxy.\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "7756951f07ecab3c7eeea3aee2564aef5b74e984",
            "filename": "docs/source/ko/tasks/multiple_choice.md",
            "status": "modified",
            "additions": 0,
            "deletions": 114,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fmultiple_choice.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fmultiple_choice.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fmultiple_choice.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -199,91 +199,6 @@ tokenized_swag = swag.map(preprocess_function, batched=True)\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Kerasë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë° ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ [ì—¬ê¸°](../training#train-a-tensorflow-model-with-keras)ë¥¼ ì‚´í´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤!\n-\n-</Tip>\n-TensorFlowì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ìµœì í™” í•¨ìˆ˜, í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ ë° ëª‡ ê°€ì§€ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒë¶€í„° ì‹œì‘í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_train_epochs = 2\n->>> total_train_steps = (len(tokenized_swag[\"train\"]) // batch_size) * num_train_epochs\n->>> optimizer, schedule = create_optimizer(init_lr=5e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n-```\n-\n-ê·¸ë¦¬ê³  [`TFAutoModelForMultipleChoice`]ë¡œ BERTë¥¼ ê°€ì ¸ì˜¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForMultipleChoice\n-\n->>> model = TFAutoModelForMultipleChoice.from_pretrained(\"google-bert/bert-base-uncased\")\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> data_collator = DataCollatorForMultipleChoice(tokenizer=tokenizer)\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_swag[\"train\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_swag[\"validation\"],\n-...     shuffle=False,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ì„ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤:\n-\n-```py\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-í›ˆë ¨ì„ ì‹œì‘í•˜ê¸° ì „ì— ì„¤ì •í•´ì•¼ í•  ë§ˆì§€ë§‰ ë‘ ê°€ì§€ëŠ” ì˜ˆì¸¡ì˜ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³  ëª¨ë¸ì„ í—ˆë¸Œë¡œ í‘¸ì‹œí•˜ëŠ” ë°©ë²•ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ ë‘ ê°€ì§€ ì‘ì—…ì€ ëª¨ë‘ [Keras ì½œë°±](../main_classes/keras_callbacks)ì„ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-`compute_metrics`í•¨ìˆ˜ë¥¼ [`~transformers.KerasMetricCallback`]ì— ì „ë‹¬í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì—…ë¡œë“œí•  ìœ„ì¹˜ë¥¼ [`~transformers.PushToHubCallback`]ì—ì„œ ì§€ì •í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-ê·¸ë¦¬ê³  ì½œë°±ì„ í•¨ê»˜ ë¬¶ìŠµë‹ˆë‹¤:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ì´ì œ ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•©ë‹ˆë‹¤! í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ì„¸íŠ¸, ì—í­ ìˆ˜, ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)ì„ í˜¸ì¶œí•˜ê³  ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=2, callbacks=callbacks)\n-```\n-\n-í›ˆë ¨ì´ ì™„ë£Œë˜ë©´ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ í—ˆë¸Œì— ì—…ë¡œë“œë˜ì–´ ëˆ„êµ¬ë‚˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n-</tf>\n </frameworkcontent>\n \n \n@@ -337,33 +252,4 @@ TensorFlowì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ìµœì í™” í•¨ìˆ˜, í•™ìŠµë¥  \n '0'\n ```\n </pt>\n-<tf>\n-ê° í”„ë¡¬í”„íŠ¸ì™€ í›„ë³´ ë‹µì•ˆ ìŒì„ í† í°í™”í•˜ì—¬ í…ì„œí”Œë¡œ í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_swag_model\")\n->>> inputs = tokenizer([[prompt, candidate1], [prompt, candidate2]], return_tensors=\"tf\", padding=True)\n-```\n-\n-ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ê³  `logits`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForMultipleChoice\n-\n->>> model = TFAutoModelForMultipleChoice.from_pretrained(\"my_awesome_swag_model\")\n->>> inputs = {k: tf.expand_dims(v, 0) for k, v in inputs.items()}\n->>> outputs = model(inputs)\n->>> logits = outputs.logits\n-```\n-\n-ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:\n-\n-```py\n->>> predicted_class = int(tf.math.argmax(logits, axis=-1)[0])\n->>> predicted_class\n-'0'\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "f0f1cab6b6480ae4bb18694a24167de881d20b84",
            "filename": "docs/source/ko/tasks/question_answering.md",
            "status": "modified",
            "additions": 0,
            "deletions": 114,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fquestion_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fquestion_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fquestion_answering.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -173,13 +173,6 @@ pip install transformers datasets evaluate\n >>> data_collator = DefaultDataCollator()\n ```\n </pt>\n-<tf>\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## í›ˆë ¨[[train]]\n@@ -236,79 +229,6 @@ pip install transformers datasets evaluate\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Kerasë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ê²ƒì— ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´, [ì—¬ê¸°](../training#train-a-tensorflow-model-with-keras)ì—ì„œ ê¸°ì´ˆ íŠœí† ë¦¬ì–¼ì„ ì‚´í´ë³´ì„¸ìš”!\n-\n-</Tip>\n-TensorFlowë¥¼ ì´ìš©í•œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ì˜µí‹°ë§ˆì´ì € í•¨ìˆ˜, í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ ë° ëª‡ ê°€ì§€ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ëŠ” ê²ƒë¶€í„° ì‹œì‘í•´ì•¼í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_epochs = 2\n->>> total_train_steps = (len(tokenized_squad[\"train\"]) // batch_size) * num_epochs\n->>> optimizer, schedule = create_optimizer(\n-...     init_lr=2e-5,\n-...     num_warmup_steps=0,\n-...     num_train_steps=total_train_steps,\n-... )\n-```\n-\n-ê·¸ ë‹¤ìŒ [`TFAutoModelForQuestionAnswering`]ìœ¼ë¡œ DistilBERTë¥¼ ê°€ì ¸ì˜µë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForQuestionAnswering\n-\n->>> model = TFAutoModelForQuestionAnswering(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]ì„ ì‚¬ìš©í•´ì„œ ë°ì´í„° ì„¸íŠ¸ë¥¼ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_squad[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_squad[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ë¡œ í›ˆë ¨í•  ëª¨ë¸ì„ ì„¤ì •í•©ë‹ˆë‹¤:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë¸ì„ Hubë¡œ í‘¸ì‹œí•  ë°©ë²•ì„ ì„¤ì •í•©ë‹ˆë‹¤. [`~transformers.PushToHubCallback`]ì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í‘¸ì‹œí•  ê²½ë¡œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_qa_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-ë“œë””ì–´ ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸ì™€ í‰ê°€ ë°ì´í„° ì„¸íŠ¸, ì—í­ ìˆ˜, ì½œë°±ì„ ì„¤ì •í•œ í›„ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)ì„ ì´ìš©í•´ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=[callback])\n-```\n-í›ˆë ¨ì´ ì™„ë£Œë˜ë©´ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ Hubì— ì—…ë¡œë“œë˜ì–´ ëˆ„êµ¬ë‚˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -385,38 +305,4 @@ TensorFlowë¥¼ ì´ìš©í•œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ì˜µí‹°ë§ˆì´ì € í•¨ìˆ˜,\n '176 billion parameters and can generate text in 46 languages natural languages and 13'\n ```\n </pt>\n-<tf>\n-í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•´ì„œ TensorFlow í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_qa_model\")\n->>> inputs = tokenizer(question, text, return_tensors=\"tf\")\n-```\n-\n-ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ê³  `logits`ì„ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForQuestionAnswering\n-\n->>> model = TFAutoModelForQuestionAnswering.from_pretrained(\"my_awesome_qa_model\")\n->>> outputs = model(**inputs)\n-```\n-\n-ëª¨ë¸ì˜ ì¶œë ¥ì—ì„œ ì‹œì‘ ë° ì¢…ë£Œ ìœ„ì¹˜ê°€ ì–´ë”˜ì§€ ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ì–»ìŠµë‹ˆë‹¤:\n-\n-```py\n->>> answer_start_index = int(tf.math.argmax(outputs.start_logits, axis=-1)[0])\n->>> answer_end_index = int(tf.math.argmax(outputs.end_logits, axis=-1)[0])\n-```\n-\n-ì˜ˆì¸¡ëœ í† í°ì„ í•´ë…í•´ì„œ ë‹µì„ ì–»ìŠµë‹ˆë‹¤:\n-\n-```py\n->>> predict_answer_tokens = inputs.input_ids[0, answer_start_index : answer_end_index + 1]\n->>> tokenizer.decode(predict_answer_tokens)\n-'176 billion parameters and can generate text in 46 languages natural languages and 13'\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "167417412c47787d0ce73426484c4f1f6135e9b0",
            "filename": "docs/source/ko/tasks/semantic_segmentation.md",
            "status": "modified",
            "additions": 0,
            "deletions": 226,
            "changes": 226,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fsemantic_segmentation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fsemantic_segmentation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fsemantic_segmentation.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -142,62 +142,6 @@ pip install -q datasets transformers evaluate\n </pt>\n </frameworkcontent>\n \n-<frameworkcontent>\n-<tf>\n-\n-ì´ë¯¸ì§€ ë°ì´í„° ì„¸íŠ¸ì— ë°ì´í„° ì¦ê°•ì„ ì ìš©í•˜ì—¬ ê³¼ì í•©ì— ëŒ€í•´ ëª¨ë¸ì„ ë³´ë‹¤ ê°•ê±´í•˜ê²Œ ë§Œë“œëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. ì´ ê°€ì´ë“œì—ì„œëŠ” [`tf.image`](https://www.tensorflow.org/api_docs/python/tf/image)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì˜ ìƒ‰ìƒ ì†ì„±ì„ ì„ì˜ë¡œ ë³€ê²½í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ, ìì‹ ì´ ì›í•˜ëŠ” ì´ë¯¸ì§€ ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤.\n-\n-ë³„ê°œì˜ ë‘ ë³€í™˜ í•¨ìˆ˜ë¥¼ ì •ì˜í•©ë‹ˆë‹¤:\n-- ì´ë¯¸ì§€ ì¦ê°•ì„ í¬í•¨í•˜ëŠ” í•™ìŠµ ë°ì´í„° ë³€í™˜\n-- ğŸ¤— Transformersì˜ ì»´í“¨í„° ë¹„ì „ ëª¨ë¸ì€ ì±„ë„ ìš°ì„  ë ˆì´ì•„ì›ƒì„ ê¸°ëŒ€í•˜ê¸° ë•Œë¬¸ì—, ì´ë¯¸ì§€ë§Œ ë°”ê¾¸ëŠ” ê²€ì¦ ë°ì´í„° ë³€í™˜\n-\n-```py\n->>> import tensorflow as tf\n-\n-\n->>> def aug_transforms(image):\n-...     image = tf.keras.utils.img_to_array(image)\n-...     image = tf.image.random_brightness(image, 0.25)\n-...     image = tf.image.random_contrast(image, 0.5, 2.0)\n-...     image = tf.image.random_saturation(image, 0.75, 1.25)\n-...     image = tf.image.random_hue(image, 0.1)\n-...     image = tf.transpose(image, (2, 0, 1))\n-...     return image\n-\n-\n->>> def transforms(image):\n-...     image = tf.keras.utils.img_to_array(image)\n-...     image = tf.transpose(image, (2, 0, 1))\n-...     return image\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ ëª¨ë¸ì„ ìœ„í•´ ë‘ ê°œì˜ ì „ì²˜ë¦¬ í•¨ìˆ˜ë¥¼ ë§Œë“¤ì–´ ì´ë¯¸ì§€ ë° ì£¼ì„ ë°°ì¹˜ë¥¼ ì¤€ë¹„í•©ë‹ˆë‹¤. ì´ í•¨ìˆ˜ë“¤ì€ ì´ë¯¸ì§€ ë³€í™˜ì„ ì ìš©í•˜ê³  ì´ì „ì— ë¡œë“œí•œ `image_processor`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ `pixel_values`ë¡œ, ì£¼ì„ì„ `label`ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. `ImageProcessor` ëŠ” ì´ë¯¸ì§€ì˜ í¬ê¸° ì¡°ì •ê³¼ ì •ê·œí™”ë„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n-\n-```py\n->>> def train_transforms(example_batch):\n-...     images = [aug_transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n-...     labels = [x for x in example_batch[\"annotation\"]]\n-...     inputs = image_processor(images, labels)\n-...     return inputs\n-\n-\n->>> def val_transforms(example_batch):\n-...     images = [transforms(x.convert(\"RGB\")) for x in example_batch[\"image\"]]\n-...     labels = [x for x in example_batch[\"annotation\"]]\n-...     inputs = image_processor(images, labels)\n-...     return inputs\n-```\n-\n-ì „ì²´ ë°ì´í„° ì§‘í•©ì— ì „ì²˜ë¦¬ ë³€í™˜ì„ ì ìš©í•˜ë ¤ë©´ ğŸ¤— Datasets [`~datasets.Dataset.set_transform`] í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n-ì¦‰ì‹œ ë³€í™˜ì´ ì ìš©ë˜ê¸° ë•Œë¬¸ì— ë” ë¹ ë¥´ê³  ë””ìŠ¤í¬ ê³µê°„ì„ ëœ ì°¨ì§€í•©ë‹ˆë‹¤:\n-\n-```py\n->>> train_ds.set_transform(train_transforms)\n->>> test_ds.set_transform(val_transforms)\n-```\n-</tf>\n-</frameworkcontent>\n-\n ## í‰ê°€í•˜ê¸°[[evaluate]]\n \n í›ˆë ¨ ì¤‘ì— ë©”íŠ¸ë¦­ì„ í¬í•¨í•˜ë©´ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€ ë°©ë²•ì„ ë¹ ë¥´ê²Œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ íƒœìŠ¤í¬ì—ì„œëŠ” [mean Intersection over Union](https://huggingface.co/spaces/evaluate-metric/accuracy) (IoU) ë©”íŠ¸ë¦­ì„ ë¡œë“œí•˜ì„¸ìš” (ë©”íŠ¸ë¦­ì„ ë¡œë“œí•˜ê³  ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ ğŸ¤— Evaluate [quick tour](https://huggingface.co/docs/evaluate/a_quick_tour)ë¥¼ ì‚´í´ë³´ì„¸ìš”).\n@@ -247,39 +191,6 @@ pip install -q datasets transformers evaluate\n </frameworkcontent>\n \n \n-<frameworkcontent>\n-<tf>\n-\n-```py\n->>> def compute_metrics(eval_pred):\n-...     logits, labels = eval_pred\n-...     logits = tf.transpose(logits, perm=[0, 2, 3, 1])\n-...     logits_resized = tf.image.resize(\n-...         logits,\n-...         size=tf.shape(labels)[1:],\n-...         method=\"bilinear\",\n-...     )\n-\n-...     pred_labels = tf.argmax(logits_resized, axis=-1)\n-...     metrics = metric.compute(\n-...         predictions=pred_labels,\n-...         references=labels,\n-...         num_labels=num_labels,\n-...         ignore_index=-1,\n-...         reduce_labels=image_processor.do_reduce_labels,\n-...     )\n-\n-...     per_category_accuracy = metrics.pop(\"per_category_accuracy\").tolist()\n-...     per_category_iou = metrics.pop(\"per_category_iou\").tolist()\n-\n-...     metrics.update({f\"accuracy_{id2label[i]}\": v for i, v in enumerate(per_category_accuracy)})\n-...     metrics.update({f\"iou_{id2label[i]}\": v for i, v in enumerate(per_category_iou)})\n-...     return {\"val_\" + k: v for k, v in metrics.items()}\n-```\n-\n-</tf>\n-</frameworkcontent>\n-\n ì´ì œ `compute_metrics` í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤. íŠ¸ë ˆì´ë‹ì„ ì„¤ì •í•  ë•Œ ì´ í•¨ìˆ˜ë¡œ ëŒì•„ê°€ê²Œ ë©ë‹ˆë‹¤.\n \n ## í•™ìŠµí•˜ê¸°[[train]]\n@@ -341,106 +252,6 @@ pip install -q datasets transformers evaluate\n </pt>\n </frameworkcontent>\n \n-<frameworkcontent>\n-<tf>\n-<Tip>\n-\n-Kerasë¡œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ëŠ” ë° ìµìˆ™í•˜ì§€ ì•Šì€ ê²½ìš°, ë¨¼ì € [ê¸°ë³¸ íŠœí† ë¦¬ì–¼](../training#train-a-tensorflow-model-with-keras)ì„ í™•ì¸í•´ë³´ì„¸ìš”!\n-\n-</Tip>\n-\n-TensorFlowì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:\n-1. í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì •ì˜í•˜ê³  ì˜µí‹°ë§ˆì´ì €ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ë¥¼ ì„¤ì •í•˜ì„¸ìš”.\n-2. ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”í•˜ì„¸ìš”.\n-3. ğŸ¤— Datasetì„ `tf.data.Dataset`ë¡œ ë³€í™˜í•˜ì„¸ìš”.\n-4. ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ì„¸ìš”.\n-5. ì½œë°±ì„ ì¶”ê°€í•˜ì—¬ ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ê³  ğŸ¤— Hubì— ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ì„¸ìš”.\n-6. `fit()` ë©”ì„œë“œë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨ì„ ì‹¤í–‰í•˜ì„¸ìš”.\n-\n-í•˜ì´í¼íŒŒë¼ë¯¸í„°, ì˜µí‹°ë§ˆì´ì €, í•™ìŠµë¥  ìŠ¤ì¼€ì¥´ëŸ¬ë¥¼ ì •ì˜í•˜ëŠ” ê²ƒìœ¼ë¡œ ì‹œì‘í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 2\n->>> num_epochs = 50\n->>> num_train_steps = len(train_ds) * num_epochs\n->>> learning_rate = 6e-5\n->>> weight_decay_rate = 0.01\n-\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=learning_rate,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=weight_decay_rate,\n-...     num_warmup_steps=0,\n-... )\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ ë ˆì´ë¸” ë§¤í•‘ê³¼ í•¨ê»˜ [`TFAutoModelForSemanticSegmentation`]ì„ ì‚¬ìš©í•˜ì—¬ SegFormerë¥¼ ë¶ˆëŸ¬ì˜¤ê³  ì˜µí‹°ë§ˆì´ì €ë¡œ ì»´íŒŒì¼í•©ë‹ˆë‹¤. íŠ¸ëœìŠ¤í¬ë¨¸ ëª¨ë¸ì€ ëª¨ë‘ ë””í´íŠ¸ë¡œ íƒœìŠ¤í¬ ê´€ë ¨ ì†ì‹¤ í•¨ìˆ˜ê°€ ìˆìœ¼ë¯€ë¡œ ì›ì¹˜ ì•Šìœ¼ë©´ ì§€ì •í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForSemanticSegmentation\n-\n->>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\n-...     checkpoint,\n-...     id2label=id2label,\n-...     label2id=label2id,\n-... )\n->>> model.compile(optimizer=optimizer)  # ì†ì‹¤ í•¨ìˆ˜ ì¸ìê°€ ì—†ìŠµë‹ˆë‹¤!\n-```\n-\n-[`~datasets.Dataset.to_tf_dataset`] ì™€ [`DefaultDataCollator`]ë¥¼ ì‚¬ìš©í•´ ë°ì´í„° ì„¸íŠ¸ë¥¼ `tf.data.Dataset` í¬ë§·ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers import DefaultDataCollator\n-\n->>> data_collator = DefaultDataCollator(return_tensors=\"tf\")\n-\n->>> tf_train_dataset = train_ds.to_tf_dataset(\n-...     columns=[\"pixel_values\", \"label\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_eval_dataset = test_ds.to_tf_dataset(\n-...     columns=[\"pixel_values\", \"label\"],\n-...     shuffle=True,\n-...     batch_size=batch_size,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-ì˜ˆì¸¡ìœ¼ë¡œ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³  ëª¨ë¸ì„ ğŸ¤— Hubë¡œ í‘¸ì‹œí•˜ë ¤ë©´ [Keras callbacks](../main_classes/keras_callbacks)ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. `compute_metrics` í•¨ìˆ˜ë¥¼ [`KerasMetricCallback`]ì— ì „ë‹¬í•˜ê³ , ëª¨ë¸ ì—…ë¡œë“œë¥¼ ìœ„í•´ [`PushToHubCallback`]ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback, PushToHubCallback\n-\n->>> metric_callback = KerasMetricCallback(\n-...     metric_fn=compute_metrics, eval_dataset=tf_eval_dataset, batch_size=batch_size, label_cols=[\"labels\"]\n-... )\n-\n->>> push_to_hub_callback = PushToHubCallback(output_dir=\"scene_segmentation\", tokenizer=image_processor)\n-\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ì´ì œ ëª¨ë¸ì„ í›ˆë ¨í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ì„¸íŠ¸, ì—í¬í¬ ìˆ˜ì™€ í•¨ê»˜ `fit()`ì„ í˜¸ì¶œí•˜ê³ , ì½œë°±ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•©ë‹ˆë‹¤:\n-\n-```py\n->>> model.fit(\n-...     tf_train_dataset,\n-...     validation_data=tf_eval_dataset,\n-...     callbacks=callbacks,\n-...     epochs=num_epochs,\n-... )\n-```\n-\n-ì¶•í•˜í•©ë‹ˆë‹¤! ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ê³  ğŸ¤— Hubì— ê³µìœ í–ˆìŠµë‹ˆë‹¤. ì´ì œ ì¶”ë¡ ì— ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n-\n-</tf>\n-</frameworkcontent>\n-\n \n ## ì¶”ë¡ í•˜ê¸°[[inference]]\n \n@@ -525,43 +336,6 @@ TensorFlowì—ì„œ ëª¨ë¸ì„ ë¯¸ì„¸ ì¡°ì •í•˜ë ¤ë©´ ë‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¥´ì„¸ìš”:\n </pt>\n </frameworkcontent>\n \n-<frameworkcontent>\n-<tf>\n-ì´ë¯¸ì§€ í”„ë¡œì„¸ì„œë¥¼ ë¡œë“œí•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•˜ê³  ì…ë ¥ì„ TensorFlow í…ì„œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import AutoImageProcessor\n-\n->>> image_processor = AutoImageProcessor.from_pretrained(\"MariaK/scene_segmentation\")\n->>> inputs = image_processor(image, return_tensors=\"tf\")\n-```\n-\n-ëª¨ë¸ì— ì…ë ¥ì„ ì „ë‹¬í•˜ê³  `logits`ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForSemanticSegmentation\n-\n->>> model = TFAutoModelForSemanticSegmentation.from_pretrained(\"MariaK/scene_segmentation\")\n->>> logits = model(**inputs).logits\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ ë¡œê·¸ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì¬ì¡°ì •í•˜ê³  í´ë˜ìŠ¤ ì°¨ì›ì— argmaxë¥¼ ì ìš©í•©ë‹ˆë‹¤:\n-\n-```py\n->>> logits = tf.transpose(logits, [0, 2, 3, 1])\n-\n->>> upsampled_logits = tf.image.resize(\n-...     logits,\n-...     # `image.size`ê°€ ë„ˆë¹„ì™€ ë†’ì´ë¥¼ ë°˜í™˜í•˜ê¸° ë•Œë¬¸ì— `image`ì˜ ëª¨ì–‘ì„ ë°˜ì „ì‹œí‚µë‹ˆë‹¤\n-...     image.size[::-1],\n-... )\n-\n->>> pred_seg = tf.math.argmax(upsampled_logits, axis=-1)[0]\n-```\n-\n-</tf>\n-</frameworkcontent>\n-\n ê²°ê³¼ë¥¼ ì‹œê°í™”í•˜ë ¤ë©´ [dataset color palette](https://github.com/tensorflow/models/blob/3f1ca33afe3c1631b733ea7e40c294273b9e406d/research/deeplab/utils/get_dataset_colormap.py#L51)ë¥¼ ê° í´ë˜ìŠ¤ë¥¼ RGB ê°’ì— ë§¤í•‘í•˜ëŠ” `ade_palette()`ë¡œ ë¡œë“œí•©ë‹ˆë‹¤. ê·¸ëŸ° ë‹¤ìŒ ì´ë¯¸ì§€ì™€ ì˜ˆì¸¡ëœ ë¶„í•  ì§€ë„(segmentation map)ì„ ê²°í•©í•˜ì—¬ êµ¬ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n \n ```py"
        },
        {
            "sha": "1eda13c05e7dbe78f55dd4aed94ea884a75adcbb",
            "filename": "docs/source/ko/tasks/sequence_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 124,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fsequence_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fsequence_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fsequence_classification.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -105,13 +105,6 @@ tokenized_imdb = imdb.map(preprocess_function, batched=True)\n >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n ```\n </pt>\n-<tf>\n-```py\n->>> from transformers import DataCollatorWithPadding\n-\n->>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## í‰ê°€í•˜ê¸°[[evaluate]]\n@@ -210,96 +203,6 @@ tokenized_imdb = imdb.map(preprocess_function, batched=True)\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ëŠ” ë°©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šì€ ê²½ìš°, [ì—¬ê¸°](../training#train-a-tensorflow-model-with-keras)ì˜ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•˜ì„¸ìš”!\n-\n-</Tip>\n-TensorFlowì—ì„œ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ë ¤ë©´, ë¨¼ì € ì˜µí‹°ë§ˆì´ì € í•¨ìˆ˜ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¥´, ê·¸ë¦¬ê³  ì¼ë¶€ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import create_optimizer\n->>> import tensorflow as tf\n-\n->>> batch_size = 16\n->>> num_epochs = 5\n->>> batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n->>> total_train_steps = int(batches_per_epoch * num_epochs)\n->>> optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ [`TFAutoModelForSequenceClassification`]ì„ ì‚¬ìš©í•˜ì—¬ DistilBERTë¥¼ ë¡œë“œí•˜ê³ , ì˜ˆìƒë˜ëŠ” ë ˆì´ë¸” ìˆ˜ì™€ ë ˆì´ë¸” ë§¤í•‘ì„ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\n-...     \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n-... )\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_imdb[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_imdb[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•  ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-í›ˆë ¨ì„ ì‹œì‘í•˜ê¸° ì „ì— ì„¤ì •í•´ì•¼í•  ë§ˆì§€ë§‰ ë‘ ê°€ì§€ëŠ” ì˜ˆì¸¡ì—ì„œ ì •í™•ë„ë¥¼ ê³„ì‚°í•˜ê³ , ëª¨ë¸ì„ Hubì— ì—…ë¡œë“œí•  ë°©ë²•ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë‘ [Keras callbacks](../main_classes/keras_callbacks)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë©ë‹ˆë‹¤.\n-\n-[`~transformers.KerasMetricCallback`]ì— `compute_metrics`ë¥¼ ì „ë‹¬í•˜ì—¬ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤.\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-[`~transformers.PushToHubCallback`]ì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì—…ë¡œë“œí•  ìœ„ì¹˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ ì½œë°±ì„ í•¨ê»˜ ë¬¶ìŠµë‹ˆë‹¤:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ë“œë””ì–´, ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)ì— í›ˆë ¨ ë°ì´í„°ì…‹, ê²€ì¦ ë°ì´í„°ì…‹, ì—í­ì˜ ìˆ˜ ë° ì½œë°±ì„ ì „ë‹¬í•˜ì—¬ íŒŒì¸ íŠœë‹í•©ë‹ˆë‹¤:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n-```\n-\n-í›ˆë ¨ì´ ì™„ë£Œë˜ë©´, ëª¨ë¸ì´ ìë™ìœ¼ë¡œ Hubì— ì—…ë¡œë“œë˜ì–´ ëª¨ë“  ì‚¬ëŒì´ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -359,31 +262,4 @@ TensorFlowì—ì„œ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ë ¤ë©´, ë¨¼ì € ì˜µí‹°ë§ˆì´ì € í•¨ìˆ˜\n 'POSITIVE'\n ```\n </pt>\n-<tf>\n-í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  TensorFlow í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\")\n-```\n-\n-ì…ë ¥ê°’ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ê³  `logits`ì„ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n->>> logits = model(**inputs).logits\n-```\n-\n-ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ëª¨ë¸ì˜ `id2label` ë§¤í•‘ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë ˆì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n->>> model.config.id2label[predicted_class_id]\n-'POSITIVE'\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "c56af19bacfe6b6c520bc113b3e5e4c003461edb",
            "filename": "docs/source/ko/tasks/summarization.md",
            "status": "modified",
            "additions": 0,
            "deletions": 119,
            "changes": 119,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fsummarization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Fsummarization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fsummarization.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -132,13 +132,6 @@ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ë©´ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì—\n >>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n ```\n </pt>\n-<tf>\n-```py\n->>> from transformers import DataCollatorForSeq2Seq\n-\n->>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## í‰ê°€[[evaluate]]\n@@ -237,91 +230,6 @@ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ë©´ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì—\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Kerasë¡œ ëª¨ë¸ íŒŒì¸íŠœë‹ì„ í•˜ëŠ” ê²ƒì´ ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´, [ì—¬ê¸°](../training#train-a-tensorflow-model-with-keras)ì—ì„œ ê¸°ë³¸ì ì¸ íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•˜ì„¸ìš”!\n-\n-</Tip>\n-TensorFlowì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´, ë¨¼ì € ì˜µí‹°ë§ˆì´ì €, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ê·¸ë¦¬ê³  ëª‡ ê°€ì§€ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers import create_optimizer, AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ [`TFAutoModelForSeq2SeqLM`]ì„ ì‚¬ìš©í•˜ì—¬ T5ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”:\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„°ì…‹ì„ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_billsum[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     tokenized_billsum[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ì„ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•  ìˆ˜ ìˆë„ë¡ êµ¬ì„±í•˜ì„¸ìš”:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-í•™ìŠµì„ ì‹œì‘í•˜ê¸° ì „ì— ì„¤ì •í•´ì•¼ í•  ë§ˆì§€ë§‰ ë‘ ê°€ì§€ëŠ” ì˜ˆì¸¡ì—ì„œ ROUGE ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³  ëª¨ë¸ì„ Hubì— í‘¸ì‹œí•˜ëŠ” ë°©ë²•ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n-ë‘ ì‘ì—… ëª¨ë‘ [Keras callbacks](../main_classes/keras_callbacks)ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-[`~transformers.KerasMetricCallback`]ì— `compute_metrics` í•¨ìˆ˜ë¥¼ ì „ë‹¬í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-[`~transformers.PushToHubCallback`]ì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ í‘¸ì‹œí•  ìœ„ì¹˜ë¥¼ ì§€ì •í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_billsum_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ ì½œë°±ì„ ë²ˆë“¤ë¡œ ë¬¶ì–´ì¤ë‹ˆë‹¤:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ë“œë””ì–´ ëª¨ë¸ í•™ìŠµì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤!\n-í•™ìŠµ ë° ê²€ì¦ ë°ì´í„°ì…‹, ì—í­ ìˆ˜ ë° ì½œë°±ê³¼ í•¨ê»˜ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)ì„ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì„¸ìš”.\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n-```\n-\n-í•™ìŠµì´ ì™„ë£Œë˜ë©´ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ Hubì— ì—…ë¡œë“œë˜ì–´ ëˆ„êµ¬ë‚˜ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -383,31 +291,4 @@ TensorFlowì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´, ë¨¼ì € ì˜µí‹°ë§ˆì´ì €, í•™ìŠµ\n 'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'\n ```\n </pt>\n-<tf>\n-í…ìŠ¤íŠ¸ë¥¼ í† í¬ë‚˜ì´ì¦ˆí•˜ê³  `input_ids`ë¥¼ TensorFlow í…ì„œë¡œ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n-```\n-\n-ìš”ì•½ë¬¸ì„ ìƒì„±í•˜ë ¤ë©´ [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] ë©”ì†Œë“œë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n-í…ìŠ¤íŠ¸ ìƒì„±ì— ëŒ€í•œ ë‹¤ì–‘í•œ ì „ëµê³¼ ìƒì„±ì„ ì œì–´í•˜ê¸° ìœ„í•œ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [í…ìŠ¤íŠ¸ ìƒì„±](../main_classes/text_generation) APIë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"stevhliu/my_awesome_billsum_model\")\n->>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=False)\n-```\n-\n-ìƒì„±ëœ í† í° IDë¥¼ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•©ë‹ˆë‹¤:\n-\n-```py\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'the inflation reduction act lowers prescription drug costs, health care costs, and energy costs. it's the most aggressive action on tackling the climate crisis in american history. it will ask the ultra-wealthy and corporations to pay their fair share.'\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "61882f1e7075c03e82b7948b0fabff4583515306",
            "filename": "docs/source/ko/tasks/token_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 144,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Ftoken_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Ftoken_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Ftoken_classification.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -163,13 +163,6 @@ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì—\n >>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n ```\n </pt>\n-<tf>\n-```py\n->>> from transformers import DataCollatorForTokenClassification\n-\n->>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## í‰ê°€[[evaluation]]\n@@ -308,99 +301,6 @@ Hugging Face ê³„ì •ì— ë¡œê·¸ì¸í•˜ì—¬ ëª¨ë¸ì„ ì—…ë¡œë“œí•˜ê³  ì»¤ë®¤ë‹ˆí‹°ì—\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Kerasë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ëŠ” ë°©ë²•ì— ìµìˆ™í•˜ì§€ ì•Šì€ ê²½ìš°, [ì—¬ê¸°](../training#train-a-tensorflow-model-with-keras)ì˜ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ í™•ì¸í•˜ì„¸ìš”!\n-\n-</Tip>\n-TensorFlowì—ì„œ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ë ¤ë©´, ë¨¼ì € ì˜µí‹°ë§ˆì´ì € í•¨ìˆ˜ì™€ í•™ìŠµë¥  ìŠ¤ì¼€ì¥´, ê·¸ë¦¬ê³  ì¼ë¶€ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•´ì•¼ í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_train_epochs = 3\n->>> num_train_steps = (len(tokenized_wnut[\"train\"]) // batch_size) * num_train_epochs\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=2e-5,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=0.01,\n-...     num_warmup_steps=0,\n-... )\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ [`TFAutoModelForSequenceClassification`]ì„ ì‚¬ìš©í•˜ì—¬ DistilBERTë¥¼ ê°€ì ¸ì˜¤ê³ , ì˜ˆìƒë˜ëŠ” ë ˆì´ë¸” ìˆ˜ì™€ ë ˆì´ë¸” ë§¤í•‘ì„ ì§€ì •í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\n-...     \"distilbert/distilbert-base-uncased\", num_labels=13, id2label=id2label, label2id=label2id\n-... )\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]ì„ ì‚¬ìš©í•˜ì—¬ ë°ì´í„° ì„¸íŠ¸ë¥¼ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_wnut[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = model.prepare_tf_dataset(\n-...     tokenized_wnut[\"validation\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-[`compile`](https://keras.io/api/models/model_training_apis/#compile-method)ë¥¼ ì‚¬ìš©í•˜ì—¬ í›ˆë ¨í•  ëª¨ë¸ì„ êµ¬ì„±í•©ë‹ˆë‹¤:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-í›ˆë ¨ì„ ì‹œì‘í•˜ê¸° ì „ì— ì„¤ì •í•´ì•¼í•  ë§ˆì§€ë§‰ ë‘ ê°€ì§€ëŠ” ì˜ˆì¸¡ì—ì„œ seqeval ì ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ , ëª¨ë¸ì„ í—ˆë¸Œì— ì—…ë¡œë“œí•  ë°©ë²•ì„ ì œê³µí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ëª¨ë‘ [Keras callbacks](../main_classes/keras_callbacks)ë¥¼ ì‚¬ìš©í•˜ì—¬ ìˆ˜í–‰ë©ë‹ˆë‹¤.\n-\n-[`~transformers.KerasMetricCallback`]ì— `compute_metrics` í•¨ìˆ˜ë¥¼ ì „ë‹¬í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-[`~transformers.PushToHubCallback`]ì—ì„œ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì—…ë¡œë“œí•  ìœ„ì¹˜ë¥¼ ì§€ì •í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_wnut_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-ê·¸ëŸ° ë‹¤ìŒ ì½œë°±ì„ í•¨ê»˜ ë¬¶ìŠµë‹ˆë‹¤:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ë“œë””ì–´, ëª¨ë¸ í›ˆë ¨ì„ ì‹œì‘í•  ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤! [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)ì— í›ˆë ¨ ë°ì´í„° ì„¸íŠ¸, ê²€ì¦ ë°ì´í„° ì„¸íŠ¸, ì—í­ì˜ ìˆ˜ ë° ì½œë°±ì„ ì „ë‹¬í•˜ì—¬ íŒŒì¸ íŠœë‹í•©ë‹ˆë‹¤:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n-```\n-\n-í›ˆë ¨ì´ ì™„ë£Œë˜ë©´, ëª¨ë¸ì´ ìë™ìœ¼ë¡œ í—ˆë¸Œì— ì—…ë¡œë“œë˜ì–´ ëˆ„êµ¬ë‚˜ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -508,48 +408,4 @@ TensorFlowì—ì„œ ëª¨ë¸ì„ íŒŒì¸ íŠœë‹í•˜ë ¤ë©´, ë¨¼ì € ì˜µí‹°ë§ˆì´ì € í•¨ìˆ˜\n  'O']\n ```\n </pt>\n-<tf>\n-í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  TensorFlow í…ì„œë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\")\n-```\n-\n-ì…ë ¥ê°’ì„ ëª¨ë¸ì— ì „ë‹¬í•˜ê³  `logits`ì„ ë°˜í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"stevhliu/my_awesome_wnut_model\")\n->>> logits = model(**inputs).logits\n-```\n-\n-ê°€ì¥ ë†’ì€ í™•ë¥ ì„ ê°€ì§„ í´ë˜ìŠ¤ë¥¼ ëª¨ë¸ì˜ `id2label` ë§¤í•‘ì„ ì‚¬ìš©í•˜ì—¬ í…ìŠ¤íŠ¸ ë ˆì´ë¸”ë¡œ ë³€í™˜í•©ë‹ˆë‹¤:\n-\n-```py\n->>> predicted_token_class_ids = tf.math.argmax(logits, axis=-1)\n->>> predicted_token_class = [model.config.id2label[t] for t in predicted_token_class_ids[0].numpy().tolist()]\n->>> predicted_token_class\n-['O',\n- 'O',\n- 'B-location',\n- 'I-location',\n- 'B-group',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'O',\n- 'B-location',\n- 'B-location',\n- 'O',\n- 'O']\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "dd82be10f6eac9885662591d842a403730b15df4",
            "filename": "docs/source/ko/tasks/translation.md",
            "status": "modified",
            "additions": 0,
            "deletions": 117,
            "changes": 117,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Ftranslation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftasks%2Ftranslation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Ftranslation.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -122,14 +122,6 @@ pip install transformers datasets evaluate sacrebleu\n >>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import DataCollatorForSeq2Seq\n-\n->>> data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=checkpoint, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## í‰ê°€[[evalulate]]\n@@ -235,89 +227,6 @@ pip install transformers datasets evaluate sacrebleu\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-<Tip>\n-\n-Kerasë¡œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ëŠ” ë°©ë²•ì´ ìµìˆ™í•˜ì§€ ì•Šë‹¤ë©´, [ì—¬ê¸°](../training#train-a-tensorflow-model-with-keras)ì—ì„œ ê¸°ë³¸ íŠœí† ë¦¬ì–¼ì„ ì‚´í´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤!\n-\n-</Tip>\n-TensorFlowì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ ìš°ì„  optimizer í•¨ìˆ˜, í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ ë“±ì˜ í›ˆë ¨ í•˜ì´í¼íŒŒë¼ë¯¸í„°ë¥¼ ì„¤ì •í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers import AdamWeightDecay\n-\n->>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n-```\n-\n-ì´ì œ [`TFAutoModelForSeq2SeqLM`]ë¡œ T5ë¥¼ ê°€ì ¸ì˜¤ì„¸ìš”:\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(checkpoint)\n-```\n-\n-[`~transformers.TFPreTrainedModel.prepare_tf_dataset`]ë¡œ ë°ì´í„° ì„¸íŠ¸ë¥¼ `tf.data.Dataset` í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì„¸ìš”:\n-\n-```py\n->>> tf_train_set = model.prepare_tf_dataset(\n-...     tokenized_books[\"train\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_test_set = model.prepare_tf_dataset(\n-...     tokenized_books[\"test\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-í›ˆë ¨í•˜ê¸° ìœ„í•´ [`compile`](https://keras.io/api/models/model_training_apis/#compile-method) ë©”ì„œë“œë¡œ ëª¨ë¸ì„ êµ¬ì„±í•˜ì„¸ìš”:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-í›ˆë ¨ì„ ì‹œì‘í•˜ê¸° ì „ì— ì˜ˆì¸¡ê°’ìœ¼ë¡œë¶€í„° SacreBLEU ë©”íŠ¸ë¦­ì„ ê³„ì‚°í•˜ëŠ” ë°©ë²•ê³¼ ëª¨ë¸ì„ Hubì— ì—…ë¡œë“œí•˜ëŠ” ë°©ë²• ë‘ ê°€ì§€ë¥¼ ë¯¸ë¦¬ ì„¤ì •í•´ë‘¬ì•¼ í•©ë‹ˆë‹¤. ë‘˜ ë‹¤ [Keras callbacks](../main_classes/keras_callbacks)ë¡œ êµ¬í˜„í•˜ì„¸ìš”.\n-\n-[`~transformers.KerasMetricCallback`]ì— `compute_metrics` í•¨ìˆ˜ë¥¼ ì „ë‹¬í•˜ì„¸ìš”.\n-\n-```py\n->>> from transformers.keras_callbacks import KerasMetricCallback\n-\n->>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n-```\n-\n-ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ì—…ë¡œë“œí•  ìœ„ì¹˜ë¥¼ [`~transformers.PushToHubCallback`]ì—ì„œ ì§€ì •í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers.keras_callbacks import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"my_awesome_opus_books_model\",\n-...     tokenizer=tokenizer,\n-... )\n-```\n-\n-ì´ì œ ì½œë°±ë“¤ì„ í•œë°ë¡œ ë¬¶ì–´ì£¼ì„¸ìš”:\n-\n-```py\n->>> callbacks = [metric_callback, push_to_hub_callback]\n-```\n-\n-ë“œë””ì–´ ëª¨ë¸ì„ í›ˆë ¨ì‹œí‚¬ ëª¨ë“  ì¤€ë¹„ë¥¼ ë§ˆì³¤êµ°ìš”! ì´ì œ í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ì„¸íŠ¸ì— [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) ë©”ì„œë“œë¥¼ ì—í­ ìˆ˜ì™€ ë§Œë“¤ì–´ë‘” ì½œë°±ê³¼ í•¨ê»˜ í˜¸ì¶œí•˜ì—¬ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ì„¸ìš”:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=callbacks)\n-```\n-\n-í•™ìŠµì´ ì™„ë£Œë˜ë©´ ëª¨ë¸ì´ ìë™ìœ¼ë¡œ Hubì— ì—…ë¡œë“œë˜ê³ , ëˆ„êµ¬ë‚˜ ì‚¬ìš©í•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤!\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -378,30 +287,4 @@ TensorFlowì—ì„œ ëª¨ë¸ì„ íŒŒì¸íŠœë‹í•˜ë ¤ë©´ ìš°ì„  optimizer í•¨ìˆ˜, í•™ìŠµ\n 'Les lignÃ©es partagent des ressources avec des bactÃ©ries enfixant l'azote.'\n ```\n </pt>\n-<tf>\n-í…ìŠ¤íŠ¸ë¥¼ í† í°í™”í•˜ê³  `input_ids`ë¥¼ TensorFlow í…ì„œë¡œ ë°˜í™˜í•˜ì„¸ìš”:\n-\n-```py\n->>> from transformers import AutoTokenizer\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(\"my_awesome_opus_books_model\")\n->>> inputs = tokenizer(text, return_tensors=\"tf\").input_ids\n-```\n-\n-[`~transformers.generation_tf_utils.TFGenerationMixin.generate`] ë©”ì„œë“œë¡œ ë²ˆì—­ì„ ìƒì„±í•˜ì„¸ìš”. ë‹¤ì–‘í•œ í…ìŠ¤íŠ¸ ìƒì„± ì „ëµ ë° ìƒì„±ì„ ì œì–´í•˜ê¸° ìœ„í•œ ë§¤ê°œë³€ìˆ˜ì— ëŒ€í•œ ìì„¸í•œ ë‚´ìš©ì€ [Text Generation](../main_classes/text_generation) APIë¥¼ ì‚´í´ë³´ì‹œê¸° ë°”ëë‹ˆë‹¤.\n-\n-```py\n->>> from transformers import TFAutoModelForSeq2SeqLM\n-\n->>> model = TFAutoModelForSeq2SeqLM.from_pretrained(\"my_awesome_opus_books_model\")\n->>> outputs = model.generate(inputs, max_new_tokens=40, do_sample=True, top_k=30, top_p=0.95)\n-```\n-\n-ìƒì„±ëœ í† í° IDë“¤ì„ ë‹¤ì‹œ í…ìŠ¤íŠ¸ë¡œ ë””ì½”ë”©í•˜ì„¸ìš”:\n-\n-```py\n->>> tokenizer.decode(outputs[0], skip_special_tokens=True)\n-'Les lugumes partagent les ressources avec des bactÃ©ries fixatrices d'azote.'\n-```\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "637a96458284fa53323432d815b4e8093b93f10c",
            "filename": "docs/source/ko/training.md",
            "status": "modified",
            "additions": 0,
            "deletions": 109,
            "changes": 109,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftraining.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fko%2Ftraining.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftraining.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -157,115 +157,6 @@ rendered properly in your Markdown viewer.\n >>> trainer.train()\n ```\n </pt>\n-<tf>\n-<a id='keras'></a>\n-\n-<Youtube id=\"rnTGBy2ax1c\"/>\n-\n-## Kerasë¡œ í…ì„œí”Œë¡œìš° ëª¨ë¸ í›ˆë ¨í•˜ê¸°[[train-a-tensorflow-model-with-keras]]\n-\n-Keras APIë¥¼ ì‚¬ìš©í•˜ì—¬ í…ì„œí”Œë¡œìš°ì—ì„œ ğŸ¤— Transformers ëª¨ë¸ì„ í›ˆë ¨í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤!\n-\n-### Kerasìš© ë°ì´í„° ë¡œë“œ[[loading-data-for-keras]]\n-\n-Keras APIë¡œ ğŸ¤— Transformers ëª¨ë¸ì„ í•™ìŠµì‹œí‚¤ë ¤ë©´ ë°ì´í„°ì…‹ì„ Kerasê°€ ì´í•´í•  ìˆ˜ ìˆëŠ” í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•´ì•¼ í•©ë‹ˆë‹¤.\n-ë°ì´í„° ì„¸íŠ¸ê°€ ì‘ì€ ê²½ìš°, ì „ì²´ë¥¼ NumPy ë°°ì—´ë¡œ ë³€í™˜í•˜ì—¬ Kerasë¡œ ì „ë‹¬í•˜ë©´ ë©ë‹ˆë‹¤.\n-ë” ë³µì¡í•œ ì‘ì—…ì„ ìˆ˜í–‰í•˜ê¸° ì „ì— ë¨¼ì € ì´ ì‘ì—…ì„ ì‹œë„í•´ ë³´ê² ìŠµë‹ˆë‹¤.\n-\n-ë¨¼ì € ë°ì´í„° ì„¸íŠ¸ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤. [GLUE ë²¤ì¹˜ë§ˆí¬](https://huggingface.co/datasets/glue)ì˜ CoLA ë°ì´í„° ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ê² ìŠµë‹ˆë‹¤.\n-ê°„ë‹¨í•œ ë°”ì´ë„ˆë¦¬ í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—…ì´ë¯€ë¡œ ì§€ê¸ˆì€ í›ˆë ¨ ë°ì´í„° ë¶„í• ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.\n-\n-```py\n-from datasets import load_dataset\n-\n-dataset = load_dataset(\"glue\", \"cola\")\n-dataset = dataset[\"train\"]  # Just take the training split for now\n-```\n-\n-ë‹¤ìŒìœ¼ë¡œ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•˜ê³  ë°ì´í„°ë¥¼ NumPy ë°°ì—´ë¡œ í† í°í™”í•©ë‹ˆë‹¤. ë ˆì´ë¸”ì€ ì´ë¯¸ 0ê³¼ 1ë¡œ ëœ ë¦¬ìŠ¤íŠ¸ì´ê¸° ë•Œë¬¸ì— í† í°í™”í•˜ì§€ ì•Šê³  ë°”ë¡œ NumPy ë°°ì—´ë¡œ ë³€í™˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n-\n-```py\n-from transformers import AutoTokenizer\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-tokenized_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n-# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n-tokenized_data = dict(tokenized_data)\n-\n-labels = np.array(dataset[\"label\"])  # Label is already an array of 0 and 1\n-```\n-\n-ë§ˆì§€ë§‰ìœ¼ë¡œ ëª¨ë¸ì„ ë¡œë“œ, [`compile`](https://keras.io/api/models/model_training_apis/#compile-method), [`fit`](https://keras.io/api/models/model_training_apis/#fit-method)í•©ë‹ˆë‹¤:\n-\n-```py\n-from transformers import TFAutoModelForSequenceClassification\n-from tensorflow.keras.optimizers import Adam\n-\n-# Load and compile our model\n-model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n-# Lower learning rates are often better for fine-tuning transformers\n-model.compile(optimizer=Adam(3e-5))\n-\n-model.fit(tokenized_data, labels)\n-```\n-\n-<Tip>\n-\n-ëª¨ë¸ì„ `compile()`í•  ë•Œ ì†ì‹¤ ì¸ìˆ˜ë¥¼ ëª¨ë¸ì— ì „ë‹¬í•  í•„ìš”ê°€ ì—†ìŠµë‹ˆë‹¤! \n-ì´ ì¸ìˆ˜ë¥¼ ë¹„ì›Œë‘ë©´ í—ˆê¹… í˜ì´ìŠ¤ ëª¨ë¸ì€ ì‘ì—…ê³¼ ëª¨ë¸ ì•„í‚¤í…ì²˜ì— ì í•©í•œ ì†ì‹¤ì„ ìë™ìœ¼ë¡œ ì„ íƒí•©ë‹ˆë‹¤. \n-ì›í•œë‹¤ë©´ ì–¸ì œë“ ì§€ ì§ì ‘ ì†ì‹¤ì„ ì§€ì •í•˜ì—¬ ì´ë¥¼ ì¬ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤!\n-\n-</Tip>\n-\n-ì´ ì ‘ê·¼ ë°©ì‹ì€ ì†Œê·œëª¨ ë°ì´í„° ì§‘í•©ì—ì„œëŠ” ì˜ ì‘ë™í•˜ì§€ë§Œ, ëŒ€ê·œëª¨ ë°ì´í„° ì§‘í•©ì—ì„œëŠ” ë¬¸ì œê°€ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™œ ê·¸ëŸ´ê¹Œìš”?\n-í† í°í™”ëœ ë°°ì—´ê³¼ ë ˆì´ë¸”ì„ ë©”ëª¨ë¦¬ì— ì™„ì „íˆ ë¡œë“œí•˜ê³  NumPyëŠ” \"ë“¤ì­‰ë‚ ì­‰í•œ\" ë°°ì—´ì„ ì²˜ë¦¬í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì—,\n-ëª¨ë“  í† í°í™”ëœ ìƒ˜í”Œì„ ì „ì²´ ë°ì´í„°ì…‹ì—ì„œ ê°€ì¥ ê¸´ ìƒ˜í”Œì˜ ê¸¸ì´ë§Œí¼ íŒ¨ë”©í•´ì•¼ í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ í•˜ë©´ ë°°ì—´ì´ í›¨ì”¬ ë” ì»¤ì§€ê³  ì´ íŒ¨ë”© í† í°ìœ¼ë¡œ ì¸í•´ í•™ìŠµ ì†ë„ë„ ëŠë ¤ì§‘ë‹ˆë‹¤!\n-\n-### ë°ì´í„°ë¥¼ tf.data.Datasetìœ¼ë¡œ ë¡œë“œí•˜ê¸°[[loading-data-as-a-tfdatadataset]]\n-\n-í•™ìŠµ ì†ë„ê°€ ëŠë ¤ì§€ëŠ” ê²ƒì„ í”¼í•˜ë ¤ë©´ ë°ì´í„°ë¥¼ `tf.data.Dataset`ìœ¼ë¡œ ë¡œë“œí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì›í•œë‹¤ë©´ ì§ì ‘\n-`tf.data` íŒŒì´í”„ë¼ì¸ì„ ì§ì ‘ ì‘ì„±í•  ìˆ˜ë„ ìˆì§€ë§Œ, ì´ ì‘ì—…ì„ ê°„í¸í•˜ê²Œ ìˆ˜í–‰í•˜ëŠ” ìˆ˜ ìˆëŠ” ë‘ ê°€ì§€ ë°©ë²•ì´ ìˆìŠµë‹ˆë‹¤:\n-\n-- [`~TFPreTrainedModel.prepare_tf_dataset`]: ëŒ€ë¶€ë¶„ì˜ ê²½ìš° ì´ ë°©ë²•ì„ ê¶Œì¥í•©ë‹ˆë‹¤. ëª¨ë¸ì˜ ë©”ì„œë“œì´ê¸° ë•Œë¬¸ì— ëª¨ë¸ì„ ê²€ì‚¬í•˜ì—¬ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì—´ì„ ìë™ìœ¼ë¡œ íŒŒì•…í•˜ê³ \n-ë‚˜ë¨¸ì§€ëŠ” ë²„ë ¤ì„œ ë” ë‹¨ìˆœí•˜ê³  ì„±ëŠ¥ì´ ì¢‹ì€ ë°ì´í„° ì§‘í•©ì„ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-- [`~datasets.Dataset.to_tf_dataset`]: ì´ ë°©ë²•ì€ ì¢€ ë” ë‚®ì€ ìˆ˜ì¤€ì´ë©°, í¬í•¨í•  'ì—´'ê³¼ 'ë ˆì´ë¸”'ì„ ì •í™•íˆ ì§€ì •í•˜ì—¬\n-ë°ì´í„°ì…‹ì„ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ì •í™•íˆ ì œì–´í•˜ê³  ì‹¶ì„ ë•Œ ìœ ìš©í•˜ë©°, í¬í•¨í•  'columns'ê³¼ 'label_cols'ì„ ì •í™•íˆ ì§€ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-[`~TFPreTrainedModel.prepare_tf_dataset`]ì„ ì‚¬ìš©í•˜ë ¤ë©´ ë¨¼ì € ë‹¤ìŒ ì½”ë“œ ìƒ˜í”Œê³¼ ê°™ì´ í† í¬ë‚˜ì´ì € ì¶œë ¥ì„ ë°ì´í„° ì„¸íŠ¸ì— ì—´ë¡œ ì¶”ê°€í•´ì•¼ í•©ë‹ˆë‹¤:\n-\n-```py\n-def tokenize_dataset(data):\n-    # Keys of the returned dictionary will be added to the dataset as columns\n-    return tokenizer(data[\"text\"])\n-\n-\n-dataset = dataset.map(tokenize_dataset)\n-```\n-\n-í—ˆê¹… í˜ì´ìŠ¤ ë°ì´í„°ì…‹ì€ ê¸°ë³¸ì ìœ¼ë¡œ ë””ìŠ¤í¬ì— ì €ì¥ë˜ë¯€ë¡œ ë©”ëª¨ë¦¬ ì‚¬ìš©ëŸ‰ì„ ëŠ˜ë¦¬ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì„ ê¸°ì–µí•˜ì„¸ìš”! \n-ì—´ì´ ì¶”ê°€ë˜ë©´ ë°ì´í„°ì…‹ì—ì„œ ë°°ì¹˜ë¥¼ ìŠ¤íŠ¸ë¦¬ë°í•˜ê³  ê° ë°°ì¹˜ì— íŒ¨ë”©ì„ ì¶”ê°€í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ì „ì²´ ë°ì´í„°ì…‹ì— íŒ¨ë”©ì„ ì¶”ê°€í•˜ëŠ” ê²ƒë³´ë‹¤ íŒ¨ë”© í† í°ì˜ ìˆ˜ë¥¼ í¬ê²Œ ì¤„ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-\n-\n-```py\n->>> tf_dataset = model.prepare_tf_dataset(dataset, batch_size=16, shuffle=True, tokenizer=tokenizer)\n-```\n-\n-ìœ„ì˜ ì½”ë“œ ìƒ˜í”Œì—ì„œëŠ” ë°°ì¹˜ê°€ ë¡œë“œë  ë•Œ ì˜¬ë°”ë¥´ê²Œ íŒ¨ë”©í•  ìˆ˜ ìˆë„ë¡ `prepare_tf_dataset`ì— í† í¬ë‚˜ì´ì €ë¥¼ ì „ë‹¬í•´ì•¼ í•©ë‹ˆë‹¤.\n-ë°ì´í„°ì…‹ì˜ ëª¨ë“  ìƒ˜í”Œ ê¸¸ì´ê°€ ê°™ê³  íŒ¨ë”©ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš° ì´ ì¸ìˆ˜ë¥¼ ê±´ë„ˆë›¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n-ìƒ˜í”Œì„ ì±„ìš°ëŠ” ê²ƒë³´ë‹¤ ë” ë³µì¡í•œ ì‘ì—…(ì˜ˆ: ë§ˆìŠ¤í‚¹ëœ ì–¸ì–´ì˜ í† í° ì†ìƒ ëª¨ë¸ë§)ì„ ìˆ˜í–‰í•˜ê¸° ìœ„í•´ í† í°ì„ ì†ìƒì‹œì¼œì•¼ í•˜ëŠ” ê²½ìš°, \n-`collate_fn` ì¸ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ìƒ˜í”Œ ëª©ë¡ì„ ë°°ì¹˜ë¡œ ë³€í™˜í•˜ê³  ì›í•˜ëŠ” ì „ì²˜ë¦¬ë¥¼ ì ìš©í•  í•¨ìˆ˜ë¥¼ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n-[ì˜ˆì‹œ](https://github.com/huggingface/transformers/tree/main/examples) ë˜ëŠ” \n-[ë…¸íŠ¸ë¶](https://huggingface.co/docs/transformers/notebooks)ì„ ì°¸ì¡°í•˜ì—¬ ì´ ì ‘ê·¼ ë°©ì‹ì´ ì‹¤ì œë¡œ ì‘ë™í•˜ëŠ” ëª¨ìŠµì„ í™•ì¸í•˜ì„¸ìš”.\n-\n-`tf.data.Dataset`ì„ ìƒì„±í•œ í›„ì—ëŠ” ì´ì „ê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ëª¨ë¸ì„ ì»´íŒŒì¼í•˜ê³  í›ˆë ¨(fit)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:\n-\n-```py\n-model.compile(optimizer=Adam(3e-5))\n-\n-model.fit(tf_dataset)\n-```\n-\n-</tf>\n </frameworkcontent>\n \n <a id='pytorch_native'></a>"
        },
        {
            "sha": "c0736f72771bd428c575b2f9f919d45ef13eb4d9",
            "filename": "docs/source/pt/create_a_model.md",
            "status": "modified",
            "additions": 1,
            "deletions": 42,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fpt%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fpt%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Fcreate_a_model.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -136,30 +136,6 @@ Quando vocÃª carregar os pesos prÃ©-treinados, a configuraÃ§Ã£o padrÃ£o do model\n >>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n ```\n </pt>\n-<tf>\n-Carregar os seus prÃ³prios atributos padrÃµes de contiguraÃ§Ã£o no modelo:\n-\n-```py\n->>> from transformers import TFDistilBertModel\n-\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")\n->>> tf_model = TFDistilBertModel(my_config)\n-```\n-\n-Isso cria um modelo com valores aleatÃ³rios ao invÃ©s de prÃ©-treinar os pesos. VocÃª nÃ£o irÃ¡ conseguir usar usar esse modelo para nada Ãºtil ainda, atÃ© vocÃª treinar ele. Treino Ã© um processo caro e demorado. Geralmente Ã© melhor utilizar um modelo prÃ©-treinado para obter melhores resultados mais rÃ¡pido, enquanto usa apenas uma fraÃ§Ã£o dos recursos necessÃ¡rios para treinar.\n-\n-Criar um modelo prÃ©-treinado com [`~TFPreTrainedModel.from_pretrained`]:\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Quando vocÃª carregar os pesos prÃ©-treinados, a configuraÃ§Ã£o padrÃ£o do modelo Ã© automaticamente carregada se o modelo Ã© provido pelo ğŸ¤— Transformers. No entanto, vocÃª ainda consegue mudar - alguns ou todos - os atributos padrÃµes de configuraÃ§Ã£o do modelo com os seus prÃ³prio atributos, se vocÃª preferir: \n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n-```\n-</tf>\n </frameworkcontent>\n \n ### Heads do modelo\n@@ -184,23 +160,6 @@ Reutilize facilmente esse ponto de parada para outra tarefe mudando para uma hea\n >>> model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n ```\n </pt>\n-<tf>\n-Por exemplo, [`TFDistilBertForSequenceClassification`] Ã© um modelo DistilBERT base com uma head de classificaÃ§Ã£o de sequÃªncia. A head de calssificaÃ§Ã£o de sequÃªncia Ã© uma camada linear no topo das saÃ­das agrupadas.\n-\n-```py\n->>> from transformers import TFDistilBertForSequenceClassification\n-\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-Reutilize facilmente esse ponto de parada para outra tarefe mudando para uma head de modelo diferente. Para uma tarefe de responder questÃµes, vocÃª usaria a head do modelo [`TFDistilBertForQuestionAnswering`]. A head de responder questÃµes Ã© similar com a de classificaÃ§Ã£o de sequÃªncias exceto o fato de que ela Ã© uma camada no topo dos estados das saÃ­das ocultas.\n-\n-```py\n->>> from transformers import TFDistilBertForQuestionAnswering\n-\n->>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Tokenizer\n@@ -356,4 +315,4 @@ Combine o extrator de features e o tokenizer no [`Wav2Vec2Processor`]:\n >>> processor = Wav2Vec2Processor(feature_extractor=feature_extractor, tokenizer=tokenizer)\n ```\n \n-Com duas classes bÃ¡sicas - configuraÃ§Ã£o e modelo - e um preprocessamento de classe adicional (tokenizer, extrator de features, ou processador), vocÃª pode criar qualquer modelo que suportado por ğŸ¤— Transformers. Qualquer uma dessas classes base sÃ£o configurÃ¡veis, te permitindo usar os atributos especÃ­ficos que vocÃª queira. VocÃª pode facilmente preparar um modelo para treinamento ou modificar um modelo prÃ©-treinado com poucas mudanÃ§as.\n\\ No newline at end of file\n+Com duas classes bÃ¡sicas - configuraÃ§Ã£o e modelo - e um preprocessamento de classe adicional (tokenizer, extrator de features, ou processador), vocÃª pode criar qualquer modelo que suportado por ğŸ¤— Transformers. Qualquer uma dessas classes base sÃ£o configurÃ¡veis, te permitindo usar os atributos especÃ­ficos que vocÃª queira. VocÃª pode facilmente preparar um modelo para treinamento ou modificar um modelo prÃ©-treinado com poucas mudanÃ§as."
        },
        {
            "sha": "1704c0fb2c7c45cbc5cf649035a237c8ee667618",
            "filename": "docs/source/pt/quicktour.md",
            "status": "modified",
            "additions": 1,
            "deletions": 84,
            "changes": 85,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fpt%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fpt%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Fquicktour.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -72,11 +72,6 @@ Instale as seguintes dependÃªncias se vocÃª ainda nÃ£o o fez:\n pip install torch\n ```\n </pt>\n-<tf>\n-```bash\n-pip install tensorflow\n-```\n-</tf>\n </frameworkcontent>\n \n Importe [`pipeline`] e especifique a tarefa que deseja completar:\n@@ -163,17 +158,6 @@ Use o [`AutoModelForSequenceClassification`] e [`AutoTokenizer`] para carregar o\n >>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n ```\n </pt>\n-<tf>\n-\n-Use o [`TFAutoModelForSequenceClassification`] and [`AutoTokenizer`] para carregar o modelo prÃ©-treinado e o tokenizer associado (mais em `TFAutoClass` abaixo):\n-\n-```py\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n-```\n-</tf>\n </frameworkcontent>\n \n EntÃ£o vocÃª pode especificar o modelo e o tokenizador na [`pipeline`] e aplicar o `classifier` no seu texto alvo:\n@@ -239,18 +223,6 @@ Assim como o [`pipeline`], o tokenizer aceitarÃ¡ uma lista de entradas. AlÃ©m di\n ... )\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> tf_batch = tokenizer(\n-...     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n-...     padding=True,\n-...     truncation=True,\n-...     max_length=512,\n-...     return_tensors=\"tf\",\n-... )\n-```\n-</tf>\n </frameworkcontent>\n \n Leia o tutorial de [prÃ©-processamento](./prÃ©-processamento) para obter mais detalhes sobre tokenizaÃ§Ã£o.\n@@ -291,37 +263,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n         [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n ```\n </pt>\n-<tf>\n-ğŸ¤— Transformers fornecem uma maneira simples e unificada de carregar instÃ¢ncias prÃ©-treinadas. Isso significa que vocÃª pode carregar um [`TFAutoModel`] como carregaria um [`AutoTokenizer`]. A Ãºnica diferenÃ§a Ã© selecionar o [`TFAutoModel`] correto para a tarefa. Como vocÃª estÃ¡ fazendo classificaÃ§Ã£o de texto ou sequÃªncia, carregue [`TFAutoModelForSequenceClassification`]:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n-```\n-\n-<Tip>\n-\n-Veja o [sumÃ¡rio de tarefas](./task_summary) para qual classe de [`AutoModel`] usar para cada tarefa.\n-\n-</Tip>\n-\n-Agora vocÃª pode passar seu grupo de entradas prÃ©-processadas diretamente para o modelo atravÃ©s da passagem de chaves de dicionÃ¡rios ao tensor.\n-\n-```py\n->>> tf_outputs = tf_model(tf_batch)\n-```\n-\n-O modelo gera as ativaÃ§Ãµes finais no atributo `logits`. Aplique a funÃ§Ã£o softmax aos `logits` para recuperar as probabilidades:\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n->>> tf_predictions  # doctest: +IGNORE_RESULT\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -358,21 +299,6 @@ Quando vocÃª estiver pronto para usÃ¡-lo novamente, recarregue com [`PreTrainedM\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")\n ```\n </pt>\n-<tf>\n-Uma vez que seu modelo estiver afinado, vocÃª pode salvÃ¡-lo com seu Tokenizer usando [`TFPreTrainedModel.save_pretrained`]:\n-\n-```py\n->>> tf_save_directory = \"./tf_save_pretrained\"\n->>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT\n->>> tf_model.save_pretrained(tf_save_directory)\n-```\n-\n-Quando vocÃª estiver pronto para usÃ¡-lo novamente, recarregue com [`TFPreTrainedModel.from_pretrained`]\n-\n-```py\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n-```\n-</tf>\n </frameworkcontent>\n \n Um recurso particularmente interessante dos ğŸ¤— Transformers Ã© a capacidade de salvar um modelo e recarregÃ¡-lo como um modelo PyTorch ou TensorFlow. Use `from_pt` ou `from_tf` para converter o modelo de um framework para outro:\n@@ -387,13 +313,4 @@ Um recurso particularmente interessante dos ğŸ¤— Transformers Ã© a capacidade de\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n-```\n-</tf>\n-</frameworkcontent>\n\\ No newline at end of file\n+</frameworkcontent>"
        },
        {
            "sha": "32182497a366c8cefaefb7aaabf0f19420585811",
            "filename": "docs/source/pt/run_scripts.md",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fpt%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fpt%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Frun_scripts.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -105,22 +105,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-Este outro script de exemplo baixa e prÃ©-processa um conjunto de dados da biblioteca ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/). Em seguida, o script ajusta um conjunto de dados usando Keras em uma arquitetura que oferece suporte Ã  sumarizaÃ§Ã£o. O exemplo a seguir mostra como ajustar [T5-small](https://huggingface.co/google-t5/t5-small) no conjunto de dados [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail). O modelo T5 requer um argumento `source_prefix` adicional devido Ã  forma como foi treinado. Este prompt informa ao T5 que esta Ã© uma tarefa de sumarizaÃ§Ã£o.\n-\n-```bash\n-python examples/tensorflow/summarization/run_summarization.py  \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## Treinamento distribuÃ­do e precisÃ£o mista\n@@ -171,24 +155,6 @@ python xla_spawn.py --num_cores 8 \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-\n-As Unidades de Processamento de Tensor (TPUs) sÃ£o projetadas especificamente para acelerar o desempenho. Os scripts do TensorFlow utilizam uma [`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy) para treinamento em TPUs. Para usar uma TPU, passe o nome do recurso TPU para o argumento `tpu`.\n-\n-```bash\n-python run_summarization.py  \\\n-    --tpu name_of_tpu_resource \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## Execute um script com ğŸ¤— Accelerate"
        },
        {
            "sha": "b608511277576e024f001b33d88795cdba63456b",
            "filename": "docs/source/pt/tasks/sequence_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 67,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fpt%2Ftasks%2Fsequence_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fpt%2Ftasks%2Fsequence_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Ftasks%2Fsequence_classification.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -86,13 +86,6 @@ Use o [`DataCollatorWithPadding`] para criar um batch de exemplos. Ele tambÃ©m *\n >>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n ```\n </pt>\n-<tf>\n-```py\n->>> from transformers import DataCollatorWithPadding\n-\n->>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Train\n@@ -147,66 +140,6 @@ O [`Trainer`] aplicarÃ¡ o preenchimento dinÃ¢mico por padrÃ£o quando vocÃª defin\n \n </Tip>\n </pt>\n-<tf>\n-Para executar o fine-tuning de um modelo no TensorFlow, comece convertendo seu conjunto de dados para o formato `tf.data.Dataset` com [`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset). Nessa execuÃ§Ã£o vocÃª deverÃ¡ especificar as entradas e rÃ³tulos (no parÃ¢metro `columns`), se deseja embaralhar o conjunto de dados, o tamanho do batch e o data collator:\n-\n-```py\n->>> tf_train_set = tokenized_imdb[\"train\"].to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"label\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = tokenized_imdb[\"test\"].to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"label\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-<Tip>\n-\n-Se vocÃª nÃ£o estiver familiarizado com o fine-tuning de um modelo com o Keras, dÃª uma olhada no tutorial bÃ¡sico [aqui](training#finetune-with-keras)!\n-\n-</Tip>\n-\n-Configure o otimizador e alguns hiperparÃ¢metros de treinamento:\n-\n-```py\n->>> from transformers import create_optimizer\n->>> import tensorflow as tf\n-\n->>> batch_size = 16\n->>> num_epochs = 5\n->>> batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n->>> total_train_steps = int(batches_per_epoch * num_epochs)\n->>> optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n-```\n-\n-Carregue o DistilBERT com [`TFAutoModelForSequenceClassification`] junto com o nÃºmero de rÃ³tulos esperados:\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2)\n-```\n-\n-Configure o modelo para treinamento com o mÃ©todo [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-Chame o mÃ©todo [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) para executar o fine-tuning do modelo:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>"
        },
        {
            "sha": "d314caf157f732b5c8896d9028d1e649b6064777",
            "filename": "docs/source/pt/tasks/token_classification.md",
            "status": "modified",
            "additions": 0,
            "deletions": 70,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fpt%2Ftasks%2Ftoken_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fpt%2Ftasks%2Ftoken_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fpt%2Ftasks%2Ftoken_classification.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -144,13 +144,6 @@ Use o [`DataCollatorForTokenClassification`] para criar um batch de exemplos. El\n >>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)\n ```\n </pt>\n-<tf>\n-```py\n->>> from transformers import DataCollatorForTokenClassification\n-\n->>> data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer, return_tensors=\"tf\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## Treinamento\n@@ -200,69 +193,6 @@ Nesse ponto, restam apenas trÃªs passos:\n >>> trainer.train()\n ```\n </pt>\n-<tf>\n-Para executar o fine-tuning de um modelo no TensorFlow, comece convertendo seu conjunto de dados para o formato `tf.data.Dataset` com [`to_tf_dataset`](https://huggingface.co/docs/datasets/package_reference/main_classes#datasets.Dataset.to_tf_dataset). Nessa execuÃ§Ã£o vocÃª deverÃ¡ especificar as entradas e rÃ³tulos (no parÃ¢metro `columns`), se deseja embaralhar o conjunto de dados, o tamanho do batch e o data collator:\n-\n-```py\n->>> tf_train_set = tokenized_wnut[\"train\"].to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n-...     shuffle=True,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-\n->>> tf_validation_set = tokenized_wnut[\"validation\"].to_tf_dataset(\n-...     columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n-...     shuffle=False,\n-...     batch_size=16,\n-...     collate_fn=data_collator,\n-... )\n-```\n-\n-<Tip>\n-\n-Se vocÃª nÃ£o estiver familiarizado com o fine-tuning de um modelo com o Keras, dÃª uma olhada no tutorial bÃ¡sico [aqui](training#finetune-with-keras)!\n-\n-</Tip>\n-\n-Configure o otimizador e alguns hiperparÃ¢metros de treinamento:\n-\n-```py\n->>> from transformers import create_optimizer\n-\n->>> batch_size = 16\n->>> num_train_epochs = 3\n->>> num_train_steps = (len(tokenized_wnut[\"train\"]) // batch_size) * num_train_epochs\n->>> optimizer, lr_schedule = create_optimizer(\n-...     init_lr=2e-5,\n-...     num_train_steps=num_train_steps,\n-...     weight_decay_rate=0.01,\n-...     num_warmup_steps=0,\n-... )\n-```\n-\n-Carregue o DistilBERT com o [`TFAutoModelForTokenClassification`] junto com o nÃºmero de rÃ³tulos esperados:\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\", num_labels=2)\n-```\n-\n-Configure o modelo para treinamento com o mÃ©todo [`compile`](https://keras.io/api/models/model_training_apis/#compile-method):\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> model.compile(optimizer=optimizer)\n-```\n-\n-Chame o mÃ©todo [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) para executar o fine-tuning do modelo:\n-\n-```py\n->>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3)\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>"
        },
        {
            "sha": "95fb1783a864de56179ea8e049ee82fc710c9d74",
            "filename": "docs/source/zh/autoclass_tutorial.md",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Fautoclass_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Fautoclass_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fautoclass_tutorial.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -127,23 +127,4 @@ TensorFlowå’ŒFlaxçš„checkpointsä¸å—å½±å“ï¼Œå¹¶ä¸”å¯ä»¥åœ¨PyTorchæ¶æ„ä¸­ä½¿\n ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬å»ºè®®ä½¿ç”¨`AutoTokenizer`ç±»å’Œ`AutoModelFor`ç±»æ¥åŠ è½½é¢„è®­ç»ƒçš„æ¨¡å‹å®ä¾‹ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿æ¯æ¬¡åŠ è½½æ­£ç¡®çš„æ¶æ„ã€‚åœ¨ä¸‹ä¸€ä¸ª[æ•™ç¨‹](preprocessing)ä¸­ï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨æ–°åŠ è½½çš„`tokenizer`, `image processor`, `feature extractor`å’Œ`processor`å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ä»¥è¿›è¡Œå¾®è°ƒã€‚\n \n </pt>\n-<tf>\n-æœ€åï¼Œ`TFAutoModelFor`ç±»å…è®¸æ‚¨åŠ è½½ç»™å®šä»»åŠ¡çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆè¯·å‚é˜…[è¿™é‡Œ](model_doc/auto)è·å–å¯ç”¨ä»»åŠ¡çš„å®Œæ•´åˆ—è¡¨ï¼‰ã€‚ä¾‹å¦‚ï¼Œä½¿ç”¨[`TFAutoModelForSequenceClassification.from_pretrained`]åŠ è½½ç”¨äºåºåˆ—åˆ†ç±»çš„æ¨¡å‹ï¼š\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-è½»æ¾åœ°é‡å¤ä½¿ç”¨ç›¸åŒçš„checkpointæ¥ä¸ºä¸åŒä»»åŠ¡åŠ è½½æ¨¡å‹æ¶æ„ï¼š\n-\n-```py\n->>> from transformers import TFAutoModelForTokenClassification\n-\n->>> model = TFAutoModelForTokenClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-ä¸€èˆ¬æ¥è¯´ï¼Œæˆ‘ä»¬æ¨èä½¿ç”¨`AutoTokenizer`ç±»å’Œ`TFAutoModelFor`ç±»æ¥åŠ è½½æ¨¡å‹çš„é¢„è®­ç»ƒå®ä¾‹ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿æ¯æ¬¡åŠ è½½æ­£ç¡®çš„æ¶æ„ã€‚åœ¨ä¸‹ä¸€ä¸ª[æ•™ç¨‹](preprocessing)ä¸­ï¼Œå­¦ä¹ å¦‚ä½•ä½¿ç”¨æ–°åŠ è½½çš„`tokenizer`, `image processor`, `feature extractor`å’Œ`processor`å¯¹æ•°æ®é›†è¿›è¡Œé¢„å¤„ç†ä»¥è¿›è¡Œå¾®è°ƒã€‚\n-\n-</tf>\n </frameworkcontent>"
        },
        {
            "sha": "862842f15db6d019de72e0ae2541ec690514203c",
            "filename": "docs/source/zh/create_a_model.md",
            "status": "modified",
            "additions": 0,
            "deletions": 41,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Fcreate_a_model.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Fcreate_a_model.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fcreate_a_model.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -137,30 +137,6 @@ DistilBertConfig {\n >>> model = DistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n ```\n </pt>\n-<tf>\n-å°†è‡ªå®šä¹‰é…ç½®å±æ€§åŠ è½½åˆ°æ¨¡å‹ä¸­ï¼š\n-\n-```py\n->>> from transformers import TFDistilBertModel\n-\n->>> my_config = DistilBertConfig.from_pretrained(\"./your_model_save_path/my_config.json\")\n->>> tf_model = TFDistilBertModel(my_config)\n-```\n-\n-è¿™æ®µä»£ç åˆ›å»ºäº†ä¸€ä¸ªå…·æœ‰éšæœºå‚æ•°è€Œä¸æ˜¯é¢„è®­ç»ƒæƒé‡çš„æ¨¡å‹ã€‚åœ¨è®­ç»ƒè¯¥æ¨¡å‹ä¹‹å‰ï¼Œæ‚¨è¿˜æ— æ³•å°†è¯¥æ¨¡å‹ç”¨äºä»»ä½•ç”¨é€”ã€‚è®­ç»ƒæ˜¯ä¸€é¡¹æ˜‚è´µä¸”è€—æ—¶çš„è¿‡ç¨‹ã€‚é€šå¸¸æ¥è¯´ï¼Œæœ€å¥½ä½¿ç”¨é¢„è®­ç»ƒæ¨¡å‹æ¥æ›´å¿«åœ°è·å¾—æ›´å¥½çš„ç»“æœï¼ŒåŒæ—¶ä»…ä½¿ç”¨è®­ç»ƒæ‰€éœ€èµ„æºçš„ä¸€å°éƒ¨åˆ†ã€‚\n-\n-ä½¿ç”¨ [`~TFPreTrainedModel.from_pretrained`] åˆ›å»ºé¢„è®­ç»ƒæ¨¡å‹ï¼š\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-å½“åŠ è½½é¢„è®­ç»ƒæƒé‡æ—¶ï¼Œå¦‚æœæ¨¡å‹æ˜¯ç”± ğŸ¤— Transformers æä¾›çš„ï¼Œå°†è‡ªåŠ¨åŠ è½½é»˜è®¤æ¨¡å‹é…ç½®ã€‚ç„¶è€Œï¼Œå¦‚æœä½ æ„¿æ„ï¼Œä»ç„¶å¯ä»¥å°†é»˜è®¤æ¨¡å‹é…ç½®çš„æŸäº›æˆ–è€…æ‰€æœ‰å±æ€§æ›¿æ¢æˆè‡ªå·±çš„é…ç½®ï¼š\n-\n-```py\n->>> tf_model = TFDistilBertModel.from_pretrained(\"distilbert/distilbert-base-uncased\", config=my_config)\n-```\n-</tf>\n </frameworkcontent>\n \n ### æ¨¡å‹å¤´ï¼ˆModel headsï¼‰\n@@ -185,23 +161,6 @@ DistilBertConfig {\n >>> model = DistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n ```\n </pt>\n-<tf>\n-ä¾‹å¦‚ï¼Œ[`TFDistilBertForSequenceClassification`] æ˜¯ä¸€ä¸ªå¸¦æœ‰åºåˆ—åˆ†ç±»å¤´ï¼ˆsequence classification headï¼‰çš„åŸºç¡€ DistilBERT æ¨¡å‹ã€‚åºåˆ—åˆ†ç±»å¤´æ˜¯æ± åŒ–è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚\n-\n-```py\n->>> from transformers import TFDistilBertForSequenceClassification\n-\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-\n-é€šè¿‡åˆ‡æ¢åˆ°ä¸åŒçš„æ¨¡å‹å¤´,å¯ä»¥è½»æ¾åœ°å°†æ­¤æ£€æŸ¥ç‚¹é‡å¤ç”¨äºå…¶ä»–ä»»åŠ¡ã€‚å¯¹äºé—®ç­”ä»»åŠ¡ï¼Œä½ å¯ä»¥ä½¿ç”¨ [`TFDistilBertForQuestionAnswering`] æ¨¡å‹å¤´ã€‚é—®ç­”å¤´ï¼ˆquestion answering headï¼‰ä¸åºåˆ—åˆ†ç±»å¤´ç±»ä¼¼ï¼Œä¸åŒç‚¹åœ¨äºå®ƒæ˜¯éšè—çŠ¶æ€è¾“å‡ºä¹‹ä¸Šçš„çº¿æ€§å±‚ã€‚\n-\n-```py\n->>> from transformers import TFDistilBertForQuestionAnswering\n-\n->>> tf_model = TFDistilBertForQuestionAnswering.from_pretrained(\"distilbert/distilbert-base-uncased\")\n-```\n-</tf>\n </frameworkcontent>\n \n ## åˆ†è¯å™¨"
        },
        {
            "sha": "ab77665f8411776005b6d61964a61816caea19e6",
            "filename": "docs/source/zh/model_sharing.md",
            "status": "modified",
            "additions": 1,
            "deletions": 48,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Fmodel_sharing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Fmodel_sharing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fmodel_sharing.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -89,30 +89,6 @@ pip install huggingface_hub\n >>> pt_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n ```\n </pt>\n-<tf>\n-\n-æŒ‡å®š`from_pt=True`å°†checkpointä»PyTorchè½¬æ¢ä¸ºTensorFlowã€‚\n-\n-```py\n->>> tf_model = TFDistilBertForSequenceClassification.from_pretrained(\"path/to/awesome-name-you-picked\", from_pt=True)\n-```\n-\n-ç„¶åï¼Œæ‚¨å¯ä»¥ä½¿ç”¨æ–°çš„checkpointä¿å­˜æ‚¨çš„æ–°TensorFlowæ¨¡å‹ï¼š\n-\n-```py\n->>> tf_model.save_pretrained(\"path/to/awesome-name-you-picked\")\n-```\n-</tf>\n-<jax>\n-\n-å¦‚æœæ¨¡å‹åœ¨Flaxä¸­å¯ç”¨ï¼Œæ‚¨è¿˜å¯ä»¥å°†PyTorch checkpointè½¬æ¢ä¸ºFlaxï¼š\n-\n-```py\n->>> flax_model = FlaxDistilBertForSequenceClassification.from_pretrained(\n-...     \"path/to/awesome-name-you-picked\", from_pt=True\n-... )\n-```\n-</jax>\n </frameworkcontent>\n \n ## åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­æ¨é€æ¨¡å‹\n@@ -146,29 +122,6 @@ pip install huggingface_hub\n >>> trainer.push_to_hub()\n ```\n </pt>\n-<tf>\n-\n-ä½¿ç”¨[`PushToHubCallback`]å°†æ¨¡å‹åˆ†äº«åˆ°Hubã€‚åœ¨[`PushToHubCallback`]å‡½æ•°ä¸­ï¼Œæ·»åŠ ä»¥ä¸‹å†…å®¹ï¼š\n-\n-- ä¸€ä¸ªç”¨äºå­˜å‚¨æ¨¡å‹çš„è¾“å‡ºç›®å½•ã€‚\n-- ä¸€ä¸ªtokenizerã€‚\n-- `hub_model_id`ï¼Œå³æ‚¨çš„Hubç”¨æˆ·åå’Œæ¨¡å‹åç§°ã€‚\n-\n-\n-```py\n->>> from transformers import PushToHubCallback\n-\n->>> push_to_hub_callback = PushToHubCallback(\n-...     output_dir=\"./your_model_save_path\", tokenizer=tokenizer, hub_model_id=\"your-username/my-awesome-model\"\n-... )\n-```\n-\n-å°†å›è°ƒå‡½æ•°æ·»åŠ åˆ° [`fit`](https://keras.io/api/models/model_training_apis/)ä¸­ï¼Œç„¶åğŸ¤— Transformers ä¼šå°†è®­ç»ƒå¥½çš„æ¨¡å‹æ¨é€åˆ° Hubï¼š\n-\n-```py\n->>> model.fit(tf_train_dataset, validation_data=tf_validation_dataset, epochs=3, callbacks=push_to_hub_callback)\n-```\n-</tf>\n </frameworkcontent>\n \n ## ä½¿ç”¨`push_to_hub`åŠŸèƒ½\n@@ -235,4 +188,4 @@ pip install huggingface_hub\n * æ‰‹åŠ¨åˆ›å»ºå¹¶ä¸Šä¼ ä¸€ä¸ª`README.md`æ–‡ä»¶ã€‚\n * åœ¨ä½ çš„æ¨¡å‹ä»“åº“ä¸­ç‚¹å‡»**ç¼–è¾‘æ¨¡å‹å¡ç‰‡**æŒ‰é’®ã€‚\n \n-å¯ä»¥å‚è€ƒDistilBertçš„[æ¨¡å‹å¡ç‰‡](https://huggingface.co/distilbert/distilbert-base-uncased)æ¥äº†è§£æ¨¡å‹å¡ç‰‡åº”è¯¥åŒ…å«çš„ä¿¡æ¯ç±»å‹ã€‚æœ‰å…³æ‚¨å¯ä»¥åœ¨`README.md`æ–‡ä»¶ä¸­æ§åˆ¶çš„æ›´å¤šé€‰é¡¹çš„ç»†èŠ‚ï¼Œä¾‹å¦‚æ¨¡å‹çš„ç¢³è¶³è¿¹æˆ–å°éƒ¨ä»¶ç¤ºä¾‹ï¼Œè¯·å‚è€ƒæ–‡æ¡£[è¿™é‡Œ](https://huggingface.co/docs/hub/models-cards)ã€‚\n\\ No newline at end of file\n+å¯ä»¥å‚è€ƒDistilBertçš„[æ¨¡å‹å¡ç‰‡](https://huggingface.co/distilbert/distilbert-base-uncased)æ¥äº†è§£æ¨¡å‹å¡ç‰‡åº”è¯¥åŒ…å«çš„ä¿¡æ¯ç±»å‹ã€‚æœ‰å…³æ‚¨å¯ä»¥åœ¨`README.md`æ–‡ä»¶ä¸­æ§åˆ¶çš„æ›´å¤šé€‰é¡¹çš„ç»†èŠ‚ï¼Œä¾‹å¦‚æ¨¡å‹çš„ç¢³è¶³è¿¹æˆ–å°éƒ¨ä»¶ç¤ºä¾‹ï¼Œè¯·å‚è€ƒæ–‡æ¡£[è¿™é‡Œ](https://huggingface.co/docs/hub/models-cards)ã€‚"
        },
        {
            "sha": "c33fdee980ed400854cf0cb02e61ae4fe89a2135",
            "filename": "docs/source/zh/preprocessing.md",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Fpreprocessing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Fpreprocessing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fpreprocessing.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -196,31 +196,6 @@ pip install datasets\n                            [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]])}\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> batch_sentences = [\n-...     \"But what about second breakfast?\",\n-...     \"Don't think he knows about second breakfast, Pip.\",\n-...     \"What about elevensies?\",\n-... ]\n->>> encoded_input = tokenizer(batch_sentences, padding=True, truncation=True, return_tensors=\"tf\")\n->>> print(encoded_input)\n-{'input_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[101, 1252, 1184, 1164, 1248, 6462, 136, 102, 0, 0, 0, 0, 0, 0, 0],\n-       [101, 1790, 112, 189, 1341, 1119, 3520, 1164, 1248, 6462, 117, 21902, 1643, 119, 102],\n-       [101, 1327, 1164, 5450, 23434, 136, 102, 0, 0, 0, 0, 0, 0, 0, 0]],\n-      dtype=int32)>,\n- 'token_type_ids': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n-       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>,\n- 'attention_mask': <tf.Tensor: shape=(2, 9), dtype=int32, numpy=\n-array([[1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0],\n-       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n-       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n-```\n-</tf>\n </frameworkcontent>\n \n ## éŸ³é¢‘"
        },
        {
            "sha": "e28101fd639356a1d47c404653cb4db616d39c4c",
            "filename": "docs/source/zh/quicktour.md",
            "status": "modified",
            "additions": 0,
            "deletions": 92,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Fquicktour.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Fquicktour.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fquicktour.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -35,12 +35,6 @@ rendered properly in your Markdown viewer.\n pip install torch\n ```\n </pt>\n-<tf>\n-\n-```bash\n-pip install tensorflow\n-```\n-</tf>\n </frameworkcontent>\n \n ## Pipeline\n@@ -143,16 +137,6 @@ label: NEGATIVE, with score: 0.5309\n >>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n ```\n </pt>\n-<tf>\n-ä½¿ç”¨ [`TFAutoModelForSequenceClassification`] å’Œ [`AutoTokenizer`] æ¥åŠ è½½é¢„è®­ç»ƒæ¨¡å‹å’Œå®ƒå…³è”çš„åˆ†è¯å™¨ï¼ˆæ›´å¤šä¿¡æ¯å¯ä»¥å‚è€ƒä¸‹ä¸€èŠ‚çš„ `TFAutoClass`ï¼‰ï¼š\n-\n-```py\n->>> from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n-\n->>> model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n->>> tokenizer = AutoTokenizer.from_pretrained(model_name)\n-```\n-</tf>\n </frameworkcontent>\n \n åœ¨ [`pipeline`] ä¸­æŒ‡å®šæ¨¡å‹å’Œåˆ†è¯å™¨ï¼Œç°åœ¨ä½ å°±å¯ä»¥åœ¨æ³•è¯­æ–‡æœ¬ä¸Šä½¿ç”¨ `classifier` äº†ï¼š\n@@ -216,18 +200,6 @@ label: NEGATIVE, with score: 0.5309\n ... )\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> tf_batch = tokenizer(\n-...     [\"We are very happy to show you the ğŸ¤— Transformers library.\", \"We hope you don't hate it.\"],\n-...     padding=True,\n-...     truncation=True,\n-...     max_length=512,\n-...     return_tensors=\"tf\",\n-... )\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -272,37 +244,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n         [0.2084, 0.1826, 0.1969, 0.1755, 0.2365]], grad_fn=<SoftmaxBackward0>)\n ```\n </pt>\n-<tf>\n-ğŸ¤— Transformers æä¾›äº†ä¸€ç§ç®€å•ç»Ÿä¸€çš„æ–¹å¼æ¥åŠ è½½é¢„è®­ç»ƒçš„å®ä¾‹ã€‚è¿™è¡¨ç¤ºä½ å¯ä»¥åƒåŠ è½½ [`AutoTokenizer`] ä¸€æ ·åŠ è½½ [`TFAutoModel`]ã€‚å”¯ä¸€ä¸åŒçš„åœ°æ–¹æ˜¯ä¸ºä½ çš„ä»»åŠ¡é€‰æ‹©æ­£ç¡®çš„ [`TFAutoModel`]ï¼Œå¯¹äºæ–‡æœ¬ï¼ˆæˆ–åºåˆ—ï¼‰åˆ†ç±»ï¼Œä½ åº”è¯¥åŠ è½½ [`TFAutoModelForSequenceClassification`]ï¼š\n-\n-```py\n->>> from transformers import TFAutoModelForSequenceClassification\n-\n->>> model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n-```\n-\n-<Tip>\n-\n-é€šè¿‡ [ä»»åŠ¡æ‘˜è¦](./task_summary) æŸ¥æ‰¾ [`AutoModel`] æ”¯æŒçš„ä»»åŠ¡.\n-\n-</Tip>\n-\n-ç°åœ¨é€šè¿‡ç›´æ¥å°†å­—å…¸çš„é”®ä¼ ç»™å¼ é‡ï¼Œå°†é¢„å¤„ç†çš„è¾“å…¥æ‰¹æ¬¡ä¼ ç»™æ¨¡å‹ã€‚\n-\n-```py\n->>> tf_outputs = tf_model(tf_batch)\n-```\n-\n-æ¨¡å‹åœ¨ `logits` å±æ€§è¾“å‡ºæœ€ç»ˆçš„æ¿€æ´»ç»“æœã€‚åœ¨ `logits` ä¸Šåº”ç”¨softmaxå‡½æ•°æ¥æŸ¥è¯¢æ¦‚ç‡ï¼š\n-\n-```py\n->>> import tensorflow as tf\n-\n->>> tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n->>> tf_predictions  # doctest: +IGNORE_RESULT\n-```\n-</tf>\n </frameworkcontent>\n \n <Tip>\n@@ -330,21 +271,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(\"./pt_save_pretrained\")\n ```\n </pt>\n-<tf>\n-å½“ä½ çš„æ¨¡å‹å¾®è°ƒå®Œæˆï¼Œä½ å°±å¯ä»¥ä½¿ç”¨ [`TFPreTrainedModel.save_pretrained`] æŠŠå®ƒå’Œå®ƒçš„åˆ†è¯å™¨ä¿å­˜ä¸‹æ¥ï¼š\n-\n-```py\n->>> tf_save_directory = \"./tf_save_pretrained\"\n->>> tokenizer.save_pretrained(tf_save_directory)  # doctest: +IGNORE_RESULT\n->>> tf_model.save_pretrained(tf_save_directory)\n-```\n-\n-å½“ä½ å‡†å¤‡å†æ¬¡ä½¿ç”¨è¿™ä¸ªæ¨¡å‹æ—¶ï¼Œå°±å¯ä»¥ä½¿ç”¨ [`TFPreTrainedModel.from_pretrained`] åŠ è½½å®ƒäº†ï¼š\n-\n-```py\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")\n-```\n-</tf>\n </frameworkcontent>\n \n ğŸ¤— Transformers æœ‰ä¸€ä¸ªç‰¹åˆ«é…·çš„åŠŸèƒ½ï¼Œå®ƒèƒ½å¤Ÿä¿å­˜ä¸€ä¸ªæ¨¡å‹ï¼Œå¹¶ä¸”å°†å®ƒåŠ è½½ä¸º PyTorch æˆ– TensorFlow æ¨¡å‹ã€‚`from_pt` æˆ– `from_tf` å‚æ•°å¯ä»¥å°†æ¨¡å‹ä»ä¸€ä¸ªæ¡†æ¶è½¬æ¢ä¸ºå¦ä¸€ä¸ªæ¡†æ¶ï¼š\n@@ -359,15 +285,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n >>> pt_model = AutoModelForSequenceClassification.from_pretrained(pt_save_directory, from_pt=True)\n ```\n </pt>\n-<tf>\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> tokenizer = AutoTokenizer.from_pretrained(tf_save_directory)\n->>> tf_model = TFAutoModelForSequenceClassification.from_pretrained(tf_save_directory, from_tf=True)\n-```\n-</tf>\n </frameworkcontent>\n \n ## è‡ªå®šä¹‰æ¨¡å‹æ„å»º\n@@ -392,15 +309,6 @@ tensor([[0.0021, 0.0018, 0.0115, 0.2121, 0.7725],\n >>> my_model = AutoModel.from_config(my_config)\n ```\n </pt>\n-<tf>\n-ä½¿ç”¨ [`TFAutoModel.from_config`] æ ¹æ®ä½ çš„è‡ªå®šä¹‰é…ç½®åˆ›å»ºä¸€ä¸ªæ¨¡å‹ï¼š\n-\n-```py\n->>> from transformers import TFAutoModel\n-\n->>> my_model = TFAutoModel.from_config(my_config)\n-```\n-</tf>\n </frameworkcontent>\n \n æŸ¥é˜… [åˆ›å»ºä¸€ä¸ªè‡ªå®šä¹‰ç»“æ„](./create_a_model) æŒ‡å—è·å–æ›´å¤šå…³äºæ„å»ºè‡ªå®šä¹‰é…ç½®çš„ä¿¡æ¯ã€‚"
        },
        {
            "sha": "c82264299a70fbd0ea5f4950b484f9cebebb2544",
            "filename": "docs/source/zh/run_scripts.md",
            "status": "modified",
            "additions": 1,
            "deletions": 36,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Frun_scripts.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Frun_scripts.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Frun_scripts.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -105,23 +105,6 @@ python examples/pytorch/summarization/run_summarization.py \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-\n-ç¤ºä¾‹è„šæœ¬ä»  ğŸ¤— [Datasets](https://huggingface.co/docs/datasets/) åº“ä¸‹è½½å¹¶é¢„å¤„ç†æ•°æ®é›†ã€‚ç„¶åï¼Œè„šæœ¬ä½¿ç”¨ Keras åœ¨æ”¯æŒæ‘˜è¦çš„æ¶æ„ä¸Šå¾®è°ƒæ•°æ®é›†ã€‚ä»¥ä¸‹ç¤ºä¾‹å±•ç¤ºäº†å¦‚ä½•åœ¨ [CNN/DailyMail](https://huggingface.co/datasets/cnn_dailymail) æ•°æ®é›†ä¸Šå¾®è°ƒ [T5-small](https://huggingface.co/google-t5/t5-small)ã€‚T5 æ¨¡å‹ç”±äºè®­ç»ƒæ–¹å¼éœ€è¦é¢å¤–çš„ `source_prefix` å‚æ•°ã€‚è¿™ä¸ªæç¤ºè®© T5 çŸ¥é“è¿™æ˜¯ä¸€ä¸ªæ‘˜è¦ä»»åŠ¡ã€‚\n-\n-```bash\n-python examples/tensorflow/summarization/run_summarization.py  \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## åˆ†å¸ƒå¼è®­ç»ƒå’Œæ··åˆç²¾åº¦\n@@ -174,24 +157,6 @@ python xla_spawn.py --num_cores 8 \\\n     --predict_with_generate\n ```\n </pt>\n-<tf>\n-\n-å¼ é‡å¤„ç†å•å…ƒï¼ˆTPUsï¼‰æ˜¯ä¸“é—¨è®¾è®¡ç”¨äºåŠ é€Ÿæ€§èƒ½çš„ã€‚TensorFlowè„šæœ¬ä½¿ç”¨[`TPUStrategy`](https://www.tensorflow.org/guide/distributed_training#tpustrategy)åœ¨TPUä¸Šè¿›è¡Œè®­ç»ƒã€‚è¦ä½¿ç”¨TPUï¼Œè¯·å°†TPUèµ„æºçš„åç§°ä¼ é€’ç»™`tpu`å‚æ•°ã€‚\n-\n-```bash\n-python run_summarization.py  \\\n-    --tpu name_of_tpu_resource \\\n-    --model_name_or_path google-t5/t5-small \\\n-    --dataset_name cnn_dailymail \\\n-    --dataset_config \"3.0.0\" \\\n-    --output_dir /tmp/tst-summarization  \\\n-    --per_device_train_batch_size 8 \\\n-    --per_device_eval_batch_size 16 \\\n-    --num_train_epochs 3 \\\n-    --do_train \\\n-    --do_eval\n-```\n-</tf>\n </frameworkcontent>\n \n ## åŸºäºğŸ¤— Accelerateè¿è¡Œè„šæœ¬\n@@ -356,4 +321,4 @@ python examples/pytorch/summarization/run_summarization.py\n     --per_device_eval_batch_size=4 \\\n     --overwrite_output_dir \\\n     --predict_with_generate\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "d383f73dc6233a7822fb16862affa51e1b85e2d4",
            "filename": "docs/source/zh/training.md",
            "status": "modified",
            "additions": 1,
            "deletions": 91,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Ftraining.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/226667ec2f98cff3a5dd949b2c68c02b112fb10b/docs%2Fsource%2Fzh%2Ftraining.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Ftraining.md?ref=226667ec2f98cff3a5dd949b2c68c02b112fb10b",
            "patch": "@@ -153,96 +153,6 @@ rendered properly in your Markdown viewer.\n >>> trainer.train()\n ```\n </pt>\n-<tf>\n-<a id='keras'></a>\n-\n-<Youtube id=\"rnTGBy2ax1c\"/>\n-\n-## ä½¿ç”¨kerasè®­ç»ƒTensorFlowæ¨¡å‹\n-\n-æ‚¨ä¹Ÿå¯ä»¥ä½¿ç”¨ Keras API åœ¨ TensorFlow ä¸­è®­ç»ƒ ğŸ¤— Transformers æ¨¡å‹ï¼\n-\n-### åŠ è½½ç”¨äº Keras çš„æ•°æ®\n-\n-å½“æ‚¨å¸Œæœ›ä½¿ç”¨ Keras API è®­ç»ƒ ğŸ¤— Transformers æ¨¡å‹æ—¶ï¼Œæ‚¨éœ€è¦å°†æ‚¨çš„æ•°æ®é›†è½¬æ¢ä¸º Keras å¯ç†è§£çš„æ ¼å¼ã€‚å¦‚æœæ‚¨çš„æ•°æ®é›†å¾ˆå°ï¼Œæ‚¨å¯ä»¥å°†æ•´ä¸ªæ•°æ®é›†è½¬æ¢ä¸ºNumPyæ•°ç»„å¹¶ä¼ é€’ç»™ Kerasã€‚åœ¨è¿›è¡Œæ›´å¤æ‚çš„æ“ä½œä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆå°è¯•è¿™ç§æ–¹æ³•ã€‚\n-\n-é¦–å…ˆï¼ŒåŠ è½½ä¸€ä¸ªæ•°æ®é›†ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ [GLUE benchmark](https://huggingface.co/datasets/glue) ä¸­çš„ CoLA æ•°æ®é›†ï¼Œå› ä¸ºå®ƒæ˜¯ä¸€ä¸ªç®€å•çš„äºŒå…ƒæ–‡æœ¬åˆ†ç±»ä»»åŠ¡ã€‚ç°åœ¨åªä½¿ç”¨è®­ç»ƒæ•°æ®é›†ã€‚\n-\n-\n-```py\n-from datasets import load_dataset\n-\n-dataset = load_dataset(\"glue\", \"cola\")\n-dataset = dataset[\"train\"]  # Just take the training split for now\n-```\n-æ¥ä¸‹æ¥ï¼ŒåŠ è½½ä¸€ä¸ª`tokenizer`å¹¶å°†æ•°æ®æ ‡è®°ä¸º NumPy æ•°ç»„ã€‚è¯·æ³¨æ„ï¼Œæ ‡ç­¾å·²ç»æ˜¯ç”± 0 å’Œ 1 ç»„æˆçš„`list`ï¼Œå› æ­¤æˆ‘ä»¬å¯ä»¥ç›´æ¥å°†å…¶è½¬æ¢ä¸º NumPy æ•°ç»„è€Œæ— éœ€è¿›è¡Œåˆ†è¯å¤„ç†ï¼\n-\n-```py\n-from transformers import AutoTokenizer\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-tokenized_data = tokenizer(dataset[\"sentence\"], return_tensors=\"np\", padding=True)\n-# Tokenizer returns a BatchEncoding, but we convert that to a dict for Keras\n-tokenized_data = dict(tokenized_data)\n-\n-labels = np.array(dataset[\"label\"])  # Label is already an array of 0 and 1\n-```\n-æœ€åï¼ŒåŠ è½½ã€[`compile`](https://keras.io/api/models/model_training_apis/#compile-method) å’Œ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) æ¨¡å‹ã€‚è¯·æ³¨æ„ï¼ŒTransformers æ¨¡å‹éƒ½æœ‰ä¸€ä¸ªé»˜è®¤çš„ä¸ä»»åŠ¡ç›¸å…³çš„æŸå¤±å‡½æ•°ï¼Œå› æ­¤é™¤éæ‚¨å¸Œæœ›è‡ªå®šä¹‰ï¼Œå¦åˆ™æ— éœ€æŒ‡å®šä¸€ä¸ªæŸå¤±å‡½æ•°ï¼š\n-\n-```py\n-from transformers import TFAutoModelForSequenceClassification\n-from tensorflow.keras.optimizers import Adam\n-\n-# Load and compile our model\n-model = TFAutoModelForSequenceClassification.from_pretrained(\"google-bert/bert-base-cased\")\n-# Lower learning rates are often better for fine-tuning transformers\n-model.compile(optimizer=Adam(3e-5))  # No loss argument!\n-\n-model.fit(tokenized_data, labels)\n-```\n-\n-<Tip>\n-\n-å½“æ‚¨ä½¿ç”¨ `compile()` ç¼–è¯‘æ¨¡å‹æ—¶ï¼Œæ— éœ€ä¼ é€’æŸå¤±å‚æ•°ï¼å¦‚æœä¸æŒ‡å®šæŸå¤±å‚æ•°ï¼ŒHugging Face æ¨¡å‹ä¼šè‡ªåŠ¨é€‰æ‹©é€‚åˆå…¶ä»»åŠ¡å’Œæ¨¡å‹æ¶æ„çš„æŸå¤±å‡½æ•°ã€‚å¦‚æœéœ€è¦ï¼Œæ‚¨å§‹ç»ˆå¯ä»¥è‡ªå·±æŒ‡å®šæŸå¤±å‡½æ•°ä»¥è¦†ç›–é»˜è®¤é…ç½®ã€‚\n-\n-</Tip>\n-\n-è¿™ç§æ–¹æ³•å¯¹äºè¾ƒå°çš„æ•°æ®é›†æ•ˆæœå¾ˆå¥½ï¼Œä½†å¯¹äºè¾ƒå¤§çš„æ•°æ®é›†ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°å®ƒå¼€å§‹å˜å¾—æœ‰é—®é¢˜ã€‚ä¸ºä»€ä¹ˆå‘¢ï¼Ÿå› ä¸ºåˆ†è¯åçš„æ•°ç»„å’Œæ ‡ç­¾å¿…é¡»å®Œå…¨åŠ è½½åˆ°å†…å­˜ä¸­ï¼Œè€Œä¸”ç”±äº NumPy æ— æ³•å¤„ç†â€œä¸è§„åˆ™â€æ•°ç»„ï¼Œå› æ­¤æ¯ä¸ªåˆ†è¯åçš„æ ·æœ¬é•¿åº¦éƒ½å¿…é¡»è¢«å¡«å……åˆ°æ•°æ®é›†ä¸­æœ€é•¿æ ·æœ¬çš„é•¿åº¦ã€‚è¿™å°†ä½¿æ‚¨çš„æ•°ç»„å˜å¾—æ›´å¤§ï¼Œè€Œæ‰€æœ‰è¿™äº›`padding tokens`ä¹Ÿä¼šå‡æ…¢è®­ç»ƒé€Ÿåº¦ï¼\n-\n-\n-### å°†æ•°æ®åŠ è½½ä¸º tf.data.Dataset\n-\n-å¦‚æœæ‚¨æƒ³é¿å…è®­ç»ƒé€Ÿåº¦å‡æ…¢ï¼Œå¯ä»¥å°†æ•°æ®åŠ è½½ä¸º `tf.data.Dataset`ã€‚è™½ç„¶æ‚¨å¯ä»¥è‡ªå·±ç¼–å†™è‡ªå·±çš„ `tf.data` æµæ°´çº¿ï¼Œä½†æˆ‘ä»¬æœ‰ä¸¤ç§æ–¹ä¾¿çš„æ–¹æ³•æ¥å®ç°è¿™ä¸€ç‚¹ï¼š\n-\n-- [`~TFPreTrainedModel.prepare_tf_dataset`]ï¼šè¿™æ˜¯æˆ‘ä»¬åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹æ¨èçš„æ–¹æ³•ã€‚å› ä¸ºå®ƒæ˜¯æ¨¡å‹ä¸Šçš„ä¸€ä¸ªæ–¹æ³•ï¼Œå®ƒå¯ä»¥æ£€æŸ¥æ¨¡å‹ä»¥è‡ªåŠ¨ç¡®å®šå“ªäº›åˆ—å¯ç”¨ä½œæ¨¡å‹è¾“å…¥ï¼Œå¹¶ä¸¢å¼ƒå…¶ä»–åˆ—ä»¥åˆ›å»ºä¸€ä¸ªæ›´ç®€å•ã€æ€§èƒ½æ›´å¥½çš„æ•°æ®é›†ã€‚\n-- [`~datasets.Dataset.to_tf_dataset`]ï¼šè¿™ä¸ªæ–¹æ³•æ›´ä½çº§ï¼Œä½†å½“æ‚¨å¸Œæœ›å®Œå…¨æ§åˆ¶æ•°æ®é›†çš„åˆ›å»ºæ–¹å¼æ—¶éå¸¸æœ‰ç”¨ï¼Œå¯ä»¥é€šè¿‡æŒ‡å®šè¦åŒ…æ‹¬çš„ç¡®åˆ‡ `columns` å’Œ `label_cols` æ¥å®ç°ã€‚\n-\n-åœ¨ä½¿ç”¨ [`~TFPreTrainedModel.prepare_tf_dataset`] ä¹‹å‰ï¼Œæ‚¨éœ€è¦å°†`tokenizer`çš„è¾“å‡ºæ·»åŠ åˆ°æ•°æ®é›†ä½œä¸ºåˆ—ï¼Œå¦‚ä¸‹é¢çš„ä»£ç ç¤ºä¾‹æ‰€ç¤ºï¼š\n-\n-```py\n-def tokenize_dataset(data):\n-    # Keys of the returned dictionary will be added to the dataset as columns\n-    return tokenizer(data[\"text\"])\n-\n-\n-dataset = dataset.map(tokenize_dataset)\n-```\n-è¯·è®°ä½ï¼Œé»˜è®¤æƒ…å†µä¸‹ï¼ŒHugging Face æ•°æ®é›†å­˜å‚¨åœ¨ç¡¬ç›˜ä¸Šï¼Œå› æ­¤è¿™ä¸ä¼šå¢åŠ æ‚¨çš„å†…å­˜ä½¿ç”¨ï¼ä¸€æ—¦åˆ—å·²ç»æ·»åŠ ï¼Œæ‚¨å¯ä»¥ä»æ•°æ®é›†ä¸­æµå¼çš„ä¼ è¾“æ‰¹æ¬¡æ•°æ®ï¼Œå¹¶ä¸ºæ¯ä¸ªæ‰¹æ¬¡æ·»åŠ `padding tokens`ï¼Œè¿™ä¸ä¸ºæ•´ä¸ªæ•°æ®é›†æ·»åŠ `padding tokens`ç›¸æ¯”ï¼Œå¤§å¤§å‡å°‘äº†`padding tokens`çš„æ•°é‡ã€‚\n-\n-```py\n->>> tf_dataset = model.prepare_tf_dataset(dataset[\"train\"], batch_size=16, shuffle=True, tokenizer=tokenizer)\n-```\n-è¯·æ³¨æ„ï¼Œåœ¨ä¸Šé¢çš„ä»£ç ç¤ºä¾‹ä¸­ï¼Œæ‚¨éœ€è¦å°†`tokenizer`ä¼ é€’ç»™`prepare_tf_dataset`ï¼Œä»¥ä¾¿å®ƒå¯ä»¥åœ¨åŠ è½½æ‰¹æ¬¡æ—¶æ­£ç¡®å¡«å……å®ƒä»¬ã€‚å¦‚æœæ•°æ®é›†ä¸­çš„æ‰€æœ‰æ ·æœ¬éƒ½å…·æœ‰ç›¸åŒçš„é•¿åº¦è€Œä¸”ä¸éœ€è¦å¡«å……ï¼Œæ‚¨å¯ä»¥è·³è¿‡æ­¤å‚æ•°ã€‚å¦‚æœéœ€è¦æ‰§è¡Œæ¯”å¡«å……æ ·æœ¬æ›´å¤æ‚çš„æ“ä½œï¼ˆä¾‹å¦‚ï¼Œç”¨äºæ©ç è¯­è¨€æ¨¡å‹çš„`tokens` æ›¿æ¢ï¼‰ï¼Œåˆ™å¯ä»¥ä½¿ç”¨ `collate_fn` å‚æ•°ï¼Œè€Œä¸æ˜¯ä¼ é€’ä¸€ä¸ªå‡½æ•°æ¥å°†æ ·æœ¬åˆ—è¡¨è½¬æ¢ä¸ºæ‰¹æ¬¡å¹¶åº”ç”¨ä»»ä½•æ‰€éœ€çš„é¢„å¤„ç†ã€‚è¯·æŸ¥çœ‹æˆ‘ä»¬çš„[ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples)æˆ–[ç¬”è®°](https://huggingface.co/docs/transformers/notebooks)ä»¥äº†è§£æ­¤æ–¹æ³•çš„å®é™…æ“ä½œã€‚\n-\n-ä¸€æ—¦åˆ›å»ºäº† `tf.data.Dataset`ï¼Œæ‚¨å¯ä»¥åƒä»¥å‰ä¸€æ ·ç¼–è¯‘å’Œè®­ç»ƒæ¨¡å‹ï¼š\n-\n-```py\n-model.compile(optimizer=Adam(3e-5))  # No loss argument!\n-\n-model.fit(tf_dataset)\n-```\n-\n-</tf>\n </frameworkcontent>\n \n <a id='pytorch_native'></a>\n@@ -404,4 +314,4 @@ torch.cuda.empty_cache()\n \n - [ğŸ¤— Transformers ç¤ºä¾‹](https://github.com/huggingface/transformers/tree/main/examples) åŒ…å«ç”¨äºåœ¨ PyTorch å’Œ TensorFlow ä¸­è®­ç»ƒå¸¸è§è‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡çš„è„šæœ¬ã€‚\n \n-- [ğŸ¤— Transformers ç¬”è®°](notebooks) åŒ…å«é’ˆå¯¹ç‰¹å®šä»»åŠ¡åœ¨ PyTorch å’Œ TensorFlow ä¸­å¾®è°ƒæ¨¡å‹çš„å„ç§`notebook`ã€‚\n\\ No newline at end of file\n+- [ğŸ¤— Transformers ç¬”è®°](notebooks) åŒ…å«é’ˆå¯¹ç‰¹å®šä»»åŠ¡åœ¨ PyTorch å’Œ TensorFlow ä¸­å¾®è°ƒæ¨¡å‹çš„å„ç§`notebook`ã€‚"
        }
    ],
    "stats": {
        "total": 6903,
        "additions": 21,
        "deletions": 6882
    }
}