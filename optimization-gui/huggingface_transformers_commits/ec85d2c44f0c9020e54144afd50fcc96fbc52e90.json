{
    "author": "cyyever",
    "message": "Avoid CUDA stream sync (#40060)\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "ec85d2c44f0c9020e54144afd50fcc96fbc52e90",
    "files": [
        {
            "sha": "d5d7c64f82615a0397d829bc646e024b871381f1",
            "filename": "src/transformers/modeling_flash_attention_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ec85d2c44f0c9020e54144afd50fcc96fbc52e90/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ec85d2c44f0c9020e54144afd50fcc96fbc52e90/src%2Ftransformers%2Fmodeling_flash_attention_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flash_attention_utils.py?ref=ec85d2c44f0c9020e54144afd50fcc96fbc52e90",
            "patch": "@@ -354,12 +354,12 @@ def prepare_fa_kwargs_from_position_ids(position_ids, is_packed_sequence: bool =\n         max_length_q = int(q_len.max())\n         max_length_k = int(last_position_ids.max()) + 1\n     else:\n-        position_ids = position_ids.flatten()\n-        indices_q = torch.arange(position_ids.size(0), device=position_ids.device, dtype=torch.int32)\n+        position_ids = position_ids.view(-1)\n+        indices_q = (position_ids == 0).nonzero().view(-1)\n \n         cu_seq_lens_q = torch.cat(\n             (\n-                indices_q[position_ids == 0],\n+                indices_q,\n                 torch.tensor(position_ids.size(), device=position_ids.device, dtype=torch.int32),\n             )\n         )"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}