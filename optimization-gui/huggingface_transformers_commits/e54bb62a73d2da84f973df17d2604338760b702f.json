{
    "author": "Cyrilvallez",
    "message": "Simplify and improve model loading logic (#41103)\n\n* remove unexpected keys from inputs (they have nothing to do there)\n\n* remove input\n\n* simplify a lot init\n\n* fix\n\n* fix check for non-persistent buffer\n\n* revert because too many old and bad models...\n\n* remove comment\n\n* type hint\n\n* make it a real test\n\n* remove model_to_load -> always use the same model\n\n* typo\n\n* remove legacy offload_folder (we never waste that memory anymore)\n\n* do not change prefix anymore\n\n* change very bad function name\n\n* create adjust method\n\n* remove useless method\n\n* restrict\n\n* BC\n\n* remove unused method\n\n* CI\n\n* remove unused args\n\n* small fix\n\n* fix\n\n* CI\n\n* CI\n\n* avoid too many loops\n\n* fix regex\n\n* cleaner\n\n* typo\n\n* fix\n\n* fix",
    "sha": "e54bb62a73d2da84f973df17d2604338760b702f",
    "files": [
        {
            "sha": "f9f4d22e4c34500d2902dfbcdfc31ced05730475",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -64,8 +64,7 @@\n     \"test_load_save_without_tied_weights\",\n     \"test_tied_weights_keys\",\n     \"test_model_weights_reload_no_missing_tied_weights\",\n-    \"test_mismatched_shapes_have_properly_initialized_weights\",\n-    \"test_matched_shapes_have_loaded_weights_when_some_mismatched_shapes_exist\",\n+    \"test_can_load_ignoring_mismatched_shapes\",\n     \"test_model_is_small\",\n     \"ModelTest::test_pipeline_\",  # None of the pipeline tests from PipelineTesterMixin (of which XxxModelTest inherits from) are running on device\n     \"ModelTester::test_pipeline_\","
        },
        {
            "sha": "130bccc993ca35785f3d0ce5726a14f70dc8bf3b",
            "filename": "examples/quantization/custom_quantization_int8_example.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/examples%2Fquantization%2Fcustom_quantization_int8_example.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fquantization%2Fcustom_quantization_int8_example.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -159,7 +159,7 @@ def _process_model_before_weight_loading(self, model, **kwargs):\n             pre_quantized=self.pre_quantized,\n         )\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model,\n         param_value: \"torch.Tensor\",\n@@ -187,7 +187,6 @@ def create_quantized_param(\n         param_name: str,\n         target_device: \"torch.device\",\n         state_dict: dict[str, Any],\n-        unexpected_keys: Optional[list[str]] = None,\n     ):\n         \"\"\"\n         Quantizes weights to INT8 symmetric format."
        },
        {
            "sha": "5e8e7bd500dd29ef0ddf53299ff978b3b1af8424",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 132,
            "deletions": 246,
            "changes": 378,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -19,13 +19,10 @@\n import gc\n import importlib.metadata\n import inspect\n-import itertools\n import json\n import os\n import re\n-import shutil\n import sys\n-import tempfile\n import warnings\n from abc import abstractmethod\n from collections import defaultdict\n@@ -133,7 +130,6 @@\n         extract_model_from_parallel,\n         get_balanced_memory,\n         get_max_memory,\n-        load_offloaded_weights,\n         offload_weight,\n         save_offload_index,\n     )\n@@ -561,26 +557,6 @@ def load_state_dict(\n             raise OSError(f\"Unable to load weights from pytorch checkpoint file '{checkpoint_file}'.\")\n \n \n-def set_initialized_submodules(model, state_dict_keys):\n-    \"\"\"\n-    Sets the `_is_hf_initialized` flag in all submodules of a given model when all its weights are in the loaded state\n-    dict.\n-    \"\"\"\n-    state_dict_keys = set(state_dict_keys)\n-    not_initialized_submodules = {}\n-    for module_name, module in model.named_modules():\n-        if module_name == \"\":\n-            # When checking if the root module is loaded there's no need to prepend module_name.\n-            module_keys = set(module.state_dict())\n-        else:\n-            module_keys = {f\"{module_name}.{k}\" for k in module.state_dict()}\n-        if module_keys.issubset(state_dict_keys):\n-            module._is_hf_initialized = True\n-        else:\n-            not_initialized_submodules[module_name] = module\n-    return not_initialized_submodules\n-\n-\n def _end_ptr(tensor: torch.Tensor) -> int:\n     # extract the end of the pointer if the tensor is a slice of a bigger tensor\n     if tensor.nelement():\n@@ -701,17 +677,12 @@ def _load_state_dict_into_meta_model(\n     model: \"PreTrainedModel\",\n     state_dict: dict,\n     shard_file: str,\n-    expected_keys: list[str],\n     reverse_renaming_mapping: dict[str, str],\n     device_map: Optional[dict] = None,\n     disk_offload_folder: Optional[str] = None,\n     disk_offload_index: Optional[dict] = None,\n-    cpu_offload_folder: Optional[str] = None,\n-    cpu_offload_index: Optional[dict] = None,\n     hf_quantizer: Optional[HfQuantizer] = None,\n-    is_safetensors: bool = False,\n     keep_in_fp32_regex: Optional[re.Pattern] = None,\n-    unexpected_keys: Optional[list[str]] = None,  # passing `unexpected` for cleanup from quantization items\n     device_mesh: Optional[\"torch.distributed.device_mesh.DeviceMesh\"] = None,\n ) -> tuple[Optional[dict], Optional[dict]]:\n     \"\"\"Load parameters from `meta_state_dict` into the model. The parameters of the `meta_state_dict` are on the meta\n@@ -732,12 +703,13 @@ def _load_state_dict_into_meta_model(\n         QuantizationMethod.BITS_AND_BYTES,\n         QuantizationMethod.TORCHAO,\n     }\n-    is_meta_state_dict = shard_file.endswith(\".safetensors\") and not is_hqq_or_bnb_or_ao\n-    file_pointer = None\n-    if is_meta_state_dict:\n-        file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device)\n+    is_safetensors = shard_file.endswith(\".safetensors\")\n+    is_meta_state_dict = is_safetensors and not is_hqq_or_bnb_or_ao\n+    file_pointer = safe_open(shard_file, framework=\"pt\", device=tensor_device) if is_meta_state_dict else None\n+    params_to_load = list(state_dict.keys())\n \n-    for param_name, empty_param in state_dict.items():\n+    for param_name in params_to_load:\n+        empty_param = state_dict[param_name]\n         # we need to use serialized_param_name as file pointer is untouched\n         if is_meta_state_dict:\n             # This is the name of the parameter as it appears on disk file\n@@ -754,19 +726,10 @@ def _load_state_dict_into_meta_model(\n         )\n \n         if device_mesh is not None:\n-            if (\n-                not is_quantized\n-                or (not hf_quantizer.requires_parameters_quantization)\n-                or (\n-                    not hf_quantizer.check_quantized_param(\n-                        model,\n-                        param,\n-                        param_name,\n-                        state_dict,\n-                        device_map=device_map,\n-                    )\n-                )\n-            ):  # In this case, the param is already on the correct device!\n+            if not is_quantized or not hf_quantizer.param_needs_quantization(\n+                model, param, param_name, state_dict, device_map=device_map\n+            ):\n+                # In this case, the param is already on the correct device!\n                 shard_and_distribute_module(\n                     model,\n                     param,\n@@ -791,7 +754,6 @@ def _load_state_dict_into_meta_model(\n                     param_name,\n                     device_mesh.get_local_rank(),\n                     state_dict,\n-                    unexpected_keys,\n                     **sharding_kwargs,\n                 )\n         else:\n@@ -813,21 +775,8 @@ def _load_state_dict_into_meta_model(\n             if param_device == \"disk\":\n                 if not is_safetensors:\n                     disk_offload_index = offload_weight(param, param_name, disk_offload_folder, disk_offload_index)\n-            elif param_device == \"cpu\" and cpu_offload_index is not None:\n-                cpu_offload_index = offload_weight(param, param_name, cpu_offload_folder, cpu_offload_index)\n-            elif (\n-                not is_quantized\n-                or (not hf_quantizer.requires_parameters_quantization)\n-                or (\n-                    not hf_quantizer.check_quantized_param(\n-                        model,\n-                        param,\n-                        param_name,\n-                        state_dict,\n-                        param_device=param_device,\n-                        device_map=device_map,\n-                    )\n-                )\n+            elif not is_quantized or not hf_quantizer.param_needs_quantization(\n+                model, param, param_name, state_dict, param_device=param_device, device_map=device_map\n             ):\n                 if is_fsdp_enabled():\n                     param_device = \"cpu\" if is_local_dist_rank_0() else \"meta\"\n@@ -836,9 +785,7 @@ def _load_state_dict_into_meta_model(\n \n             else:\n                 # TODO naming is stupid it loads it as well\n-                hf_quantizer.create_quantized_param(\n-                    model, param, param_name, param_device, state_dict, unexpected_keys\n-                )\n+                hf_quantizer.create_quantized_param(model, param, param_name, param_device, state_dict)\n \n                 # For quantized modules with FSDP/DeepSpeed Stage 3, we need to quantize the parameter on the GPU\n                 # and then cast it to CPU to avoid excessive memory usage on each GPU\n@@ -861,10 +808,14 @@ def _load_state_dict_into_meta_model(\n                     value = type(value)(value.data.to(param_to), **val_kwargs, **value.__dict__)\n                     setattr(module, param_type, value)\n \n+        # Remove the param from the state dict if it was not loaded on the fly to avoid wasting memory\n+        if not is_meta_state_dict:\n+            del state_dict[param_name]\n+\n     if file_pointer is not None:\n         file_pointer.__exit__(None, None, None)\n \n-    return disk_offload_index, cpu_offload_index\n+    return disk_offload_index\n \n \n def load_shard_file(args):\n@@ -878,22 +829,17 @@ def load_shard_file(args):\n         hf_quantizer,\n         key_renaming_mapping,\n         weights_only,\n-        model_to_load,\n-        expected_keys,\n+        model,\n         reverse_key_renaming_mapping,\n         disk_offload_folder,\n         disk_offload_index,\n-        cpu_offload_folder,\n-        cpu_offload_index,\n-        is_offloaded_safetensors,\n         keep_in_fp32_regex,\n-        unexpected_keys,\n         device_mesh,\n     ) = args\n \n     # Skip the load for shards that only contain disk-offloaded weights\n     if shard_file in disk_only_shard_files:\n-        return [], disk_offload_index, cpu_offload_index\n+        return [], disk_offload_index\n \n     map_location = \"cpu\"\n     if (\n@@ -921,39 +867,33 @@ def load_shard_file(args):\n \n     # Fix the key names\n     state_dict = {key_renaming_mapping[k]: v for k, v in state_dict.items() if k in key_renaming_mapping}\n-    metadata = None\n-    if shard_file.endswith(\".safetensors\") and is_safetensors_available():\n-        with safe_open(shard_file, framework=\"pt\") as f:\n-            metadata = f.metadata()\n \n-    if hf_quantizer:\n-        state_dict = hf_quantizer.update_state_dict_with_metadata(state_dict, metadata)\n+    if hf_quantizer is not None and hf_quantizer.quantization_config.quant_method == QuantizationMethod.TORCHAO:\n+        if shard_file.endswith(\".safetensors\") and is_safetensors_available():\n+            with safe_open(shard_file, framework=\"pt\") as f:\n+                metadata = f.metadata()\n+            state_dict = hf_quantizer.update_state_dict_with_metadata(state_dict, metadata)\n \n     error_msgs = []\n \n     if is_deepspeed_zero3_enabled() and not is_quantized:\n-        error_msgs += _load_state_dict_into_zero3_model(model_to_load, state_dict)\n+        error_msgs += _load_state_dict_into_zero3_model(model, state_dict)\n     # Skip it with fsdp on ranks other than 0\n     elif not (is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized):\n-        disk_offload_index, cpu_offload_index = _load_state_dict_into_meta_model(\n-            model_to_load,\n+        disk_offload_index = _load_state_dict_into_meta_model(\n+            model,\n             state_dict,\n             shard_file,\n-            expected_keys,\n             reverse_key_renaming_mapping,\n             device_map=device_map,\n             disk_offload_folder=disk_offload_folder,\n             disk_offload_index=disk_offload_index,\n-            cpu_offload_folder=cpu_offload_folder,\n-            cpu_offload_index=cpu_offload_index,\n             hf_quantizer=hf_quantizer,\n-            is_safetensors=is_offloaded_safetensors,\n             keep_in_fp32_regex=keep_in_fp32_regex,\n-            unexpected_keys=unexpected_keys,\n             device_mesh=device_mesh,\n         )\n \n-    return error_msgs, disk_offload_index, cpu_offload_index\n+    return error_msgs, disk_offload_index\n \n \n def load_shard_files_with_threadpool(args_list):\n@@ -970,18 +910,13 @@ def load_shard_files_with_threadpool(args_list):\n         with logging.tqdm(total=len(args_list), desc=\"Loading checkpoint shards\") as pbar:\n             futures = [executor.submit(load_shard_file, arg) for arg in args_list]\n             for future in as_completed(futures):\n-                result = future.result()\n-                (\n-                    _error_msgs,\n-                    disk_offload_index,\n-                    cpu_offload_index,\n-                ) = result\n+                _error_msgs, disk_offload_index = future.result()\n \n                 error_msgs += _error_msgs\n \n                 pbar.update(1)\n \n-    return error_msgs, disk_offload_index, cpu_offload_index\n+    return error_msgs, disk_offload_index\n \n \n def _add_variant(weights_name: str, variant: Optional[str] = None) -> str:\n@@ -1423,7 +1358,7 @@ def _find_missing_and_unexpected_keys(\n     \"\"\"\n     prefix = model.base_model_prefix\n \n-    # Compute expected keys, i.e. keys that the FULL model (not model_to_load) expects\n+    # Compute expected keys, i.e. keys that the full model expects\n     expected_keys = list(model.state_dict().keys())\n     if hf_quantizer is not None:\n         expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, checkpoint_keys)\n@@ -1449,7 +1384,6 @@ def _find_missing_and_unexpected_keys(\n \n     if hf_quantizer is not None:\n         missing_keys = hf_quantizer.update_missing_keys(model, missing_keys, prefix)\n-        unexpected_keys = hf_quantizer.update_unexpected_keys(model, unexpected_keys, prefix)\n \n     return missing_keys, unexpected_keys\n \n@@ -4558,10 +4492,6 @@ def from_pretrained(\n                 If provided, it has to contain dimension named `\"tp\"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism\n             offload_folder (`str` or `os.PathLike`, *optional*):\n                 If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n-            offload_state_dict (`bool`, *optional*):\n-                If `True`, will temporarily offload the CPU state dict to the hard drive to avoid getting out of CPU\n-                RAM if the weight of the CPU state dict + the biggest shard of the checkpoint does not fit. Defaults to\n-                `True` when there is some disk offload.\n             offload_buffers (`bool`, *optional*):\n                 Whether or not to offload the buffers with the model parameters.\n             quantization_config (`Union[QuantizationConfigMixin,Dict]`, *optional*):\n@@ -4631,7 +4561,6 @@ def from_pretrained(\n         device_map = kwargs.pop(\"device_map\", None)\n         max_memory = kwargs.pop(\"max_memory\", None)\n         offload_folder = kwargs.pop(\"offload_folder\", None)\n-        offload_state_dict = kwargs.pop(\"offload_state_dict\", False)\n         offload_buffers = kwargs.pop(\"offload_buffers\", False)\n         load_in_8bit = kwargs.pop(\"load_in_8bit\", False)\n         load_in_4bit = kwargs.pop(\"load_in_4bit\", False)\n@@ -4667,6 +4596,7 @@ def from_pretrained(\n         _ = kwargs.pop(\"low_cpu_mem_usage\", None)\n         _ = kwargs.pop(\"from_tf\", None)\n         _ = kwargs.pop(\"from_flax\", None)\n+        _ = kwargs.pop(\"offload_state_dict\", None)\n \n         # For BC on torch_dtype argument\n         if torch_dtype is not None:\n@@ -5015,14 +4945,7 @@ def _assign_original_dtype(module):\n             torch.set_default_dtype(dtype_orig)\n \n         # Finalize model weight initialization\n-        (\n-            model,\n-            missing_keys,\n-            unexpected_keys,\n-            mismatched_keys,\n-            offload_index,\n-            error_msgs,\n-        ) = cls._load_pretrained_model(\n+        model, missing_keys, unexpected_keys, mismatched_keys, offload_index, error_msgs = cls._load_pretrained_model(\n             model,\n             state_dict,\n             checkpoint_files,\n@@ -5031,7 +4954,6 @@ def _assign_original_dtype(module):\n             sharded_metadata=sharded_metadata,\n             device_map=device_map,\n             disk_offload_folder=offload_folder,\n-            offload_state_dict=offload_state_dict,\n             dtype=dtype,\n             hf_quantizer=hf_quantizer,\n             keep_in_fp32_regex=keep_in_fp32_regex,\n@@ -5183,6 +5105,14 @@ def _get_key_renaming_mapping(\n         prefix = self.base_model_prefix\n         _prefix = f\"{prefix}.\"\n \n+        if loading_task_model_from_base_state_dict:\n+            task_specific_expected_keys, base_model_keys = [], []\n+            for key in self.state_dict():\n+                if key.startswith(_prefix):\n+                    base_model_keys.append(key[len(_prefix) :])\n+                else:\n+                    task_specific_expected_keys.append(key)\n+\n         renamed_keys = {}\n         key_renaming_mapping = {}\n         for key in checkpoint_keys:\n@@ -5200,6 +5130,13 @@ def _get_key_renaming_mapping(\n \n             # In this case, we need to add the prefix to the keys, to match them to the expected keys\n             if loading_task_model_from_base_state_dict:\n+                # small sanity check: if we find a key that is only part of the task-specific keys, we raise\n+                # (if it's also part of the base model, we do not raise and assume it comes from there)\n+                if new_key in task_specific_expected_keys and new_key not in base_model_keys:\n+                    raise ValueError(\n+                        \"The state dictionary of the model you are trying to load is corrupted. Are you sure it was \"\n+                        \"properly saved?\"\n+                    )\n                 new_key = \".\".join([prefix, new_key])\n             # In this case we need to remove the prefix from the key to match them to the expected keys, and use\n             # only the keys starting with the prefix\n@@ -5253,7 +5190,6 @@ def _load_pretrained_model(\n         sharded_metadata: Optional[dict] = None,\n         device_map: Optional[dict] = None,\n         disk_offload_folder: Optional[str] = None,\n-        offload_state_dict: Optional[bool] = None,\n         dtype: Optional[torch.dtype] = None,\n         hf_quantizer: Optional[HfQuantizer] = None,\n         keep_in_fp32_regex: Optional[re.Pattern] = None,\n@@ -5285,7 +5221,6 @@ def _load_pretrained_model(\n \n         # Check if we are in a special state, i.e. loading from a state dict coming from a different architecture\n         prefix = model.base_model_prefix\n-        _prefix = f\"{prefix}.\"\n         has_prefix_module = any(s.startswith(prefix) for s in original_checkpoint_keys) if len(prefix) > 0 else False\n         expects_prefix_module = hasattr(model, prefix) if len(prefix) > 0 else False\n         loading_task_model_from_base_state_dict = not has_prefix_module and expects_prefix_module\n@@ -5324,10 +5259,10 @@ def _load_pretrained_model(\n \n         # Move missing (and potentially mismatched) keys back to cpu from meta device (because they won't be moved when\n         # loading the weights as they are not in the loaded state dict)\n-        model._move_missing_keys_from_meta_to_cpu(missing_keys + mismatched_keys, unexpected_keys, dtype, hf_quantizer)\n+        model._move_missing_keys_from_meta_to_cpu(missing_keys + mismatched_keys, dtype, hf_quantizer)\n \n         # correctly initialize the missing (and potentially mismatched) keys\n-        model._initialize_missing_keys(checkpoint_keys, ignore_mismatched_sizes, is_quantized)\n+        model._initialize_missing_keys(missing_keys + mismatched_keys, is_quantized)\n \n         # Set some modules to fp32 if needed\n         if keep_in_fp32_regex is not None:\n@@ -5336,30 +5271,6 @@ def _load_pretrained_model(\n                     # param = param.to(torch.float32) does not work here as only in the local scope.\n                     param.data = param.data.to(torch.float32)\n \n-        # Make sure we are able to load base models as well as derived models (specific task models, with heads)\n-        model_to_load = model\n-        # In this case, we load a ForTaskModel with keys from a BaseModel -> only load keys to the BaseModel\n-        if loading_task_model_from_base_state_dict:\n-            model_to_load = getattr(model, prefix)\n-            # Here we need to remove the prefix we added to correctly find missing/unexpected keys, as we will load\n-            # in the submodule\n-            key_renaming_mapping = {k: v[len(_prefix) :] for k, v in key_renaming_mapping.items()}\n-            checkpoint_keys = list(key_renaming_mapping.values())\n-            unexpected_keys = [k[len(_prefix) :] if k.startswith(_prefix) else k for k in unexpected_keys]\n-            # We need to update the device map as well\n-            if device_map is not None:\n-                device_map = {k[len(_prefix) :] if k.startswith(_prefix) else k: v for k, v in device_map.items()}\n-            # small sanity check: the base model should not contain task-specific head keys\n-            task_specific_expected_keys = [s for s in model.state_dict() if not s.startswith(_prefix)]\n-            base_model_expected_keys = list(model_to_load.state_dict().keys())\n-            if any(\n-                key in task_specific_expected_keys and key not in base_model_expected_keys for key in unexpected_keys\n-            ):\n-                raise ValueError(\n-                    \"The state dictionary of the model you are trying to load is corrupted. Are you sure it was \"\n-                    \"properly saved?\"\n-                )\n-\n         # Get reverse key mapping\n         reverse_key_renaming_mapping = {v: k for k, v in key_renaming_mapping.items()}\n \n@@ -5369,8 +5280,6 @@ def _load_pretrained_model(\n         disk_only_shard_files = []\n         # Prepare parameters offloading if needed\n         if device_map is not None and \"disk\" in device_map.values():\n-            if offload_state_dict is None:\n-                offload_state_dict = True\n             if disk_offload_folder is not None:\n                 os.makedirs(disk_offload_folder, exist_ok=True)\n             is_offloaded_safetensors = checkpoint_files is not None and checkpoint_files[0].endswith(\".safetensors\")\n@@ -5408,31 +5317,22 @@ def _load_pretrained_model(\n             else:\n                 disk_offload_index = {}\n \n-        # This offload index if for params that are supposed to be on the \"cpu\", either with or without a device_map\n-        # It allows to load parameters one-by-one from the state dict, avoiding a memory peak of 2 x state_dict_size,\n-        # i.e. 1x to load it, and 1x to copy it to model\n-        cpu_offload_folder = None\n-        cpu_offload_index = None\n-        if offload_state_dict:\n-            cpu_offload_folder = tempfile.mkdtemp()\n-            cpu_offload_index = {}\n-\n         # To be able to iterate, even if we don't use it if the state_dict is already provided\n         elif state_dict is not None:\n             checkpoint_files = [\"\"]\n \n         # Compute expected model keys\n-        expected_keys = list(model_to_load.state_dict().keys())\n+        expected_keys = list(model.state_dict().keys())\n         if hf_quantizer is not None:\n-            expected_keys = hf_quantizer.update_expected_keys(model_to_load, expected_keys, checkpoint_keys)\n+            expected_keys = hf_quantizer.update_expected_keys(model, expected_keys, checkpoint_keys)\n \n         if logger.level >= logging.WARNING:\n-            verify_tp_plan(expected_keys, getattr(model_to_load, \"_tp_plan\", None))\n+            verify_tp_plan(expected_keys, getattr(model, \"_tp_plan\", None))\n \n         # Warmup cuda to load the weights much faster on devices\n         if device_map is not None and not is_hqq_or_quark:\n             expanded_device_map = expand_device_map(device_map, expected_keys)\n-            caching_allocator_warmup(model_to_load, expanded_device_map, hf_quantizer)\n+            caching_allocator_warmup(model, expanded_device_map, hf_quantizer)\n \n         # Prepare and compatabilize arguments for serial and parallel shard loading\n         args_list = [\n@@ -5446,16 +5346,11 @@ def _load_pretrained_model(\n                 hf_quantizer,\n                 key_renaming_mapping,\n                 weights_only,\n-                model_to_load,\n-                expected_keys,\n+                model,\n                 reverse_key_renaming_mapping,\n                 disk_offload_folder,\n                 disk_offload_index,\n-                cpu_offload_folder,\n-                cpu_offload_index,\n-                is_offloaded_safetensors,\n                 keep_in_fp32_regex,\n-                unexpected_keys,\n                 device_mesh,\n             )\n             for shard_file in checkpoint_files\n@@ -5467,40 +5362,20 @@ def _load_pretrained_model(\n             os.environ.get(\"HF_ENABLE_PARALLEL_LOADING\", \"\").upper() in ENV_VARS_TRUE_VALUES\n             and not is_deepspeed_zero3_enabled()\n         ):\n-            _error_msgs, disk_offload_index, cpu_offload_index = load_shard_files_with_threadpool(args_list)\n+            _error_msgs, disk_offload_index = load_shard_files_with_threadpool(args_list)\n             error_msgs += _error_msgs\n         else:\n             if len(args_list) > 1:\n                 args_list = logging.tqdm(args_list, desc=\"Loading checkpoint shards\")\n \n             for args in args_list:\n-                _error_msgs, disk_offload_index, cpu_offload_index = load_shard_file(args)\n+                _error_msgs, disk_offload_index = load_shard_file(args)\n                 error_msgs += _error_msgs\n \n-        # Adjust offloaded weights name and save if needed\n-        if disk_offload_index is not None and len(disk_offload_index) > 0:\n-            if loading_task_model_from_base_state_dict:\n-                # We need to add the prefix of the base model\n-                prefix = cls.base_model_prefix\n-                if not is_offloaded_safetensors:\n-                    for weight_name in disk_offload_index:\n-                        shutil.move(\n-                            os.path.join(disk_offload_folder, f\"{weight_name}.dat\"),\n-                            os.path.join(disk_offload_folder, f\"{prefix}.{weight_name}.dat\"),\n-                        )\n-                disk_offload_index = {f\"{prefix}.{key}\": value for key, value in disk_offload_index.items()}\n-            if not is_offloaded_safetensors:\n-                save_offload_index(disk_offload_index, disk_offload_folder)\n-                disk_offload_index = None\n-\n-        # one-at-a-time param loading for the cpu offloaded params\n-        if offload_state_dict:\n-            # Load back temporarily offloaded state dict\n-            load_offloaded_weights(model_to_load, cpu_offload_index, cpu_offload_folder)\n-            shutil.rmtree(cpu_offload_folder)\n-\n-        if hf_quantizer is not None:\n-            missing_keys = hf_quantizer.update_missing_keys_after_loading(model_to_load, missing_keys, prefix)\n+        # Save offloaded index if needed\n+        if disk_offload_index is not None and len(disk_offload_index) > 0 and not is_offloaded_safetensors:\n+            save_offload_index(disk_offload_index, disk_offload_folder)\n+            disk_offload_index = None\n \n         # Post-processing for tensor parallelism\n         if device_mesh is not None:\n@@ -5535,22 +5410,10 @@ def _load_pretrained_model(\n                         device_mesh,\n                     )\n \n-        # Model-specific exceptions for missing and unexpected keys (e.g. if the modeling change over time, or any other reason...)\n-        # We should remove them here to avoid raising warnings if they are present in the lists\n-        if cls._keys_to_ignore_on_load_missing is not None:\n-            for pattern in cls._keys_to_ignore_on_load_missing:\n-                missing_keys = [k for k in missing_keys if re.search(pattern, k) is None]\n-\n-        if cls._keys_to_ignore_on_load_unexpected is not None:\n-            for pattern in cls._keys_to_ignore_on_load_unexpected:\n-                unexpected_keys = [k for k in unexpected_keys if re.search(pattern, k) is None]\n-\n-        # Old checkpoints may have keys for rotary_emb.inv_freq for each layer, however we moved this buffer to the main model\n-        # (so the buffer name has changed). Remove them in such a case. This is another exception that was not added to\n-        # `_keys_to_ignore_on_load_unexpected` as it touches many models\n-        has_inv_freq_buffers = any(buffer.endswith(\"rotary_emb.inv_freq\") for buffer, _ in model.named_buffers())\n-        if has_inv_freq_buffers:\n-            unexpected_keys = [k for k in unexpected_keys if \"rotary_emb.inv_freq\" not in k]\n+        # Remove potential model-specific exceptions from the warnings\n+        missing_keys, unexpected_keys = model._adjust_missing_and_unexpected_keys(\n+            missing_keys, unexpected_keys, loading_task_model_from_base_state_dict\n+        )\n \n         # All potential warnings/infos\n         if len(error_msgs) > 0:\n@@ -5572,21 +5435,12 @@ def _load_pretrained_model(\n                 f\" {model.__class__.__name__} from the checkpoint of a model that you expect to be exactly identical\"\n                 \" (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\"\n             )\n-        else:\n-            logger.info(f\"All model checkpoint weights were used when initializing {model.__class__.__name__}.\\n\")\n         if len(missing_keys) > 0:\n             logger.warning(\n                 f\"Some weights of {model.__class__.__name__} were not initialized from the model checkpoint at\"\n                 f\" {pretrained_model_name_or_path} and are newly initialized: {missing_keys}\\nYou should probably\"\n                 \" TRAIN this model on a down-stream task to be able to use it for predictions and inference.\"\n             )\n-        elif len(mismatched_keys) == 0:\n-            logger.info(\n-                f\"All the weights of {model.__class__.__name__} were initialized from the model checkpoint at\"\n-                f\" {pretrained_model_name_or_path}.\\nIf your task is similar to the task the model of the checkpoint\"\n-                f\" was trained on, you can already use {model.__class__.__name__} for predictions without further\"\n-                \" training.\"\n-            )\n         if len(mismatched_keys) > 0:\n             mismatched_warning = \"\\n\".join(\n                 [\n@@ -5836,12 +5690,8 @@ def is_backend_compatible(cls):\n         return cls._supports_attention_backend\n \n     def _move_missing_keys_from_meta_to_cpu(\n-        self,\n-        missing_keys: list[str],\n-        unexpected_keys: list[str],\n-        dtype: Optional[torch.dtype],\n-        hf_quantizer: Optional[HfQuantizer],\n-    ) -> \"PreTrainedModel\":\n+        self, missing_keys: list[str], dtype: torch.dtype, hf_quantizer: Optional[HfQuantizer]\n+    ) -> None:\n         \"\"\"Move the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts) back\n         from meta device to cpu.\n         \"\"\"\n@@ -5861,56 +5711,92 @@ def _move_missing_keys_from_meta_to_cpu(\n             # Buffers are not initialized on the meta device, so we still need this check to avoid overwriting them\n             if param.device == torch.device(\"meta\"):\n                 value = torch.empty_like(param, dtype=dtype, device=\"cpu\")\n-                if (\n-                    not is_quantized\n-                    or (getattr(hf_quantizer, \"requires_parameters_quantization\", False))\n-                    or not hf_quantizer.check_quantized_param(self, param_value=value, param_name=key, state_dict={})\n+                if not is_quantized or not hf_quantizer.param_needs_quantization(\n+                    self, param_value=value, param_name=key, state_dict={}\n                 ):\n                     _load_parameter_into_model(self, key, value)\n                 else:\n-                    hf_quantizer.create_quantized_param(self, value, key, \"cpu\", model_state_dict, unexpected_keys)\n+                    hf_quantizer.create_quantized_param(self, value, key, \"cpu\", model_state_dict)\n \n-    def _initialize_missing_keys(\n-        self,\n-        loaded_keys: list[str],\n-        ignore_mismatched_sizes: bool,\n-        is_quantized: bool,\n-    ) -> \"PreTrainedModel\":\n+    def _initialize_missing_keys(self, missing_keys: list[str], is_quantized: bool) -> None:\n         \"\"\"Initialize the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts), according to\n         `_initialize_weights`. Indeed, since the corresponding weights are missing from the state dict, they will not be replaced and need to\n         be initialized correctly (i.e. weight initialization distribution).\n         Also take care of setting the `_is_hf_initialized` flag for keys that are not missing.\n         \"\"\"\n-        if not ignore_mismatched_sizes:\n-            not_initialized_submodules = set_initialized_submodules(self, loaded_keys)\n-            # If we're about to tie the output embeds to the input embeds we don't need to init them\n+        for key in self.state_dict():\n+            # If it's part of the keys that will be loaded, mark it as already initialized\n+            if key not in missing_keys:\n+                param_or_buffer = self.get_parameter_or_buffer(key)\n+                param_or_buffer._is_hf_initialized = True\n+\n+        def set_is_initialized_for_modules(module):\n+            # A module is already initialized if and only if all its children are also already initialized, and all\n+            # its immediate `nn.Parameter` and persistent buffers are also already initialized\n             if (\n-                hasattr(self.config.get_text_config(decoder=True), \"tie_word_embeddings\")\n-                and self.config.get_text_config(decoder=True).tie_word_embeddings\n+                all(getattr(child, \"_is_hf_initialized\", False) for child in module.children())\n+                and all(getattr(param, \"_is_hf_initialized\", False) for param in module.parameters(recurse=False))\n+                and all(\n+                    getattr(buffer, \"_is_hf_initialized\", False)\n+                    for buffer in module.buffers(recurse=False)\n+                    if buffer not in module._non_persistent_buffers_set\n+                )\n             ):\n-                output_embeddings = self.get_output_embeddings()\n-                if output_embeddings is not None:\n-                    # Still need to initialize if there is a bias term since biases are not tied.\n-                    if not hasattr(output_embeddings, \"bias\") or output_embeddings.bias is None:\n-                        output_embeddings._is_hf_initialized = True\n-        else:\n-            not_initialized_submodules = dict(self.named_modules())\n+                module._is_hf_initialized = True\n+\n+        # Set the flag on the modules as well. We do it recursively (depth-first), as it's more efficient (we do not\n+        # need to check the entire state dict of each module, only the immediate children, so we only iterate once over\n+        # each param)\n+        self.apply(set_is_initialized_for_modules)\n+\n         # This will only initialize submodules that are not marked as initialized by the line above.\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n             import deepspeed\n \n             not_initialized_parameters = list(\n-                set(\n-                    itertools.chain.from_iterable(\n-                        submodule.parameters(recurse=False) for submodule in not_initialized_submodules.values()\n-                    )\n-                )\n+                {v for v in self.state_dict().values() if not getattr(v, \"_is_hf_initialized\", False)}\n             )\n             with deepspeed.zero.GatheredParameters(not_initialized_parameters, modifier_rank=0):\n                 self.initialize_weights()\n         else:\n             self.initialize_weights()\n \n+    def _adjust_missing_and_unexpected_keys(\n+        self, missing_keys: list[str], unexpected_keys: list[str], loading_task_model_from_base_state_dict: bool\n+    ) -> tuple[list[str], list[str]]:\n+        \"\"\"Adjust the `missing_keys` and `unexpected_keys` based on current model's exception rules, to avoid\n+        raising unneeded warnings/errors.\n+        \"\"\"\n+        # Old checkpoints may have keys for rotary_emb.inv_freq for each layer, however we moved this buffer to the main model\n+        # (so the buffer name has changed). Remove them in such a case. This is another exception that was not added to\n+        # `_keys_to_ignore_on_load_unexpected` as it touches many models -> we add it manually to the existing patterns\n+        has_inv_freq_buffers = any(buffer.endswith(\"rotary_emb.inv_freq\") for buffer, _ in self.named_buffers())\n+        additional_unexpected_patterns = [r\"rotary_emb\\.inv_freq\"] if has_inv_freq_buffers else []\n+\n+        missing_patterns = self._keys_to_ignore_on_load_missing or []\n+        unexpected_patterns = (self._keys_to_ignore_on_load_unexpected or []) + additional_unexpected_patterns\n+        ignore_missing_regex, ignore_unexpected_regex = None, None\n+        if len(missing_patterns) > 0:\n+            ignore_missing_regex = re.compile(\"|\".join(rf\"({pattern})\" for pattern in missing_patterns))\n+        if len(unexpected_patterns) > 0:\n+            ignore_unexpected_regex = re.compile(\"|\".join(rf\"({pattern})\" for pattern in unexpected_patterns))\n+\n+        # Clean-up missing keys\n+        if ignore_missing_regex is not None:\n+            missing_keys = [key for key in missing_keys if ignore_missing_regex.search(key) is None]\n+\n+        # Clean-up unexpected keys\n+        if ignore_unexpected_regex is not None:\n+            unexpected_keys = [key for key in unexpected_keys if ignore_unexpected_regex.search(key) is None]\n+\n+        # Note: only the unexpected keys should remove the added prefix here, to correctly display the original name\n+        # in the warnings. For missing keys, we should show the prefix in the warning as it's part of the final model\n+        if loading_task_model_from_base_state_dict:\n+            _prefix = f\"{self.base_model_prefix}.\"\n+            unexpected_keys = [k[len(_prefix) :] if k.startswith(_prefix) else k for k in unexpected_keys]\n+\n+        return missing_keys, unexpected_keys\n+\n     def get_parameter_or_buffer(self, target: str):\n         \"\"\"\n         Return the parameter or buffer given by `target` if it exists, otherwise throw an error. This combines"
        },
        {
            "sha": "1ccbd50352e71f8fcdfe1393af327be85c371765",
            "filename": "src/transformers/models/deberta_v2/modeling_deberta_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeberta_v2%2Fmodeling_deberta_v2.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -911,7 +911,7 @@ def forward(self, sequence_output, word_embeddings):\n @auto_docstring\n class DebertaV2ForMaskedLM(DebertaV2PreTrainedModel):\n     _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n-    _keys_to_ignore_on_load_unexpected = r\"mask_predictions.*\"\n+    _keys_to_ignore_on_load_unexpected = [r\"mask_predictions.*\"]\n \n     def __init__(self, config):\n         super().__init__(config)"
        },
        {
            "sha": "aa123acd3948b343a1d862a3ffbbab217d95c52e",
            "filename": "src/transformers/quantizers/base.py",
            "status": "modified",
            "additions": 11,
            "deletions": 33,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fbase.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -128,27 +128,6 @@ def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> li\n         \"\"\"\n         return missing_keys\n \n-    def update_unexpected_keys(self, model, unexpected_keys: list[str], prefix: str) -> list[str]:\n-        \"\"\"\n-        Override this method if you want to adjust the `unexpected_keys`.\n-\n-        Args:\n-            unexpected_keys (`list[str]`, *optional*):\n-                The list of unexpected keys in the checkpoint compared to the state dict of the model\n-        \"\"\"\n-        return unexpected_keys\n-\n-    def update_missing_keys_after_loading(self, model, missing_keys: list[str], prefix: str) -> list[str]:\n-        \"\"\"\n-        Override this method if you want to adjust the `missing_keys` after loading the model params,\n-        but before the model is post-processed.\n-\n-        Args:\n-            missing_keys (`list[str]`, *optional*):\n-                The list of missing keys in the checkpoint compared to the state dict of the model\n-        \"\"\"\n-        return missing_keys\n-\n     def update_expected_keys(self, model, expected_keys: list[str], loaded_keys: list[str]) -> list[str]:\n         \"\"\"\n         Override this method if you want to adjust the `update_expected_keys`.\n@@ -182,18 +161,17 @@ def adjust_max_memory(self, max_memory: dict[str, Union[int, str]]) -> dict[str,\n         \"\"\"adjust max_memory argument for infer_auto_device_map() if extra memory is needed for quantization\"\"\"\n         return max_memory\n \n-    def check_quantized_param(\n-        self,\n-        model: \"PreTrainedModel\",\n-        param_value: \"torch.Tensor\",\n-        param_name: str,\n-        state_dict: dict[str, Any],\n-        **kwargs,\n-    ) -> bool:\n-        \"\"\"\n-        checks if a loaded state_dict component is part of quantized param + some validation; only defined if\n-        requires_parameters_quantization == True for quantization methods that require to create a new parameters\n-        for quantization.\n+    def check_quantized_param(self, *args, **kwargs) -> bool:\n+        \"\"\"DEPRECATED -> remove in v5\"\"\"\n+        logger.warning_once(\n+            \"`check_quantized_param` is deprecated in favor of `param_needs_quantization`, which is a much \"\n+            \"more self.explanatory name for what the method achieves. It will be removed in v5\"\n+        )\n+        return self.param_needs_quantization(*args, **kwargs)\n+\n+    def param_needs_quantization(self, model: \"PreTrainedModel\", param_name: str, **kwargs) -> bool:\n+        \"\"\"\n+        Check whether a given param needs quantization as defined by `create_quantized_param`.\n         \"\"\"\n         return False\n "
        },
        {
            "sha": "77df9c9fc933131e0a15984f015568400e4d6eb7",
            "filename": "src/transformers/quantizers/quantizer_bnb_4bit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_4bit.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -132,7 +132,7 @@ def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n                 \"calculation. You may encounter unexpected behavior, or pass your own device map\"\n             )\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n@@ -160,7 +160,6 @@ def create_quantized_param(\n         param_name: str,\n         target_device: \"torch.device\",\n         state_dict: dict[str, Any],\n-        unexpected_keys: Optional[list[str]] = None,\n     ):\n         \"\"\"\n         combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()\n@@ -218,8 +217,6 @@ def create_quantized_param(\n             for k, v in state_dict.items():\n                 if param_name + \".\" in k:\n                     quantized_stats[k] = v\n-                    if unexpected_keys is not None and k in unexpected_keys:\n-                        unexpected_keys.remove(k)\n \n             param_kwargs = {}\n             if self.is_bnb_supports_quant_storage_module:\n@@ -241,6 +238,7 @@ def create_quantized_param(\n                 new_value = new_value.T\n \n             kwargs = old_value.__dict__\n+            kwargs.pop(\"_is_hf_initialized\", None)\n             new_value = bnb.nn.Params4bit(new_value, requires_grad=False, **kwargs).to(target_device)\n \n         module._parameters[tensor_name] = new_value"
        },
        {
            "sha": "5aa814355fdb5acb7d390fd71dcc6c16a43680c0",
            "filename": "src/transformers/quantizers/quantizer_bnb_8bit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 13,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_bnb_8bit.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -158,7 +158,7 @@ def adjust_target_dtype(self, target_dtype: \"torch.dtype\") -> \"torch.dtype\":\n             logger.info(\"target_dtype {target_dtype} is replaced by `torch.int8` for 8-bit BnB quantization\")\n         return torch.int8\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n@@ -187,19 +187,15 @@ def create_quantized_param(\n         param_name: str,\n         target_device: \"torch.device\",\n         state_dict: dict[str, Any],\n-        unexpected_keys: Optional[list[str]] = None,\n     ):\n         \"\"\"\n         combines logic from _load_state_dict_into_meta_model and .integrations.bitsandbytes.py::set_module_quantized_tensor_to_device()\n-        needs aux items from state dicts, if found - removes them from unexpected_keys\n+        needs aux items from state dicts, if found\n         \"\"\"\n         import bitsandbytes as bnb\n \n         fp16_statistics_key = param_name.replace(\"weight\", \"SCB\")\n-        fp16_weights_format_key = param_name.replace(\"weight\", \"weight_format\")\n-\n         fp16_statistics = state_dict.get(fp16_statistics_key)\n-        fp16_weights_format = state_dict.get(fp16_weights_format_key)\n \n         module, tensor_name = get_module_from_name(model, param_name)\n         if tensor_name not in module._parameters:\n@@ -230,18 +226,12 @@ def create_quantized_param(\n                 new_value = new_value.T\n \n         kwargs = old_value.__dict__\n+        kwargs.pop(\"_is_hf_initialized\", None)\n         new_value = bnb.nn.Int8Params(new_value, requires_grad=False, **kwargs).to(target_device)\n \n         module._parameters[tensor_name] = new_value\n         if fp16_statistics is not None:\n             setattr(module.weight, \"SCB\", fp16_statistics.to(target_device))\n-            if unexpected_keys is not None:\n-                unexpected_keys.remove(fp16_statistics_key)\n-\n-        # We just need to pop the `weight_format` keys from the state dict to remove unneeded\n-        # messages. The correct format is correctly retrieved during the first forward pass.\n-        if fp16_weights_format is not None and unexpected_keys is not None:\n-            unexpected_keys.remove(fp16_weights_format_key)\n \n     def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):\n         model.is_loaded_in_8bit = True"
        },
        {
            "sha": "010365367981f9c36f8828351d8dd1dfc1a56b15",
            "filename": "src/transformers/quantizers/quantizer_eetq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_eetq.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -100,7 +100,7 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n             logger.info(\"We suggest you to set `dtype=torch.float16` for better efficiency with EETQ.\")\n         return dtype\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n@@ -130,7 +130,6 @@ def create_quantized_param(\n         param_name: str,\n         target_device: \"torch.device\",\n         state_dict: dict[str, Any],\n-        unexpected_keys: Optional[list[str]] = None,\n     ):\n         \"\"\"\n         quantizes weights into qweight and weight_scales"
        },
        {
            "sha": "9259be3509373c63bf57c8caa0a9714bc337d3df",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -105,7 +105,7 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n             )\n         return dtype\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n@@ -142,7 +142,6 @@ def create_quantized_param(\n         param_name: str,\n         target_device: \"torch.device\",\n         state_dict: dict[str, Any],\n-        unexpected_keys: Optional[list[str]] = None,\n     ):\n         \"\"\"\n         Quantizes weights into weight and weight_scale\n@@ -194,8 +193,6 @@ def create_quantized_param(\n \n         module._parameters[tensor_name] = torch.nn.Parameter(new_value.to(target_device))\n \n-        if unexpected_keys is not None and param_name in unexpected_keys:\n-            unexpected_keys.remove(param_name)\n         del param_name\n \n     def _process_model_after_weight_loading(self, model: \"PreTrainedModel\", **kwargs):"
        },
        {
            "sha": "7a16c7597e12f8fefe77049f7892b93ea12b93e4",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -82,7 +82,6 @@ def create_quantized_param(\n         param_name: str,\n         target_device: \"torch.device\",\n         state_dict: dict[str, Any],\n-        unexpected_keys: Optional[list[str]] = None,\n     ):\n         \"\"\"\n         Quantizes weights to FP8 format using Block-wise quantization\n@@ -129,7 +128,7 @@ def create_quantized_param(\n         _load_parameter_into_model(model, param_name, quantized_param)\n         _load_parameter_into_model(model, param_name.rsplit(\".\", 1)[0] + \".weight_scale_inv\", scale)\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\","
        },
        {
            "sha": "dba76fb9780916107e6e8fc5b6db05551ce9a9e4",
            "filename": "src/transformers/quantizers/quantizer_fp_quant.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fp_quant.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -90,7 +90,6 @@ def create_quantized_param(\n         param_name: str,\n         target_device: \"torch.device\",\n         state_dict: dict[str, Any],\n-        unexpected_keys: Optional[list[str]] = None,\n     ):\n         module, _ = get_module_from_name(model, param_name)\n \n@@ -122,9 +121,6 @@ def create_quantized_param(\n         # Let pre-forward handle the quantization and set None where necessary\n         module.pre_forward()\n \n-        if unexpected_keys is not None and param_name in unexpected_keys:\n-            unexpected_keys.remove(param_name)\n-\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n@@ -163,7 +159,7 @@ def is_trainable(self, model: Optional[\"PreTrainedModel\"] = None):\n     def is_serializable(self, safe_serialization=None):\n         return True\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\","
        },
        {
            "sha": "ecd7b11930832190d561bc9854ef6a87eae77c4c",
            "filename": "src/transformers/quantizers/quantizer_higgs.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_higgs.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -88,7 +88,6 @@ def create_quantized_param(\n         param_name: str,\n         target_device: \"torch.device\",\n         state_dict: dict[str, Any],\n-        unexpected_keys: Optional[list[str]] = None,\n     ):\n         from ..integrations import quantize_with_higgs\n \n@@ -117,9 +116,6 @@ def create_quantized_param(\n             else:\n                 raise ValueError(f\"Unexpected key {key} in module {module}\")\n \n-        if unexpected_keys is not None and param_name in unexpected_keys:\n-            unexpected_keys.remove(param_name)\n-\n     def _process_model_before_weight_loading(\n         self,\n         model: \"PreTrainedModel\",\n@@ -184,7 +180,7 @@ def is_trainable(self) -> bool:\n     def is_serializable(self, safe_serialization=None):\n         return True\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\","
        },
        {
            "sha": "e3d3d27ccda317a67f9d149ca97af229f1787ff3",
            "filename": "src/transformers/quantizers/quantizer_hqq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_hqq.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -46,7 +46,6 @@ class HqqHfQuantizer(HfQuantizer):\n     \"\"\"\n     HQQ quantizer base HF class.\n     nn.Linear modules are first tagged with quant_config in _process_model_before_weight_loading().\n-    The actual quantization and offloading to the GPU is done in check_quantized_param().\n     \"\"\"\n \n     use_keep_in_fp32_modules = False\n@@ -152,7 +151,7 @@ def _find_hqq_quantizable_layers(model, layers):\n \n         return list(new_keys)\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n@@ -182,7 +181,6 @@ def create_quantized_param(\n         param_name: str,\n         target_device: \"torch.device\",\n         state_dict: dict[str, Any],\n-        unexpected_keys: list[str],\n     ):\n         \"\"\"\n         Each nn.Linear layer is processed here.\n@@ -216,8 +214,6 @@ def weight(_self: HQQLinear):\n         for k, v in state_dict.items():\n             if layer_name + \".\" in k:\n                 module_state_dict[k.split(\".\")[-1]] = v\n-                if unexpected_keys is not None and k in unexpected_keys:\n-                    unexpected_keys.remove(k)\n \n         if self.pre_quantized:\n             if isinstance(module, HQQLinear):"
        },
        {
            "sha": "9c290f6f055ec8be16dd4324c0b525466cd6e3c6",
            "filename": "src/transformers/quantizers/quantizer_mxfp4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_mxfp4.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -147,7 +147,7 @@ def update_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n             )\n         return dtype\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n@@ -178,7 +178,6 @@ def create_quantized_param(\n         param_name: str,\n         target_device: \"torch.device\",\n         state_dict: dict[str, Any],\n-        unexpected_keys: Optional[list[str]] = None,\n         **kwargs,\n     ):\n         from ..integrations import ("
        },
        {
            "sha": "622e6a777e2e943625d9796e6b2e69c69f690dbf",
            "filename": "src/transformers/quantizers/quantizer_quanto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quanto.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -103,17 +103,14 @@ def update_missing_keys(self, model, missing_keys: list[str], prefix: str) -> li\n                         not_missing_keys.append(missing)\n         return [k for k in missing_keys if k not in not_missing_keys]\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n         param_name: str,\n         state_dict: dict[str, Any],\n         **kwargs,\n     ) -> bool:\n-        \"\"\"\n-        Check if a parameter needs to be quantized.\n-        \"\"\"\n         if is_optimum_quanto_available():\n             from optimum.quanto import QModuleMixin\n "
        },
        {
            "sha": "165b00b6129c89279f7da6b25505ca340029bed3",
            "filename": "src/transformers/quantizers/quantizer_quark.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_quark.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -82,7 +82,7 @@ def _process_model_before_weight_loading(self, model: \"PreTrainedModel\", **kwarg\n \n         return model\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n@@ -92,9 +92,7 @@ def check_quantized_param(\n     ) -> bool:\n         return True\n \n-    def create_quantized_param(\n-        self, model, param, param_name, param_device, state_dict, unexpected_keys\n-    ) -> \"torch.nn.Parameter\":\n+    def create_quantized_param(self, model, param, param_name, param_device, state_dict) -> \"torch.nn.Parameter\":\n         postfix = param_name.split(\".\")[-1]\n \n         if postfix in CHECKPOINT_KEYS:"
        },
        {
            "sha": "8c0254b64554f85bf8cb580efef3cfccfceed3a6",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -170,7 +170,7 @@ def get_state_dict_and_metadata(self, model, safe_serialization: Optional[bool]\n                     f\"In order to use safetensors with torchao, please use torchao version >= 0.14.0. Current version: {TORCHAO_VERSION}\"\n                 )\n         else:\n-            return super().get_state_dict_and_metadata(model)\n+            return None, {}\n \n     def adjust_target_dtype(self, dtype: \"torch.dtype\") -> \"torch.dtype\":\n         if version.parse(importlib.metadata.version(\"accelerate\")) > version.parse(\"0.19.0\"):\n@@ -229,7 +229,7 @@ def _process_model_before_weight_loading(\n             ]\n         return\n \n-    def check_quantized_param(\n+    def param_needs_quantization(\n         self,\n         model: \"PreTrainedModel\",\n         param_value: \"torch.Tensor\",\n@@ -262,7 +262,6 @@ def create_quantized_param(\n         param_name: str,\n         target_device: \"torch.device\",\n         state_dict: dict[str, Any],\n-        unexpected_keys: list[str],\n     ):\n         \"\"\"\n         Each nn.Linear layer that needs to be quantized is processed here.\n@@ -322,7 +321,7 @@ def update_state_dict_with_metadata(self, state_dict, metadata):\n         if TORCHAO_VERSION >= version.parse(\"0.14.0\") and is_metadata_torchao(metadata):\n             return unflatten_tensor_state_dict(state_dict, metadata)\n         else:\n-            return super().update_state_dict_with_metadata(state_dict, metadata)\n+            return state_dict\n \n     def _process_model_after_weight_loading(self, model, **kwargs):\n         \"\"\"No process required for torchao quantized model\"\"\""
        },
        {
            "sha": "a5906ea14109d739b691235ae8a4fedaab402959",
            "filename": "tests/models/bamba/test_modeling_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbamba%2Ftest_modeling_bamba.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -360,13 +360,6 @@ def test_initialization(self):\n                             msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                         )\n \n-    def test_mismatched_shapes_have_properly_initialized_weights(self):\n-        r\"\"\"\n-        Overriding the test_mismatched_shapes_have_properly_initialized_weights test because A_log and D params of the\n-        Bamba mixer are initialized differently and we tested that in test_initialization\n-        \"\"\"\n-        self.skipTest(reason=\"Cumbersome and redundant for Bamba\")\n-\n     def test_attention_outputs(self):\n         r\"\"\"\n         Overriding the test_attention_outputs test as the Bamba model outputs attention only for its attention layers"
        },
        {
            "sha": "27eb8e32713be90c6e24d53d0be51f9f81641f69",
            "filename": "tests/models/falcon_h1/test_modeling_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 31,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffalcon_h1%2Ftest_modeling_falcon_h1.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -310,37 +310,6 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n-    # def test_initialization(self):\n-    #     r\"\"\"\n-    #     Overriding the test_initialization test as the A_log and D params of the FalconH1 mixer are initialized differently\n-    #     \"\"\"\n-    #     config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-    #     configs_no_init = _config_zero_init(config)\n-    #     for model_class in self.all_model_classes:\n-    #         model = model_class(config=configs_no_init)\n-    #         for name, param in model.named_parameters():\n-    #             if param.requires_grad:\n-    #                 if \"A_log\" in name:\n-    #                     A = torch.arange(1, config.mamba_n_heads + 1, dtype=torch.float32)\n-    #                     torch.testing.assert_close(param.data, torch.log(A), rtol=1e-5, atol=1e-5)\n-    #                 elif \"D\" in name:\n-    #                     D = torch.ones(config.mamba_n_heads, dtype=torch.float32)\n-    #                     torch.testing.assert_close(param.data, D, rtol=1e-5, atol=1e-5)\n-    #                 else:\n-    #                     self.assertIn(\n-    #                         ((param.data.mean() * 1e9).round() / 1e9).item(),\n-    #                         [0.0, 1.0],\n-    #                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n-    #                     )\n-\n-    def test_mismatched_shapes_have_properly_initialized_weights(self):\n-        r\"\"\"\n-        Overriding the test_mismatched_shapes_have_properly_initialized_weights test because A_log and D params of the\n-        FalconH1 mixer are initialized differently and we tested that in test_initialization\n-        \"\"\"\n-        self.skipTest(reason=\"Cumbersome and redundant for FalconH1\")\n-\n     def test_attention_outputs(self):\n         r\"\"\"\n         Overriding the test_attention_outputs test as the FalconH1 model outputs attention only for its attention layers"
        },
        {
            "sha": "cce6ef3020b5d35a485c0e932fd0e2eec316f2fa",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -356,10 +356,6 @@ def test_initialization(self):\n     def test_load_with_mismatched_shapes(self):\n         pass\n \n-    @unittest.skip(\"Loading nested configs with overwritten `kwargs` isn't supported yet, FIXME @raushan.\")\n-    def test_mismatched_shapes_have_properly_initialized_weights(self):\n-        pass\n-\n     def test_automodelforcausallm(self):\n         \"\"\"\n         Regression test for #36741/#36917 -- make sure `AutoModelForCausalLM` works with a Gemma3 config, i.e. that"
        },
        {
            "sha": "65e6decf4b29072cf398511da3437e13baf0cbf5",
            "filename": "tests/models/jamba/test_modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjamba%2Ftest_modeling_jamba.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -454,13 +454,6 @@ def test_initialization(self):\n                             msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                         )\n \n-    def test_mismatched_shapes_have_properly_initialized_weights(self):\n-        r\"\"\"\n-        Overriding the test_mismatched_shapes_have_properly_initialized_weights test because A_log and D params of the\n-        Mamba block are initialized differently and we tested that in test_initialization\n-        \"\"\"\n-        self.skipTest(reason=\"Cumbersome and redundant for Jamba\")\n-\n     def test_attention_outputs(self):\n         r\"\"\"\n         Overriding the test_attention_outputs test as the Jamba model outputs attention only for its attention layers"
        },
        {
            "sha": "c33ecdf5a5f086edfdf9d8c8c415bc03c14fddeb",
            "filename": "tests/models/qwen3_next/test_modeling_qwen3_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_next%2Ftest_modeling_qwen3_next.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -315,10 +315,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    @unittest.skip(\"Redundant with `test_initialization`, and fails because of the same param (`A_log`)\")\n-    def test_mismatched_shapes_have_properly_initialized_weights(self):\n-        pass\n-\n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     def test_eager_matches_sdpa_inference(\n         self,"
        },
        {
            "sha": "01c2153c2a369ee81340e98ccf54aa7e190b7989",
            "filename": "tests/models/timm_wrapper/test_modeling_timm_wrapper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_wrapper%2Ftest_modeling_timm_wrapper.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -163,10 +163,6 @@ def test_can_init_all_missing_weights(self):\n     def test_initialization(self):\n         pass\n \n-    @unittest.skip(reason=\"TimmWrapper initialization is managed on the timm side\")\n-    def test_mismatched_shapes_have_properly_initialized_weights(self):\n-        pass\n-\n     def test_gradient_checkpointing(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         model = TimmWrapperModel._from_config(config)"
        },
        {
            "sha": "a5580a7814dd46d34acaf0e689bc1bae32844fc1",
            "filename": "tests/models/zamba/test_modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba%2Ftest_modeling_zamba.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -391,13 +391,6 @@ def test_initialization(self):\n                             msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                         )\n \n-    def test_mismatched_shapes_have_properly_initialized_weights(self):\n-        r\"\"\"\n-        Overriding the test_mismatched_shapes_have_properly_initialized_weights test because A_log and D params of the\n-        Mamba block are initialized differently and we tested that in test_initialization\n-        \"\"\"\n-        self.skipTest(\"Cumbersome and redundant for Zamba\")\n-\n     def test_attention_outputs(self):\n         r\"\"\"\n         Overriding the test_attention_outputs test as the Zamba model outputs attention only for its attention layers"
        },
        {
            "sha": "cc9b55f9eb67c41919abeecd199ecb56ed60ab67",
            "filename": "tests/models/zamba2/test_modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzamba2%2Ftest_modeling_zamba2.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -420,14 +420,6 @@ def test_initialization(self):\n                             msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                         )\n \n-    @unittest.skip(reason=\"Cumbersome and redundant for Zamba2\")\n-    def test_mismatched_shapes_have_properly_initialized_weights(self):\n-        r\"\"\"\n-        Overriding the test_mismatched_shapes_have_properly_initialized_weights test because A_log and D params of the\n-        Mamba block are initialized differently and we tested that in test_initialization\n-        \"\"\"\n-        pass\n-\n     def test_attention_outputs(self):\n         r\"\"\"\n         Overriding the test_attention_outputs test as the Zamba2 model outputs attention only for its attention layers"
        },
        {
            "sha": "043ce5348f71a27e09088320458db70fa2edbf6a",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 50,
            "deletions": 157,
            "changes": 207,
            "blob_url": "https://github.com/huggingface/transformers/blob/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e54bb62a73d2da84f973df17d2604338760b702f/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=e54bb62a73d2da84f973df17d2604338760b702f",
            "patch": "@@ -112,7 +112,6 @@\n     is_torch_bf16_available_on_device,\n     is_torch_fp16_available_on_device,\n )\n-from transformers.utils.generic import ContextManagers\n \n from .generation.test_utils import GenerationTesterMixin\n \n@@ -129,7 +128,7 @@\n \n     from transformers import MODEL_MAPPING\n     from transformers.cache_utils import Cache, DynamicCache\n-    from transformers.modeling_utils import load_state_dict, no_init_weights\n+    from transformers.modeling_utils import load_state_dict\n     from transformers.pytorch_utils import id_tensor_storage\n \n from transformers.utils.fx import _FX_SUPPORTED_MODELS_WITH_KV_CACHE, symbolic_trace\n@@ -3244,12 +3243,13 @@ def test_load_with_mismatched_shapes(self):\n                     else:\n                         new_model_without_prefix(input_ids)\n \n-    def test_mismatched_shapes_have_properly_initialized_weights(self):\n+    def test_can_load_ignoring_mismatched_shapes(self):\n         if not self.test_mismatched_shapes:\n             self.skipTest(reason=\"test_mismatched_shapes is set to False\")\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n         configs_no_init = _config_zero_init(config)\n+        configs_no_init.num_labels = 3\n \n         for model_class in self.all_model_classes:\n             mappings = [\n@@ -3263,66 +3263,6 @@ def test_mismatched_shapes_have_properly_initialized_weights(self):\n             if not is_classication_model:\n                 continue\n \n-            # TODO: ydshieh\n-            is_special_classes = model_class.__name__ in [\n-                \"wav2vec2.masked_spec_embed\",\n-                \"Wav2Vec2ForSequenceClassification\",\n-                \"CLIPForImageClassification\",\n-                \"MetaClip2ForImageClassification\",\n-                \"Siglip2ForImageClassification\",\n-                \"RegNetForImageClassification\",\n-                \"ResNetForImageClassification\",\n-                \"UniSpeechSatForSequenceClassification\",\n-                \"Wav2Vec2BertForSequenceClassification\",\n-                \"PvtV2ForImageClassification\",\n-                \"Wav2Vec2ConformerForSequenceClassification\",\n-                \"WavLMForSequenceClassification\",\n-                \"SwiftFormerForImageClassification\",\n-                \"SEWForSequenceClassification\",\n-                \"BitForImageClassification\",\n-                \"SEWDForSequenceClassification\",\n-                \"SiglipForImageClassification\",\n-                \"HubertForSequenceClassification\",\n-                \"Swinv2ForImageClassification\",\n-                \"Data2VecAudioForSequenceClassification\",\n-                \"UniSpeechForSequenceClassification\",\n-                \"PvtForImageClassification\",\n-                \"ModernBertForSequenceClassification\",\n-                \"ModernBertForTokenClassification\",\n-                \"TimmWrapperForImageClassification\",\n-                \"ModernBertForQuestionAnswering\",\n-                \"ModernBertDecoderForSequenceClassification\",\n-                \"ModernBertDecoderForCausalLM\",\n-            ]\n-            special_param_names = [\n-                r\"^bit\\.\",\n-                r\"^classifier\\.weight\",\n-                r\"^classifier\\.bias\",\n-                r\"^classifier\\..+\\.weight\",\n-                r\"^classifier\\..+\\.bias\",\n-                r\"^data2vec_audio\\.\",\n-                r\"^dist_head\\.\",\n-                r\"^head\\.\",\n-                r\"^hubert\\.\",\n-                r\"^pvt\\.\",\n-                r\"^pvt_v2\\.\",\n-                r\"^regnet\\.\",\n-                r\"^resnet\\.\",\n-                r\"^sew\\.\",\n-                r\"^sew_d\\.\",\n-                r\"^swiftformer\\.\",\n-                r\"^swinv2\\.\",\n-                r\"^transformers\\.models\\.swiftformer\\.\",\n-                r\"^timm_model\\.\",\n-                r\"^unispeech\\.\",\n-                r\"^unispeech_sat\\.\",\n-                r\"^vision_model\\.\",\n-                r\"^wav2vec2\\.\",\n-                r\"^wav2vec2_bert\\.\",\n-                r\"^wav2vec2_conformer\\.\",\n-                r\"^wavlm\\.\",\n-            ]\n-\n             with self.subTest(msg=f\"Testing {model_class}\"):\n                 with tempfile.TemporaryDirectory() as tmp_dir:\n                     model = model_class(configs_no_init)\n@@ -3338,101 +3278,54 @@ def test_mismatched_shapes_have_properly_initialized_weights(self):\n                         new_model = model_class.from_pretrained(tmp_dir, num_labels=42, ignore_mismatched_sizes=True)\n                     self.assertIn(\"the shapes did not match\", cl.out)\n \n-                    for name, param in new_model.named_parameters():\n-                        if param.requires_grad:\n-                            param_mean = ((param.data.mean() * 1e9).round() / 1e9).item()\n-                            if not (\n-                                is_special_classes\n-                                and any(len(re.findall(target, name)) > 0 for target in special_param_names)\n-                            ):\n-                                self.assertIn(\n-                                    param_mean,\n-                                    [0.0, 1.0],\n-                                    msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n-                                )\n-                            else:\n-                                # Here we allow the parameters' mean to be in the range [-5.0, 5.0] instead of being\n-                                # either `0.0` or `1.0`, because their initializations are not using\n-                                # `config.initializer_factor` (or something similar). The purpose of this test is simply\n-                                # to make sure they are properly initialized (to avoid very large value or even `nan`).\n-                                self.assertGreaterEqual(\n-                                    param_mean,\n-                                    -5.0,\n-                                    msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n-                                )\n-                                self.assertLessEqual(\n-                                    param_mean,\n-                                    5.0,\n-                                    msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n-                                )\n-\n-    def test_matched_shapes_have_loaded_weights_when_some_mismatched_shapes_exist(self):\n-        # 1. Create a dummy class. Should have buffers as well? To make sure we test __init__\n-        class MyClass(PreTrainedModel):\n-            config_class = PretrainedConfig\n-\n-            def __init__(self, config=None):\n-                super().__init__(config if config is not None else PretrainedConfig())\n-                self.linear = nn.Linear(10, config.num_labels, bias=True)\n-                self.embedding = nn.Embedding(10, 10)\n-                self.std = 1\n-\n-            def _init_weights(self, module):\n-                if isinstance(module, nn.Linear):\n-                    module.weight.data = nn.init.kaiming_uniform_(module.weight.data, np.sqrt(5))\n-                    if module.bias is not None:\n-                        module.bias.data = module.bias.data.normal_(mean=0.0, std=self.std)\n-\n-        # Used to make sure the weights with matched shape are loaded correctly\n-        config = PretrainedConfig()\n-        config.num_labels = 3\n-        model = MyClass(config=config)\n-\n-        # Used to make sure the weights with mismatched shape are properly initialized\n-        set_seed(0)\n-        config = PretrainedConfig()\n-        config.num_labels = 4\n-        # not to init. the weights during the creation: to match the logic in `from_pretrained`, so we can keep the\n-        # same sequence of random ops in the execution path to allow us to compare `target_model` and `new_model` below\n-        # for `linear` part.\n-        with ContextManagers([no_init_weights()]):\n-            target_model = MyClass(config=config)\n-        target_model.apply(target_model._initialize_weights)\n+                    # Find the name of the module with the mismatched size\n+                    top_linear_modules = [\n+                        (name, module) for name, module in new_model.named_children() if isinstance(module, nn.Linear)\n+                    ]\n+                    # Some old model have the Linear classification layer inside a ClassificationHead module or nn.Sequential\n+                    if len(top_linear_modules) == 0:\n+                        # ClassificationHead case\n+                        if any(\n+                            module.__class__.__name__.endswith(\"ClassificationHead\") for module in new_model.children()\n+                        ):\n+                            head_name, head_module = next(\n+                                (name, module)\n+                                for name, module in new_model.named_children()\n+                                if module.__class__.__name__.endswith(\"ClassificationHead\")\n+                            )\n+                        # nn.Sequential case\n+                        elif any(isinstance(module, nn.Sequential) for module in new_model.children()):\n+                            head_name, head_module = next(\n+                                (name, module)\n+                                for name, module in new_model.named_children()\n+                                if isinstance(module, nn.Sequential)\n+                            )\n+                        # Unknown at this point -> skip (only xlm, perceiver, levit, flaubert, audio_spectrogram_transformer as of 23/09/2025)\n+                        else:\n+                            self.skipTest(\"Could not locate the classification Linear layer.\")\n+                        top_linear_modules = [\n+                            (f\"{head_name}.{name}\", module)\n+                            for name, module in head_module.named_children()\n+                            if isinstance(module, nn.Linear)\n+                        ]\n+                    # Usually we have only 1, but swiftformer and deit have 2 Linear layers using `num_labels`\n+                    mismatched_modules = [name for name, module in top_linear_modules if module.out_features == 42]\n \n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            state_dict = model.state_dict()\n-            del state_dict[\"linear.weight\"]\n-\n-            model.config.save_pretrained(tmpdirname)\n-            torch.save(state_dict, os.path.join(tmpdirname, \"pytorch_model.bin\"))\n-\n-            set_seed(0)\n-            new_model = MyClass.from_pretrained(tmpdirname, num_labels=4, ignore_mismatched_sizes=True)\n-\n-            for key in new_model.state_dict():\n-                # check weight values for weights with matched shapes are identical\n-                # (i.e. correctly loaded from the checkpoint)\n-                if key not in [\"linear.weight\", \"linear.bias\"]:\n-                    max_diff = torch.max(torch.abs(model.state_dict()[key] - new_model.state_dict()[key]))\n-                    self.assertLessEqual(\n-                        max_diff.item(),\n-                        1e-6,\n-                        msg=f\"the weight values for `{key}` in `new_model` and `model` are  not identical\",\n-                    )\n-                else:\n-                    # check we have some mismatched shapes\n-                    self.assertNotEqual(\n-                        model.state_dict()[key].shape,\n-                        new_model.state_dict()[key].shape,\n-                        msg=f\"the weight shapes for {key} in `model` and `new_model` should differ\",\n-                    )\n-                    # check the weights with mismatched shape are properly initialized\n-                    max_diff = torch.max(torch.abs(new_model.state_dict()[key] - target_model.state_dict()[key]))\n-                    self.assertLessEqual(\n-                        max_diff.item(),\n-                        1e-6,\n-                        msg=f\"the weight values for `{key}` in `new_model` and `target_model` are not identical\",\n-                    )\n+                    for (k1, v1), (k2, v2) in zip(new_model.named_parameters(), model.named_parameters()):\n+                        # Sanity check: params must have all the same name\n+                        self.assertEqual(k1, k2)\n+                        # Each param except the mismatched ones must be exactly similar\n+                        if not any(k1.startswith(mismatched_module) for mismatched_module in mismatched_modules):\n+                            self.assertTrue((v1 == v2).all())\n+                        # Check that the dims are indeed mismatched between old and new models\n+                        else:\n+                            # The old model should have `num_labels=3` (here it's the first dim of shape, as Linear layers\n+                            # are transposed)\n+                            self.assertEqual(v2.shape[0], 3)\n+                            # Make sure the mean of the new Linear layer is correctly centered around 0 (we cannot use\n+                            # a lower value for the check as some models hardcode a std of 0.02 instead of using the\n+                            # config, which we set very small with `config_no_init`)\n+                            self.assertLessEqual(v1.data.mean().item(), 1e-1, f\"Issue with {k1}\")\n \n     def test_model_is_small(self):\n         # Just a consistency check to make sure we are not running tests on 1M parameter models."
        }
    ],
    "stats": {
        "total": 781,
        "additions": 214,
        "deletions": 567
    }
}