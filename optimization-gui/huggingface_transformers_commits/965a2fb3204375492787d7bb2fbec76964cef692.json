{
    "author": "Cyrilvallez",
    "message": "More model refactoring! (#35359)\n\n* cohere\r\n\r\n* style\r\n\r\n* phi3\r\n\r\n* style\r\n\r\n* small fix\r\n\r\n* small fix\r\n\r\n* phi3 longrope\r\n\r\n* oups\r\n\r\n* Update rope (only for phi3 still)\r\n\r\n* Update test_modeling_rope_utils.py\r\n\r\n* Update modeling_phi3.py\r\n\r\n* fix\r\n\r\n* fix copies\r\n\r\n* style\r\n\r\n* Fix copied from bad renaming",
    "sha": "965a2fb3204375492787d7bb2fbec76964cef692",
    "files": [
        {
            "sha": "b2d343e0237fa0492208e1a00ec899f548d7d5e4",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 10,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -279,25 +279,20 @@ def _compute_longrope_parameters(\n     # `original_max_position_embeddings` field containing the pretrained value. They use the ratio between these two\n     # values to compute the default attention scaling factor, instead of using `factor`.\n     if hasattr(config, \"original_max_position_embeddings\"):\n-        if seq_len and seq_len < config.original_max_position_embeddings:\n-            expanded_max_position_embeddings = config.original_max_position_embeddings\n-        else:\n-            expanded_max_position_embeddings = config.max_position_embeddings\n-        max_position_embeddings = config.original_max_position_embeddings\n-        factor = expanded_max_position_embeddings / max_position_embeddings\n+        original_max_position_embeddings = config.original_max_position_embeddings\n+        factor = config.max_position_embeddings / config.original_max_position_embeddings\n     else:\n-        max_position_embeddings = config.max_position_embeddings\n-        expanded_max_position_embeddings = max_position_embeddings * factor\n+        original_max_position_embeddings = config.max_position_embeddings\n \n     # Sets the attention factor as suggested in the paper\n     if attention_factor is None:\n         if factor <= 1.0:\n             attention_factor = 1.0\n         else:\n-            attention_factor = math.sqrt(1 + math.log(factor) / math.log(max_position_embeddings))\n+            attention_factor = math.sqrt(1 + math.log(factor) / math.log(original_max_position_embeddings))\n \n     # Compute the inverse frequencies -- scaled based on the target sequence length\n-    if expanded_max_position_embeddings > max_position_embeddings:\n+    if seq_len and seq_len > original_max_position_embeddings:\n         ext_factors = torch.tensor(long_factor, dtype=torch.float32, device=device)\n     else:\n         ext_factors = torch.tensor(short_factor, dtype=torch.float32, device=device)"
        },
        {
            "sha": "cc0aa4dd1346f62874e3b6c2c08b4d04f67b1b7e",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -723,11 +723,7 @@ def _init_weights(self, module):\n \n \n class AriaTextRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: AriaTextConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: AriaTextConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "e17e5ef712cf7a2d5001dc1ccfc5e4d62001bb91",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -120,11 +120,7 @@ def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=\n \n \n class BambaRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: BambaConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: BambaConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "0c752621ce824185b8b9461890070384044f753b",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 140,
            "deletions": 390,
            "changes": 530,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/cohere/modular_cohere.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_cohere.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 Cohere team. All rights reserved.\n #\n@@ -20,45 +26,32 @@\n \n # This file is based on the LLama model definition file in transformers\n \n-\"\"\"PyTorch Cohere model.\"\"\"\n \n-import math\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPast,\n-    CausalLMOutputWithPast,\n-)\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n-from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_2_available,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n from .configuration_cohere import CohereConfig\n \n \n-if is_flash_attn_2_available():\n-    from ...modeling_flash_attention_utils import _flash_attention_forward\n-\n-\n logger = logging.get_logger(__name__)\n-\n _CONFIG_FOR_DOC = \"CohereConfig\"\n \n \n@@ -79,49 +72,17 @@ def forward(self, hidden_states):\n         return hidden_states.to(input_dtype)\n \n \n-ALL_LAYERNORM_LAYERS.append(CohereLayerNorm)\n-\n-\n class CohereRotaryEmbedding(nn.Module):\n-    # Note: the forward pass of this RoPE is slightly different from Llama's, resulting in different `sin`/`cos` for\n-    # the same parameterization. The differences are highlighted with a comment.\n-\n-    def __init__(\n-        self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n-        device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[CohereConfig] = None,\n-    ):\n+    def __init__(self, config: CohereConfig, device=None):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`CohereRotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n@@ -161,7 +122,7 @@ def forward(self, x, position_ids):\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.repeat_interleave(freqs, 2, dim=-1)  # This line differs from Llama's implementation\n+            emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n             cos = emb.cos()\n             sin = emb.sin()\n \n@@ -172,6 +133,60 @@ def forward(self, x, position_ids):\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n+class CohereMLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n def rotate_half(x):\n     # Split and rotate. Note that this function is different from e.g. Llama.\n     x1 = x[..., ::2]\n@@ -210,199 +225,64 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed.to(dtype=dtype), k_embed.to(dtype=dtype)\n \n \n-class CohereMLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-        self.config = config\n-        self.hidden_size = config.hidden_size\n-        self.intermediate_size = config.intermediate_size\n-        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n-        self.act_fn = ACT2FN[config.hidden_act]\n-\n-    # Ignore copy\n-    def forward(self, x):\n-        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n-        return down_proj\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.repeat_kv\n-def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n-    \"\"\"\n-    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n-    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n-    \"\"\"\n-    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n-    if n_rep == 1:\n-        return hidden_states\n-    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n-    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n-\n-\n class CohereAttention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n         self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n-        self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.rope_theta = config.rope_theta\n         self.is_causal = True\n-        self.use_qk_norm = config.use_qk_norm\n-\n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n \n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.use_qk_norm = config.use_qk_norm\n         if self.use_qk_norm:\n             # When sharding the model using Tensor Parallelism, need to be careful to use n_local_heads\n-            self.q_norm = CohereLayerNorm(hidden_size=(self.num_heads, self.head_dim), eps=config.layer_norm_eps)\n-            self.k_norm = CohereLayerNorm(\n-                hidden_size=(self.num_key_value_heads, self.head_dim), eps=config.layer_norm_eps\n+            self.q_norm = CohereLayerNorm(\n+                hidden_size=(config.num_attention_heads, self.head_dim), eps=config.layer_norm_eps\n             )\n-\n-        self.q_proj = nn.Linear(self.hidden_size, self.num_heads * self.head_dim, bias=config.attention_bias)\n-        self.k_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.v_proj = nn.Linear(self.hidden_size, self.num_key_value_heads * self.head_dim, bias=config.attention_bias)\n-        self.o_proj = nn.Linear(self.hidden_size, self.hidden_size, bias=config.attention_bias)\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n-        if self.use_qk_norm:\n-            query_states = self.q_norm(query_states)\n-            key_states = self.k_norm(key_states)\n-\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:  # no matter the length, we just slice it\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights = attn_weights + causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n+            self.k_norm = CohereLayerNorm(\n+                hidden_size=(config.num_key_value_heads, self.head_dim), eps=config.layer_norm_eps\n             )\n \n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-# NO LONGER EXIST Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with Llama->Cohere\n-# TODO cyril: modular\n-class CohereFlashAttention2(CohereAttention):\n-    \"\"\"\n-    Cohere flash attention module. This module inherits from `CohereAttention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n-\n-    # Ignore copy\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-        **kwargs,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if isinstance(past_key_value, StaticCache):\n-            raise ValueError(\n-                \"`static` cache implementation is not compatible with `attn_implementation==flash_attention_2` \"\n-                \"make sure to use `sdpa` in the mean time, and open an issue at https://github.com/huggingface/transformers\"\n-            )\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n+        query_states = self.q_proj(hidden_states).view(hidden_shape)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape)\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n-        if self.use_qk_norm:\n+        if self.use_qk_norm:  # main diff from Llama\n             query_states = self.q_norm(query_states)\n             key_states = self.k_norm(key_states)\n \n         query_states = query_states.transpose(1, 2)\n         key_states = key_states.transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n \n         cos, sin = position_embeddings\n         query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n@@ -412,169 +292,37 @@ def forward(\n             cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        dropout_rate = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32. (CohereLayerNorm handles it correctly)\n-\n-        input_dtype = query_states.dtype\n-        if input_dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n             else:\n-                target_dtype = self.q_proj.weight.dtype\n-\n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            dropout=dropout_rate,\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class CohereSdpaAttention(CohereAttention):\n-    \"\"\"\n-    Cohere attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `CohereAttention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"CohereModel is using CohereSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-                cache_position=cache_position,\n-                position_embeddings=position_embeddings,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        query_states = self.q_proj(hidden_states)\n-        key_states = self.k_proj(hidden_states)\n-        value_states = self.v_proj(hidden_states)\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim)\n-        if self.use_qk_norm:\n-            query_states = self.q_norm(query_states)\n-            key_states = self.k_norm(key_states)\n-\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        cos, sin = position_embeddings\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n-\n-        if past_key_value is not None:\n-            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        # if attention_mask is not None and cache_position is not None:\n-        if attention_mask is not None:\n-            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and causal_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n-\n-\n-COHERE_ATTENTION_CLASSES = {\n-    \"eager\": CohereAttention,\n-    \"flash_attention_2\": CohereFlashAttention2,\n-    \"sdpa\": CohereSdpaAttention,\n-}\n+        return attn_output, attn_weights\n \n \n class CohereDecoderLayer(nn.Module):\n     def __init__(self, config: CohereConfig, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n-\n-        self.self_attn = COHERE_ATTENTION_CLASSES[config._attn_implementation](config=config, layer_idx=layer_idx)\n-\n+        self.self_attn = CohereAttention(config=config, layer_idx=layer_idx)\n         self.mlp = CohereMLP(config)\n         self.input_layernorm = CohereLayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n \n@@ -583,25 +331,26 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n         position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n             hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n             attention_mask (`torch.FloatTensor`, *optional*):\n                 attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n                 query_sequence_length, key_sequence_length)` if default attention is used.\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n@@ -613,7 +362,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states_attention, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states_attention, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -622,6 +371,7 @@ def forward(\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n \n         # Fully Connected\n@@ -631,19 +381,16 @@ def forward(\n         hidden_states = residual + hidden_states_attention + hidden_states_mlp\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n COHERE_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings etc.).\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n \n     This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n     Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n@@ -661,7 +408,6 @@ def forward(\n     \"The bare Cohere Model outputting raw hidden-states without any specific head on top.\",\n     COHERE_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaPreTrainedModel with Llama->Cohere\n class CoherePreTrainedModel(PreTrainedModel):\n     config_class = CohereConfig\n     base_model_prefix = \"model\"\n@@ -754,15 +500,17 @@ def _init_weights(self, module):\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices depicting the position of the input sequence tokens in the sequence. Contrarily to `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n \"\"\"\n \n \n @add_start_docstrings(\n     \"The bare Cohere Model outputting raw hidden-states without any specific head on top.\",\n     COHERE_START_DOCSTRING,\n )\n-# copied from transformers.models.llama.modeling_llama.LlamaModel with Llama->Cohere, LLAMA->COHERE\n-# TODO cyril: modular\n class CohereModel(CoherePreTrainedModel):\n     \"\"\"\n     Transformer decoder consisting of *config.num_hidden_layers* layers. Each layer is a [`CohereDecoderLayer`]\n@@ -771,7 +519,6 @@ class CohereModel(CoherePreTrainedModel):\n         config: CohereConfig\n     \"\"\"\n \n-    # Ignore copy\n     def __init__(self, config: CohereConfig):\n         super().__init__(config)\n         self.padding_idx = config.pad_token_id\n@@ -800,7 +547,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1023,18 +770,21 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with Llama->Cohere\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class CohereForCausalLM(CoherePreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n-    # Ignore copy\n     def __init__(self, config):\n         super().__init__(config)\n         self.model = CohereModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n         self.logit_scale = config.logit_scale\n         self.tie_word_embeddings = config.tie_word_embeddings\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -1056,15 +806,14 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n-    # Ignore copy\n     @add_start_docstrings_to_model_forward(COHERE_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1073,7 +822,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1123,16 +872,17 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n-        logits = logits * self.logit_scale\n+        logits = logits * self.logit_scale  # main diff from Llama\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "b3483103461e9a41a1853c5420a594d840d99732",
            "filename": "src/transformers/models/cohere/modular_cohere.py",
            "status": "added",
            "additions": 393,
            "deletions": 0,
            "changes": 393,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodular_cohere.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -0,0 +1,393 @@\n+# coding=utf-8\n+# Copyright 2024 Cohere team. All rights reserved.\n+#\n+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n+# and OPT implementations in this library. It has been modified from its\n+# original forms to accommodate minor architectural differences compared\n+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+# This file is based on the LLama model definition file in transformers\n+\n+\"\"\"PyTorch Cohere model.\"\"\"\n+\n+from typing import Callable, List, Optional, Tuple, Union\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...cache_utils import Cache\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_outputs import CausalLMOutputWithPast\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n+from ...utils import LossKwargs, logging\n+from ..llama.modeling_llama import (\n+    LlamaAttention,\n+    LlamaForCausalLM,\n+    LlamaMLP,\n+    LlamaModel,\n+    LlamaRotaryEmbedding,\n+    eager_attention_forward,\n+)\n+from .configuration_cohere import CohereConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CONFIG_FOR_DOC = \"CohereConfig\"\n+\n+\n+class CohereLayerNorm(nn.Module):\n+    def __init__(self, hidden_size=None, eps=1e-5, bias=False):\n+        \"\"\"The hidden size can be a tuple or an int. The tuple is used for QKNorm to normalize across head_dim\"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        mean = hidden_states.mean(-1, keepdim=True)\n+        variance = (hidden_states - mean).pow(2).mean(-1, keepdim=True)\n+        hidden_states = (hidden_states - mean) * torch.rsqrt(variance + self.variance_epsilon)\n+        hidden_states = self.weight.to(torch.float32) * hidden_states\n+        return hidden_states.to(input_dtype)\n+\n+\n+ALL_LAYERNORM_LAYERS.append(CohereLayerNorm)\n+\n+\n+class CohereRotaryEmbedding(LlamaRotaryEmbedding):\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+def rotate_half(x):\n+    # Split and rotate. Note that this function is different from e.g. Llama.\n+    x1 = x[..., ::2]\n+    x2 = x[..., 1::2]\n+    rot_x = torch.stack([-x2, x1], dim=-1).flatten(-2)\n+    return rot_x\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    dtype = q.dtype\n+    q = q.float()\n+    k = k.float()\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed.to(dtype=dtype), k_embed.to(dtype=dtype)\n+\n+\n+class CohereMLP(LlamaMLP):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+\n+\n+class CohereAttention(LlamaAttention):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: CohereConfig, layer_idx: Optional[int] = None):\n+        super().__init__(config, layer_idx)\n+        self.use_qk_norm = config.use_qk_norm\n+        if self.use_qk_norm:\n+            # When sharding the model using Tensor Parallelism, need to be careful to use n_local_heads\n+            self.q_norm = CohereLayerNorm(\n+                hidden_size=(config.num_attention_heads, self.head_dim), eps=config.layer_norm_eps\n+            )\n+            self.k_norm = CohereLayerNorm(\n+                hidden_size=(config.num_key_value_heads, self.head_dim), eps=config.layer_norm_eps\n+            )\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_proj(hidden_states).view(hidden_shape)\n+        key_states = self.k_proj(hidden_states).view(hidden_shape)\n+        value_states = self.v_proj(hidden_states).view(hidden_shape)\n+\n+        if self.use_qk_norm:  # main diff from Llama\n+            query_states = self.q_norm(query_states)\n+            key_states = self.k_norm(key_states)\n+\n+        query_states = query_states.transpose(1, 2)\n+        key_states = key_states.transpose(1, 2)\n+        value_states = value_states.transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; position_ids needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class CohereDecoderLayer(nn.Module):\n+    def __init__(self, config: CohereConfig, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = CohereAttention(config=config, layer_idx=layer_idx)\n+        self.mlp = CohereMLP(config)\n+        self.input_layernorm = CohereLayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*):\n+                attention mask of size `(batch_size, sequence_length)` if flash attention is used or `(batch_size, 1,\n+                query_sequence_length, key_sequence_length)` if default attention is used.\n+            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            position_embeddings (`Tuple[torch.FloatTensor, torch.FloatTensor]`, *optional*):\n+                Tuple containing the cosine and sine positional embeddings of shape `(batch_size, seq_len, head_dim)`,\n+                with `head_dim` being the embedding dimension of each attention head.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states_attention, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+\n+        # Fully Connected\n+        hidden_states_mlp = self.mlp(hidden_states)\n+\n+        # Add everything together\n+        hidden_states = residual + hidden_states_attention + hidden_states_mlp\n+\n+        outputs = (hidden_states,)\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+class CohereModel(LlamaModel):\n+    def __init__(self, config: CohereConfig):\n+        super().__init__(config)\n+        self.layers = nn.ModuleList(\n+            [CohereDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.rotary_emb = CohereRotaryEmbedding(config=config)\n+        self.norm = CohereLayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n+\n+\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n+class CohereForCausalLM(LlamaForCausalLM):\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = CohereModel(config)\n+        self.logit_scale = config.logit_scale\n+        self.tie_word_embeddings = config.tie_word_embeddings\n+\n+    def forward(\n+        self,\n+        input_ids: torch.LongTensor = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        num_logits_to_keep: int = 0,\n+        **kwargs: Unpack[KwargsForCausalLM],\n+    ) -> Union[Tuple, CausalLMOutputWithPast]:\n+        r\"\"\"\n+        Args:\n+            labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n+                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+\n+            num_logits_to_keep (`int`, *optional*):\n+                Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all\n+                `input_ids` (special case). Only last token logits are needed for generation, and calculating them only for that\n+                token can save memory, which becomes pretty significant for long sequences or large vocabulary size.\n+\n+        Returns:\n+\n+        Example:\n+\n+        ```python\n+        >> from transformers import AutoTokenizer, CohereForCausalLM\n+\n+        >> model = CohereForCausalLM.from_pretrained(\"CohereForAI/c4ai-command-r-v01\")\n+        >> tokenizer = AutoTokenizer.from_pretrained(\"CohereForAI/c4ai-command-r-v01\")\n+\n+        >> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >> # Generate\n+        >> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n+        outputs = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs[0]\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n+        logits = logits * self.logit_scale  # main diff from Llama\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[1:]\n+            return (loss,) + output if loss is not None else output\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )"
        },
        {
            "sha": "d0463573d2b1d8b68e48ed38c43a03c8ceb6dc63",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 35,
            "deletions": 60,
            "changes": 95,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -28,10 +28,13 @@\n from ...activations import ACT2FN\n from ...cache_utils import Cache, HybridCache\n from ...generation import GenerationMixin\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n     is_flash_attn_2_available,\n@@ -46,50 +49,20 @@\n \n \n logger = logging.get_logger(__name__)\n-\n _CONFIG_FOR_DOC = \"Cohere2Config\"\n \n \n class Cohere2RotaryEmbedding(nn.Module):\n-    # Note: the forward pass of this RoPE is slightly different from Llama's, resulting in different `sin`/`cos` for\n-    # the same parameterization. The differences are highlighted with a comment.\n-\n-    def __init__(\n-        self,\n-        dim=None,\n-        max_position_embeddings=2048,\n-        base=10000,\n-        device=None,\n-        scaling_factor=1.0,\n-        rope_type=\"default\",\n-        config: Optional[Cohere2Config] = None,\n-    ):\n+    def __init__(self, config: Cohere2Config, device=None):\n         super().__init__()\n-        # TODO (joao): remove the `if` below, only used for BC\n         self.rope_kwargs = {}\n-        if config is None:\n-            logger.warning_once(\n-                \"`Cohere2RotaryEmbedding` can now be fully parameterized by passing the model config through the \"\n-                \"`config` argument. All other arguments will be removed in v4.46\"\n-            )\n-            self.rope_kwargs = {\n-                \"rope_type\": rope_type,\n-                \"factor\": scaling_factor,\n-                \"dim\": dim,\n-                \"base\": base,\n-                \"max_position_embeddings\": max_position_embeddings,\n-            }\n-            self.rope_type = rope_type\n-            self.max_seq_len_cached = max_position_embeddings\n-            self.original_max_seq_len = max_position_embeddings\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n         else:\n-            # BC: \"rope_type\" was originally \"type\"\n-            if config.rope_scaling is not None:\n-                self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n-            else:\n-                self.rope_type = \"default\"\n-            self.max_seq_len_cached = config.max_position_embeddings\n-            self.original_max_seq_len = config.max_position_embeddings\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n@@ -129,7 +102,7 @@ def forward(self, x, position_ids):\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.repeat_interleave(freqs, 2, dim=-1)  # This line differs from Llama's implementation\n+            emb = torch.repeat_interleave(freqs, 2, dim=-1)  # diff from Llama: we interleave() instead of cat()\n             cos = emb.cos()\n             sin = emb.sin()\n \n@@ -157,6 +130,18 @@ def forward(self, hidden_states):\n         return hidden_states.to(input_dtype)\n \n \n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n def rotate_half(x):\n     # Split and rotate. Note that this function is different from e.g. Llama.\n     x1 = x[..., ::2]\n@@ -195,18 +180,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed.to(dtype=dtype), k_embed.to(dtype=dtype)\n \n \n-def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n-    \"\"\"\n-    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n-    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n-    \"\"\"\n-    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n-    if n_rep == 1:\n-        return hidden_states\n-    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n-    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n-\n-\n def eager_attention_forward(\n     config: Cohere2Config,\n     query: torch.Tensor,\n@@ -425,7 +398,6 @@ def __init__(self, config):\n         self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n         self.act_fn = ACT2FN[config.hidden_act]\n \n-    # Ignore copy\n     def forward(self, x):\n         down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n         return down_proj\n@@ -436,7 +408,6 @@ def __init__(self, config: Cohere2Config, layer_idx: int):\n         super().__init__()\n         self.hidden_size = config.hidden_size\n         self.self_attn = Cohere2Attention(config, layer_idx)\n-\n         self.mlp = Cohere2MLP(config)\n         self.input_layernorm = Cohere2LayerNorm(hidden_size=(config.hidden_size), eps=config.layer_norm_eps)\n         self.config = config\n@@ -521,7 +492,8 @@ def forward(\n \n COHERE2_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n-    library implements for all its model (such as downloading or saving, resizing the input embeddings etc.).\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n \n     This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n     Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n@@ -874,18 +846,21 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-# TODO: re-enable check: Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM with Llama->Cohere2\n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class Cohere2ForCausalLM(Cohere2PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n-    # Ignore copy\n     def __init__(self, config: Cohere2Config):\n         super().__init__(config)\n         self.model = Cohere2Model(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n         self.logit_scale = config.logit_scale\n         self.tie_word_embeddings = config.tie_word_embeddings\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -907,15 +882,14 @@ def set_decoder(self, decoder):\n     def get_decoder(self):\n         return self.model\n \n-    # Ignore copy\n     @add_start_docstrings_to_model_forward(COHERE2_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -924,7 +898,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -974,16 +948,17 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n             cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         logits = self.lm_head(hidden_states[:, -num_logits_to_keep:, :])\n-        logits = logits * self.logit_scale\n+        logits = logits * self.logit_scale  # main diff from Llama\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]"
        },
        {
            "sha": "b162dfa854e5d6ddab2ee72464184886ff22c92b",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -110,11 +110,7 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Falcon\n class FalconRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: FalconConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: FalconConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "ab698f58231ade4eef80e271b05bee5b67cfc3a8",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -92,11 +92,7 @@ def forward(self, x):\n \n \n class GemmaRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: GemmaConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: GemmaConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "0bf2b154f9c221a5a95e1534ccd4c619bed6d495",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -324,11 +324,7 @@ def forward(\n \n \n class Gemma2RotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: Gemma2Config,\n-        device=None,\n-    ):\n+    def __init__(self, config: Gemma2Config, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "c9add254955da10ebea6b32d20db78e34f8327af",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -62,7 +62,6 @@ def __init__(self, config):\n         self.config = config\n         self.gate_up_proj = nn.Linear(config.hidden_size, 2 * config.intermediate_size, bias=False)\n         self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n-\n         self.activation_fn = ACT2FN[config.hidden_act]\n \n     def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n@@ -256,11 +255,7 @@ def extra_repr(self):\n \n \n class GlmRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: GlmConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: GlmConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "67320a52cb78fabee863a7c8dee29f6ad92a2800",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -491,11 +491,7 @@ def __init__(self, config, layer_idx=None):\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->GPTNeoX\n class GPTNeoXRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: GPTNeoXConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: GPTNeoXConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "3ca81bc5e461ff954a76146cd429bc3064ec8d21",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -225,11 +225,7 @@ def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n \n # Copied from transformers.models.gpt_neox.modeling_gpt_neox.GPTNeoXRotaryEmbedding with GPTNeoX->GPTNeoXJapanese\n class GPTNeoXJapaneseRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: GPTNeoXJapaneseConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: GPTNeoXJapaneseConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "811005505f959fb3cf4fb9cf7a21525e215b900a",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -309,11 +309,7 @@ def forward(\n \n \n class GraniteRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: GraniteConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: GraniteConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "9279e22196e20ddea0bd308435f32e494516f1bf",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -158,11 +158,7 @@ def extra_repr(self):\n \n # Copied from transformers.models.granite.modeling_granite.GraniteRotaryEmbedding with Granite->GraniteMoe\n class GraniteMoeRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: GraniteMoeConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: GraniteMoeConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "e53e438492014a79558575c52f4fef9bef5b8254",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -386,11 +386,7 @@ def extra_repr(self):\n \n # Copied from transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding with Gemma->JetMoe\n class JetMoeRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: JetMoeConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: JetMoeConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "f62efca58b15f0fe2c58a470e92b236e24daa9a4",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -80,11 +80,7 @@ def extra_repr(self):\n \n \n class LlamaRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: LlamaConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: LlamaConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "617d2a571188178d6a2b074b9f44294a89e3c89b",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -365,11 +365,7 @@ def forward(self, x: torch.Tensor):\n \n # Copied from transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding with Mistral->Mimi\n class MimiRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: MimiConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: MimiConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n@@ -1063,7 +1059,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Mimi\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1073,6 +1069,14 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and past_key_values is not None:\n+                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n+                if is_padding_right:\n+                    raise ValueError(\n+                        \"You are attempting to perform batched generation with padding_side='right'\"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of Mimi. Make sure to \"\n+                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n+                    )\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None"
        },
        {
            "sha": "8c0d8af3ece7373da954ac93b4d709e67f6b879f",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -270,11 +270,7 @@ def forward(\n \n \n class MistralRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: MistralConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: MistralConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "a726b69fb6688f486cfaefbe7d0e7caa9c595161",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -392,11 +392,7 @@ def forward(\n \n \n class MixtralRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: MixtralConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: MixtralConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "caf2b980e4ec576f159dacb18eca9a5595bf674d",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 19,
            "deletions": 7,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -308,11 +308,7 @@ def forward(self, x, layer_idx=None):\n \n # Copied from transformers.models.mistral.modeling_mistral.MistralRotaryEmbedding with Mistral->Moshi\n class MoshiRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: MoshiConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: MoshiConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n@@ -1292,7 +1288,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Moshi\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1302,6 +1298,14 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and past_key_values is not None:\n+                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n+                if is_padding_right:\n+                    raise ValueError(\n+                        \"You are attempting to perform batched generation with padding_side='right'\"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of Moshi. Make sure to \"\n+                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n+                    )\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n@@ -1596,7 +1600,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Moshi\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1606,6 +1610,14 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and past_key_values is not None:\n+                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n+                if is_padding_right:\n+                    raise ValueError(\n+                        \"You are attempting to perform batched generation with padding_side='right'\"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of Moshi. Make sure to \"\n+                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n+                    )\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None"
        },
        {
            "sha": "159ca9113ba146b2cab22696cc5cfc1578bb2413",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -274,11 +274,7 @@ def forward(\n \n \n class OlmoRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: OlmoConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: OlmoConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "b0a9ac8f9d1e81e452cfe248f91012775c1cb7e2",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -275,11 +275,7 @@ def forward(\n \n \n class Olmo2RotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: Olmo2Config,\n-        device=None,\n-    ):\n+    def __init__(self, config: Olmo2Config, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "411b25a585ec6bc3b6024689a08d1ea28a5b0904",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -158,11 +158,7 @@ def extra_repr(self):\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Olmoe\n class OlmoeRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: OlmoeConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: OlmoeConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "8851c7b227588e358d3fef0622a209c628d99716",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -57,11 +57,7 @@\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Persimmon\n class PersimmonRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: PersimmonConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: PersimmonConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "f4f6485a04544caeba818a6762270c90be4518cb",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -270,11 +270,7 @@ def forward(\n \n \n class PhiRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: PhiConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: PhiConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "bf3dec351e676e9be63e7d0a31ea2933fb7211ea",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 274,
            "deletions": 632,
            "changes": 906,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -1,3 +1,9 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/phi3/modular_phi3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_phi3.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n # coding=utf-8\n # Copyright 2024 Microsoft and the HuggingFace Inc. team. All rights reserved.\n #\n@@ -13,34 +19,31 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\"\"\"PyTorch Phi-3 model.\"\"\"\n \n-import math\n-import warnings\n-from typing import List, Optional, Tuple, Union\n+from typing import Callable, List, Optional, Tuple, Union\n \n import torch\n-import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, SlidingWindowCache, StaticCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n-from ...modeling_flash_attention_utils import _flash_attention_forward\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import (\n     BaseModelOutputWithPast,\n     CausalLMOutputWithPast,\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n from ...utils import (\n+    LossKwargs,\n     add_code_sample_docstrings,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_flash_attn_greater_or_equal_2_10,\n     logging,\n     replace_return_docstrings,\n )\n@@ -53,194 +56,31 @@\n _CONFIG_FOR_DOC = \"Phi3Config\"\n \n \n-# Copied from transformers.models.llama.modeling_llama.LlamaRMSNorm with Llama->Phi3\n-class Phi3RMSNorm(nn.Module):\n-    def __init__(self, hidden_size, eps=1e-6):\n-        \"\"\"\n-        Phi3RMSNorm is equivalent to T5LayerNorm\n-        \"\"\"\n-        super().__init__()\n-        self.weight = nn.Parameter(torch.ones(hidden_size))\n-        self.variance_epsilon = eps\n-\n-    def forward(self, hidden_states):\n-        input_dtype = hidden_states.dtype\n-        hidden_states = hidden_states.to(torch.float32)\n-        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n-        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n-        return self.weight * hidden_states.to(input_dtype)\n-\n-    def extra_repr(self):\n-        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n-\n-\n-# copied from transformers.models.gemma.modeling_gemma.GemmaRotaryEmbedding with gemma->phi3, Gemma->Phi3\n-# TODO cyril: modular\n-class Phi3RotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+class Phi3MLP(nn.Module):\n+    def __init__(self, config):\n         super().__init__()\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n-        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        self.inv_freq.to(x.device)\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-class Phi3SuScaledRotaryEmbedding(Phi3RotaryEmbedding):\n-    def __init__(self, dim, config, device=None):\n-        warnings.warn(\n-            \"The class Phi3SuScaledRotaryEmbedding is deprecated and will be removed in version 5 of Transformers. Please\"\n-            \" use Phi3LongRoPEScaledRotaryEmbedding instead.\",\n-            FutureWarning,\n-        )\n-        super().__init__(dim, config.max_position_embeddings, config.rope_theta, device)\n-\n-        self.short_factor = config.rope_scaling[\"short_factor\"]\n-        self.long_factor = config.rope_scaling[\"long_factor\"]\n-        self.original_max_position_embeddings = config.original_max_position_embeddings\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.original_max_position_embeddings:\n-            ext_factors = torch.tensor(self.long_factor, dtype=torch.float32, device=x.device)\n-        else:\n-            ext_factors = torch.tensor(self.short_factor, dtype=torch.float32, device=x.device)\n-        inv_freq_shape = torch.arange(0, self.dim, 2, dtype=torch.int64, device=x.device).float() / self.dim\n-        self.inv_freq = 1.0 / (ext_factors * self.base**inv_freq_shape)\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            scale = self.max_position_embeddings / self.original_max_position_embeddings\n-            if scale <= 1.0:\n-                scaling_factor = 1.0\n-            else:\n-                scaling_factor = math.sqrt(1 + math.log(scale) / math.log(self.original_max_position_embeddings))\n-            cos = emb.cos() * scaling_factor\n-            sin = emb.sin() * scaling_factor\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-class Phi3YarnScaledRotaryEmbedding(Phi3RotaryEmbedding):\n-    def __init__(self, dim, config, device=None):\n-        warnings.warn(\n-            \"The class Phi3YarnScaledRotaryEmbedding is deprecated and will be removed in version 5 of Transformers\",\n-            FutureWarning,\n-        )\n-        super().__init__(dim, config.max_position_embeddings, config.rope_theta, device)\n-\n-        self.short_factor = config.rope_scaling[\"short_factor\"]\n-        self.long_factor = config.rope_scaling[\"long_factor\"]\n-        self.original_max_position_embeddings = config.original_max_position_embeddings\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        seq_len = torch.max(position_ids) + 1\n-        if seq_len > self.original_max_position_embeddings:\n-            ext_factors = torch.tensor(self.long_factor, dtype=torch.float32, device=x.device)\n-        else:\n-            ext_factors = torch.tensor(self.short_factor, dtype=torch.float32, device=x.device)\n-\n-        inv_freq_shape = torch.arange(0, self.dim, 2, dtype=torch.int64, device=x.device).float() / self.dim\n-        self.inv_freq = 1.0 / (ext_factors * self.base**inv_freq_shape)\n-\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-\n-            scale = self.max_position_embeddings / self.original_max_position_embeddings\n-            if scale <= 1.0:\n-                scaling_factor = 1.0\n-            else:\n-                scaling_factor = 0.1 * math.log(scale) + 1.0\n-\n-            cos = emb.cos() * scaling_factor\n-            sin = emb.sin() * scaling_factor\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n-\n-\n-class Phi3LongRoPEScaledRotaryEmbedding(Phi3RotaryEmbedding):\n-    def __init__(self, dim, config, device=None):\n-        super().__init__(dim, config.max_position_embeddings, config.rope_theta, device)\n-\n-        self.short_factor = config.rope_scaling[\"short_factor\"]\n-        self.long_factor = config.rope_scaling[\"long_factor\"]\n-        self.original_max_position_embeddings = config.original_max_position_embeddings\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        seq_len = seq_len or torch.max(position_ids) + 1\n-        if seq_len > self.original_max_position_embeddings:\n-            ext_factors = torch.tensor(self.long_factor, dtype=torch.float32, device=x.device)\n-        else:\n-            ext_factors = torch.tensor(self.short_factor, dtype=torch.float32, device=x.device)\n-\n-        inv_freq_shape = torch.arange(0, self.dim, 2, dtype=torch.int64, device=x.device).float() / self.dim\n-        self.inv_freq = 1.0 / (ext_factors * self.base**inv_freq_shape)\n-\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n+        self.config = config\n+        self.gate_up_proj = nn.Linear(config.hidden_size, 2 * config.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n+        self.activation_fn = ACT2FN[config.hidden_act]\n \n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n+    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n+        up_states = self.gate_up_proj(hidden_states)\n \n-            scale = self.max_position_embeddings / self.original_max_position_embeddings\n-            if scale <= 1.0:\n-                scaling_factor = 1.0\n-            else:\n-                scaling_factor = math.sqrt(1 + math.log(scale) / math.log(self.original_max_position_embeddings))\n+        gate, up_states = up_states.chunk(2, dim=-1)\n+        up_states = up_states * self.activation_fn(gate)\n \n-            cos = emb.cos() * scaling_factor\n-            sin = emb.sin() * scaling_factor\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+        return self.down_proj(up_states)\n \n \n-# Copied from transformers.models.llama.modeling_llama.rotate_half\n def rotate_half(x):\n     \"\"\"Rotates half the hidden dims of the input.\"\"\"\n     x1 = x[..., : x.shape[-1] // 2]\n     x2 = x[..., x.shape[-1] // 2 :]\n     return torch.cat((-x2, x1), dim=-1)\n \n \n-# Copied from transformers.models.llama.modeling_llama.apply_rotary_pos_emb\n def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n \n@@ -268,26 +108,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     return q_embed, k_embed\n \n \n-class Phi3MLP(nn.Module):\n-    def __init__(self, config):\n-        super().__init__()\n-\n-        self.config = config\n-        self.gate_up_proj = nn.Linear(config.hidden_size, 2 * config.intermediate_size, bias=False)\n-        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n-\n-        self.activation_fn = ACT2FN[config.hidden_act]\n-\n-    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n-        up_states = self.gate_up_proj(hidden_states)\n-\n-        gate, up_states = up_states.chunk(2, dim=-1)\n-        up_states = up_states * self.activation_fn(gate)\n-\n-        return self.down_proj(up_states)\n-\n-\n-# Copied from transformers.models.llama.modeling_llama.repeat_kv with llama->phi\n def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     \"\"\"\n     This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n@@ -300,381 +120,150 @@ def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n     return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Phi3Attention(nn.Module):\n     \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n \n     def __init__(self, config: Phi3Config, layer_idx: Optional[int] = None):\n         super().__init__()\n         self.config = config\n         self.layer_idx = layer_idx\n-        if layer_idx is None:\n-            logger.warning_once(\n-                f\"Instantiating {self.__class__.__name__} without passing a `layer_idx` is not recommended and will \"\n-                \"lead to errors during the forward call if caching is used. Please make sure to provide a `layer_idx` \"\n-                \"when creating this class.\"\n-            )\n-\n-        self.attention_dropout = config.attention_dropout\n-        self.hidden_size = config.hidden_size\n-        self.num_heads = config.num_attention_heads\n-        self.head_dim = self.hidden_size // self.num_heads\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n         self.num_key_value_heads = config.num_key_value_heads\n-        self.num_key_value_groups = self.num_heads // self.num_key_value_heads\n-        self.max_position_embeddings = config.max_position_embeddings\n-        self.original_max_position_embeddings = config.original_max_position_embeddings\n-        self.rope_theta = config.rope_theta\n-        self.rope_scaling = config.rope_scaling\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n         self.is_causal = True\n \n-        if (self.head_dim * self.num_heads) != self.hidden_size:\n-            raise ValueError(\n-                f\"hidden_size must be divisible by num_heads (got `hidden_size`: {self.hidden_size}\"\n-                f\" and `num_heads`: {self.num_heads}).\"\n-            )\n-\n-        op_size = self.num_heads * self.head_dim + 2 * (self.num_key_value_heads * self.head_dim)\n-        self.o_proj = nn.Linear(self.num_heads * self.head_dim, self.hidden_size, bias=False)\n-        self.qkv_proj = nn.Linear(self.hidden_size, op_size, bias=False)\n-        self._init_rope()\n-\n-    def _init_rope(self):\n-        if self.rope_scaling is None:\n-            self.rotary_emb = Phi3RotaryEmbedding(\n-                self.head_dim,\n-                max_position_embeddings=self.max_position_embeddings,\n-                base=self.rope_theta,\n-            )\n-        else:\n-            scaling_type = self.config.rope_scaling[\"type\"]\n-            if scaling_type == \"longrope\":\n-                self.rotary_emb = Phi3LongRoPEScaledRotaryEmbedding(self.head_dim, self.config)\n-            else:\n-                raise ValueError(f\"Unknown RoPE scaling type {scaling_type}\")\n+        op_size = config.num_attention_heads * self.head_dim + 2 * (config.num_key_value_heads * self.head_dim)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.qkv_proj = nn.Linear(config.hidden_size, op_size, bias=False)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n         past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        logger.warning_once(\"You are not running the flash-attention implementation, expect numerical differences.\")\n-\n-        bsz, q_len, _ = hidden_states.size()\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n \n         qkv = self.qkv_proj(hidden_states)\n-        query_pos = self.num_heads * self.head_dim\n+        query_pos = self.config.num_attention_heads * self.head_dim\n         query_states = qkv[..., :query_pos]\n         key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]\n         value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]\n \n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n \n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n-                )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)\n-\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n \n         if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n             key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n \n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) / math.sqrt(self.head_dim)\n-\n-        if attention_mask is not None:\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-            attn_weights += causal_mask\n-\n-        # upcast attention to fp32\n-        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(value_states.dtype)\n-        attn_weights = nn.functional.dropout(attn_weights, p=self.attention_dropout, training=self.training)\n-\n-        attn_output = torch.matmul(attn_weights, value_states)\n-\n-        if attn_output.size() != (bsz, self.num_heads, q_len, self.head_dim):\n-            raise ValueError(\n-                f\"`attn_output` should be of size {(bsz, self.num_heads, q_len, self.head_dim)}, but is\"\n-                f\" {attn_output.size()}\"\n-            )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-class Phi3FlashAttention2(Phi3Attention):\n-    \"\"\"\n-    Phi-3 flash attention module. This module inherits from `Phi3Attention` as the weights of the module stays\n-    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n-    flash attention and deal with padding tokens in case the input contains any of them.\n-    \"\"\"\n-\n-    def __init__(self, *args, **kwargs):\n-        super().__init__(*args, **kwargs)\n-\n-        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n-        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n-        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n-        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.LongTensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        # Phi3FlashAttention2 attention does not support output_attentions\n-\n-        output_attentions = False\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        qkv = self.qkv_proj(hidden_states)\n-        query_pos = self.num_heads * self.head_dim\n-        query_states = qkv[..., :query_pos]\n-        key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]\n-        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]\n-\n-        # Flash attention requires the input to have the shape\n-        # batch_size x seq_length x head_dim x hidden_dim\n-        # therefore we just need to keep the original shape\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            if self.layer_idx is None:\n-                raise ValueError(\n-                    f\"The cache structure has changed since version v4.36. If you are using {self.__class__.__name__} \"\n-                    \"for auto-regressive decoding with k/v caching, please make sure to initialize the attention class \"\n-                    \"with a layer index.\"\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n                 )\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-\n-        # Because the input can be padded, the absolute sequence length depends on the max position id.\n-        rotary_seq_len = (\n-            max(kv_seq_len, position_ids[:, -1].max().item() + 1) if position_ids is not None else kv_seq_len\n-        )\n-\n-        cos, sin = self.rotary_emb(value_states, seq_len=rotary_seq_len, position_ids=position_ids)\n-\n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        # repeat k/v heads if n_kv_heads < n_heads\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        attn_dropout = self.attention_dropout if self.training else 0.0\n-\n-        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n-        # therefore the input hidden states gets silently casted in float32. Hence, we need\n-        # cast them back in the correct dtype just to be sure everything works as expected.\n-        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n-        # in fp32.\n-\n-        if query_states.dtype == torch.float32:\n-            if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n-            # Handle the case where the model is quantized\n-            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n-                target_dtype = self.config._pre_quantization_dtype\n             else:\n-                target_dtype = self.qkv_proj.weight.dtype\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-            logger.warning_once(\n-                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n-                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n-                f\" {target_dtype}.\"\n-            )\n-\n-            query_states = query_states.to(target_dtype)\n-            key_states = key_states.to(target_dtype)\n-            value_states = value_states.to(target_dtype)\n-\n-        # Reashape to the expected shape for Flash Attention\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n-\n-        attn_output = _flash_attention_forward(\n+        attn_output, attn_weights = attention_interface(\n+            self,\n             query_states,\n             key_states,\n             value_states,\n             attention_mask,\n-            q_len,\n-            position_ids=position_ids,\n-            dropout=attn_dropout,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n             sliding_window=getattr(self.config, \"sliding_window\", None),\n-            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n-            is_causal=self.is_causal,\n+            **kwargs,\n         )\n \n-        attn_output = attn_output.reshape(bsz, q_len, self.hidden_size).contiguous()\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n \n-        if not output_attentions:\n-            attn_weights = None\n-\n-        return attn_output, attn_weights, past_key_value\n-\n-\n-# NO LONGER EXIST copied from transformers.models.llama.modeling_llama.LlamaSdpaAttention with Llama->Phi3\n-# TODO cyril: modular\n-class Phi3SdpaAttention(Phi3Attention):\n-    \"\"\"\n-    Phi3 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n-    `Phi3Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n-    SDPA API.\n-    \"\"\"\n-\n-    # Adapted from Phi3Attention.forward\n-    def forward(\n-        self,\n-        hidden_states: torch.Tensor,\n-        attention_mask: Optional[torch.Tensor] = None,\n-        position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Cache] = None,\n-        output_attentions: bool = False,\n-        use_cache: bool = False,\n-        cache_position: Optional[torch.LongTensor] = None,\n-    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Phi3Model is using Phi3SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                attention_mask=attention_mask,\n-                position_ids=position_ids,\n-                past_key_value=past_key_value,\n-                output_attentions=output_attentions,\n-                use_cache=use_cache,\n-            )\n-\n-        bsz, q_len, _ = hidden_states.size()\n-\n-        qkv = self.qkv_proj(hidden_states)\n-        query_pos = self.num_heads * self.head_dim\n-        query_states = qkv[..., :query_pos]\n-        key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]\n-        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]\n-\n-        query_states = query_states.view(bsz, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(bsz, q_len, self.num_key_value_heads, self.head_dim).transpose(1, 2)\n-\n-        kv_seq_len = key_states.shape[-2]\n-        if past_key_value is not None:\n-            kv_seq_len += past_key_value.get_usable_length(kv_seq_len, self.layer_idx)\n-        cos, sin = self.rotary_emb(value_states, position_ids, seq_len=kv_seq_len)\n \n-        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin, position_ids)\n-\n-        if past_key_value is not None:\n-            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}  # Specific to RoPE models\n-            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n-\n-        key_states = repeat_kv(key_states, self.num_key_value_groups)\n-        value_states = repeat_kv(value_states, self.num_key_value_groups)\n-\n-        causal_mask = attention_mask\n-        if attention_mask is not None:\n-            causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n-\n-        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n-        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n-        if query_states.device.type == \"cuda\" and attention_mask is not None:\n-            query_states = query_states.contiguous()\n-            key_states = key_states.contiguous()\n-            value_states = value_states.contiguous()\n-\n-        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n-        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n-        # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if causal_mask is None and q_len > 1 else False\n-\n-        attn_output = torch.nn.functional.scaled_dot_product_attention(\n-            query_states,\n-            key_states,\n-            value_states,\n-            attn_mask=causal_mask,\n-            dropout_p=self.attention_dropout if self.training else 0.0,\n-            is_causal=is_causal,\n-        )\n-\n-        attn_output = attn_output.transpose(1, 2).contiguous()\n-        attn_output = attn_output.view(bsz, q_len, self.hidden_size)\n-\n-        attn_output = self.o_proj(attn_output)\n-\n-        return attn_output, None, past_key_value\n+class Phi3RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Phi3RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n \n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return self.weight * hidden_states.to(input_dtype)\n \n-PHI3_ATTENTION_CLASSES = {\n-    \"eager\": Phi3Attention,\n-    \"flash_attention_2\": Phi3FlashAttention2,\n-    \"sdpa\": Phi3SdpaAttention,\n-}\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n \n \n class Phi3DecoderLayer(nn.Module):\n     def __init__(self, config: Phi3Config, layer_idx: int):\n         super().__init__()\n-\n-        self.config = config\n-        self.self_attn = PHI3_ATTENTION_CLASSES[config._attn_implementation](config, layer_idx=layer_idx)\n-\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = Phi3Attention(config=config, layer_idx=layer_idx)\n         self.mlp = Phi3MLP(config)\n         self.input_layernorm = Phi3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.post_attention_layernorm = Phi3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.config = config\n         self.resid_attn_dropout = nn.Dropout(config.resid_pdrop)\n         self.resid_mlp_dropout = nn.Dropout(config.resid_pdrop)\n-        self.post_attention_layernorm = Phi3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n \n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[Tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        **kwargs,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n         \"\"\"\n         Args:\n@@ -685,53 +274,128 @@ def forward(\n             position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n                 Indices of positions of each input sequence tokens in the position embeddings. Selected in the range\n                 `[0, config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+            past_key_value (`Cache`, *optional*): cached past key and value projection states\n             output_attentions (`bool`, *optional*):\n                 Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n                 returned tensors for more detail.\n             use_cache (`bool`, *optional*):\n                 If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n                 (see `past_key_values`).\n-            past_key_value (`Tuple(torch.FloatTensor)`, *optional*): cached past key and value projection states\n             cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n                 Indices depicting the position of the input sequence tokens in the sequence\n             kwargs (`dict`, *optional*):\n                 Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n                 into the model\n         \"\"\"\n-\n         residual = hidden_states\n \n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        attn_outputs, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_value=past_key_value,\n             output_attentions=output_attentions,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n-\n-        hidden_states = residual + self.resid_attn_dropout(attn_outputs)\n+        hidden_states = residual + self.resid_attn_dropout(hidden_states)  # main diff with Llama\n \n         residual = hidden_states\n         hidden_states = self.post_attention_layernorm(hidden_states)\n         hidden_states = self.mlp(hidden_states)\n-        hidden_states = residual + self.resid_mlp_dropout(hidden_states)\n+        hidden_states = residual + self.resid_mlp_dropout(hidden_states)  # main diff with Llama\n \n         outputs = (hidden_states,)\n-\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n+class Phi3RotaryEmbedding(nn.Module):\n+    def __init__(self, config: Phi3Config, device=None):\n+        super().__init__()\n+        self.rope_kwargs = {}\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(\n+                self.config, device, seq_len=seq_len, **self.rope_kwargs\n+            )\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n+\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+        elif self.rope_type == \"longrope\":\n+            self._longrope_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+    def _longrope_frequency_update(self, position_ids, device):\n+        \"\"\"Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.\"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if hasattr(self.config, \"original_max_position_embeddings\"):\n+            original_max_position_embeddings = self.config.original_max_position_embeddings\n+        else:\n+            original_max_position_embeddings = self.config.max_position_embeddings\n+        if seq_len > original_max_position_embeddings:\n+            if not hasattr(self, \"long_inv_freq\"):\n+                self.long_inv_freq, _ = self.rope_init_fn(\n+                    self.config, device, seq_len=original_max_position_embeddings + 1\n+                )\n+            self.register_buffer(\"inv_freq\", self.long_inv_freq, persistent=False)\n+        else:\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+\n+\n PHI3_START_DOCSTRING = r\"\"\"\n     This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n     library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n@@ -750,19 +414,20 @@ def forward(\n \n \n @add_start_docstrings(\n-    \"The bare Phi-3 model outputting raw hidden-states without any specific head on top.\",\n+    \"The bare Phi3 Model outputting raw hidden-states without any specific head on top.\",\n     PHI3_START_DOCSTRING,\n )\n class Phi3PreTrainedModel(PreTrainedModel):\n     config_class = Phi3Config\n     base_model_prefix = \"model\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Phi3DecoderLayer\"]\n-    _skip_keys_device_placement = \"past_key_values\"\n+    _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n     _supports_cache_class = True\n-\n+    _supports_quantized_cache = True\n+    _supports_static_cache = True\n     _version = \"0.0.5\"\n \n     def _init_weights(self, module):\n@@ -853,7 +518,7 @@ def _init_weights(self, module):\n \n \n @add_start_docstrings(\n-    \"The bare Phi-3 model outputting raw hidden-states without any specific head on top.\",\n+    \"The bare Phi3 Model outputting raw hidden-states without any specific head on top.\",\n     PHI3_START_DOCSTRING,\n )\n class Phi3Model(Phi3PreTrainedModel):\n@@ -870,14 +535,13 @@ def __init__(self, config: Phi3Config):\n         self.vocab_size = config.vocab_size\n \n         self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n-        self.embed_dropout = nn.Dropout(config.embd_pdrop)\n         self.layers = nn.ModuleList(\n             [Phi3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n-        self._attn_implementation = config._attn_implementation\n         self.norm = Phi3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n-\n+        self.rotary_emb = Phi3RotaryEmbedding(config=config)\n         self.gradient_checkpointing = False\n+\n         # Initialize weights and apply final processing\n         self.post_init()\n \n@@ -893,54 +557,43 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n+        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n     ) -> Union[Tuple, BaseModelOutputWithPast]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n         )\n         use_cache = use_cache if use_cache is not None else self.config.use_cache\n-\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if self.gradient_checkpointing and self.training:\n-            if use_cache:\n-                logger.warning_once(\n-                    \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\"\n-                )\n-                use_cache = False\n-\n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        if self.gradient_checkpointing and self.training and use_cache:\n+            logger.warning_once(\n+                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n+            )\n+            use_cache = False\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n+\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n                 past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n             )\n+\n         if position_ids is None:\n             position_ids = cache_position.unsqueeze(0)\n \n@@ -950,12 +603,14 @@ def forward(\n \n         hidden_states = inputs_embeds\n \n+        # create position embeddings to be shared across the decoder layers\n+        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n+\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n-        for decoder_layer in self.layers:\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n                 all_hidden_states += (hidden_states,)\n \n@@ -969,6 +624,7 @@ def forward(\n                     output_attentions,\n                     use_cache,\n                     cache_position,\n+                    position_embeddings,\n                 )\n             else:\n                 layer_outputs = decoder_layer(\n@@ -979,13 +635,12 @@ def forward(\n                     output_attentions=output_attentions,\n                     use_cache=use_cache,\n                     cache_position=cache_position,\n+                    position_embeddings=position_embeddings,\n+                    **flash_attn_kwargs,\n                 )\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -995,18 +650,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n-        if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n-        return BaseModelOutputWithPast(\n+        output = BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values if use_cache else None,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n+        return output if return_dict else output.to_tuple()\n \n     def _update_causal_mask(\n         self,\n@@ -1017,6 +667,14 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and past_key_values is not None:\n+                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n+                if is_padding_right:\n+                    raise ValueError(\n+                        \"You are attempting to perform batched generation with padding_side='right'\"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of Phi3. Make sure to \"\n+                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n+                    )\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n@@ -1084,7 +742,6 @@ def _update_causal_mask(\n         return causal_mask\n \n     @staticmethod\n-    # Copied from transformers.models.mistral.modeling_mistral.MistralModel._prepare_4d_causal_attention_mask_with_cache_position with Mistral->Phi3\n     def _prepare_4d_causal_attention_mask_with_cache_position(\n         attention_mask: torch.Tensor,\n         sequence_length: int,\n@@ -1152,10 +809,13 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n+class KwargsForCausalLM(FlashAttentionKwargs, LossKwargs): ...\n+\n+\n class Phi3ForCausalLM(Phi3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.__init__ with Llama->Phi3\n     def __init__(self, config):\n         super().__init__(config)\n         self.model = Phi3Model(config)\n@@ -1165,39 +825,32 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.model.embed_tokens\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_input_embeddings\n     def set_input_embeddings(self, value):\n         self.model.embed_tokens = value\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_output_embeddings\n     def get_output_embeddings(self):\n         return self.lm_head\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_output_embeddings\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_head = new_embeddings\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_decoder\n     def get_decoder(self):\n         return self.model\n \n-    # Ignore copy\n     @add_start_docstrings_to_model_forward(PHI3_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=CausalLMOutputWithPast, config_class=_CONFIG_FOR_DOC)\n     def forward(\n         self,\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Union[Cache, List[torch.FloatTensor]]] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1206,7 +859,7 @@ def forward(\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         num_logits_to_keep: int = 0,\n-        **loss_kwargs,\n+        **kwargs: Unpack[KwargsForCausalLM],\n     ) -> Union[Tuple, CausalLMOutputWithPast]:\n         r\"\"\"\n         Args:\n@@ -1227,27 +880,17 @@ def forward(\n         ```python\n         >>> from transformers import AutoTokenizer, Phi3ForCausalLM\n \n-        >>> model = Phi3ForCausalLM.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"microsoft/phi-3-mini-4k-instruct\")\n+        >>> model = Phi3ForCausalLM.from_pretrained(\"meta-phi3/Phi3-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-phi3/Phi3-2-7b-hf\")\n \n-        >>> prompt = \"This is an example script .\"\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n         >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n \n         >>> # Generate\n         >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n         >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-        'This is an example script .\\n Certainly! Below is a sample script that demonstrates a simple task, such as calculating the sum'\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n         ```\"\"\"\n-        if (\n-            use_cache\n-            and self.config.rope_scaling\n-            and cache_position is not None\n-            and cache_position[0] == self.config.original_max_position_embeddings\n-        ):\n-            logger.warning(\n-                f\"If you are not using the generate method, you may encounter nonsensical outputs after the {self.config.original_max_position_embeddings}th token, as the KV cache needs to be recomputed.\"\n-            )\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -1265,6 +908,8 @@ def forward(\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n+            **kwargs,\n         )\n \n         hidden_states = outputs[0]\n@@ -1273,7 +918,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            loss = self.loss_function(logits, labels, self.vocab_size, **loss_kwargs)\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if not return_dict:\n             output = (logits,) + outputs[1:]\n@@ -1329,7 +974,7 @@ def prepare_inputs_for_generation(\n \n @add_start_docstrings(\n     \"\"\"\n-    The [`Phi3Model`] with a sequence classification head on top (linear layer).\n+    The Phi3 Model transformer with a sequence classification head on top (linear layer).\n \n     [`Phi3ForSequenceClassification`] uses the last token in order to do the classification, as other causal models\n     (e.g. GPT-2) do.\n@@ -1342,7 +987,6 @@ def prepare_inputs_for_generation(\n     \"\"\",\n     PHI3_START_DOCSTRING,\n )\n-# Copied from transformers.models.llama.modeling_llama.LlamaForSequenceClassification with Llama->Phi3, LLAMA->PHI3, self.transformer->self.model, transformer_outputs->model_outputs\n class Phi3ForSequenceClassification(Phi3PreTrainedModel):\n     def __init__(self, config):\n         super().__init__(config)\n@@ -1381,7 +1025,7 @@ def forward(\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        model_outputs = self.model(\n+        transformer_outputs = self.model(\n             input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1392,7 +1036,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n         )\n-        hidden_states = model_outputs[0]\n+        hidden_states = transformer_outputs[0]\n         logits = self.score(hidden_states)\n \n         if input_ids is not None:\n@@ -1420,44 +1064,48 @@ def forward(\n             loss = self.loss_function(logits=logits, labels=labels, pooled_logits=pooled_logits, config=self.config)\n \n         if not return_dict:\n-            output = (pooled_logits,) + model_outputs[1:]\n+            output = (pooled_logits,) + transformer_outputs[1:]\n             return ((loss,) + output) if loss is not None else output\n \n         return SequenceClassifierOutputWithPast(\n             loss=loss,\n             logits=pooled_logits,\n-            past_key_values=model_outputs.past_key_values,\n-            hidden_states=model_outputs.hidden_states,\n-            attentions=model_outputs.attentions,\n+            past_key_values=transformer_outputs.past_key_values,\n+            hidden_states=transformer_outputs.hidden_states,\n+            attentions=transformer_outputs.attentions,\n         )\n \n \n @add_start_docstrings(\n     \"\"\"\n-    [`Phi3Model`] with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for\n-    Named-Entity-Recognition (NER) tasks.\n+    The Phi3 Model transformer with a token classification head on top (a linear layer on top of the hidden-states\n+    output) e.g. for Named-Entity-Recognition (NER) tasks.\n     \"\"\",\n     PHI3_START_DOCSTRING,\n )\n-# Copied from transformers.models.mpt.modeling_mpt.MptForTokenClassification with Mpt->Phi3,MPT->PHI3,self.transformer->self.model,transformer_outputs->model_outputs\n class Phi3ForTokenClassification(Phi3PreTrainedModel):\n-    def __init__(self, config: Phi3Config):\n+    def __init__(self, config):\n         super().__init__(config)\n         self.num_labels = config.num_labels\n-\n         self.model = Phi3Model(config)\n-        if hasattr(config, \"classifier_dropout\") and config.classifier_dropout is not None:\n+        if getattr(config, \"classifier_dropout\", None) is not None:\n             classifier_dropout = config.classifier_dropout\n-        elif hasattr(config, \"hidden_dropout\") and config.hidden_dropout is not None:\n+        elif getattr(config, \"hidden_dropout\", None) is not None:\n             classifier_dropout = config.hidden_dropout\n         else:\n             classifier_dropout = 0.1\n         self.dropout = nn.Dropout(classifier_dropout)\n-        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n+        self.score = nn.Linear(config.hidden_size, config.num_labels)\n \n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_input_embeddings(self):\n+        return self.model.embed_tokens\n+\n+    def set_input_embeddings(self, value):\n+        self.model.embed_tokens = value\n+\n     @add_start_docstrings_to_model_forward(PHI3_INPUTS_DOCSTRING)\n     @add_code_sample_docstrings(\n         checkpoint=_CHECKPOINT_FOR_DOC,\n@@ -1467,16 +1115,16 @@ def __init__(self, config: Phi3Config):\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[Tuple[Tuple[torch.Tensor, torch.Tensor], ...]] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n-        inputs_embeds: Optional[torch.Tensor] = None,\n-        labels: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[List[torch.FloatTensor]] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n-        **deprecated_arguments,\n-    ) -> Union[Tuple[torch.Tensor], TokenClassifierOutput]:\n+    ) -> Union[Tuple, TokenClassifierOutput]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n             Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\n@@ -1485,40 +1133,34 @@ def forward(\n         \"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n-        model_outputs = self.model(\n+        outputs = self.model(\n             input_ids,\n-            past_key_values=past_key_values,\n             attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n         )\n-\n-        hidden_states = model_outputs[0]\n-        hidden_states = self.dropout(hidden_states)\n-        logits = self.classifier(hidden_states)\n+        sequence_output = outputs[0]\n+        sequence_output = self.dropout(sequence_output)\n+        logits = self.score(sequence_output)\n \n         loss = None\n         if labels is not None:\n-            # move labels to correct device to enable model parallelism\n-            labels = labels.to(logits.device)\n-            batch_size, seq_length = labels.shape\n-            loss_fct = CrossEntropyLoss()\n-            loss = loss_fct(\n-                logits.view(batch_size * seq_length, self.num_labels), labels.view(batch_size * seq_length)\n-            )\n+            loss = self.loss_function(logits, labels, self.config)\n \n         if not return_dict:\n-            output = (logits,) + model_outputs[2:]\n+            output = (logits,) + outputs[2:]\n             return ((loss,) + output) if loss is not None else output\n \n         return TokenClassifierOutput(\n             loss=loss,\n             logits=logits,\n-            hidden_states=model_outputs.hidden_states,\n-            attentions=model_outputs.attentions,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n         )\n \n "
        },
        {
            "sha": "04941f181dfd7549ec71d4a36c4f852847295ae4",
            "filename": "src/transformers/models/phi3/modular_phi3.py",
            "status": "added",
            "additions": 311,
            "deletions": 0,
            "changes": 311,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -0,0 +1,311 @@\n+# coding=utf-8\n+# Copyright 2024 Microsoft and the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\"\"\"PyTorch Phi-3 model.\"\"\"\n+\n+from typing import Callable, Optional, Tuple\n+\n+import torch\n+import torch.utils.checkpoint\n+from torch import nn\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ...utils import logging\n+from ..mistral.modeling_mistral import (\n+    MistralDecoderLayer,\n+    MistralForCausalLM,\n+    MistralForSequenceClassification,\n+    MistralForTokenClassification,\n+    MistralPreTrainedModel,\n+    MistralRotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+from .configuration_phi3 import Phi3Config\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+_CHECKPOINT_FOR_DOC = \"microsoft/Phi-3-mini-4k-instruct\"\n+_CONFIG_FOR_DOC = \"Phi3Config\"\n+\n+\n+class Phi3MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+\n+        self.config = config\n+        self.gate_up_proj = nn.Linear(config.hidden_size, 2 * config.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_size, bias=False)\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, hidden_states: torch.FloatTensor) -> torch.FloatTensor:\n+        up_states = self.gate_up_proj(hidden_states)\n+\n+        gate, up_states = up_states.chunk(2, dim=-1)\n+        up_states = up_states * self.activation_fn(gate)\n+\n+        return self.down_proj(up_states)\n+\n+\n+class Phi3Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Phi3Config, layer_idx: Optional[int] = None):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.num_key_value_heads = config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        op_size = config.num_attention_heads * self.head_dim + 2 * (config.num_key_value_heads * self.head_dim)\n+        self.o_proj = nn.Linear(config.num_attention_heads * self.head_dim, config.hidden_size, bias=False)\n+        self.qkv_proj = nn.Linear(config.hidden_size, op_size, bias=False)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_value: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        qkv = self.qkv_proj(hidden_states)\n+        query_pos = self.config.num_attention_heads * self.head_dim\n+        query_states = qkv[..., :query_pos]\n+        key_states = qkv[..., query_pos : query_pos + self.num_key_value_heads * self.head_dim]\n+        value_states = qkv[..., query_pos + self.num_key_value_heads * self.head_dim :]\n+\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_value is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=getattr(self.config, \"sliding_window\", None),\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Phi3DecoderLayer(MistralDecoderLayer):\n+    def __init__(self, config: Phi3Config, layer_idx: int):\n+        super().__init__(config, layer_idx)\n+        self.config = config\n+        self.self_attn = Phi3Attention(config=config, layer_idx=layer_idx)\n+        self.mlp = Phi3MLP(config)\n+        self.resid_attn_dropout = nn.Dropout(config.resid_pdrop)\n+        self.resid_mlp_dropout = nn.Dropout(config.resid_pdrop)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_value: Optional[Cache] = None,\n+        output_attentions: Optional[bool] = False,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[FlashAttentionKwargs],\n+    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`):\n+                input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`, *optional*): attention mask of size\n+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n+            position_ids (`torch.LongTensor` of shape `({0})`, *optional*):\n+                Indices of positions of each input sequence tokens in the position embeddings. Selected in the range\n+                `[0, config.n_positions - 1]`. [What are position IDs?](../glossary#position-ids)\n+            past_key_value (`Cache`, *optional*): cached past key and value projection states\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            use_cache (`bool`, *optional*):\n+                If set to `True`, `past_key_values` key value states are returned and can be used to speed up decoding\n+                (see `past_key_values`).\n+            cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+                Indices depicting the position of the input sequence tokens in the sequence\n+            kwargs (`dict`, *optional*):\n+                Arbitrary kwargs to be ignored, used for FSDP and other methods that injects code\n+                into the model\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.input_layernorm(hidden_states)\n+\n+        # Self Attention\n+        hidden_states, self_attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_value=past_key_value,\n+            output_attentions=output_attentions,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = residual + self.resid_attn_dropout(hidden_states)  # main diff with Llama\n+\n+        residual = hidden_states\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + self.resid_mlp_dropout(hidden_states)  # main diff with Llama\n+\n+        outputs = (hidden_states,)\n+        if output_attentions:\n+            outputs += (self_attn_weights,)\n+\n+        return outputs\n+\n+\n+class Phi3RotaryEmbedding(MistralRotaryEmbedding):\n+    def __init__(self, config: Phi3Config, device=None):\n+        super().__init__(config, device)\n+\n+    def _longrope_frequency_update(self, position_ids, device):\n+        \"\"\"Longrope uses long factor if sequence is larger than original pretraining length, short otherwise.\"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if hasattr(self.config, \"original_max_position_embeddings\"):\n+            original_max_position_embeddings = self.config.original_max_position_embeddings\n+        else:\n+            original_max_position_embeddings = self.config.max_position_embeddings\n+        if seq_len > original_max_position_embeddings:\n+            if not hasattr(self, \"long_inv_freq\"):\n+                self.long_inv_freq, _ = self.rope_init_fn(\n+                    self.config, device, seq_len=original_max_position_embeddings + 1\n+                )\n+            self.register_buffer(\"inv_freq\", self.long_inv_freq, persistent=False)\n+        else:\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+\n+    @torch.no_grad()\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+        elif self.rope_type == \"longrope\":\n+            self._longrope_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n+        device_type = x.device.type\n+        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos()\n+            sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n+        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+\n+\n+class Phi3PreTrainedModel(MistralPreTrainedModel):\n+    _version = \"0.0.5\"\n+\n+\n+class Phi3ForCausalLM(MistralForCausalLM, Phi3PreTrainedModel):\n+    def prepare_inputs_for_generation(\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        position_ids=None,\n+        use_cache=True,\n+        num_logits_to_keep=None,\n+        **kwargs,\n+    ):\n+        # Overwritten -- this model may need to switch between short and long rope, invalidating the cache in the\n+        # process\n+\n+        # When the first time input length reached long and short factor switching point, enforce re-compute cache\n+        # It will cause downside of slower at this single token position, however, better than current failure.\n+        if (\n+            past_key_values\n+            and self.config.rope_scaling\n+            and input_ids.shape[1] >= self.config.original_max_position_embeddings + 1\n+        ):\n+            past_length = cache_position[0]\n+            if past_length <= self.config.original_max_position_embeddings:\n+                past_key_values = None\n+\n+        model_inputs = Phi3PreTrainedModel().prepare_inputs_for_generation(\n+            input_ids=input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            use_cache=use_cache,\n+            num_logits_to_keep=num_logits_to_keep,\n+            **kwargs,\n+        )\n+        return model_inputs\n+\n+\n+class Phi3ForSequenceClassification(MistralForSequenceClassification):\n+    pass\n+\n+\n+class Phi3ForTokenClassification(MistralForTokenClassification):\n+    pass"
        },
        {
            "sha": "b8b05d93811e615ab0dd19d199a935debcf86baa",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -1173,7 +1173,7 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Phimoe\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1183,6 +1183,14 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and past_key_values is not None:\n+                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n+                if is_padding_right:\n+                    raise ValueError(\n+                        \"You are attempting to perform batched generation with padding_side='right'\"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of Phimoe. Make sure to \"\n+                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n+                    )\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None"
        },
        {
            "sha": "5ec1b6bdae9ce3cb642c97db34d64e50b33759ef",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -283,11 +283,7 @@ def forward(\n \n \n class Qwen2RotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: Qwen2Config,\n-        device=None,\n-    ):\n+    def __init__(self, config: Qwen2Config, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "0cb12f07a53f3d0421e9ce65d8dd8d822770d3df",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 10,
            "deletions": 6,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -167,11 +167,7 @@ def extra_repr(self):\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->Qwen2Moe\n class Qwen2MoeRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: Qwen2MoeConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: Qwen2MoeConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n@@ -1064,7 +1060,7 @@ def forward(\n             router_logits=all_router_logits,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Qwen2Moe\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1074,6 +1070,14 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and past_key_values is not None:\n+                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n+                if is_padding_right:\n+                    raise ValueError(\n+                        \"You are attempting to perform batched generation with padding_side='right'\"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen2Moe. Make sure to \"\n+                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n+                    )\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None"
        },
        {
            "sha": "4893143e2c1664530c89b82ba9b5838892e97c53",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -1160,7 +1160,7 @@ def forward(\n             attentions=all_self_attns,\n         )\n \n-    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask\n+    # Copied from transformers.models.phi3.modeling_phi3.Phi3Model._update_causal_mask with Phi3->Qwen2VL\n     def _update_causal_mask(\n         self,\n         attention_mask: torch.Tensor,\n@@ -1170,6 +1170,14 @@ def _update_causal_mask(\n         output_attentions: bool,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n+            if attention_mask is not None and past_key_values is not None:\n+                is_padding_right = attention_mask[:, -1].sum().item() != input_tensor.size()[0]\n+                if is_padding_right:\n+                    raise ValueError(\n+                        \"You are attempting to perform batched generation with padding_side='right'\"\n+                        \" this may lead to unexpected behaviour for Flash Attention version of Qwen2VL. Make sure to \"\n+                        \" call `tokenizer.padding_side  = 'left'` before tokenizing the input. \"\n+                    )\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None"
        },
        {
            "sha": "b4aa52d5c19af35869ce19b8c83c563fa2eb29b8",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -63,11 +63,7 @@\n \n # Copied from transformers.models.llama.modeling_llama.LlamaRotaryEmbedding with Llama->StableLm\n class StableLmRotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: StableLmConfig,\n-        device=None,\n-    ):\n+    def __init__(self, config: StableLmConfig, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "a510ca1e1ca26a0e0e0b1cc597d534fd934cb6ba",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -274,11 +274,7 @@ def forward(\n \n \n class Starcoder2RotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: Starcoder2Config,\n-        device=None,\n-    ):\n+    def __init__(self, config: Starcoder2Config, device=None):\n         super().__init__()\n         self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\""
        },
        {
            "sha": "6ec663c6636fa6b81c548855efae0897c115a131",
            "filename": "tests/models/phi3/test_modeling_phi3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi3%2Ftest_modeling_phi3.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -459,6 +459,9 @@ def test_model_rope_scaling_short_long_factor(self, scaling_type):\n             \"long_factor\": [5.0 for _ in range(n_factors)],\n         }\n         input_tensor = ids_tensor([1, 4090], config.vocab_size)\n+        # Make sure we don't have padding tokens. If this is the case, then the actual number of \"true\" tokens may be shorter\n+        # than `config.original_max_position_embeddings + 5`, invalidating this test\n+        input_tensor[input_tensor == config.pad_token_id] += 1\n         model = Phi3ForCausalLM(config)\n         model.to(torch_device)\n         model.eval()"
        },
        {
            "sha": "9fe7d21b2265acc9e069802fb672a62e458160b6",
            "filename": "tests/utils/test_modeling_rope_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 16,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_rope_utils.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -311,10 +311,10 @@ def test_longrope_rope_numerically(self):\n         self.assertEqual(config.rope_theta, 10000.0)\n         self.assertFalse(hasattr(config, \"partial_rotary_factor\"))\n \n-        # longrope applies scaling on EACH inv frequency, `short_factor` or `long_factor`, depending on `factor`\n+        # longrope applies scaling on EACH inv frequency, `short_factor` or `long_factor`, depending on the seq_len\n         dim = config.hidden_size // config.num_attention_heads\n-        short_factor = [2.0] * (dim // 2)  # scaling applied when factor == 1.0\n-        long_factor = torch.ones(dim // 2).cumsum(0).tolist()  # scaling applied when factor > 1.0\n+        short_factor = [2.0] * (dim // 2)  # scaling applied when seq_len <= max_position_embeddings\n+        long_factor = torch.ones(dim // 2).cumsum(0).tolist()  # scaling applied when seq_len > max_position_embeddings\n \n         rope_fn = ROPE_INIT_FUNCTIONS[\"default\"]\n         default_inv_freq, _ = rope_fn(config=config, device=torch_device)\n@@ -353,26 +353,18 @@ def test_longrope_rope_numerically(self):\n             # Verify that \"TypeError: '<' not supported between instances of 'NoneType' and 'int'\" is not raised.\n             rope_config_validation(config)\n \n-        # Check 2: Factor == 1.0 -> short factor is applied to the default frequencies\n-        factor = 1.0\n+        # Check 2: seq_len == 0 -> short factor is applied to the default frequencies\n         config.rope_scaling = {\n             \"rope_type\": \"longrope\",\n-            \"factor\": factor,\n+            \"factor\": 1.0,\n             \"short_factor\": short_factor,\n             \"long_factor\": long_factor,\n         }\n-        inv_freq, _ = rope_fn(config=config, device=torch_device)\n+        inv_freq, _ = rope_fn(config=config, device=torch_device, seq_len=0)\n         torch.testing.assert_close(inv_freq, default_inv_freq / torch.tensor(short_factor).to(torch_device))\n \n-        # Check 3: Factor > 1.0 -> long factor is applied to the default frequencies\n-        factor = 10.0\n-        config.rope_scaling = {\n-            \"rope_type\": \"longrope\",\n-            \"factor\": factor,\n-            \"short_factor\": short_factor,\n-            \"long_factor\": long_factor,\n-        }\n-        inv_freq, _ = rope_fn(config=config, device=torch_device)\n+        # Check 3: seq_len > max_position_embeddings -> long factor is applied to the default frequencies\n+        inv_freq, _ = rope_fn(config=config, device=torch_device, seq_len=config.max_position_embeddings + 1)\n         torch.testing.assert_close(inv_freq, default_inv_freq / torch.tensor(long_factor).to(torch_device))\n \n     def test_llama3_rope_numerically(self):"
        },
        {
            "sha": "9be492476c1e2d9ddec736b441d60d4d3e4d803d",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/965a2fb3204375492787d7bb2fbec76964cef692/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/965a2fb3204375492787d7bb2fbec76964cef692/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=965a2fb3204375492787d7bb2fbec76964cef692",
            "patch": "@@ -51,6 +51,8 @@\n     # generation configs (TODO joao)\n     \"Gemma2Config\": [\"tie_word_embeddings\", \"cache_implementation\"],\n     \"Cohere2Config\": [\"cache_implementation\"],\n+    # Dropout with this value was declared but never used\n+    \"Phi3Config\": [\"embd_pdrop\"],\n     # used to compute the property `self.chunk_length`\n     \"EncodecConfig\": [\"overlap\"],\n     # used to compute the property `self.layers_block_type`"
        }
    ],
    "stats": {
        "total": 2490,
        "additions": 1250,
        "deletions": 1240
    }
}