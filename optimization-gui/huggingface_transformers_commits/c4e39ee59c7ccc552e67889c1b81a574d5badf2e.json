{
    "author": "kaixuanliu",
    "message": "adjust input and output texts for test_modeling_recurrent_gemma.py (#39190)\n\n* adjust input and output texts for test_modeling_recurrent_gemma.py\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* fix bug\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* adjust\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* update Expectation match\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* fix\n\n---------\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "c4e39ee59c7ccc552e67889c1b81a574d5badf2e",
    "files": [
        {
            "sha": "6235097e261a5063c25b9681cd8efb7259917478",
            "filename": "tests/models/recurrent_gemma/test_modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 15,
            "deletions": 4,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/c4e39ee59c7ccc552e67889c1b81a574d5badf2e/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c4e39ee59c7ccc552e67889c1b81a574d5badf2e/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frecurrent_gemma%2Ftest_modeling_recurrent_gemma.py?ref=c4e39ee59c7ccc552e67889c1b81a574d5badf2e",
            "patch": "@@ -20,6 +20,7 @@\n \n from transformers import AutoModelForCausalLM, AutoTokenizer, RecurrentGemmaConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n+    Expectations,\n     require_bitsandbytes,\n     require_read_token,\n     require_torch,\n@@ -215,20 +216,30 @@ def test_2b_generate(self):\n     @require_read_token\n     def test_2b_sample(self):\n         set_seed(0)\n-        EXPECTED_TEXT = ['Where is Paris ?\\n\\nChoose the word or phrase that is closest in meaning to the word in capital letters.\\n\\nREDEEM\\n(A) sort out\\n(B) think over\\n(C) turn in\\n(D) take back\\n\\nWrite the correct word in the space next to each definition. Use each word only once.\\n\\nto badly damage\\n\\nOn the lines provided below, write <em>P</em> if the underlined word group is a phrase and <em>NP</em> if it is not a phrase. Example $\\\\underline{\\\\text{P}}$ 1. We have finally discovered the secret $\\\\underline{\\\\text{of delicious pizza. }}$']  # fmt: skip\n+        expectations = Expectations(\n+            {\n+                (None, None): [\n+                    \"What is Deep learning ?\\n\\nDeep learning is the next frontier in computer vision. It is an Artificial Intelligence (AI) discipline that is rapidly being adopted across industries. The success of Deep\"\n+                ],\n+                (\"cuda\", 8): [\n+                    \"What is Deep learning ?\\n\\nDeep learning is the next frontier in computer vision, itâ€™s an incredibly powerful branch of artificial intelligence.\\n\\nWhat is Dalle?\\n\\nDalle is\",\n+                ],\n+            }\n+        )\n+        EXPECTED_TEXT = expectations.get_expectation()\n         model = AutoModelForCausalLM.from_pretrained(self.model_id).to(torch_device)\n \n         tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n-        inputs = tokenizer(\"Where is Paris ?\", return_tensors=\"pt\", padding=True).to(torch_device)\n-        output = model.generate(**inputs, max_new_tokens=128, do_sample=True)\n+        inputs = tokenizer(\"What is Deep learning ?\", return_tensors=\"pt\", padding=True).to(torch_device)\n+        output = model.generate(**inputs, max_new_tokens=32, do_sample=True)\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n \n         self.assertEqual(output_text, EXPECTED_TEXT)\n \n     @require_bitsandbytes\n     @require_read_token\n     def test_model_2b_8bit(self):\n-        EXPECTED_TEXTS = ['Hello I am doing a project on the topic of \"The impact of the internet on the society\" and I am looking', \"Hi today I'm going to show you how to make a simple and easy to make a simple and easy\"]  # fmt: skip\n+        EXPECTED_TEXTS = ['Hello I am doing a project on the topic of \"The impact of social media on the society\" and I am looking', \"Hi today I'm going to show you how to make a simple and easy to make a 3D\"]  # fmt: skip\n \n         model = AutoModelForCausalLM.from_pretrained(\n             \"gg-hf/recurrent-gemma-2b-hf\", device_map={\"\": torch_device}, load_in_8bit=True, torch_dtype=torch.bfloat16"
        }
    ],
    "stats": {
        "total": 19,
        "additions": 15,
        "deletions": 4
    }
}