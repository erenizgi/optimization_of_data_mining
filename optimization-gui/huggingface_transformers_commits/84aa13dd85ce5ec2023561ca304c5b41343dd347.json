{
    "author": "Cyrilvallez",
    "message": "Fix deepspeed loading (#37281)\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* fix and remove all imports\n\n* Update modeling_utils.py\n\n* Update modeling_utils.py\n\n* style\n\n* Update modeling_utils.py",
    "sha": "84aa13dd85ce5ec2023561ca304c5b41343dd347",
    "files": [
        {
            "sha": "cbeb8579064531996161fc3c1f2bc6b8f520b945",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 36,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/84aa13dd85ce5ec2023561ca304c5b41343dd347/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/84aa13dd85ce5ec2023561ca304c5b41343dd347/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=84aa13dd85ce5ec2023561ca304c5b41343dd347",
            "patch": "@@ -57,7 +57,7 @@\n from .dynamic_module_utils import custom_object_save\n from .generation import CompileConfig, GenerationConfig, GenerationMixin\n from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n-from .integrations.deepspeed import _load_state_dict_into_zero3_model\n+from .integrations.deepspeed import _load_state_dict_into_zero3_model, is_deepspeed_available\n from .integrations.flash_attention import flash_attention_forward\n from .integrations.flex_attention import flex_attention_forward\n from .integrations.sdpa_attention import sdpa_attention_forward\n@@ -153,6 +153,10 @@\n     from safetensors.torch import load_file as safe_load_file\n     from safetensors.torch import save_file as safe_save_file\n \n+\n+if is_deepspeed_available():\n+    import deepspeed\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -2021,8 +2025,6 @@ def _from_config(cls, config, **kwargs):\n             )\n \n         if is_deepspeed_zero3_enabled() and not _is_quantized and not _is_ds_init_called:\n-            import deepspeed\n-\n             logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n             # this immediately partitions the model across all gpus, to avoid the overhead in time\n             # and memory copying it on CPU or each GPU first\n@@ -2662,8 +2664,6 @@ def resize_token_embeddings(\n         # Since we are basically reusing the same old embeddings with new weight values, gathering is required\n         is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n-            import deepspeed\n-\n             with deepspeed.zero.GatheredParameters(model_embeds.weight, modifier_rank=None):\n                 vocab_size = model_embeds.weight.shape[0]\n         else:\n@@ -2694,8 +2694,6 @@ def _resize_token_embeddings(self, new_num_tokens, pad_to_multiple_of=None, mean\n         # Update new_num_tokens with the actual size of new_embeddings\n         if pad_to_multiple_of is not None:\n             if is_deepspeed_zero3_enabled() and not is_quantized:\n-                import deepspeed\n-\n                 with deepspeed.zero.GatheredParameters(new_embeddings.weight, modifier_rank=None):\n                     new_num_tokens = new_embeddings.weight.shape[0]\n             else:\n@@ -2784,8 +2782,6 @@ def _get_resized_embeddings(\n \n         is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n-            import deepspeed\n-\n             with deepspeed.zero.GatheredParameters(old_embeddings.weight, modifier_rank=None):\n                 old_num_tokens, old_embedding_dim = old_embeddings.weight.size()\n         else:\n@@ -2830,8 +2826,6 @@ def _get_resized_embeddings(\n \n             added_num_tokens = new_num_tokens - old_num_tokens\n             if is_deepspeed_zero3_enabled() and not is_quantized:\n-                import deepspeed\n-\n                 with deepspeed.zero.GatheredParameters([old_embeddings.weight], modifier_rank=None):\n                     self._init_added_embeddings_weights_with_mean(\n                         old_embeddings, new_embeddings, old_embedding_dim, old_num_tokens, added_num_tokens\n@@ -2847,8 +2841,6 @@ def _get_resized_embeddings(\n         n = min(old_num_tokens, new_num_tokens)\n \n         if is_deepspeed_zero3_enabled() and not is_quantized:\n-            import deepspeed\n-\n             params = [old_embeddings.weight, new_embeddings.weight]\n             with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                 new_embeddings.weight.data[:n, :] = old_embeddings.weight.data[:n, :]\n@@ -2859,8 +2851,6 @@ def _get_resized_embeddings(\n         # This ensures correct functionality when a Custom Embedding class is passed as input.\n         # The input and output embedding types remain consistent. (c.f. https://github.com/huggingface/transformers/pull/31979)\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n-            import deepspeed\n-\n             params = [old_embeddings.weight, new_embeddings.weight]\n             with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                 old_embeddings.weight = new_embeddings.weight\n@@ -2918,8 +2908,6 @@ def _get_resized_lm_head(\n \n         is_quantized = hasattr(self, \"hf_quantizer\") and self.hf_quantizer is not None\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n-            import deepspeed\n-\n             with deepspeed.zero.GatheredParameters(old_lm_head.weight, modifier_rank=None):\n                 old_num_tokens, old_lm_head_dim = (\n                     old_lm_head.weight.size() if not transposed else old_lm_head.weight.t().size()\n@@ -2970,8 +2958,6 @@ def _get_resized_lm_head(\n \n             added_num_tokens = new_num_tokens - old_num_tokens\n             if is_deepspeed_zero3_enabled() and not is_quantized:\n-                import deepspeed\n-\n                 params = [old_lm_head.weight]\n                 if has_new_lm_head_bias:\n                     params += [old_lm_head.bias]\n@@ -2992,8 +2978,6 @@ def _get_resized_lm_head(\n         num_tokens_to_copy = min(old_num_tokens, new_num_tokens)\n \n         if is_deepspeed_zero3_enabled() and not is_quantized:\n-            import deepspeed\n-\n             params = [old_lm_head.weight, old_lm_head.bias, new_lm_head.weight, new_lm_head.bias]\n             with deepspeed.zero.GatheredParameters(params, modifier_rank=0):\n                 self._copy_lm_head_original_to_resized(\n@@ -3738,14 +3722,8 @@ def float(self, *args):\n             return super().float(*args)\n \n     @classmethod\n-    def get_init_context(\n-        cls: Type[SpecificPreTrainedModelType],\n-        is_quantized=None,\n-        _is_ds_init_called=None,\n-    ):\n+    def get_init_context(cls, is_quantized: bool, _is_ds_init_called: bool):\n         if is_deepspeed_zero3_enabled() and not is_quantized and not _is_ds_init_called:\n-            import deepspeed\n-\n             logger.info(\"Detected DeepSpeed ZeRO-3: activating zero.init() for this model\")\n             init_contexts = [\n                 deepspeed.zero.Init(config_dict_or_path=deepspeed_config()),\n@@ -4644,6 +4622,10 @@ def _load_pretrained_model(\n     ):\n         # Useful flags\n         is_quantized = hf_quantizer is not None\n+        is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in [\n+            QuantizationMethod.HQQ,\n+            QuantizationMethod.BITS_AND_BYTES,\n+        ]\n \n         # Get all the keys of the state dicts that we have to initialize the model\n         if sharded_metadata is not None:\n@@ -4805,23 +4787,19 @@ def _load_pretrained_model(\n             expected_keys = hf_quantizer.update_expected_keys(model_to_load, expected_keys, checkpoint_keys)\n \n         # Warmup cuda to load the weights much faster on devices\n-        if device_map is not None:  # and hf_quantizer is None:\n+        if device_map is not None:\n             expanded_device_map = expand_device_map(device_map, expected_keys)\n             caching_allocator_warmup(model_to_load, expanded_device_map, factor=2 if hf_quantizer is None else 4)\n \n         error_msgs = []\n-        is_hqq_or_bnb = is_quantized and hf_quantizer.quantization_config.quant_method in [\n-            QuantizationMethod.HQQ,\n-            QuantizationMethod.BITS_AND_BYTES,\n-        ]\n         # Iterate on all the shards to load the weights\n         for shard_file in checkpoint_files:\n             # Skip the load for shards that only contain disk-offloaded weights\n             if shard_file in disk_only_shard_files:\n                 continue\n \n             map_location = \"cpu\"\n-            if shard_file.endswith(\".safetensors\") and not is_hqq_or_bnb:\n+            if shard_file.endswith(\".safetensors\") and not is_hqq_or_bnb and not is_deepspeed_zero3_enabled():\n                 map_location = \"meta\"\n             elif (\n                 device_map is not None\n@@ -5267,8 +5245,6 @@ def _initialize_missing_keys(\n             not_initialized_submodules = dict(self.named_modules())\n         # This will only initialize submodules that are not marked as initialized by the line above.\n         if is_deepspeed_zero3_enabled() and not is_quantized:\n-            import deepspeed\n-\n             not_initialized_parameters = list(\n                 set(\n                     itertools.chain.from_iterable("
        }
    ],
    "stats": {
        "total": 48,
        "additions": 12,
        "deletions": 36
    }
}