{
    "author": "Rocketknight1",
    "message": "Deprecate TF + JAX (#38758)\n\n* Scatter deprecation warnings around\n\n* Delete the tests\n\n* Make logging work properly!",
    "sha": "9f563ada706a99454009e95c35a1d5dc7aabc4c9",
    "files": [
        {
            "sha": "bba7d2e351dfea947fde1a39779381879b5d613d",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f563ada706a99454009e95c35a1d5dc7aabc4c9/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f563ada706a99454009e95c35a1d5dc7aabc4c9/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=9f563ada706a99454009e95c35a1d5dc7aabc4c9",
            "patch": "@@ -108,6 +108,10 @@ def _get_is_as_tensor_fns(self, tensor_type: Optional[Union[str, TensorType]] =\n \n         # Get a function reference for the correct framework\n         if tensor_type == TensorType.TENSORFLOW:\n+            logger.warning_once(\n+                \"TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We \"\n+                \"recommend migrating to PyTorch classes or pinning your version of Transformers.\"\n+            )\n             if not is_tf_available():\n                 raise ImportError(\n                     \"Unable to convert output to TensorFlow tensors format, TensorFlow is not installed.\"\n@@ -138,6 +142,10 @@ def as_tensor(value):\n \n             is_tensor = torch.is_tensor\n         elif tensor_type == TensorType.JAX:\n+            logger.warning_once(\n+                \"TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We \"\n+                \"recommend migrating to PyTorch classes or pinning your version of Transformers.\"\n+            )\n             if not is_flax_available():\n                 raise ImportError(\"Unable to convert output to JAX tensors format, JAX is not installed.\")\n             import jax.numpy as jnp  # noqa: F811"
        },
        {
            "sha": "4b260b14f6e63004ad6329a7303dfe8469b05093",
            "filename": "src/transformers/modeling_flax_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f563ada706a99454009e95c35a1d5dc7aabc4c9/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f563ada706a99454009e95c35a1d5dc7aabc4c9/src%2Ftransformers%2Fmodeling_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_flax_utils.py?ref=9f563ada706a99454009e95c35a1d5dc7aabc4c9",
            "patch": "@@ -179,6 +179,10 @@ def __init__(\n         dtype: jnp.dtype = jnp.float32,\n         _do_init: bool = True,\n     ):\n+        logger.warning_once(\n+            \"TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We \"\n+            \"recommend migrating to PyTorch classes or pinning your version of Transformers.\"\n+        )\n         if config is None:\n             raise ValueError(\"config cannot be None\")\n "
        },
        {
            "sha": "0ff83744ae6dffbbb2c99e667973f50c69cc4232",
            "filename": "src/transformers/modeling_tf_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f563ada706a99454009e95c35a1d5dc7aabc4c9/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f563ada706a99454009e95c35a1d5dc7aabc4c9/src%2Ftransformers%2Fmodeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_tf_utils.py?ref=9f563ada706a99454009e95c35a1d5dc7aabc4c9",
            "patch": "@@ -1199,6 +1199,10 @@ def __init__(self, config, *inputs, **kwargs):\n         self.name_or_path = config.name_or_path\n         self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n         self._set_save_spec(self.input_signature)\n+        logger.warning_once(\n+            \"TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We \"\n+            \"recommend migrating to PyTorch classes or pinning your version of Transformers.\"\n+        )\n \n     def get_config(self):\n         return self.config.to_dict()"
        },
        {
            "sha": "8878ad00b9d887962293f60e93d362432ea2195f",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f563ada706a99454009e95c35a1d5dc7aabc4c9/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f563ada706a99454009e95c35a1d5dc7aabc4c9/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=9f563ada706a99454009e95c35a1d5dc7aabc4c9",
            "patch": "@@ -937,6 +937,11 @@ def __init__(\n     ):\n         if framework is None:\n             framework, model = infer_framework_load_model(model, config=model.config)\n+        if framework in (\"tf\", \"jax\"):\n+            logger.warning_once(\n+                \"TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We \"\n+                \"recommend migrating to PyTorch classes or pinning your version of Transformers.\"\n+            )\n \n         self.task = task\n         self.model = model"
        },
        {
            "sha": "eac2fcfb7122195859617b110e55e9213b9b1e20",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f563ada706a99454009e95c35a1d5dc7aabc4c9/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f563ada706a99454009e95c35a1d5dc7aabc4c9/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=9f563ada706a99454009e95c35a1d5dc7aabc4c9",
            "patch": "@@ -2838,6 +2838,12 @@ def __call__(\n             \"split_special_tokens\": kwargs.pop(\"split_special_tokens\", self.split_special_tokens),\n             \"verbose\": verbose,\n         }\n+\n+        if return_tensors in (\"tf\", \"jax\"):\n+            logger.warning_once(\n+                \"TensorFlow and JAX classes are deprecated and will be removed in Transformers v5. We \"\n+                \"recommend migrating to PyTorch classes or pinning your version of Transformers.\"\n+            )\n         all_kwargs.update(kwargs)\n         if text is None and text_target is None:\n             raise ValueError(\"You need to specify either `text` or `text_target`.\")"
        },
        {
            "sha": "ca8eeec59131a89af8da1cf4a6a7f53995554070",
            "filename": "tests/models/albert/test_modeling_flax_albert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 161,
            "changes": 161,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Falbert%2Ftest_modeling_flax_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Falbert%2Ftest_modeling_flax_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_flax_albert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,161 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import AlbertConfig, is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor, random_attention_mask\n-\n-\n-if is_flax_available():\n-    import jax.numpy as jnp\n-\n-    from transformers.models.albert.modeling_flax_albert import (\n-        FlaxAlbertForMaskedLM,\n-        FlaxAlbertForMultipleChoice,\n-        FlaxAlbertForPreTraining,\n-        FlaxAlbertForQuestionAnswering,\n-        FlaxAlbertForSequenceClassification,\n-        FlaxAlbertForTokenClassification,\n-        FlaxAlbertModel,\n-    )\n-\n-\n-class FlaxAlbertModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_attention_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_choices=4,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_attention_mask = use_attention_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_choices = num_choices\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        attention_mask = None\n-        if self.use_attention_mask:\n-            attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        config = AlbertConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, input_ids, token_type_ids, attention_mask\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, token_type_ids, attention_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\n-        return config, inputs_dict\n-\n-\n-@require_flax\n-class FlaxAlbertModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            FlaxAlbertModel,\n-            FlaxAlbertForPreTraining,\n-            FlaxAlbertForMaskedLM,\n-            FlaxAlbertForMultipleChoice,\n-            FlaxAlbertForQuestionAnswering,\n-            FlaxAlbertForSequenceClassification,\n-            FlaxAlbertForTokenClassification,\n-            FlaxAlbertForQuestionAnswering,\n-        )\n-        if is_flax_available()\n-        else ()\n-    )\n-\n-    def setUp(self):\n-        self.model_tester = FlaxAlbertModelTester(self)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"albert/albert-base-v2\")\n-            outputs = model(np.ones((1, 1)))\n-            self.assertIsNotNone(outputs)\n-\n-\n-@require_flax\n-class FlaxAlbertModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_inference_no_head_absolute_embedding(self):\n-        model = FlaxAlbertModel.from_pretrained(\"albert/albert-base-v2\")\n-        input_ids = np.array([[0, 345, 232, 328, 740, 140, 1695, 69, 6078, 1588, 2]])\n-        attention_mask = np.array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n-        output = model(input_ids, attention_mask=attention_mask)[0]\n-        expected_shape = (1, 11, 768)\n-        self.assertEqual(output.shape, expected_shape)\n-        expected_slice = np.array(\n-            [[[-0.6513, 1.5035, -0.2766], [-0.6515, 1.5046, -0.2780], [-0.6512, 1.5049, -0.2784]]]\n-        )\n-\n-        self.assertTrue(jnp.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4))"
        },
        {
            "sha": "339943de9e9050a5e9a9eba8587fafb30737d512",
            "filename": "tests/models/albert/test_modeling_tf_albert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 328,
            "changes": 328,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Falbert%2Ftest_modeling_tf_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Falbert%2Ftest_modeling_tf_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_tf_albert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,328 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import AlbertConfig, is_tf_available\n-from transformers.models.auto import get_values\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import TF_MODEL_FOR_PRETRAINING_MAPPING\n-    from transformers.models.albert.modeling_tf_albert import (\n-        TFAlbertForMaskedLM,\n-        TFAlbertForMultipleChoice,\n-        TFAlbertForPreTraining,\n-        TFAlbertForQuestionAnswering,\n-        TFAlbertForSequenceClassification,\n-        TFAlbertForTokenClassification,\n-        TFAlbertModel,\n-    )\n-\n-\n-class TFAlbertModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        embedding_size=16,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = 13\n-        self.seq_length = 7\n-        self.is_training = True\n-        self.use_input_mask = True\n-        self.use_token_type_ids = True\n-        self.use_labels = True\n-        self.vocab_size = 99\n-        self.embedding_size = 16\n-        self.hidden_size = 32\n-        self.num_hidden_layers = 2\n-        self.num_attention_heads = 4\n-        self.intermediate_size = 37\n-        self.hidden_act = \"gelu\"\n-        self.hidden_dropout_prob = 0.1\n-        self.attention_probs_dropout_prob = 0.1\n-        self.max_position_embeddings = 512\n-        self.type_vocab_size = 16\n-        self.type_sequence_label_size = 2\n-        self.initializer_range = 0.02\n-        self.num_labels = 3\n-        self.num_choices = 4\n-        self.scope = None\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = AlbertConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            embedding_size=self.embedding_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def create_and_check_albert_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFAlbertModel(config=config)\n-        # inputs = {'input_ids': input_ids,\n-        #           'attention_mask': input_mask,\n-        #           'token_type_ids': token_type_ids}\n-        # sequence_output, pooled_output = model(**inputs)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def create_and_check_albert_for_pretraining(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFAlbertForPreTraining(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.prediction_logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-        self.parent.assertEqual(result.sop_logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_albert_for_masked_lm(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFAlbertForMaskedLM(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_albert_for_sequence_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFAlbertForSequenceClassification(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_albert_for_question_answering(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFAlbertForQuestionAnswering(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-\n-    def create_and_check_albert_for_multiple_choice(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_choices = self.num_choices\n-        model = TFAlbertForMultipleChoice(config=config)\n-        multiple_choice_inputs_ids = tf.tile(tf.expand_dims(input_ids, 1), (1, self.num_choices, 1))\n-        multiple_choice_input_mask = tf.tile(tf.expand_dims(input_mask, 1), (1, self.num_choices, 1))\n-        multiple_choice_token_type_ids = tf.tile(tf.expand_dims(token_type_ids, 1), (1, self.num_choices, 1))\n-        inputs = {\n-            \"input_ids\": multiple_choice_inputs_ids,\n-            \"attention_mask\": multiple_choice_input_mask,\n-            \"token_type_ids\": multiple_choice_token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertListEqual(list(result[\"logits\"].shape), [self.batch_size, self.num_choices])\n-\n-    def create_and_check_albert_for_token_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFAlbertForTokenClassification(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertListEqual(list(result[\"logits\"].shape), [self.batch_size, self.seq_length, self.num_labels])\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFAlbertModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFAlbertModel,\n-            TFAlbertForPreTraining,\n-            TFAlbertForMaskedLM,\n-            TFAlbertForSequenceClassification,\n-            TFAlbertForQuestionAnswering,\n-            TFAlbertForTokenClassification,\n-            TFAlbertForMultipleChoice,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFAlbertModel,\n-            \"fill-mask\": TFAlbertForMaskedLM,\n-            \"question-answering\": TFAlbertForQuestionAnswering,\n-            \"text-classification\": TFAlbertForSequenceClassification,\n-            \"token-classification\": TFAlbertForTokenClassification,\n-            \"zero-shot\": TFAlbertForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    # special case for ForPreTraining model\n-    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n-        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n-\n-        if return_labels:\n-            if model_class in get_values(TF_MODEL_FOR_PRETRAINING_MAPPING):\n-                inputs_dict[\"sentence_order_label\"] = tf.zeros(self.model_tester.batch_size, dtype=tf.int32)\n-\n-        return inputs_dict\n-\n-    def setUp(self):\n-        self.model_tester = TFAlbertModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=AlbertConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_albert_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_albert_model(*config_and_inputs)\n-\n-    def test_for_pretraining(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_albert_for_pretraining(*config_and_inputs)\n-\n-    def test_for_masked_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_albert_for_masked_lm(*config_and_inputs)\n-\n-    def test_for_multiple_choice(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_albert_for_multiple_choice(*config_and_inputs)\n-\n-    def test_for_sequence_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_albert_for_sequence_classification(*config_and_inputs)\n-\n-    def test_for_question_answering(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_albert_for_question_answering(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"albert/albert-base-v1\"\n-        model = TFAlbertModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-\n-@require_tf\n-class TFAlbertModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_inference_masked_lm(self):\n-        model = TFAlbertForPreTraining.from_pretrained(\"albert/albert-base-v2\")\n-        input_ids = tf.constant([[0, 1, 2, 3, 4, 5]])\n-        output = model(input_ids)[0]\n-\n-        expected_shape = [1, 6, 30000]\n-        self.assertEqual(output.shape, expected_shape)\n-\n-        expected_slice = tf.constant(\n-            [\n-                [\n-                    [4.595668, 0.74462754, -1.818147],\n-                    [4.5954347, 0.7454184, -1.8188258],\n-                    [4.5954905, 0.7448235, -1.8182316],\n-                ]\n-            ]\n-        )\n-        tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=1e-4)"
        },
        {
            "sha": "8880972e044e401c8a9a3ba63d80feab79bdc800",
            "filename": "tests/models/auto/test_modeling_flax_auto.py",
            "status": "removed",
            "additions": 0,
            "deletions": 102,
            "changes": 102,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fauto%2Ftest_modeling_flax_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fauto%2Ftest_modeling_flax_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_modeling_flax_auto.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,102 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-from transformers import AutoConfig, AutoTokenizer, BertConfig, TensorType, is_flax_available\n-from transformers.testing_utils import DUMMY_UNKNOWN_IDENTIFIER, require_flax, slow\n-\n-\n-if is_flax_available():\n-    import jax\n-\n-    from transformers.models.auto.modeling_flax_auto import FlaxAutoModel\n-    from transformers.models.bert.modeling_flax_bert import FlaxBertModel\n-    from transformers.models.roberta.modeling_flax_roberta import FlaxRobertaModel\n-\n-\n-@require_flax\n-class FlaxAutoModelTest(unittest.TestCase):\n-    @slow\n-    def test_bert_from_pretrained(self):\n-        for model_name in [\"google-bert/bert-base-cased\", \"google-bert/bert-large-uncased\"]:\n-            with self.subTest(model_name):\n-                config = AutoConfig.from_pretrained(model_name)\n-                self.assertIsNotNone(config)\n-                self.assertIsInstance(config, BertConfig)\n-\n-                model = FlaxAutoModel.from_pretrained(model_name)\n-                self.assertIsNotNone(model)\n-                self.assertIsInstance(model, FlaxBertModel)\n-\n-    @slow\n-    def test_roberta_from_pretrained(self):\n-        for model_name in [\"FacebookAI/roberta-base\", \"FacebookAI/roberta-large\"]:\n-            with self.subTest(model_name):\n-                config = AutoConfig.from_pretrained(model_name)\n-                self.assertIsNotNone(config)\n-                self.assertIsInstance(config, BertConfig)\n-\n-                model = FlaxAutoModel.from_pretrained(model_name)\n-                self.assertIsNotNone(model)\n-                self.assertIsInstance(model, FlaxRobertaModel)\n-\n-    @slow\n-    def test_bert_jax_jit(self):\n-        for model_name in [\"google-bert/bert-base-cased\", \"google-bert/bert-large-uncased\"]:\n-            tokenizer = AutoTokenizer.from_pretrained(model_name)\n-            model = FlaxBertModel.from_pretrained(model_name)\n-            tokens = tokenizer(\"Do you support jax jitted function?\", return_tensors=TensorType.JAX)\n-\n-            @jax.jit\n-            def eval(**kwargs):\n-                return model(**kwargs)\n-\n-            eval(**tokens).block_until_ready()\n-\n-    @slow\n-    def test_roberta_jax_jit(self):\n-        for model_name in [\"FacebookAI/roberta-base\", \"FacebookAI/roberta-large\"]:\n-            tokenizer = AutoTokenizer.from_pretrained(model_name)\n-            model = FlaxRobertaModel.from_pretrained(model_name)\n-            tokens = tokenizer(\"Do you support jax jitted function?\", return_tensors=TensorType.JAX)\n-\n-            @jax.jit\n-            def eval(**kwargs):\n-                return model(**kwargs)\n-\n-            eval(**tokens).block_until_ready()\n-\n-    def test_repo_not_found(self):\n-        with self.assertRaisesRegex(\n-            EnvironmentError, \"bert-base is not a local folder and is not a valid model identifier\"\n-        ):\n-            _ = FlaxAutoModel.from_pretrained(\"bert-base\")\n-\n-    def test_revision_not_found(self):\n-        with self.assertRaisesRegex(\n-            EnvironmentError, r\"aaaaaa is not a valid git identifier \\(branch name, tag name or commit id\\)\"\n-        ):\n-            _ = FlaxAutoModel.from_pretrained(DUMMY_UNKNOWN_IDENTIFIER, revision=\"aaaaaa\")\n-\n-    def test_model_file_not_found(self):\n-        with self.assertRaisesRegex(\n-            EnvironmentError,\n-            \"hf-internal-testing/config-no-model does not appear to have a file named flax_model.msgpack\",\n-        ):\n-            _ = FlaxAutoModel.from_pretrained(\"hf-internal-testing/config-no-model\")\n-\n-    def test_model_from_pt_suggestion(self):\n-        with self.assertRaisesRegex(EnvironmentError, \"Use `from_pt=True` to load this model\"):\n-            _ = FlaxAutoModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")"
        },
        {
            "sha": "9957df16298c29b2c91edc9aa9f01dc0a64ab461",
            "filename": "tests/models/auto/test_modeling_tf_auto.py",
            "status": "removed",
            "additions": 0,
            "deletions": 310,
            "changes": 310,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fauto%2Ftest_modeling_tf_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fauto%2Ftest_modeling_tf_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fauto%2Ftest_modeling_tf_auto.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,310 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from __future__ import annotations\n-\n-import copy\n-import tempfile\n-import unittest\n-\n-from transformers import CONFIG_MAPPING, AutoConfig, BertConfig, GPT2Config, T5Config, TapasConfig, is_tf_available\n-from transformers.testing_utils import (\n-    DUMMY_UNKNOWN_IDENTIFIER,\n-    SMALL_MODEL_IDENTIFIER,\n-    RequestCounter,\n-    require_tensorflow_probability,\n-    require_tf,\n-    slow,\n-)\n-\n-from ..bert.test_modeling_bert import BertModelTester\n-\n-\n-if is_tf_available():\n-    from transformers import (\n-        TFAutoModel,\n-        TFAutoModelForCausalLM,\n-        TFAutoModelForMaskedLM,\n-        TFAutoModelForPreTraining,\n-        TFAutoModelForQuestionAnswering,\n-        TFAutoModelForSeq2SeqLM,\n-        TFAutoModelForSequenceClassification,\n-        TFAutoModelForTableQuestionAnswering,\n-        TFAutoModelForTokenClassification,\n-        TFAutoModelWithLMHead,\n-        TFBertForMaskedLM,\n-        TFBertForPreTraining,\n-        TFBertForQuestionAnswering,\n-        TFBertForSequenceClassification,\n-        TFBertModel,\n-        TFFunnelBaseModel,\n-        TFFunnelModel,\n-        TFGPT2LMHeadModel,\n-        TFRobertaForMaskedLM,\n-        TFT5ForConditionalGeneration,\n-        TFTapasForQuestionAnswering,\n-    )\n-    from transformers.models.auto.modeling_tf_auto import (\n-        TF_MODEL_FOR_CAUSAL_LM_MAPPING,\n-        TF_MODEL_FOR_MASKED_LM_MAPPING,\n-        TF_MODEL_FOR_PRETRAINING_MAPPING,\n-        TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n-        TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING,\n-        TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING,\n-        TF_MODEL_MAPPING,\n-    )\n-\n-\n-class NewModelConfig(BertConfig):\n-    model_type = \"new-model\"\n-\n-\n-if is_tf_available():\n-\n-    class TFNewModel(TFBertModel):\n-        config_class = NewModelConfig\n-\n-\n-@require_tf\n-class TFAutoModelTest(unittest.TestCase):\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"google-bert/bert-base-cased\"\n-        config = AutoConfig.from_pretrained(model_name)\n-        self.assertIsNotNone(config)\n-        self.assertIsInstance(config, BertConfig)\n-\n-        model = TFAutoModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, TFBertModel)\n-\n-    @slow\n-    def test_model_for_pretraining_from_pretrained(self):\n-        model_name = \"google-bert/bert-base-cased\"\n-        config = AutoConfig.from_pretrained(model_name)\n-        self.assertIsNotNone(config)\n-        self.assertIsInstance(config, BertConfig)\n-\n-        model = TFAutoModelForPreTraining.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, TFBertForPreTraining)\n-\n-    @slow\n-    def test_model_for_causal_lm(self):\n-        model_name = \"openai-community/gpt2\"\n-        config = AutoConfig.from_pretrained(model_name)\n-        self.assertIsNotNone(config)\n-        self.assertIsInstance(config, GPT2Config)\n-\n-        model = TFAutoModelForCausalLM.from_pretrained(model_name)\n-        model, loading_info = TFAutoModelForCausalLM.from_pretrained(model_name, output_loading_info=True)\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, TFGPT2LMHeadModel)\n-\n-    @slow\n-    def test_lmhead_model_from_pretrained(self):\n-        model_name = \"openai-community/gpt2\"\n-        config = AutoConfig.from_pretrained(model_name)\n-        self.assertIsNotNone(config)\n-        self.assertIsInstance(config, GPT2Config)\n-\n-        model = TFAutoModelWithLMHead.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, TFGPT2LMHeadModel)\n-\n-    @slow\n-    def test_model_for_masked_lm(self):\n-        model_name = \"google-bert/bert-base-uncased\"\n-        config = AutoConfig.from_pretrained(model_name)\n-        self.assertIsNotNone(config)\n-        self.assertIsInstance(config, BertConfig)\n-\n-        model = TFAutoModelForMaskedLM.from_pretrained(model_name)\n-        model, loading_info = TFAutoModelForMaskedLM.from_pretrained(model_name, output_loading_info=True)\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, TFBertForMaskedLM)\n-\n-    @slow\n-    def test_model_for_encoder_decoder_lm(self):\n-        model_name = \"google-t5/t5-base\"\n-        config = AutoConfig.from_pretrained(model_name)\n-        self.assertIsNotNone(config)\n-        self.assertIsInstance(config, T5Config)\n-\n-        model = TFAutoModelForSeq2SeqLM.from_pretrained(model_name)\n-        model, loading_info = TFAutoModelForSeq2SeqLM.from_pretrained(model_name, output_loading_info=True)\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, TFT5ForConditionalGeneration)\n-\n-    @slow\n-    def test_sequence_classification_model_from_pretrained(self):\n-        #     model_name = 'openai-community/gpt2'\n-        for model_name in [\"google-bert/bert-base-uncased\"]:\n-            config = AutoConfig.from_pretrained(model_name)\n-            self.assertIsNotNone(config)\n-            self.assertIsInstance(config, BertConfig)\n-\n-            model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n-            self.assertIsNotNone(model)\n-            self.assertIsInstance(model, TFBertForSequenceClassification)\n-\n-    @slow\n-    def test_question_answering_model_from_pretrained(self):\n-        #     model_name = 'openai-community/gpt2'\n-        for model_name in [\"google-bert/bert-base-uncased\"]:\n-            config = AutoConfig.from_pretrained(model_name)\n-            self.assertIsNotNone(config)\n-            self.assertIsInstance(config, BertConfig)\n-\n-            model = TFAutoModelForQuestionAnswering.from_pretrained(model_name)\n-            self.assertIsNotNone(model)\n-            self.assertIsInstance(model, TFBertForQuestionAnswering)\n-\n-    @slow\n-    @require_tensorflow_probability\n-    def test_table_question_answering_model_from_pretrained(self):\n-        model_name = \"google/tapas-base\"\n-        config = AutoConfig.from_pretrained(model_name)\n-        self.assertIsNotNone(config)\n-        self.assertIsInstance(config, TapasConfig)\n-\n-        model = TFAutoModelForTableQuestionAnswering.from_pretrained(model_name)\n-        model, loading_info = TFAutoModelForTableQuestionAnswering.from_pretrained(\n-            model_name, output_loading_info=True\n-        )\n-        self.assertIsNotNone(model)\n-        self.assertIsInstance(model, TFTapasForQuestionAnswering)\n-\n-    def test_from_pretrained_identifier(self):\n-        model = TFAutoModelWithLMHead.from_pretrained(SMALL_MODEL_IDENTIFIER)\n-        self.assertIsInstance(model, TFBertForMaskedLM)\n-        self.assertEqual(model.num_parameters(), 14410)\n-        self.assertEqual(model.num_parameters(only_trainable=True), 14410)\n-\n-    def test_from_identifier_from_model_type(self):\n-        model = TFAutoModelWithLMHead.from_pretrained(DUMMY_UNKNOWN_IDENTIFIER)\n-        self.assertIsInstance(model, TFRobertaForMaskedLM)\n-        self.assertEqual(model.num_parameters(), 14410)\n-        self.assertEqual(model.num_parameters(only_trainable=True), 14410)\n-\n-    def test_from_pretrained_with_tuple_values(self):\n-        # For the auto model mapping, FunnelConfig has two models: FunnelModel and FunnelBaseModel\n-        model = TFAutoModel.from_pretrained(\"sgugger/funnel-random-tiny\")\n-        self.assertIsInstance(model, TFFunnelModel)\n-\n-        config = copy.deepcopy(model.config)\n-        config.architectures = [\"FunnelBaseModel\"]\n-        model = TFAutoModel.from_config(config)\n-        model.build_in_name_scope()\n-\n-        self.assertIsInstance(model, TFFunnelBaseModel)\n-\n-        with tempfile.TemporaryDirectory() as tmp_dir:\n-            model.save_pretrained(tmp_dir)\n-            model = TFAutoModel.from_pretrained(tmp_dir)\n-            self.assertIsInstance(model, TFFunnelBaseModel)\n-\n-    def test_new_model_registration(self):\n-        try:\n-            AutoConfig.register(\"new-model\", NewModelConfig)\n-\n-            auto_classes = [\n-                TFAutoModel,\n-                TFAutoModelForCausalLM,\n-                TFAutoModelForMaskedLM,\n-                TFAutoModelForPreTraining,\n-                TFAutoModelForQuestionAnswering,\n-                TFAutoModelForSequenceClassification,\n-                TFAutoModelForTokenClassification,\n-            ]\n-\n-            for auto_class in auto_classes:\n-                with self.subTest(auto_class.__name__):\n-                    # Wrong config class will raise an error\n-                    with self.assertRaises(ValueError):\n-                        auto_class.register(BertConfig, TFNewModel)\n-                    auto_class.register(NewModelConfig, TFNewModel)\n-                    # Trying to register something existing in the Transformers library will raise an error\n-                    with self.assertRaises(ValueError):\n-                        auto_class.register(BertConfig, TFBertModel)\n-\n-                    # Now that the config is registered, it can be used as any other config with the auto-API\n-                    tiny_config = BertModelTester(self).get_config()\n-                    config = NewModelConfig(**tiny_config.to_dict())\n-\n-                    model = auto_class.from_config(config)\n-                    model.build_in_name_scope()\n-\n-                    self.assertIsInstance(model, TFNewModel)\n-\n-                    with tempfile.TemporaryDirectory() as tmp_dir:\n-                        model.save_pretrained(tmp_dir)\n-                        new_model = auto_class.from_pretrained(tmp_dir)\n-                        self.assertIsInstance(new_model, TFNewModel)\n-\n-        finally:\n-            if \"new-model\" in CONFIG_MAPPING._extra_content:\n-                del CONFIG_MAPPING._extra_content[\"new-model\"]\n-            for mapping in (\n-                TF_MODEL_MAPPING,\n-                TF_MODEL_FOR_PRETRAINING_MAPPING,\n-                TF_MODEL_FOR_QUESTION_ANSWERING_MAPPING,\n-                TF_MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING,\n-                TF_MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING,\n-                TF_MODEL_FOR_CAUSAL_LM_MAPPING,\n-                TF_MODEL_FOR_MASKED_LM_MAPPING,\n-            ):\n-                if NewModelConfig in mapping._extra_content:\n-                    del mapping._extra_content[NewModelConfig]\n-\n-    def test_repo_not_found(self):\n-        with self.assertRaisesRegex(\n-            EnvironmentError, \"bert-base is not a local folder and is not a valid model identifier\"\n-        ):\n-            _ = TFAutoModel.from_pretrained(\"bert-base\")\n-\n-    def test_revision_not_found(self):\n-        with self.assertRaisesRegex(\n-            EnvironmentError, r\"aaaaaa is not a valid git identifier \\(branch name, tag name or commit id\\)\"\n-        ):\n-            _ = TFAutoModel.from_pretrained(DUMMY_UNKNOWN_IDENTIFIER, revision=\"aaaaaa\")\n-\n-    def test_model_file_not_found(self):\n-        with self.assertRaisesRegex(\n-            EnvironmentError,\n-            \"hf-internal-testing/config-no-model does not appear to have a file named pytorch_model.bin\",\n-        ):\n-            _ = TFAutoModel.from_pretrained(\"hf-internal-testing/config-no-model\")\n-\n-    def test_model_from_pt_suggestion(self):\n-        with self.assertRaisesRegex(EnvironmentError, \"Use `from_pt=True` to load this model\"):\n-            _ = TFAutoModel.from_pretrained(\"hf-internal-testing/tiny-bert-pt-only\")\n-\n-    @unittest.skip(\"Failing on main\")\n-    def test_cached_model_has_minimum_calls_to_head(self):\n-        # Make sure we have cached the model.\n-        _ = TFAutoModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-        with RequestCounter() as counter:\n-            _ = TFAutoModel.from_pretrained(\"hf-internal-testing/tiny-random-bert\")\n-        self.assertEqual(counter[\"GET\"], 0)\n-        self.assertEqual(counter[\"HEAD\"], 1)\n-        self.assertEqual(counter.total_calls, 1)\n-\n-        # With a sharded checkpoint\n-        _ = TFAutoModel.from_pretrained(\"ArthurZ/tiny-random-bert-sharded\")\n-        with RequestCounter() as counter:\n-            _ = TFAutoModel.from_pretrained(\"ArthurZ/tiny-random-bert-sharded\")\n-        self.assertEqual(counter[\"GET\"], 0)\n-        self.assertEqual(counter[\"HEAD\"], 1)\n-        self.assertEqual(counter.total_calls, 1)"
        },
        {
            "sha": "118398d3dfd347c5e70444c318ee49560be52950",
            "filename": "tests/models/bart/test_modeling_flax_bart.py",
            "status": "removed",
            "additions": 0,
            "deletions": 763,
            "changes": 763,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbart%2Ftest_modeling_flax_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbart%2Ftest_modeling_flax_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_flax_bart.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,763 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import unittest\n-\n-import numpy as np\n-import timeout_decorator  # noqa\n-\n-from transformers import BartConfig, BartTokenizer, is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-\n-\n-if is_flax_available():\n-    import os\n-\n-    # The slow tests are often failing with OOM error on GPU\n-    # This makes JAX allocate exactly what is needed on demand, and deallocate memory that is no longer needed\n-    # but will be slower as stated here https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n-    os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n-\n-    import jax\n-    import jax.numpy as jnp\n-\n-    from transformers.models.bart.modeling_flax_bart import (\n-        FlaxBartForConditionalGeneration,\n-        FlaxBartForQuestionAnswering,\n-        FlaxBartForSequenceClassification,\n-        FlaxBartModel,\n-        shift_tokens_right,\n-    )\n-\n-\n-def prepare_bart_inputs_dict(\n-    config,\n-    input_ids,\n-    decoder_input_ids=None,\n-    attention_mask=None,\n-    decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n-):\n-    if attention_mask is None:\n-        attention_mask = np.where(input_ids != config.pad_token_id, 1, 0)\n-    if decoder_attention_mask is None:\n-        decoder_attention_mask = np.where(decoder_input_ids != config.pad_token_id, 1, 0)\n-    if head_mask is None:\n-        head_mask = np.ones((config.encoder_layers, config.encoder_attention_heads))\n-    if decoder_head_mask is None:\n-        decoder_head_mask = np.ones((config.decoder_layers, config.decoder_attention_heads))\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = np.ones((config.decoder_layers, config.decoder_attention_heads))\n-    return {\n-        \"input_ids\": input_ids,\n-        \"decoder_input_ids\": decoder_input_ids,\n-        \"attention_mask\": attention_mask,\n-        \"decoder_attention_mask\": attention_mask,\n-    }\n-\n-\n-class FlaxBartModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_labels=False,\n-        vocab_size=99,\n-        hidden_size=16,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=4,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=32,\n-        eos_token_id=2,\n-        pad_token_id=1,\n-        bos_token_id=0,\n-        initializer_range=0.02,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-        self.initializer_range = initializer_range\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = np.clip(ids_tensor([self.batch_size, self.seq_length - 1], self.vocab_size), 3, self.vocab_size)\n-        input_ids = np.concatenate((input_ids, 2 * np.ones((self.batch_size, 1), dtype=np.int64)), -1)\n-\n-        decoder_input_ids = shift_tokens_right(input_ids, 1, 2)\n-\n-        config = BartConfig(\n-            vocab_size=self.vocab_size,\n-            d_model=self.hidden_size,\n-            encoder_layers=self.num_hidden_layers,\n-            decoder_layers=self.num_hidden_layers,\n-            encoder_attention_heads=self.num_attention_heads,\n-            decoder_attention_heads=self.num_attention_heads,\n-            encoder_ffn_dim=self.intermediate_size,\n-            decoder_ffn_dim=self.intermediate_size,\n-            dropout=self.hidden_dropout_prob,\n-            attention_dropout=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            eos_token_id=self.eos_token_id,\n-            bos_token_id=self.bos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            initializer_range=self.initializer_range,\n-            use_cache=False,\n-        )\n-        inputs_dict = prepare_bart_inputs_dict(config, input_ids, decoder_input_ids)\n-        return config, inputs_dict\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config, inputs_dict = self.prepare_config_and_inputs()\n-        return config, inputs_dict\n-\n-    def check_use_cache_forward(self, model_class_name, config, inputs_dict):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        encoder_outputs = model.encode(inputs_dict[\"input_ids\"])\n-\n-        decoder_input_ids, decoder_attention_mask = (\n-            inputs_dict[\"decoder_input_ids\"],\n-            inputs_dict[\"decoder_attention_mask\"],\n-        )\n-\n-        past_key_values = model.init_cache(decoder_input_ids.shape[0], max_decoder_length, encoder_outputs)\n-        decoder_attention_mask = jnp.ones((decoder_input_ids.shape[0], max_decoder_length), dtype=\"i4\")\n-\n-        decoder_position_ids = jnp.broadcast_to(\n-            jnp.arange(decoder_input_ids.shape[-1] - 1)[None, :],\n-            (decoder_input_ids.shape[0], decoder_input_ids.shape[-1] - 1),\n-        )\n-        outputs_cache = model.decode(\n-            decoder_input_ids[:, :-1],\n-            encoder_outputs,\n-            decoder_attention_mask=decoder_attention_mask,\n-            past_key_values=past_key_values,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-\n-        decoder_position_ids = jnp.array(decoder_input_ids.shape[0] * [[decoder_input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model.decode(\n-            decoder_input_ids[:, -1:],\n-            encoder_outputs,\n-            decoder_attention_mask=decoder_attention_mask,\n-            past_key_values=outputs_cache.past_key_values,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-\n-        outputs = model.decode(decoder_input_ids, encoder_outputs)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-    def check_use_cache_forward_with_attn_mask(self, model_class_name, config, inputs_dict):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        encoder_outputs = model.encode(inputs_dict[\"input_ids\"])\n-\n-        decoder_input_ids, decoder_attention_mask = (\n-            inputs_dict[\"decoder_input_ids\"],\n-            inputs_dict[\"decoder_attention_mask\"],\n-        )\n-\n-        decoder_attention_mask_cache = jnp.concatenate(\n-            [\n-                decoder_attention_mask,\n-                jnp.zeros((decoder_attention_mask.shape[0], max_decoder_length - decoder_attention_mask.shape[1])),\n-            ],\n-            axis=-1,\n-        )\n-\n-        past_key_values = model.init_cache(decoder_input_ids.shape[0], max_decoder_length, encoder_outputs)\n-        decoder_position_ids = jnp.broadcast_to(\n-            jnp.arange(decoder_input_ids.shape[-1] - 1)[None, :],\n-            (decoder_input_ids.shape[0], decoder_input_ids.shape[-1] - 1),\n-        )\n-\n-        outputs_cache = model.decode(\n-            decoder_input_ids[:, :-1],\n-            encoder_outputs,\n-            decoder_attention_mask=decoder_attention_mask_cache,\n-            past_key_values=past_key_values,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-        decoder_position_ids = jnp.array(decoder_input_ids.shape[0] * [[decoder_input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model.decode(\n-            decoder_input_ids[:, -1:],\n-            encoder_outputs,\n-            past_key_values=outputs_cache.past_key_values,\n-            decoder_attention_mask=decoder_attention_mask_cache,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-\n-        outputs = model.decode(decoder_input_ids, encoder_outputs, decoder_attention_mask=decoder_attention_mask)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-\n-@require_flax\n-class BartHeadTests(unittest.TestCase):\n-    vocab_size = 99\n-\n-    def _get_config_and_data(self):\n-        input_ids = np.array(\n-            [\n-                [71, 82, 18, 33, 46, 91, 2],\n-                [68, 34, 26, 58, 30, 82, 2],\n-                [5, 97, 17, 39, 94, 40, 2],\n-                [76, 83, 94, 25, 70, 78, 2],\n-                [87, 59, 41, 35, 48, 66, 2],\n-                [55, 13, 16, 58, 5, 2, 1],  # note padding\n-                [64, 27, 31, 51, 12, 75, 2],\n-                [52, 64, 86, 17, 83, 39, 2],\n-                [48, 61, 9, 24, 71, 82, 2],\n-                [26, 1, 60, 48, 22, 13, 2],\n-                [21, 5, 62, 28, 14, 76, 2],\n-                [45, 98, 37, 86, 59, 48, 2],\n-                [70, 70, 50, 9, 28, 0, 2],\n-            ],\n-            dtype=np.int64,\n-        )\n-\n-        batch_size = input_ids.shape[0]\n-        config = BartConfig(\n-            vocab_size=self.vocab_size,\n-            d_model=24,\n-            encoder_layers=2,\n-            decoder_layers=2,\n-            encoder_attention_heads=2,\n-            decoder_attention_heads=2,\n-            encoder_ffn_dim=32,\n-            decoder_ffn_dim=32,\n-            max_position_embeddings=48,\n-            eos_token_id=2,\n-            pad_token_id=1,\n-            bos_token_id=0,\n-        )\n-        return config, input_ids, batch_size\n-\n-    def test_sequence_classification_forward(self):\n-        config, input_ids, batch_size = self._get_config_and_data()\n-        model = FlaxBartForSequenceClassification(config)\n-        outputs = model(input_ids=input_ids, decoder_input_ids=input_ids)\n-        expected_shape = (batch_size, config.num_labels)\n-        self.assertEqual(outputs[\"logits\"].shape, expected_shape)\n-\n-    def test_question_answering_forward(self):\n-        config, input_ids, batch_size = self._get_config_and_data()\n-        model = FlaxBartForQuestionAnswering(config)\n-        outputs = model(input_ids=input_ids)\n-\n-        self.assertEqual(outputs[\"start_logits\"].shape, input_ids.shape)\n-        self.assertEqual(outputs[\"end_logits\"].shape, input_ids.shape)\n-\n-    # @timeout_decorator.timeout(1)  # not working with the decorator so far\n-    def test_lm_forward(self):\n-        config, input_ids, batch_size = self._get_config_and_data()\n-        lm_model = FlaxBartForConditionalGeneration(config)\n-        outputs = lm_model(input_ids=input_ids)\n-        expected_shape = (batch_size, input_ids.shape[1], config.vocab_size)\n-        self.assertEqual(outputs[\"logits\"].shape, expected_shape)\n-\n-    def test_lm_uneven_forward(self):\n-        config = BartConfig(\n-            vocab_size=self.vocab_size,\n-            d_model=14,\n-            encoder_layers=2,\n-            decoder_layers=2,\n-            encoder_attention_heads=2,\n-            decoder_attention_heads=2,\n-            encoder_ffn_dim=8,\n-            decoder_ffn_dim=8,\n-            max_position_embeddings=48,\n-        )\n-        lm_model = FlaxBartForConditionalGeneration(config)\n-        context = np.array([[71, 82, 18, 33, 46, 91, 2], [68, 34, 26, 58, 30, 2, 1]], dtype=np.int64)\n-        summary = np.array([[82, 71, 82, 18, 2], [58, 68, 2, 1, 1]], dtype=np.int64)\n-        outputs = lm_model(input_ids=context, decoder_input_ids=summary)\n-        expected_shape = (*summary.shape, config.vocab_size)\n-        self.assertEqual(outputs[\"logits\"].shape, expected_shape)\n-\n-    def test_shift_tokens_right(self):\n-        input_ids = np.array([[71, 82, 18, 33, 2, 1, 1], [68, 34, 26, 58, 30, 82, 2]], dtype=np.int64)\n-        shifted = shift_tokens_right(input_ids, 1, 2)\n-        n_pad_before = np.equal(input_ids, 1).astype(np.float32).sum()\n-        n_pad_after = np.equal(shifted, 1).astype(np.float32).sum()\n-        self.assertEqual(shifted.shape, input_ids.shape)\n-        self.assertEqual(n_pad_after, n_pad_before - 1)\n-        self.assertTrue(np.equal(shifted[:, 0], 2).all())\n-\n-\n-@require_flax\n-class FlaxBartModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    is_encoder_decoder = True\n-    all_model_classes = (\n-        (\n-            FlaxBartModel,\n-            FlaxBartForConditionalGeneration,\n-            FlaxBartForSequenceClassification,\n-            FlaxBartForQuestionAnswering,\n-        )\n-        if is_flax_available()\n-        else ()\n-    )\n-\n-    def setUp(self):\n-        self.model_tester = FlaxBartModelTester(self)\n-\n-    def test_use_cache_forward(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n-        for model_class in self.all_model_classes:\n-            self.model_tester.check_use_cache_forward(model_class, config, inputs_dict)\n-\n-    def test_use_cache_forward_with_attn_mask(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n-        for model_class in self.all_model_classes:\n-            self.model_tester.check_use_cache_forward_with_attn_mask(model_class, config, inputs_dict)\n-\n-    def test_encode(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                model = model_class(config)\n-\n-                @jax.jit\n-                def encode_jitted(input_ids, attention_mask=None, **kwargs):\n-                    return model.encode(input_ids=input_ids, attention_mask=attention_mask)\n-\n-                with self.subTest(\"JIT Enabled\"):\n-                    jitted_outputs = encode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                with self.subTest(\"JIT Disabled\"):\n-                    with jax.disable_jit():\n-                        outputs = encode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                self.assertEqual(len(outputs), len(jitted_outputs))\n-                for jitted_output, output in zip(jitted_outputs, outputs):\n-                    self.assertEqual(jitted_output.shape, output.shape)\n-\n-    def test_decode(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                model = model_class(config)\n-                encoder_outputs = model.encode(inputs_dict[\"input_ids\"], inputs_dict[\"attention_mask\"])\n-\n-                prepared_inputs_dict = {\n-                    \"decoder_input_ids\": inputs_dict[\"decoder_input_ids\"],\n-                    \"decoder_attention_mask\": inputs_dict[\"decoder_attention_mask\"],\n-                    \"encoder_outputs\": encoder_outputs,\n-                }\n-\n-                @jax.jit\n-                def decode_jitted(decoder_input_ids, decoder_attention_mask, encoder_outputs):\n-                    return model.decode(\n-                        decoder_input_ids=decoder_input_ids,\n-                        decoder_attention_mask=decoder_attention_mask,\n-                        encoder_outputs=encoder_outputs,\n-                    )\n-\n-                with self.subTest(\"JIT Enabled\"):\n-                    jitted_outputs = decode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                with self.subTest(\"JIT Disabled\"):\n-                    with jax.disable_jit():\n-                        outputs = decode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                self.assertEqual(len(outputs), len(jitted_outputs))\n-                for jitted_output, output in zip(jitted_outputs, outputs):\n-                    self.assertEqual(jitted_output.shape, output.shape)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"facebook/bart-base\", from_pt=True)\n-            # FlaxBartForSequenceClassification expects eos token in input_ids\n-            input_ids = np.ones((1, 1)) * model.config.eos_token_id\n-            outputs = model(input_ids)\n-            self.assertIsNotNone(outputs)\n-\n-    @slow\n-    def test_summarization_fast(self):\n-        model = FlaxBartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-cnn-6-6\")\n-        tokenizer = BartTokenizer.from_pretrained(\"sshleifer/distilbart-cnn-6-6\")\n-\n-        input_str = (\n-            \"This sentence is made of three parts. Each part is important on its own. One part is about animals, the\"\n-            \" other part about planes, and the last part about housing.\"\n-        )\n-\n-        input_ids = tokenizer(input_str, return_tensors=\"np\").input_ids\n-        sequences = model.generate(input_ids, num_beams=2, min_length=None, max_length=20).sequences\n-\n-        output_str = tokenizer.batch_decode(sequences)[0]\n-\n-        assert (\n-            output_str == \"</s><s>This sentence is made of three parts. One part is about animals, the other part</s>\"\n-        )\n-\n-    @slow\n-    def test_cnn_summarization_same_as_fairseq(self):\n-        model = FlaxBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n-        tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n-\n-        FRANCE_ARTICLE = (  # @noq\n-            \" Marseille, France (CNN)The French prosecutor leading an investigation into the crash of Germanwings\"\n-            \" Flight 9525 insisted Wednesday that he was not aware of any video footage from on board the plane.\"\n-            ' Marseille prosecutor Brice Robin told CNN that \"so far no videos were used in the crash investigation.\"'\n-            ' He added, \"A person who has such a video needs to immediately give it to the investigators.\" Robin\\'s'\n-            \" comments follow claims by two magazines, German daily Bild and French Paris Match, of a cell phone video\"\n-            \" showing the harrowing final seconds from on board Germanwings Flight 9525 as it crashed into the French\"\n-            \" Alps. All 150 on board were killed. Paris Match and Bild reported that the video was recovered from a\"\n-            \" phone at the wreckage site. The two publications described the supposed video, but did not post it on\"\n-            \" their websites. The publications said that they watched the video, which was found by a source close to\"\n-            \" the investigation. \\\"One can hear cries of 'My God' in several languages,\\\" Paris Match reported.\"\n-            ' \"Metallic banging can also be heard more than three times, perhaps of the pilot trying to open the'\n-            \" cockpit door with a heavy object.  Towards the end, after a heavy shake, stronger than the others, the\"\n-            ' screaming intensifies. Then nothing.\" \"It is a very disturbing scene,\" said Julian Reichelt,'\n-            \" editor-in-chief of Bild online. An official with France's accident investigation agency, the BEA, said\"\n-            \" the agency is not aware of any such video. Lt. Col. Jean-Marc Menichini, a French Gendarmerie spokesman\"\n-            \" in charge of communications on rescue efforts around the Germanwings crash site, told CNN that the\"\n-            ' reports were \"completely wrong\" and \"unwarranted.\" Cell phones have been collected at the site, he said,'\n-            ' but that they \"hadn\\'t been exploited yet.\" Menichini said he believed the cell phones would need to be'\n-            \" sent to the Criminal Research Institute in Rosny sous-Bois, near Paris, in order to be analyzed by\"\n-            \" specialized technicians working hand-in-hand with investigators. But none of the cell phones found so\"\n-            \" far have been sent to the institute, Menichini said. Asked whether staff involved in the search could\"\n-            ' have leaked a memory card to the media, Menichini answered with a categorical \"no.\" Reichelt told \"Erin'\n-            ' Burnett: Outfront\" that he had watched the video and stood by the report, saying Bild and Paris Match'\n-            ' are \"very confident\" that the clip is real. He noted that investigators only revealed they\\'d recovered'\n-            ' cell phones from the crash site after Bild and Paris Match published their reports. \"That is something'\n-            \" we did not know before. ... Overall we can say many things of the investigation weren't revealed by the\"\n-            ' investigation at the beginning,\" he said. What was mental state of Germanwings co-pilot? German airline'\n-            \" Lufthansa confirmed Tuesday that co-pilot Andreas Lubitz had battled depression years before he took the\"\n-            \" controls of Germanwings Flight 9525, which he's accused of deliberately crashing last week in the\"\n-            ' French Alps. Lubitz told his Lufthansa flight training school in 2009 that he had a \"previous episode of'\n-            ' severe depression,\" the airline said Tuesday. Email correspondence between Lubitz and the school'\n-            \" discovered in an internal investigation, Lufthansa said, included medical documents he submitted in\"\n-            \" connection with resuming his flight training. The announcement indicates that Lufthansa, the parent\"\n-            \" company of Germanwings, knew of Lubitz's battle with depression, allowed him to continue training and\"\n-            \" ultimately put him in the cockpit. Lufthansa, whose CEO Carsten Spohr previously said Lubitz was 100%\"\n-            ' fit to fly, described its statement Tuesday as a \"swift and seamless clarification\" and said it was'\n-            \" sharing the information and documents -- including training and medical records -- with public\"\n-            \" prosecutors. Spohr traveled to the crash site Wednesday, where recovery teams have been working for the\"\n-            \" past week to recover human remains and plane debris scattered across a steep mountainside. He saw the\"\n-            \" crisis center set up in Seyne-les-Alpes, laid a wreath in the village of Le Vernet, closer to the crash\"\n-            \" site, where grieving families have left flowers at a simple stone memorial. Menichini told CNN late\"\n-            \" Tuesday that no visible human remains were left at the site but recovery teams would keep searching.\"\n-            \" French President Francois Hollande, speaking Tuesday, said that it should be possible to identify all\"\n-            \" the victims using DNA analysis by the end of the week, sooner than authorities had previously suggested.\"\n-            \" In the meantime, the recovery of the victims' personal belongings will start Wednesday, Menichini said.\"\n-            \" Among those personal belongings could be more cell phones belonging to the 144 passengers and six crew\"\n-            \" on board. Check out the latest from our correspondents . The details about Lubitz's correspondence with\"\n-            \" the flight school during his training were among several developments as investigators continued to\"\n-            \" delve into what caused the crash and Lubitz's possible motive for downing the jet. A Lufthansa\"\n-            \" spokesperson told CNN on Tuesday that Lubitz had a valid medical certificate, had passed all his\"\n-            ' examinations and \"held all the licenses required.\" Earlier, a spokesman for the prosecutor\\'s office in'\n-            \" Dusseldorf, Christoph Kumpa, said medical records reveal Lubitz suffered from suicidal tendencies at\"\n-            \" some point before his aviation career and underwent psychotherapy before he got his pilot's license.\"\n-            \" Kumpa emphasized there's no evidence suggesting Lubitz was suicidal or acting aggressively before the\"\n-            \" crash. Investigators are looking into whether Lubitz feared his medical condition would cause him to\"\n-            \" lose his pilot's license, a European government official briefed on the investigation told CNN on\"\n-            ' Tuesday. While flying was \"a big part of his life,\" the source said, it\\'s only one theory being'\n-            \" considered. Another source, a law enforcement official briefed on the investigation, also told CNN that\"\n-            \" authorities believe the primary motive for Lubitz to bring down the plane was that he feared he would\"\n-            \" not be allowed to fly because of his medical problems. Lubitz's girlfriend told investigators he had\"\n-            \" seen an eye doctor and a neuropsychologist, both of whom deemed him unfit to work recently and concluded\"\n-            \" he had psychological issues, the European government official said. But no matter what details emerge\"\n-            \" about his previous mental health struggles, there's more to the story, said Brian Russell, a forensic\"\n-            ' psychologist. \"Psychology can explain why somebody would turn rage inward on themselves about the fact'\n-            \" that maybe they weren't going to keep doing their job and they're upset about that and so they're\"\n-            ' suicidal,\" he said. \"But there is no mental illness that explains why somebody then feels entitled to'\n-            \" also take that rage and turn it outward on 149 other people who had nothing to do with the person's\"\n-            ' problems.\" Germanwings crash compensation: What we know . Who was the captain of Germanwings Flight'\n-            \" 9525? CNN's Margot Haddad reported from Marseille and Pamela Brown from Dusseldorf, while Laura\"\n-            \" Smith-Spark wrote from London. CNN's Frederik Pleitgen, Pamela Boykoff, Antonia Mortensen, Sandrine\"\n-            \" Amiel and Anna-Maja Rappard contributed to this report.\"\n-        )\n-\n-        SHORTER_ARTICLE = (\n-            \" (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on\"\n-            \" Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The\"\n-            \" formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based.\"\n-            \" The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its\"\n-            ' jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East'\n-            ' Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the'\n-            \" situation in Palestinian territories, paving the way for possible war crimes investigations against\"\n-            \" Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and\"\n-            \" the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the\"\n-            \" body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a\"\n-            ' move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the'\n-            ' world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an'\n-            ' ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge'\n-            \" Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the\"\n-            ' Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine'\n-            \" acquires all the rights as well as responsibilities that come with being a State Party to the Statute.\"\n-            ' These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights'\n-            ' Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should'\n-            \" immediately end their pressure, and countries that support universal acceptance of the court's treaty\"\n-            ' should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the'\n-            \" group. \\\"What's objectionable is the attempts to undermine international justice, not Palestine's\"\n-            ' decision to join a treaty to which over 100 countries around the world are members.\" In January, when'\n-            \" the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an\"\n-            ' outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\"'\n-            \" disagreed with the court's decision. \\\"As we have said repeatedly, we do not believe that Palestine is a\"\n-            ' state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in'\n-            ' a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We'\n-            ' will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\"'\n-            \" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the\"\n-            ' territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the'\n-            \" court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou\"\n-            ' Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war'\n-            \" between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry\"\n-            \" will include alleged war crimes committed since June. The International Criminal Court was set up in\"\n-            \" 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder\"\n-            \" and Faith Karimi contributed to this report.\"\n-        )\n-\n-        # The below article tests that we don't add any hypotheses outside of the top n_beams\n-        IRAN_ARTICLE = (\n-            \" (CNN)The United States and its negotiating partners reached a very strong framework agreement with Iran\"\n-            \" in Lausanne, Switzerland, on Thursday that limits Iran's nuclear program in such a way as to effectively\"\n-            \" block it from building a nuclear weapon. Expect pushback anyway, if the recent past is any harbinger.\"\n-            \" Just last month, in an attempt to head off such an agreement, House Speaker John Boehner invited Israeli\"\n-            \" Prime Minister Benjamin Netanyahu to preemptively blast it before Congress, and 47 senators sent a\"\n-            \" letter to the Iranian leadership warning them away from a deal. The debate that has already begun since\"\n-            \" the announcement of the new framework will likely result in more heat than light. It will not be helped\"\n-            \" by the gathering swirl of dubious assumptions and doubtful assertions. Let us address some of these: .\"\n-            \" The most misleading assertion, despite universal rejection by experts, is that the negotiations'\"\n-            \" objective at the outset was the total elimination of any nuclear program in Iran. That is the position\"\n-            \" of Netanyahu and his acolytes in the U.S. Congress. But that is not and never was the objective. If it\"\n-            \" had been, there would have been no Iranian team at the negotiating table. Rather, the objective has\"\n-            \" always been to structure an agreement or series of agreements so that Iran could not covertly develop a\"\n-            \" nuclear arsenal before the United States and its allies could respond. The new framework has exceeded\"\n-            \" expectations in achieving that goal. It would reduce Iran's low-enriched uranium stockpile, cut by\"\n-            \" two-thirds its number of installed centrifuges and implement a rigorous inspection regime. Another\"\n-            \" dubious assumption of opponents is that the Iranian nuclear program is a covert weapons program. Despite\"\n-            \" sharp accusations by some in the United States and its allies, Iran denies having such a program, and\"\n-            \" U.S. intelligence contends that Iran has not yet made the decision to build a nuclear weapon. Iran's\"\n-            \" continued cooperation with International Atomic Energy Agency inspections is further evidence on this\"\n-            \" point, and we'll know even more about Iran's program in the coming months and years because of the deal.\"\n-            \" In fact, the inspections provisions that are part of this agreement are designed to protect against any\"\n-            \" covert action by the Iranians. What's more, the rhetoric of some members of Congress has implied that\"\n-            \" the negotiations have been between only the United States and Iran (i.e., the 47 senators' letter\"\n-            \" warning that a deal might be killed by Congress or a future president). This of course is not the case.\"\n-            \" The talks were between Iran and the five permanent members of the U.N. Security Council (United States,\"\n-            \" United Kingdom, France, China and Russia) plus Germany, dubbed the P5+1. While the United States has\"\n-            \" played a leading role in the effort, it negotiated the terms alongside its partners. If the agreement\"\n-            \" reached by the P5+1 is rejected by Congress, it could result in an unraveling of the sanctions on Iran\"\n-            \" and threaten NATO cohesion in other areas. Another questionable assertion is that this agreement\"\n-            \" contains a sunset clause, after which Iran will be free to do as it pleases. Again, this is not the\"\n-            \" case. Some of the restrictions on Iran's nuclear activities, such as uranium enrichment, will be eased\"\n-            \" or eliminated over time, as long as 15 years. But most importantly, the framework agreement includes\"\n-            \" Iran's ratification of the Additional Protocol, which allows IAEA inspectors expanded access to nuclear\"\n-            \" sites both declared and nondeclared. This provision will be permanent. It does not sunset. Thus, going\"\n-            \" forward, if Iran decides to enrich uranium to weapons-grade levels, monitors will be able to detect such\"\n-            \" a move in a matter of days and alert the U.N. Security Council. Many in Congress have said that the\"\n-            ' agreement should be a formal treaty requiring the Senate to \"advise and consent.\" But the issue is not'\n-            \" suited for a treaty. Treaties impose equivalent obligations on all signatories. For example, the New\"\n-            \" START treaty limits Russia and the United States to 1,550 deployed strategic warheads. But any agreement\"\n-            \" with Iran will not be so balanced.  The restrictions and obligations in the final framework agreement\"\n-            \" will be imposed almost exclusively on Iran. The P5+1 are obligated only to ease and eventually remove\"\n-            \" most but not all economic sanctions, which were imposed as leverage to gain this final deal. Finally\"\n-            \" some insist that any agreement must address Iranian missile programs, human rights violations or support\"\n-            \" for Hamas or Hezbollah.  As important as these issues are, and they must indeed be addressed, they are\"\n-            \" unrelated to the most important aim of a nuclear deal: preventing a nuclear Iran.  To include them in\"\n-            \" the negotiations would be a poison pill. This agreement should be judged on its merits and on how it\"\n-            \" affects the security of our negotiating partners and allies, including Israel. Those judgments should be\"\n-            \" fact-based, not based on questionable assertions or dubious assumptions.\"\n-        )\n-\n-        ARTICLE_SUBWAY = (\n-            \" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York. A\"\n-            \" year later, she got married again in Westchester County, but to a different man and without divorcing\"\n-            \" her first husband.  Only 18 days after that marriage, she got hitched yet again. Then, Barrientos\"\n-            ' declared \"I do\" five more times, sometimes only within two weeks of each other. In 2010, she married'\n-            \" once more, this time in the Bronx. In an application for a marriage license, she stated it was her\"\n-            ' \"first and only\" marriage. Barrientos, now 39, is facing two criminal counts of \"offering a false'\n-            ' instrument for filing in the first degree,\" referring to her false statements on the 2010 marriage'\n-            \" license application, according to court documents. Prosecutors said the marriages were part of an\"\n-            \" immigration scam. On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to\"\n-            \" her attorney, Christopher Wright, who declined to comment further. After leaving court, Barrientos was\"\n-            \" arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New\"\n-            \" York subway through an emergency exit, said Detective Annette Markowski, a police spokeswoman. In total,\"\n-            \" Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.  All\"\n-            \" occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be\"\n-            \" married to four men, and at one time, she was married to eight men at once, prosecutors say. Prosecutors\"\n-            \" said the immigration scam involved some of her husbands, who filed for permanent residence status\"\n-            \" shortly after the marriages.  Any divorces happened only after such filings were approved. It was\"\n-            \" unclear whether any of the men will be prosecuted. The case was referred to the Bronx District\"\n-            \" Attorney's Office by Immigration and Customs Enforcement and the Department of Homeland Security's\"\n-            ' Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt,'\n-            \" Turkey, Georgia, Pakistan and Mali. Her eighth husband, Rashid Rajput, was deported in 2006 to his\"\n-            \" native Pakistan after an investigation by the Joint Terrorism Task Force. If convicted, Barrientos faces\"\n-            \" up to four years in prison.  Her next court appearance is scheduled for May 18.\"\n-        )\n-\n-        dct = tokenizer.batch_encode_plus(\n-            [FRANCE_ARTICLE, SHORTER_ARTICLE, IRAN_ARTICLE, ARTICLE_SUBWAY],\n-            max_length=1024,\n-            padding=\"max_length\",\n-            truncation_strategy=\"only_first\",\n-            truncation=True,\n-            return_tensors=\"np\",\n-        )\n-\n-        self.assertEqual(1024, dct[\"input_ids\"].shape[1])\n-        hypotheses_batch = model.generate(\n-            input_ids=dct[\"input_ids\"],\n-            attention_mask=dct[\"attention_mask\"],\n-            num_beams=2,\n-        ).sequences\n-        assert (hypotheses_batch[:, 1] == 0).all().item()\n-\n-        EXPECTED = [\n-            \"A French prosecutor says he is not aware of any video footage from on board the plane. Two German\"\n-            \" magazines claim to have found a cell phone video showing the crash. The publications say they watched\"\n-            \" the video, which was found by a source close to the investigation. All 150 on board the Germanwings\"\n-            \" flight were killed.\",\n-            \"Palestinian Authority becomes 123rd member of the International Criminal Court. The move gives the court\"\n-            \" jurisdiction over alleged crimes in Palestinian territories. Israel and the United States opposed the\"\n-            \" Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki said it was a\"\n-            \" move toward greater justice.\",\n-            \"U.S. and its negotiating partners reached a strong framework agreement with Iran. Peter Bergen: The\"\n-            \" debate that has already begun will likely result in more heat than light. Bergen: The most misleading\"\n-            \" assertion is that the negotiations' objective at the outset was the total elimination of any nuclear\"\n-            \" program.\",\n-            \"Liana Barrientos, 39, has been married 10 times, sometimes within two weeks of each other. Prosecutors\"\n-            \" say the marriages were part of an immigration scam. She pleaded not guilty at State Supreme Court in the\"\n-            \" Bronx on Friday. If convicted, Barrientos faces up to four years in prison.\",\n-        ]\n-\n-        generated_summaries = tokenizer.batch_decode(\n-            hypotheses_batch.tolist(), clean_up_tokenization_spaces=True, skip_special_tokens=True\n-        )\n-        assert generated_summaries == EXPECTED\n-\n-\n-class FlaxBartStandaloneDecoderModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_attention_mask=True,\n-        use_labels=False,\n-        vocab_size=99,\n-        hidden_size=16,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=4,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=32,\n-        eos_token_id=2,\n-        pad_token_id=1,\n-        bos_token_id=0,\n-        initializer_range=0.02,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_attention_mask = use_attention_mask\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-        self.initializer_range = initializer_range\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = jnp.clip(ids_tensor([self.batch_size, self.seq_length], self.vocab_size), 3, self.vocab_size)\n-\n-        attention_mask = None\n-        if self.use_attention_mask:\n-            attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        config = BartConfig(\n-            vocab_size=self.vocab_size,\n-            d_model=self.hidden_size,\n-            encoder_layers=self.num_hidden_layers,\n-            decoder_layers=self.num_hidden_layers,\n-            encoder_attention_heads=self.num_attention_heads,\n-            decoder_attention_heads=self.num_attention_heads,\n-            encoder_ffn_dim=self.intermediate_size,\n-            decoder_ffn_dim=self.intermediate_size,\n-            dropout=self.hidden_dropout_prob,\n-            attention_dropout=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            eos_token_id=self.eos_token_id,\n-            bos_token_id=self.bos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            initializer_range=self.initializer_range,\n-            use_cache=False,\n-        )\n-\n-        return config, input_ids, attention_mask\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-        return config, inputs_dict\n-\n-    def prepare_config_and_inputs_for_decoder(self):\n-        config, input_ids, attention_mask = self.prepare_config_and_inputs()\n-\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            attention_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )"
        },
        {
            "sha": "63b2e8bd6d8aa0eb869b2a14a255d24b34c48bb1",
            "filename": "tests/models/bart/test_modeling_tf_bart.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1131,
            "changes": 1131,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbart%2Ftest_modeling_tf_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbart%2Ftest_modeling_tf_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_tf_bart.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,1131 +0,0 @@\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from __future__ import annotations\n-\n-import copy\n-import tempfile\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import BartConfig, BartTokenizer, is_tf_available\n-from transformers.testing_utils import require_tf, slow\n-from transformers.utils import cached_property\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-from ...utils.test_modeling_tf_core import TFCoreModelTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import TFBartForConditionalGeneration, TFBartForSequenceClassification, TFBartModel\n-\n-\n-@require_tf\n-class TFBartModelTester:\n-    config_cls = BartConfig\n-    config_updates = {}\n-    hidden_act = \"gelu\"\n-\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_labels=False,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=20,\n-        eos_token_id=2,\n-        pad_token_id=1,\n-        bos_token_id=0,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        # Ids are clipped to avoid \"beginng of sequence\", \"end of sequence\", and \"pad\" tokens\n-        input_ids = tf.clip_by_value(\n-            ids_tensor([self.batch_size, self.seq_length - 1], self.vocab_size),\n-            clip_value_min=self.eos_token_id + 1,\n-            clip_value_max=self.vocab_size + 1,\n-        )\n-        # Explicitly add \"end of sequence\" to the inputs\n-        eos_tensor = tf.expand_dims(tf.constant([self.eos_token_id] * self.batch_size), 1)\n-        input_ids = tf.concat([input_ids, eos_tensor], axis=1)\n-\n-        decoder_input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        config = self.config_cls(\n-            vocab_size=self.vocab_size,\n-            d_model=self.hidden_size,\n-            encoder_layers=self.num_hidden_layers,\n-            decoder_layers=self.num_hidden_layers,\n-            encoder_attention_heads=self.num_attention_heads,\n-            decoder_attention_heads=self.num_attention_heads,\n-            encoder_ffn_dim=self.intermediate_size,\n-            decoder_ffn_dim=self.intermediate_size,\n-            dropout=self.hidden_dropout_prob,\n-            attention_dropout=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            eos_token_ids=[2],\n-            bos_token_id=self.bos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            decoder_start_token_id=self.pad_token_id,\n-            **self.config_updates,\n-        )\n-        inputs_dict = prepare_bart_inputs_dict(config, input_ids, decoder_input_ids)\n-        return config, inputs_dict\n-\n-    def check_decoder_model_past_large_inputs(self, config, inputs_dict):\n-        model = TFBartModel(config=config).get_decoder()\n-        input_ids = inputs_dict[\"input_ids\"]\n-\n-        input_ids = input_ids[:1, :]\n-        attention_mask = inputs_dict[\"attention_mask\"][:1, :]\n-        self.batch_size = 1\n-\n-        # first forward pass\n-        outputs = model(input_ids, attention_mask=attention_mask, use_cache=True)\n-\n-        output, past_key_values = outputs.to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_attn_mask = tf.cast(ids_tensor((self.batch_size, 3), 2), tf.int8)\n-\n-        # append to next input_ids and\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        next_attention_mask = tf.concat([attention_mask, next_attn_mask], axis=-1)\n-\n-        output_from_no_past = model(next_input_ids, attention_mask=next_attention_mask)\n-        output_from_no_past = output_from_no_past[0]\n-\n-        output_from_past = model(next_tokens, attention_mask=next_attention_mask, past_key_values=past_key_values)\n-        output_from_past = output_from_past[0]\n-\n-        self.parent.assertEqual(next_tokens.shape[1], output_from_past.shape[1])\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), output_from_past.shape[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)\n-\n-\n-def prepare_bart_inputs_dict(\n-    config,\n-    input_ids,\n-    decoder_input_ids,\n-    attention_mask=None,\n-    decoder_attention_mask=None,\n-):\n-    if attention_mask is None:\n-        attention_mask = tf.cast(tf.math.not_equal(input_ids, config.pad_token_id), tf.int8)\n-    if decoder_attention_mask is None:\n-        decoder_attention_mask = tf.concat(\n-            [\n-                tf.ones(decoder_input_ids[:, :1].shape, dtype=tf.int8),\n-                tf.cast(tf.math.not_equal(decoder_input_ids[:, 1:], config.pad_token_id), tf.int8),\n-            ],\n-            axis=-1,\n-        )\n-    return {\n-        \"input_ids\": input_ids,\n-        \"decoder_input_ids\": decoder_input_ids,\n-        \"attention_mask\": attention_mask,\n-        \"decoder_attention_mask\": decoder_attention_mask,\n-    }\n-\n-\n-@require_tf\n-class TFBartModelTest(TFModelTesterMixin, TFCoreModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (TFBartForConditionalGeneration, TFBartForSequenceClassification, TFBartModel) if is_tf_available() else ()\n-    )\n-    all_generative_model_classes = (TFBartForConditionalGeneration,) if is_tf_available() else ()\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFBartModel,\n-            \"summarization\": TFBartForConditionalGeneration,\n-            \"text-classification\": TFBartForSequenceClassification,\n-            \"text2text-generation\": TFBartForConditionalGeneration,\n-            \"translation\": TFBartForConditionalGeneration,\n-            \"zero-shot\": TFBartForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    is_encoder_decoder = True\n-    test_pruning = False\n-    test_onnx = True\n-    onnx_min_opset = 10\n-\n-    def setUp(self):\n-        self.model_tester = TFBartModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=BartConfig)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_decoder_model_past_large_inputs(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-        self.model_tester.check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n-    # TODO (Joao): fix me\n-    @unittest.skip(\"Onnx compliance broke with TF 2.10\")\n-    def test_onnx_compliancy(self):\n-        pass\n-\n-    # TFBartForSequenceClassification does not support inputs_embeds\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in (TFBartForConditionalGeneration, TFBartModel):\n-            model = model_class(config)\n-\n-            inputs = copy.deepcopy(inputs_dict)\n-\n-            if not self.is_encoder_decoder:\n-                input_ids = inputs[\"input_ids\"]\n-                del inputs[\"input_ids\"]\n-            else:\n-                encoder_input_ids = inputs[\"input_ids\"]\n-                decoder_input_ids = inputs.get(\"decoder_input_ids\", encoder_input_ids)\n-                del inputs[\"input_ids\"]\n-                inputs.pop(\"decoder_input_ids\", None)\n-\n-            if not self.is_encoder_decoder:\n-                inputs[\"inputs_embeds\"] = model.get_input_embeddings()(input_ids)\n-            else:\n-                inputs[\"inputs_embeds\"] = model.get_input_embeddings()(encoder_input_ids)\n-                inputs[\"decoder_inputs_embeds\"] = model.get_input_embeddings()(decoder_input_ids)\n-\n-            inputs = self._prepare_for_class(inputs, model_class)\n-\n-            model(inputs)\n-\n-    # TFBartForSequenceClassification does not support inputs_embeds\n-    @slow\n-    def test_graph_mode_with_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in (TFBartForConditionalGeneration, TFBartModel):\n-            model = model_class(config)\n-\n-            inputs = copy.deepcopy(inputs_dict)\n-\n-            if not self.is_encoder_decoder:\n-                input_ids = inputs[\"input_ids\"]\n-                del inputs[\"input_ids\"]\n-            else:\n-                encoder_input_ids = inputs[\"input_ids\"]\n-                decoder_input_ids = inputs.get(\"decoder_input_ids\", encoder_input_ids)\n-                del inputs[\"input_ids\"]\n-                inputs.pop(\"decoder_input_ids\", None)\n-\n-            if not self.is_encoder_decoder:\n-                inputs[\"inputs_embeds\"] = model.get_input_embeddings()(input_ids)\n-            else:\n-                inputs[\"inputs_embeds\"] = model.get_input_embeddings()(encoder_input_ids)\n-                inputs[\"decoder_inputs_embeds\"] = model.get_input_embeddings()(decoder_input_ids)\n-\n-            inputs = self._prepare_for_class(inputs, model_class)\n-\n-            @tf.function\n-            def run_in_graph_mode():\n-                return model(inputs)\n-\n-            outputs = run_in_graph_mode()\n-            self.assertIsNotNone(outputs)\n-\n-    @slow\n-    def test_save_load_after_resize_token_embeddings(self):\n-        # Custom version of this test to ensure \"end of sequence\" tokens are present throughout\n-        if not self.test_resize_embeddings:\n-            return\n-        config, original_inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            # create a model with resized (expended) embeddings\n-            new_tokens_size = 10\n-            old_total_size = config.vocab_size\n-            new_total_size = old_total_size + new_tokens_size\n-            model = model_class(config=copy.deepcopy(config))  # `resize_token_embeddings` mutates `config`\n-            model.build_in_name_scope()\n-            model.resize_token_embeddings(new_total_size)\n-\n-            # fetch the output for an input exclusively made of new members of the vocabulary\n-            inputs_dict = copy.deepcopy(original_inputs_dict)\n-            ids_feat_name = None\n-            if \"input_ids\" in inputs_dict:\n-                ids_feat_name = \"input_ids\"\n-            elif \"decoder_input_ids\" in inputs_dict:\n-                ids_feat_name = \"decoder_input_ids\"\n-            else:\n-                assert False, \"No input ids feature found in the inputs dict\"\n-\n-            new_vocab_input_ids = ids_tensor(inputs_dict[ids_feat_name].shape, new_tokens_size)\n-            new_vocab_input_ids += old_total_size\n-\n-            # Replace last id with EOS token\n-            new_vocab_input_ids = new_vocab_input_ids[:, :-1]\n-            new_vocab_input_ids = tf.concat(\n-                [new_vocab_input_ids, tf.ones((tf.shape(new_vocab_input_ids)[0], 1), dtype=tf.int32) * 2], axis=1\n-            )\n-\n-            inputs_dict[ids_feat_name] = new_vocab_input_ids\n-            if \"input_ids\" in inputs_dict:\n-                inputs_dict[\"input_ids\"] = new_vocab_input_ids\n-            if \"decoder_input_ids\" in inputs_dict:\n-                inputs_dict[\"decoder_input_ids\"] = new_vocab_input_ids\n-            prepared_inputs = self._prepare_for_class(inputs_dict, model_class)\n-            outputs = model(**prepared_inputs)\n-\n-            # save and load the model\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname, saved_model=False)\n-                model = model_class.from_pretrained(tmpdirname)\n-                restored_model_outputs = model(**prepared_inputs)\n-\n-                # check that the output for the restored model is the same\n-                self.assert_outputs_same(restored_model_outputs, outputs)\n-\n-\n-def _long_tensor(tok_lst):\n-    return tf.constant(tok_lst, dtype=tf.int32)\n-\n-\n-@require_tf\n-class TFBartHeadTests(unittest.TestCase):\n-    vocab_size = 99\n-\n-    def _get_config_and_data(self):\n-        eos_column_vector = tf.ones((4, 1), dtype=tf.int32) * 2\n-        input_ids = tf.concat([ids_tensor((4, 6), self.vocab_size - 3) + 3, eos_column_vector], axis=1)\n-        batch_size = input_ids.shape[0]\n-        config = BartConfig(\n-            vocab_size=self.vocab_size,\n-            d_model=24,\n-            encoder_layers=2,\n-            decoder_layers=2,\n-            encoder_attention_heads=2,\n-            decoder_attention_heads=2,\n-            encoder_ffn_dim=32,\n-            decoder_ffn_dim=32,\n-            max_position_embeddings=48,\n-            eos_token_id=2,\n-            pad_token_id=1,\n-            bos_token_id=0,\n-            decoder_start_token_id=2,\n-        )\n-        return config, input_ids, batch_size\n-\n-    def test_lm_forward(self):\n-        config, input_ids, batch_size = self._get_config_and_data()\n-        decoder_lm_labels = ids_tensor([batch_size, input_ids.shape[1]], self.vocab_size)\n-        lm_model = TFBartForConditionalGeneration(config)\n-        outputs = lm_model(input_ids=input_ids, labels=decoder_lm_labels, decoder_input_ids=input_ids, use_cache=False)\n-        expected_shape = (batch_size, input_ids.shape[1], config.vocab_size)\n-        self.assertEqual(outputs.logits.shape, expected_shape)\n-\n-    def test_lm_uneven_forward(self):\n-        config = BartConfig(\n-            vocab_size=10,\n-            d_model=24,\n-            encoder_layers=2,\n-            decoder_layers=2,\n-            encoder_attention_heads=2,\n-            decoder_attention_heads=2,\n-            encoder_ffn_dim=32,\n-            decoder_ffn_dim=32,\n-            max_position_embeddings=48,\n-        )\n-        lm_model = TFBartForConditionalGeneration(config)\n-        context = tf.fill((7, 2), 4)\n-        summary = tf.fill((7, 7), 6)\n-        outputs = lm_model(input_ids=context, decoder_input_ids=summary, use_cache=False)\n-        expected_shape = (*summary.shape, config.vocab_size)\n-        self.assertEqual(outputs.logits.shape, expected_shape)\n-\n-\n-@require_tf\n-class TFBartForSequenceClassificationTest(unittest.TestCase):\n-    def test_model_fails_for_uneven_eos_tokens(self):\n-        config = BartConfig(eos_token_id=2)\n-        model = TFBartForSequenceClassification(config)\n-        inputs = {\n-            \"input_ids\": tf.constant([[1, 2, 2, 2], [1, 3, 2, 2], [2, 2, 3, 3]]),\n-            \"attention_mask\": tf.constant([[1, 1, 1, 1], [1, 1, 1, 1], [1, 1, 1, 1]]),\n-        }\n-        with self.assertRaises(tf.errors.InvalidArgumentError):\n-            model(inputs)\n-\n-\n-@slow\n-@require_tf\n-class TFBartModelIntegrationTest(unittest.TestCase):\n-    def test_inference_no_head(self):\n-        model = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-large\").model\n-\n-        input_ids = _long_tensor([[0, 31414, 232, 328, 740, 1140, 12695, 69, 46078, 1588, 2]])\n-        attention_mask = tf.cast(tf.math.not_equal(input_ids, model.config.pad_token_id), tf.int8)\n-        output = model(input_ids=input_ids, attention_mask=attention_mask)[0]\n-        expected_shape = (1, 11, 1024)\n-        self.assertEqual(output.shape, expected_shape)\n-        expected_slice = tf.convert_to_tensor(\n-            [[0.7144, 0.8143, -1.2813], [0.7144, 0.8143, -1.2813], [-0.0467, 2.5911, -2.1845]],\n-        )\n-        tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=1e-3)\n-\n-    def test_cnn_summarization_same_as_fairseq_hard(self):\n-        hf = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n-        tok = self.tok\n-\n-        FRANCE_ARTICLE = (  # @noqa\n-            \" Marseille, France (CNN)The French prosecutor leading an investigation into the crash of Germanwings\"\n-            \" Flight 9525 insisted Wednesday that he was not aware of any video footage from on board the plane.\"\n-            ' Marseille prosecutor Brice Robin told CNN that \"so far no videos were used in the crash investigation.\"'\n-            ' He added, \"A person who has such a video needs to immediately give it to the investigators.\" Robin\\'s'\n-            \" comments follow claims by two magazines, German daily Bild and French Paris Match, of a cell phone video\"\n-            \" showing the harrowing final seconds from on board Germanwings Flight 9525 as it crashed into the French\"\n-            \" Alps. All 150 on board were killed. Paris Match and Bild reported that the video was recovered from a\"\n-            \" phone at the wreckage site. The two publications described the supposed video, but did not post it on\"\n-            \" their websites. The publications said that they watched the video, which was found by a source close to\"\n-            \" the investigation. \\\"One can hear cries of 'My God' in several languages,\\\" Paris Match reported.\"\n-            ' \"Metallic banging can also be heard more than three times, perhaps of the pilot trying to open the'\n-            \" cockpit door with a heavy object.  Towards the end, after a heavy shake, stronger than the others, the\"\n-            ' screaming intensifies. Then nothing.\" \"It is a very disturbing scene,\" said Julian Reichelt,'\n-            \" editor-in-chief of Bild online. An official with France's accident investigation agency, the BEA, said\"\n-            \" the agency is not aware of any such video. Lt. Col. Jean-Marc Menichini, a French Gendarmerie spokesman\"\n-            \" in charge of communications on rescue efforts around the Germanwings crash site, told CNN that the\"\n-            ' reports were \"completely wrong\" and \"unwarranted.\" Cell phones have been collected at the site, he said,'\n-            ' but that they \"hadn\\'t been exploited yet.\" Menichini said he believed the cell phones would need to be'\n-            \" sent to the Criminal Research Institute in Rosny sous-Bois, near Paris, in order to be analyzed by\"\n-            \" specialized technicians working hand-in-hand with investigators. But none of the cell phones found so\"\n-            \" far have been sent to the institute, Menichini said. Asked whether staff involved in the search could\"\n-            ' have leaked a memory card to the media, Menichini answered with a categorical \"no.\" Reichelt told \"Erin'\n-            ' Burnett: Outfront\" that he had watched the video and stood by the report, saying Bild and Paris Match'\n-            ' are \"very confident\" that the clip is real. He noted that investigators only revealed they\\'d recovered'\n-            ' cell phones from the crash site after Bild and Paris Match published their reports. \"That is something'\n-            \" we did not know before. ... Overall we can say many things of the investigation weren't revealed by the\"\n-            ' investigation at the beginning,\" he said. What was mental state of Germanwings co-pilot? German airline'\n-            \" Lufthansa confirmed Tuesday that co-pilot Andreas Lubitz had battled depression years before he took the\"\n-            \" controls of Germanwings Flight 9525, which he's accused of deliberately crashing last week in the\"\n-            ' French Alps. Lubitz told his Lufthansa flight training school in 2009 that he had a \"previous episode of'\n-            ' severe depression,\" the airline said Tuesday. Email correspondence between Lubitz and the school'\n-            \" discovered in an internal investigation, Lufthansa said, included medical documents he submitted in\"\n-            \" connection with resuming his flight training. The announcement indicates that Lufthansa, the parent\"\n-            \" company of Germanwings, knew of Lubitz's battle with depression, allowed him to continue training and\"\n-            \" ultimately put him in the cockpit. Lufthansa, whose CEO Carsten Spohr previously said Lubitz was 100%\"\n-            ' fit to fly, described its statement Tuesday as a \"swift and seamless clarification\" and said it was'\n-            \" sharing the information and documents -- including training and medical records -- with public\"\n-            \" prosecutors. Spohr traveled to the crash site Wednesday, where recovery teams have been working for the\"\n-            \" past week to recover human remains and plane debris scattered across a steep mountainside. He saw the\"\n-            \" crisis center set up in Seyne-les-Alpes, laid a wreath in the village of Le Vernet, closer to the crash\"\n-            \" site, where grieving families have left flowers at a simple stone memorial. Menichini told CNN late\"\n-            \" Tuesday that no visible human remains were left at the site but recovery teams would keep searching.\"\n-            \" French President Francois Hollande, speaking Tuesday, said that it should be possible to identify all\"\n-            \" the victims using DNA analysis by the end of the week, sooner than authorities had previously suggested.\"\n-            \" In the meantime, the recovery of the victims' personal belongings will start Wednesday, Menichini said.\"\n-            \" Among those personal belongings could be more cell phones belonging to the 144 passengers and six crew\"\n-            \" on board. Check out the latest from our correspondents . The details about Lubitz's correspondence with\"\n-            \" the flight school during his training were among several developments as investigators continued to\"\n-            \" delve into what caused the crash and Lubitz's possible motive for downing the jet. A Lufthansa\"\n-            \" spokesperson told CNN on Tuesday that Lubitz had a valid medical certificate, had passed all his\"\n-            ' examinations and \"held all the licenses required.\" Earlier, a spokesman for the prosecutor\\'s office in'\n-            \" Dusseldorf, Christoph Kumpa, said medical records reveal Lubitz suffered from suicidal tendencies at\"\n-            \" some point before his aviation career and underwent psychotherapy before he got his pilot's license.\"\n-            \" Kumpa emphasized there's no evidence suggesting Lubitz was suicidal or acting aggressively before the\"\n-            \" crash. Investigators are looking into whether Lubitz feared his medical condition would cause him to\"\n-            \" lose his pilot's license, a European government official briefed on the investigation told CNN on\"\n-            ' Tuesday. While flying was \"a big part of his life,\" the source said, it\\'s only one theory being'\n-            \" considered. Another source, a law enforcement official briefed on the investigation, also told CNN that\"\n-            \" authorities believe the primary motive for Lubitz to bring down the plane was that he feared he would\"\n-            \" not be allowed to fly because of his medical problems. Lubitz's girlfriend told investigators he had\"\n-            \" seen an eye doctor and a neuropsychologist, both of whom deemed him unfit to work recently and concluded\"\n-            \" he had psychological issues, the European government official said. But no matter what details emerge\"\n-            \" about his previous mental health struggles, there's more to the story, said Brian Russell, a forensic\"\n-            ' psychologist. \"Psychology can explain why somebody would turn rage inward on themselves about the fact'\n-            \" that maybe they weren't going to keep doing their job and they're upset about that and so they're\"\n-            ' suicidal,\" he said. \"But there is no mental illness that explains why somebody then feels entitled to'\n-            \" also take that rage and turn it outward on 149 other people who had nothing to do with the person's\"\n-            ' problems.\" Germanwings crash compensation: What we know . Who was the captain of Germanwings Flight'\n-            \" 9525? CNN's Margot Haddad reported from Marseille and Pamela Brown from Dusseldorf, while Laura\"\n-            \" Smith-Spark wrote from London. CNN's Frederik Pleitgen, Pamela Boykoff, Antonia Mortensen, Sandrine\"\n-            \" Amiel and Anna-Maja Rappard contributed to this report.\"\n-        )\n-        EXPECTED_SUMMARY_FRANCE = (\n-            \"French prosecutor says he's not aware of any video footage from on board the plane. German daily Bild\"\n-            \" and French Paris Match claim to have found a cell phone video of the crash. A French Gendarmerie\"\n-            ' spokesman calls the reports \"completely wrong\" and \"unwarranted\" German airline Lufthansa confirms'\n-            \" co-pilot Andreas Lubitz had battled depression.\"\n-        )\n-\n-        SHORTER_ARTICLE = (\n-            \" (CNN)The Palestinian Authority officially became the 123rd member of the International Criminal Court on\"\n-            \" Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The\"\n-            \" formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based.\"\n-            \" The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its\"\n-            ' jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East'\n-            ' Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the'\n-            \" situation in Palestinian territories, paving the way for possible war crimes investigations against\"\n-            \" Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and\"\n-            \" the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the\"\n-            \" body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a\"\n-            ' move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the'\n-            ' world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an'\n-            ' ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge'\n-            \" Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the\"\n-            ' Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine'\n-            \" acquires all the rights as well as responsibilities that come with being a State Party to the Statute.\"\n-            ' These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights'\n-            ' Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should'\n-            \" immediately end their pressure, and countries that support universal acceptance of the court's treaty\"\n-            ' should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the'\n-            \" group. \\\"What's objectionable is the attempts to undermine international justice, not Palestine's\"\n-            ' decision to join a treaty to which over 100 countries around the world are members.\" In January, when'\n-            \" the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an\"\n-            ' outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\"'\n-            \" disagreed with the court's decision. \\\"As we have said repeatedly, we do not believe that Palestine is a\"\n-            ' state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in'\n-            ' a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We'\n-            ' will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\"'\n-            \" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the\"\n-            ' territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the'\n-            \" court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou\"\n-            ' Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war'\n-            \" between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry\"\n-            \" will include alleged war crimes committed since June. The International Criminal Court was set up in\"\n-            \" 2002 to prosecute genocide, crimes against humanity and war crimes. CNN's Vasco Cotovio, Kareem Khadder\"\n-            \" and Faith Karimi contributed to this report.\"\n-        )\n-        EXPECTED_SUMMARY_SHORTER = (\n-            \"The Palestinian Authority becomes the 123rd member of the International Criminal Court. The move gives\"\n-            \" the court jurisdiction over alleged crimes in Palestinian territories. Israel and the United States\"\n-            \" opposed the Palestinians' efforts to join the body. But Palestinian Foreign Minister Riad al-Malki said\"\n-            \" it was a move toward greater justice.\"\n-        )\n-\n-        # The below article tests that we don't add any hypotheses outside of the top n_beams\n-        IRAN_ARTICLE = (\n-            \" (CNN)The United States and its negotiating partners reached a very strong framework agreement with Iran\"\n-            \" in Lausanne, Switzerland, on Thursday that limits Iran's nuclear program in such a way as to effectively\"\n-            \" block it from building a nuclear weapon. Expect pushback anyway, if the recent past is any harbinger.\"\n-            \" Just last month, in an attempt to head off such an agreement, House Speaker John Boehner invited Israeli\"\n-            \" Prime Minister Benjamin Netanyahu to preemptively blast it before Congress, and 47 senators sent a\"\n-            \" letter to the Iranian leadership warning them away from a deal. The debate that has already begun since\"\n-            \" the announcement of the new framework will likely result in more heat than light. It will not be helped\"\n-            \" by the gathering swirl of dubious assumptions and doubtful assertions. Let us address some of these: .\"\n-            \" The most misleading assertion, despite universal rejection by experts, is that the negotiations'\"\n-            \" objective at the outset was the total elimination of any nuclear program in Iran. That is the position\"\n-            \" of Netanyahu and his acolytes in the U.S. Congress. But that is not and never was the objective. If it\"\n-            \" had been, there would have been no Iranian team at the negotiating table. Rather, the objective has\"\n-            \" always been to structure an agreement or series of agreements so that Iran could not covertly develop a\"\n-            \" nuclear arsenal before the United States and its allies could respond. The new framework has exceeded\"\n-            \" expectations in achieving that goal. It would reduce Iran's low-enriched uranium stockpile, cut by\"\n-            \" two-thirds its number of installed centrifuges and implement a rigorous inspection regime. Another\"\n-            \" dubious assumption of opponents is that the Iranian nuclear program is a covert weapons program. Despite\"\n-            \" sharp accusations by some in the United States and its allies, Iran denies having such a program, and\"\n-            \" U.S. intelligence contends that Iran has not yet made the decision to build a nuclear weapon. Iran's\"\n-            \" continued cooperation with International Atomic Energy Agency inspections is further evidence on this\"\n-            \" point, and we'll know even more about Iran's program in the coming months and years because of the deal.\"\n-            \" In fact, the inspections provisions that are part of this agreement are designed to protect against any\"\n-            \" covert action by the Iranians. What's more, the rhetoric of some members of Congress has implied that\"\n-            \" the negotiations have been between only the United States and Iran (i.e., the 47 senators' letter\"\n-            \" warning that a deal might be killed by Congress or a future president). This of course is not the case.\"\n-            \" The talks were between Iran and the five permanent members of the U.N. Security Council (United States,\"\n-            \" United Kingdom, France, China and Russia) plus Germany, dubbed the P5+1. While the United States has\"\n-            \" played a leading role in the effort, it negotiated the terms alongside its partners. If the agreement\"\n-            \" reached by the P5+1 is rejected by Congress, it could result in an unraveling of the sanctions on Iran\"\n-            \" and threaten NATO cohesion in other areas. Another questionable assertion is that this agreement\"\n-            \" contains a sunset clause, after which Iran will be free to do as it pleases. Again, this is not the\"\n-            \" case. Some of the restrictions on Iran's nuclear activities, such as uranium enrichment, will be eased\"\n-            \" or eliminated over time, as long as 15 years. But most importantly, the framework agreement includes\"\n-            \" Iran's ratification of the Additional Protocol, which allows IAEA inspectors expanded access to nuclear\"\n-            \" sites both declared and nondeclared. This provision will be permanent. It does not sunset. Thus, going\"\n-            \" forward, if Iran decides to enrich uranium to weapons-grade levels, monitors will be able to detect such\"\n-            \" a move in a matter of days and alert the U.N. Security Council. Many in Congress have said that the\"\n-            ' agreement should be a formal treaty requiring the Senate to \"advise and consent.\" But the issue is not'\n-            \" suited for a treaty. Treaties impose equivalent obligations on all signatories. For example, the New\"\n-            \" START treaty limits Russia and the United States to 1,550 deployed strategic warheads. But any agreement\"\n-            \" with Iran will not be so balanced.  The restrictions and obligations in the final framework agreement\"\n-            \" will be imposed almost exclusively on Iran. The P5+1 are obligated only to ease and eventually remove\"\n-            \" most but not all economic sanctions, which were imposed as leverage to gain this final deal. Finally\"\n-            \" some insist that any agreement must address Iranian missile programs, human rights violations or support\"\n-            \" for Hamas or Hezbollah.  As important as these issues are, and they must indeed be addressed, they are\"\n-            \" unrelated to the most important aim of a nuclear deal: preventing a nuclear Iran.  To include them in\"\n-            \" the negotiations would be a poison pill. This agreement should be judged on its merits and on how it\"\n-            \" affects the security of our negotiating partners and allies, including Israel. Those judgments should be\"\n-            \" fact-based, not based on questionable assertions or dubious assumptions.\"\n-        )\n-        EXPECTED_SUMMARY_IRAN = (\n-            \"The U.S. and its negotiating partners reached a very strong framework agreement with Iran. Peter Bergen:\"\n-            \" The debate that has already begun will likely result in more heat than light. He says the agreement\"\n-            \" limits Iran's nuclear program in such a way as to effectively block it from building a nuclear weapon.\"\n-            \" Bergen says the most important aim of a nuclear deal is preventing a nuclear Iran.\"\n-        )\n-\n-        ARTICLE_SUBWAY = (\n-            \" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York. A\"\n-            \" year later, she got married again in Westchester County, but to a different man and without divorcing\"\n-            \" her first husband.  Only 18 days after that marriage, she got hitched yet again. Then, Barrientos\"\n-            ' declared \"I do\" five more times, sometimes only within two weeks of each other. In 2010, she married'\n-            \" once more, this time in the Bronx. In an application for a marriage license, she stated it was her\"\n-            ' \"first and only\" marriage. Barrientos, now 39, is facing two criminal counts of \"offering a false'\n-            ' instrument for filing in the first degree,\" referring to her false statements on the 2010 marriage'\n-            \" license application, according to court documents. Prosecutors said the marriages were part of an\"\n-            \" immigration scam. On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to\"\n-            \" her attorney, Christopher Wright, who declined to comment further. After leaving court, Barrientos was\"\n-            \" arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New\"\n-            \" York subway through an emergency exit, said Detective Annette Markowski, a police spokeswoman. In total,\"\n-            \" Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.  All\"\n-            \" occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be\"\n-            \" married to four men, and at one time, she was married to eight men at once, prosecutors say. Prosecutors\"\n-            \" said the immigration scam involved some of her husbands, who filed for permanent residence status\"\n-            \" shortly after the marriages.  Any divorces happened only after such filings were approved. It was\"\n-            \" unclear whether any of the men will be prosecuted. The case was referred to the Bronx District\"\n-            \" Attorney's Office by Immigration and Customs Enforcement and the Department of Homeland Security's\"\n-            ' Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt,'\n-            \" Turkey, Georgia, Pakistan and Mali. Her eighth husband, Rashid Rajput, was deported in 2006 to his\"\n-            \" native Pakistan after an investigation by the Joint Terrorism Task Force. If convicted, Barrientos faces\"\n-            \" up to four years in prison.  Her next court appearance is scheduled for May 18.\"\n-        )\n-        EXPECTED_SUMMARY_SUBWAY = (\n-            \"Liana Barrientos has been married 10 times, sometimes within two weeks of each other. Prosecutors say the\"\n-            \" marriages were part of an immigration scam. On Friday, she pleaded not guilty at State Supreme Court in\"\n-            \" the Bronx. She was arrested and charged with theft of service and criminal trespass for allegedly\"\n-            \" sneaking into the subway.\"\n-        )\n-\n-        dct = tok(\n-            [FRANCE_ARTICLE, SHORTER_ARTICLE, IRAN_ARTICLE, ARTICLE_SUBWAY],\n-            max_length=1024,\n-            truncation_strategy=\"only_first\",\n-            padding=\"longest\",\n-            truncation=True,\n-            return_tensors=\"tf\",\n-        )\n-        self.assertEqual(1024, dct[\"input_ids\"].shape[1])\n-        hypotheses_batch = hf.generate(\n-            input_ids=dct[\"input_ids\"],\n-            attention_mask=dct[\"attention_mask\"],\n-        )\n-\n-        assert hypotheses_batch[:, 1].numpy().tolist() == [0, 0, 0, 0]  # test force_bos_token_to_be_generated\n-        decoded = tok.batch_decode(hypotheses_batch, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n-        expected_batch = [\n-            EXPECTED_SUMMARY_FRANCE,\n-            EXPECTED_SUMMARY_SHORTER,\n-            EXPECTED_SUMMARY_IRAN,\n-            EXPECTED_SUMMARY_SUBWAY,\n-        ]\n-        assert decoded == expected_batch\n-\n-    @cached_property\n-    def tok(self):\n-        return BartTokenizer.from_pretrained(\"facebook/bart-large\")\n-\n-    @slow\n-    def test_contrastive_search_bart(self):\n-        article = (\n-            \" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York. A\"\n-            \" year later, she got married again in Westchester County, but to a different man and without divorcing\"\n-            \" her first husband.  Only 18 days after that marriage, she got hitched yet again. Then, Barrientos\"\n-            ' declared \"I do\" five more times, sometimes only within two weeks of each other. In 2010, she married'\n-            \" once more, this time in the Bronx. In an application for a marriage license, she stated it was her\"\n-            ' \"first and only\" marriage. Barrientos, now 39, is facing two criminal counts of \"offering a false'\n-            ' instrument for filing in the first degree,\" referring to her false statements on the 2010 marriage'\n-            \" license application, according to court documents. Prosecutors said the marriages were part of an\"\n-            \" immigration scam. On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to\"\n-            \" her attorney, Christopher Wright, who declined to comment further. After leaving court, Barrientos was\"\n-            \" arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New\"\n-            \" York subway through an emergency exit, said Detective Annette Markowski, a police spokeswoman. In total,\"\n-            \" Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.  All\"\n-            \" occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be\"\n-            \" married to four men, and at one time, she was married to eight men at once, prosecutors say. Prosecutors\"\n-            \" said the immigration scam involved some of her husbands, who filed for permanent residence status\"\n-            \" shortly after the marriages.  Any divorces happened only after such filings were approved. It was\"\n-            \" unclear whether any of the men will be prosecuted. The case was referred to the Bronx District\"\n-            \" Attorney's Office by Immigration and Customs Enforcement and the Department of Homeland Security's\"\n-            ' Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt,'\n-            \" Turkey, Georgia, Pakistan and Mali. Her eighth husband, Rashid Rajput, was deported in 2006 to his\"\n-            \" native Pakistan after an investigation by the Joint Terrorism Task Force. If convicted, Barrientos faces\"\n-            \" up to four years in prison.  Her next court appearance is scheduled for May 18.\"\n-        )\n-        bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n-        bart_model = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n-        input_ids = bart_tokenizer(\n-            article, add_special_tokens=False, truncation=True, max_length=512, return_tensors=\"tf\"\n-        ).input_ids\n-\n-        outputs = bart_model.generate(input_ids, penalty_alpha=0.5, top_k=5, max_length=64)\n-        generated_text = bart_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-\n-        self.assertListEqual(\n-            generated_text,\n-            [\n-                \"Liana Barrientos, 39, pleaded not guilty to charges related to false marriage statements. \"\n-                \"Prosecutors say she married at least 10 times, sometimes within two weeks of each other. She is \"\n-                \"accused of being part of an immigration scam to get permanent residency. If convicted, she faces up \"\n-                \"to four years in\"\n-            ],\n-        )\n-\n-    @slow\n-    def test_contrastive_search_bart_xla(self):\n-        article = (\n-            \" New York (CNN)When Liana Barrientos was 23 years old, she got married in Westchester County, New York. A\"\n-            \" year later, she got married again in Westchester County, but to a different man and without divorcing\"\n-            \" her first husband.  Only 18 days after that marriage, she got hitched yet again. Then, Barrientos\"\n-            ' declared \"I do\" five more times, sometimes only within two weeks of each other. In 2010, she married'\n-            \" once more, this time in the Bronx. In an application for a marriage license, she stated it was her\"\n-            ' \"first and only\" marriage. Barrientos, now 39, is facing two criminal counts of \"offering a false'\n-            ' instrument for filing in the first degree,\" referring to her false statements on the 2010 marriage'\n-            \" license application, according to court documents. Prosecutors said the marriages were part of an\"\n-            \" immigration scam. On Friday, she pleaded not guilty at State Supreme Court in the Bronx, according to\"\n-            \" her attorney, Christopher Wright, who declined to comment further. After leaving court, Barrientos was\"\n-            \" arrested and charged with theft of service and criminal trespass for allegedly sneaking into the New\"\n-            \" York subway through an emergency exit, said Detective Annette Markowski, a police spokeswoman. In total,\"\n-            \" Barrientos has been married 10 times, with nine of her marriages occurring between 1999 and 2002.  All\"\n-            \" occurred either in Westchester County, Long Island, New Jersey or the Bronx. She is believed to still be\"\n-            \" married to four men, and at one time, she was married to eight men at once, prosecutors say. Prosecutors\"\n-            \" said the immigration scam involved some of her husbands, who filed for permanent residence status\"\n-            \" shortly after the marriages.  Any divorces happened only after such filings were approved. It was\"\n-            \" unclear whether any of the men will be prosecuted. The case was referred to the Bronx District\"\n-            \" Attorney's Office by Immigration and Customs Enforcement and the Department of Homeland Security's\"\n-            ' Investigation Division. Seven of the men are from so-called \"red-flagged\" countries, including Egypt,'\n-            \" Turkey, Georgia, Pakistan and Mali. Her eighth husband, Rashid Rajput, was deported in 2006 to his\"\n-            \" native Pakistan after an investigation by the Joint Terrorism Task Force. If convicted, Barrientos faces\"\n-            \" up to four years in prison.  Her next court appearance is scheduled for May 18.\"\n-        )\n-        bart_tokenizer = BartTokenizer.from_pretrained(\"facebook/bart-large-cnn\")\n-        bart_model = TFBartForConditionalGeneration.from_pretrained(\"facebook/bart-large-cnn\")\n-        input_ids = bart_tokenizer(\n-            article, add_special_tokens=False, truncation=True, max_length=512, return_tensors=\"tf\"\n-        ).input_ids\n-\n-        xla_generate = tf.function(bart_model.generate, jit_compile=True)\n-        # no_repeat_ngram_size set to 0 because it isn't compatible with XLA, but doesn't change the original output\n-        outputs = xla_generate(input_ids, penalty_alpha=0.5, top_k=5, max_length=64, no_repeat_ngram_size=0)\n-        generated_text = bart_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-\n-        self.assertListEqual(\n-            generated_text,\n-            [\n-                \"Liana Barrientos, 39, pleaded not guilty to charges related to false marriage statements. \"\n-                \"Prosecutors say she married at least 10 times, sometimes within two weeks of each other. She is \"\n-                \"accused of being part of an immigration scam to get permanent residency. If convicted, she faces up \"\n-                \"to four years in\"\n-            ],\n-        )\n-\n-\n-@slow\n-@require_tf\n-class FasterTFBartModelIntegrationTests(unittest.TestCase):\n-    \"\"\"These tests are useful for debugging since they operate on a model with 1 encoder layer and 1 decoder layer.\"\"\"\n-\n-    @cached_property\n-    def tok(self):\n-        return BartTokenizer.from_pretrained(\"facebook/bart-large\")\n-\n-    @cached_property\n-    def xsum_1_1_model(self):\n-        return TFBartForConditionalGeneration.from_pretrained(\"sshleifer/distilbart-xsum-1-1\")\n-\n-    def test_xsum_1_1_generation(self):\n-        model = self.xsum_1_1_model\n-        assert model.model.decoder.embed_tokens == model.model.shared\n-        ARTICLE = (\n-            \"The Palestinian Authority officially became the 123rd member of the International Criminal Court on\"\n-            \" Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The\"\n-            \" formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based.\"\n-            \" The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its\"\n-            ' jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East'\n-            ' Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the'\n-            \" situation in Palestinian territories, paving the way for possible war crimes investigations against\"\n-            \" Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and\"\n-            \" the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the\"\n-            \" body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a\"\n-            ' move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the'\n-            ' world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an'\n-            ' ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge'\n-            \" Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the\"\n-            ' Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine'\n-            \" acquires all the rights as well as responsibilities that come with being a State Party to the Statute.\"\n-            ' These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights'\n-            ' Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should'\n-            \" immediately end their pressure, and countries that support universal acceptance of the court's treaty\"\n-            ' should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the'\n-            \" group. \\\"What's objectionable is the attempts to undermine international justice, not Palestine's\"\n-            ' decision to join a treaty to which over 100 countries around the world are members.\" In January, when'\n-            \" the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an\"\n-            ' outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\"'\n-            \" disagreed with the court's decision. \\\"As we have said repeatedly, we do not believe that Palestine is a\"\n-            ' state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in'\n-            ' a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We'\n-            ' will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\"'\n-            \" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the\"\n-            ' territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the'\n-            \" court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou\"\n-            ' Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war'\n-            \" between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry\"\n-            \" will include alleged war crimes committed since June. The International Criminal Court was set up in\"\n-            \" 2002 to prosecute genocide, crimes against humanity and war crimes.\"\n-        )\n-        EXPECTED = (\n-            \" The International Criminal Court (ICC) has announced that it has been announced by the International\"\n-            \" Criminal court.\"\n-        )\n-        dct = self.tok(ARTICLE, return_tensors=\"tf\")\n-        generated_ids = model.generate(**dct, num_beams=4)\n-        result = self.tok.batch_decode(generated_ids, skip_special_tokens=True)[0]\n-        assert result == EXPECTED\n-\n-    def test_xsum_1_1_xla_generation(self):\n-        # same test as above, but with `no_repeat_ngram_size=0` (not compatible with XLA) and XLA comparison enabled\n-        model = self.xsum_1_1_model\n-        assert model.model.decoder.embed_tokens == model.model.shared\n-        ARTICLE = (\n-            \"The Palestinian Authority officially became the 123rd member of the International Criminal Court on\"\n-            \" Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories. The\"\n-            \" formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is based.\"\n-            \" The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted its\"\n-            ' jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including East'\n-            ' Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination into the'\n-            \" situation in Palestinian territories, paving the way for possible war crimes investigations against\"\n-            \" Israelis. As members of the court, Palestinians may be subject to counter-charges as well. Israel and\"\n-            \" the United States, neither of which is an ICC member, opposed the Palestinians' efforts to join the\"\n-            \" body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony, said it was a\"\n-            ' move toward greater justice. \"As Palestine formally becomes a State Party to the Rome Statute today, the'\n-            ' world is also a step closer to ending a long era of impunity and injustice,\" he said, according to an'\n-            ' ICC news release. \"Indeed, today brings us closer to our shared goals of justice and peace.\" Judge'\n-            \" Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was just the first step for the\"\n-            ' Palestinians. \"As the Rome Statute today enters into force for the State of Palestine, Palestine'\n-            \" acquires all the rights as well as responsibilities that come with being a State Party to the Statute.\"\n-            ' These are substantive commitments, which cannot be taken lightly,\" she said. Rights group Human Rights'\n-            ' Watch welcomed the development. \"Governments seeking to penalize Palestine for joining the ICC should'\n-            \" immediately end their pressure, and countries that support universal acceptance of the court's treaty\"\n-            ' should speak out to welcome its membership,\" said Balkees Jarrah, international justice counsel for the'\n-            \" group. \\\"What's objectionable is the attempts to undermine international justice, not Palestine's\"\n-            ' decision to join a treaty to which over 100 countries around the world are members.\" In January, when'\n-            \" the preliminary ICC examination was opened, Israeli Prime Minister Benjamin Netanyahu described it as an\"\n-            ' outrage, saying the court was overstepping its boundaries. The United States also said it \"strongly\"'\n-            \" disagreed with the court's decision. \\\"As we have said repeatedly, we do not believe that Palestine is a\"\n-            ' state and therefore we do not believe that it is eligible to join the ICC,\" the State Department said in'\n-            ' a statement. It urged the warring sides to resolve their differences through direct negotiations. \"We'\n-            ' will continue to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\"'\n-            \" it said. But the ICC begs to differ with the definition of a state for its purposes and refers to the\"\n-            ' territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows the'\n-            \" court to review evidence and determine whether to investigate suspects on both sides. Prosecutor Fatou\"\n-            ' Bensouda said her office would \"conduct its analysis in full independence and impartiality.\" The war'\n-            \" between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The inquiry\"\n-            \" will include alleged war crimes committed since June. The International Criminal Court was set up in\"\n-            \" 2002 to prosecute genocide, crimes against humanity and war crimes.\"\n-        )\n-        EXPECTED = (\n-            \" The International Criminal Court (ICC) has announced that it is to be investigated by the International\"\n-            \" Criminal Court (ICC) over allegations of war crimes.\"\n-        )\n-\n-        dct = self.tok(ARTICLE, return_tensors=\"tf\")\n-        generated_ids = model.generate(**dct, num_beams=4, no_repeat_ngram_size=0)\n-        result = self.tok.batch_decode(generated_ids, skip_special_tokens=True)[0]\n-        assert result == EXPECTED\n-\n-        xla_generate = tf.function(model.generate, jit_compile=True)\n-        generated_ids = xla_generate(**dct, num_beams=4, no_repeat_ngram_size=0)\n-        result = self.tok.batch_decode(generated_ids, skip_special_tokens=True)[0]\n-        assert result == EXPECTED\n-\n-    def test_xsum_1_1_batch_generation(self):\n-        batch = self.tok(\n-            [\n-                \"The Palestinian Authority officially became the 123rd member of the International Criminal Court on\"\n-                \" Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories.\"\n-                \" The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is\"\n-                \" based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted\"\n-                ' its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including'\n-                ' East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination'\n-                \" into the situation in Palestinian territories, paving the way for possible war crimes investigations\"\n-                \" against Israelis. As members of the court, Palestinians may be subject to counter-charges as well.\"\n-                \" Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts\"\n-                \" to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony,\"\n-                ' said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome'\n-                ' Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he'\n-                ' said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of'\n-                ' justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was'\n-                ' just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State'\n-                \" of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a\"\n-                ' State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she'\n-                ' said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize'\n-                \" Palestine for joining the ICC should immediately end their pressure, and countries that support\"\n-                \" universal acceptance of the court's treaty should speak out to welcome its membership,\\\" said\"\n-                \" Balkees Jarrah, international justice counsel for the group. \\\"What's objectionable is the attempts\"\n-                \" to undermine international justice, not Palestine's decision to join a treaty to which over 100\"\n-                ' countries around the world are members.\" In January, when the preliminary ICC examination was'\n-                \" opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was\"\n-                ' overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court\\'s'\n-                ' decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we'\n-                ' do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It'\n-                ' urged the warring sides to resolve their differences through direct negotiations. \"We will continue'\n-                ' to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said.'\n-                \" But the ICC begs to differ with the definition of a state for its purposes and refers to the\"\n-                ' territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows'\n-                \" the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor\"\n-                ' Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\"'\n-                \" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The\"\n-                \" inquiry will include alleged war crimes committed since June. The International Criminal Court was\"\n-                \" set up in 2002 to prosecute genocide, crimes against humanity and war crimes.\",\n-                \"The French prosecutor leading an investigation into the crash of Germanwings Flight 9525 insisted\"\n-                \" Wednesday that he was not aware of any video footage from on board the plane. Marseille prosecutor\"\n-                ' Brice Robin told CNN that \"so far no videos were used in the crash investigation.\" He added, \"A'\n-                \" person who has such a video needs to immediately give it to the investigators.\\\" Robin's comments\"\n-                \" follow claims by two magazines, German daily Bild and French Paris Match, of a cell phone video\"\n-                \" showing the harrowing final seconds from on board Germanwings Flight 9525 as it crashed into the\"\n-                \" French Alps. All 150 on board were killed. Paris Match and Bild reported that the video was\"\n-                \" recovered from a phone at the wreckage site. The two publications described the supposed video, but\"\n-                \" did not post it on their websites. The publications said that they watched the video, which was\"\n-                \" found by a source close to the investigation. \\\"One can hear cries of 'My God' in several\"\n-                ' languages,\" Paris Match reported. \"Metallic banging can also be heard more than three times, perhaps'\n-                \" of the pilot trying to open the cockpit door with a heavy object.  Towards the end, after a heavy\"\n-                ' shake, stronger than the others, the screaming intensifies. Then nothing.\" \"It is a very disturbing'\n-                \" scene,\\\" said Julian Reichelt, editor-in-chief of Bild online. An official with France's accident\"\n-                \" investigation agency, the BEA, said the agency is not aware of any such video. Lt. Col. Jean-Marc\"\n-                \" Menichini, a French Gendarmerie spokesman in charge of communications on rescue efforts around the\"\n-                ' Germanwings crash site, told CNN that the reports were \"completely wrong\" and \"unwarranted.\" Cell'\n-                ' phones have been collected at the site, he said, but that they \"hadn\\'t been exploited yet.\"'\n-                \" Menichini said he believed the cell phones would need to be sent to the Criminal Research Institute\"\n-                \" in Rosny sous-Bois, near Paris, in order to be analyzed by specialized technicians working\"\n-                \" hand-in-hand with investigators. But none of the cell phones found so far have been sent to the\"\n-                \" institute, Menichini said. Asked whether staff involved in the search could have leaked a memory\"\n-                ' card to the media, Menichini answered with a categorical \"no.\" Reichelt told \"Erin Burnett:'\n-                ' Outfront\" that he had watched the video and stood by the report, saying Bild and Paris Match are'\n-                ' \"very confident\" that the clip is real. He noted that investigators only revealed they\\'d recovered'\n-                ' cell phones from the crash site after Bild and Paris Match published their reports. \"That is'\n-                \" something we did not know before. ... Overall we can say many things of the investigation weren't\"\n-                ' revealed by the investigation at the beginning,\" he said. What was mental state of Germanwings'\n-                \" co-pilot? German airline Lufthansa confirmed Tuesday that co-pilot Andreas Lubitz had battled\"\n-                \" depression years before he took the controls of Germanwings Flight 9525, which he's accused of\"\n-                \" deliberately crashing last week in the French Alps. Lubitz told his Lufthansa flight training school\"\n-                ' in 2009 that he had a \"previous episode of severe depression,\" the airline said Tuesday. Email'\n-                \" correspondence between Lubitz and the school discovered in an internal investigation, Lufthansa\"\n-                \" said, included medical documents he submitted in connection with resuming his flight training. The\"\n-                \" announcement indicates that Lufthansa, the parent company of Germanwings, knew of Lubitz's battle\"\n-                \" with depression, allowed him to continue training and ultimately put him in the cockpit. Lufthansa,\"\n-                \" whose CEO Carsten Spohr previously said Lubitz was 100% fit to fly, described its statement Tuesday\"\n-                ' as a \"swift and seamless clarification\" and said it was sharing the information and documents --'\n-                \" including training and medical records -- with public prosecutors. Spohr traveled to the crash site\"\n-                \" Wednesday, where recovery teams have been working for the past week to recover human remains and\"\n-                \" plane debris scattered across a steep mountainside. He saw the crisis center set up in\"\n-                \" Seyne-les-Alpes, laid a wreath in the village of Le Vernet, closer to the crash site, where grieving\"\n-                \" families have left flowers at a simple stone memorial. Menichini told CNN late Tuesday that no\"\n-                \" visible human remains were left at the site but recovery teams would keep searching. French\"\n-                \" President Francois Hollande, speaking Tuesday, said that it should be possible to identify all the\"\n-                \" victims using DNA analysis by the end of the week, sooner than authorities had previously suggested.\"\n-                \" In the meantime, the recovery of the victims' personal belongings will start Wednesday, Menichini\"\n-                \" said. Among those personal belongings could be more cell phones belonging to the 144 passengers and\"\n-                \" six crew on board. Check out the latest from our correspondents . The details about Lubitz's\"\n-                \" correspondence with the flight school during his training were among several developments as\"\n-                \" investigators continued to delve into what caused the crash and Lubitz's possible motive for\"\n-                \" downing the jet. A Lufthansa spokesperson told CNN on Tuesday that Lubitz had a valid medical\"\n-                ' certificate, had passed all his examinations and \"held all the licenses required.\" Earlier, a'\n-                \" spokesman for the prosecutor's office in Dusseldorf, Christoph Kumpa, said medical records reveal\"\n-                \" Lubitz suffered from suicidal tendencies at some point before his aviation career and underwent\"\n-                \" psychotherapy before he got his pilot's license. Kumpa emphasized there's no evidence suggesting\"\n-                \" Lubitz was suicidal or acting aggressively before the crash. Investigators are looking into whether\"\n-                \" Lubitz feared his medical condition would cause him to lose his pilot's license, a European\"\n-                ' government official briefed on the investigation told CNN on Tuesday. While flying was \"a big part'\n-                \" of his life,\\\" the source said, it's only one theory being considered. Another source, a law\"\n-                \" enforcement official briefed on the investigation, also told CNN that authorities believe the\"\n-                \" primary motive for Lubitz to bring down the plane was that he feared he would not be allowed to fly\"\n-                \" because of his medical problems. Lubitz's girlfriend told investigators he had seen an eye doctor\"\n-                \" and a neuropsychologist, both of whom deemed him unfit to work recently and concluded he had\"\n-                \" psychological issues, the European government official said. But no matter what details emerge about\"\n-                \" his previous mental health struggles, there's more to the story, said Brian Russell, a forensic\"\n-                ' psychologist. \"Psychology can explain why somebody would turn rage inward on themselves about the'\n-                \" fact that maybe they weren't going to keep doing their job and they're upset about that and so\"\n-                ' they\\'re suicidal,\" he said. \"But there is no mental illness that explains why somebody then feels'\n-                \" entitled to also take that rage and turn it outward on 149 other people who had nothing to do with\"\n-                \" the person's problems.\\\" Germanwings crash compensation: What we know . Who was the captain of\"\n-                \" Germanwings Flight 9525? CNN's Margot Haddad reported from Marseille and Pamela Brown from\"\n-                \" Dusseldorf, while Laura Smith-Spark wrote from London. CNN's Frederik Pleitgen, Pamela Boykoff,\"\n-                \" Antonia Mortensen, Sandrine Amiel and Anna-Maja Rappard contributed to this report.\",\n-            ],\n-            return_tensors=\"tf\",\n-            padding=\"longest\",\n-            truncation=True,\n-        )\n-        generated_ids = self.xsum_1_1_model.generate(**batch, num_beams=4)\n-        result = self.tok.batch_decode(generated_ids, skip_special_tokens=True)\n-        assert (\n-            result[0]\n-            == \" The International Criminal Court (ICC) has announced that it has been announced by the International\"\n-            \" Criminal court.\"\n-        )\n-        assert (\n-            result[1]\n-            == \" An investigation into the crash that killed at least 10 people in the French capital has been\"\n-            \" released by the French police investigating the crash.\"\n-        )\n-\n-    def test_encoder_equiv(self):\n-        batch = self.tok(\n-            [\n-                \"The Palestinian Authority officially became the 123rd member of the International Criminal Court on\"\n-                \" Wednesday, a step that gives the court jurisdiction over alleged crimes in Palestinian territories.\"\n-                \" The formal accession was marked with a ceremony at The Hague, in the Netherlands, where the court is\"\n-                \" based. The Palestinians signed the ICC's founding Rome Statute in January, when they also accepted\"\n-                ' its jurisdiction over alleged crimes committed \"in the occupied Palestinian territory, including'\n-                ' East Jerusalem, since June 13, 2014.\" Later that month, the ICC opened a preliminary examination'\n-                \" into the situation in Palestinian territories, paving the way for possible war crimes investigations\"\n-                \" against Israelis. As members of the court, Palestinians may be subject to counter-charges as well.\"\n-                \" Israel and the United States, neither of which is an ICC member, opposed the Palestinians' efforts\"\n-                \" to join the body. But Palestinian Foreign Minister Riad al-Malki, speaking at Wednesday's ceremony,\"\n-                ' said it was a move toward greater justice. \"As Palestine formally becomes a State Party to the Rome'\n-                ' Statute today, the world is also a step closer to ending a long era of impunity and injustice,\" he'\n-                ' said, according to an ICC news release. \"Indeed, today brings us closer to our shared goals of'\n-                ' justice and peace.\" Judge Kuniko Ozaki, a vice president of the ICC, said acceding to the treaty was'\n-                ' just the first step for the Palestinians. \"As the Rome Statute today enters into force for the State'\n-                \" of Palestine, Palestine acquires all the rights as well as responsibilities that come with being a\"\n-                ' State Party to the Statute. These are substantive commitments, which cannot be taken lightly,\" she'\n-                ' said. Rights group Human Rights Watch welcomed the development. \"Governments seeking to penalize'\n-                \" Palestine for joining the ICC should immediately end their pressure, and countries that support\"\n-                \" universal acceptance of the court's treaty should speak out to welcome its membership,\\\" said\"\n-                \" Balkees Jarrah, international justice counsel for the group. \\\"What's objectionable is the attempts\"\n-                \" to undermine international justice, not Palestine's decision to join a treaty to which over 100\"\n-                ' countries around the world are members.\" In January, when the preliminary ICC examination was'\n-                \" opened, Israeli Prime Minister Benjamin Netanyahu described it as an outrage, saying the court was\"\n-                ' overstepping its boundaries. The United States also said it \"strongly\" disagreed with the court\\'s'\n-                ' decision. \"As we have said repeatedly, we do not believe that Palestine is a state and therefore we'\n-                ' do not believe that it is eligible to join the ICC,\" the State Department said in a statement. It'\n-                ' urged the warring sides to resolve their differences through direct negotiations. \"We will continue'\n-                ' to oppose actions against Israel at the ICC as counterproductive to the cause of peace,\" it said.'\n-                \" But the ICC begs to differ with the definition of a state for its purposes and refers to the\"\n-                ' territories as \"Palestine.\" While a preliminary examination is not a formal investigation, it allows'\n-                \" the court to review evidence and determine whether to investigate suspects on both sides. Prosecutor\"\n-                ' Fatou Bensouda said her office would \"conduct its analysis in full independence and impartiality.\"'\n-                \" The war between Israel and Hamas militants in Gaza last summer left more than 2,000 people dead. The\"\n-                \" inquiry will include alleged war crimes committed since June. The International Criminal Court was\"\n-                \" set up in 2002 to prosecute genocide, crimes against humanity and war crimes.\",\n-                \"The French prosecutor leading an investigation into the crash of Germanwings Flight 9525 insisted\"\n-                \" Wednesday that he was not aware of any video footage from on board the plane. Marseille prosecutor\"\n-                ' Brice Robin told CNN that \"so far no videos were used in the crash investigation.\" He added, \"A'\n-                \" person who has such a video needs to immediately give it to the investigators.\\\" Robin's comments\"\n-                \" follow claims by two magazines, German daily Bild and French Paris Match, of a cell phone video\"\n-                \" showing the harrowing final seconds from on board Germanwings Flight 9525 as it crashed into the\"\n-                \" French Alps. All 150 on board were killed. Paris Match and Bild reported that the video was\"\n-                \" recovered from a phone at the wreckage site. The two publications described the supposed video, but\"\n-                \" did not post it on their websites. The publications said that they watched the video, which was\"\n-                \" found by a source close to the investigation. \\\"One can hear cries of 'My God' in several\"\n-                ' languages,\" Paris Match reported. \"Metallic banging can also be heard more than three times, perhaps'\n-                \" of the pilot trying to open the cockpit door with a heavy object.  Towards the end, after a heavy\"\n-                ' shake, stronger than the others, the screaming intensifies. Then nothing.\" \"It is a very disturbing'\n-                \" scene,\\\" said Julian Reichelt, editor-in-chief of Bild online. An official with France's accident\"\n-                \" investigation agency, the BEA, said the agency is not aware of any such video. Lt. Col. Jean-Marc\"\n-                \" Menichini, a French Gendarmerie spokesman in charge of communications on rescue efforts around the\"\n-                ' Germanwings crash site, told CNN that the reports were \"completely wrong\" and \"unwarranted.\" Cell'\n-                ' phones have been collected at the site, he said, but that they \"hadn\\'t been exploited yet.\"'\n-                \" Menichini said he believed the cell phones would need to be sent to the Criminal Research Institute\"\n-                \" in Rosny sous-Bois, near Paris, in order to be analyzed by specialized technicians working\"\n-                \" hand-in-hand with investigators. But none of the cell phones found so far have been sent to the\"\n-                \" institute, Menichini said. Asked whether staff involved in the search could have leaked a memory\"\n-                ' card to the media, Menichini answered with a categorical \"no.\" Reichelt told \"Erin Burnett:'\n-                ' Outfront\" that he had watched the video and stood by the report, saying Bild and Paris Match are'\n-                ' \"very confident\" that the clip is real. He noted that investigators only revealed they\\'d recovered'\n-                ' cell phones from the crash site after Bild and Paris Match published their reports. \"That is'\n-                \" something we did not know before. ... Overall we can say many things of the investigation weren't\"\n-                ' revealed by the investigation at the beginning,\" he said. What was mental state of Germanwings'\n-                \" co-pilot? German airline Lufthansa confirmed Tuesday that co-pilot Andreas Lubitz had battled\"\n-                \" depression years before he took the controls of Germanwings Flight 9525, which he's accused of\"\n-                \" deliberately crashing last week in the French Alps. Lubitz told his Lufthansa flight training school\"\n-                ' in 2009 that he had a \"previous episode of severe depression,\" the airline said Tuesday. Email'\n-                \" correspondence between Lubitz and the school discovered in an internal investigation, Lufthansa\"\n-                \" said, included medical documents he submitted in connection with resuming his flight training. The\"\n-                \" announcement indicates that Lufthansa, the parent company of Germanwings, knew of Lubitz's battle\"\n-                \" with depression, allowed him to continue training and ultimately put him in the cockpit. Lufthansa,\"\n-                \" whose CEO Carsten Spohr previously said Lubitz was 100% fit to fly, described its statement Tuesday\"\n-                ' as a \"swift and seamless clarification\" and said it was sharing the information and documents --'\n-                \" including training and medical records -- with public prosecutors. Spohr traveled to the crash site\"\n-                \" Wednesday, where recovery teams have been working for the past week to recover human remains and\"\n-                \" plane debris scattered across a steep mountainside. He saw the crisis center set up in\"\n-                \" Seyne-les-Alpes, laid a wreath in the village of Le Vernet, closer to the crash site, where grieving\"\n-                \" families have left flowers at a simple stone memorial. Menichini told CNN late Tuesday that no\"\n-                \" visible human remains were left at the site but recovery teams would keep searching. French\"\n-                \" President Francois Hollande, speaking Tuesday, said that it should be possible to identify all the\"\n-                \" victims using DNA analysis by the end of the week, sooner than authorities had previously suggested.\"\n-                \" In the meantime, the recovery of the victims' personal belongings will start Wednesday, Menichini\"\n-                \" said. Among those personal belongings could be more cell phones belonging to the 144 passengers and\"\n-                \" six crew on board. Check out the latest from our correspondents . The details about Lubitz's\"\n-                \" correspondence with the flight school during his training were among several developments as\"\n-                \" investigators continued to delve into what caused the crash and Lubitz's possible motive for\"\n-                \" downing the jet. A Lufthansa spokesperson told CNN on Tuesday that Lubitz had a valid medical\"\n-                ' certificate, had passed all his examinations and \"held all the licenses required.\" Earlier, a'\n-                \" spokesman for the prosecutor's office in Dusseldorf, Christoph Kumpa, said medical records reveal\"\n-                \" Lubitz suffered from suicidal tendencies at some point before his aviation career and underwent\"\n-                \" psychotherapy before he got his pilot's license. Kumpa emphasized there's no evidence suggesting\"\n-                \" Lubitz was suicidal or acting aggressively before the crash. Investigators are looking into whether\"\n-                \" Lubitz feared his medical condition would cause him to lose his pilot's license, a European\"\n-                ' government official briefed on the investigation told CNN on Tuesday. While flying was \"a big part'\n-                \" of his life,\\\" the source said, it's only one theory being considered. Another source, a law\"\n-                \" enforcement official briefed on the investigation, also told CNN that authorities believe the\"\n-                \" primary motive for Lubitz to bring down the plane was that he feared he would not be allowed to fly\"\n-                \" because of his medical problems. Lubitz's girlfriend told investigators he had seen an eye doctor\"\n-                \" and a neuropsychologist, both of whom deemed him unfit to work recently and concluded he had\"\n-                \" psychological issues, the European government official said. But no matter what details emerge about\"\n-                \" his previous mental health struggles, there's more to the story, said Brian Russell, a forensic\"\n-                ' psychologist. \"Psychology can explain why somebody would turn rage inward on themselves about the'\n-                \" fact that maybe they weren't going to keep doing their job and they're upset about that and so\"\n-                ' they\\'re suicidal,\" he said. \"But there is no mental illness that explains why somebody then feels'\n-                \" entitled to also take that rage and turn it outward on 149 other people who had nothing to do with\"\n-                \" the person's problems.\\\" Germanwings crash compensation: What we know . Who was the captain of\"\n-                \" Germanwings Flight 9525? CNN's Margot Haddad reported from Marseille and Pamela Brown from\"\n-                \" Dusseldorf, while Laura Smith-Spark wrote from London. CNN's Frederik Pleitgen, Pamela Boykoff,\"\n-                \" Antonia Mortensen, Sandrine Amiel and Anna-Maja Rappard contributed to this report.\",\n-            ],\n-            return_tensors=\"tf\",\n-            padding=\"longest\",\n-            truncation=True,\n-        )\n-        features = self.xsum_1_1_model.get_encoder()(**batch).last_hidden_state\n-\n-        expected = np.array([[-0.0828, -0.0251, -0.0674], [0.1277, 0.3311, -0.0255], [0.2613, -0.0840, -0.2763]])\n-        assert np.allclose(features[0, :3, :3].numpy(), expected, atol=1e-3)"
        },
        {
            "sha": "2ac3668d3b094ad690657317bfe7380887d8d554",
            "filename": "tests/models/beit/test_modeling_flax_beit.py",
            "status": "removed",
            "additions": 0,
            "deletions": 294,
            "changes": 294,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbeit%2Ftest_modeling_flax_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbeit%2Ftest_modeling_flax_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_flax_beit.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,294 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import inspect\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import BeitConfig\n-from transformers.testing_utils import require_flax, require_vision, slow\n-from transformers.utils import cached_property, is_flax_available, is_vision_available\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, floats_tensor, ids_tensor\n-\n-\n-if is_flax_available():\n-    import jax\n-\n-    from transformers import FlaxBeitForImageClassification, FlaxBeitForMaskedImageModeling, FlaxBeitModel\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-    from transformers import BeitImageProcessor\n-\n-\n-class FlaxBeitModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        vocab_size=100,\n-        batch_size=13,\n-        image_size=30,\n-        patch_size=2,\n-        num_channels=3,\n-        is_training=True,\n-        use_labels=True,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        type_sequence_label_size=10,\n-        initializer_range=0.02,\n-        num_labels=3,\n-    ):\n-        self.parent = parent\n-        self.vocab_size = vocab_size\n-        self.batch_size = batch_size\n-        self.image_size = image_size\n-        self.patch_size = patch_size\n-        self.num_channels = num_channels\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-\n-        # in BeiT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n-        num_patches = (image_size // patch_size) ** 2\n-        self.seq_length = num_patches + 1\n-\n-    def prepare_config_and_inputs(self):\n-        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-\n-        labels = None\n-        if self.use_labels:\n-            labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-\n-        config = BeitConfig(\n-            vocab_size=self.vocab_size,\n-            image_size=self.image_size,\n-            patch_size=self.patch_size,\n-            num_channels=self.num_channels,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, pixel_values, labels\n-\n-    def create_and_check_model(self, config, pixel_values, labels):\n-        model = FlaxBeitModel(config=config)\n-        result = model(pixel_values)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_masked_lm(self, config, pixel_values, labels):\n-        model = FlaxBeitForMaskedImageModeling(config=config)\n-        result = model(pixel_values)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length - 1, self.vocab_size))\n-\n-    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n-        config.num_labels = self.type_sequence_label_size\n-        model = FlaxBeitForImageClassification(config=config)\n-        result = model(pixel_values)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n-\n-        # test greyscale images\n-        config.num_channels = 1\n-        model = FlaxBeitForImageClassification(config)\n-\n-        pixel_values = floats_tensor([self.batch_size, 1, self.image_size, self.image_size])\n-        result = model(pixel_values)\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            pixel_values,\n-            labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n-        return config, inputs_dict\n-\n-\n-@require_flax\n-class FlaxBeitModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (FlaxBeitModel, FlaxBeitForImageClassification, FlaxBeitForMaskedImageModeling) if is_flax_available() else ()\n-    )\n-\n-    def setUp(self) -> None:\n-        self.model_tester = FlaxBeitModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=BeitConfig, has_text_modality=False, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    # We need to override this test because Beit's forward signature is different than text models.\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.__call__)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"pixel_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    # We need to override this test because Beit expects pixel_values instead of input_ids\n-    def test_jit_compilation(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                model = model_class(config)\n-\n-                @jax.jit\n-                def model_jitted(pixel_values, **kwargs):\n-                    return model(pixel_values=pixel_values, **kwargs)\n-\n-                with self.subTest(\"JIT Enabled\"):\n-                    jitted_outputs = model_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                with self.subTest(\"JIT Disabled\"):\n-                    with jax.disable_jit():\n-                        outputs = model_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                self.assertEqual(len(outputs), len(jitted_outputs))\n-                for jitted_output, output in zip(jitted_outputs, outputs):\n-                    self.assertEqual(jitted_output.shape, output.shape)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_for_masked_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n-\n-    def test_for_image_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"microsoft/beit-base-patch16-224\")\n-            outputs = model(np.ones((1, 3, 224, 224)))\n-            self.assertIsNotNone(outputs)\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n-    return image\n-\n-\n-@require_vision\n-@require_flax\n-class FlaxBeitModelIntegrationTest(unittest.TestCase):\n-    @cached_property\n-    def default_image_processor(self):\n-        return BeitImageProcessor.from_pretrained(\"microsoft/beit-base-patch16-224\") if is_vision_available() else None\n-\n-    @slow\n-    def test_inference_masked_image_modeling_head(self):\n-        model = FlaxBeitForMaskedImageModeling.from_pretrained(\"microsoft/beit-base-patch16-224-pt22k\")\n-\n-        image_processor = self.default_image_processor\n-        image = prepare_img()\n-        pixel_values = image_processor(images=image, return_tensors=\"np\").pixel_values\n-\n-        # prepare bool_masked_pos\n-        bool_masked_pos = np.ones((1, 196), dtype=bool)\n-\n-        # forward pass\n-        outputs = model(pixel_values=pixel_values, bool_masked_pos=bool_masked_pos)\n-        logits = outputs.logits\n-\n-        # verify the logits\n-        expected_shape = (1, 196, 8192)\n-        self.assertEqual(logits.shape, expected_shape)\n-\n-        expected_slice = np.array(\n-            [[-3.2437, 0.5072, -13.9174], [-3.2456, 0.4948, -13.9401], [-3.2033, 0.5121, -13.8550]]\n-        )\n-\n-        self.assertTrue(np.allclose(logits[bool_masked_pos][:3, :3], expected_slice, atol=1e-2))\n-\n-    @slow\n-    def test_inference_image_classification_head_imagenet_1k(self):\n-        model = FlaxBeitForImageClassification.from_pretrained(\"microsoft/beit-base-patch16-224\")\n-\n-        image_processor = self.default_image_processor\n-        image = prepare_img()\n-        inputs = image_processor(images=image, return_tensors=\"np\")\n-\n-        # forward pass\n-        outputs = model(**inputs)\n-        logits = outputs.logits\n-\n-        # verify the logits\n-        expected_shape = (1, 1000)\n-        self.assertEqual(logits.shape, expected_shape)\n-\n-        expected_slice = np.array([-1.2385, -1.0987, -1.0108])\n-\n-        self.assertTrue(np.allclose(logits[0, :3], expected_slice, atol=1e-4))\n-\n-        expected_class_idx = 281\n-        self.assertEqual(logits.argmax(-1).item(), expected_class_idx)\n-\n-    @slow\n-    def test_inference_image_classification_head_imagenet_22k(self):\n-        model = FlaxBeitForImageClassification.from_pretrained(\"microsoft/beit-large-patch16-224-pt22k-ft22k\")\n-\n-        image_processor = self.default_image_processor\n-        image = prepare_img()\n-        inputs = image_processor(images=image, return_tensors=\"np\")\n-\n-        # forward pass\n-        outputs = model(**inputs)\n-        logits = outputs.logits\n-\n-        # verify the logits\n-        expected_shape = (1, 21841)\n-        self.assertEqual(logits.shape, expected_shape)\n-\n-        expected_slice = np.array([1.6881, -0.2787, 0.5901])\n-\n-        self.assertTrue(np.allclose(logits[0, :3], expected_slice, atol=1e-4))\n-\n-        expected_class_idx = 2396\n-        self.assertEqual(logits.argmax(-1).item(), expected_class_idx)"
        },
        {
            "sha": "72d5c951e68b6c2af1fc1bff6e8b65573eb0e0da",
            "filename": "tests/models/bert/test_modeling_flax_bert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 163,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbert%2Ftest_modeling_flax_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbert%2Ftest_modeling_flax_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_flax_bert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,163 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import BertConfig, is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-\n-\n-if is_flax_available():\n-    from transformers.models.bert.modeling_flax_bert import (\n-        FlaxBertForMaskedLM,\n-        FlaxBertForMultipleChoice,\n-        FlaxBertForNextSentencePrediction,\n-        FlaxBertForPreTraining,\n-        FlaxBertForQuestionAnswering,\n-        FlaxBertForSequenceClassification,\n-        FlaxBertForTokenClassification,\n-        FlaxBertModel,\n-    )\n-\n-\n-class FlaxBertModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_attention_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_choices=4,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_attention_mask = use_attention_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_choices = num_choices\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        attention_mask = None\n-        if self.use_attention_mask:\n-            attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        config = BertConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, input_ids, token_type_ids, attention_mask\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, token_type_ids, attention_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\n-        return config, inputs_dict\n-\n-    def prepare_config_and_inputs_for_decoder(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, token_type_ids, attention_mask = config_and_inputs\n-\n-        config.is_decoder = True\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            attention_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n-\n-@require_flax\n-class FlaxBertModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    test_head_masking = True\n-\n-    all_model_classes = (\n-        (\n-            FlaxBertModel,\n-            FlaxBertForPreTraining,\n-            FlaxBertForMaskedLM,\n-            FlaxBertForMultipleChoice,\n-            FlaxBertForQuestionAnswering,\n-            FlaxBertForNextSentencePrediction,\n-            FlaxBertForSequenceClassification,\n-            FlaxBertForTokenClassification,\n-            FlaxBertForQuestionAnswering,\n-        )\n-        if is_flax_available()\n-        else ()\n-    )\n-\n-    def setUp(self):\n-        self.model_tester = FlaxBertModelTester(self)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        # Only check this for base model, not necessary for all model classes.\n-        # This will also help speed-up tests.\n-        model = FlaxBertModel.from_pretrained(\"google-bert/bert-base-cased\")\n-        outputs = model(np.ones((1, 1)))\n-        self.assertIsNotNone(outputs)"
        },
        {
            "sha": "b9fbdc9d437f615618fb92c0ccb3925f50dff007",
            "filename": "tests/models/bert/test_modeling_tf_bert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 764,
            "changes": 764,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbert%2Ftest_modeling_tf_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbert%2Ftest_modeling_tf_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_tf_bert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,764 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import BertConfig, is_tf_available\n-from transformers.models.auto import get_values\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-from ...utils.test_modeling_tf_core import TFCoreModelTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import TF_MODEL_FOR_PRETRAINING_MAPPING\n-    from transformers.models.bert.modeling_tf_bert import (\n-        TFBertForMaskedLM,\n-        TFBertForMultipleChoice,\n-        TFBertForNextSentencePrediction,\n-        TFBertForPreTraining,\n-        TFBertForQuestionAnswering,\n-        TFBertForSequenceClassification,\n-        TFBertForTokenClassification,\n-        TFBertLMHeadModel,\n-        TFBertModel,\n-    )\n-\n-\n-class TFBertModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = 13\n-        self.seq_length = 7\n-        self.is_training = True\n-        self.use_input_mask = True\n-        self.use_token_type_ids = True\n-        self.use_labels = True\n-        self.vocab_size = 99\n-        self.hidden_size = 32\n-        self.num_hidden_layers = 2\n-        self.num_attention_heads = 4\n-        self.intermediate_size = 37\n-        self.hidden_act = \"gelu\"\n-        self.hidden_dropout_prob = 0.1\n-        self.attention_probs_dropout_prob = 0.1\n-        self.max_position_embeddings = 512\n-        self.type_vocab_size = 16\n-        self.type_sequence_label_size = 2\n-        self.initializer_range = 0.02\n-        self.num_labels = 3\n-        self.num_choices = 4\n-        self.scope = None\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = BertConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        config.is_decoder = True\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFBertModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def create_and_check_causal_lm_base_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.is_decoder = True\n-\n-        model = TFBertModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-\n-        model = TFBertModel(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-            \"encoder_hidden_states\": encoder_hidden_states,\n-            \"encoder_attention_mask\": encoder_attention_mask,\n-        }\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs, token_type_ids=token_type_ids, encoder_hidden_states=encoder_hidden_states)\n-\n-        # Also check the case where encoder outputs are not passed\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def create_and_check_causal_lm_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.is_decoder = True\n-\n-        model = TFBertLMHeadModel(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        prediction_scores = model(inputs)[\"logits\"]\n-        self.parent.assertListEqual(\n-            list(prediction_scores.numpy().shape), [self.batch_size, self.seq_length, self.vocab_size]\n-        )\n-\n-    def create_and_check_causal_lm_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-\n-        model = TFBertLMHeadModel(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-            \"encoder_hidden_states\": encoder_hidden_states,\n-            \"encoder_attention_mask\": encoder_attention_mask,\n-        }\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs, token_type_ids=token_type_ids, encoder_hidden_states=encoder_hidden_states)\n-\n-        prediction_scores = result[\"logits\"]\n-        self.parent.assertListEqual(\n-            list(prediction_scores.numpy().shape), [self.batch_size, self.seq_length, self.vocab_size]\n-        )\n-\n-    def create_and_check_causal_lm_model_past(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        config.is_decoder = True\n-\n-        model = TFBertLMHeadModel(config=config)\n-\n-        # first forward pass\n-        outputs = model(input_ids, use_cache=True)\n-        outputs_use_cache_conf = model(input_ids)\n-        outputs_no_past = model(input_ids, use_cache=False)\n-\n-        self.parent.assertTrue(len(outputs) == len(outputs_use_cache_conf))\n-        self.parent.assertTrue(len(outputs) == len(outputs_no_past) + 1)\n-\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n-\n-        # append to next input_ids and attn_mask\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-\n-        output_from_no_past = model(next_input_ids, output_hidden_states=True).hidden_states[0]\n-        output_from_past = model(\n-            next_tokens, past_key_values=past_key_values, output_hidden_states=True\n-        ).hidden_states[0]\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), output_from_past.shape[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, 0, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)\n-\n-    def create_and_check_causal_lm_model_past_with_attn_mask(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        config.is_decoder = True\n-\n-        model = TFBertLMHeadModel(config=config)\n-\n-        # create attention mask\n-        half_seq_length = self.seq_length // 2\n-        attn_mask_begin = tf.ones((self.batch_size, half_seq_length), dtype=tf.int32)\n-        attn_mask_end = tf.zeros((self.batch_size, self.seq_length - half_seq_length), dtype=tf.int32)\n-        attn_mask = tf.concat([attn_mask_begin, attn_mask_end], axis=1)\n-\n-        # first forward pass\n-        outputs = model(input_ids, attention_mask=attn_mask, use_cache=True)\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n-\n-        past_key_values = outputs.past_key_values\n-\n-        # change a random masked slice from input_ids\n-        random_seq_idx_to_change = ids_tensor((1,), half_seq_length).numpy() + 1\n-        random_other_next_tokens = ids_tensor((self.batch_size, self.seq_length), config.vocab_size)\n-        vector_condition = tf.range(self.seq_length) == (self.seq_length - random_seq_idx_to_change)\n-        condition = tf.transpose(\n-            tf.broadcast_to(tf.expand_dims(vector_condition, -1), (self.seq_length, self.batch_size))\n-        )\n-        input_ids = tf.where(condition, random_other_next_tokens, input_ids)\n-\n-        # append to next input_ids and\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        attn_mask = tf.concat(\n-            [attn_mask, tf.ones((attn_mask.shape[0], 1), dtype=tf.int32)],\n-            axis=1,\n-        )\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=attn_mask,\n-            output_hidden_states=True,\n-        ).hidden_states[0]\n-        output_from_past = model(\n-            next_tokens, past_key_values=past_key_values, attention_mask=attn_mask, output_hidden_states=True\n-        ).hidden_states[0]\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), output_from_past.shape[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, 0, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)\n-\n-    def create_and_check_causal_lm_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        config.is_decoder = True\n-\n-        model = TFBertLMHeadModel(config=config)\n-\n-        input_ids = input_ids[:1, :]\n-        input_mask = input_mask[:1, :]\n-        self.batch_size = 1\n-\n-        # first forward pass\n-        outputs = model(input_ids, attention_mask=input_mask, use_cache=True)\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_attn_mask = ids_tensor((self.batch_size, 3), 2)\n-\n-        # append to next input_ids and\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        next_attention_mask = tf.concat([input_mask, next_attn_mask], axis=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            output_hidden_states=True,\n-        ).hidden_states[0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        ).hidden_states[0]\n-\n-        self.parent.assertEqual(next_tokens.shape[1], output_from_past.shape[1])\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), output_from_past.shape[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-\n-        model = TFBertLMHeadModel(config=config)\n-\n-        input_ids = input_ids[:1, :]\n-        input_mask = input_mask[:1, :]\n-        encoder_hidden_states = encoder_hidden_states[:1, :, :]\n-        encoder_attention_mask = encoder_attention_mask[:1, :]\n-        self.batch_size = 1\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_attn_mask = ids_tensor((self.batch_size, 3), 2)\n-\n-        # append to next input_ids and\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        next_attention_mask = tf.concat([input_mask, next_attn_mask], axis=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        ).hidden_states[0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        ).hidden_states[0]\n-\n-        self.parent.assertEqual(next_tokens.shape[1], output_from_past.shape[1])\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), output_from_past.shape[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)\n-\n-    def create_and_check_for_masked_lm(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFBertForMaskedLM(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_for_next_sequence_prediction(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFBertForNextSentencePrediction(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, 2))\n-\n-    def create_and_check_for_pretraining(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFBertForPreTraining(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.prediction_logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-        self.parent.assertEqual(result.seq_relationship_logits.shape, (self.batch_size, 2))\n-\n-    def create_and_check_for_sequence_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFBertForSequenceClassification(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_for_multiple_choice(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_choices = self.num_choices\n-        model = TFBertForMultipleChoice(config=config)\n-        multiple_choice_inputs_ids = tf.tile(tf.expand_dims(input_ids, 1), (1, self.num_choices, 1))\n-        multiple_choice_input_mask = tf.tile(tf.expand_dims(input_mask, 1), (1, self.num_choices, 1))\n-        multiple_choice_token_type_ids = tf.tile(tf.expand_dims(token_type_ids, 1), (1, self.num_choices, 1))\n-        inputs = {\n-            \"input_ids\": multiple_choice_inputs_ids,\n-            \"attention_mask\": multiple_choice_input_mask,\n-            \"token_type_ids\": multiple_choice_token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_choices))\n-\n-    def create_and_check_for_token_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFBertForTokenClassification(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def create_and_check_for_question_answering(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFBertForQuestionAnswering(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-\n-        result = model(inputs)\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFBertModelTest(TFModelTesterMixin, TFCoreModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFBertModel,\n-            TFBertForMaskedLM,\n-            TFBertLMHeadModel,\n-            TFBertForNextSentencePrediction,\n-            TFBertForPreTraining,\n-            TFBertForQuestionAnswering,\n-            TFBertForSequenceClassification,\n-            TFBertForTokenClassification,\n-            TFBertForMultipleChoice,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFBertModel,\n-            \"fill-mask\": TFBertForMaskedLM,\n-            \"question-answering\": TFBertForQuestionAnswering,\n-            \"text-classification\": TFBertForSequenceClassification,\n-            \"text-generation\": TFBertLMHeadModel,\n-            \"token-classification\": TFBertForTokenClassification,\n-            \"zero-shot\": TFBertForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_head_masking = False\n-    test_onnx = True\n-    onnx_min_opset = 10\n-\n-    # special case for ForPreTraining model\n-    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n-        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n-\n-        if return_labels:\n-            if model_class in get_values(TF_MODEL_FOR_PRETRAINING_MAPPING):\n-                inputs_dict[\"next_sentence_label\"] = tf.zeros(self.model_tester.batch_size, dtype=tf.int32)\n-\n-        return inputs_dict\n-\n-    def setUp(self):\n-        self.model_tester = TFBertModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=BertConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        \"\"\"Test the base model\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_causal_lm_base_model(self):\n-        \"\"\"Test the base model of the causal LM model\n-\n-        is_decoder=True, no cross_attention, no encoder outputs\n-        \"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_causal_lm_base_model(*config_and_inputs)\n-\n-    def test_model_as_decoder(self):\n-        \"\"\"Test the base model as a decoder (of an encoder-decoder architecture)\n-\n-        is_decoder=True + cross_attention + pass encoder outputs\n-        \"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n-\n-    def test_for_masked_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n-\n-    def test_for_causal_lm(self):\n-        \"\"\"Test the causal LM model\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_causal_lm_model(*config_and_inputs)\n-\n-    def test_causal_lm_model_as_decoder(self):\n-        \"\"\"Test the causal LM model as a decoder\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        self.model_tester.create_and_check_causal_lm_model_as_decoder(*config_and_inputs)\n-\n-    def test_causal_lm_model_past(self):\n-        \"\"\"Test causal LM model with `past_key_values`\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_causal_lm_model_past(*config_and_inputs)\n-\n-    def test_causal_lm_model_past_with_attn_mask(self):\n-        \"\"\"Test the causal LM model with `past_key_values` and `attention_mask`\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_causal_lm_model_past_with_attn_mask(*config_and_inputs)\n-\n-    def test_causal_lm_model_past_with_large_inputs(self):\n-        \"\"\"Test the causal LM model with `past_key_values` and a longer decoder sequence length\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_causal_lm_model_past_large_inputs(*config_and_inputs)\n-\n-    def test_decoder_model_past_with_large_inputs(self):\n-        \"\"\"Similar to `test_causal_lm_model_past_with_large_inputs` but with cross-attention\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n-    def test_for_multiple_choice(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_multiple_choice(*config_and_inputs)\n-\n-    def test_for_next_sequence_prediction(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_next_sequence_prediction(*config_and_inputs)\n-\n-    def test_for_pretraining(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_pretraining(*config_and_inputs)\n-\n-    def test_for_question_answering(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_question_answering(*config_and_inputs)\n-\n-    def test_for_sequence_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n-\n-    def test_for_token_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_token_classification(*config_and_inputs)\n-\n-    def test_model_from_pretrained(self):\n-        model = TFBertModel.from_pretrained(\"jplu/tiny-tf-bert-random\")\n-        self.assertIsNotNone(model)\n-\n-    def test_custom_load_tf_weights(self):\n-        model, output_loading_info = TFBertForTokenClassification.from_pretrained(\n-            \"jplu/tiny-tf-bert-random\", output_loading_info=True\n-        )\n-        self.assertEqual(sorted(output_loading_info[\"unexpected_keys\"]), [])\n-        for layer in output_loading_info[\"missing_keys\"]:\n-            self.assertTrue(layer.split(\"_\")[0] in [\"dropout\", \"classifier\"])\n-\n-    # TODO (Joao): fix me\n-    @unittest.skip(\"Onnx compliance broke with TF 2.10\")\n-    def test_onnx_compliancy(self):\n-        pass\n-\n-\n-@require_tf\n-class TFBertModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_inference_masked_lm(self):\n-        model = TFBertForPreTraining.from_pretrained(\"lysandre/tiny-bert-random\")\n-        input_ids = tf.constant([[0, 1, 2, 3, 4, 5]])\n-        output = model(input_ids)[0]\n-\n-        expected_shape = [1, 6, 32000]\n-        self.assertEqual(output.shape, expected_shape)\n-\n-        print(output[:, :3, :3])\n-\n-        expected_slice = tf.constant(\n-            [\n-                [\n-                    [-0.05243197, -0.04498899, 0.05512108],\n-                    [-0.07444685, -0.01064632, 0.04352357],\n-                    [-0.05020351, 0.05530146, 0.00700043],\n-                ]\n-            ]\n-        )\n-        tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=1e-4)"
        },
        {
            "sha": "fe1790bf75c09ecbf8bef081a500d5a424057417",
            "filename": "tests/models/big_bird/test_modeling_flax_big_bird.py",
            "status": "removed",
            "additions": 0,
            "deletions": 214,
            "changes": 214,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_flax_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_flax_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_flax_big_bird.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,214 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-from transformers import BigBirdConfig, is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor, random_attention_mask\n-\n-\n-if is_flax_available():\n-    import jax\n-\n-    from transformers.models.big_bird.modeling_flax_big_bird import (\n-        FlaxBigBirdForCausalLM,\n-        FlaxBigBirdForMaskedLM,\n-        FlaxBigBirdForMultipleChoice,\n-        FlaxBigBirdForPreTraining,\n-        FlaxBigBirdForQuestionAnswering,\n-        FlaxBigBirdForSequenceClassification,\n-        FlaxBigBirdForTokenClassification,\n-        FlaxBigBirdModel,\n-    )\n-\n-\n-class FlaxBigBirdModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=2,\n-        seq_length=56,\n-        is_training=True,\n-        use_attention_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=2,\n-        intermediate_size=7,\n-        hidden_act=\"gelu_new\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_choices=4,\n-        attention_type=\"block_sparse\",\n-        use_bias=True,\n-        rescale_embeddings=False,\n-        block_size=2,\n-        num_random_blocks=3,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_attention_mask = use_attention_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_choices = num_choices\n-\n-        self.rescale_embeddings = rescale_embeddings\n-        self.attention_type = attention_type\n-        self.use_bias = use_bias\n-        self.block_size = block_size\n-        self.num_random_blocks = num_random_blocks\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        attention_mask = None\n-        if self.use_attention_mask:\n-            attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        config = BigBirdConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            attention_type=self.attention_type,\n-            block_size=self.block_size,\n-            num_random_blocks=self.num_random_blocks,\n-            use_bias=self.use_bias,\n-            rescale_embeddings=self.rescale_embeddings,\n-        )\n-\n-        return config, input_ids, token_type_ids, attention_mask\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, token_type_ids, attention_mask = config_and_inputs\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"token_type_ids\": token_type_ids,\n-            \"attention_mask\": attention_mask,\n-        }\n-        return config, inputs_dict\n-\n-\n-@require_flax\n-class FlaxBigBirdModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            FlaxBigBirdForCausalLM,\n-            FlaxBigBirdModel,\n-            FlaxBigBirdForPreTraining,\n-            FlaxBigBirdForMaskedLM,\n-            FlaxBigBirdForMultipleChoice,\n-            FlaxBigBirdForQuestionAnswering,\n-            FlaxBigBirdForSequenceClassification,\n-            FlaxBigBirdForTokenClassification,\n-        )\n-        if is_flax_available()\n-        else ()\n-    )\n-\n-    test_attn_probs = False\n-    test_mismatched_shapes = False\n-\n-    def setUp(self):\n-        self.model_tester = FlaxBigBirdModelTester(self)\n-\n-    @slow\n-    # copied from `test_modeling_flax_common` because it takes much longer than other models\n-    def test_from_pretrained_save_pretrained(self):\n-        super().test_from_pretrained_save_pretrained()\n-\n-    @slow\n-    # copied from `test_modeling_flax_common` because it takes much longer than other models\n-    def test_from_pretrained_with_no_automatic_init(self):\n-        super().test_from_pretrained_with_no_automatic_init()\n-\n-    @slow\n-    # copied from `test_modeling_flax_common` because it takes much longer than other models\n-    def test_no_automatic_init(self):\n-        super().test_no_automatic_init()\n-\n-    @slow\n-    # copied from `test_modeling_flax_common` because it takes much longer than other models\n-    def test_hidden_states_output(self):\n-        super().test_hidden_states_output()\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"google/bigbird-roberta-base\")\n-            self.assertIsNotNone(model)\n-\n-    def test_attention_outputs(self):\n-        if self.test_attn_probs:\n-            super().test_attention_outputs()\n-\n-    @slow\n-    # copied from `test_modeling_flax_common` because it takes much longer than other models\n-    def test_jit_compilation(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                model = model_class(config)\n-\n-                @jax.jit\n-                def model_jitted(input_ids, attention_mask=None, **kwargs):\n-                    return model(input_ids=input_ids, attention_mask=attention_mask, **kwargs)\n-\n-                with self.subTest(\"JIT Enabled\"):\n-                    jitted_outputs = model_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                with self.subTest(\"JIT Disabled\"):\n-                    with jax.disable_jit():\n-                        outputs = model_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                self.assertEqual(len(outputs), len(jitted_outputs))\n-                for jitted_output, output in zip(jitted_outputs, outputs):\n-                    self.assertEqual(jitted_output.shape, output.shape)"
        },
        {
            "sha": "1d3f77ee38e1be54e78940e988b2647dbdc34cdf",
            "filename": "tests/models/blenderbot/test_modeling_flax_blenderbot.py",
            "status": "removed",
            "additions": 0,
            "deletions": 416,
            "changes": 416,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_flax_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_flax_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_flax_blenderbot.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,416 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-import numpy as np\n-import timeout_decorator  # noqa\n-\n-from transformers import BlenderbotConfig, is_flax_available\n-from transformers.testing_utils import jax_device, require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor\n-\n-\n-if is_flax_available():\n-    import os\n-\n-    # The slow tests are often failing with OOM error on GPU\n-    # This makes JAX allocate exactly what is needed on demand, and deallocate memory that is no longer needed\n-    # but will be slower as stated here https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n-    os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n-\n-    import jax\n-    import jax.numpy as jnp\n-\n-    from transformers import BlenderbotTokenizer\n-    from transformers.models.blenderbot.modeling_flax_blenderbot import (\n-        FlaxBlenderbotForConditionalGeneration,\n-        FlaxBlenderbotModel,\n-        shift_tokens_right,\n-    )\n-\n-\n-def prepare_blenderbot_inputs_dict(\n-    config,\n-    input_ids,\n-    decoder_input_ids=None,\n-    attention_mask=None,\n-    decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n-):\n-    if attention_mask is None:\n-        attention_mask = np.where(input_ids != config.pad_token_id, 1, 0)\n-    if decoder_attention_mask is None:\n-        decoder_attention_mask = np.where(decoder_input_ids != config.pad_token_id, 1, 0)\n-    if head_mask is None:\n-        head_mask = np.ones((config.encoder_layers, config.encoder_attention_heads))\n-    if decoder_head_mask is None:\n-        decoder_head_mask = np.ones((config.decoder_layers, config.decoder_attention_heads))\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = np.ones((config.decoder_layers, config.decoder_attention_heads))\n-    return {\n-        \"input_ids\": input_ids,\n-        \"decoder_input_ids\": decoder_input_ids,\n-        \"attention_mask\": attention_mask,\n-        \"decoder_attention_mask\": attention_mask,\n-    }\n-\n-\n-class FlaxBlenderbotModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_labels=False,\n-        vocab_size=99,\n-        hidden_size=16,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=4,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=50,\n-        eos_token_id=2,\n-        pad_token_id=1,\n-        bos_token_id=0,\n-        initializer_range=0.02,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-        self.initializer_range = initializer_range\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = np.clip(ids_tensor([self.batch_size, self.seq_length - 1], self.vocab_size), 3, self.vocab_size)\n-        input_ids = np.concatenate((input_ids, 2 * np.ones((self.batch_size, 1), dtype=np.int64)), -1)\n-\n-        decoder_input_ids = shift_tokens_right(input_ids, 1, 2)\n-\n-        config = BlenderbotConfig(\n-            vocab_size=self.vocab_size,\n-            d_model=self.hidden_size,\n-            encoder_layers=self.num_hidden_layers,\n-            decoder_layers=self.num_hidden_layers,\n-            encoder_attention_heads=self.num_attention_heads,\n-            decoder_attention_heads=self.num_attention_heads,\n-            encoder_ffn_dim=self.intermediate_size,\n-            decoder_ffn_dim=self.intermediate_size,\n-            dropout=self.hidden_dropout_prob,\n-            attention_dropout=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            eos_token_id=self.eos_token_id,\n-            bos_token_id=self.bos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            initializer_range=self.initializer_range,\n-            use_cache=False,\n-        )\n-        inputs_dict = prepare_blenderbot_inputs_dict(config, input_ids, decoder_input_ids)\n-        return config, inputs_dict\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config, inputs_dict = self.prepare_config_and_inputs()\n-        return config, inputs_dict\n-\n-    def check_use_cache_forward(self, model_class_name, config, inputs_dict):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        encoder_outputs = model.encode(inputs_dict[\"input_ids\"])\n-\n-        decoder_input_ids, decoder_attention_mask = (\n-            inputs_dict[\"decoder_input_ids\"],\n-            inputs_dict[\"decoder_attention_mask\"],\n-        )\n-\n-        past_key_values = model.init_cache(decoder_input_ids.shape[0], max_decoder_length, encoder_outputs)\n-        decoder_attention_mask = jnp.ones((decoder_input_ids.shape[0], max_decoder_length), dtype=\"i4\")\n-\n-        decoder_position_ids = jnp.broadcast_to(\n-            jnp.arange(decoder_input_ids.shape[-1] - 1)[None, :],\n-            (decoder_input_ids.shape[0], decoder_input_ids.shape[-1] - 1),\n-        )\n-        outputs_cache = model.decode(\n-            decoder_input_ids[:, :-1],\n-            encoder_outputs,\n-            decoder_attention_mask=decoder_attention_mask,\n-            past_key_values=past_key_values,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-\n-        decoder_position_ids = jnp.array(decoder_input_ids.shape[0] * [[decoder_input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model.decode(\n-            decoder_input_ids[:, -1:],\n-            encoder_outputs,\n-            decoder_attention_mask=decoder_attention_mask,\n-            past_key_values=outputs_cache.past_key_values,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-\n-        outputs = model.decode(decoder_input_ids, encoder_outputs)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-    def check_use_cache_forward_with_attn_mask(self, model_class_name, config, inputs_dict):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        encoder_outputs = model.encode(inputs_dict[\"input_ids\"])\n-\n-        decoder_input_ids, decoder_attention_mask = (\n-            inputs_dict[\"decoder_input_ids\"],\n-            inputs_dict[\"decoder_attention_mask\"],\n-        )\n-\n-        decoder_attention_mask_cache = jnp.concatenate(\n-            [\n-                decoder_attention_mask,\n-                jnp.zeros((decoder_attention_mask.shape[0], max_decoder_length - decoder_attention_mask.shape[1])),\n-            ],\n-            axis=-1,\n-        )\n-\n-        past_key_values = model.init_cache(decoder_input_ids.shape[0], max_decoder_length, encoder_outputs)\n-        decoder_position_ids = jnp.broadcast_to(\n-            jnp.arange(decoder_input_ids.shape[-1] - 1)[None, :],\n-            (decoder_input_ids.shape[0], decoder_input_ids.shape[-1] - 1),\n-        )\n-\n-        outputs_cache = model.decode(\n-            decoder_input_ids[:, :-1],\n-            encoder_outputs,\n-            decoder_attention_mask=decoder_attention_mask_cache,\n-            past_key_values=past_key_values,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-        decoder_position_ids = jnp.array(decoder_input_ids.shape[0] * [[decoder_input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model.decode(\n-            decoder_input_ids[:, -1:],\n-            encoder_outputs,\n-            past_key_values=outputs_cache.past_key_values,\n-            decoder_attention_mask=decoder_attention_mask_cache,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-\n-        outputs = model.decode(decoder_input_ids, encoder_outputs, decoder_attention_mask=decoder_attention_mask)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-\n-@require_flax\n-class BlenderbotHeadTests(unittest.TestCase):\n-    vocab_size = 99\n-\n-    def _get_config_and_data(self):\n-        input_ids = np.array(\n-            [\n-                [71, 82, 18, 33, 46, 91, 2],\n-                [68, 34, 26, 58, 30, 82, 2],\n-                [5, 97, 17, 39, 94, 40, 2],\n-                [76, 83, 94, 25, 70, 78, 2],\n-                [87, 59, 41, 35, 48, 66, 2],\n-                [55, 13, 16, 58, 5, 2, 1],  # note padding\n-                [64, 27, 31, 51, 12, 75, 2],\n-                [52, 64, 86, 17, 83, 39, 2],\n-                [48, 61, 9, 24, 71, 82, 2],\n-                [26, 1, 60, 48, 22, 13, 2],\n-                [21, 5, 62, 28, 14, 76, 2],\n-                [45, 98, 37, 86, 59, 48, 2],\n-                [70, 70, 50, 9, 28, 0, 2],\n-            ],\n-            dtype=np.int64,\n-        )\n-\n-        batch_size = input_ids.shape[0]\n-        config = BlenderbotConfig(\n-            vocab_size=self.vocab_size,\n-            d_model=24,\n-            encoder_layers=2,\n-            decoder_layers=2,\n-            encoder_attention_heads=2,\n-            decoder_attention_heads=2,\n-            encoder_ffn_dim=32,\n-            decoder_ffn_dim=32,\n-            max_position_embeddings=48,\n-            eos_token_id=2,\n-            pad_token_id=1,\n-            bos_token_id=0,\n-        )\n-        return config, input_ids, batch_size\n-\n-    # @timeout_decorator.timeout(1)  # not working with the decorator so far\n-    def test_lm_forward(self):\n-        config, input_ids, batch_size = self._get_config_and_data()\n-        lm_model = FlaxBlenderbotForConditionalGeneration(config)\n-        outputs = lm_model(input_ids=input_ids)\n-        expected_shape = (batch_size, input_ids.shape[1], config.vocab_size)\n-        self.assertEqual(outputs[\"logits\"].shape, expected_shape)\n-\n-    def test_lm_uneven_forward(self):\n-        config = BlenderbotConfig(\n-            vocab_size=self.vocab_size,\n-            d_model=14,\n-            encoder_layers=2,\n-            decoder_layers=2,\n-            encoder_attention_heads=2,\n-            decoder_attention_heads=2,\n-            encoder_ffn_dim=8,\n-            decoder_ffn_dim=8,\n-            max_position_embeddings=48,\n-        )\n-        lm_model = FlaxBlenderbotForConditionalGeneration(config)\n-        context = np.array([[71, 82, 18, 33, 46, 91, 2], [68, 34, 26, 58, 30, 2, 1]], dtype=np.int64)\n-        summary = np.array([[82, 71, 82, 18, 2], [58, 68, 2, 1, 1]], dtype=np.int64)\n-        outputs = lm_model(input_ids=context, decoder_input_ids=summary)\n-        expected_shape = (*summary.shape, config.vocab_size)\n-        self.assertEqual(outputs[\"logits\"].shape, expected_shape)\n-\n-    def test_shift_tokens_right(self):\n-        input_ids = np.array([[71, 82, 18, 33, 2, 1, 1], [68, 34, 26, 58, 30, 82, 2]], dtype=np.int64)\n-        shifted = shift_tokens_right(input_ids, 1, 2)\n-        n_pad_before = np.equal(input_ids, 1).astype(np.float32).sum()\n-        n_pad_after = np.equal(shifted, 1).astype(np.float32).sum()\n-        self.assertEqual(shifted.shape, input_ids.shape)\n-        self.assertEqual(n_pad_after, n_pad_before - 1)\n-        self.assertTrue(np.equal(shifted[:, 0], 2).all())\n-\n-\n-@require_flax\n-class FlaxBlenderbotModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    is_encoder_decoder = True\n-    all_model_classes = (\n-        (\n-            FlaxBlenderbotModel,\n-            FlaxBlenderbotForConditionalGeneration,\n-        )\n-        if is_flax_available()\n-        else ()\n-    )\n-\n-    def setUp(self):\n-        self.model_tester = FlaxBlenderbotModelTester(self)\n-\n-    def test_use_cache_forward(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n-        for model_class in self.all_model_classes:\n-            self.model_tester.check_use_cache_forward(model_class, config, inputs_dict)\n-\n-    def test_use_cache_forward_with_attn_mask(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n-        for model_class in self.all_model_classes:\n-            self.model_tester.check_use_cache_forward_with_attn_mask(model_class, config, inputs_dict)\n-\n-    def test_encode(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                model = model_class(config)\n-\n-                @jax.jit\n-                def encode_jitted(input_ids, attention_mask=None, **kwargs):\n-                    return model.encode(input_ids=input_ids, attention_mask=attention_mask)\n-\n-                with self.subTest(\"JIT Enabled\"):\n-                    jitted_outputs = encode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                with self.subTest(\"JIT Disabled\"):\n-                    with jax.disable_jit():\n-                        outputs = encode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                self.assertEqual(len(outputs), len(jitted_outputs))\n-                for jitted_output, output in zip(jitted_outputs, outputs):\n-                    self.assertEqual(jitted_output.shape, output.shape)\n-\n-    def test_decode(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                model = model_class(config)\n-                encoder_outputs = model.encode(inputs_dict[\"input_ids\"], inputs_dict[\"attention_mask\"])\n-\n-                prepared_inputs_dict = {\n-                    \"decoder_input_ids\": inputs_dict[\"decoder_input_ids\"],\n-                    \"decoder_attention_mask\": inputs_dict[\"decoder_attention_mask\"],\n-                    \"encoder_outputs\": encoder_outputs,\n-                }\n-\n-                @jax.jit\n-                def decode_jitted(decoder_input_ids, decoder_attention_mask, encoder_outputs):\n-                    return model.decode(\n-                        decoder_input_ids=decoder_input_ids,\n-                        decoder_attention_mask=decoder_attention_mask,\n-                        encoder_outputs=encoder_outputs,\n-                    )\n-\n-                with self.subTest(\"JIT Enabled\"):\n-                    jitted_outputs = decode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                with self.subTest(\"JIT Disabled\"):\n-                    with jax.disable_jit():\n-                        outputs = decode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                self.assertEqual(len(outputs), len(jitted_outputs))\n-                for jitted_output, output in zip(jitted_outputs, outputs):\n-                    self.assertEqual(jitted_output.shape, output.shape)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"facebook/blenderbot-400M-distill\")\n-            # FlaxBlenderbotForSequenceClassification expects eos token in input_ids\n-            input_ids = np.ones((1, 1)) * model.config.eos_token_id\n-            outputs = model(input_ids)\n-            self.assertIsNotNone(outputs)\n-\n-    @unittest.skipUnless(jax_device != \"cpu\", \"3B test too slow on CPU.\")\n-    @slow\n-    def test_generation_from_short_input_same_as_parlai_3B(self):\n-        FASTER_GEN_KWARGS = {\"num_beams\": 1, \"early_stopping\": True, \"min_length\": 15, \"max_length\": 25}\n-        TOK_DECODE_KW = {\"skip_special_tokens\": True, \"clean_up_tokenization_spaces\": True}\n-\n-        model = FlaxBlenderbotForConditionalGeneration.from_pretrained(\"facebook/blenderbot-3B\", from_pt=True)\n-        tokenizer = BlenderbotTokenizer.from_pretrained(\"facebook/blenderbot-3B\")\n-\n-        src_text = [\"Sam\"]\n-        model_inputs = tokenizer(src_text, return_tensors=\"jax\")\n-\n-        generated_utterances = model.generate(**model_inputs, **FASTER_GEN_KWARGS)\n-        tgt_text = 'Sam is a great name. It means \"sun\" in Gaelic.'\n-\n-        generated_txt = tokenizer.batch_decode(generated_utterances, **TOK_DECODE_KW)\n-        assert generated_txt[0].strip() == tgt_text"
        },
        {
            "sha": "435c37d5fcc3dfa4883f03d2908629000a813308",
            "filename": "tests/models/blenderbot/test_modeling_tf_blenderbot.py",
            "status": "removed",
            "additions": 0,
            "deletions": 234,
            "changes": 234,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_tf_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_tf_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot%2Ftest_modeling_tf_blenderbot.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,234 +0,0 @@\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import BlenderbotConfig, BlenderbotTokenizer, is_tf_available\n-from transformers.testing_utils import require_tf, require_tokenizers, slow\n-from transformers.utils import cached_property\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import TFAutoModelForSeq2SeqLM, TFBlenderbotForConditionalGeneration, TFBlenderbotModel\n-\n-\n-@require_tf\n-class TFBlenderbotModelTester:\n-    config_cls = BlenderbotConfig\n-    config_updates = {}\n-    hidden_act = \"gelu\"\n-\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_labels=False,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=50,\n-        eos_token_id=2,\n-        pad_token_id=1,\n-        bos_token_id=0,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length - 1], self.vocab_size)\n-        eos_tensor = tf.expand_dims(tf.constant([self.eos_token_id] * self.batch_size), 1)\n-        input_ids = tf.concat([input_ids, eos_tensor], axis=1)\n-\n-        decoder_input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        config = self.config_cls(\n-            vocab_size=self.vocab_size,\n-            d_model=self.hidden_size,\n-            encoder_layers=self.num_hidden_layers,\n-            decoder_layers=self.num_hidden_layers,\n-            encoder_attention_heads=self.num_attention_heads,\n-            decoder_attention_heads=self.num_attention_heads,\n-            encoder_ffn_dim=self.intermediate_size,\n-            decoder_ffn_dim=self.intermediate_size,\n-            dropout=self.hidden_dropout_prob,\n-            attention_dropout=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            eos_token_ids=[2],\n-            bos_token_id=self.bos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            decoder_start_token_id=self.pad_token_id,\n-            **self.config_updates,\n-        )\n-        inputs_dict = prepare_blenderbot_inputs_dict(config, input_ids, decoder_input_ids)\n-        return config, inputs_dict\n-\n-    def check_decoder_model_past_large_inputs(self, config, inputs_dict):\n-        model = TFBlenderbotModel(config=config).get_decoder()\n-        input_ids = inputs_dict[\"input_ids\"]\n-\n-        input_ids = input_ids[:1, :]\n-        attention_mask = inputs_dict[\"attention_mask\"][:1, :]\n-        head_mask = inputs_dict[\"head_mask\"]\n-        self.batch_size = 1\n-\n-        # first forward pass\n-        outputs = model(input_ids, attention_mask=attention_mask, head_mask=head_mask, use_cache=True)\n-\n-        output, past_key_values = outputs.to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_attn_mask = tf.cast(ids_tensor((self.batch_size, 3), 2), tf.int8)\n-\n-        # append to next input_ids and\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        next_attention_mask = tf.concat([attention_mask, next_attn_mask], axis=-1)\n-\n-        output_from_no_past = model(next_input_ids, attention_mask=next_attention_mask)[0]\n-        output_from_past = model(next_tokens, attention_mask=next_attention_mask, past_key_values=past_key_values)[0]\n-\n-        self.parent.assertEqual(next_tokens.shape[1], output_from_past.shape[1])\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), output_from_past.shape[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)\n-\n-\n-def prepare_blenderbot_inputs_dict(\n-    config,\n-    input_ids,\n-    decoder_input_ids,\n-    attention_mask=None,\n-    decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n-):\n-    if attention_mask is None:\n-        attention_mask = tf.cast(tf.math.not_equal(input_ids, config.pad_token_id), tf.int8)\n-    if decoder_attention_mask is None:\n-        decoder_attention_mask = tf.concat(\n-            [\n-                tf.ones(decoder_input_ids[:, :1].shape, dtype=tf.int8),\n-                tf.cast(tf.math.not_equal(decoder_input_ids[:, 1:], config.pad_token_id), tf.int8),\n-            ],\n-            axis=-1,\n-        )\n-    if head_mask is None:\n-        head_mask = tf.ones((config.encoder_layers, config.encoder_attention_heads))\n-    if decoder_head_mask is None:\n-        decoder_head_mask = tf.ones((config.decoder_layers, config.decoder_attention_heads))\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = tf.ones((config.decoder_layers, config.decoder_attention_heads))\n-    return {\n-        \"input_ids\": input_ids,\n-        \"decoder_input_ids\": decoder_input_ids,\n-        \"attention_mask\": attention_mask,\n-        \"decoder_attention_mask\": decoder_attention_mask,\n-        \"head_mask\": head_mask,\n-        \"decoder_head_mask\": decoder_head_mask,\n-        \"cross_attn_head_mask\": cross_attn_head_mask,\n-    }\n-\n-\n-@require_tf\n-class TFBlenderbotModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFBlenderbotForConditionalGeneration, TFBlenderbotModel) if is_tf_available() else ()\n-    all_generative_model_classes = (TFBlenderbotForConditionalGeneration,) if is_tf_available() else ()\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFBlenderbotModel,\n-            \"summarization\": TFBlenderbotForConditionalGeneration,\n-            \"text2text-generation\": TFBlenderbotForConditionalGeneration,\n-            \"translation\": TFBlenderbotForConditionalGeneration,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    is_encoder_decoder = True\n-    test_pruning = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFBlenderbotModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=BlenderbotConfig)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_decoder_model_past_large_inputs(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-        self.model_tester.check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n-\n-@require_tokenizers\n-@require_tf\n-class TFBlenderbot400MIntegrationTests(unittest.TestCase):\n-    src_text = [\"My friends are cool but they eat too many carbs.\"]\n-    model_name = \"facebook/blenderbot-400M-distill\"\n-\n-    @cached_property\n-    def tokenizer(self):\n-        return BlenderbotTokenizer.from_pretrained(self.model_name)\n-\n-    @cached_property\n-    def model(self):\n-        model = TFAutoModelForSeq2SeqLM.from_pretrained(self.model_name)\n-        return model\n-\n-    @slow\n-    def test_generation_from_long_input(self):\n-        model_inputs = self.tokenizer(self.src_text, return_tensors=\"tf\")\n-        generated_ids = self.model.generate(\n-            model_inputs.input_ids,\n-        )\n-        generated_words = self.tokenizer.batch_decode(generated_ids.numpy(), skip_special_tokens=True)[0]\n-        assert (\n-            generated_words\n-            == \" That's unfortunate. Are they trying to lose weight or are they just trying to be healthier?\"\n-        )"
        },
        {
            "sha": "7d810feb8d3f00d5afa85201dfb6a0bd255242ac",
            "filename": "tests/models/blenderbot_small/test_modeling_flax_blenderbot_small.py",
            "status": "removed",
            "additions": 0,
            "deletions": 409,
            "changes": 409,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_flax_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_flax_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_flax_blenderbot_small.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,409 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-import numpy as np\n-import timeout_decorator  # noqa\n-\n-from transformers import BlenderbotSmallConfig, is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor\n-\n-\n-if is_flax_available():\n-    import os\n-\n-    # The slow tests are often failing with OOM error on GPU\n-    # This makes JAX allocate exactly what is needed on demand, and deallocate memory that is no longer needed\n-    # but will be slower as stated here https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n-    os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n-\n-    import jax\n-    import jax.numpy as jnp\n-\n-    from transformers.models.blenderbot_small.modeling_flax_blenderbot_small import (\n-        FlaxBlenderbotSmallForConditionalGeneration,\n-        FlaxBlenderbotSmallModel,\n-        shift_tokens_right,\n-    )\n-\n-\n-def prepare_blenderbot_inputs_dict(\n-    config,\n-    input_ids,\n-    decoder_input_ids=None,\n-    attention_mask=None,\n-    decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n-):\n-    if attention_mask is None:\n-        attention_mask = np.where(input_ids != config.pad_token_id, 1, 0)\n-    if decoder_attention_mask is None:\n-        decoder_attention_mask = np.where(decoder_input_ids != config.pad_token_id, 1, 0)\n-    if head_mask is None:\n-        head_mask = np.ones((config.encoder_layers, config.encoder_attention_heads))\n-    if decoder_head_mask is None:\n-        decoder_head_mask = np.ones((config.decoder_layers, config.decoder_attention_heads))\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = np.ones((config.decoder_layers, config.decoder_attention_heads))\n-    return {\n-        \"input_ids\": input_ids,\n-        \"decoder_input_ids\": decoder_input_ids,\n-        \"attention_mask\": attention_mask,\n-        \"decoder_attention_mask\": attention_mask,\n-    }\n-\n-\n-class FlaxBlenderbotSmallModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_labels=False,\n-        vocab_size=99,\n-        hidden_size=16,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=4,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=50,\n-        eos_token_id=2,\n-        pad_token_id=1,\n-        bos_token_id=0,\n-        initializer_range=0.02,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-        self.initializer_range = initializer_range\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = np.clip(ids_tensor([self.batch_size, self.seq_length - 1], self.vocab_size), 3, self.vocab_size)\n-        input_ids = np.concatenate((input_ids, 2 * np.ones((self.batch_size, 1), dtype=np.int64)), -1)\n-\n-        decoder_input_ids = shift_tokens_right(input_ids, 1, 2)\n-\n-        config = BlenderbotSmallConfig(\n-            vocab_size=self.vocab_size,\n-            d_model=self.hidden_size,\n-            encoder_layers=self.num_hidden_layers,\n-            decoder_layers=self.num_hidden_layers,\n-            encoder_attention_heads=self.num_attention_heads,\n-            decoder_attention_heads=self.num_attention_heads,\n-            encoder_ffn_dim=self.intermediate_size,\n-            decoder_ffn_dim=self.intermediate_size,\n-            dropout=self.hidden_dropout_prob,\n-            attention_dropout=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            eos_token_id=self.eos_token_id,\n-            bos_token_id=self.bos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            initializer_range=self.initializer_range,\n-            use_cache=False,\n-        )\n-        inputs_dict = prepare_blenderbot_inputs_dict(config, input_ids, decoder_input_ids)\n-        return config, inputs_dict\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config, inputs_dict = self.prepare_config_and_inputs()\n-        return config, inputs_dict\n-\n-    def check_use_cache_forward(self, model_class_name, config, inputs_dict):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        encoder_outputs = model.encode(inputs_dict[\"input_ids\"])\n-\n-        decoder_input_ids, decoder_attention_mask = (\n-            inputs_dict[\"decoder_input_ids\"],\n-            inputs_dict[\"decoder_attention_mask\"],\n-        )\n-\n-        past_key_values = model.init_cache(decoder_input_ids.shape[0], max_decoder_length, encoder_outputs)\n-        decoder_attention_mask = jnp.ones((decoder_input_ids.shape[0], max_decoder_length), dtype=\"i4\")\n-\n-        decoder_position_ids = jnp.broadcast_to(\n-            jnp.arange(decoder_input_ids.shape[-1] - 1)[None, :],\n-            (decoder_input_ids.shape[0], decoder_input_ids.shape[-1] - 1),\n-        )\n-        outputs_cache = model.decode(\n-            decoder_input_ids[:, :-1],\n-            encoder_outputs,\n-            decoder_attention_mask=decoder_attention_mask,\n-            past_key_values=past_key_values,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-\n-        decoder_position_ids = jnp.array(decoder_input_ids.shape[0] * [[decoder_input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model.decode(\n-            decoder_input_ids[:, -1:],\n-            encoder_outputs,\n-            decoder_attention_mask=decoder_attention_mask,\n-            past_key_values=outputs_cache.past_key_values,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-\n-        outputs = model.decode(decoder_input_ids, encoder_outputs)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-    def check_use_cache_forward_with_attn_mask(self, model_class_name, config, inputs_dict):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        encoder_outputs = model.encode(inputs_dict[\"input_ids\"])\n-\n-        decoder_input_ids, decoder_attention_mask = (\n-            inputs_dict[\"decoder_input_ids\"],\n-            inputs_dict[\"decoder_attention_mask\"],\n-        )\n-\n-        decoder_attention_mask_cache = jnp.concatenate(\n-            [\n-                decoder_attention_mask,\n-                jnp.zeros((decoder_attention_mask.shape[0], max_decoder_length - decoder_attention_mask.shape[1])),\n-            ],\n-            axis=-1,\n-        )\n-\n-        past_key_values = model.init_cache(decoder_input_ids.shape[0], max_decoder_length, encoder_outputs)\n-        decoder_position_ids = jnp.broadcast_to(\n-            jnp.arange(decoder_input_ids.shape[-1] - 1)[None, :],\n-            (decoder_input_ids.shape[0], decoder_input_ids.shape[-1] - 1),\n-        )\n-\n-        outputs_cache = model.decode(\n-            decoder_input_ids[:, :-1],\n-            encoder_outputs,\n-            decoder_attention_mask=decoder_attention_mask_cache,\n-            past_key_values=past_key_values,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-        decoder_position_ids = jnp.array(decoder_input_ids.shape[0] * [[decoder_input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model.decode(\n-            decoder_input_ids[:, -1:],\n-            encoder_outputs,\n-            past_key_values=outputs_cache.past_key_values,\n-            decoder_attention_mask=decoder_attention_mask_cache,\n-            decoder_position_ids=decoder_position_ids,\n-        )\n-\n-        outputs = model.decode(decoder_input_ids, encoder_outputs, decoder_attention_mask=decoder_attention_mask)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-\n-@require_flax\n-class BlenderbotHeadTests(unittest.TestCase):\n-    vocab_size = 99\n-\n-    def _get_config_and_data(self):\n-        input_ids = np.array(\n-            [\n-                [71, 82, 18, 33, 46, 91, 2],\n-                [68, 34, 26, 58, 30, 82, 2],\n-                [5, 97, 17, 39, 94, 40, 2],\n-                [76, 83, 94, 25, 70, 78, 2],\n-                [87, 59, 41, 35, 48, 66, 2],\n-                [55, 13, 16, 58, 5, 2, 1],  # note padding\n-                [64, 27, 31, 51, 12, 75, 2],\n-                [52, 64, 86, 17, 83, 39, 2],\n-                [48, 61, 9, 24, 71, 82, 2],\n-                [26, 1, 60, 48, 22, 13, 2],\n-                [21, 5, 62, 28, 14, 76, 2],\n-                [45, 98, 37, 86, 59, 48, 2],\n-                [70, 70, 50, 9, 28, 0, 2],\n-            ],\n-            dtype=np.int64,\n-        )\n-\n-        batch_size = input_ids.shape[0]\n-        config = BlenderbotSmallConfig(\n-            vocab_size=self.vocab_size,\n-            d_model=24,\n-            encoder_layers=2,\n-            decoder_layers=2,\n-            encoder_attention_heads=2,\n-            decoder_attention_heads=2,\n-            encoder_ffn_dim=32,\n-            decoder_ffn_dim=32,\n-            max_position_embeddings=48,\n-            eos_token_id=2,\n-            pad_token_id=1,\n-            bos_token_id=0,\n-        )\n-        return config, input_ids, batch_size\n-\n-    # @timeout_decorator.timeout(1)  # not working with the decorator so far\n-    def test_lm_forward(self):\n-        config, input_ids, batch_size = self._get_config_and_data()\n-        lm_model = FlaxBlenderbotSmallForConditionalGeneration(config)\n-        outputs = lm_model(input_ids=input_ids)\n-        expected_shape = (batch_size, input_ids.shape[1], config.vocab_size)\n-        self.assertEqual(outputs[\"logits\"].shape, expected_shape)\n-\n-    def test_lm_uneven_forward(self):\n-        config = BlenderbotSmallConfig(\n-            vocab_size=self.vocab_size,\n-            d_model=14,\n-            encoder_layers=2,\n-            decoder_layers=2,\n-            encoder_attention_heads=2,\n-            decoder_attention_heads=2,\n-            encoder_ffn_dim=8,\n-            decoder_ffn_dim=8,\n-            max_position_embeddings=48,\n-        )\n-        lm_model = FlaxBlenderbotSmallForConditionalGeneration(config)\n-        context = np.array([[71, 82, 18, 33, 46, 91, 2], [68, 34, 26, 58, 30, 2, 1]], dtype=np.int64)\n-        summary = np.array([[82, 71, 82, 18, 2], [58, 68, 2, 1, 1]], dtype=np.int64)\n-        outputs = lm_model(input_ids=context, decoder_input_ids=summary)\n-        expected_shape = (*summary.shape, config.vocab_size)\n-        self.assertEqual(outputs[\"logits\"].shape, expected_shape)\n-\n-    def test_shift_tokens_right(self):\n-        input_ids = np.array([[71, 82, 18, 33, 2, 1, 1], [68, 34, 26, 58, 30, 82, 2]], dtype=np.int64)\n-        shifted = shift_tokens_right(input_ids, 1, 2)\n-        n_pad_before = np.equal(input_ids, 1).astype(np.float32).sum()\n-        n_pad_after = np.equal(shifted, 1).astype(np.float32).sum()\n-        self.assertEqual(shifted.shape, input_ids.shape)\n-        self.assertEqual(n_pad_after, n_pad_before - 1)\n-        self.assertTrue(np.equal(shifted[:, 0], 2).all())\n-\n-\n-@require_flax\n-class FlaxBlenderbotSmallModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    is_encoder_decoder = True\n-    all_model_classes = (\n-        (\n-            FlaxBlenderbotSmallModel,\n-            FlaxBlenderbotSmallForConditionalGeneration,\n-        )\n-        if is_flax_available()\n-        else ()\n-    )\n-\n-    def is_pipeline_test_to_skip(\n-        self,\n-        pipeline_test_case_name,\n-        config_class,\n-        model_architecture,\n-        tokenizer_name,\n-        image_processor_name,\n-        feature_extractor_name,\n-        processor_name,\n-    ):\n-        return pipeline_test_case_name == \"TextGenerationPipelineTests\"\n-\n-    def setUp(self):\n-        self.model_tester = FlaxBlenderbotSmallModelTester(self)\n-\n-    def test_use_cache_forward(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n-        for model_class in self.all_model_classes:\n-            self.model_tester.check_use_cache_forward(model_class, config, inputs_dict)\n-\n-    def test_use_cache_forward_with_attn_mask(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n-        for model_class in self.all_model_classes:\n-            self.model_tester.check_use_cache_forward_with_attn_mask(model_class, config, inputs_dict)\n-\n-    def test_encode(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                model = model_class(config)\n-\n-                @jax.jit\n-                def encode_jitted(input_ids, attention_mask=None, **kwargs):\n-                    return model.encode(input_ids=input_ids, attention_mask=attention_mask)\n-\n-                with self.subTest(\"JIT Enabled\"):\n-                    jitted_outputs = encode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                with self.subTest(\"JIT Disabled\"):\n-                    with jax.disable_jit():\n-                        outputs = encode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                self.assertEqual(len(outputs), len(jitted_outputs))\n-                for jitted_output, output in zip(jitted_outputs, outputs):\n-                    self.assertEqual(jitted_output.shape, output.shape)\n-\n-    def test_decode(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                model = model_class(config)\n-                encoder_outputs = model.encode(inputs_dict[\"input_ids\"], inputs_dict[\"attention_mask\"])\n-\n-                prepared_inputs_dict = {\n-                    \"decoder_input_ids\": inputs_dict[\"decoder_input_ids\"],\n-                    \"decoder_attention_mask\": inputs_dict[\"decoder_attention_mask\"],\n-                    \"encoder_outputs\": encoder_outputs,\n-                }\n-\n-                @jax.jit\n-                def decode_jitted(decoder_input_ids, decoder_attention_mask, encoder_outputs):\n-                    return model.decode(\n-                        decoder_input_ids=decoder_input_ids,\n-                        decoder_attention_mask=decoder_attention_mask,\n-                        encoder_outputs=encoder_outputs,\n-                    )\n-\n-                with self.subTest(\"JIT Enabled\"):\n-                    jitted_outputs = decode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                with self.subTest(\"JIT Disabled\"):\n-                    with jax.disable_jit():\n-                        outputs = decode_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                self.assertEqual(len(outputs), len(jitted_outputs))\n-                for jitted_output, output in zip(jitted_outputs, outputs):\n-                    self.assertEqual(jitted_output.shape, output.shape)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"facebook/blenderbot_small-90M\")\n-            # FlaxBlenderbotForSequenceClassification expects eos token in input_ids\n-            input_ids = np.ones((1, 1)) * model.config.eos_token_id\n-            outputs = model(input_ids)\n-            self.assertIsNotNone(outputs)"
        },
        {
            "sha": "70cb580cd6ce2cd6f21a312721881a1126dfd70e",
            "filename": "tests/models/blenderbot_small/test_modeling_tf_blenderbot_small.py",
            "status": "removed",
            "additions": 0,
            "deletions": 256,
            "changes": 256,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_tf_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_tf_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblenderbot_small%2Ftest_modeling_tf_blenderbot_small.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,256 +0,0 @@\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import BlenderbotSmallConfig, BlenderbotSmallTokenizer, is_tf_available\n-from transformers.testing_utils import require_tf, require_tokenizers, slow\n-from transformers.utils import cached_property\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import TFAutoModelForSeq2SeqLM, TFBlenderbotSmallForConditionalGeneration, TFBlenderbotSmallModel\n-\n-\n-@require_tf\n-class TFBlenderbotSmallModelTester:\n-    config_cls = BlenderbotSmallConfig\n-    config_updates = {}\n-    hidden_act = \"gelu\"\n-\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_labels=False,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=50,\n-        eos_token_id=2,\n-        pad_token_id=1,\n-        bos_token_id=0,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length - 1], self.vocab_size)\n-        eos_tensor = tf.expand_dims(tf.constant([self.eos_token_id] * self.batch_size), 1)\n-        input_ids = tf.concat([input_ids, eos_tensor], axis=1)\n-\n-        decoder_input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        config = self.config_cls(\n-            vocab_size=self.vocab_size,\n-            d_model=self.hidden_size,\n-            encoder_layers=self.num_hidden_layers,\n-            decoder_layers=self.num_hidden_layers,\n-            encoder_attention_heads=self.num_attention_heads,\n-            decoder_attention_heads=self.num_attention_heads,\n-            encoder_ffn_dim=self.intermediate_size,\n-            decoder_ffn_dim=self.intermediate_size,\n-            dropout=self.hidden_dropout_prob,\n-            attention_dropout=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            eos_token_ids=[2],\n-            bos_token_id=self.bos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            decoder_start_token_id=self.pad_token_id,\n-            **self.config_updates,\n-        )\n-        inputs_dict = prepare_blenderbot_small_inputs_dict(config, input_ids, decoder_input_ids)\n-        return config, inputs_dict\n-\n-    def check_decoder_model_past_large_inputs(self, config, inputs_dict):\n-        model = TFBlenderbotSmallModel(config=config).get_decoder()\n-        input_ids = inputs_dict[\"input_ids\"]\n-\n-        input_ids = input_ids[:1, :]\n-        attention_mask = inputs_dict[\"attention_mask\"][:1, :]\n-        head_mask = inputs_dict[\"head_mask\"]\n-        self.batch_size = 1\n-\n-        # first forward pass\n-        outputs = model(input_ids, attention_mask=attention_mask, head_mask=head_mask, use_cache=True)\n-\n-        output, past_key_values = outputs.to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_attn_mask = tf.cast(ids_tensor((self.batch_size, 3), 2), tf.int8)\n-\n-        # append to next input_ids and\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        next_attention_mask = tf.concat([attention_mask, next_attn_mask], axis=-1)\n-\n-        output_from_no_past = model(next_input_ids, attention_mask=next_attention_mask)[0]\n-        output_from_past = model(next_tokens, attention_mask=next_attention_mask, past_key_values=past_key_values)[0]\n-\n-        self.parent.assertEqual(next_tokens.shape[1], output_from_past.shape[1])\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), output_from_past.shape[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)\n-\n-\n-def prepare_blenderbot_small_inputs_dict(\n-    config,\n-    input_ids,\n-    decoder_input_ids,\n-    attention_mask=None,\n-    decoder_attention_mask=None,\n-    head_mask=None,\n-    decoder_head_mask=None,\n-    cross_attn_head_mask=None,\n-):\n-    if attention_mask is None:\n-        attention_mask = tf.cast(tf.math.not_equal(input_ids, config.pad_token_id), tf.int8)\n-    if decoder_attention_mask is None:\n-        decoder_attention_mask = tf.concat(\n-            [\n-                tf.ones(decoder_input_ids[:, :1].shape, dtype=tf.int8),\n-                tf.cast(tf.math.not_equal(decoder_input_ids[:, 1:], config.pad_token_id), tf.int8),\n-            ],\n-            axis=-1,\n-        )\n-    if head_mask is None:\n-        head_mask = tf.ones((config.encoder_layers, config.encoder_attention_heads))\n-    if decoder_head_mask is None:\n-        decoder_head_mask = tf.ones((config.decoder_layers, config.decoder_attention_heads))\n-    if cross_attn_head_mask is None:\n-        cross_attn_head_mask = tf.ones((config.decoder_layers, config.decoder_attention_heads))\n-    return {\n-        \"input_ids\": input_ids,\n-        \"decoder_input_ids\": decoder_input_ids,\n-        \"attention_mask\": attention_mask,\n-        \"decoder_attention_mask\": decoder_attention_mask,\n-        \"head_mask\": head_mask,\n-        \"decoder_head_mask\": decoder_head_mask,\n-        \"cross_attn_head_mask\": cross_attn_head_mask,\n-    }\n-\n-\n-@require_tf\n-class TFBlenderbotSmallModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (TFBlenderbotSmallForConditionalGeneration, TFBlenderbotSmallModel) if is_tf_available() else ()\n-    )\n-    all_generative_model_classes = (TFBlenderbotSmallForConditionalGeneration,) if is_tf_available() else ()\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFBlenderbotSmallModel,\n-            \"summarization\": TFBlenderbotSmallForConditionalGeneration,\n-            \"text2text-generation\": TFBlenderbotSmallForConditionalGeneration,\n-            \"translation\": TFBlenderbotSmallForConditionalGeneration,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    is_encoder_decoder = True\n-    test_pruning = False\n-    test_onnx = False\n-\n-    def is_pipeline_test_to_skip(\n-        self,\n-        pipeline_test_case_name,\n-        config_class,\n-        model_architecture,\n-        tokenizer_name,\n-        image_processor_name,\n-        feature_extractor_name,\n-        processor_name,\n-    ):\n-        return pipeline_test_case_name == \"TextGenerationPipelineTests\"\n-\n-    def setUp(self):\n-        self.model_tester = TFBlenderbotSmallModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=BlenderbotSmallConfig)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_decoder_model_past_large_inputs(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_common()\n-        self.model_tester.check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n-\n-@require_tokenizers\n-@require_tf\n-class TFBlenderbot90MIntegrationTests(unittest.TestCase):\n-    src_text = [\n-        \"Social anxiety\\nWow, I am never shy. Do you have anxiety?\\nYes. I end up sweating and blushing and feel like \"\n-        \"  i'm going to throw up.\\nand why is that?\"\n-    ]\n-    model_name = \"facebook/blenderbot_small-90M\"\n-\n-    @cached_property\n-    def tokenizer(self):\n-        # use \"old\" tokenizer here because of bug when downloading new tokenizer\n-        return BlenderbotSmallTokenizer.from_pretrained(\"facebook/blenderbot-90M\")\n-\n-    @cached_property\n-    def model(self):\n-        model = TFAutoModelForSeq2SeqLM.from_pretrained(self.model_name)\n-        return model\n-\n-    @slow\n-    def test_90_generation_from_long_input(self):\n-        model_inputs = self.tokenizer(self.src_text, return_tensors=\"tf\")\n-        generated_ids = self.model.generate(\n-            model_inputs.input_ids,\n-            attention_mask=model_inputs.attention_mask,\n-            num_beams=2,\n-            use_cache=True,\n-        )\n-        generated_words = self.tokenizer.batch_decode(generated_ids.numpy(), skip_special_tokens=True)[0]\n-        assert generated_words in (\n-            \"i don't know. i just feel like i'm going to throw up. it's not fun.\",\n-            \"i'm not sure. i just feel like i've been feeling like i have to be in a certain place\",\n-            \"i'm not sure. i just feel like i've been in a bad situation.\",\n-        )"
        },
        {
            "sha": "71269048a343fd2964b513205ff59d4d5545fc13",
            "filename": "tests/models/blip/test_modeling_tf_blip.py",
            "status": "removed",
            "additions": 0,
            "deletions": 878,
            "changes": 878,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,878 +0,0 @@\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Testing suite for the TensorFlow Blip model.\"\"\"\n-\n-from __future__ import annotations\n-\n-import inspect\n-import tempfile\n-import unittest\n-\n-import numpy as np\n-import requests\n-\n-from transformers import BlipConfig, BlipTextConfig, BlipVisionConfig\n-from transformers.testing_utils import require_tf, require_vision, slow\n-from transformers.utils import is_tf_available, is_vision_available\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import (\n-        TFBlipForConditionalGeneration,\n-        TFBlipForImageTextRetrieval,\n-        TFBlipForQuestionAnswering,\n-        TFBlipModel,\n-        TFBlipTextModel,\n-        TFBlipVisionModel,\n-    )\n-    from transformers.modeling_tf_utils import keras\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-    from transformers import BlipProcessor\n-\n-\n-class TFBlipVisionModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=12,\n-        image_size=30,\n-        patch_size=2,\n-        num_channels=3,\n-        is_training=True,\n-        hidden_size=32,\n-        projection_dim=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        dropout=0.1,\n-        attention_dropout=0.1,\n-        initializer_range=1e-10,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.image_size = image_size\n-        self.patch_size = patch_size\n-        self.num_channels = num_channels\n-        self.is_training = is_training\n-        self.hidden_size = hidden_size\n-        self.projection_dim = projection_dim\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.dropout = dropout\n-        self.attention_dropout = attention_dropout\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-\n-        # in ViT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n-        num_patches = (image_size // patch_size) ** 2\n-        self.seq_length = num_patches + 1\n-\n-    def prepare_config_and_inputs(self):\n-        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-        config = self.get_config()\n-\n-        return config, pixel_values\n-\n-    def get_config(self):\n-        return BlipVisionConfig(\n-            image_size=self.image_size,\n-            patch_size=self.patch_size,\n-            num_channels=self.num_channels,\n-            hidden_size=self.hidden_size,\n-            projection_dim=self.projection_dim,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            dropout=self.dropout,\n-            attention_dropout=self.attention_dropout,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-    def create_and_check_model(self, config, pixel_values):\n-        model = TFBlipVisionModel(config=config)\n-        result = model(pixel_values)\n-        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n-        image_size = (self.image_size, self.image_size)\n-        patch_size = (self.patch_size, self.patch_size)\n-        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFBlipVisionModelTest(TFModelTesterMixin, unittest.TestCase):\n-    \"\"\"\n-    Here we also overwrite some of the tests of test_modeling_common.py, as Blip does not use input_ids, inputs_embeds,\n-    attention_mask and seq_length.\n-    \"\"\"\n-\n-    all_model_classes = (TFBlipVisionModel,) if is_tf_available() else ()\n-    fx_compatible = False\n-    test_pruning = False\n-    test_resize_embeddings = False\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFBlipVisionModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=BlipVisionConfig, has_text_modality=False, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    @unittest.skip(reason=\"Blip does not use inputs_embeds\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.call)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"pixel_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    def test_model_common_attributes(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            self.assertIsInstance(model.get_input_embeddings(), (keras.layers.Layer))\n-            x = model.get_output_embeddings()\n-            self.assertTrue(x is None or isinstance(x, keras.layers.Layer))\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"Salesforce/blip-vqa-base\"\n-        model = TFBlipVisionModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-\n-class TFBlipTextModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=12,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        projection_dim=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        dropout=0.1,\n-        attention_dropout=0.1,\n-        max_position_embeddings=512,\n-        initializer_range=0.02,\n-        bos_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.projection_dim = projection_dim\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.dropout = dropout\n-        self.attention_dropout = attention_dropout\n-        self.max_position_embeddings = max_position_embeddings\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-        self.bos_token_id = bos_token_id\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        if input_mask is not None:\n-            input_mask = input_mask.numpy()\n-            batch_size, seq_length = input_mask.shape\n-            rnd_start_indices = np.random.randint(1, seq_length - 1, size=(batch_size,))\n-            for batch_idx, start_index in enumerate(rnd_start_indices):\n-                input_mask[batch_idx, :start_index] = 1\n-                input_mask[batch_idx, start_index:] = 0\n-            input_mask = tf.convert_to_tensor(input_mask)\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, input_mask\n-\n-    def get_config(self):\n-        return BlipTextConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            projection_dim=self.projection_dim,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            dropout=self.dropout,\n-            attention_dropout=self.attention_dropout,\n-            max_position_embeddings=self.max_position_embeddings,\n-            initializer_range=self.initializer_range,\n-            bos_token_id=self.bos_token_id,\n-        )\n-\n-    def create_and_check_model(self, config, input_ids, input_mask):\n-        model = TFBlipTextModel(config=config)\n-        result = model(input_ids, attention_mask=input_mask, training=False)\n-        result = model(input_ids, training=False)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, input_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFBlipTextModelTest(TFModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFBlipTextModel,) if is_tf_available() else ()\n-    fx_compatible = False\n-    test_pruning = False\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFBlipTextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=BlipTextConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    @unittest.skip(reason=\"Blip does not use inputs_embeds\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"Salesforce/blip-vqa-base\"\n-        model = TFBlipTextModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-\n-class TFBlipModelTester:\n-    def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n-        if text_kwargs is None:\n-            text_kwargs = {}\n-        if vision_kwargs is None:\n-            vision_kwargs = {}\n-\n-        self.parent = parent\n-        self.text_model_tester = TFBlipTextModelTester(parent, **text_kwargs)\n-        self.vision_model_tester = TFBlipVisionModelTester(parent, **vision_kwargs)\n-        self.is_training = is_training\n-\n-    def prepare_config_and_inputs(self):\n-        text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n-        vision_config, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, attention_mask, pixel_values\n-\n-    def get_config(self):\n-        return BlipConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n-        )\n-\n-    def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n-        model = TFBlipModel(config)\n-        result = model(input_ids, pixel_values, attention_mask, training=False)\n-        self.parent.assertEqual(\n-            result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)\n-        )\n-        self.parent.assertEqual(\n-            result.logits_per_text.shape, (self.text_model_tester.batch_size, self.vision_model_tester.batch_size)\n-        )\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask, pixel_values = config_and_inputs\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"pixel_values\": pixel_values,\n-            \"return_loss\": True,\n-        }\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFBlipModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFBlipModel,) if is_tf_available() else ()\n-    pipeline_model_mapping = (\n-        {\"feature-extraction\": TFBlipModel, \"image-to-text\": TFBlipForConditionalGeneration}\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_head_masking = False\n-    test_pruning = False\n-    test_resize_embeddings = False\n-    test_attention_outputs = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFBlipModelTester(self)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n-    def test_hidden_states_output(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Inputs_embeds is tested in individual model tests\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n-    def test_retain_grad_hidden_states_attentions(self):\n-        pass\n-\n-    @unittest.skip(reason=\"BlipModel does not have input/output embeddings\")\n-    def test_model_common_attributes(self):\n-        pass\n-\n-    def test_load_vision_text_config(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        # Save BlipConfig and check if we can load BlipVisionConfig from it\n-        with tempfile.TemporaryDirectory() as tmp_dir_name:\n-            config.save_pretrained(tmp_dir_name)\n-            vision_config = BlipVisionConfig.from_pretrained(tmp_dir_name)\n-            self.assertDictEqual(config.vision_config.to_dict(), vision_config.to_dict())\n-\n-        # Save BlipConfig and check if we can load BlipTextConfig from it\n-        with tempfile.TemporaryDirectory() as tmp_dir_name:\n-            config.save_pretrained(tmp_dir_name)\n-            text_config = BlipTextConfig.from_pretrained(tmp_dir_name)\n-            self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"Salesforce/blip-vqa-base\"\n-        model = TFBlipModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    @unittest.skip(\"Matt: Re-enable this test when we have a proper export function for TF models.\")\n-    def test_saved_model_creation(self):\n-        # This fails because the if return_loss: conditional can return None or a Tensor and TF hates that.\n-        # We could fix that by setting the bool to a constant when exporting, but that requires a dedicated export\n-        # function that we don't have yet.\n-        pass\n-\n-\n-class BlipTextRetrievalModelTester:\n-    def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n-        if text_kwargs is None:\n-            text_kwargs = {}\n-        if vision_kwargs is None:\n-            vision_kwargs = {}\n-\n-        self.parent = parent\n-        self.text_model_tester = TFBlipTextModelTester(parent, **text_kwargs)\n-        self.vision_model_tester = TFBlipVisionModelTester(parent, **vision_kwargs)\n-        self.is_training = is_training\n-\n-    def prepare_config_and_inputs(self):\n-        text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n-        vision_config, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, attention_mask, pixel_values\n-\n-    def get_config(self):\n-        return BlipConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n-        )\n-\n-    def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n-        model = TFBlipModel(config)\n-        result = model(input_ids, pixel_values, attention_mask, training=False)\n-        self.parent.assertEqual(\n-            result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)\n-        )\n-        self.parent.assertEqual(\n-            result.logits_per_text.shape, (self.text_model_tester.batch_size, self.vision_model_tester.batch_size)\n-        )\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask, pixel_values = config_and_inputs\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"pixel_values\": pixel_values,\n-        }\n-        return config, inputs_dict\n-\n-\n-class BlipTextImageModelsModelTester:\n-    def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n-        if text_kwargs is None:\n-            text_kwargs = {}\n-        if vision_kwargs is None:\n-            vision_kwargs = {}\n-\n-        self.parent = parent\n-        self.text_model_tester = TFBlipTextModelTester(parent, **text_kwargs)\n-        self.vision_model_tester = TFBlipVisionModelTester(parent, **vision_kwargs)\n-        self.is_training = is_training\n-\n-    def prepare_config_and_inputs(self):\n-        text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n-        vision_config, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, attention_mask, pixel_values\n-\n-    def get_config(self):\n-        return BlipConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n-        )\n-\n-    def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n-        model = TFBlipModel(config)\n-        result = model(input_ids, pixel_values, attention_mask, training=False)\n-        self.parent.assertEqual(\n-            result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)\n-        )\n-        self.parent.assertEqual(\n-            result.logits_per_text.shape, (self.text_model_tester.batch_size, self.vision_model_tester.batch_size)\n-        )\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask, pixel_values = config_and_inputs\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"labels\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"pixel_values\": pixel_values,\n-        }\n-        return config, inputs_dict\n-\n-\n-class BlipVQAModelsModelTester:\n-    def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n-        if text_kwargs is None:\n-            text_kwargs = {}\n-        if vision_kwargs is None:\n-            vision_kwargs = {}\n-\n-        self.parent = parent\n-        self.text_model_tester = TFBlipTextModelTester(parent, **text_kwargs)\n-        self.vision_model_tester = TFBlipVisionModelTester(parent, **vision_kwargs)\n-        self.is_training = is_training\n-\n-    def prepare_config_and_inputs(self):\n-        text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n-        vision_config, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, attention_mask, pixel_values\n-\n-    def get_config(self):\n-        return BlipConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n-        )\n-\n-    def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n-        model = TFBlipModel(config)\n-        result = model(input_ids, pixel_values, attention_mask, training=False)\n-        self.parent.assertEqual(\n-            result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)\n-        )\n-        self.parent.assertEqual(\n-            result.logits_per_text.shape, (self.text_model_tester.batch_size, self.vision_model_tester.batch_size)\n-        )\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask, pixel_values = config_and_inputs\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"decoder_input_ids\": input_ids,\n-            \"labels\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"pixel_values\": pixel_values,\n-        }\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-@require_vision\n-class TFBlipVQAModelTest(TFModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFBlipForQuestionAnswering,) if is_tf_available() else ()\n-    test_head_masking = False\n-    test_pruning = False\n-    test_resize_embeddings = False\n-    test_attention_outputs = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = BlipVQAModelsModelTester(self)\n-\n-    def _prepare_inputs_for_vqa(self):\n-        _, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        inputs_dict[\"labels\"] = inputs_dict[\"input_ids\"]\n-        inputs_dict[\"decoder_input_ids\"] = inputs_dict[\"input_ids\"]\n-        inputs_dict.pop(\"return_loss\")\n-        return inputs_dict\n-\n-    def test_class_name_consistency(self):\n-        \"\"\"\n-        Tests that all VQA models have a class name that ends with \"ForQuestionAnswering\"\n-        \"\"\"\n-        for model_class in self.all_model_classes:\n-            model = model_class(self.model_tester.get_config())\n-            self.assertTrue(\n-                model.__class__.__name__.endswith(\"ForQuestionAnswering\"),\n-                f\"Class name should end with 'ForVisualQuestionAnswering' got {model.__class__.__name__}\",\n-            )\n-\n-    def test_training(self):\n-        \"\"\"\n-        Tests that all VQA models can be trained on a single batch\n-        \"\"\"\n-        for model_class in self.all_model_classes:\n-            model = model_class(self.model_tester.get_config())\n-            loss = model(**self.model_tester.prepare_config_and_inputs_for_common()[1], training=True).loss\n-\n-            self.assertIsNotNone(loss, \"Loss should not be None\")\n-\n-    @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n-    def test_hidden_states_output(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Inputs_embeds is tested in individual model tests\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n-    def test_retain_grad_hidden_states_attentions(self):\n-        pass\n-\n-    @unittest.skip(reason=\"BlipModel does not have input/output embeddings\")\n-    def test_model_common_attributes(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Tested in individual model tests\")\n-    def test_compile_tf_model(self):\n-        pass\n-\n-    @unittest.skip(\"Model doesn't have a clean loss output.\")\n-    def test_keras_fit(self):\n-        pass\n-\n-\n-@require_tf\n-class TFBlipTextRetrievalModelTest(TFModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFBlipForImageTextRetrieval,) if is_tf_available() else ()\n-    test_head_masking = False\n-    test_pruning = False\n-    test_resize_embeddings = False\n-    test_attention_outputs = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = BlipTextRetrievalModelTester(self)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n-    def test_hidden_states_output(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Inputs_embeds is tested in individual model tests\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n-    def test_retain_grad_hidden_states_attentions(self):\n-        pass\n-\n-    @unittest.skip(reason=\"BlipModel does not have input/output embeddings\")\n-    def test_model_common_attributes(self):\n-        pass\n-\n-    def test_training(self):\n-        if not self.model_tester.is_training:\n-            return\n-\n-        for model_class in self.all_model_classes[:-1]:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            config.return_dict = True\n-\n-            model = model_class(config)\n-            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n-\n-            # hardcode labels to be the same as input_ids\n-            inputs[\"labels\"] = inputs[\"input_ids\"]\n-\n-            loss = model(**inputs, training=True).loss\n-            self.assertTrue(loss is not None)\n-\n-    def test_load_vision_text_config(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        # Save BlipConfig and check if we can load BlipVisionConfig from it\n-        with tempfile.TemporaryDirectory() as tmp_dir_name:\n-            config.save_pretrained(tmp_dir_name)\n-            vision_config = BlipVisionConfig.from_pretrained(tmp_dir_name)\n-            self.assertDictEqual(config.vision_config.to_dict(), vision_config.to_dict())\n-\n-        # Save BlipConfig and check if we can load BlipTextConfig from it\n-        with tempfile.TemporaryDirectory() as tmp_dir_name:\n-            config.save_pretrained(tmp_dir_name)\n-            text_config = BlipTextConfig.from_pretrained(tmp_dir_name)\n-            self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"Salesforce/blip-vqa-base\"\n-        model = TFBlipModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    @unittest.skip(reason=\"Tested in individual model tests\")\n-    def test_compile_tf_model(self):\n-        pass\n-\n-    @unittest.skip(\"Model doesn't have a clean loss output.\")\n-    def test_keras_fit(self):\n-        pass\n-\n-\n-@require_tf\n-class TFBlipTextImageModelTest(TFModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFBlipForConditionalGeneration,) if is_tf_available() else ()\n-    test_head_masking = False\n-    test_pruning = False\n-    test_resize_embeddings = False\n-    test_attention_outputs = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = BlipTextImageModelsModelTester(self)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n-    def test_hidden_states_output(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Inputs_embeds is tested in individual model tests\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.call)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            if model.config.is_encoder_decoder:\n-                expected_arg_names = [\n-                    \"input_ids\",\n-                    \"attention_mask\",\n-                    \"decoder_input_ids\",\n-                    \"decoder_attention_mask\",\n-                ]\n-                expected_arg_names.extend(\n-                    [\"head_mask\", \"decoder_head_mask\", \"cross_attn_head_mask\", \"encoder_outputs\"]\n-                    if \"head_mask\" and \"decoder_head_mask\" and \"cross_attn_head_mask\" in arg_names\n-                    else [\"encoder_outputs\"]\n-                )\n-                self.assertListEqual(arg_names[: len(expected_arg_names)], expected_arg_names)\n-            else:\n-                expected_arg_names = (\n-                    [\"input_ids\"] if model_class != TFBlipForConditionalGeneration else [\"pixel_values\"]\n-                )\n-                self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    @unittest.skip(reason=\"Tested in individual model tests\")\n-    def test_compile_tf_model(self):\n-        pass\n-\n-    @unittest.skip(\"Has some odd input names!\")\n-    def test_keras_fit(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n-    def test_retain_grad_hidden_states_attentions(self):\n-        pass\n-\n-    @unittest.skip(reason=\"BlipModel does not have input/output embeddings\")\n-    def test_model_common_attributes(self):\n-        pass\n-\n-    def test_training(self):\n-        if not self.model_tester.is_training:\n-            return\n-\n-        for model_class in self.all_model_classes[:-1]:\n-            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-            config.return_dict = True\n-\n-            model = model_class(config)\n-            inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n-\n-            # hardcode labels to be the same as input_ids\n-            inputs[\"labels\"] = inputs[\"input_ids\"]\n-\n-            loss = model(**inputs, training=True).loss\n-            self.assertIsNotNone(loss)\n-\n-    def test_load_vision_text_config(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        # Save BlipConfig and check if we can load BlipVisionConfig from it\n-        with tempfile.TemporaryDirectory() as tmp_dir_name:\n-            config.save_pretrained(tmp_dir_name)\n-            vision_config = BlipVisionConfig.from_pretrained(tmp_dir_name)\n-            self.assertDictEqual(config.vision_config.to_dict(), vision_config.to_dict())\n-\n-        # Save BlipConfig and check if we can load BlipTextConfig from it\n-        with tempfile.TemporaryDirectory() as tmp_dir_name:\n-            config.save_pretrained(tmp_dir_name)\n-            text_config = BlipTextConfig.from_pretrained(tmp_dir_name)\n-            self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"Salesforce/blip-vqa-base\"\n-        model = TFBlipModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    url = \"https://huggingface.co/hf-internal-testing/blip-test-image/resolve/main/demo.jpg\"\n-    im = Image.open(requests.get(url, stream=True).raw)\n-    return im\n-\n-\n-@require_vision\n-@require_tf\n-@slow\n-class TFBlipModelIntegrationTest(unittest.TestCase):\n-    def test_inference_image_captioning(self):\n-        model = TFBlipForConditionalGeneration.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n-        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-image-captioning-base\")\n-        image = prepare_img()\n-\n-        # image only\n-        inputs = processor(images=image, return_tensors=\"tf\")\n-\n-        predictions = model.generate(**inputs)\n-\n-        # Test output\n-        self.assertEqual(\n-            predictions[0].numpy().tolist(), [30522, 1037, 2450, 3564, 2006, 1996, 3509, 2007, 2014, 3899, 102]\n-        )\n-\n-        # image and context\n-        context = [\"a picture of\"]\n-        inputs = processor(images=image, text=context, return_tensors=\"tf\")\n-\n-        predictions = model.generate(**inputs)\n-\n-        # Test output\n-        self.assertEqual(\n-            predictions[0].numpy().tolist(),\n-            [30522, 1037, 3861, 1997, 1037, 2450, 1998, 2014, 3899, 2006, 1996, 3509, 102],\n-        )\n-\n-    def test_inference_vqa(self):\n-        model = TFBlipForQuestionAnswering.from_pretrained(\"Salesforce/blip-vqa-base\")\n-        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-vqa-base\")\n-\n-        image = prepare_img()\n-        text = \"how many dogs are in the picture?\"\n-        inputs = processor(image, text=text, return_tensors=\"tf\")\n-        out = model.generate(**inputs)\n-\n-        # Test output\n-        self.assertEqual(out[0].numpy().tolist(), [30522, 1015, 102])\n-\n-    def test_inference_itm(self):\n-        model = TFBlipForImageTextRetrieval.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n-        processor = BlipProcessor.from_pretrained(\"Salesforce/blip-itm-base-coco\")\n-\n-        image = prepare_img()\n-        text = \"A woman and her dog sitting in a beach\"\n-\n-        inputs = processor(image, text, return_tensors=\"tf\")\n-\n-        out_itm = model(**inputs)\n-        out = model(**inputs, use_itm_head=False, training=False)\n-\n-        expected_scores = tf.convert_to_tensor([[0.0029, 0.9971]])\n-        self.assertTrue(np.allclose(tf.nn.softmax(out_itm[0]).numpy(), expected_scores, rtol=1e-3, atol=1e-3))\n-        self.assertTrue(np.allclose(out[0], tf.convert_to_tensor([[0.5162]]), rtol=1e-3, atol=1e-3))"
        },
        {
            "sha": "757ce98eed8ce47d6b76c11f16a326f592b83050",
            "filename": "tests/models/blip/test_modeling_tf_blip_text.py",
            "status": "removed",
            "additions": 0,
            "deletions": 169,
            "changes": 169,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_tf_blip_text.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,169 +0,0 @@\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Testing suite for the TensorFlow Blip model.\"\"\"\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import BlipTextConfig\n-from transformers.testing_utils import require_tf, slow\n-from transformers.utils import is_tf_available\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import TFBlipTextModel\n-\n-\n-class BlipTextModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=12,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        projection_dim=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        dropout=0.1,\n-        attention_dropout=0.1,\n-        max_position_embeddings=512,\n-        initializer_range=0.02,\n-        bos_token_id=0,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.projection_dim = projection_dim\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.dropout = dropout\n-        self.attention_dropout = attention_dropout\n-        self.max_position_embeddings = max_position_embeddings\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-        self.bos_token_id = bos_token_id\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        if input_mask is not None:\n-            input_mask = input_mask.numpy()\n-            batch_size, seq_length = input_mask.shape\n-            rnd_start_indices = np.random.randint(1, seq_length - 1, size=(batch_size,))\n-            for batch_idx, start_index in enumerate(rnd_start_indices):\n-                input_mask[batch_idx, :start_index] = 1\n-                input_mask[batch_idx, start_index:] = 0\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, tf.convert_to_tensor(input_mask)\n-\n-    def get_config(self):\n-        return BlipTextConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            projection_dim=self.projection_dim,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            dropout=self.dropout,\n-            attention_dropout=self.attention_dropout,\n-            max_position_embeddings=self.max_position_embeddings,\n-            initializer_range=self.initializer_range,\n-            bos_token_id=self.bos_token_id,\n-        )\n-\n-    def create_and_check_model(self, config, input_ids, input_mask):\n-        model = TFBlipTextModel(config=config)\n-        result = model(input_ids, attention_mask=input_mask, training=False)\n-        result = model(input_ids, training=False)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, input_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class BlipTextModelTest(TFModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFBlipTextModel,) if is_tf_available() else ()\n-    test_onnx = False\n-    test_pruning = False\n-    test_head_masking = False\n-\n-    def setUp(self):\n-        self.model_tester = BlipTextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=BlipTextConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_training(self):\n-        pass\n-\n-    def test_training_gradient_checkpointing(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing_use_reentrant(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing_use_reentrant_false(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Blip does not use inputs_embeds\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"Salesforce/blip-vqa-base\"\n-        model = TFBlipTextModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)"
        },
        {
            "sha": "77b30fe19e0dc930b5fd67f31bf212ffc042e5b9",
            "filename": "tests/models/bloom/test_modeling_flax_bloom.py",
            "status": "removed",
            "additions": 0,
            "deletions": 248,
            "changes": 248,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbloom%2Ftest_modeling_flax_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fbloom%2Ftest_modeling_flax_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbloom%2Ftest_modeling_flax_bloom.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,248 +0,0 @@\n-# Copyright 2023 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import BloomConfig, BloomTokenizerFast, is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor\n-\n-\n-if is_flax_available():\n-    import os\n-\n-    # The slow tests are often failing with OOM error on GPU\n-    # This makes JAX allocate exactly what is needed on demand, and deallocate memory that is no longer needed\n-    # but will be slower as stated here https://jax.readthedocs.io/en/latest/gpu_memory_allocation.html\n-    os.environ[\"XLA_PYTHON_CLIENT_ALLOCATOR\"] = \"platform\"\n-\n-    import jax.numpy as jnp\n-\n-    from transformers import FlaxBloomForCausalLM, FlaxBloomModel\n-\n-\n-def prepare_bloom_inputs_dict(config, input_ids, attention_mask=None):\n-    if attention_mask is None:\n-        attention_mask = np.where(input_ids != config.pad_token_id, 1, 0)\n-    return {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-\n-\n-@require_flax\n-class FlaxBloomModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_labels=False,\n-        vocab_size=99,\n-        hidden_size=16,\n-        n_layer=2,\n-        n_head=4,\n-        hidden_act=\"gelu\",\n-        hidden_dropout=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        eos_token_id=2,\n-        pad_token_id=1,\n-        bos_token_id=0,\n-        initializer_range=0.02,\n-        apply_residual_connection_post_layernorm=False,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = n_layer\n-        self.num_attention_heads = n_head\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout = hidden_dropout\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n-        self.bos_token_id = bos_token_id\n-        self.initializer_range = initializer_range\n-        self.is_encoder_decoder = False\n-        self.apply_residual_connection_post_layernorm = apply_residual_connection_post_layernorm\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = np.clip(ids_tensor([self.batch_size, self.seq_length - 1], self.vocab_size), 3, self.vocab_size)\n-        input_ids = np.concatenate((input_ids, 2 * np.ones((self.batch_size, 1), dtype=np.int64)), -1)\n-\n-        config = BloomConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            n_layer=self.num_hidden_layers,\n-            n_head=self.num_attention_heads,\n-            hidden_dropout=self.hidden_dropout,\n-            attention_dropout=self.attention_probs_dropout_prob,\n-            eos_token_id=self.eos_token_id,\n-            bos_token_id=self.bos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            is_encoder_decoder=False,\n-            use_cache=False,\n-        )\n-        inputs_dict = prepare_bloom_inputs_dict(config, input_ids)\n-        return config, inputs_dict\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config, inputs_dict = self.prepare_config_and_inputs()\n-        return config, inputs_dict\n-\n-    def check_use_cache_forward(self, model_class_name, config, inputs_dict):\n-        max_length = 20\n-        model = model_class_name(config)\n-\n-        input_ids = inputs_dict[\"input_ids\"]\n-        attention_mask = jnp.ones((input_ids.shape[0], max_length), dtype=\"i4\")\n-\n-        past_key_values = model.init_cache(input_ids.shape[0], max_length)\n-\n-        outputs_cache = model(\n-            input_ids[:, :-1],\n-            attention_mask=attention_mask,\n-            past_key_values=past_key_values,\n-        )\n-\n-        outputs_cache_next = model(\n-            input_ids[:, -1:],\n-            attention_mask=attention_mask,\n-            past_key_values=outputs_cache.past_key_values,\n-        )\n-\n-        outputs = model(input_ids)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-    def check_use_cache_forward_with_attn_mask(self, model_class_name, config, inputs_dict):\n-        max_length = 20\n-        model = model_class_name(config)\n-\n-        input_ids, attention_mask = (\n-            inputs_dict[\"input_ids\"],\n-            inputs_dict[\"attention_mask\"],\n-        )\n-\n-        attention_mask_cache = jnp.concatenate(\n-            [\n-                attention_mask,\n-                jnp.zeros((attention_mask.shape[0], max_length - attention_mask.shape[1])),\n-            ],\n-            axis=-1,\n-        )\n-\n-        past_key_values = model.init_cache(input_ids.shape[0], max_length)\n-\n-        outputs_cache = model(\n-            input_ids[:, :-1],\n-            attention_mask=attention_mask_cache,\n-            past_key_values=past_key_values,\n-        )\n-        outputs_cache_next = model(\n-            input_ids[:, -1:],\n-            past_key_values=outputs_cache.past_key_values,\n-            attention_mask=attention_mask_cache,\n-        )\n-\n-        outputs = model(input_ids, attention_mask=attention_mask)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-\n-@require_flax\n-class FlaxBloomModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (FlaxBloomModel, FlaxBloomForCausalLM) if is_flax_available() else ()\n-\n-    def setUp(self):\n-        self.model_tester = FlaxBloomModelTester(self)\n-\n-    def test_use_cache_forward(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n-        for model_class in self.all_model_classes:\n-            self.model_tester.check_use_cache_forward(model_class, config, inputs_dict)\n-\n-    def test_use_cache_forward_with_attn_mask(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs()\n-        for model_class in self.all_model_classes:\n-            self.model_tester.check_use_cache_forward_with_attn_mask(model_class, config, inputs_dict)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"bigscience/bloom-560m\")\n-            input_ids = np.ones((1, 1)) * model.config.eos_token_id\n-            outputs = model(input_ids)\n-            self.assertIsNotNone(outputs)\n-\n-\n-@slow\n-@require_flax\n-class FlaxBloomGenerationTest(unittest.TestCase):\n-    all_model_classes = (FlaxBloomForCausalLM,) if is_flax_available() else ()\n-\n-    def setUp(self):\n-        self.model_id = \"bigscience/bloom-560m\"\n-        self.tokenizer = BloomTokenizerFast.from_pretrained(self.model_id, padding_side=\"left\")\n-        self.model_tester = FlaxBloomModelTester(self)\n-        self.model = FlaxBloomForCausalLM.from_pretrained(self.model_id, from_pt=True, revision=\"gs555750\")\n-\n-    def test_model_batched_gen(self):\n-        # tests if the model outputs the same generation for the same batched input\n-        input_sentences = [\n-            \"Hello there is this string is definitely longer I believe that\",\n-            \"Hello there is this string is definitely longer I believe that\",\n-        ]\n-        inputs = self.tokenizer(input_sentences, return_tensors=\"np\", padding=True, truncation=True)\n-        sequences_fx = self.model.generate(**inputs, max_length=20).sequences\n-        self.assertEqual(sequences_fx[0].tolist(), sequences_fx[1].tolist())\n-\n-    def test_model_batched_padding_left(self):\n-        # tests if the model outputs the same generation for an input that is part of a batch\n-        # and a single input\n-        input_sentences_batch = [\n-            \"Hello there is this string is definitely longer I believe that\",\n-            \"Hi I want to order\",\n-        ]\n-        inputs = self.tokenizer(input_sentences_batch, return_tensors=\"np\", padding=True, truncation=True)\n-        sequences_fx_batch = self.model.generate(**inputs, max_length=20).sequences\n-\n-        input_sentence_simple = \"Hi I want to order\"\n-        inputs_simple = self.tokenizer(input_sentence_simple, return_tensors=\"np\")\n-        sequences_fx_simple = self.model.generate(**inputs_simple, max_length=20).sequences\n-\n-        self.assertEqual(sequences_fx_batch[1][6:].tolist(), sequences_fx_simple[0][:-6].tolist())\n-\n-    def test_batch_generated_text(self):\n-        input_sentences = [\n-            \"Hello what is\",\n-            \"Running a quick test with the\",\n-        ]\n-        inputs = self.tokenizer(input_sentences, return_tensors=\"np\", padding=True, truncation=True)\n-        generated_ids = self.model.generate(**inputs, max_length=20).sequences\n-        generated_text = self.tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n-\n-        # these generations match those of the PyTorch model, ensuring correctness\n-        EXPECTED_GENERATIONS = [\n-            \"Hello what is the best way to get the data from the server? I have tried\",\n-            \"Running a quick test with the following command:\\nsudo apt-get install python3\\nsudo apt-get install python2\",\n-        ]\n-\n-        self.assertListEqual(generated_text, EXPECTED_GENERATIONS)"
        },
        {
            "sha": "f9f8ba61d0e58bef3147b0935b7b365a8ad38963",
            "filename": "tests/models/camembert/test_modeling_tf_camembert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 55,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fcamembert%2Ftest_modeling_tf_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fcamembert%2Ftest_modeling_tf_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcamembert%2Ftest_modeling_tf_camembert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,55 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import is_tf_available\n-from transformers.testing_utils import require_sentencepiece, require_tf, require_tokenizers, slow\n-\n-\n-if is_tf_available():\n-    import numpy as np\n-    import tensorflow as tf\n-\n-    from transformers import TFCamembertModel\n-\n-\n-@require_tf\n-@require_sentencepiece\n-@require_tokenizers\n-class TFCamembertModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_output_embeds_base_model(self):\n-        model = TFCamembertModel.from_pretrained(\"jplu/tf-camembert-base\")\n-\n-        input_ids = tf.convert_to_tensor(\n-            [[5, 121, 11, 660, 16, 730, 25543, 110, 83, 6]],\n-            dtype=tf.int32,\n-        )  # J'aime le camembert !\"\n-\n-        output = model(input_ids)[\"last_hidden_state\"]\n-        expected_shape = tf.TensorShape((1, 10, 768))\n-        self.assertEqual(output.shape, expected_shape)\n-        # compare the actual values for a slice.\n-        expected_slice = tf.convert_to_tensor(\n-            [[[-0.0254, 0.0235, 0.1027], [0.0606, -0.1811, -0.0418], [-0.1561, -0.1127, 0.2687]]],\n-            dtype=tf.float32,\n-        )\n-        # camembert = torch.hub.load('pytorch/fairseq', 'camembert.v0')\n-        # camembert.eval()\n-        # expected_slice = roberta.model.forward(input_ids)[0][:, :3, :3].detach()\n-\n-        self.assertTrue(np.allclose(output[:, :3, :3].numpy(), expected_slice.numpy(), atol=1e-4))"
        },
        {
            "sha": "d499f4bf7dcb06dbbcb998cb59dd0da7496da60a",
            "filename": "tests/models/clip/test_modeling_flax_clip.py",
            "status": "removed",
            "additions": 0,
            "deletions": 468,
            "changes": 468,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fclip%2Ftest_modeling_flax_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fclip%2Ftest_modeling_flax_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_flax_clip.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,468 +0,0 @@\n-import inspect\n-import tempfile\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import CLIPConfig, CLIPTextConfig, CLIPVisionConfig, is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-\n-\n-if is_flax_available():\n-    import jax\n-\n-    from transformers.models.clip.modeling_flax_clip import (\n-        FlaxCLIPModel,\n-        FlaxCLIPTextModel,\n-        FlaxCLIPTextModelWithProjection,\n-        FlaxCLIPVisionModel,\n-    )\n-\n-\n-class FlaxCLIPVisionModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=12,\n-        image_size=30,\n-        patch_size=2,\n-        num_channels=3,\n-        is_training=True,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        dropout=0.1,\n-        attention_dropout=0.1,\n-        initializer_range=0.02,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.image_size = image_size\n-        self.patch_size = patch_size\n-        self.num_channels = num_channels\n-        self.is_training = is_training\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.dropout = dropout\n-        self.attention_dropout = attention_dropout\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-        config = CLIPVisionConfig(\n-            image_size=self.image_size,\n-            patch_size=self.patch_size,\n-            num_channels=self.num_channels,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            dropout=self.dropout,\n-            attention_dropout=self.attention_dropout,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, pixel_values\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n-        return config, inputs_dict\n-\n-\n-@require_flax\n-class FlaxCLIPVisionModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    \"\"\"\n-    Here we also overwrite some of the tests of test_modeling_common.py, as CLIP does not use input_ids, inputs_embeds,\n-    attention_mask and seq_length.\n-    \"\"\"\n-\n-    all_model_classes = (FlaxCLIPVisionModel,) if is_flax_available() else ()\n-\n-    def setUp(self):\n-        self.model_tester = FlaxCLIPVisionModelTester(self)\n-\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.__call__)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"pixel_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    def test_jit_compilation(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                model = model_class(config)\n-\n-                @jax.jit\n-                def model_jitted(pixel_values, **kwargs):\n-                    return model(pixel_values=pixel_values, **kwargs).to_tuple()\n-\n-                with self.subTest(\"JIT Enabled\"):\n-                    jitted_outputs = model_jitted(**prepared_inputs_dict)\n-\n-                with self.subTest(\"JIT Disabled\"):\n-                    with jax.disable_jit():\n-                        outputs = model_jitted(**prepared_inputs_dict)\n-\n-                self.assertEqual(len(outputs), len(jitted_outputs))\n-                for jitted_output, output in zip(jitted_outputs, outputs):\n-                    self.assertEqual(jitted_output.shape, output.shape)\n-\n-    def test_hidden_states_output(self):\n-        def check_hidden_states_output(inputs_dict, config, model_class):\n-            model = model_class(config)\n-\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-            hidden_states = outputs.hidden_states\n-\n-            self.assertEqual(len(hidden_states), self.model_tester.num_hidden_layers + 1)\n-\n-            # CLIP has a different seq_length\n-            image_size = (self.model_tester.image_size, self.model_tester.image_size)\n-            patch_size = (self.model_tester.patch_size, self.model_tester.patch_size)\n-            num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-            seq_length = num_patches + 1\n-\n-            self.assertListEqual(\n-                list(hidden_states[0].shape[-2:]),\n-                [seq_length, self.model_tester.hidden_size],\n-            )\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_hidden_states\"] = True\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-            # check that output_hidden_states also work using config\n-            del inputs_dict[\"output_hidden_states\"]\n-            config.output_hidden_states = True\n-\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-    def test_attention_outputs(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.return_dict = True\n-\n-        # in CLIP, the seq_len equals the number of patches + 1 (we add 1 for the [CLS] token)\n-        image_size = (self.model_tester.image_size, self.model_tester.image_size)\n-        patch_size = (self.model_tester.patch_size, self.model_tester.patch_size)\n-        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-        seq_length = num_patches + 1\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_attentions\"] = True\n-            inputs_dict[\"output_hidden_states\"] = False\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-            attentions = outputs.attentions\n-            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n-\n-            # check that output_attentions also work using config\n-            del inputs_dict[\"output_attentions\"]\n-            config.output_attentions = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-            attentions = outputs.attentions\n-            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n-\n-            self.assertListEqual(\n-                list(attentions[0].shape[-3:]),\n-                [self.model_tester.num_attention_heads, seq_length, seq_length],\n-            )\n-            out_len = len(outputs)\n-\n-            # Check attention is always last and order is fine\n-            inputs_dict[\"output_attentions\"] = True\n-            inputs_dict[\"output_hidden_states\"] = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-\n-            added_hidden_states = 1\n-            self.assertEqual(out_len + added_hidden_states, len(outputs))\n-\n-            self_attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n-            self.assertEqual(len(self_attentions), self.model_tester.num_hidden_layers)\n-\n-            self.assertListEqual(\n-                list(self_attentions[0].shape[-3:]),\n-                [self.model_tester.num_attention_heads, seq_length, seq_length],\n-            )\n-\n-    # FlaxCLIPVisionModel does not have any base model\n-    def test_save_load_from_base(self):\n-        pass\n-\n-    # FlaxCLIPVisionModel does not have any base model\n-    def test_save_load_to_base(self):\n-        pass\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"openai/clip-vit-base-patch32\", from_pt=True)\n-            outputs = model(np.ones((1, 3, 224, 224)))\n-            self.assertIsNotNone(outputs)\n-\n-\n-class FlaxCLIPTextModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=12,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        dropout=0.1,\n-        attention_dropout=0.1,\n-        max_position_embeddings=512,\n-        initializer_range=0.02,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.dropout = dropout\n-        self.attention_dropout = attention_dropout\n-        self.max_position_embeddings = max_position_embeddings\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        if input_mask is not None:\n-            batch_size, seq_length = input_mask.shape\n-            rnd_start_indices = np.random.randint(1, seq_length - 1, size=(batch_size,))\n-            for batch_idx, start_index in enumerate(rnd_start_indices):\n-                input_mask[batch_idx, :start_index] = 1\n-                input_mask[batch_idx, start_index:] = 0\n-\n-        config = CLIPTextConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            dropout=self.dropout,\n-            attention_dropout=self.attention_dropout,\n-            max_position_embeddings=self.max_position_embeddings,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, input_ids, input_mask\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, input_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_flax\n-class FlaxCLIPTextModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (FlaxCLIPTextModel, FlaxCLIPTextModelWithProjection) if is_flax_available() else ()\n-\n-    def setUp(self):\n-        self.model_tester = FlaxCLIPTextModelTester(self)\n-\n-    # FlaxCLIPTextModel does not have any base model\n-    def test_save_load_from_base(self):\n-        pass\n-\n-    # FlaxCLIPVisionModel does not have any base model\n-    def test_save_load_to_base(self):\n-        pass\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"openai/clip-vit-base-patch32\", from_pt=True)\n-            outputs = model(np.ones((1, 1)))\n-            self.assertIsNotNone(outputs)\n-\n-\n-class FlaxCLIPModelTester:\n-    def __init__(self, parent, is_training=True):\n-        self.parent = parent\n-        self.text_model_tester = FlaxCLIPTextModelTester(parent)\n-        self.vision_model_tester = FlaxCLIPVisionModelTester(parent)\n-        self.is_training = is_training\n-\n-    def prepare_config_and_inputs(self):\n-        text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n-        vision_config, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n-\n-        config = CLIPConfig.from_text_vision_configs(text_config, vision_config, projection_dim=64)\n-\n-        return config, input_ids, attention_mask, pixel_values\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask, pixel_values = config_and_inputs\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"pixel_values\": pixel_values,\n-        }\n-        return config, inputs_dict\n-\n-\n-@require_flax\n-class FlaxCLIPModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (FlaxCLIPModel,) if is_flax_available() else ()\n-    test_attention_outputs = False\n-\n-    def setUp(self):\n-        self.model_tester = FlaxCLIPModelTester(self)\n-\n-    # hidden_states are tested in individual model tests\n-    def test_hidden_states_output(self):\n-        pass\n-\n-    def test_jit_compilation(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                model = model_class(config)\n-\n-                @jax.jit\n-                def model_jitted(input_ids, pixel_values, **kwargs):\n-                    return model(input_ids=input_ids, pixel_values=pixel_values, **kwargs).to_tuple()\n-\n-                with self.subTest(\"JIT Enabled\"):\n-                    jitted_outputs = model_jitted(**prepared_inputs_dict)\n-\n-                with self.subTest(\"JIT Disabled\"):\n-                    with jax.disable_jit():\n-                        outputs = model_jitted(**prepared_inputs_dict)\n-\n-                self.assertEqual(len(outputs), len(jitted_outputs))\n-                for jitted_output, output in zip(jitted_outputs[:4], outputs[:4]):\n-                    self.assertEqual(jitted_output.shape, output.shape)\n-\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.__call__)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"input_ids\", \"pixel_values\", \"attention_mask\", \"position_ids\"]\n-            self.assertListEqual(arg_names[:4], expected_arg_names)\n-\n-    def test_get_image_features(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        model = FlaxCLIPModel(config)\n-\n-        @jax.jit\n-        def model_jitted(pixel_values):\n-            return model.get_image_features(pixel_values=pixel_values)\n-\n-        with self.subTest(\"JIT Enabled\"):\n-            jitted_output = model_jitted(inputs_dict[\"pixel_values\"])\n-\n-        with self.subTest(\"JIT Disabled\"):\n-            with jax.disable_jit():\n-                output = model_jitted(inputs_dict[\"pixel_values\"])\n-\n-        self.assertEqual(jitted_output.shape, output.shape)\n-        self.assertTrue(np.allclose(jitted_output, output, atol=1e-3))\n-\n-    def test_get_text_features(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        model = FlaxCLIPModel(config)\n-\n-        @jax.jit\n-        def model_jitted(input_ids, attention_mask, **kwargs):\n-            return model.get_text_features(input_ids=input_ids, attention_mask=attention_mask)\n-\n-        with self.subTest(\"JIT Enabled\"):\n-            jitted_output = model_jitted(**inputs_dict)\n-\n-        with self.subTest(\"JIT Disabled\"):\n-            with jax.disable_jit():\n-                output = model_jitted(**inputs_dict)\n-\n-        self.assertEqual(jitted_output.shape, output.shape)\n-        self.assertTrue(np.allclose(jitted_output, output, atol=1e-3))\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"openai/clip-vit-base-patch32\", from_pt=True)\n-            outputs = model(input_ids=np.ones((1, 1)), pixel_values=np.ones((1, 3, 224, 224)))\n-            self.assertIsNotNone(outputs)\n-\n-    # overwrite from common since FlaxCLIPModel returns nested output\n-    # which is not supported in the common test\n-    def test_from_pretrained_save_pretrained(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            if model_class.__name__ != \"FlaxBertModel\":\n-                continue\n-\n-            with self.subTest(model_class.__name__):\n-                model = model_class(config)\n-\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                outputs = model(**prepared_inputs_dict).to_tuple()\n-\n-                # verify that normal save_pretrained works as expected\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    model.save_pretrained(tmpdirname)\n-                    model_loaded = model_class.from_pretrained(tmpdirname)\n-\n-                outputs_loaded = model_loaded(**prepared_inputs_dict).to_tuple()[:4]\n-                for output_loaded, output in zip(outputs_loaded, outputs):\n-                    self.assert_almost_equals(output_loaded, output, 1e-3)\n-\n-                # verify that save_pretrained for distributed training\n-                # with `params=params` works as expected\n-                with tempfile.TemporaryDirectory() as tmpdirname:\n-                    model.save_pretrained(tmpdirname, params=model.params)\n-                    model_loaded = model_class.from_pretrained(tmpdirname)\n-\n-                outputs_loaded = model_loaded(**prepared_inputs_dict).to_tuple()[:4]\n-                for output_loaded, output in zip(outputs_loaded, outputs):\n-                    self.assert_almost_equals(output_loaded, output, 1e-3)"
        },
        {
            "sha": "27db72e3970657ae1adbe7371f2a1b1788334d67",
            "filename": "tests/models/clip/test_modeling_tf_clip.py",
            "status": "removed",
            "additions": 0,
            "deletions": 662,
            "changes": 662,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fclip%2Ftest_modeling_tf_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fclip%2Ftest_modeling_tf_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_tf_clip.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,662 +0,0 @@\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Testing suite for the TensorFlow CLIP model.\"\"\"\n-\n-from __future__ import annotations\n-\n-import inspect\n-import os\n-import tempfile\n-import unittest\n-from importlib import import_module\n-\n-import requests\n-\n-from transformers import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n-from transformers.testing_utils import require_tf, require_vision, slow\n-from transformers.utils import is_tf_available, is_vision_available\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import TFCLIPModel, TFCLIPTextModel, TFCLIPVisionModel, TFSharedEmbeddings\n-    from transformers.modeling_tf_utils import keras\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-    from transformers import CLIPProcessor\n-\n-\n-class TFCLIPVisionModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=12,\n-        image_size=30,\n-        patch_size=2,\n-        num_channels=3,\n-        is_training=True,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        dropout=0.1,\n-        attention_dropout=0.1,\n-        initializer_range=0.02,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.image_size = image_size\n-        self.patch_size = patch_size\n-        self.num_channels = num_channels\n-        self.is_training = is_training\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.dropout = dropout\n-        self.attention_dropout = attention_dropout\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-        config = self.get_config()\n-\n-        return config, pixel_values\n-\n-    def get_config(self):\n-        return CLIPVisionConfig(\n-            image_size=self.image_size,\n-            patch_size=self.patch_size,\n-            num_channels=self.num_channels,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            dropout=self.dropout,\n-            attention_dropout=self.attention_dropout,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-    def create_and_check_model(self, config, pixel_values):\n-        model = TFCLIPVisionModel(config=config)\n-        result = model(pixel_values, training=False)\n-        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n-        image_size = (self.image_size, self.image_size)\n-        patch_size = (self.patch_size, self.patch_size)\n-        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFCLIPVisionModelTest(TFModelTesterMixin, unittest.TestCase):\n-    \"\"\"\n-    Here we also overwrite some of the tests of test_modeling_common.py, as CLIP does not use input_ids, inputs_embeds,\n-    attention_mask and seq_length.\n-    \"\"\"\n-\n-    all_model_classes = (TFCLIPVisionModel,) if is_tf_available() else ()\n-\n-    test_pruning = False\n-    test_resize_embeddings = False\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFCLIPVisionModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=CLIPVisionConfig, has_text_modality=False, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_inputs_embeds(self):\n-        # CLIP does not use inputs_embeds\n-        pass\n-\n-    def test_graph_mode_with_inputs_embeds(self):\n-        # CLIP does not use inputs_embeds\n-        pass\n-\n-    def test_model_common_attributes(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            self.assertIsInstance(model.get_input_embeddings(), (keras.layers.Layer))\n-            x = model.get_output_embeddings()\n-            self.assertTrue(x is None or isinstance(x, keras.layers.Layer))\n-\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.call)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"pixel_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_attention_outputs(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.return_dict = True\n-\n-        # in CLIP, the seq_len equals the number of patches + 1 (we add 1 for the [CLS] token)\n-        image_size = (self.model_tester.image_size, self.model_tester.image_size)\n-        patch_size = (self.model_tester.patch_size, self.model_tester.patch_size)\n-        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-        seq_len = num_patches + 1\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_attentions\"] = True\n-            inputs_dict[\"output_hidden_states\"] = False\n-            config.return_dict = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class), training=False)\n-            attentions = outputs.attentions\n-            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n-\n-            # check that output_attentions also work using config\n-            del inputs_dict[\"output_attentions\"]\n-            config.output_attentions = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class), training=False)\n-            attentions = outputs.attentions\n-            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n-\n-            out_len = len(outputs)\n-\n-            # Check attention is always last and order is fine\n-            inputs_dict[\"output_attentions\"] = True\n-            inputs_dict[\"output_hidden_states\"] = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class), training=False)\n-\n-            added_hidden_states = 1\n-            self.assertEqual(out_len + added_hidden_states, len(outputs))\n-\n-            self_attentions = outputs.attentions\n-\n-            self.assertEqual(len(self_attentions), self.model_tester.num_hidden_layers)\n-\n-            self.assertListEqual(\n-                list(self_attentions[0].shape[-3:]),\n-                [self.model_tester.num_attention_heads, seq_len, seq_len],\n-            )\n-\n-    def test_hidden_states_output(self):\n-        def check_hidden_states_output(inputs_dict, config, model_class):\n-            model = model_class(config)\n-\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class), training=False)\n-\n-            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n-\n-            expected_num_layers = getattr(\n-                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n-            )\n-            self.assertEqual(len(hidden_states), expected_num_layers)\n-\n-            # CLIP has a different seq_length\n-            image_size = (self.model_tester.image_size, self.model_tester.image_size)\n-            patch_size = (self.model_tester.patch_size, self.model_tester.patch_size)\n-            num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-            seq_length = num_patches + 1\n-\n-            self.assertListEqual(\n-                list(hidden_states[0].shape[-2:]),\n-                [seq_length, self.model_tester.hidden_size],\n-            )\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_hidden_states\"] = True\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-            # check that output_hidden_states also work using config\n-            del inputs_dict[\"output_hidden_states\"]\n-            config.output_hidden_states = True\n-\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"openai/clip-vit-base-patch32\"\n-        model = TFCLIPVisionModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    @slow\n-    def test_saved_model_creation_extended(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.output_hidden_states = True\n-        config.output_attentions = True\n-\n-        if hasattr(config, \"use_cache\"):\n-            config.use_cache = True\n-\n-        # in CLIP, the seq_len equals the number of patches + 1 (we add 1 for the [CLS] token)\n-        image_size = (self.model_tester.image_size, self.model_tester.image_size)\n-        patch_size = (self.model_tester.patch_size, self.model_tester.patch_size)\n-        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-        seq_len = num_patches + 1\n-\n-        for model_class in self.all_model_classes:\n-            class_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-            model = model_class(config)\n-            num_out = len(model(class_inputs_dict))\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname, saved_model=True)\n-                saved_model_dir = os.path.join(tmpdirname, \"saved_model\", \"1\")\n-                model = keras.models.load_model(saved_model_dir)\n-                outputs = model(class_inputs_dict)\n-                output_hidden_states = outputs[\"hidden_states\"]\n-                output_attentions = outputs[\"attentions\"]\n-\n-                # Check num outputs\n-                self.assertEqual(len(outputs), num_out)\n-\n-                # Check num layers\n-                expected_num_layers = getattr(\n-                    self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n-                )\n-\n-                self.assertEqual(len(output_hidden_states), expected_num_layers)\n-                self.assertEqual(len(output_attentions), self.model_tester.num_hidden_layers)\n-\n-                # Check attention outputs\n-                image_size = (self.model_tester.image_size, self.model_tester.image_size)\n-                patch_size = (self.model_tester.patch_size, self.model_tester.patch_size)\n-                num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-                seq_len = num_patches + 1\n-\n-                self.assertListEqual(\n-                    list(output_attentions[0].shape[-3:]),\n-                    [self.model_tester.num_attention_heads, seq_len, seq_len],\n-                )\n-\n-                # Check hidden states\n-                self.assertListEqual(\n-                    list(output_hidden_states[0].shape[-2:]),\n-                    [seq_len, self.model_tester.hidden_size],\n-                )\n-\n-\n-class TFCLIPTextModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=12,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        dropout=0.1,\n-        attention_dropout=0.1,\n-        max_position_embeddings=512,\n-        initializer_range=0.02,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.dropout = dropout\n-        self.attention_dropout = attention_dropout\n-        self.max_position_embeddings = max_position_embeddings\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-            # make sure the first token has attention mask `1` to ensure that, after combining the causal mask, there\n-            # is still at least one token being attended to for each batch.\n-            # TODO: Change `random_attention_mask` in PT/TF/Flax common test file, after a discussion with the team.\n-            input_mask = tf.concat(\n-                [tf.ones_like(input_mask[:, :1], dtype=input_mask.dtype), input_mask[:, 1:]], axis=-1\n-            )\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, input_mask\n-\n-    def get_config(self):\n-        return CLIPTextConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            dropout=self.dropout,\n-            attention_dropout=self.attention_dropout,\n-            max_position_embeddings=self.max_position_embeddings,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-    def create_and_check_model(self, config, input_ids, input_mask):\n-        model = TFCLIPTextModel(config=config)\n-        result = model(input_ids, attention_mask=input_mask, training=False)\n-        result = model(input_ids, training=False)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, input_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFCLIPTextModelTest(TFModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFCLIPTextModel,) if is_tf_available() else ()\n-    test_pruning = False\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFCLIPTextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=CLIPTextConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_inputs_embeds(self):\n-        # CLIP does not use inputs_embeds\n-        pass\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"openai/clip-vit-base-patch32\"\n-        model = TFCLIPTextModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    @slow\n-    def test_saved_model_creation_extended(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.output_hidden_states = True\n-        config.output_attentions = True\n-\n-        if hasattr(config, \"use_cache\"):\n-            config.use_cache = True\n-\n-        for model_class in self.all_model_classes:\n-            class_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-            model = model_class(config)\n-            num_out = len(model(class_inputs_dict))\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname, saved_model=True)\n-                saved_model_dir = os.path.join(tmpdirname, \"saved_model\", \"1\")\n-                model = keras.models.load_model(saved_model_dir)\n-                outputs = model(class_inputs_dict)\n-                output_hidden_states = outputs[\"hidden_states\"]\n-                output_attentions = outputs[\"attentions\"]\n-\n-                # Check number of outputs\n-                self.assertEqual(len(outputs), num_out)\n-\n-                # Check number of layers\n-                expected_num_layers = getattr(\n-                    self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n-                )\n-\n-                # Check hidden states\n-                self.assertEqual(len(output_hidden_states), expected_num_layers)\n-                self.assertListEqual(\n-                    list(output_hidden_states[0].shape[-2:]),\n-                    [self.model_tester.seq_length, self.model_tester.hidden_size],\n-                )\n-\n-                # Check attention outputs\n-                self.assertEqual(len(output_attentions), self.model_tester.num_hidden_layers)\n-\n-                seq_length = self.model_tester.seq_length\n-                key_length = getattr(self.model_tester, \"key_length\", seq_length)\n-\n-                self.assertListEqual(\n-                    list(output_attentions[0].shape[-3:]),\n-                    [self.model_tester.num_attention_heads, seq_length, key_length],\n-                )\n-\n-\n-class TFCLIPModelTester:\n-    def __init__(self, parent, is_training=True):\n-        self.parent = parent\n-        self.text_model_tester = TFCLIPTextModelTester(parent)\n-        self.vision_model_tester = TFCLIPVisionModelTester(parent)\n-        self.is_training = is_training\n-\n-    def prepare_config_and_inputs(self):\n-        text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n-        vision_config, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, attention_mask, pixel_values\n-\n-    def get_config(self):\n-        return CLIPConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n-        )\n-\n-    def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n-        model = TFCLIPModel(config)\n-        result = model(input_ids, pixel_values, attention_mask, training=False)\n-        self.parent.assertEqual(\n-            result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)\n-        )\n-        self.parent.assertEqual(\n-            result.logits_per_text.shape, (self.text_model_tester.batch_size, self.vision_model_tester.batch_size)\n-        )\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask, pixel_values = config_and_inputs\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"pixel_values\": pixel_values,\n-            \"return_loss\": True,\n-        }\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFCLIPModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFCLIPModel,) if is_tf_available() else ()\n-    pipeline_model_mapping = {\"feature-extraction\": TFCLIPModel} if is_tf_available() else {}\n-    test_head_masking = False\n-    test_pruning = False\n-    test_resize_embeddings = False\n-    test_attention_outputs = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFCLIPModelTester(self)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    # hidden_states are tested in individual model tests\n-    def test_hidden_states_output(self):\n-        pass\n-\n-    # input_embeds are tested in individual model tests\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    # CLIPModel does not have input/output embeddings\n-    def test_model_common_attributes(self):\n-        pass\n-\n-    # overwrite from common since `TFCLIPModelTester` set `return_loss` to `True` and causes the preparation of\n-    # `symbolic_inputs` failed.\n-    def test_keras_save_load(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        # remove `return_loss` to make code work\n-        if self.__class__.__name__ == \"TFCLIPModelTest\":\n-            inputs_dict.pop(\"return_loss\", None)\n-\n-        tf_main_layer_classes = {\n-            module_member\n-            for model_class in self.all_model_classes\n-            for module in (import_module(model_class.__module__),)\n-            for module_member_name in dir(module)\n-            if module_member_name.endswith(\"MainLayer\")\n-            # This condition is required, since `modeling_tf_clip.py` has 3 classes whose names end with `MainLayer`.\n-            and module_member_name[: -len(\"MainLayer\")] == model_class.__name__[: -len(\"Model\")]\n-            for module_member in (getattr(module, module_member_name),)\n-            if isinstance(module_member, type)\n-            and keras.layers.Layer in module_member.__bases__\n-            and getattr(module_member, \"_keras_serializable\", False)\n-        }\n-        for main_layer_class in tf_main_layer_classes:\n-            # T5MainLayer needs an embed_tokens parameter when called without the inputs_embeds parameter\n-            if \"T5\" in main_layer_class.__name__:\n-                # Take the same values than in TFT5ModelTester for this shared layer\n-                shared = TFSharedEmbeddings(99, 32, name=\"shared\")\n-                config.use_cache = inputs_dict.pop(\"use_cache\", None)\n-                main_layer = main_layer_class(config, embed_tokens=shared)\n-            else:\n-                main_layer = main_layer_class(config)\n-\n-            symbolic_inputs = {\n-                name: keras.Input(tensor.shape[1:], dtype=tensor.dtype) for name, tensor in inputs_dict.items()\n-            }\n-\n-            model = keras.Model(symbolic_inputs, outputs=main_layer(symbolic_inputs))\n-            outputs = model(inputs_dict)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                filepath = os.path.join(tmpdirname, \"keras_model.h5\")\n-                model.save(filepath)\n-                if \"T5\" in main_layer_class.__name__:\n-                    model = keras.models.load_model(\n-                        filepath,\n-                        custom_objects={\n-                            main_layer_class.__name__: main_layer_class,\n-                            \"TFSharedEmbeddings\": TFSharedEmbeddings,\n-                        },\n-                    )\n-                else:\n-                    model = keras.models.load_model(\n-                        filepath, custom_objects={main_layer_class.__name__: main_layer_class}\n-                    )\n-                assert isinstance(model, keras.Model)\n-                after_outputs = model(inputs_dict)\n-                self.assert_outputs_same(after_outputs, outputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"openai/clip-vit-base-patch32\"\n-        model = TFCLIPModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    @unittest.skip(reason=\"Currently `saved_model` doesn't work with nested outputs.\")\n-    @slow\n-    def test_saved_model_creation(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Currently `saved_model` doesn't work with nested outputs.\")\n-    @slow\n-    def test_saved_model_creation_extended(self):\n-        pass\n-\n-    @unittest.skip(reason=\"`saved_model` doesn't work with nested outputs so no preparation happens.\")\n-    @slow\n-    def test_prepare_serving_output(self):\n-        pass\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-    im = Image.open(requests.get(url, stream=True).raw)\n-    return im\n-\n-\n-@require_vision\n-@require_tf\n-class TFCLIPModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_inference(self):\n-        model_name = \"openai/clip-vit-base-patch32\"\n-        model = TFCLIPModel.from_pretrained(model_name)\n-        processor = CLIPProcessor.from_pretrained(model_name)\n-\n-        image = prepare_img()\n-        inputs = processor(\n-            text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, padding=True, return_tensors=\"tf\"\n-        )\n-\n-        outputs = model(**inputs, training=False)\n-\n-        # verify the logits\n-        self.assertEqual(\n-            outputs.logits_per_image.shape,\n-            tf.TensorShape((inputs.pixel_values.shape[0], inputs.input_ids.shape[0])),\n-        )\n-        self.assertEqual(\n-            outputs.logits_per_text.shape,\n-            tf.TensorShape((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n-        )\n-\n-        expected_logits = tf.constant([[24.5701, 19.3049]])\n-\n-        tf.debugging.assert_near(outputs.logits_per_image, expected_logits, atol=1e-3)"
        },
        {
            "sha": "7bd21778eb8b5d945bda718e18db828a47105069",
            "filename": "tests/models/convbert/test_modeling_tf_convbert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 424,
            "changes": 424,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fconvbert%2Ftest_modeling_tf_convbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fconvbert%2Ftest_modeling_tf_convbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvbert%2Ftest_modeling_tf_convbert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,424 +0,0 @@\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-from __future__ import annotations\n-\n-import os\n-import tempfile\n-import unittest\n-\n-from transformers import ConvBertConfig, is_tf_available\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import (\n-        TFConvBertForMaskedLM,\n-        TFConvBertForMultipleChoice,\n-        TFConvBertForQuestionAnswering,\n-        TFConvBertForSequenceClassification,\n-        TFConvBertForTokenClassification,\n-        TFConvBertModel,\n-    )\n-    from transformers.modeling_tf_utils import keras\n-\n-\n-class TFConvBertModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = 13\n-        self.seq_length = 7\n-        self.is_training = True\n-        self.use_input_mask = True\n-        self.use_token_type_ids = True\n-        self.use_labels = True\n-        self.vocab_size = 99\n-        self.hidden_size = 384\n-        self.num_hidden_layers = 2\n-        self.num_attention_heads = 4\n-        self.intermediate_size = 37\n-        self.hidden_act = \"gelu\"\n-        self.hidden_dropout_prob = 0.1\n-        self.attention_probs_dropout_prob = 0.1\n-        self.max_position_embeddings = 512\n-        self.type_vocab_size = 16\n-        self.type_sequence_label_size = 2\n-        self.initializer_range = 0.02\n-        self.num_labels = 3\n-        self.num_choices = 4\n-        self.embedding_size = 128\n-        self.head_ratio = 2\n-        self.conv_kernel_size = 9\n-        self.num_groups = 1\n-        self.scope = None\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = ConvBertConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            initializer_range=self.initializer_range,\n-            return_dict=True,\n-        )\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFConvBertModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_masked_lm(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFConvBertForMaskedLM(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_for_sequence_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFConvBertForSequenceClassification(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_for_multiple_choice(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_choices = self.num_choices\n-        model = TFConvBertForMultipleChoice(config=config)\n-        multiple_choice_inputs_ids = tf.tile(tf.expand_dims(input_ids, 1), (1, self.num_choices, 1))\n-        multiple_choice_input_mask = tf.tile(tf.expand_dims(input_mask, 1), (1, self.num_choices, 1))\n-        multiple_choice_token_type_ids = tf.tile(tf.expand_dims(token_type_ids, 1), (1, self.num_choices, 1))\n-        inputs = {\n-            \"input_ids\": multiple_choice_inputs_ids,\n-            \"attention_mask\": multiple_choice_input_mask,\n-            \"token_type_ids\": multiple_choice_token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_choices))\n-\n-    def create_and_check_for_token_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFConvBertForTokenClassification(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def create_and_check_for_question_answering(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFConvBertForQuestionAnswering(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-\n-        result = model(inputs)\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFConvBertModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFConvBertModel,\n-            TFConvBertForMaskedLM,\n-            TFConvBertForQuestionAnswering,\n-            TFConvBertForSequenceClassification,\n-            TFConvBertForTokenClassification,\n-            TFConvBertForMultipleChoice,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFConvBertModel,\n-            \"fill-mask\": TFConvBertForMaskedLM,\n-            \"question-answering\": TFConvBertForQuestionAnswering,\n-            \"text-classification\": TFConvBertForSequenceClassification,\n-            \"token-classification\": TFConvBertForTokenClassification,\n-            \"zero-shot\": TFConvBertForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_pruning = False\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFConvBertModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=ConvBertConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_for_masked_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n-\n-    def test_for_multiple_choice(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_multiple_choice(*config_and_inputs)\n-\n-    def test_for_question_answering(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_question_answering(*config_and_inputs)\n-\n-    def test_for_sequence_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n-\n-    def test_for_token_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_token_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_saved_model_creation_extended(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.output_hidden_states = True\n-        config.output_attentions = True\n-\n-        if hasattr(config, \"use_cache\"):\n-            config.use_cache = True\n-\n-        encoder_seq_length = getattr(self.model_tester, \"encoder_seq_length\", self.model_tester.seq_length)\n-        encoder_key_length = getattr(self.model_tester, \"key_length\", encoder_seq_length)\n-\n-        for model_class in self.all_model_classes:\n-            class_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-            model = model_class(config)\n-            num_out = len(model(class_inputs_dict))\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname, saved_model=True)\n-                saved_model_dir = os.path.join(tmpdirname, \"saved_model\", \"1\")\n-                model = keras.models.load_model(saved_model_dir)\n-                outputs = model(class_inputs_dict)\n-\n-                if self.is_encoder_decoder:\n-                    output_hidden_states = outputs[\"encoder_hidden_states\"]\n-                    output_attentions = outputs[\"encoder_attentions\"]\n-                else:\n-                    output_hidden_states = outputs[\"hidden_states\"]\n-                    output_attentions = outputs[\"attentions\"]\n-\n-                self.assertEqual(len(outputs), num_out)\n-\n-                expected_num_layers = getattr(\n-                    self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n-                )\n-\n-                self.assertEqual(len(output_hidden_states), expected_num_layers)\n-                self.assertListEqual(\n-                    list(output_hidden_states[0].shape[-2:]),\n-                    [self.model_tester.seq_length, self.model_tester.hidden_size],\n-                )\n-\n-                self.assertEqual(len(output_attentions), self.model_tester.num_hidden_layers)\n-                self.assertListEqual(\n-                    list(output_attentions[0].shape[-3:]),\n-                    [self.model_tester.num_attention_heads / 2, encoder_seq_length, encoder_key_length],\n-                )\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model = TFConvBertModel.from_pretrained(\"YituTech/conv-bert-base\")\n-        self.assertIsNotNone(model)\n-\n-    def test_attention_outputs(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.return_dict = True\n-        decoder_seq_length = getattr(self.model_tester, \"decoder_seq_length\", self.model_tester.seq_length)\n-        encoder_seq_length = getattr(self.model_tester, \"encoder_seq_length\", self.model_tester.seq_length)\n-        decoder_key_length = getattr(self.model_tester, \"key_length\", decoder_seq_length)\n-        encoder_key_length = getattr(self.model_tester, \"key_length\", encoder_seq_length)\n-\n-        def check_decoder_attentions_output(outputs):\n-            out_len = len(outputs)\n-            self.assertEqual(out_len % 2, 0)\n-            decoder_attentions = outputs.decoder_attentions\n-            self.assertEqual(len(decoder_attentions), self.model_tester.num_hidden_layers)\n-            self.assertListEqual(\n-                list(decoder_attentions[0].shape[-3:]),\n-                [self.model_tester.num_attention_heads / 2, decoder_seq_length, decoder_key_length],\n-            )\n-\n-        def check_encoder_attentions_output(outputs):\n-            attentions = [\n-                t.numpy() for t in (outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions)\n-            ]\n-            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n-            self.assertListEqual(\n-                list(attentions[0].shape[-3:]),\n-                [self.model_tester.num_attention_heads / 2, encoder_seq_length, encoder_key_length],\n-            )\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_attentions\"] = True\n-            config.output_hidden_states = False\n-            model = model_class(config)\n-            outputs = model(self._prepare_for_class(inputs_dict, model_class))\n-            out_len = len(outputs)\n-            self.assertEqual(config.output_hidden_states, False)\n-            check_encoder_attentions_output(outputs)\n-\n-            if self.is_encoder_decoder:\n-                model = model_class(config)\n-                outputs = model(self._prepare_for_class(inputs_dict, model_class))\n-                self.assertEqual(config.output_hidden_states, False)\n-                check_decoder_attentions_output(outputs)\n-\n-            # Check that output attentions can also be changed via the config\n-            del inputs_dict[\"output_attentions\"]\n-            config.output_attentions = True\n-            model = model_class(config)\n-            outputs = model(self._prepare_for_class(inputs_dict, model_class))\n-            self.assertEqual(config.output_hidden_states, False)\n-            check_encoder_attentions_output(outputs)\n-\n-            # Check attention is always last and order is fine\n-            inputs_dict[\"output_attentions\"] = True\n-            config.output_hidden_states = True\n-            model = model_class(config)\n-            outputs = model(self._prepare_for_class(inputs_dict, model_class))\n-\n-            self.assertEqual(out_len + (2 if self.is_encoder_decoder else 1), len(outputs))\n-            self.assertEqual(model.config.output_hidden_states, True)\n-            check_encoder_attentions_output(outputs)\n-\n-\n-@require_tf\n-class TFConvBertModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_inference_masked_lm(self):\n-        model = TFConvBertModel.from_pretrained(\"YituTech/conv-bert-base\")\n-        input_ids = tf.constant([[0, 1, 2, 3, 4, 5]])\n-        output = model(input_ids)[0]\n-\n-        expected_shape = [1, 6, 768]\n-        self.assertEqual(output.shape, expected_shape)\n-\n-        expected_slice = tf.constant(\n-            [\n-                [\n-                    [-0.03475493, -0.4686034, -0.30638832],\n-                    [0.22637248, -0.26988646, -0.7423424],\n-                    [0.10324868, -0.45013508, -0.58280784],\n-                ]\n-            ]\n-        )\n-        tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=1e-4)"
        },
        {
            "sha": "1e46e57fb25dff696e18839670caf95357d80257",
            "filename": "tests/models/convnext/test_modeling_tf_convnext.py",
            "status": "removed",
            "additions": 0,
            "deletions": 300,
            "changes": 300,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fconvnext%2Ftest_modeling_tf_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fconvnext%2Ftest_modeling_tf_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnext%2Ftest_modeling_tf_convnext.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,300 +0,0 @@\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Testing suite for the TensorFlow ConvNext model.\"\"\"\n-\n-from __future__ import annotations\n-\n-import inspect\n-import unittest\n-\n-from transformers import ConvNextConfig\n-from transformers.testing_utils import require_tf, require_vision, slow\n-from transformers.utils import cached_property, is_tf_available, is_vision_available\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import TFConvNextForImageClassification, TFConvNextModel\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-    from transformers import ConvNextImageProcessor\n-\n-\n-class TFConvNextModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        image_size=32,\n-        num_channels=3,\n-        num_stages=4,\n-        hidden_sizes=[10, 20, 30, 40],\n-        depths=[2, 2, 3, 2],\n-        is_training=True,\n-        use_labels=True,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        type_sequence_label_size=10,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.image_size = image_size\n-        self.num_channels = num_channels\n-        self.num_stages = num_stages\n-        self.hidden_sizes = hidden_sizes\n-        self.depths = depths\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-\n-        labels = None\n-        if self.use_labels:\n-            labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-\n-        config = self.get_config()\n-\n-        return config, pixel_values, labels\n-\n-    def get_config(self):\n-        return ConvNextConfig(\n-            num_channels=self.num_channels,\n-            hidden_sizes=self.hidden_sizes,\n-            depths=self.depths,\n-            num_stages=self.num_stages,\n-            hidden_act=self.hidden_act,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-    def create_and_check_model(self, config, pixel_values, labels):\n-        model = TFConvNextModel(config=config)\n-        result = model(pixel_values, training=False)\n-        # expected last hidden states: B, C, H // 32, W // 32\n-        self.parent.assertEqual(\n-            result.last_hidden_state.shape,\n-            (self.batch_size, self.hidden_sizes[-1], self.image_size // 32, self.image_size // 32),\n-        )\n-\n-    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n-        config.num_labels = self.type_sequence_label_size\n-        model = TFConvNextForImageClassification(config)\n-        result = model(pixel_values, labels=labels, training=False)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values, labels = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFConvNextModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    \"\"\"\n-    Here we also overwrite some of the tests of test_modeling_common.py, as ConvNext does not use input_ids, inputs_embeds,\n-    attention_mask and seq_length.\n-    \"\"\"\n-\n-    all_model_classes = (TFConvNextModel, TFConvNextForImageClassification) if is_tf_available() else ()\n-    pipeline_model_mapping = (\n-        {\"feature-extraction\": TFConvNextModel, \"image-classification\": TFConvNextForImageClassification}\n-        if is_tf_available()\n-        else {}\n-    )\n-\n-    test_pruning = False\n-    test_onnx = False\n-    test_resize_embeddings = False\n-    test_head_masking = False\n-    has_attentions = False\n-\n-    def setUp(self):\n-        self.model_tester = TFConvNextModelTester(self)\n-        self.config_tester = ConfigTester(\n-            self,\n-            config_class=ConvNextConfig,\n-            has_text_modality=False,\n-            hidden_size=37,\n-        )\n-\n-    @unittest.skip(reason=\"ConvNext does not use inputs_embeds\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skipIf(\n-        not is_tf_available() or len(tf.config.list_physical_devices(\"GPU\")) == 0,\n-        reason=\"TF does not support backprop for grouped convolutions on CPU.\",\n-    )\n-    @slow\n-    def test_keras_fit(self):\n-        super().test_keras_fit()\n-\n-    @unittest.skip(reason=\"ConvNext does not support input and output embeddings\")\n-    def test_model_common_attributes(self):\n-        pass\n-\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.call)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"pixel_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    @unittest.skipIf(\n-        not is_tf_available() or len(tf.config.list_physical_devices(\"GPU\")) == 0,\n-        reason=\"TF does not support backprop for grouped convolutions on CPU.\",\n-    )\n-    def test_dataset_conversion(self):\n-        super().test_dataset_conversion()\n-\n-    def test_hidden_states_output(self):\n-        def check_hidden_states_output(inputs_dict, config, model_class):\n-            model = model_class(config)\n-\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n-\n-            expected_num_stages = self.model_tester.num_stages\n-            self.assertEqual(len(hidden_states), expected_num_stages + 1)\n-\n-            # ConvNext's feature maps are of shape (batch_size, num_channels, height, width)\n-            self.assertListEqual(\n-                list(hidden_states[0].shape[-2:]),\n-                [self.model_tester.image_size // 4, self.model_tester.image_size // 4],\n-            )\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_hidden_states\"] = True\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-            # check that output_hidden_states also work using config\n-            del inputs_dict[\"output_hidden_states\"]\n-            config.output_hidden_states = True\n-\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-    # Since ConvNext does not have any attention we need to rewrite this test.\n-    def test_model_outputs_equivalence(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        def check_equivalence(model, tuple_inputs, dict_inputs, additional_kwargs={}):\n-            tuple_output = model(tuple_inputs, return_dict=False, **additional_kwargs)\n-            dict_output = model(dict_inputs, return_dict=True, **additional_kwargs).to_tuple()\n-\n-            def recursive_check(tuple_object, dict_object):\n-                if isinstance(tuple_object, (list, tuple)):\n-                    for tuple_iterable_value, dict_iterable_value in zip(tuple_object, dict_object):\n-                        recursive_check(tuple_iterable_value, dict_iterable_value)\n-                elif tuple_object is None:\n-                    return\n-                else:\n-                    self.assertTrue(\n-                        all(tf.equal(tuple_object, dict_object)),\n-                        msg=(\n-                            \"Tuple and dict output are not equal. Difference:\"\n-                            f\" {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\"\n-                        ),\n-                    )\n-\n-                recursive_check(tuple_output, dict_output)\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-\n-            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n-            dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n-            check_equivalence(model, tuple_inputs, dict_inputs)\n-\n-            tuple_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n-            dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n-            check_equivalence(model, tuple_inputs, dict_inputs)\n-\n-            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n-            dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n-            check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n-\n-            tuple_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n-            dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n-            check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n-\n-    def test_for_image_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model = TFConvNextModel.from_pretrained(\"facebook/convnext-tiny-224\")\n-        self.assertIsNotNone(model)\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n-    return image\n-\n-\n-@require_tf\n-@require_vision\n-class TFConvNextModelIntegrationTest(unittest.TestCase):\n-    @cached_property\n-    def default_image_processor(self):\n-        return ConvNextImageProcessor.from_pretrained(\"facebook/convnext-tiny-224\") if is_vision_available() else None\n-\n-    @slow\n-    def test_inference_image_classification_head(self):\n-        model = TFConvNextForImageClassification.from_pretrained(\"facebook/convnext-tiny-224\")\n-\n-        image_processor = self.default_image_processor\n-        image = prepare_img()\n-        inputs = image_processor(images=image, return_tensors=\"tf\")\n-\n-        # forward pass\n-        outputs = model(**inputs)\n-\n-        # verify the logits\n-        expected_shape = tf.TensorShape((1, 1000))\n-        self.assertEqual(outputs.logits.shape, expected_shape)\n-\n-        expected_slice = tf.constant([-0.0260, -0.4739, 0.1911])\n-\n-        tf.debugging.assert_near(outputs.logits[0, :3], expected_slice, atol=1e-4)"
        },
        {
            "sha": "08e458609c7612dcf9ca895216a202d12411e455",
            "filename": "tests/models/convnextv2/test_modeling_tf_convnextv2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 306,
            "changes": 306,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_tf_convnextv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_tf_convnextv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnextv2%2Ftest_modeling_tf_convnextv2.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,306 +0,0 @@\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Testing suite for the TensorFlow ConvNext model.\"\"\"\n-\n-from __future__ import annotations\n-\n-import inspect\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import ConvNextV2Config\n-from transformers.testing_utils import require_tf, require_vision, slow\n-from transformers.utils import cached_property, is_tf_available, is_vision_available\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import TFConvNextV2ForImageClassification, TFConvNextV2Model\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-    from transformers import ConvNextImageProcessor\n-\n-\n-class TFConvNextV2ModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        image_size=32,\n-        num_channels=3,\n-        num_stages=4,\n-        hidden_sizes=[10, 20, 30, 40],\n-        depths=[2, 2, 3, 2],\n-        is_training=True,\n-        use_labels=True,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        type_sequence_label_size=10,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.image_size = image_size\n-        self.num_channels = num_channels\n-        self.num_stages = num_stages\n-        self.hidden_sizes = hidden_sizes\n-        self.depths = depths\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-\n-        labels = None\n-        if self.use_labels:\n-            labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-\n-        config = self.get_config()\n-\n-        return config, pixel_values, labels\n-\n-    def get_config(self):\n-        return ConvNextV2Config(\n-            num_channels=self.num_channels,\n-            hidden_sizes=self.hidden_sizes,\n-            depths=self.depths,\n-            num_stages=self.num_stages,\n-            hidden_act=self.hidden_act,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-    def create_and_check_model(self, config, pixel_values, labels):\n-        model = TFConvNextV2Model(config=config)\n-        result = model(pixel_values, training=False)\n-        # expected last hidden states: batch_size, channels, height // 32, width // 32\n-        self.parent.assertEqual(\n-            result.last_hidden_state.shape,\n-            (self.batch_size, self.hidden_sizes[-1], self.image_size // 32, self.image_size // 32),\n-        )\n-\n-    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n-        config.num_labels = self.type_sequence_label_size\n-        model = TFConvNextV2ForImageClassification(config)\n-        result = model(pixel_values, labels=labels, training=False)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values, labels = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFConvNextV2ModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    \"\"\"\n-    Here we also overwrite some of the tests of test_modeling_common.py, as ConvNext does not use input_ids, inputs_embeds,\n-    attention_mask and seq_length.\n-    \"\"\"\n-\n-    all_model_classes = (TFConvNextV2Model, TFConvNextV2ForImageClassification) if is_tf_available() else ()\n-    pipeline_model_mapping = (\n-        {\"feature-extraction\": TFConvNextV2Model, \"image-classification\": TFConvNextV2ForImageClassification}\n-        if is_tf_available()\n-        else {}\n-    )\n-\n-    test_pruning = False\n-    test_onnx = False\n-    test_resize_embeddings = False\n-    test_head_masking = False\n-    has_attentions = False\n-\n-    def setUp(self):\n-        self.model_tester = TFConvNextV2ModelTester(self)\n-        self.config_tester = ConfigTester(\n-            self,\n-            config_class=ConvNextV2Config,\n-            has_text_modality=False,\n-            hidden_size=37,\n-        )\n-\n-    @unittest.skip(reason=\"ConvNext does not use inputs_embeds\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skipIf(\n-        not is_tf_available() or len(tf.config.list_physical_devices(\"GPU\")) == 0,\n-        reason=\"TF does not support backprop for grouped convolutions on CPU.\",\n-    )\n-    @slow\n-    def test_keras_fit(self):\n-        super().test_keras_fit()\n-\n-    @unittest.skip(reason=\"ConvNext does not support input and output embeddings\")\n-    def test_model_common_attributes(self):\n-        pass\n-\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.call)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"pixel_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    @unittest.skipIf(\n-        not is_tf_available() or len(tf.config.list_physical_devices(\"GPU\")) == 0,\n-        reason=\"TF does not support backprop for grouped convolutions on CPU.\",\n-    )\n-    def test_dataset_conversion(self):\n-        super().test_dataset_conversion()\n-\n-    def test_hidden_states_output(self):\n-        def check_hidden_states_output(inputs_dict, config, model_class):\n-            model = model_class(config)\n-\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n-\n-            expected_num_stages = self.model_tester.num_stages\n-            self.assertEqual(len(hidden_states), expected_num_stages + 1)\n-\n-            # ConvNext's feature maps are of shape (batch_size, num_channels, height, width)\n-            self.assertListEqual(\n-                list(hidden_states[0].shape[-2:]),\n-                [self.model_tester.image_size // 4, self.model_tester.image_size // 4],\n-            )\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_hidden_states\"] = True\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-            # check that output_hidden_states also work using config\n-            del inputs_dict[\"output_hidden_states\"]\n-            config.output_hidden_states = True\n-\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-    # Since ConvNext does not have any attention we need to rewrite this test.\n-    def test_model_outputs_equivalence(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        def check_equivalence(model, tuple_inputs, dict_inputs, additional_kwargs={}):\n-            tuple_output = model(tuple_inputs, return_dict=False, **additional_kwargs)\n-            dict_output = model(dict_inputs, return_dict=True, **additional_kwargs).to_tuple()\n-\n-            def recursive_check(tuple_object, dict_object):\n-                if isinstance(tuple_object, (list, tuple)):\n-                    for tuple_iterable_value, dict_iterable_value in zip(tuple_object, dict_object):\n-                        recursive_check(tuple_iterable_value, dict_iterable_value)\n-                elif tuple_object is None:\n-                    return\n-                else:\n-                    self.assertTrue(\n-                        all(tf.equal(tuple_object, dict_object)),\n-                        msg=(\n-                            \"Tuple and dict output are not equal. Difference:\"\n-                            f\" {tf.math.reduce_max(tf.abs(tuple_object - dict_object))}\"\n-                        ),\n-                    )\n-\n-                recursive_check(tuple_output, dict_output)\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-\n-            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n-            dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n-            check_equivalence(model, tuple_inputs, dict_inputs)\n-\n-            tuple_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n-            dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n-            check_equivalence(model, tuple_inputs, dict_inputs)\n-\n-            tuple_inputs = self._prepare_for_class(inputs_dict, model_class)\n-            dict_inputs = self._prepare_for_class(inputs_dict, model_class)\n-            check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n-\n-            tuple_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n-            dict_inputs = self._prepare_for_class(inputs_dict, model_class, return_labels=True)\n-            check_equivalence(model, tuple_inputs, dict_inputs, {\"output_hidden_states\": True})\n-\n-    def test_for_image_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model = TFConvNextV2Model.from_pretrained(\"facebook/convnextv2-tiny-1k-224\")\n-        self.assertIsNotNone(model)\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n-    return image\n-\n-\n-@require_tf\n-@require_vision\n-class TFConvNextV2ModelIntegrationTest(unittest.TestCase):\n-    @cached_property\n-    def default_image_processor(self):\n-        return (\n-            ConvNextImageProcessor.from_pretrained(\"facebook/convnextv2-tiny-1k-224\")\n-            if is_vision_available()\n-            else None\n-        )\n-\n-    @slow\n-    def test_inference_image_classification_head(self):\n-        model = TFConvNextV2ForImageClassification.from_pretrained(\"facebook/convnextv2-tiny-1k-224\")\n-\n-        image_processor = self.default_image_processor\n-        image = prepare_img()\n-        inputs = image_processor(images=image, return_tensors=\"tf\")\n-\n-        # forward pass\n-        outputs = model(**inputs)\n-\n-        # verify the logits\n-        expected_shape = tf.TensorShape((1, 1000))\n-        self.assertEqual(outputs.logits.shape, expected_shape)\n-\n-        expected_slice = np.array([0.9996, 0.1966, -0.4386])\n-\n-        self.assertTrue(np.allclose(outputs.logits[0, :3].numpy(), expected_slice, atol=1e-4))"
        },
        {
            "sha": "38623d442a3e04b8bfb6154fb3805f5d5770afe8",
            "filename": "tests/models/ctrl/test_modeling_tf_ctrl.py",
            "status": "removed",
            "additions": 0,
            "deletions": 292,
            "changes": 292,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fctrl%2Ftest_modeling_tf_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fctrl%2Ftest_modeling_tf_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fctrl%2Ftest_modeling_tf_ctrl.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,292 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import CTRLConfig, is_tf_available\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers.modeling_tf_utils import keras\n-    from transformers.models.ctrl.modeling_tf_ctrl import (\n-        TFCTRLForSequenceClassification,\n-        TFCTRLLMHeadModel,\n-        TFCTRLModel,\n-    )\n-\n-\n-class TFCTRLModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-    ):\n-        self.parent = parent\n-        self.batch_size = 13\n-        self.seq_length = 7\n-        self.is_training = True\n-        self.use_token_type_ids = True\n-        self.use_input_mask = True\n-        self.use_labels = True\n-        self.use_mc_token_ids = True\n-        self.vocab_size = 99\n-        self.hidden_size = 32\n-        self.num_hidden_layers = 2\n-        self.num_attention_heads = 4\n-        self.intermediate_size = 37\n-        self.hidden_act = \"gelu\"\n-        self.hidden_dropout_prob = 0.1\n-        self.attention_probs_dropout_prob = 0.1\n-        self.max_position_embeddings = 512\n-        self.type_vocab_size = 16\n-        self.type_sequence_label_size = 2\n-        self.initializer_range = 0.02\n-        self.num_labels = 3\n-        self.num_choices = 4\n-        self.scope = None\n-        self.pad_token_id = self.vocab_size - 1\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        mc_token_ids = None\n-        if self.use_mc_token_ids:\n-            mc_token_ids = ids_tensor([self.batch_size, self.num_choices], self.seq_length)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = CTRLConfig(\n-            vocab_size=self.vocab_size,\n-            n_embd=self.hidden_size,\n-            n_layer=self.num_hidden_layers,\n-            n_head=self.num_attention_heads,\n-            dff=self.intermediate_size,\n-            # hidden_act=self.hidden_act,\n-            # hidden_dropout_prob=self.hidden_dropout_prob,\n-            # attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            n_positions=self.max_position_embeddings,\n-            # type_vocab_size=self.type_vocab_size,\n-            # initializer_range=self.initializer_range,\n-            pad_token_id=self.pad_token_id,\n-        )\n-\n-        head_mask = ids_tensor([self.num_hidden_layers, self.num_attention_heads], 2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        )\n-\n-    def create_and_check_ctrl_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n-        model = TFCTRLModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-\n-        inputs = [input_ids, None, input_mask]  # None is the input for 'past'\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_ctrl_lm_head(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n-        model = TFCTRLLMHeadModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_ctrl_for_sequence_classification(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n-    ):\n-        config.num_labels = self.num_labels\n-        sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"token_type_ids\": token_type_ids,\n-            \"labels\": sequence_labels,\n-        }\n-        model = TFCTRLForSequenceClassification(config)\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-\n-        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFCTRLModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFCTRLModel, TFCTRLLMHeadModel, TFCTRLForSequenceClassification) if is_tf_available() else ()\n-    all_generative_model_classes = (TFCTRLLMHeadModel,) if is_tf_available() else ()\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFCTRLModel,\n-            \"text-classification\": TFCTRLForSequenceClassification,\n-            \"text-generation\": TFCTRLLMHeadModel,\n-            \"zero-shot\": TFCTRLForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    # TODO: Fix the failed tests\n-    def is_pipeline_test_to_skip(\n-        self,\n-        pipeline_test_case_name,\n-        config_class,\n-        model_architecture,\n-        tokenizer_name,\n-        image_processor_name,\n-        feature_extractor_name,\n-        processor_name,\n-    ):\n-        if pipeline_test_case_name == \"ZeroShotClassificationPipelineTests\":\n-            # Get `tokenizer does not have a padding token` error for both fast/slow tokenizers.\n-            # `CTRLConfig` was never used in pipeline tests, either because of a missing checkpoint or because a tiny\n-            # config could not be created.\n-            return True\n-\n-        return False\n-\n-    def setUp(self):\n-        self.model_tester = TFCTRLModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=CTRLConfig, n_embd=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_ctrl_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_ctrl_model(*config_and_inputs)\n-\n-    def test_ctrl_lm_head(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_ctrl_lm_head(*config_and_inputs)\n-\n-    def test_ctrl_sequence_classification_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_ctrl_for_sequence_classification(*config_and_inputs)\n-\n-    def test_model_common_attributes(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        list_lm_models = [TFCTRLLMHeadModel]\n-        list_other_models_with_output_ebd = [TFCTRLForSequenceClassification]\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.build_in_name_scope()  # may be needed for the get_bias() call below\n-            assert isinstance(model.get_input_embeddings(), keras.layers.Layer)\n-\n-            if model_class in list_lm_models:\n-                x = model.get_output_embeddings()\n-                assert isinstance(x, keras.layers.Layer)\n-                name = model.get_bias()\n-                assert isinstance(name, dict)\n-                for k, v in name.items():\n-                    assert isinstance(v, tf.Variable)\n-            elif model_class in list_other_models_with_output_ebd:\n-                x = model.get_output_embeddings()\n-                assert isinstance(x, keras.layers.Layer)\n-                name = model.get_bias()\n-                assert name is None\n-            else:\n-                x = model.get_output_embeddings()\n-                assert x is None\n-                name = model.get_bias()\n-                assert name is None\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"Salesforce/ctrl\"\n-        model = TFCTRLModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-\n-@require_tf\n-class TFCTRLModelLanguageGenerationTest(unittest.TestCase):\n-    @slow\n-    def test_lm_generate_ctrl(self):\n-        model = TFCTRLLMHeadModel.from_pretrained(\"Salesforce/ctrl\")\n-        input_ids = tf.convert_to_tensor([[11859, 0, 1611, 8]], dtype=tf.int32)  # Legal the president is\n-        expected_output_ids = [\n-            11859,\n-            0,\n-            1611,\n-            8,\n-            5,\n-            150,\n-            26449,\n-            2,\n-            19,\n-            348,\n-            469,\n-            3,\n-            2595,\n-            48,\n-            20740,\n-            246533,\n-            246533,\n-            19,\n-            30,\n-            5,\n-        ]  # Legal the president is a good guy and I don't want to lose my job. \\n \\n I have a\n-\n-        output_ids = model.generate(input_ids, do_sample=False)\n-        self.assertListEqual(output_ids[0].numpy().tolist(), expected_output_ids)"
        },
        {
            "sha": "211529719aa1d58b00b1323d35fcbd8ef39b5f7e",
            "filename": "tests/models/cvt/test_modeling_tf_cvt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 286,
            "changes": 286,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fcvt%2Ftest_modeling_tf_cvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fcvt%2Ftest_modeling_tf_cvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcvt%2Ftest_modeling_tf_cvt.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,286 +0,0 @@\n-\"\"\"Testing suite for the Tensorflow CvT model.\"\"\"\n-\n-from __future__ import annotations\n-\n-import inspect\n-import unittest\n-from math import floor\n-\n-import numpy as np\n-\n-from transformers import CvtConfig\n-from transformers.testing_utils import require_tf, require_vision, slow\n-from transformers.utils import cached_property, is_tf_available, is_vision_available\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import TFCvtForImageClassification, TFCvtModel\n-    from transformers.modeling_tf_utils import keras\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-    from transformers import AutoImageProcessor\n-\n-\n-class TFCvtConfigTester(ConfigTester):\n-    def create_and_test_config_common_properties(self):\n-        config = self.config_class(**self.inputs_dict)\n-        self.parent.assertTrue(hasattr(config, \"embed_dim\"))\n-        self.parent.assertTrue(hasattr(config, \"num_heads\"))\n-\n-\n-class TFCvtModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        image_size=64,\n-        num_channels=3,\n-        embed_dim=[16, 32, 48],\n-        num_heads=[1, 2, 3],\n-        depth=[1, 2, 10],\n-        patch_sizes=[7, 3, 3],\n-        patch_stride=[4, 2, 2],\n-        patch_padding=[2, 1, 1],\n-        stride_kv=[2, 2, 2],\n-        cls_token=[False, False, True],\n-        attention_drop_rate=[0.0, 0.0, 0.0],\n-        initializer_range=0.02,\n-        layer_norm_eps=1e-12,\n-        is_training=True,\n-        use_labels=True,\n-        num_labels=2,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.image_size = image_size\n-        self.patch_sizes = patch_sizes\n-        self.patch_stride = patch_stride\n-        self.patch_padding = patch_padding\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.num_labels = num_labels\n-        self.num_channels = num_channels\n-        self.embed_dim = embed_dim\n-        self.num_heads = num_heads\n-        self.stride_kv = stride_kv\n-        self.depth = depth\n-        self.cls_token = cls_token\n-        self.attention_drop_rate = attention_drop_rate\n-        self.initializer_range = initializer_range\n-        self.layer_norm_eps = layer_norm_eps\n-\n-    def prepare_config_and_inputs(self):\n-        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-\n-        labels = None\n-        if self.use_labels:\n-            # create a random int32 tensor of given shape\n-            labels = ids_tensor([self.batch_size], self.num_labels)\n-\n-        config = self.get_config()\n-        return config, pixel_values, labels\n-\n-    def get_config(self):\n-        return CvtConfig(\n-            image_size=self.image_size,\n-            num_labels=self.num_labels,\n-            num_channels=self.num_channels,\n-            embed_dim=self.embed_dim,\n-            num_heads=self.num_heads,\n-            patch_sizes=self.patch_sizes,\n-            patch_padding=self.patch_padding,\n-            patch_stride=self.patch_stride,\n-            stride_kv=self.stride_kv,\n-            depth=self.depth,\n-            cls_token=self.cls_token,\n-            attention_drop_rate=self.attention_drop_rate,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-    def create_and_check_model(self, config, pixel_values, labels):\n-        model = TFCvtModel(config=config)\n-        result = model(pixel_values, training=False)\n-        image_size = (self.image_size, self.image_size)\n-        height, width = image_size[0], image_size[1]\n-        for i in range(len(self.depth)):\n-            height = floor(((height + 2 * self.patch_padding[i] - self.patch_sizes[i]) / self.patch_stride[i]) + 1)\n-            width = floor(((width + 2 * self.patch_padding[i] - self.patch_sizes[i]) / self.patch_stride[i]) + 1)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.embed_dim[-1], height, width))\n-\n-    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n-        config.num_labels = self.num_labels\n-        model = TFCvtForImageClassification(config)\n-        result = model(pixel_values, labels=labels, training=False)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values, labels = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFCvtModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    \"\"\"\n-    Here we also overwrite some of the tests of test_modeling_common.py, as Cvt\n-    does not use input_ids, inputs_embeds, attention_mask and seq_length.\n-    \"\"\"\n-\n-    all_model_classes = (TFCvtModel, TFCvtForImageClassification) if is_tf_available() else ()\n-    pipeline_model_mapping = (\n-        {\"feature-extraction\": TFCvtModel, \"image-classification\": TFCvtForImageClassification}\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_pruning = False\n-    test_resize_embeddings = False\n-    test_head_masking = False\n-    has_attentions = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFCvtModelTester(self)\n-        self.config_tester = TFCvtConfigTester(self, config_class=CvtConfig, has_text_modality=False, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.create_and_test_config_common_properties()\n-        self.config_tester.create_and_test_config_to_json_string()\n-        self.config_tester.create_and_test_config_to_json_file()\n-        self.config_tester.create_and_test_config_from_and_save_pretrained()\n-        self.config_tester.create_and_test_config_with_num_labels()\n-        self.config_tester.check_config_can_be_init_without_params()\n-        self.config_tester.check_config_arguments_init()\n-\n-    @unittest.skip(reason=\"Cvt does not output attentions\")\n-    def test_attention_outputs(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Cvt does not use inputs_embeds\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Cvt does not support input and output embeddings\")\n-    def test_model_common_attributes(self):\n-        pass\n-\n-    @unittest.skipIf(\n-        not is_tf_available() or len(tf.config.list_physical_devices(\"GPU\")) == 0,\n-        reason=\"TF does not support backprop for grouped convolutions on CPU.\",\n-    )\n-    def test_dataset_conversion(self):\n-        super().test_dataset_conversion()\n-\n-    @unittest.skipIf(\n-        not is_tf_available() or len(tf.config.list_physical_devices(\"GPU\")) == 0,\n-        reason=\"TF does not support backprop for grouped convolutions on CPU.\",\n-    )\n-    @slow\n-    def test_keras_fit(self):\n-        super().test_keras_fit()\n-\n-    @unittest.skip(reason=\"Get `Failed to determine best cudnn convolution algo.` error after using TF 2.12+cuda 11.8\")\n-    def test_keras_fit_mixed_precision(self):\n-        policy = keras.mixed_precision.Policy(\"mixed_float16\")\n-        keras.mixed_precision.set_global_policy(policy)\n-        super().test_keras_fit()\n-        keras.mixed_precision.set_global_policy(\"float32\")\n-\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.call)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"pixel_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    def test_hidden_states_output(self):\n-        def check_hidden_states_output(inputs_dict, config, model_class):\n-            model = model_class(config)\n-\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-            hidden_states = outputs.hidden_states\n-\n-            expected_num_layers = len(self.model_tester.depth)\n-            self.assertEqual(len(hidden_states), expected_num_layers)\n-\n-            # verify the first hidden states (first block)\n-            self.assertListEqual(\n-                list(hidden_states[0].shape[-3:]),\n-                [\n-                    self.model_tester.embed_dim[0],\n-                    self.model_tester.image_size // 4,\n-                    self.model_tester.image_size // 4,\n-                ],\n-            )\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_hidden_states\"] = True\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-            # check that output_hidden_states also work using config\n-            del inputs_dict[\"output_hidden_states\"]\n-            config.output_hidden_states = True\n-\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_for_image_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"microsoft/cvt-13\"\n-        model = TFCvtModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n-    return image\n-\n-\n-@require_tf\n-@require_vision\n-class TFCvtModelIntegrationTest(unittest.TestCase):\n-    @cached_property\n-    def default_image_processor(self):\n-        return AutoImageProcessor.from_pretrained(\"microsoft/cvt-13\")\n-\n-    @slow\n-    def test_inference_image_classification_head(self):\n-        model = TFCvtForImageClassification.from_pretrained(\"microsoft/cvt-13\")\n-\n-        image_processor = self.default_image_processor\n-        image = prepare_img()\n-        inputs = image_processor(images=image, return_tensors=\"tf\")\n-\n-        # forward pass\n-        outputs = model(**inputs)\n-\n-        # verify the logits\n-        expected_shape = tf.TensorShape((1, 1000))\n-        self.assertEqual(outputs.logits.shape, expected_shape)\n-\n-        expected_slice = tf.constant([0.9285, 0.9015, -0.3150])\n-        self.assertTrue(np.allclose(outputs.logits[0, :3].numpy(), expected_slice, atol=1e-4))"
        },
        {
            "sha": "3f88801534027c6f7385c6e3793990f5f5b61b91",
            "filename": "tests/models/data2vec/test_modeling_tf_data2vec_vision.py",
            "status": "removed",
            "additions": 0,
            "deletions": 491,
            "changes": 491,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_tf_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_tf_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_tf_data2vec_vision.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,491 +0,0 @@\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Testing suite for the TensorFlow Data2VecVision model.\"\"\"\n-\n-from __future__ import annotations\n-\n-import collections.abc\n-import inspect\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import Data2VecVisionConfig\n-from transformers.file_utils import cached_property, is_tf_available, is_vision_available\n-from transformers.testing_utils import require_tf, require_vision, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import (\n-        TFData2VecVisionForImageClassification,\n-        TFData2VecVisionForSemanticSegmentation,\n-        TFData2VecVisionModel,\n-    )\n-    from transformers.modeling_tf_utils import keras\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-    from transformers import BeitImageProcessor\n-\n-\n-class TFData2VecVisionModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        vocab_size=100,\n-        batch_size=13,\n-        image_size=30,\n-        patch_size=2,\n-        num_channels=3,\n-        is_training=True,\n-        use_labels=True,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        type_sequence_label_size=10,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        scope=None,\n-        out_indices=[0, 1, 2, 3],\n-    ):\n-        self.parent = parent\n-        self.vocab_size = 100\n-        self.batch_size = batch_size\n-        self.image_size = image_size\n-        self.patch_size = patch_size\n-        self.num_channels = num_channels\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-        self.out_indices = out_indices\n-        self.num_labels = num_labels\n-\n-    def prepare_config_and_inputs(self):\n-        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-\n-        labels = None\n-        pixel_labels = None\n-        if self.use_labels:\n-            labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            pixel_labels = ids_tensor([self.batch_size, self.image_size, self.image_size], self.num_labels)\n-\n-        config = self.get_config()\n-\n-        return config, pixel_values, labels, pixel_labels\n-\n-    def get_config(self):\n-        return Data2VecVisionConfig(\n-            vocab_size=self.vocab_size,\n-            image_size=self.image_size,\n-            patch_size=self.patch_size,\n-            num_channels=self.num_channels,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            out_indices=self.out_indices,\n-        )\n-\n-    def create_and_check_model(self, config, pixel_values, labels, pixel_labels):\n-        model = TFData2VecVisionModel(config=config)\n-        result = model(pixel_values, training=False)\n-        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n-        image_size = (\n-            self.image_size\n-            if isinstance(self.image_size, collections.abc.Iterable)\n-            else (self.image_size, self.image_size)\n-        )\n-        patch_size = (\n-            self.patch_size\n-            if isinstance(self.image_size, collections.abc.Iterable)\n-            else (self.patch_size, self.patch_size)\n-        )\n-        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n-\n-    def create_and_check_for_image_classification(self, config, pixel_values, labels, pixel_labels):\n-        config.num_labels = self.type_sequence_label_size\n-        model = TFData2VecVisionForImageClassification(config)\n-\n-        result = model(pixel_values, labels=labels, training=False)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n-\n-    def create_and_check_for_image_segmentation(self, config, pixel_values, labels, pixel_labels):\n-        config.num_labels = self.num_labels\n-        model = TFData2VecVisionForSemanticSegmentation(config)\n-        result = model(pixel_values, training=False)\n-        self.parent.assertEqual(\n-            result.logits.shape, (self.batch_size, self.num_labels, self.image_size * 2, self.image_size * 2)\n-        )\n-        result = model(pixel_values, labels=pixel_labels)\n-        self.parent.assertEqual(\n-            result.logits.shape, (self.batch_size, self.num_labels, self.image_size * 2, self.image_size * 2)\n-        )\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values, labels, pixel_labels = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n-        return config, inputs_dict\n-\n-    def prepare_config_and_inputs_for_keras_fit(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values, _, _ = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values, \"labels\": tf.zeros(self.batch_size)}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFData2VecVisionModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    \"\"\"\n-    Here we also overwrite some of the tests of test_modeling_common.py, as Data2VecVision does not use input_ids, inputs_embeds,\n-    attention_mask and seq_length.\n-    \"\"\"\n-\n-    all_model_classes = (\n-        (TFData2VecVisionModel, TFData2VecVisionForImageClassification, TFData2VecVisionForSemanticSegmentation)\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\"feature-extraction\": TFData2VecVisionModel, \"image-classification\": TFData2VecVisionForImageClassification}\n-        if is_tf_available()\n-        else {}\n-    )\n-\n-    test_pruning = False\n-    test_onnx = False\n-    test_resize_embeddings = False\n-    test_head_masking = False\n-\n-    def setUp(self):\n-        self.model_tester = TFData2VecVisionModelTester(self)\n-        self.config_tester = ConfigTester(\n-            self, config_class=Data2VecVisionConfig, has_text_modality=False, hidden_size=37\n-        )\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    @unittest.skip(reason=\"Data2VecVision does not use inputs_embeds\")\n-    def test_inputs_embeds(self):\n-        # Data2VecVision does not use inputs_embeds\n-        pass\n-\n-    def test_model_common_attributes(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            self.assertIsInstance(model.get_input_embeddings(), (keras.layers.Layer))\n-            x = model.get_output_embeddings()\n-            self.assertTrue(x is None or isinstance(x, keras.layers.Layer))\n-\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.call)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"pixel_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_for_image_segmentation(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_image_segmentation(*config_and_inputs)\n-\n-    def test_attention_outputs(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.return_dict = True\n-\n-        # in Data2VecVision, the seq_len equals the number of patches + 1 (we add 1 for the [CLS] token)\n-        image_size = (\n-            self.model_tester.image_size\n-            if isinstance(self.model_tester.image_size, collections.abc.Iterable)\n-            else (self.model_tester.image_size, self.model_tester.image_size)\n-        )\n-        patch_size = (\n-            self.model_tester.patch_size\n-            if isinstance(self.model_tester.patch_size, collections.abc.Iterable)\n-            else (self.model_tester.patch_size, self.model_tester.patch_size)\n-        )\n-        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-        seq_len = num_patches + 1\n-        encoder_seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_len)\n-        encoder_key_length = getattr(self.model_tester, \"key_length\", encoder_seq_length)\n-        chunk_length = getattr(self.model_tester, \"chunk_length\", None)\n-        if chunk_length is not None and hasattr(self.model_tester, \"num_hashes\"):\n-            encoder_seq_length = encoder_seq_length * self.model_tester.num_hashes\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_attentions\"] = True\n-            inputs_dict[\"output_hidden_states\"] = False\n-            config.return_dict = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class), training=False)\n-            attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n-            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n-\n-            # check that output_attentions also work using config\n-            del inputs_dict[\"output_attentions\"]\n-            config.output_attentions = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class), training=False)\n-            attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n-            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n-\n-            self.assertListEqual(\n-                list(attentions[0].shape[-3:]),\n-                [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n-            )\n-            out_len = len(outputs)\n-\n-            # Check attention is always last and order is fine\n-            inputs_dict[\"output_attentions\"] = True\n-            inputs_dict[\"output_hidden_states\"] = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class), training=False)\n-\n-            self.assertEqual(out_len + 1, len(outputs))\n-\n-            self_attentions = outputs.attentions\n-\n-            self.assertEqual(len(self_attentions), self.model_tester.num_hidden_layers)\n-            self.assertListEqual(\n-                list(self_attentions[0].shape[-3:]),\n-                [self.model_tester.num_attention_heads, encoder_seq_length, encoder_key_length],\n-            )\n-\n-    def test_hidden_states_output(self):\n-        def check_hidden_states_output(inputs_dict, config, model_class):\n-            model = model_class(config)\n-\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-\n-            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n-\n-            expected_num_layers = getattr(\n-                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n-            )\n-            self.assertEqual(len(hidden_states), expected_num_layers)\n-\n-            # Data2VecVision has a different seq_length\n-            image_size = (\n-                self.model_tester.image_size\n-                if isinstance(self.model_tester.image_size, collections.abc.Iterable)\n-                else (self.model_tester.image_size, self.model_tester.image_size)\n-            )\n-            patch_size = (\n-                self.model_tester.patch_size\n-                if isinstance(self.model_tester.patch_size, collections.abc.Iterable)\n-                else (self.model_tester.patch_size, self.model_tester.patch_size)\n-            )\n-            num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-            seq_length = num_patches + 1\n-\n-            self.assertListEqual(\n-                list(hidden_states[0].shape[-2:]),\n-                [seq_length, self.model_tester.hidden_size],\n-            )\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_hidden_states\"] = True\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-            # check that output_hidden_states also work using config\n-            del inputs_dict[\"output_hidden_states\"]\n-            config.output_hidden_states = True\n-\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-    # Overriding this method since the base method won't be compatible with Data2VecVision.\n-    @slow\n-    def test_keras_fit(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            # Since `TFData2VecVisionModel` cannot operate with the default `fit()` method.\n-            if model_class.__name__ != \"TFData2VecVisionModel\":\n-                model = model_class(config)\n-                if getattr(model, \"hf_compute_loss\", None):\n-                    # Test that model correctly compute the loss with kwargs\n-                    _, prepared_for_class = self.model_tester.prepare_config_and_inputs_for_keras_fit()\n-\n-                    label_names = {\"labels\"}\n-                    self.assertGreater(len(label_names), 0, msg=\"No matching label names found!\")\n-                    labels = {key: val for key, val in prepared_for_class.items() if key in label_names}\n-                    inputs_minus_labels = {\n-                        key: val for key, val in prepared_for_class.items() if key not in label_names\n-                    }\n-                    self.assertGreater(len(inputs_minus_labels), 0)\n-                    model.compile(optimizer=keras.optimizers.SGD(0.0), run_eagerly=True)\n-\n-                    # Make sure the model fits without crashing regardless of where we pass the labels\n-                    history1 = model.fit(\n-                        prepared_for_class,\n-                        validation_data=prepared_for_class,\n-                        steps_per_epoch=1,\n-                        validation_steps=1,\n-                        shuffle=False,\n-                    )\n-                    val_loss1 = history1.history[\"val_loss\"][0]\n-                    history2 = model.fit(\n-                        inputs_minus_labels,\n-                        labels,\n-                        validation_data=(inputs_minus_labels, labels),\n-                        steps_per_epoch=1,\n-                        validation_steps=1,\n-                        shuffle=False,\n-                    )\n-                    val_loss2 = history2.history[\"val_loss\"][0]\n-                    self.assertTrue(np.allclose(val_loss1, val_loss2, atol=1e-2, rtol=1e-3))\n-\n-    # Overriding this method since the base method won't be compatible with Data2VecVision.\n-    def test_loss_computation(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            # Since `TFData2VecVisionModel` won't have labels against which we\n-            # could compute loss.\n-            if model_class.__name__ != \"TFData2VecVisionModel\":\n-                model = model_class(config)\n-                if getattr(model, \"hf_compute_loss\", None):\n-                    # The number of elements in the loss should be the same as the number of elements in the label\n-                    _, prepared_for_class = self.model_tester.prepare_config_and_inputs_for_keras_fit()\n-                    added_label = prepared_for_class[\n-                        sorted(prepared_for_class.keys() - inputs_dict.keys(), reverse=True)[0]\n-                    ]\n-                    loss_size = tf.size(added_label)\n-\n-                    # Test that model correctly compute the loss with kwargs\n-                    possible_input_names = {\"input_ids\", \"pixel_values\", \"input_features\"}\n-                    input_name = possible_input_names.intersection(set(prepared_for_class)).pop()\n-                    model_input = prepared_for_class.pop(input_name)\n-\n-                    loss = model(model_input, **prepared_for_class)[0]\n-                    self.assertEqual(loss.shape, [loss_size])\n-\n-                    # Test that model correctly compute the loss with a dict\n-                    _, prepared_for_class = self.model_tester.prepare_config_and_inputs_for_keras_fit()\n-                    loss = model(**prepared_for_class)[0]\n-                    self.assertEqual(loss.shape, [loss_size])\n-\n-                    # Test that model correctly compute the loss with a tuple\n-                    label_keys = prepared_for_class.keys() - inputs_dict.keys()\n-                    signature = inspect.signature(model.call).parameters\n-                    signature_names = list(signature.keys())\n-\n-                    # Create a dictionary holding the location of the tensors in the tuple\n-                    tuple_index_mapping = {0: input_name}\n-                    for label_key in label_keys:\n-                        label_key_index = signature_names.index(label_key)\n-                        tuple_index_mapping[label_key_index] = label_key\n-                    sorted_tuple_index_mapping = sorted(tuple_index_mapping.items())\n-                    # Initialize a list with their default values, update the values and convert to a tuple\n-                    list_input = []\n-\n-                    for name in signature_names:\n-                        if name != \"kwargs\":\n-                            list_input.append(signature[name].default)\n-\n-                    for index, value in sorted_tuple_index_mapping:\n-                        list_input[index] = prepared_for_class[value]\n-\n-                    tuple_input = tuple(list_input)\n-\n-                    # Send to model\n-                    loss = model(tuple_input[:-1])[0]\n-\n-                    self.assertEqual(loss.shape, [loss_size])\n-\n-    def test_for_image_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"facebook/data2vec-vision-base-ft1k\"\n-        model = TFData2VecVisionModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n-    return image\n-\n-\n-@require_tf\n-@require_vision\n-class TFData2VecVisionModelIntegrationTest(unittest.TestCase):\n-    @cached_property\n-    def default_image_processor(self):\n-        return (\n-            BeitImageProcessor.from_pretrained(\"facebook/data2vec-vision-base-ft1k\") if is_vision_available() else None\n-        )\n-\n-    @slow\n-    def test_inference_image_classification_head_imagenet_1k(self):\n-        model = TFData2VecVisionForImageClassification.from_pretrained(\"facebook/data2vec-vision-base-ft1k\")\n-\n-        image_processor = self.default_image_processor\n-        image = prepare_img()\n-        inputs = image_processor(images=image, return_tensors=\"tf\")\n-\n-        # forward pass\n-        outputs = model(**inputs)\n-        logits = outputs.logits\n-\n-        # verify the logits\n-        expected_shape = tf.convert_to_tensor([1, 1000])\n-        self.assertEqual(logits.shape, expected_shape)\n-\n-        expected_slice = tf.convert_to_tensor([0.3277, -0.1395, 0.0911])\n-\n-        tf.debugging.assert_near(logits[0, :3], expected_slice, atol=1e-4)\n-\n-        expected_top2 = [model.config.label2id[i] for i in [\"remote control, remote\", \"tabby, tabby cat\"]]\n-        self.assertEqual(tf.nn.top_k(outputs.logits[0], 2).indices.numpy().tolist(), expected_top2)"
        },
        {
            "sha": "ea1e716dd66c346e12006c051501f00fc2f84062",
            "filename": "tests/models/deberta/test_modeling_tf_deberta.py",
            "status": "removed",
            "additions": 0,
            "deletions": 295,
            "changes": 295,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdeberta%2Ftest_modeling_tf_deberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdeberta%2Ftest_modeling_tf_deberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta%2Ftest_modeling_tf_deberta.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,295 +0,0 @@\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import DebertaConfig, is_tf_available\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import (\n-        TFDebertaForMaskedLM,\n-        TFDebertaForQuestionAnswering,\n-        TFDebertaForSequenceClassification,\n-        TFDebertaForTokenClassification,\n-        TFDebertaModel,\n-    )\n-\n-\n-class TFDebertaModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = 13\n-        self.seq_length = 7\n-        self.is_training = True\n-        self.use_input_mask = True\n-        self.use_token_type_ids = True\n-        self.use_labels = True\n-        self.vocab_size = 99\n-        self.hidden_size = 32\n-        self.num_hidden_layers = 2\n-        self.num_attention_heads = 4\n-        self.intermediate_size = 37\n-        self.hidden_act = \"gelu\"\n-        self.hidden_dropout_prob = 0.1\n-        self.attention_probs_dropout_prob = 0.1\n-        self.max_position_embeddings = 512\n-        self.type_vocab_size = 16\n-        self.relative_attention = False\n-        self.max_relative_positions = -1\n-        self.position_biased_input = True\n-        self.type_sequence_label_size = 2\n-        self.initializer_range = 0.02\n-        self.num_labels = 3\n-        self.num_choices = 4\n-        self.scope = None\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-\n-        config = DebertaConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            relative_attention=self.relative_attention,\n-            max_relative_positions=self.max_relative_positions,\n-            position_biased_input=self.position_biased_input,\n-            initializer_range=self.initializer_range,\n-            return_dict=True,\n-        )\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDebertaModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_masked_lm(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDebertaForMaskedLM(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_for_sequence_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFDebertaForSequenceClassification(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_for_token_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFDebertaForTokenClassification(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def create_and_check_for_question_answering(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDebertaForQuestionAnswering(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-\n-        result = model(inputs)\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFDebertaModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFDebertaModel,\n-            TFDebertaForMaskedLM,\n-            TFDebertaForQuestionAnswering,\n-            TFDebertaForSequenceClassification,\n-            TFDebertaForTokenClassification,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFDebertaModel,\n-            \"fill-mask\": TFDebertaForMaskedLM,\n-            \"question-answering\": TFDebertaForQuestionAnswering,\n-            \"text-classification\": TFDebertaForSequenceClassification,\n-            \"token-classification\": TFDebertaForTokenClassification,\n-            \"zero-shot\": TFDebertaForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFDebertaModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=DebertaConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_for_masked_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n-\n-    def test_for_question_answering(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_question_answering(*config_and_inputs)\n-\n-    def test_for_sequence_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n-\n-    def test_for_token_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_token_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model = TFDebertaModel.from_pretrained(\"kamalkraj/deberta-base\")\n-        self.assertIsNotNone(model)\n-\n-\n-@require_tf\n-class TFDeBERTaModelIntegrationTest(unittest.TestCase):\n-    @unittest.skip(reason=\"Model not available yet\")\n-    def test_inference_masked_lm(self):\n-        pass\n-\n-    @slow\n-    def test_inference_no_head(self):\n-        model = TFDebertaModel.from_pretrained(\"kamalkraj/deberta-base\")\n-        input_ids = tf.constant([[0, 31414, 232, 328, 740, 1140, 12695, 69, 46078, 1588, 2]])\n-        attention_mask = tf.constant([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n-        output = model(input_ids, attention_mask=attention_mask)[0]\n-\n-        expected_slice = tf.constant(\n-            [\n-                [\n-                    [-0.59855896, -0.80552566, -0.8462135],\n-                    [1.4484025, -0.93483794, -0.80593085],\n-                    [0.3122741, 0.00316059, -1.4131377],\n-                ]\n-            ]\n-        )\n-        tf.debugging.assert_near(output[:, 1:4, 1:4], expected_slice, atol=1e-4)"
        },
        {
            "sha": "b69e2eb489880ad7823151f01e52d8389affe26e",
            "filename": "tests/models/deberta_v2/test_modeling_tf_deberta_v2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 309,
            "changes": 309,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_tf_deberta_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_tf_deberta_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeberta_v2%2Ftest_modeling_tf_deberta_v2.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,309 +0,0 @@\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import DebertaV2Config, is_tf_available\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import (\n-        TFDebertaV2ForMaskedLM,\n-        TFDebertaV2ForMultipleChoice,\n-        TFDebertaV2ForQuestionAnswering,\n-        TFDebertaV2ForSequenceClassification,\n-        TFDebertaV2ForTokenClassification,\n-        TFDebertaV2Model,\n-    )\n-\n-\n-class TFDebertaV2ModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        relative_attention=False,\n-        position_biased_input=True,\n-        pos_att_type=\"None\",\n-        num_labels=3,\n-        num_choices=4,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.relative_attention = relative_attention\n-        self.position_biased_input = position_biased_input\n-        self.pos_att_type = pos_att_type\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-\n-        config = DebertaV2Config(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            relative_attention=self.relative_attention,\n-            position_biased_input=self.position_biased_input,\n-            initializer_range=self.initializer_range,\n-            return_dict=True,\n-        )\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDebertaV2Model(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_masked_lm(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDebertaV2ForMaskedLM(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_for_sequence_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFDebertaV2ForSequenceClassification(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_for_token_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFDebertaV2ForTokenClassification(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def create_and_check_for_question_answering(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDebertaV2ForQuestionAnswering(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-\n-        result = model(inputs)\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-\n-    def create_and_check_for_multiple_choice(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_choices = self.num_choices\n-        model = TFDebertaV2ForMultipleChoice(config=config)\n-        multiple_choice_inputs_ids = tf.tile(tf.expand_dims(input_ids, 1), (1, self.num_choices, 1))\n-        multiple_choice_input_mask = tf.tile(tf.expand_dims(input_mask, 1), (1, self.num_choices, 1))\n-        multiple_choice_token_type_ids = tf.tile(tf.expand_dims(token_type_ids, 1), (1, self.num_choices, 1))\n-        inputs = {\n-            \"input_ids\": multiple_choice_inputs_ids,\n-            \"attention_mask\": multiple_choice_input_mask,\n-            \"token_type_ids\": multiple_choice_token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_choices))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFDebertaModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFDebertaV2Model,\n-            TFDebertaV2ForMaskedLM,\n-            TFDebertaV2ForQuestionAnswering,\n-            TFDebertaV2ForMultipleChoice,\n-            TFDebertaV2ForSequenceClassification,\n-            TFDebertaV2ForTokenClassification,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFDebertaV2Model,\n-            \"fill-mask\": TFDebertaV2ForMaskedLM,\n-            \"question-answering\": TFDebertaV2ForQuestionAnswering,\n-            \"text-classification\": TFDebertaV2ForSequenceClassification,\n-            \"token-classification\": TFDebertaV2ForTokenClassification,\n-            \"zero-shot\": TFDebertaV2ForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFDebertaV2ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=DebertaV2Config, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_for_masked_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n-\n-    def test_for_question_answering(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_question_answering(*config_and_inputs)\n-\n-    def test_for_sequence_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n-\n-    def test_for_token_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_token_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model = TFDebertaV2Model.from_pretrained(\"kamalkraj/deberta-v2-xlarge\")\n-        self.assertIsNotNone(model)\n-\n-\n-@require_tf\n-class TFDeBERTaV2ModelIntegrationTest(unittest.TestCase):\n-    @unittest.skip(reason=\"Model not available yet\")\n-    def test_inference_masked_lm(self):\n-        pass\n-\n-    @slow\n-    def test_inference_no_head(self):\n-        model = TFDebertaV2Model.from_pretrained(\"kamalkraj/deberta-v2-xlarge\")\n-        input_ids = tf.constant([[0, 31414, 232, 328, 740, 1140, 12695, 69, 46078, 1588, 2]])\n-        attention_mask = tf.constant([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n-        output = model(input_ids, attention_mask=attention_mask)[0]\n-\n-        expected_slice = tf.constant(\n-            [[[0.2356, 0.1948, 0.0369], [-0.1063, 0.3586, -0.5152], [-0.6399, -0.0259, -0.2525]]]\n-        )\n-        tf.debugging.assert_near(output[:, 1:4, 1:4], expected_slice, atol=1e-4)"
        },
        {
            "sha": "1ca091f52695066f3192ac2851b50fb072bd8f98",
            "filename": "tests/models/deit/test_modeling_tf_deit.py",
            "status": "removed",
            "additions": 0,
            "deletions": 311,
            "changes": 311,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdeit%2Ftest_modeling_tf_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdeit%2Ftest_modeling_tf_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeit%2Ftest_modeling_tf_deit.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,311 +0,0 @@\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Testing suite for the TensorFlow DeiT model.\"\"\"\n-\n-from __future__ import annotations\n-\n-import inspect\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import DeiTConfig\n-from transformers.testing_utils import require_tf, require_vision, slow\n-from transformers.utils import cached_property, is_tf_available, is_vision_available\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import (\n-        TFDeiTForImageClassification,\n-        TFDeiTForImageClassificationWithTeacher,\n-        TFDeiTForMaskedImageModeling,\n-        TFDeiTModel,\n-    )\n-    from transformers.modeling_tf_utils import keras\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-    from transformers import DeiTImageProcessor\n-\n-\n-class TFDeiTModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        image_size=30,\n-        patch_size=2,\n-        num_channels=3,\n-        is_training=True,\n-        use_labels=True,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        type_sequence_label_size=10,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        scope=None,\n-        encoder_stride=2,\n-        attn_implementation=\"eager\",\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.image_size = image_size\n-        self.patch_size = patch_size\n-        self.num_channels = num_channels\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-        self.encoder_stride = encoder_stride\n-        self.attn_implementation = attn_implementation\n-\n-        # in DeiT, the seq length equals the number of patches + 2 (we add 2 for the [CLS] and distilation tokens)\n-        num_patches = (image_size // patch_size) ** 2\n-        self.seq_length = num_patches + 2\n-\n-    def prepare_config_and_inputs(self):\n-        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-\n-        labels = None\n-        if self.use_labels:\n-            labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-\n-        config = self.get_config()\n-\n-        return config, pixel_values, labels\n-\n-    def get_config(self):\n-        return DeiTConfig(\n-            image_size=self.image_size,\n-            patch_size=self.patch_size,\n-            num_channels=self.num_channels,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            encoder_stride=self.encoder_stride,\n-            attn_implementation=self.attn_implementation,\n-        )\n-\n-    def create_and_check_model(self, config, pixel_values, labels):\n-        model = TFDeiTModel(config=config)\n-        result = model(pixel_values)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_masked_image_modeling(self, config, pixel_values, labels):\n-        model = TFDeiTForMaskedImageModeling(config=config)\n-        result = model(pixel_values)\n-        self.parent.assertEqual(\n-            result.reconstruction.shape, (self.batch_size, self.num_channels, self.image_size, self.image_size)\n-        )\n-\n-        # test greyscale images\n-        config.num_channels = 1\n-        model = TFDeiTForMaskedImageModeling(config)\n-\n-        pixel_values = floats_tensor([self.batch_size, 1, self.image_size, self.image_size])\n-        result = model(pixel_values)\n-        self.parent.assertEqual(result.reconstruction.shape, (self.batch_size, 1, self.image_size, self.image_size))\n-\n-    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n-        config.num_labels = self.type_sequence_label_size\n-        model = TFDeiTForImageClassification(config)\n-        result = model(pixel_values, labels=labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n-\n-        # test greyscale images\n-        config.num_channels = 1\n-        model = TFDeiTForImageClassification(config)\n-\n-        pixel_values = floats_tensor([self.batch_size, 1, self.image_size, self.image_size])\n-        result = model(pixel_values, labels=labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values, labels = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFDeiTModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    \"\"\"\n-    Here we also overwrite some of the tests of test_modeling_tf_common.py, as DeiT does not use input_ids, inputs_embeds,\n-    attention_mask and seq_length.\n-    \"\"\"\n-\n-    all_model_classes = (\n-        (\n-            TFDeiTModel,\n-            TFDeiTForImageClassification,\n-            TFDeiTForImageClassificationWithTeacher,\n-            TFDeiTForMaskedImageModeling,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFDeiTModel,\n-            \"image-classification\": (TFDeiTForImageClassification, TFDeiTForImageClassificationWithTeacher),\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-\n-    test_pruning = False\n-    test_resize_embeddings = False\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFDeiTModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=DeiTConfig, has_text_modality=False, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    @unittest.skip(reason=\"DeiT does not use inputs_embeds\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    def test_model_common_attributes(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            self.assertIsInstance(model.get_input_embeddings(), (keras.layers.Layer))\n-            x = model.get_output_embeddings()\n-            self.assertTrue(x is None or isinstance(x, keras.layers.Dense))\n-\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.call)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"pixel_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_for_masked_image_modeling(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_masked_image_modeling(*config_and_inputs)\n-\n-    def test_for_image_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n-\n-    # special case for DeiTForImageClassificationWithTeacher model\n-    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n-        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n-\n-        if return_labels:\n-            if \"labels\" in inputs_dict and \"labels\" not in inspect.signature(model_class.call).parameters:\n-                del inputs_dict[\"labels\"]\n-\n-        return inputs_dict\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"facebook/deit-base-distilled-patch16-224\"\n-        model = TFDeiTModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n-    return image\n-\n-\n-@require_tf\n-@require_vision\n-class DeiTModelIntegrationTest(unittest.TestCase):\n-    @cached_property\n-    def default_image_processor(self):\n-        return (\n-            DeiTImageProcessor.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\n-            if is_vision_available()\n-            else None\n-        )\n-\n-    @slow\n-    def test_inference_image_classification_head(self):\n-        model = TFDeiTForImageClassificationWithTeacher.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\n-\n-        image_processor = self.default_image_processor\n-        image = prepare_img()\n-        inputs = image_processor(images=image, return_tensors=\"tf\")\n-\n-        # forward pass\n-        outputs = model(**inputs)\n-\n-        # verify the logits\n-        expected_shape = tf.TensorShape((1, 1000))\n-        self.assertEqual(outputs.logits.shape, expected_shape)\n-\n-        expected_slice = tf.constant([-1.0266, 0.1912, -1.2861])\n-\n-        self.assertTrue(np.allclose(outputs.logits[0, :3], expected_slice, atol=1e-4))\n-\n-    @slow\n-    def test_inference_interpolate_pos_encoding(self):\n-        model = TFDeiTForImageClassificationWithTeacher.from_pretrained(\"facebook/deit-base-distilled-patch16-224\")\n-\n-        image_processor = self.default_image_processor\n-        # image size is {\"height\": 480, \"width\": 640}\n-        image = prepare_img()\n-        image_processor.size = {\"height\": 480, \"width\": 640}\n-        # center crop set to False so image is not center cropped to 224x224\n-        inputs = image_processor(images=image, return_tensors=\"tf\", do_center_crop=False)\n-        # forward pass\n-        outputs = model(**inputs, interpolate_pos_encoding=True)\n-\n-        # verify the logits\n-        expected_shape = tf.TensorShape((1, 1000))\n-        self.assertEqual(outputs.logits.shape, expected_shape)"
        },
        {
            "sha": "161e49e3db948d22534adf7a9f8f544be2af746f",
            "filename": "tests/models/dinov2/test_modeling_flax_dinov2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 270,
            "changes": 270,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdinov2%2Ftest_modeling_flax_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdinov2%2Ftest_modeling_flax_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2%2Ftest_modeling_flax_dinov2.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,270 +0,0 @@\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Testing suite for the Flax Dinov2 model.\"\"\"\n-\n-import inspect\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import Dinov2Config\n-from transformers.testing_utils import require_flax, require_vision, slow\n-from transformers.utils import cached_property, is_flax_available, is_vision_available\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, floats_tensor\n-\n-\n-if is_flax_available():\n-    import jax\n-\n-    from transformers.models.dinov2.modeling_flax_dinov2 import FlaxDinov2ForImageClassification, FlaxDinov2Model\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-    from transformers import AutoImageProcessor\n-\n-\n-class FlaxDinov2ModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=2,\n-        image_size=30,\n-        patch_size=2,\n-        num_channels=3,\n-        is_training=True,\n-        use_labels=True,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        type_sequence_label_size=10,\n-        initializer_range=0.02,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.image_size = image_size\n-        self.patch_size = patch_size\n-        self.num_channels = num_channels\n-        self.is_training = is_training\n-        self.use_labels = use_labels\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-\n-        # in Dinov2, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n-        num_patches = (image_size // patch_size) ** 2\n-        self.seq_length = num_patches + 1\n-\n-    def prepare_config_and_inputs(self):\n-        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-\n-        config = Dinov2Config(\n-            image_size=self.image_size,\n-            patch_size=self.patch_size,\n-            num_channels=self.num_channels,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, pixel_values\n-\n-    # Copied from transformers.models.vit.test_modeling_flax_vit.FlaxViTModelTester.prepare_config_and_inputs with ViT -> Dinov2\n-    def create_and_check_model(self, config, pixel_values):\n-        model = FlaxDinov2Model(config=config)\n-        result = model(pixel_values)\n-        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n-        image_size = (self.image_size, self.image_size)\n-        patch_size = (self.patch_size, self.patch_size)\n-        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n-\n-    # Copied from transformers.models.vit.test_modeling_flax_vit.FlaxViTModelTester.create_and_check_for_image_classification with ViT -> Dinov2\n-    def create_and_check_for_image_classification(self, config, pixel_values):\n-        config.num_labels = self.type_sequence_label_size\n-        model = FlaxDinov2ForImageClassification(config=config)\n-        result = model(pixel_values)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n-\n-        # test greyscale images\n-        config.num_channels = 1\n-        model = FlaxDinov2ForImageClassification(config)\n-\n-        pixel_values = floats_tensor([self.batch_size, 1, self.image_size, self.image_size])\n-        result = model(pixel_values)\n-\n-    # Copied from transformers.models.vit.test_modeling_flax_vit.FlaxViTModelTester.prepare_config_and_inputs_for_common\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            pixel_values,\n-        ) = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n-        return config, inputs_dict\n-\n-\n-@require_flax\n-# Copied from transformers.models.vit.test_modeling_flax_vit.FlaxViTModelTest with google/vit-base-patch16-224 -> facebook/dinov2-base\n-class FlaxDionv2ModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (FlaxDinov2Model, FlaxDinov2ForImageClassification) if is_flax_available() else ()\n-\n-    def setUp(self) -> None:\n-        self.model_tester = FlaxDinov2ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=Dinov2Config, has_text_modality=False, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_for_image_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n-\n-    # We need to override this test because Dinov2's forward signature is different than text models.\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.__call__)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"pixel_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    # We need to override this test because Dinov2 expects pixel_values instead of input_ids\n-    def test_jit_compilation(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            with self.subTest(model_class.__name__):\n-                prepared_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-                model = model_class(config)\n-\n-                @jax.jit\n-                def model_jitted(pixel_values, **kwargs):\n-                    return model(pixel_values=pixel_values, **kwargs)\n-\n-                with self.subTest(\"JIT Enabled\"):\n-                    jitted_outputs = model_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                with self.subTest(\"JIT Disabled\"):\n-                    with jax.disable_jit():\n-                        outputs = model_jitted(**prepared_inputs_dict).to_tuple()\n-\n-                self.assertEqual(len(outputs), len(jitted_outputs))\n-                for jitted_output, output in zip(jitted_outputs, outputs):\n-                    self.assertEqual(jitted_output.shape, output.shape)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"facebook/dinov2-base\")\n-            outputs = model(np.ones((1, 3, 224, 224)))\n-            self.assertIsNotNone(outputs)\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n-    return [image, image]\n-\n-\n-@require_vision\n-@require_flax\n-class FlaxDinov2ModelIntegrationTest(unittest.TestCase):\n-    @cached_property\n-    def default_image_processor(self):\n-        return AutoImageProcessor.from_pretrained(\"facebook/dinov2-base\") if is_vision_available() else None\n-\n-    @slow\n-    def test_inference_no_head(self):\n-        model = FlaxDinov2Model.from_pretrained(\"facebook/dinov2-base\")\n-\n-        image_processor = self.default_image_processor\n-        image = prepare_img()\n-        pixel_values = image_processor(images=image, return_tensors=\"np\").pixel_values\n-\n-        # forward pass\n-        outputs = model(pixel_values=pixel_values)\n-\n-        # verify the logits\n-        expected_shape = (2, 257, 768)\n-        self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n-\n-        expected_slice = np.array(\n-            [\n-                [\n-                    [-2.1629121, -0.46566057, 1.0925977],\n-                    [-3.5971704, -1.0283585, -1.1780515],\n-                    [-2.900407, 1.1334689, -0.74357724],\n-                ],\n-                [\n-                    [-2.1629121, -0.46566057, 1.0925977],\n-                    [-3.5971704, -1.0283585, -1.1780515],\n-                    [-2.900407, 1.1334689, -0.74357724],\n-                ],\n-            ]\n-        )\n-\n-        self.assertTrue(np.allclose(outputs.last_hidden_state[:2, :3, :3], expected_slice, atol=1e-4))\n-\n-    @slow\n-    def test_inference_image_classification_head_imagenet_1k(self):\n-        model = FlaxDinov2ForImageClassification.from_pretrained(\n-            \"facebook/dinov2-base-imagenet1k-1-layer\", from_pt=True\n-        )\n-\n-        image_processor = self.default_image_processor\n-        image = prepare_img()\n-        inputs = image_processor(images=image, return_tensors=\"np\")\n-\n-        # forward pass\n-        outputs = model(**inputs)\n-        logits = outputs.logits\n-\n-        # verify the logits\n-        expected_shape = (2, 1000)\n-        self.assertEqual(logits.shape, expected_shape)\n-\n-        expected_slice = np.array([[-2.1776447, 0.36716992, 0.13870952], [-2.1776447, 0.36716992, 0.13870952]])\n-\n-        self.assertTrue(np.allclose(logits[:2, :3], expected_slice, atol=1e-3))\n-\n-        expected_class_idx = 281\n-        self.assertEqual(logits[0].argmax(-1).item(), expected_class_idx)\n-        self.assertEqual(logits[1].argmax(-1).item(), expected_class_idx)"
        },
        {
            "sha": "50655771ed14a575afd9388bab48d1440a291c67",
            "filename": "tests/models/distilbert/test_modeling_flax_distilbert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 152,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_flax_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_flax_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_flax_distilbert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,152 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import DistilBertConfig, is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor, random_attention_mask\n-\n-\n-if is_flax_available():\n-    import jax.numpy as jnp\n-\n-    from transformers.models.distilbert.modeling_flax_distilbert import (\n-        FlaxDistilBertForMaskedLM,\n-        FlaxDistilBertForMultipleChoice,\n-        FlaxDistilBertForQuestionAnswering,\n-        FlaxDistilBertForSequenceClassification,\n-        FlaxDistilBertForTokenClassification,\n-        FlaxDistilBertModel,\n-    )\n-\n-\n-class FlaxDistilBertModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_attention_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_choices=4,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_attention_mask = use_attention_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_choices = num_choices\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        attention_mask = None\n-        if self.use_attention_mask:\n-            attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        config = DistilBertConfig(\n-            vocab_size=self.vocab_size,\n-            dim=self.hidden_size,\n-            n_layers=self.num_hidden_layers,\n-            n_heads=self.num_attention_heads,\n-            hidden_dim=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            dropout=self.hidden_dropout_prob,\n-            attention_dropout=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            initializer_range=self.initializer_range,\n-            tie_weights_=True,\n-        )\n-\n-        return config, input_ids, attention_mask\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-        return config, inputs_dict\n-\n-\n-@require_flax\n-class FlaxDistilBertModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            FlaxDistilBertModel,\n-            FlaxDistilBertForMaskedLM,\n-            FlaxDistilBertForMultipleChoice,\n-            FlaxDistilBertForQuestionAnswering,\n-            FlaxDistilBertForSequenceClassification,\n-            FlaxDistilBertForTokenClassification,\n-            FlaxDistilBertForQuestionAnswering,\n-        )\n-        if is_flax_available()\n-        else ()\n-    )\n-\n-    def setUp(self):\n-        self.model_tester = FlaxDistilBertModelTester(self)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"distilbert-base-uncased\")\n-            outputs = model(np.ones((1, 1)))\n-            self.assertIsNotNone(outputs)\n-\n-\n-@require_flax\n-class FlaxDistilBertModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_inference_no_head_absolute_embedding(self):\n-        model = FlaxDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n-        input_ids = np.array([[0, 345, 232, 328, 740, 140, 1695, 69, 6078, 1588, 2]])\n-        attention_mask = np.array([[0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]])\n-        output = model(input_ids, attention_mask=attention_mask)[0]\n-        expected_shape = (1, 11, 768)\n-        self.assertEqual(output.shape, expected_shape)\n-        expected_slice = np.array([[[-0.1639, 0.3299, 0.1648], [-0.1746, 0.3289, 0.1710], [-0.1884, 0.3357, 0.1810]]])\n-\n-        self.assertTrue(jnp.allclose(output[:, 1:4, 1:4], expected_slice, atol=1e-4))"
        },
        {
            "sha": "674acdad26e88d5f1d65477bb4383db48898a33d",
            "filename": "tests/models/distilbert/test_modeling_tf_distilbert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 259,
            "changes": 259,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_tf_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_tf_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_tf_distilbert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,259 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import DistilBertConfig, is_tf_available\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers.models.distilbert.modeling_tf_distilbert import (\n-        TFDistilBertForMaskedLM,\n-        TFDistilBertForMultipleChoice,\n-        TFDistilBertForQuestionAnswering,\n-        TFDistilBertForSequenceClassification,\n-        TFDistilBertForTokenClassification,\n-        TFDistilBertModel,\n-    )\n-\n-\n-class TFDistilBertModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-    ):\n-        self.parent = parent\n-        self.batch_size = 13\n-        self.seq_length = 7\n-        self.is_training = True\n-        self.use_input_mask = True\n-        self.use_token_type_ids = False\n-        self.use_labels = True\n-        self.vocab_size = 99\n-        self.hidden_size = 32\n-        self.num_hidden_layers = 2\n-        self.num_attention_heads = 4\n-        self.intermediate_size = 37\n-        self.hidden_act = \"gelu\"\n-        self.hidden_dropout_prob = 0.1\n-        self.attention_probs_dropout_prob = 0.1\n-        self.max_position_embeddings = 512\n-        self.type_vocab_size = 16\n-        self.type_sequence_label_size = 2\n-        self.initializer_range = 0.02\n-        self.num_labels = 3\n-        self.num_choices = 4\n-        self.scope = None\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = DistilBertConfig(\n-            vocab_size=self.vocab_size,\n-            dim=self.hidden_size,\n-            n_layers=self.num_hidden_layers,\n-            n_heads=self.num_attention_heads,\n-            hidden_dim=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            dropout=self.hidden_dropout_prob,\n-            attention_dropout=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def create_and_check_distilbert_model(\n-        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDistilBertModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-\n-        result = model(inputs)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_distilbert_for_masked_lm(\n-        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDistilBertForMaskedLM(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_distilbert_for_question_answering(\n-        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDistilBertForQuestionAnswering(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-\n-    def create_and_check_distilbert_for_sequence_classification(\n-        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFDistilBertForSequenceClassification(config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_distilbert_for_multiple_choice(\n-        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_choices = self.num_choices\n-        model = TFDistilBertForMultipleChoice(config)\n-        multiple_choice_inputs_ids = tf.tile(tf.expand_dims(input_ids, 1), (1, self.num_choices, 1))\n-        multiple_choice_input_mask = tf.tile(tf.expand_dims(input_mask, 1), (1, self.num_choices, 1))\n-        inputs = {\n-            \"input_ids\": multiple_choice_inputs_ids,\n-            \"attention_mask\": multiple_choice_input_mask,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_choices))\n-\n-    def create_and_check_distilbert_for_token_classification(\n-        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFDistilBertForTokenClassification(config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (config, input_ids, input_mask, sequence_labels, token_labels, choice_labels) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFDistilBertModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFDistilBertModel,\n-            TFDistilBertForMaskedLM,\n-            TFDistilBertForQuestionAnswering,\n-            TFDistilBertForSequenceClassification,\n-            TFDistilBertForTokenClassification,\n-            TFDistilBertForMultipleChoice,\n-        )\n-        if is_tf_available()\n-        else None\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFDistilBertModel,\n-            \"fill-mask\": TFDistilBertForMaskedLM,\n-            \"question-answering\": TFDistilBertForQuestionAnswering,\n-            \"text-classification\": TFDistilBertForSequenceClassification,\n-            \"token-classification\": TFDistilBertForTokenClassification,\n-            \"zero-shot\": TFDistilBertForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFDistilBertModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=DistilBertConfig, dim=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_distilbert_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_distilbert_model(*config_and_inputs)\n-\n-    def test_for_masked_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_distilbert_for_masked_lm(*config_and_inputs)\n-\n-    def test_for_question_answering(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_distilbert_for_question_answering(*config_and_inputs)\n-\n-    def test_for_sequence_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_distilbert_for_sequence_classification(*config_and_inputs)\n-\n-    def test_for_multiple_choice(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_distilbert_for_multiple_choice(*config_and_inputs)\n-\n-    def test_for_token_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_distilbert_for_token_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"distilbert/distilbert-base-cased\"\n-        model = TFDistilBertModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-\n-@require_tf\n-class TFDistilBertModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_inference_masked_lm(self):\n-        model = TFDistilBertModel.from_pretrained(\"distilbert-base-uncased\")\n-        input_ids = tf.constant([[0, 1, 2, 3, 4, 5]])\n-        output = model(input_ids)[0]\n-\n-        expected_shape = [1, 6, 768]\n-        self.assertEqual(output.shape, expected_shape)\n-\n-        expected_slice = tf.constant(\n-            [\n-                [\n-                    [0.19261885, -0.13732955, 0.4119799],\n-                    [0.22150156, -0.07422661, 0.39037204],\n-                    [0.22756018, -0.0896414, 0.3701467],\n-                ]\n-            ]\n-        )\n-        tf.debugging.assert_near(output[:, :3, :3], expected_slice, atol=1e-4)"
        },
        {
            "sha": "81427f3d945a7091cd6ba52cd51e3bc896ce09c0",
            "filename": "tests/models/dpr/test_modeling_tf_dpr.py",
            "status": "removed",
            "additions": 0,
            "deletions": 256,
            "changes": 256,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdpr%2Ftest_modeling_tf_dpr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fdpr%2Ftest_modeling_tf_dpr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpr%2Ftest_modeling_tf_dpr.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,256 +0,0 @@\n-# Copyright 2020 Huggingface\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import is_tf_available\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import numpy\n-    import tensorflow as tf\n-\n-    from transformers import (\n-        BertConfig,\n-        DPRConfig,\n-        TFDPRContextEncoder,\n-        TFDPRQuestionEncoder,\n-        TFDPRReader,\n-    )\n-\n-\n-class TFDPRModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        scope=None,\n-        projection_dim=0,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.scope = scope\n-        self.projection_dim = projection_dim\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            # follow test_modeling_tf_ctrl.py\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = BertConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-        )\n-        config = DPRConfig(projection_dim=self.projection_dim, **config.to_dict())\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def create_and_check_dpr_context_encoder(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDPRContextEncoder(config=config)\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n-        result = model(input_ids, token_type_ids=token_type_ids)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.projection_dim or self.hidden_size))\n-\n-    def create_and_check_dpr_question_encoder(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDPRQuestionEncoder(config=config)\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n-        result = model(input_ids, token_type_ids=token_type_ids)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.projection_dim or self.hidden_size))\n-\n-    def create_and_check_dpr_reader(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFDPRReader(config=config)\n-        result = model(input_ids, attention_mask=input_mask)\n-\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.relevance_logits.shape, (self.batch_size,))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFDPRModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFDPRContextEncoder,\n-            TFDPRQuestionEncoder,\n-            TFDPRReader,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = {\"feature-extraction\": TFDPRQuestionEncoder} if is_tf_available() else {}\n-\n-    test_resize_embeddings = False\n-    test_missing_keys = False\n-    test_pruning = False\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFDPRModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=DPRConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_dpr_context_encoder_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_dpr_context_encoder(*config_and_inputs)\n-\n-    def test_dpr_question_encoder_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_dpr_question_encoder(*config_and_inputs)\n-\n-    def test_dpr_reader_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_dpr_reader(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"facebook/dpr-ctx_encoder-single-nq-base\"\n-        model = TFDPRContextEncoder.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-        model_name = \"facebook/dpr-ctx_encoder-single-nq-base\"\n-        model = TFDPRContextEncoder.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-        model_name = \"facebook/dpr-ctx_encoder-single-nq-base\"\n-        model = TFDPRQuestionEncoder.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-        model_name = \"facebook/dpr-ctx_encoder-single-nq-base\"\n-        model = TFDPRReader.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-\n-@require_tf\n-class TFDPRModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_inference_no_head(self):\n-        model = TFDPRQuestionEncoder.from_pretrained(\"facebook/dpr-question_encoder-single-nq-base\")\n-\n-        input_ids = tf.constant(\n-            [[101, 7592, 1010, 2003, 2026, 3899, 10140, 1029, 102]]\n-        )  # [CLS] hello, is my dog cute? [SEP]\n-        output = model(input_ids)[0]  # embedding shape = (1, 768)\n-        # compare the actual values for a slice.\n-        expected_slice = tf.constant(\n-            [\n-                [\n-                    0.03236253,\n-                    0.12753335,\n-                    0.16818509,\n-                    0.00279786,\n-                    0.3896933,\n-                    0.24264945,\n-                    0.2178971,\n-                    -0.02335227,\n-                    -0.08481959,\n-                    -0.14324117,\n-                ]\n-            ]\n-        )\n-        self.assertTrue(numpy.allclose(output[:, :10].numpy(), expected_slice.numpy(), atol=1e-4))"
        },
        {
            "sha": "698a492fc3c7972f7fdbdeca6ba53537e3819d0d",
            "filename": "tests/models/electra/test_modeling_flax_electra.py",
            "status": "removed",
            "additions": 0,
            "deletions": 136,
            "changes": 136,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Felectra%2Ftest_modeling_flax_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Felectra%2Ftest_modeling_flax_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_modeling_flax_electra.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,136 +0,0 @@\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import ElectraConfig, is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor, random_attention_mask\n-\n-\n-if is_flax_available():\n-    from transformers.models.electra.modeling_flax_electra import (\n-        FlaxElectraForCausalLM,\n-        FlaxElectraForMaskedLM,\n-        FlaxElectraForMultipleChoice,\n-        FlaxElectraForPreTraining,\n-        FlaxElectraForQuestionAnswering,\n-        FlaxElectraForSequenceClassification,\n-        FlaxElectraForTokenClassification,\n-        FlaxElectraModel,\n-    )\n-\n-\n-class FlaxElectraModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_attention_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        embedding_size=24,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_choices=4,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_attention_mask = use_attention_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.embedding_size = embedding_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_choices = num_choices\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        attention_mask = None\n-        if self.use_attention_mask:\n-            attention_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        config = ElectraConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            embedding_size=self.embedding_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, input_ids, token_type_ids, attention_mask\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, token_type_ids, attention_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": attention_mask}\n-        return config, inputs_dict\n-\n-\n-@require_flax\n-class FlaxElectraModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    test_head_masking = True\n-\n-    all_model_classes = (\n-        (\n-            FlaxElectraModel,\n-            FlaxElectraForCausalLM,\n-            FlaxElectraForMaskedLM,\n-            FlaxElectraForPreTraining,\n-            FlaxElectraForTokenClassification,\n-            FlaxElectraForQuestionAnswering,\n-            FlaxElectraForMultipleChoice,\n-            FlaxElectraForSequenceClassification,\n-        )\n-        if is_flax_available()\n-        else ()\n-    )\n-\n-    def setUp(self):\n-        self.model_tester = FlaxElectraModelTester(self)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            if model_class_name == FlaxElectraForMaskedLM:\n-                model = model_class_name.from_pretrained(\"google/electra-small-generator\")\n-            else:\n-                model = model_class_name.from_pretrained(\"google/electra-small-discriminator\")\n-            outputs = model(np.ones((1, 1)))\n-            self.assertIsNotNone(outputs)"
        },
        {
            "sha": "de9e61ea5408caa57e220b92f3b615f0e17da0bc",
            "filename": "tests/models/electra/test_modeling_tf_electra.py",
            "status": "removed",
            "additions": 0,
            "deletions": 615,
            "changes": 615,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Felectra%2Ftest_modeling_tf_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Felectra%2Ftest_modeling_tf_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_modeling_tf_electra.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,615 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import ElectraConfig, is_tf_available\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers.models.electra.modeling_tf_electra import (\n-        TFElectraForMaskedLM,\n-        TFElectraForMultipleChoice,\n-        TFElectraForPreTraining,\n-        TFElectraForQuestionAnswering,\n-        TFElectraForSequenceClassification,\n-        TFElectraForTokenClassification,\n-        TFElectraModel,\n-    )\n-\n-\n-class TFElectraModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-    ):\n-        self.parent = parent\n-        self.batch_size = 13\n-        self.seq_length = 7\n-        self.is_training = True\n-        self.use_input_mask = True\n-        self.use_token_type_ids = True\n-        self.use_labels = True\n-        self.vocab_size = 99\n-        self.hidden_size = 32\n-        self.num_hidden_layers = 2\n-        self.num_attention_heads = 4\n-        self.intermediate_size = 37\n-        self.hidden_act = \"gelu\"\n-        self.hidden_dropout_prob = 0.1\n-        self.attention_probs_dropout_prob = 0.1\n-        self.max_position_embeddings = 512\n-        self.type_vocab_size = 16\n-        self.type_sequence_label_size = 2\n-        self.initializer_range = 0.02\n-        self.num_labels = 3\n-        self.num_choices = 4\n-        self.scope = None\n-        self.embedding_size = 128\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = ElectraConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        config.is_decoder = True\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n-    def create_and_check_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFElectraModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_causal_lm_base_model(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.is_decoder = True\n-\n-        model = TFElectraModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-\n-        model = TFElectraModel(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-            \"encoder_hidden_states\": encoder_hidden_states,\n-            \"encoder_attention_mask\": encoder_attention_mask,\n-        }\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs, token_type_ids=token_type_ids, encoder_hidden_states=encoder_hidden_states)\n-\n-        # Also check the case where encoder outputs are not passed\n-        result = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_causal_lm_base_model_past(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        config.is_decoder = True\n-\n-        model = TFElectraModel(config=config)\n-\n-        # first forward pass\n-        outputs = model(input_ids, use_cache=True)\n-        outputs_use_cache_conf = model(input_ids)\n-        outputs_no_past = model(input_ids, use_cache=False)\n-\n-        self.parent.assertTrue(len(outputs) == len(outputs_use_cache_conf))\n-        self.parent.assertTrue(len(outputs) == len(outputs_no_past) + 1)\n-\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n-\n-        # append to next input_ids and attn_mask\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-\n-        output_from_no_past = model(next_input_ids, output_hidden_states=True).hidden_states[0]\n-        output_from_past = model(\n-            next_tokens, past_key_values=past_key_values, output_hidden_states=True\n-        ).hidden_states[0]\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), output_from_past.shape[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, 0, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)\n-\n-    def create_and_check_causal_lm_base_model_past_with_attn_mask(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        config.is_decoder = True\n-\n-        model = TFElectraModel(config=config)\n-\n-        # create attention mask\n-        half_seq_length = self.seq_length // 2\n-        attn_mask_begin = tf.ones((self.batch_size, half_seq_length), dtype=tf.int32)\n-        attn_mask_end = tf.zeros((self.batch_size, self.seq_length - half_seq_length), dtype=tf.int32)\n-        attn_mask = tf.concat([attn_mask_begin, attn_mask_end], axis=1)\n-\n-        # first forward pass\n-        outputs = model(input_ids, attention_mask=attn_mask, use_cache=True)\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n-\n-        past_key_values = outputs.past_key_values\n-\n-        # change a random masked slice from input_ids\n-        random_seq_idx_to_change = ids_tensor((1,), half_seq_length).numpy() + 1\n-        random_other_next_tokens = ids_tensor((self.batch_size, self.seq_length), config.vocab_size)\n-        vector_condition = tf.range(self.seq_length) == (self.seq_length - random_seq_idx_to_change)\n-        condition = tf.transpose(\n-            tf.broadcast_to(tf.expand_dims(vector_condition, -1), (self.seq_length, self.batch_size))\n-        )\n-        input_ids = tf.where(condition, random_other_next_tokens, input_ids)\n-\n-        # append to next input_ids and\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        attn_mask = tf.concat(\n-            [attn_mask, tf.ones((attn_mask.shape[0], 1), dtype=tf.int32)],\n-            axis=1,\n-        )\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=attn_mask,\n-            output_hidden_states=True,\n-        ).hidden_states[0]\n-        output_from_past = model(\n-            next_tokens, past_key_values=past_key_values, attention_mask=attn_mask, output_hidden_states=True\n-        ).hidden_states[0]\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), output_from_past.shape[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, 0, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)\n-\n-    def create_and_check_causal_lm_base_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        config.is_decoder = True\n-\n-        model = TFElectraModel(config=config)\n-\n-        input_ids = input_ids[:1, :]\n-        input_mask = input_mask[:1, :]\n-        self.batch_size = 1\n-\n-        # first forward pass\n-        outputs = model(input_ids, attention_mask=input_mask, use_cache=True)\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_attn_mask = ids_tensor((self.batch_size, 3), 2)\n-\n-        # append to next input_ids and\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        next_attention_mask = tf.concat([input_mask, next_attn_mask], axis=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            output_hidden_states=True,\n-        ).hidden_states[0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        ).hidden_states[0]\n-\n-        self.parent.assertEqual(next_tokens.shape[1], output_from_past.shape[1])\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), output_from_past.shape[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)\n-\n-    def create_and_check_decoder_model_past_large_inputs(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-\n-        model = TFElectraModel(config=config)\n-\n-        input_ids = input_ids[:1, :]\n-        input_mask = input_mask[:1, :]\n-        encoder_hidden_states = encoder_hidden_states[:1, :, :]\n-        encoder_attention_mask = encoder_attention_mask[:1, :]\n-        self.batch_size = 1\n-\n-        # first forward pass\n-        outputs = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            use_cache=True,\n-        )\n-        past_key_values = outputs.past_key_values\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_attn_mask = ids_tensor((self.batch_size, 3), 2)\n-\n-        # append to next input_ids and\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        next_attention_mask = tf.concat([input_mask, next_attn_mask], axis=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            output_hidden_states=True,\n-        ).hidden_states[0]\n-        output_from_past = model(\n-            next_tokens,\n-            attention_mask=next_attention_mask,\n-            encoder_hidden_states=encoder_hidden_states,\n-            encoder_attention_mask=encoder_attention_mask,\n-            past_key_values=past_key_values,\n-            output_hidden_states=True,\n-        ).hidden_states[0]\n-\n-        self.parent.assertEqual(next_tokens.shape[1], output_from_past.shape[1])\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), output_from_past.shape[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)\n-\n-    def create_and_check_for_masked_lm(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFElectraForMaskedLM(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_for_pretraining(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFElectraForPreTraining(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length))\n-\n-    def create_and_check_for_sequence_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFElectraForSequenceClassification(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_for_multiple_choice(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_choices = self.num_choices\n-        model = TFElectraForMultipleChoice(config=config)\n-        multiple_choice_inputs_ids = tf.tile(tf.expand_dims(input_ids, 1), (1, self.num_choices, 1))\n-        multiple_choice_input_mask = tf.tile(tf.expand_dims(input_mask, 1), (1, self.num_choices, 1))\n-        multiple_choice_token_type_ids = tf.tile(tf.expand_dims(token_type_ids, 1), (1, self.num_choices, 1))\n-        inputs = {\n-            \"input_ids\": multiple_choice_inputs_ids,\n-            \"attention_mask\": multiple_choice_input_mask,\n-            \"token_type_ids\": multiple_choice_token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_choices))\n-\n-    def create_and_check_for_question_answering(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFElectraForQuestionAnswering(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-\n-    def create_and_check_for_token_classification(\n-        self, config, input_ids, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFElectraForTokenClassification(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFElectraModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFElectraModel,\n-            TFElectraForMaskedLM,\n-            TFElectraForPreTraining,\n-            TFElectraForTokenClassification,\n-            TFElectraForMultipleChoice,\n-            TFElectraForSequenceClassification,\n-            TFElectraForQuestionAnswering,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFElectraModel,\n-            \"fill-mask\": TFElectraForMaskedLM,\n-            \"question-answering\": TFElectraForQuestionAnswering,\n-            \"text-classification\": TFElectraForSequenceClassification,\n-            \"token-classification\": TFElectraForTokenClassification,\n-            \"zero-shot\": TFElectraForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFElectraModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=ElectraConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        \"\"\"Test the base model\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_causal_lm_base_model(self):\n-        \"\"\"Test the base model of the causal LM model\n-\n-        is_decoder=True, no cross_attention, no encoder outputs\n-        \"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_causal_lm_base_model(*config_and_inputs)\n-\n-    def test_model_as_decoder(self):\n-        \"\"\"Test the base model as a decoder (of an encoder-decoder architecture)\n-\n-        is_decoder=True + cross_attention + pass encoder outputs\n-        \"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n-\n-    def test_causal_lm_base_model_past(self):\n-        \"\"\"Test causal LM base model with `past_key_values`\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_causal_lm_base_model_past(*config_and_inputs)\n-\n-    def test_causal_lm_base_model_past_with_attn_mask(self):\n-        \"\"\"Test the causal LM base model with `past_key_values` and `attention_mask`\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_causal_lm_base_model_past_with_attn_mask(*config_and_inputs)\n-\n-    def test_causal_lm_base_model_past_with_large_inputs(self):\n-        \"\"\"Test the causal LM base model with `past_key_values` and a longer decoder sequence length\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_causal_lm_base_model_past_large_inputs(*config_and_inputs)\n-\n-    def test_decoder_model_past_with_large_inputs(self):\n-        \"\"\"Similar to `test_causal_lm_base_model_past_with_large_inputs` but with cross-attention\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n-\n-    def test_for_masked_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n-\n-    def test_for_pretraining(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_pretraining(*config_and_inputs)\n-\n-    def test_for_question_answering(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_question_answering(*config_and_inputs)\n-\n-    def test_for_sequence_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n-\n-    def test_for_multiple_choice(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_multiple_choice(*config_and_inputs)\n-\n-    def test_for_token_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_token_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        #     model_name = 'google/electra-small-generator'\n-        for model_name in [\"google/electra-small-discriminator\"]:\n-            model = TFElectraModel.from_pretrained(model_name)\n-            self.assertIsNotNone(model)\n-\n-\n-@require_tf\n-class TFElectraModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_inference_masked_lm(self):\n-        model = TFElectraForPreTraining.from_pretrained(\"lysandre/tiny-electra-random\")\n-        input_ids = tf.constant([[0, 1, 2, 3, 4, 5]])\n-        output = model(input_ids)[0]\n-\n-        expected_shape = [1, 6]\n-        self.assertEqual(output.shape, expected_shape)\n-\n-        print(output[:, :3])\n-\n-        expected_slice = tf.constant([[-0.24651965, 0.8835437, 1.823782]])\n-        tf.debugging.assert_near(output[:, :3], expected_slice, atol=1e-4)"
        },
        {
            "sha": "b17f9ed37bc6ce6467408ec1429bbf8682c6b58b",
            "filename": "tests/models/encoder_decoder/test_modeling_flax_encoder_decoder.py",
            "status": "removed",
            "additions": 0,
            "deletions": 498,
            "changes": 498,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_flax_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_flax_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_flax_encoder_decoder.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,498 +0,0 @@\n-# Copyright 2020 HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import tempfile\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import ids_tensor\n-from ..bart.test_modeling_flax_bart import FlaxBartStandaloneDecoderModelTester\n-from ..bert.test_modeling_flax_bert import FlaxBertModelTester\n-from ..gpt2.test_modeling_flax_gpt2 import FlaxGPT2ModelTester\n-\n-\n-if is_flax_available():\n-    from transformers import (\n-        AutoTokenizer,\n-        EncoderDecoderConfig,\n-        FlaxBartForCausalLM,\n-        FlaxBertForCausalLM,\n-        FlaxBertModel,\n-        FlaxEncoderDecoderModel,\n-        FlaxGPT2LMHeadModel,\n-    )\n-\n-\n-@require_flax\n-class FlaxEncoderDecoderMixin:\n-    def get_encoder_decoder_model(self, config, decoder_config):\n-        raise NotImplementedError\n-\n-    def prepare_config_and_inputs(self):\n-        raise NotImplementedError\n-\n-    def get_pretrained_model(self):\n-        raise NotImplementedError\n-\n-    def check_encoder_decoder_model_from_pretrained_configs(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        **kwargs,\n-    ):\n-        encoder_decoder_config = EncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-        self.assertTrue(encoder_decoder_config.decoder.is_decoder)\n-\n-        enc_dec_model = FlaxEncoderDecoderModel(encoder_decoder_config)\n-\n-        self.assertTrue(enc_dec_model.config.is_encoder_decoder)\n-\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-        )\n-\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"logits\"].shape, (decoder_input_ids.shape + (decoder_config.vocab_size,))\n-        )\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"encoder_last_hidden_state\"].shape, (input_ids.shape + (config.hidden_size,))\n-        )\n-\n-    def check_encoder_decoder_model_from_pretrained(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        return_dict,\n-        **kwargs,\n-    ):\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        kwargs = {\"encoder_model\": encoder_model, \"decoder_model\": decoder_model, \"return_dict\": return_dict}\n-        enc_dec_model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(**kwargs)\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-            return_dict=True,\n-        )\n-\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"logits\"].shape, (decoder_input_ids.shape + (decoder_config.vocab_size,))\n-        )\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"encoder_last_hidden_state\"].shape, (input_ids.shape + (config.hidden_size,))\n-        )\n-\n-    def check_save_and_load(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        **kwargs,\n-    ):\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        kwargs = {\"encoder_model\": encoder_model, \"decoder_model\": decoder_model}\n-        enc_dec_model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(**kwargs)\n-\n-        outputs = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-        )\n-        out_2 = np.array(outputs[0])\n-        out_2[np.isnan(out_2)] = 0\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            enc_dec_model.save_pretrained(tmpdirname)\n-            FlaxEncoderDecoderModel.from_pretrained(tmpdirname)\n-\n-            after_outputs = enc_dec_model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-            )\n-            out_1 = np.array(after_outputs[0])\n-            out_1[np.isnan(out_1)] = 0\n-            max_diff = np.amax(np.abs(out_1 - out_2))\n-            self.assertLessEqual(max_diff, 1e-5)\n-\n-    def check_encoder_decoder_model_from_encoder_decoder_pretrained(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        **kwargs,\n-    ):\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        # assert that model attributes match those of configs\n-        self.assertEqual(config.use_cache, encoder_model.config.use_cache)\n-        self.assertEqual(decoder_config.use_cache, decoder_model.config.use_cache)\n-\n-        with tempfile.TemporaryDirectory() as enc_tmpdir:\n-            with tempfile.TemporaryDirectory() as dec_tmpdir:\n-                encoder_model.save_pretrained(enc_tmpdir)\n-                decoder_model.save_pretrained(dec_tmpdir)\n-                # load a model from pretrained encoder and decoder checkpoints, setting one encoder and one decoder kwarg opposite to that specified in their respective configs\n-                enc_dec_model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(\n-                    encoder_pretrained_model_name_or_path=enc_tmpdir,\n-                    decoder_pretrained_model_name_or_path=dec_tmpdir,\n-                    encoder_use_cache=not config.use_cache,\n-                    decoder_use_cache=not decoder_config.use_cache,\n-                )\n-\n-        # assert that setting encoder and decoder kwargs opposite to those in the configs has correctly been applied\n-        self.assertNotEqual(config.use_cache, enc_dec_model.config.encoder.use_cache)\n-        self.assertNotEqual(decoder_config.use_cache, enc_dec_model.config.decoder.use_cache)\n-\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            decoder_input_ids=decoder_input_ids,\n-            decoder_attention_mask=decoder_attention_mask,\n-            output_hidden_states=True,\n-            return_dict=True,\n-        )\n-\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"logits\"].shape, (decoder_input_ids.shape + (decoder_config.vocab_size,))\n-        )\n-\n-    def check_encoder_decoder_model_output_attentions(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        **kwargs,\n-    ):\n-        # make the decoder inputs a different shape from the encoder inputs to harden the test\n-        decoder_input_ids = decoder_input_ids[:, :-1]\n-        decoder_attention_mask = decoder_attention_mask[:, :-1]\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        kwargs = {\"encoder_model\": encoder_model, \"decoder_model\": decoder_model}\n-        enc_dec_model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(**kwargs)\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-            output_attentions=True,\n-        )\n-\n-        encoder_attentions = outputs_encoder_decoder[\"encoder_attentions\"]\n-        self.assertEqual(len(encoder_attentions), config.num_hidden_layers)\n-\n-        self.assertEqual(\n-            encoder_attentions[0].shape[-3:], (config.num_attention_heads, input_ids.shape[-1], input_ids.shape[-1])\n-        )\n-\n-        decoder_attentions = outputs_encoder_decoder[\"decoder_attentions\"]\n-        num_decoder_layers = (\n-            decoder_config.num_decoder_layers\n-            if hasattr(decoder_config, \"num_decoder_layers\")\n-            else decoder_config.num_hidden_layers\n-        )\n-        self.assertEqual(len(decoder_attentions), num_decoder_layers)\n-\n-        self.assertEqual(\n-            decoder_attentions[0].shape[-3:],\n-            (decoder_config.num_attention_heads, decoder_input_ids.shape[-1], decoder_input_ids.shape[-1]),\n-        )\n-\n-        cross_attentions = outputs_encoder_decoder[\"cross_attentions\"]\n-        self.assertEqual(len(cross_attentions), num_decoder_layers)\n-\n-        cross_attention_input_seq_len = decoder_input_ids.shape[-1] * (\n-            1 + (decoder_config.ngram if hasattr(decoder_config, \"ngram\") else 0)\n-        )\n-        self.assertEqual(\n-            cross_attentions[0].shape[-3:],\n-            (decoder_config.num_attention_heads, cross_attention_input_seq_len, input_ids.shape[-1]),\n-        )\n-\n-    def check_encoder_decoder_model_generate(self, input_ids, config, decoder_config, **kwargs):\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        kwargs = {\"encoder_model\": encoder_model, \"decoder_model\": decoder_model}\n-        enc_dec_model = FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(**kwargs)\n-\n-        pad_token_id = enc_dec_model.config.decoder.pad_token_id\n-        eos_token_id = enc_dec_model.config.decoder.eos_token_id\n-        decoder_start_token_id = enc_dec_model.config.decoder.decoder_start_token_id\n-\n-        # Copied from generation.utils (GPT2 doesn't have `pad_token_id`)\n-        if pad_token_id is None and eos_token_id is not None:\n-            pad_token_id = eos_token_id\n-        if decoder_start_token_id is None:\n-            decoder_start_token_id = enc_dec_model.config.decoder.bos_token_id\n-\n-        # Bert does not have a bos token id, so use pad_token_id instead\n-        # Copied from `test_modeling_encoder_decoder.py`\n-        if decoder_start_token_id is None:\n-            decoder_start_token_id = pad_token_id\n-\n-        generated_output = enc_dec_model.generate(\n-            input_ids,\n-            pad_token_id=pad_token_id,\n-            eos_token_id=eos_token_id,\n-            decoder_start_token_id=decoder_start_token_id,\n-        )\n-        generated_sequences = generated_output.sequences\n-        self.assertEqual(generated_sequences.shape, (input_ids.shape[0],) + (decoder_config.max_length,))\n-\n-    def test_encoder_decoder_model_from_pretrained_configs(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_from_pretrained_configs(**input_ids_dict)\n-\n-    def test_encoder_decoder_model_from_pretrained(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_from_pretrained(**input_ids_dict, return_dict=False)\n-\n-    def test_encoder_decoder_model_from_pretrained_return_dict(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_from_pretrained(**input_ids_dict, return_dict=True)\n-\n-    def test_save_and_load_from_pretrained(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_save_and_load(**input_ids_dict)\n-\n-    def test_encoder_decoder_model_from_encoder_decoder_pretrained(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_from_encoder_decoder_pretrained(**input_ids_dict)\n-\n-    def test_encoder_decoder_model_output_attentions(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_output_attentions(**input_ids_dict)\n-\n-    def test_encoder_decoder_model_generate(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_generate(**input_ids_dict)\n-\n-    def assert_almost_equals(self, a: np.ndarray, b: np.ndarray, tol: float):\n-        diff = np.abs(a - b).max()\n-        self.assertLessEqual(diff, tol, f\"Difference between torch and flax is {diff} (>= {tol}).\")\n-\n-    @slow\n-    def test_real_model_save_load_from_pretrained(self):\n-        model_2 = self.get_pretrained_model()\n-        input_ids = ids_tensor([13, 5], model_2.config.encoder.vocab_size)\n-        decoder_input_ids = ids_tensor([13, 1], model_2.config.encoder.vocab_size)\n-        attention_mask = ids_tensor([13, 5], vocab_size=2)\n-\n-        outputs = model_2(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-        )\n-        out_2 = np.array(outputs[0])\n-        out_2[np.isnan(out_2)] = 0\n-\n-        with tempfile.TemporaryDirectory() as tmp_dirname:\n-            model_2.save_pretrained(tmp_dirname)\n-            model_1 = FlaxEncoderDecoderModel.from_pretrained(tmp_dirname)\n-\n-            after_outputs = model_1(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-            )\n-            out_1 = np.array(after_outputs[0])\n-            out_1[np.isnan(out_1)] = 0\n-            max_diff = np.amax(np.abs(out_1 - out_2))\n-            self.assertLessEqual(max_diff, 1e-5)\n-\n-\n-@require_flax\n-class FlaxGPT2EncoderDecoderModelTest(FlaxEncoderDecoderMixin, unittest.TestCase):\n-    def get_encoder_decoder_model(self, config, decoder_config):\n-        encoder_model = FlaxBertModel(config)\n-        decoder_model = FlaxGPT2LMHeadModel(decoder_config)\n-        return encoder_model, decoder_model\n-\n-    def prepare_config_and_inputs(self):\n-        model_tester_encoder = FlaxBertModelTester(self, batch_size=13)\n-        model_tester_decoder = FlaxGPT2ModelTester(self, batch_size=13)\n-        encoder_config_and_inputs = model_tester_encoder.prepare_config_and_inputs()\n-        decoder_config_and_inputs = model_tester_decoder.prepare_config_and_inputs_for_decoder()\n-        (config, input_ids, token_type_ids, attention_mask) = encoder_config_and_inputs\n-        (\n-            decoder_config,\n-            decoder_input_ids,\n-            decoder_attention_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        ) = decoder_config_and_inputs\n-\n-        # make sure that cross attention layers are added\n-        decoder_config.add_cross_attention = True\n-        return {\n-            \"config\": config,\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_config\": decoder_config,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"encoder_hidden_states\": encoder_hidden_states,\n-        }\n-\n-    def get_pretrained_model(self):\n-        return FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(\n-            \"google-bert/bert-base-cased\", \"openai-community/gpt2\"\n-        )\n-\n-    @slow\n-    def test_bert2gpt2_summarization(self):\n-        tokenizer_in = AutoTokenizer.from_pretrained(\"google-bert/bert-base-cased\")\n-        tokenizer_out = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n-\n-        model = FlaxEncoderDecoderModel.from_pretrained(\n-            \"patrickvonplaten/bert2gpt2-cnn_dailymail-fp16\", pad_token_id=tokenizer_out.eos_token_id\n-        )\n-\n-        ARTICLE_STUDENTS = \"\"\"(CNN)Sigma Alpha Epsilon is under fire for a video showing party-bound fraternity members singing a racist chant. SAE's national chapter suspended the students, but University of Oklahoma President David Boren took it a step further, saying the university's affiliation with the fraternity is permanently done. The news is shocking, but it's not the first time SAE has faced controversy. SAE was founded March 9, 1856, at the University of Alabama, five years before the American Civil War, according to the fraternity website. When the war began, the group had fewer than 400 members, of which \"369 went to war for the Confederate States and seven for the Union Army,\" the website says. The fraternity now boasts more than 200,000 living alumni, along with about 15,000 undergraduates populating 219 chapters and 20 \"colonies\" seeking full membership at universities. SAE has had to work hard to change recently after a string of member deaths, many blamed on the hazing of new recruits, SAE national President Bradley Cohen wrote in a message on the fraternity's website. The fraternity's website lists more than 130 chapters cited or suspended for \"health and safety incidents\" since 2010. At least 30 of the incidents involved hazing, and dozens more involved alcohol. However, the list is missing numerous incidents from recent months. Among them, according to various media outlets: Yale University banned the SAEs from campus activities last month after members allegedly tried to interfere with a sexual misconduct investigation connected to an initiation rite. Stanford University in December suspended SAE housing privileges after finding sorority members attending a fraternity function were subjected to graphic sexual content. And Johns Hopkins University in November suspended the fraternity for underage drinking. \"The media has labeled us as the 'nation's deadliest fraternity,' \" Cohen said. In 2011, for example, a student died while being coerced into excessive alcohol consumption, according to a lawsuit. SAE's previous insurer dumped the fraternity. \"As a result, we are paying Lloyd's of London the highest insurance rates in the Greek-letter world,\" Cohen said. Universities have turned down SAE's attempts to open new chapters, and the fraternity had to close 12 in 18 months over hazing incidents.\"\"\"\n-\n-        EXPECTED_SUMMARY_STUDENTS = \"\"\"SAE's national chapter suspended the students, but university president says it's permanent.\\nSAE's national chapter has had to work hard to change recently.\\nSAE's chapter has more than 200,000 members.\\nSAE's chapter has been criticized for its hazing of new recruits.\"\"\"\n-\n-        input_dict = tokenizer_in(ARTICLE_STUDENTS, return_tensors=\"np\")\n-        output_ids = model.generate(input_dict[\"input_ids\"]).sequences\n-        summary = tokenizer_out.batch_decode(output_ids, skip_special_tokens=True)\n-\n-        self.assertEqual(summary, [EXPECTED_SUMMARY_STUDENTS])\n-\n-\n-@require_flax\n-class FlaxBartEncoderDecoderModelTest(FlaxEncoderDecoderMixin, unittest.TestCase):\n-    def get_encoder_decoder_model(self, config, decoder_config):\n-        encoder_model = FlaxBertModel(config)\n-        decoder_model = FlaxBartForCausalLM(decoder_config)\n-        return encoder_model, decoder_model\n-\n-    def prepare_config_and_inputs(self):\n-        model_tester_encoder = FlaxBertModelTester(self, batch_size=13)\n-        model_tester_decoder = FlaxBartStandaloneDecoderModelTester(self, batch_size=13)\n-        encoder_config_and_inputs = model_tester_encoder.prepare_config_and_inputs()\n-        decoder_config_and_inputs = model_tester_decoder.prepare_config_and_inputs_for_decoder()\n-        (config, input_ids, token_type_ids, attention_mask) = encoder_config_and_inputs\n-        (\n-            decoder_config,\n-            decoder_input_ids,\n-            decoder_attention_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        ) = decoder_config_and_inputs\n-\n-        # make sure that cross attention layers are added\n-        decoder_config.add_cross_attention = True\n-        return {\n-            \"config\": config,\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_config\": decoder_config,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"encoder_hidden_states\": encoder_hidden_states,\n-        }\n-\n-    def get_pretrained_model(self):\n-        return FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(\n-            \"google-bert/bert-base-cased\", \"facebook/bart-base\"\n-        )\n-\n-\n-@require_flax\n-class FlaxBertEncoderDecoderModelTest(FlaxEncoderDecoderMixin, unittest.TestCase):\n-    def get_encoder_decoder_model(self, config, decoder_config):\n-        encoder_model = FlaxBertModel(config)\n-        decoder_model = FlaxBertForCausalLM(decoder_config)\n-        return encoder_model, decoder_model\n-\n-    def prepare_config_and_inputs(self):\n-        model_tester_encoder = FlaxBertModelTester(self, batch_size=13)\n-        model_tester_decoder = FlaxBertModelTester(self, batch_size=13)\n-        encoder_config_and_inputs = model_tester_encoder.prepare_config_and_inputs()\n-        decoder_config_and_inputs = model_tester_decoder.prepare_config_and_inputs_for_decoder()\n-        (config, input_ids, token_type_ids, attention_mask) = encoder_config_and_inputs\n-        (\n-            decoder_config,\n-            decoder_input_ids,\n-            decoder_attention_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        ) = decoder_config_and_inputs\n-\n-        # make sure that cross attention layers are added\n-        decoder_config.add_cross_attention = True\n-        return {\n-            \"config\": config,\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_config\": decoder_config,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"encoder_hidden_states\": encoder_hidden_states,\n-        }\n-\n-    def get_pretrained_model(self):\n-        return FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(\n-            \"google-bert/bert-base-cased\", \"google-bert/bert-base-cased\"\n-        )\n-\n-\n-@require_flax\n-class FlaxEncoderDecoderModelTest(unittest.TestCase):\n-    def get_from_encoderdecoder_pretrained_model(self):\n-        return FlaxEncoderDecoderModel.from_encoder_decoder_pretrained(\n-            \"google-bert/bert-base-cased\", \"openai-community/gpt2\"\n-        )\n-\n-    def _check_configuration_tie(self, model):\n-        module = model.module.bind(model.params)\n-\n-        assert id(module.decoder.config) == id(model.config.decoder)\n-        assert id(module.encoder.config) == id(model.config.encoder)\n-\n-    @slow\n-    def test_configuration_tie(self):\n-        model = self.get_from_encoderdecoder_pretrained_model()\n-        self._check_configuration_tie(model)"
        },
        {
            "sha": "5e1da3242b90c2e5919e379023536197d7470762",
            "filename": "tests/models/encoder_decoder/test_modeling_tf_encoder_decoder.py",
            "status": "removed",
            "additions": 0,
            "deletions": 850,
            "changes": 850,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_tf_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_tf_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_tf_encoder_decoder.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,850 +0,0 @@\n-# Copyright 2020 HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import os\n-import tempfile\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import is_tf_available\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_modeling_tf_common import ids_tensor\n-from ..bert.test_modeling_tf_bert import TFBertModelTester\n-from ..gpt2.test_modeling_tf_gpt2 import TFGPT2ModelTester\n-from ..rembert.test_modeling_tf_rembert import TFRemBertModelTester\n-from ..roberta.test_modeling_tf_roberta import TFRobertaModelTester\n-\n-\n-if is_tf_available():\n-    from transformers import (\n-        AutoConfig,\n-        AutoTokenizer,\n-        EncoderDecoderConfig,\n-        TFAutoModel,\n-        TFAutoModelForCausalLM,\n-        TFBertLMHeadModel,\n-        TFBertModel,\n-        TFEncoderDecoderModel,\n-        TFGPT2LMHeadModel,\n-        TFRemBertForCausalLM,\n-        TFRemBertModel,\n-        TFRobertaForCausalLM,\n-        TFRobertaModel,\n-    )\n-    from transformers.modeling_tf_outputs import TFBaseModelOutput\n-\n-\n-@require_tf\n-class TFEncoderDecoderMixin:\n-    def get_encoder_decoder_model(self, config, decoder_config):\n-        raise NotImplementedError\n-\n-    def prepare_config_and_inputs(self):\n-        raise NotImplementedError\n-\n-    def get_pretrained_model(self):\n-        raise NotImplementedError\n-\n-    def check_encoder_decoder_model_from_pretrained_configs(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        **kwargs,\n-    ):\n-        encoder_decoder_config = EncoderDecoderConfig.from_encoder_decoder_configs(config, decoder_config)\n-        self.assertTrue(encoder_decoder_config.decoder.is_decoder)\n-\n-        enc_dec_model = TFEncoderDecoderModel(encoder_decoder_config)\n-\n-        self.assertTrue(enc_dec_model.config.is_encoder_decoder)\n-\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-            kwargs=kwargs,\n-        )\n-\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"logits\"].shape, (decoder_input_ids.shape + (decoder_config.vocab_size,))\n-        )\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"encoder_last_hidden_state\"].shape, (input_ids.shape + (config.hidden_size,))\n-        )\n-\n-    def check_encoder_decoder_model(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        **kwargs,\n-    ):\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        enc_dec_model = TFEncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n-        self.assertTrue(enc_dec_model.config.decoder.is_decoder)\n-        self.assertTrue(enc_dec_model.config.decoder.add_cross_attention)\n-        self.assertTrue(enc_dec_model.config.is_encoder_decoder)\n-\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-            kwargs=kwargs,\n-        )\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"logits\"].shape, (decoder_input_ids.shape + (decoder_config.vocab_size,))\n-        )\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"encoder_last_hidden_state\"].shape, (input_ids.shape + (config.hidden_size,))\n-        )\n-\n-        encoder_outputs = TFBaseModelOutput(last_hidden_state=encoder_hidden_states)\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=None,\n-            encoder_outputs=encoder_outputs,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-            kwargs=kwargs,\n-        )\n-\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"logits\"].shape, (decoder_input_ids.shape + (decoder_config.vocab_size,))\n-        )\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"encoder_last_hidden_state\"].shape, (input_ids.shape + (config.hidden_size,))\n-        )\n-\n-    def check_encoder_decoder_model_from_pretrained(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        return_dict,\n-        **kwargs,\n-    ):\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        kwargs = {\"encoder_model\": encoder_model, \"decoder_model\": decoder_model, \"return_dict\": return_dict}\n-        enc_dec_model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(**kwargs)\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-            return_dict=True,\n-            kwargs=kwargs,\n-        )\n-\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"logits\"].shape, (decoder_input_ids.shape + (decoder_config.vocab_size,))\n-        )\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"encoder_last_hidden_state\"].shape, (input_ids.shape + (config.hidden_size,))\n-        )\n-\n-    def check_save_and_load(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        **kwargs,\n-    ):\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        enc_dec_model = TFEncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n-\n-        outputs = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-            kwargs=kwargs,\n-        )\n-        out_2 = np.array(outputs[0])\n-        out_2[np.isnan(out_2)] = 0\n-\n-        with tempfile.TemporaryDirectory() as tmpdirname:\n-            enc_dec_model.save_pretrained(tmpdirname)\n-            enc_dec_model = TFEncoderDecoderModel.from_pretrained(tmpdirname)\n-\n-            after_outputs = enc_dec_model(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-                decoder_attention_mask=decoder_attention_mask,\n-                kwargs=kwargs,\n-            )\n-            out_1 = np.array(after_outputs[0])\n-            out_1[np.isnan(out_1)] = 0\n-            max_diff = np.amax(np.abs(out_1 - out_2))\n-            self.assertLessEqual(max_diff, 1e-5)\n-\n-    def check_encoder_decoder_model_labels(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        labels,\n-        **kwargs,\n-    ):\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        enc_dec_model = TFEncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n-\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-            labels=labels,\n-            kwargs=kwargs,\n-        )\n-\n-        # Make sure `loss` exist\n-        self.assertIn(\"loss\", outputs_encoder_decoder)\n-\n-        batch_size, seq_len = decoder_input_ids.shape\n-        expected_shape = (batch_size, seq_len, decoder_config.vocab_size)\n-        self.assertEqual(outputs_encoder_decoder[\"logits\"].shape, expected_shape)\n-        self.assertEqual(\n-            outputs_encoder_decoder[\"encoder_last_hidden_state\"].shape, (input_ids.shape + (config.hidden_size,))\n-        )\n-\n-    def _check_output_with_attentions(\n-        self, outputs_encoder_decoder, config, input_ids, decoder_config, decoder_input_ids\n-    ):\n-        encoder_attentions = outputs_encoder_decoder[\"encoder_attentions\"]\n-        self.assertEqual(len(encoder_attentions), config.num_hidden_layers)\n-\n-        self.assertEqual(\n-            encoder_attentions[0].shape[-3:], (config.num_attention_heads, input_ids.shape[-1], input_ids.shape[-1])\n-        )\n-\n-        decoder_attentions = outputs_encoder_decoder[\"decoder_attentions\"]\n-        num_decoder_layers = (\n-            decoder_config.num_decoder_layers\n-            if hasattr(decoder_config, \"num_decoder_layers\")\n-            else decoder_config.num_hidden_layers\n-        )\n-        self.assertEqual(len(decoder_attentions), num_decoder_layers)\n-\n-        self.assertEqual(\n-            decoder_attentions[0].shape[-3:],\n-            (decoder_config.num_attention_heads, decoder_input_ids.shape[-1], decoder_input_ids.shape[-1]),\n-        )\n-\n-        cross_attentions = outputs_encoder_decoder[\"cross_attentions\"]\n-        self.assertEqual(len(cross_attentions), num_decoder_layers)\n-\n-        cross_attention_input_seq_len = decoder_input_ids.shape[-1] * (\n-            1 + (decoder_config.ngram if hasattr(decoder_config, \"ngram\") else 0)\n-        )\n-        self.assertEqual(\n-            cross_attentions[0].shape[-3:],\n-            (decoder_config.num_attention_heads, cross_attention_input_seq_len, input_ids.shape[-1]),\n-        )\n-\n-    def check_encoder_decoder_model_output_attentions(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        **kwargs,\n-    ):\n-        # make the decoder inputs a different shape from the encoder inputs to harden the test\n-        decoder_input_ids = decoder_input_ids[:, :-1]\n-        decoder_attention_mask = decoder_attention_mask[:, :-1]\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        enc_dec_model = TFEncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-            output_attentions=True,\n-            kwargs=kwargs,\n-        )\n-        self._check_output_with_attentions(\n-            outputs_encoder_decoder, config, input_ids, decoder_config, decoder_input_ids\n-        )\n-\n-    def check_encoder_decoder_model_output_attentions_from_config(\n-        self,\n-        config,\n-        input_ids,\n-        attention_mask,\n-        encoder_hidden_states,\n-        decoder_config,\n-        decoder_input_ids,\n-        decoder_attention_mask,\n-        **kwargs,\n-    ):\n-        # Similar to `check_encoder_decoder_model_output_attentions`, but with `output_attentions` triggered from the\n-        # config file. Contrarily to most models, changing the model's config won't work -- the defaults are loaded\n-        # from the inner models' configurations.\n-\n-        decoder_input_ids = decoder_input_ids[:, :-1]\n-        decoder_attention_mask = decoder_attention_mask[:, :-1]\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        enc_dec_model = TFEncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n-        enc_dec_model.config.output_attentions = True  # model config -> won't work\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-            kwargs=kwargs,\n-        )\n-        self.assertTrue(\n-            all(\n-                key not in outputs_encoder_decoder\n-                for key in [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n-            )\n-        )\n-\n-        config.output_attentions = True  # inner model config -> will work\n-        decoder_config.output_attentions = True\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        enc_dec_model = TFEncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n-        outputs_encoder_decoder = enc_dec_model(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-            decoder_attention_mask=decoder_attention_mask,\n-            kwargs=kwargs,\n-        )\n-        self._check_output_with_attentions(\n-            outputs_encoder_decoder, config, input_ids, decoder_config, decoder_input_ids\n-        )\n-\n-    def check_encoder_decoder_model_generate(self, input_ids, config, decoder_config, **kwargs):\n-        encoder_model, decoder_model = self.get_encoder_decoder_model(config, decoder_config)\n-        enc_dec_model = TFEncoderDecoderModel(encoder=encoder_model, decoder=decoder_model)\n-\n-        # Generate until max length\n-        if hasattr(enc_dec_model.config, \"eos_token_id\"):\n-            enc_dec_model.config.eos_token_id = None\n-        if hasattr(enc_dec_model.config, \"decoder\") and hasattr(enc_dec_model.config.decoder, \"eos_token_id\"):\n-            enc_dec_model.config.decoder.eos_token_id = None\n-        if hasattr(enc_dec_model.generation_config, \"eos_token_id\"):\n-            enc_dec_model.generation_config.eos_token_id = None\n-\n-        # Bert does not have a bos token id, so use pad_token_id instead\n-        generated_output = enc_dec_model.generate(\n-            input_ids, decoder_start_token_id=enc_dec_model.config.decoder.pad_token_id\n-        )\n-        self.assertEqual(tuple(generated_output.shape.as_list()), (input_ids.shape[0],) + (decoder_config.max_length,))\n-\n-    def test_encoder_decoder_model(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model(**input_ids_dict)\n-\n-    def test_encoder_decoder_model_from_pretrained_configs(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_from_pretrained_configs(**input_ids_dict)\n-\n-    def test_encoder_decoder_model_from_pretrained(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_from_pretrained(**input_ids_dict, return_dict=False)\n-\n-    def test_encoder_decoder_model_from_pretrained_return_dict(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_from_pretrained(**input_ids_dict, return_dict=True)\n-\n-    def test_save_and_load_from_pretrained(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_save_and_load(**input_ids_dict)\n-\n-    def test_encoder_decoder_model_labels(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_labels(**input_ids_dict)\n-\n-    def test_encoder_decoder_model_output_attentions(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_output_attentions(**input_ids_dict)\n-\n-    def test_encoder_decoder_model_output_attentions_from_config(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_output_attentions_from_config(**input_ids_dict)\n-\n-    def test_encoder_decoder_model_generate(self):\n-        input_ids_dict = self.prepare_config_and_inputs()\n-        self.check_encoder_decoder_model_generate(**input_ids_dict)\n-\n-    def assert_almost_equals(self, a: np.ndarray, b: np.ndarray, tol: float):\n-        diff = np.abs(a - b).max()\n-        self.assertLessEqual(diff, tol, f\"Difference between torch and tf is {diff} (>= {tol}).\")\n-\n-    def test_model_save_load_from_pretrained(self):\n-        model_2 = self.get_pretrained_model()\n-        input_ids = ids_tensor([13, 5], model_2.config.encoder.vocab_size)\n-        decoder_input_ids = ids_tensor([13, 1], model_2.config.decoder.vocab_size)\n-        attention_mask = ids_tensor([13, 5], vocab_size=2)\n-\n-        outputs = model_2(\n-            input_ids=input_ids,\n-            decoder_input_ids=decoder_input_ids,\n-            attention_mask=attention_mask,\n-        )\n-        out_2 = np.array(outputs[0])\n-        out_2[np.isnan(out_2)] = 0\n-\n-        with tempfile.TemporaryDirectory() as tmp_dirname:\n-            model_2.save_pretrained(tmp_dirname)\n-            model_1 = TFEncoderDecoderModel.from_pretrained(tmp_dirname)\n-\n-            after_outputs = model_1(\n-                input_ids=input_ids,\n-                decoder_input_ids=decoder_input_ids,\n-                attention_mask=attention_mask,\n-            )\n-            out_1 = np.array(after_outputs[0])\n-            out_1[np.isnan(out_1)] = 0\n-            max_diff = np.amax(np.abs(out_1 - out_2))\n-            self.assertLessEqual(max_diff, 1e-5)\n-\n-\n-@require_tf\n-class TFBertEncoderDecoderModelTest(TFEncoderDecoderMixin, unittest.TestCase):\n-    def setUp(self):\n-        self.encoder_model_tester = TFBertModelTester(self, batch_size=13)\n-        self.decoder_model_tester = TFBertModelTester(self, batch_size=13)\n-\n-    def get_pretrained_model(self):\n-        return TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n-            \"hf-internal-testing/tiny-random-bert\",\n-            \"hf-internal-testing/tiny-random-bert\",\n-        )\n-\n-    def get_encoder_decoder_model(self, config, decoder_config):\n-        encoder_model = TFBertModel(config, name=\"encoder\")\n-        decoder_model = TFBertLMHeadModel(decoder_config, name=\"decoder\")\n-        return encoder_model, decoder_model\n-\n-    def prepare_config_and_inputs(self):\n-        encoder_config_and_inputs = self.encoder_model_tester.prepare_config_and_inputs()\n-        decoder_config_and_inputs = self.decoder_model_tester.prepare_config_and_inputs_for_decoder()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            attention_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = encoder_config_and_inputs\n-        (\n-            decoder_config,\n-            decoder_input_ids,\n-            decoder_token_type_ids,\n-            decoder_attention_mask,\n-            decoder_sequence_labels,\n-            decoder_token_labels,\n-            decoder_choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        ) = decoder_config_and_inputs\n-\n-        # make sure that cross attention layers are added\n-        decoder_config.add_cross_attention = True\n-        #  disable cache for now\n-        decoder_config.use_cache = False\n-        return {\n-            \"config\": config,\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_config\": decoder_config,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"decoder_token_type_ids\": decoder_token_type_ids,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"decoder_sequence_labels\": decoder_sequence_labels,\n-            \"decoder_token_labels\": decoder_token_labels,\n-            \"decoder_choice_labels\": decoder_choice_labels,\n-            \"encoder_hidden_states\": encoder_hidden_states,\n-            \"labels\": decoder_token_labels,\n-        }\n-\n-\n-@require_tf\n-class TFGPT2EncoderDecoderModelTest(TFEncoderDecoderMixin, unittest.TestCase):\n-    def setUp(self):\n-        self.encoder_model_tester = TFBertModelTester(self, batch_size=13)\n-        self.decoder_model_tester = TFGPT2ModelTester(self)\n-\n-    def get_pretrained_model(self):\n-        return TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n-            \"hf-internal-testing/tiny-random-bert\",\n-            \"hf-internal-testing/tiny-random-gpt2\",\n-        )\n-\n-    def get_encoder_decoder_model(self, config, decoder_config):\n-        encoder_model = TFBertModel(config, name=\"encoder\")\n-        decoder_model = TFGPT2LMHeadModel(decoder_config, name=\"decoder\")\n-        return encoder_model, decoder_model\n-\n-    def prepare_config_and_inputs(self):\n-        encoder_config_and_inputs = self.encoder_model_tester.prepare_config_and_inputs()\n-        decoder_config_and_inputs = self.decoder_model_tester.prepare_config_and_inputs_for_decoder()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            attention_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = encoder_config_and_inputs\n-        (\n-            decoder_config,\n-            decoder_input_ids,\n-            decoder_attention_mask,\n-            decoder_head_mask,\n-            decoder_token_type_ids,\n-            decoder_sequence_labels,\n-            decoder_token_labels,\n-            decoder_choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        ) = decoder_config_and_inputs\n-\n-        # make sure that cross attention layers are added\n-        decoder_config.add_cross_attention = True\n-        # disable cache for now\n-        decoder_config.use_cache = False\n-        return {\n-            \"config\": config,\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"decoder_config\": decoder_config,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"decoder_token_type_ids\": decoder_token_type_ids,\n-            \"decoder_attention_mask\": decoder_attention_mask,\n-            \"decoder_sequence_labels\": decoder_sequence_labels,\n-            \"decoder_token_labels\": decoder_token_labels,\n-            \"decoder_choice_labels\": decoder_choice_labels,\n-            \"encoder_hidden_states\": encoder_hidden_states,\n-            \"labels\": decoder_token_labels,\n-        }\n-\n-\n-@require_tf\n-class TFRoBertaEncoderDecoderModelTest(TFEncoderDecoderMixin, unittest.TestCase):\n-    def setUp(self):\n-        self.encoder_model_tester = TFRobertaModelTester(self)\n-        self.decoder_model_tester = TFRobertaModelTester(self)\n-\n-    def get_pretrained_model(self):\n-        return TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n-            \"hf-internal-testing/tiny-random-roberta\",\n-            \"hf-internal-testing/tiny-random-roberta\",\n-        )\n-\n-    def get_encoder_decoder_model(self, config, decoder_config):\n-        encoder_model = TFRobertaModel(config, name=\"encoder\")\n-        decoder_model = TFRobertaForCausalLM(decoder_config, name=\"decoder\")\n-        return encoder_model, decoder_model\n-\n-    def prepare_config_and_inputs(self):\n-        encoder_config_and_inputs = self.encoder_model_tester.prepare_config_and_inputs()\n-        decoder_config_and_inputs = self.decoder_model_tester.prepare_config_and_inputs_for_decoder()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = encoder_config_and_inputs\n-        (\n-            decoder_config,\n-            decoder_input_ids,\n-            decoder_token_type_ids,\n-            decoder_input_mask,\n-            decoder_sequence_labels,\n-            decoder_token_labels,\n-            decoder_choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        ) = decoder_config_and_inputs\n-\n-        # make sure that cross attention layers are added\n-        decoder_config.add_cross_attention = True\n-        #  disable cache for now\n-        decoder_config.use_cache = False\n-        return {\n-            \"config\": config,\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"decoder_config\": decoder_config,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"decoder_token_type_ids\": decoder_token_type_ids,\n-            \"decoder_attention_mask\": decoder_input_mask,\n-            \"decoder_sequence_labels\": decoder_sequence_labels,\n-            \"decoder_token_labels\": decoder_token_labels,\n-            \"decoder_choice_labels\": decoder_choice_labels,\n-            \"encoder_hidden_states\": encoder_hidden_states,\n-            \"labels\": decoder_token_labels,\n-        }\n-\n-\n-@require_tf\n-class TFRembertEncoderDecoderModelTest(TFEncoderDecoderMixin, unittest.TestCase):\n-    def setUp(self):\n-        self.encoder_model_tester = TFRemBertModelTester(self)\n-        self.decoder_model_tester = TFRemBertModelTester(self)\n-\n-    def get_pretrained_model(self):\n-        return TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n-            \"hf-internal-testing/tiny-random-rembert\",\n-            \"hf-internal-testing/tiny-random-rembert\",\n-        )\n-\n-    def get_encoder_decoder_model(self, config, decoder_config):\n-        encoder_model = TFRemBertModel(config, name=\"encoder\")\n-        decoder_model = TFRemBertForCausalLM(decoder_config, name=\"decoder\")\n-        return encoder_model, decoder_model\n-\n-    def prepare_config_and_inputs(self):\n-        encoder_config_and_inputs = self.encoder_model_tester.prepare_config_and_inputs()\n-        decoder_config_and_inputs = self.decoder_model_tester.prepare_config_and_inputs_for_decoder()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = encoder_config_and_inputs\n-        (\n-            decoder_config,\n-            decoder_input_ids,\n-            decoder_token_type_ids,\n-            decoder_input_mask,\n-            decoder_sequence_labels,\n-            decoder_token_labels,\n-            decoder_choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        ) = decoder_config_and_inputs\n-\n-        # make sure that cross attention layers are added\n-        decoder_config.add_cross_attention = True\n-        #  disable cache for now\n-        decoder_config.use_cache = False\n-        return {\n-            \"config\": config,\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"decoder_config\": decoder_config,\n-            \"decoder_input_ids\": decoder_input_ids,\n-            \"decoder_token_type_ids\": decoder_token_type_ids,\n-            \"decoder_attention_mask\": decoder_input_mask,\n-            \"decoder_sequence_labels\": decoder_sequence_labels,\n-            \"decoder_token_labels\": decoder_token_labels,\n-            \"decoder_choice_labels\": decoder_choice_labels,\n-            \"encoder_hidden_states\": encoder_hidden_states,\n-            \"labels\": decoder_token_labels,\n-        }\n-\n-\n-@require_tf\n-class TFEncoderDecoderModelTest(unittest.TestCase):\n-    def get_from_encoderdecoder_pretrained_model(self):\n-        return TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n-            \"google-bert/bert-base-cased\", \"google-bert/bert-base-cased\"\n-        )\n-\n-    def get_decoder_config(self):\n-        config = AutoConfig.from_pretrained(\"google-bert/bert-base-cased\")\n-        config.is_decoder = True\n-        config.add_cross_attention = True\n-        return config\n-\n-    def get_encoderdecoder_model(self):\n-        return TFEncoderDecoderModel.from_pretrained(\"patrickvonplaten/bert2bert-cnn_dailymail-fp16\")\n-\n-    def get_encoder_decoder_models(self):\n-        encoder_model = TFBertModel.from_pretrained(\"google-bert/bert-base-cased\", name=\"encoder\")\n-        decoder_model = TFBertLMHeadModel.from_pretrained(\n-            \"google-bert/bert-base-cased\", config=self.get_decoder_config(), name=\"decoder\"\n-        )\n-        return {\"encoder\": encoder_model, \"decoder\": decoder_model}\n-\n-    def _check_configuration_tie(self, model):\n-        assert id(model.decoder.config) == id(model.config.decoder)\n-        assert id(model.encoder.config) == id(model.config.encoder)\n-\n-    @slow\n-    def test_configuration_tie(self):\n-        model = self.get_from_encoderdecoder_pretrained_model()\n-        self._check_configuration_tie(model)\n-\n-        model = TFEncoderDecoderModel(**self.get_encoder_decoder_models())\n-        self._check_configuration_tie(model)\n-\n-        # # This should be enabled once we upload the TF version of\n-        # # \"patrickvonplaten/bert2bert-cnn_dailymail-fp16\" to the Hub.\n-        # model = self.get_encoderdecoder_model()\n-        # self._check_configuration_tie(model)\n-\n-\n-@require_tf\n-class TFEncoderDecoderModelSaveLoadTests(unittest.TestCase):\n-    def get_encoder_decoder_config(self):\n-        encoder_config = AutoConfig.from_pretrained(\"google-bert/bert-base-uncased\")\n-        decoder_config = AutoConfig.from_pretrained(\n-            \"google-bert/bert-base-uncased\", is_decoder=True, add_cross_attention=True\n-        )\n-        return EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n-\n-    def get_encoder_decoder_config_small(self):\n-        encoder_config = AutoConfig.from_pretrained(\"hf-internal-testing/tiny-bert\")\n-        decoder_config = AutoConfig.from_pretrained(\n-            \"hf-internal-testing/tiny-bert\", is_decoder=True, add_cross_attention=True\n-        )\n-        return EncoderDecoderConfig.from_encoder_decoder_configs(encoder_config, decoder_config)\n-\n-    def test_encoder_decoder_save_load_from_encoder_decoder(self):\n-        config = self.get_encoder_decoder_config_small()\n-\n-        # create two random BERT models for bert2bert & initialize weights (+cross_attention weights)\n-        encoder = TFBertModel(config.encoder)\n-        encoder.build_in_name_scope()\n-        decoder = TFBertLMHeadModel(config.decoder)\n-        decoder.build_in_name_scope()\n-\n-        encoder_decoder_orig = TFEncoderDecoderModel(encoder=encoder, decoder=decoder)\n-\n-        input_ids = ids_tensor([13, 5], encoder.config.vocab_size)\n-        decoder_input_ids = ids_tensor([13, 1], decoder.config.vocab_size)\n-\n-        logits_orig = encoder_decoder_orig(input_ids=input_ids, decoder_input_ids=decoder_input_ids).logits\n-\n-        with tempfile.TemporaryDirectory() as tmp_dirname:\n-            encoder_path = os.path.join(tmp_dirname, \"encoder\")\n-            decoder_path = os.path.join(tmp_dirname, \"decoder\")\n-\n-            encoder.save_pretrained(encoder_path)\n-            decoder.save_pretrained(decoder_path)\n-\n-            encoder_decoder = TFEncoderDecoderModel.from_encoder_decoder_pretrained(encoder_path, decoder_path)\n-\n-        logits_1 = encoder_decoder(input_ids=input_ids, decoder_input_ids=decoder_input_ids).logits\n-\n-        self.assertTrue(logits_orig.numpy().sum() - logits_1.numpy().sum() < 1e-3)\n-\n-        max_diff = np.max(np.abs(logits_1.numpy() - logits_orig.numpy()))\n-        self.assertAlmostEqual(max_diff, 0.0, places=4)\n-\n-        with tempfile.TemporaryDirectory() as tmp_dirname:\n-            encoder_decoder.save_pretrained(tmp_dirname)\n-            encoder_decoder = TFEncoderDecoderModel.from_pretrained(tmp_dirname)\n-\n-        logits_2 = encoder_decoder(input_ids=input_ids, decoder_input_ids=decoder_input_ids).logits\n-\n-        max_diff = np.max(np.abs(logits_2.numpy() - logits_orig.numpy()))\n-        self.assertAlmostEqual(max_diff, 0.0, places=4)\n-\n-    @slow\n-    def test_encoder_decoder_from_pretrained(self):\n-        load_weight_prefix = TFEncoderDecoderModel.load_weight_prefix\n-\n-        config = self.get_encoder_decoder_config()\n-        encoder_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-        decoder_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n-\n-        input_ids = encoder_tokenizer(\"who sings does he love me with reba\", return_tensors=\"tf\").input_ids\n-        decoder_input_ids = decoder_tokenizer(\"Linda Davis\", return_tensors=\"tf\").input_ids\n-\n-        with tempfile.TemporaryDirectory() as tmp_dirname:\n-            # Since most of HF's models don't have pretrained cross-attention layers, they are randomly\n-            # initialized even if we create models using `from_pretrained` method.\n-            # For the tests, the decoder need to be a model with pretrained cross-attention layers.\n-            # So we create pretrained models (without `load_weight_prefix`), save them, and later,\n-            # we load them using `from_pretrained`.\n-            # (we don't need to do this for encoder, but let's make the code more similar between encoder/decoder)\n-            encoder = TFAutoModel.from_pretrained(\"google-bert/bert-base-uncased\", name=\"encoder\")\n-            # It's necessary to specify `add_cross_attention=True` here.\n-            decoder = TFAutoModelForCausalLM.from_pretrained(\n-                \"google-bert/bert-base-uncased\", is_decoder=True, add_cross_attention=True, name=\"decoder\"\n-            )\n-            pretrained_encoder_dir = os.path.join(tmp_dirname, \"pretrained_encoder\")\n-            pretrained_decoder_dir = os.path.join(tmp_dirname, \"pretrained_decoder\")\n-            encoder.save_pretrained(pretrained_encoder_dir)\n-            decoder.save_pretrained(pretrained_decoder_dir)\n-            del encoder\n-            del decoder\n-\n-            enc_dec_model = TFEncoderDecoderModel.from_encoder_decoder_pretrained(\n-                pretrained_encoder_dir,\n-                pretrained_decoder_dir,\n-            )\n-            # check that the from pretrained methods work\n-            enc_dec_model.save_pretrained(tmp_dirname)\n-            enc_dec_model = TFEncoderDecoderModel.from_pretrained(tmp_dirname)\n-\n-            output = enc_dec_model(input_ids, decoder_input_ids=decoder_input_ids, labels=decoder_input_ids)\n-\n-            loss_pretrained = output.loss\n-            del enc_dec_model\n-\n-            # Create the model using `__init__` with loaded ``pretrained`` encoder / decoder\n-            encoder = TFAutoModel.from_pretrained(\n-                pretrained_encoder_dir, load_weight_prefix=load_weight_prefix, name=\"encoder\"\n-            )\n-            decoder = TFAutoModelForCausalLM.from_pretrained(\n-                pretrained_decoder_dir, load_weight_prefix=load_weight_prefix, name=\"decoder\"\n-            )\n-            enc_dec_model = TFEncoderDecoderModel(config=config, encoder=encoder, decoder=decoder)\n-\n-        output = enc_dec_model(input_ids, decoder_input_ids=decoder_input_ids, labels=decoder_input_ids)\n-\n-        loss_init = output.loss\n-\n-        max_diff = np.max(np.abs(loss_pretrained - loss_init))\n-        expected_diff = 0.0\n-\n-        self.assertAlmostEqual(max_diff, expected_diff, places=4)"
        },
        {
            "sha": "c7478ab3c0b7c69bddba2d47b0321b3ba8b7460a",
            "filename": "tests/models/esm/test_modeling_tf_esm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 323,
            "changes": 323,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fesm%2Ftest_modeling_tf_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fesm%2Ftest_modeling_tf_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fesm%2Ftest_modeling_tf_esm.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,323 +0,0 @@\n-# Copyright 2022 The HuggingFace Inc. Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import EsmConfig, is_tf_available\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import numpy\n-    import tensorflow as tf\n-\n-    from transformers.modeling_tf_utils import keras\n-    from transformers.models.esm.modeling_tf_esm import (\n-        TFEsmForMaskedLM,\n-        TFEsmForSequenceClassification,\n-        TFEsmForTokenClassification,\n-        TFEsmModel,\n-    )\n-\n-\n-# copied from tests.test_modeling_tf_roberta\n-class TFEsmModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-    ):\n-        self.parent = parent\n-        self.batch_size = 13\n-        self.seq_length = 7\n-        self.is_training = True\n-        self.use_input_mask = True\n-        self.use_labels = True\n-        self.vocab_size = 99\n-        self.hidden_size = 32\n-        self.num_hidden_layers = 2\n-        self.num_attention_heads = 4\n-        self.intermediate_size = 37\n-        self.hidden_act = \"gelu\"\n-        self.hidden_dropout_prob = 0.1\n-        self.attention_probs_dropout_prob = 0.1\n-        self.max_position_embeddings = 512\n-        self.type_vocab_size = 16\n-        self.type_sequence_label_size = 2\n-        self.initializer_range = 0.02\n-        self.num_labels = 3\n-        self.num_choices = 4\n-        self.scope = None\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = EsmConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            pad_token_id=1,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        config.is_decoder = True\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n-    def create_and_check_model(self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels):\n-        model = TFEsmModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_model_as_decoder(\n-        self,\n-        config,\n-        input_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-        encoder_hidden_states,\n-        encoder_attention_mask,\n-    ):\n-        config.add_cross_attention = True\n-\n-        model = TFEsmModel(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"encoder_hidden_states\": encoder_hidden_states,\n-            \"encoder_attention_mask\": encoder_attention_mask,\n-        }\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs, encoder_hidden_states=encoder_hidden_states)\n-\n-        # Also check the case where encoder outputs are not passed\n-        result = model(input_ids, attention_mask=input_mask)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_for_masked_lm(\n-        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFEsmForMaskedLM(config=config)\n-        result = model([input_ids, input_mask])\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_for_token_classification(\n-        self, config, input_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFEsmForTokenClassification(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFEsmModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFEsmModel,\n-            TFEsmForMaskedLM,\n-            TFEsmForSequenceClassification,\n-            TFEsmForTokenClassification,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFEsmModel,\n-            \"fill-mask\": TFEsmForMaskedLM,\n-            \"text-classification\": TFEsmForSequenceClassification,\n-            \"token-classification\": TFEsmForTokenClassification,\n-            \"zero-shot\": TFEsmForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFEsmModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=EsmConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        \"\"\"Test the base model\"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_as_decoder(self):\n-        \"\"\"Test the base model as a decoder (of an encoder-decoder architecture)\n-\n-        is_decoder=True + cross_attention + pass encoder outputs\n-        \"\"\"\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs_for_decoder()\n-        self.model_tester.create_and_check_model_as_decoder(*config_and_inputs)\n-\n-    def test_for_masked_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n-\n-    def test_for_token_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_token_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"facebook/esm2_t6_8M_UR50D\"\n-        model = TFEsmModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    @unittest.skip(\"Protein models do not support embedding resizing.\")\n-    def test_resize_token_embeddings(self):\n-        pass\n-\n-    @unittest.skip(\"Protein models do not support embedding resizing.\")\n-    def test_save_load_after_resize_token_embeddings(self):\n-        pass\n-\n-    def test_model_common_attributes(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            assert isinstance(model.get_input_embeddings(), keras.layers.Layer)\n-            if model_class is TFEsmForMaskedLM:\n-                # Output embedding test differs from the main test because they're a matrix, not a layer\n-                name = model.get_bias()\n-                assert isinstance(name, dict)\n-                for k, v in name.items():\n-                    assert isinstance(v, tf.Variable)\n-            else:\n-                x = model.get_output_embeddings()\n-                assert x is None\n-                name = model.get_bias()\n-                assert name is None\n-\n-\n-@require_tf\n-class TFEsmModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_inference_masked_lm(self):\n-        model = TFEsmForMaskedLM.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n-\n-        input_ids = tf.constant([[0, 1, 2, 3, 4, 5]])\n-        output = model(input_ids)[0]\n-        expected_shape = [1, 6, 33]\n-        self.assertEqual(list(output.numpy().shape), expected_shape)\n-        # compare the actual values for a slice.\n-        expected_slice = tf.constant(\n-            [\n-                [\n-                    [8.921518, -10.589814, -6.4671307],\n-                    [-6.3967156, -13.911377, -1.1211915],\n-                    [-7.781247, -13.951557, -3.740592],\n-                ]\n-            ]\n-        )\n-        self.assertTrue(numpy.allclose(output[:, :3, :3].numpy(), expected_slice.numpy(), atol=1e-2))\n-\n-    @slow\n-    def test_inference_no_head(self):\n-        model = TFEsmModel.from_pretrained(\"facebook/esm2_t6_8M_UR50D\")\n-\n-        input_ids = tf.constant([[0, 6, 4, 13, 5, 4, 16, 12, 11, 7, 2]])\n-        output = model(input_ids)[0]\n-        # compare the actual values for a slice.\n-        expected_slice = tf.constant(\n-            [\n-                [\n-                    [0.14443092, 0.54125327, 0.3247739],\n-                    [0.30340484, 0.00526676, 0.31077722],\n-                    [0.32278043, -0.24987096, 0.3414628],\n-                ]\n-            ]\n-        )\n-        self.assertTrue(numpy.allclose(output[:, :3, :3].numpy(), expected_slice.numpy(), atol=1e-4))"
        },
        {
            "sha": "1a2931c398fc8ad49e0747cfb29ef0009410662b",
            "filename": "tests/models/flaubert/test_modeling_tf_flaubert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 398,
            "changes": 398,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fflaubert%2Ftest_modeling_tf_flaubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fflaubert%2Ftest_modeling_tf_flaubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflaubert%2Ftest_modeling_tf_flaubert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,398 +0,0 @@\n-# Copyright 2018 The Google AI Language Team Authors.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import is_tf_available\n-from transformers.testing_utils import require_sentencepiece, require_tf, require_tokenizers, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import numpy as np\n-    import tensorflow as tf\n-\n-    from transformers import (\n-        FlaubertConfig,\n-        TFFlaubertForMultipleChoice,\n-        TFFlaubertForQuestionAnsweringSimple,\n-        TFFlaubertForSequenceClassification,\n-        TFFlaubertForTokenClassification,\n-        TFFlaubertModel,\n-        TFFlaubertWithLMHeadModel,\n-    )\n-\n-\n-class TFFlaubertModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-    ):\n-        self.parent = parent\n-        self.batch_size = 13\n-        self.seq_length = 7\n-        self.is_training = True\n-        self.use_input_lengths = True\n-        self.use_token_type_ids = True\n-        self.use_labels = True\n-        self.gelu_activation = True\n-        self.sinusoidal_embeddings = False\n-        self.causal = False\n-        self.asm = False\n-        self.n_langs = 2\n-        self.vocab_size = 99\n-        self.n_special = 0\n-        self.hidden_size = 32\n-        self.num_hidden_layers = 2\n-        self.num_attention_heads = 4\n-        self.hidden_dropout_prob = 0.1\n-        self.attention_probs_dropout_prob = 0.1\n-        self.max_position_embeddings = 512\n-        self.type_vocab_size = 16\n-        self.type_sequence_label_size = 2\n-        self.initializer_range = 0.02\n-        self.num_labels = 3\n-        self.num_choices = 4\n-        self.summary_type = \"last\"\n-        self.use_proj = True\n-        self.scope = None\n-        self.bos_token_id = 0\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-        input_mask = random_attention_mask([self.batch_size, self.seq_length], dtype=tf.float32)\n-\n-        input_lengths = None\n-        if self.use_input_lengths:\n-            input_lengths = (\n-                ids_tensor([self.batch_size], vocab_size=2) + self.seq_length - 2\n-            )  # small variation of seq_length\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.n_langs)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        is_impossible_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            is_impossible_labels = ids_tensor([self.batch_size], 2, dtype=tf.float32)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = FlaubertConfig(\n-            vocab_size=self.vocab_size,\n-            n_special=self.n_special,\n-            emb_dim=self.hidden_size,\n-            n_layers=self.num_hidden_layers,\n-            n_heads=self.num_attention_heads,\n-            dropout=self.hidden_dropout_prob,\n-            attention_dropout=self.attention_probs_dropout_prob,\n-            gelu_activation=self.gelu_activation,\n-            sinusoidal_embeddings=self.sinusoidal_embeddings,\n-            asm=self.asm,\n-            causal=self.causal,\n-            n_langs=self.n_langs,\n-            max_position_embeddings=self.max_position_embeddings,\n-            initializer_range=self.initializer_range,\n-            summary_type=self.summary_type,\n-            use_proj=self.use_proj,\n-            bos_token_id=self.bos_token_id,\n-        )\n-\n-        return (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_lengths,\n-            sequence_labels,\n-            token_labels,\n-            is_impossible_labels,\n-            choice_labels,\n-            input_mask,\n-        )\n-\n-    def create_and_check_flaubert_model(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_lengths,\n-        sequence_labels,\n-        token_labels,\n-        is_impossible_labels,\n-        choice_labels,\n-        input_mask,\n-    ):\n-        model = TFFlaubertModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"lengths\": input_lengths, \"langs\": token_type_ids}\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_flaubert_lm_head(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_lengths,\n-        sequence_labels,\n-        token_labels,\n-        is_impossible_labels,\n-        choice_labels,\n-        input_mask,\n-    ):\n-        model = TFFlaubertWithLMHeadModel(config)\n-\n-        inputs = {\"input_ids\": input_ids, \"lengths\": input_lengths, \"langs\": token_type_ids}\n-        result = model(inputs)\n-\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_flaubert_qa(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_lengths,\n-        sequence_labels,\n-        token_labels,\n-        is_impossible_labels,\n-        choice_labels,\n-        input_mask,\n-    ):\n-        model = TFFlaubertForQuestionAnsweringSimple(config)\n-\n-        inputs = {\"input_ids\": input_ids, \"lengths\": input_lengths}\n-\n-        result = model(inputs)\n-\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-\n-    def create_and_check_flaubert_sequence_classif(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_lengths,\n-        sequence_labels,\n-        token_labels,\n-        is_impossible_labels,\n-        choice_labels,\n-        input_mask,\n-    ):\n-        model = TFFlaubertForSequenceClassification(config)\n-\n-        inputs = {\"input_ids\": input_ids, \"lengths\": input_lengths}\n-\n-        result = model(inputs)\n-\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n-\n-    def create_and_check_flaubert_for_token_classification(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_lengths,\n-        sequence_labels,\n-        token_labels,\n-        is_impossible_labels,\n-        choice_labels,\n-        input_mask,\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFFlaubertForTokenClassification(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def create_and_check_flaubert_for_multiple_choice(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_lengths,\n-        sequence_labels,\n-        token_labels,\n-        is_impossible_labels,\n-        choice_labels,\n-        input_mask,\n-    ):\n-        config.num_choices = self.num_choices\n-        model = TFFlaubertForMultipleChoice(config=config)\n-        multiple_choice_inputs_ids = tf.tile(tf.expand_dims(input_ids, 1), (1, self.num_choices, 1))\n-        multiple_choice_input_mask = tf.tile(tf.expand_dims(input_mask, 1), (1, self.num_choices, 1))\n-        multiple_choice_token_type_ids = tf.tile(tf.expand_dims(token_type_ids, 1), (1, self.num_choices, 1))\n-        inputs = {\n-            \"input_ids\": multiple_choice_inputs_ids,\n-            \"attention_mask\": multiple_choice_input_mask,\n-            \"token_type_ids\": multiple_choice_token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_choices))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_lengths,\n-            sequence_labels,\n-            token_labels,\n-            is_impossible_labels,\n-            choice_labels,\n-            input_mask,\n-        ) = config_and_inputs\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"token_type_ids\": token_type_ids,\n-            \"langs\": token_type_ids,\n-            \"lengths\": input_lengths,\n-        }\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFFlaubertModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFFlaubertModel,\n-            TFFlaubertWithLMHeadModel,\n-            TFFlaubertForSequenceClassification,\n-            TFFlaubertForQuestionAnsweringSimple,\n-            TFFlaubertForTokenClassification,\n-            TFFlaubertForMultipleChoice,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    all_generative_model_classes = (\n-        (TFFlaubertWithLMHeadModel,) if is_tf_available() else ()\n-    )  # TODO (PVP): Check other models whether language generation is also applicable\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFFlaubertModel,\n-            \"fill-mask\": TFFlaubertWithLMHeadModel,\n-            \"question-answering\": TFFlaubertForQuestionAnsweringSimple,\n-            \"text-classification\": TFFlaubertForSequenceClassification,\n-            \"token-classification\": TFFlaubertForTokenClassification,\n-            \"zero-shot\": TFFlaubertForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    # TODO: Fix the failed tests\n-    def is_pipeline_test_to_skip(\n-        self,\n-        pipeline_test_case_name,\n-        config_class,\n-        model_architecture,\n-        tokenizer_name,\n-        image_processor_name,\n-        feature_extractor_name,\n-        processor_name,\n-    ):\n-        if (\n-            pipeline_test_case_name == \"QAPipelineTests\"\n-            and tokenizer_name is not None\n-            and not tokenizer_name.endswith(\"Fast\")\n-        ):\n-            # `QAPipelineTests` fails for a few models when the slower tokenizer are used.\n-            # (The slower tokenizers were never used for pipeline tests before the pipeline testing rework)\n-            # TODO: check (and possibly fix) the `QAPipelineTests` with slower tokenizer\n-            return True\n-\n-        return False\n-\n-    def setUp(self):\n-        self.model_tester = TFFlaubertModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=FlaubertConfig, emb_dim=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_flaubert_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_flaubert_model(*config_and_inputs)\n-\n-    def test_flaubert_lm_head(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_flaubert_lm_head(*config_and_inputs)\n-\n-    def test_flaubert_qa(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_flaubert_qa(*config_and_inputs)\n-\n-    def test_flaubert_sequence_classif(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_flaubert_sequence_classif(*config_and_inputs)\n-\n-    def test_for_token_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_flaubert_for_token_classification(*config_and_inputs)\n-\n-    def test_for_multiple_choice(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_flaubert_for_multiple_choice(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"hf-internal-testing/tiny-random-flaubert\"\n-        model = TFFlaubertModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-\n-@require_tf\n-@require_sentencepiece\n-@require_tokenizers\n-class TFFlaubertModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_output_embeds_base_model(self):\n-        model = TFFlaubertModel.from_pretrained(\"jplu/tf-flaubert-small-cased\")\n-\n-        input_ids = tf.convert_to_tensor(\n-            [[0, 158, 735, 2592, 1424, 6727, 82, 1]],\n-            dtype=tf.int32,\n-        )  # \"J'aime flaubert !\"\n-\n-        output = model(input_ids)[0]\n-        expected_shape = tf.TensorShape((1, 8, 512))\n-        self.assertEqual(output.shape, expected_shape)\n-        # compare the actual values for a slice.\n-        expected_slice = tf.convert_to_tensor(\n-            [\n-                [\n-                    [-1.8768773, -1.566555, 0.27072418],\n-                    [-1.6920038, -0.5873505, 1.9329599],\n-                    [-2.9563985, -1.6993835, 1.7972052],\n-                ]\n-            ],\n-            dtype=tf.float32,\n-        )\n-\n-        self.assertTrue(np.allclose(output[:, :3, :3].numpy(), expected_slice.numpy(), atol=1e-4))"
        },
        {
            "sha": "673982eb7b65435bf96147b7e4e27081078299b2",
            "filename": "tests/models/funnel/test_modeling_tf_funnel.py",
            "status": "removed",
            "additions": 0,
            "deletions": 414,
            "changes": 414,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Ffunnel%2Ftest_modeling_tf_funnel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Ffunnel%2Ftest_modeling_tf_funnel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffunnel%2Ftest_modeling_tf_funnel.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,414 +0,0 @@\n-# Copyright 2020 HuggingFace Inc. team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import FunnelConfig, is_tf_available\n-from transformers.testing_utils import require_tf\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import (\n-        TFFunnelBaseModel,\n-        TFFunnelForMaskedLM,\n-        TFFunnelForMultipleChoice,\n-        TFFunnelForPreTraining,\n-        TFFunnelForQuestionAnswering,\n-        TFFunnelForSequenceClassification,\n-        TFFunnelForTokenClassification,\n-        TFFunnelModel,\n-    )\n-\n-\n-class TFFunnelModelTester:\n-    \"\"\"You can also import this e.g, from .test_modeling_funnel import FunnelModelTester\"\"\"\n-\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        block_sizes=[1, 1, 2],\n-        num_decoder_layers=1,\n-        d_model=32,\n-        n_head=4,\n-        d_head=8,\n-        d_inner=37,\n-        hidden_act=\"gelu_new\",\n-        hidden_dropout=0.1,\n-        attention_dropout=0.1,\n-        activation_dropout=0.0,\n-        max_position_embeddings=512,\n-        type_vocab_size=3,\n-        initializer_std=0.02,  # Set to a smaller value, so we can keep the small error threshold (1e-5) in the test\n-        num_labels=3,\n-        num_choices=4,\n-        scope=None,\n-        base=False,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.block_sizes = block_sizes\n-        self.num_decoder_layers = num_decoder_layers\n-        self.d_model = d_model\n-        self.n_head = n_head\n-        self.d_head = d_head\n-        self.d_inner = d_inner\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout = hidden_dropout\n-        self.attention_dropout = attention_dropout\n-        self.activation_dropout = activation_dropout\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = 2\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.scope = scope\n-        self.initializer_std = initializer_std\n-\n-        # Used in the tests to check the size of the first attention layer\n-        self.num_attention_heads = n_head\n-        # Used in the tests to check the size of the first hidden state\n-        self.hidden_size = self.d_model\n-        # Used in the tests to check the number of output hidden states/attentions\n-        self.num_hidden_layers = sum(self.block_sizes) + (0 if base else self.num_decoder_layers)\n-        # FunnelModel adds two hidden layers: input embeddings and the sum of the upsampled encoder hidden state with\n-        # the last hidden state of the first block (which is the first hidden state of the decoder).\n-        if not base:\n-            self.expected_num_hidden_layers = self.num_hidden_layers + 2\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = FunnelConfig(\n-            vocab_size=self.vocab_size,\n-            block_sizes=self.block_sizes,\n-            num_decoder_layers=self.num_decoder_layers,\n-            d_model=self.d_model,\n-            n_head=self.n_head,\n-            d_head=self.d_head,\n-            d_inner=self.d_inner,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout=self.hidden_dropout,\n-            attention_dropout=self.attention_dropout,\n-            activation_dropout=self.activation_dropout,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            initializer_std=self.initializer_std,\n-        )\n-\n-        return (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        )\n-\n-    def create_and_check_model(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        model = TFFunnelModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.d_model))\n-\n-        config.truncate_seq = False\n-        model = TFFunnelModel(config=config)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.d_model))\n-\n-        config.separate_cls = False\n-        model = TFFunnelModel(config=config)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.d_model))\n-\n-    def create_and_check_base_model(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        model = TFFunnelBaseModel(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-\n-        inputs = [input_ids, input_mask]\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, 2, self.d_model))\n-\n-        config.truncate_seq = False\n-        model = TFFunnelBaseModel(config=config)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, 3, self.d_model))\n-\n-        config.separate_cls = False\n-        model = TFFunnelBaseModel(config=config)\n-        result = model(input_ids)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, 2, self.d_model))\n-\n-    def create_and_check_for_pretraining(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        model = TFFunnelForPreTraining(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length))\n-\n-    def create_and_check_for_masked_lm(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        model = TFFunnelForMaskedLM(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_for_sequence_classification(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFFunnelForSequenceClassification(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_for_multiple_choice(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        config.num_choices = self.num_choices\n-        model = TFFunnelForMultipleChoice(config=config)\n-        multiple_choice_inputs_ids = tf.tile(tf.expand_dims(input_ids, 1), (1, self.num_choices, 1))\n-        multiple_choice_input_mask = tf.tile(tf.expand_dims(input_mask, 1), (1, self.num_choices, 1))\n-        multiple_choice_token_type_ids = tf.tile(tf.expand_dims(token_type_ids, 1), (1, self.num_choices, 1))\n-        inputs = {\n-            \"input_ids\": multiple_choice_inputs_ids,\n-            \"attention_mask\": multiple_choice_input_mask,\n-            \"token_type_ids\": multiple_choice_token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_choices))\n-\n-    def create_and_check_for_token_classification(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFFunnelForTokenClassification(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def create_and_check_for_question_answering(\n-        self,\n-        config,\n-        input_ids,\n-        token_type_ids,\n-        input_mask,\n-        sequence_labels,\n-        token_labels,\n-        choice_labels,\n-    ):\n-        model = TFFunnelForQuestionAnswering(config=config)\n-        inputs = {\"input_ids\": input_ids, \"attention_mask\": input_mask, \"token_type_ids\": token_type_ids}\n-        result = model(inputs)\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"token_type_ids\": token_type_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFFunnelModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFFunnelModel,\n-            TFFunnelForMaskedLM,\n-            TFFunnelForPreTraining,\n-            TFFunnelForQuestionAnswering,\n-            TFFunnelForTokenClassification,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": (TFFunnelBaseModel, TFFunnelModel),\n-            \"fill-mask\": TFFunnelForMaskedLM,\n-            \"question-answering\": TFFunnelForQuestionAnswering,\n-            \"text-classification\": TFFunnelForSequenceClassification,\n-            \"token-classification\": TFFunnelForTokenClassification,\n-            \"zero-shot\": TFFunnelForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFFunnelModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=FunnelConfig)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_for_pretraining(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_pretraining(*config_and_inputs)\n-\n-    def test_for_masked_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n-\n-    def test_for_token_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_token_classification(*config_and_inputs)\n-\n-    def test_for_question_answering(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_question_answering(*config_and_inputs)\n-\n-\n-@require_tf\n-class TFFunnelBaseModelTest(TFModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (TFFunnelBaseModel, TFFunnelForMultipleChoice, TFFunnelForSequenceClassification) if is_tf_available() else ()\n-    )\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFFunnelModelTester(self, base=True)\n-        self.config_tester = ConfigTester(self, config_class=FunnelConfig)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_base_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_base_model(*config_and_inputs)\n-\n-    def test_for_sequence_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n-\n-    def test_for_multiple_choice(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_multiple_choice(*config_and_inputs)"
        },
        {
            "sha": "8bd5a5bb416c4e5bc14946b64847cccb70dbbea1",
            "filename": "tests/models/gemma/test_modeling_flax_gemma.py",
            "status": "removed",
            "additions": 0,
            "deletions": 264,
            "changes": 264,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgemma%2Ftest_modeling_flax_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgemma%2Ftest_modeling_flax_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma%2Ftest_modeling_flax_gemma.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,264 +0,0 @@\n-# Copyright 2024 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import AutoTokenizer, GemmaConfig, is_flax_available\n-from transformers.testing_utils import require_flax, require_read_token, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor\n-\n-\n-if is_flax_available():\n-    import jax\n-    import jax.numpy as jnp\n-\n-    from transformers.models.gemma.modeling_flax_gemma import (\n-        FlaxGemmaForCausalLM,\n-        FlaxGemmaModel,\n-    )\n-\n-\n-class FlaxGemmaModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=2,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        num_key_value_heads=2,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        initializer_range=0.02,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.num_key_value_heads = num_key_value_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.initializer_range = initializer_range\n-        self.scope = None\n-        self.bos_token_id = vocab_size - 1\n-        self.eos_token_id = vocab_size - 1\n-        self.pad_token_id = vocab_size - 1\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = np.tril(np.ones((self.batch_size, self.seq_length)))\n-\n-        config = GemmaConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            num_key_value_heads=self.num_key_value_heads,\n-            head_dim=self.hidden_size // self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            use_cache=True,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, input_ids, input_mask\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-        return config, inputs_dict\n-\n-    def check_use_cache_forward(self, model_class_name, config, input_ids, attention_mask):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        past_key_values = model.init_cache(input_ids.shape[0], max_decoder_length)\n-        attention_mask = jnp.ones((input_ids.shape[0], max_decoder_length), dtype=\"i4\")\n-\n-        position_ids = jnp.broadcast_to(\n-            jnp.arange(input_ids.shape[-1] - 1)[None, :], (input_ids.shape[0], input_ids.shape[-1] - 1)\n-        )\n-        outputs_cache = model(\n-            input_ids[:, :-1],\n-            attention_mask=attention_mask,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n-\n-        position_ids = jnp.array(input_ids.shape[0] * [[input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model(\n-            input_ids[:, -1:],\n-            attention_mask=attention_mask,\n-            past_key_values=outputs_cache.past_key_values,\n-            position_ids=position_ids,\n-        )\n-\n-        outputs = model(input_ids)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-    def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input_ids, attention_mask):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        attention_mask_cache = jnp.concatenate(\n-            [attention_mask, jnp.zeros((attention_mask.shape[0], max_decoder_length - attention_mask.shape[1]))],\n-            axis=-1,\n-        )\n-\n-        past_key_values = model.init_cache(input_ids.shape[0], max_decoder_length)\n-        position_ids = jnp.broadcast_to(\n-            jnp.arange(input_ids.shape[-1] - 1)[None, :], (input_ids.shape[0], input_ids.shape[-1] - 1)\n-        )\n-\n-        outputs_cache = model(\n-            input_ids[:, :-1],\n-            attention_mask=attention_mask_cache,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n-        position_ids = jnp.array(input_ids.shape[0] * [[input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model(\n-            input_ids[:, -1:],\n-            past_key_values=outputs_cache.past_key_values,\n-            attention_mask=attention_mask_cache,\n-            position_ids=position_ids,\n-        )\n-\n-        outputs = model(input_ids, attention_mask=attention_mask)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-\n-@require_flax\n-class FlaxGemmaModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (FlaxGemmaModel, FlaxGemmaForCausalLM) if is_flax_available() else ()\n-\n-    def setUp(self):\n-        self.model_tester = FlaxGemmaModelTester(self)\n-\n-    def test_use_cache_forward(self):\n-        for model_class_name in self.all_model_classes:\n-            config, input_ids, attention_mask = self.model_tester.prepare_config_and_inputs()\n-            self.model_tester.check_use_cache_forward(model_class_name, config, input_ids, attention_mask)\n-\n-    def test_use_cache_forward_with_attn_mask(self):\n-        for model_class_name in self.all_model_classes:\n-            config, input_ids, attention_mask = self.model_tester.prepare_config_and_inputs()\n-            self.model_tester.check_use_cache_forward_with_attn_mask(\n-                model_class_name, config, input_ids, attention_mask\n-            )\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"google/gemma-2b\", from_pt=True)\n-            outputs = model(np.ones((1, 1)))\n-            self.assertIsNotNone(outputs)\n-\n-\n-@slow\n-@require_flax\n-@require_read_token\n-class FlaxGemmaIntegrationTest(unittest.TestCase):\n-    input_text = [\"The capital of France is\", \"To play the perfect cover drive\"]\n-    model_id = \"google/gemma-2b\"\n-    revision = \"flax\"\n-\n-    def setUp(self):\n-        self.model, self.params = FlaxGemmaForCausalLM.from_pretrained(\n-            self.model_id, revision=self.revision, _do_init=False\n-        )\n-        self.tokenizer = AutoTokenizer.from_pretrained(self.model_id)\n-        self.tokenizer.padding_side = \"left\"\n-\n-    def test_logits(self):\n-        inputs = self.tokenizer(self.input_text, return_tensors=\"np\", padding=True)\n-        # fmt: off\n-        EXPECTED_MEAN = [\n-            [-16.427, -21.386, -35.491, -36.258, -31.401, -36.370, -37.598],\n-            [-21.386, -32.150, -33.155, -34.344, -34.706, -34.678, -38.495],\n-        ]\n-        EXPECTED_SLICE = [-33.462, -16.481, -30.837, -32.195, -33.113]\n-        # fmt: on\n-\n-        logits = self.model(**inputs, params=self.params).logits\n-\n-        diff_mean = jnp.abs(logits.mean(-1) - np.array(EXPECTED_MEAN)).max()\n-        diff_slice = jnp.abs(logits[0, -1, 475:480] - np.array(EXPECTED_SLICE)).max()\n-\n-        self.assertAlmostEqual(diff_mean, 0, places=3)\n-        self.assertAlmostEqual(diff_slice, 0, places=3)\n-\n-    def test_generation(self):\n-        EXPECTED_TEXTS = [\n-            \"The capital of France is a city of contrasts. It is a city of history, of art, of culture, of fashion\",\n-            \"To play the perfect cover drive, you need to have a good technique and a good mindset.\\n\\nThe cover drive is a shot\",\n-        ]\n-        inputs = self.tokenizer(self.input_text, return_tensors=\"np\", padding=True)\n-\n-        output = self.model.generate(**inputs, params=self.params, max_new_tokens=20, do_sample=False)\n-        output_text = self.tokenizer.batch_decode(output.sequences, skip_special_tokens=True)\n-\n-        self.assertEqual(output_text, EXPECTED_TEXTS)\n-\n-    def test_jit_generation(self):\n-        EXPECTED_TEXTS = [\n-            \"The capital of France is a city of contrasts. It is a city of history, culture, and art, but it is\",\n-            \"To play the perfect cover drive, you need to have a good technique and a good mindset.\\n\\nThe cover drive is a shot\",\n-        ]\n-        inputs = self.tokenizer(self.input_text, return_tensors=\"np\", padding=True)\n-\n-        def generate(input_ids, attention_mask):\n-            outputs = self.model.generate(\n-                input_ids, attention_mask=attention_mask, params=self.params, max_new_tokens=20, do_sample=False\n-            )\n-            return outputs\n-\n-        jit_generate = jax.jit(generate)\n-        output_sequences = jit_generate(**inputs).sequences\n-        output_text = self.tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n-\n-        self.assertEqual(output_text, EXPECTED_TEXTS)"
        },
        {
            "sha": "3297a3c45db79b847a1e4350cf11be636f331955",
            "filename": "tests/models/gpt2/test_modeling_flax_gpt2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 254,
            "changes": 254,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgpt2%2Ftest_modeling_flax_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgpt2%2Ftest_modeling_flax_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_flax_gpt2.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,254 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import GPT2Config, GPT2Tokenizer, is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-\n-\n-if is_flax_available():\n-    import jax\n-    import jax.numpy as jnp\n-\n-    from transformers.models.gpt2.modeling_flax_gpt2 import FlaxGPT2LMHeadModel, FlaxGPT2Model\n-\n-\n-class FlaxGPT2ModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=14,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        initializer_range=0.02,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.initializer_range = initializer_range\n-        self.scope = None\n-        self.bos_token_id = vocab_size - 1\n-        self.eos_token_id = vocab_size - 1\n-        self.pad_token_id = vocab_size - 1\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        config = GPT2Config(\n-            vocab_size=self.vocab_size,\n-            n_embd=self.hidden_size,\n-            n_layer=self.num_hidden_layers,\n-            n_head=self.num_attention_heads,\n-            n_positions=self.max_position_embeddings,\n-            use_cache=False,\n-            bos_token_id=self.bos_token_id,\n-            eos_token_id=self.eos_token_id,\n-            pad_token_id=self.pad_token_id,\n-        )\n-\n-        return (config, input_ids, input_mask)\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-        return config, inputs_dict\n-\n-    def prepare_config_and_inputs_for_decoder(self):\n-        config, input_ids, attention_mask = self.prepare_config_and_inputs()\n-\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            attention_mask,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n-    def check_use_cache_forward(self, model_class_name, config, input_ids, attention_mask):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        past_key_values = model.init_cache(input_ids.shape[0], max_decoder_length)\n-        attention_mask = jnp.ones((input_ids.shape[0], max_decoder_length), dtype=\"i4\")\n-\n-        position_ids = jnp.broadcast_to(\n-            jnp.arange(input_ids.shape[-1] - 1)[None, :], (input_ids.shape[0], input_ids.shape[-1] - 1)\n-        )\n-        outputs_cache = model(\n-            input_ids[:, :-1],\n-            attention_mask=attention_mask,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n-\n-        position_ids = jnp.array(input_ids.shape[0] * [[input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model(\n-            input_ids[:, -1:],\n-            attention_mask=attention_mask,\n-            past_key_values=outputs_cache.past_key_values,\n-            position_ids=position_ids,\n-        )\n-\n-        outputs = model(input_ids)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-    def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input_ids, attention_mask):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        attention_mask_cache = jnp.concatenate(\n-            [attention_mask, jnp.zeros((attention_mask.shape[0], max_decoder_length - attention_mask.shape[1]))],\n-            axis=-1,\n-        )\n-\n-        past_key_values = model.init_cache(input_ids.shape[0], max_decoder_length)\n-        position_ids = jnp.broadcast_to(\n-            jnp.arange(input_ids.shape[-1] - 1)[None, :], (input_ids.shape[0], input_ids.shape[-1] - 1)\n-        )\n-\n-        outputs_cache = model(\n-            input_ids[:, :-1],\n-            attention_mask=attention_mask_cache,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n-        position_ids = jnp.array(input_ids.shape[0] * [[input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model(\n-            input_ids[:, -1:],\n-            past_key_values=outputs_cache.past_key_values,\n-            attention_mask=attention_mask_cache,\n-            position_ids=position_ids,\n-        )\n-\n-        outputs = model(input_ids, attention_mask=attention_mask)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-    def check_bool_attention_mask_in_generation(self, model_class_name, config, input_ids, attention_mask):\n-        model = model_class_name(config)\n-\n-        output_int_att_mask = model.generate(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask,\n-            max_new_tokens=3,\n-        )\n-\n-        output_bool_att_mask = model.generate(\n-            input_ids=input_ids,\n-            attention_mask=attention_mask.astype(bool),\n-            max_new_tokens=3,\n-        )\n-\n-        self.parent.assertTrue(\n-            (output_bool_att_mask.sequences == output_int_att_mask.sequences).all(),\n-            \"Generated response differ between boolean and integer attention mask\",\n-        )\n-\n-\n-@require_flax\n-class FlaxGPT2ModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (FlaxGPT2Model, FlaxGPT2LMHeadModel) if is_flax_available() else ()\n-\n-    def setUp(self):\n-        self.model_tester = FlaxGPT2ModelTester(self)\n-\n-    def test_use_cache_forward(self):\n-        for model_class_name in self.all_model_classes:\n-            config, input_ids, attention_mask = self.model_tester.prepare_config_and_inputs()\n-            self.model_tester.check_use_cache_forward(model_class_name, config, input_ids, attention_mask)\n-\n-    def test_use_cache_forward_with_attn_mask(self):\n-        for model_class_name in self.all_model_classes:\n-            config, input_ids, attention_mask = self.model_tester.prepare_config_and_inputs()\n-            self.model_tester.check_use_cache_forward_with_attn_mask(\n-                model_class_name, config, input_ids, attention_mask\n-            )\n-\n-    def test_bool_attention_mask_in_generation(self):\n-        for model_class_name in self.all_generative_model_classes:\n-            config, input_ids, attention_mask = self.model_tester.prepare_config_and_inputs()\n-            self.model_tester.check_bool_attention_mask_in_generation(\n-                model_class_name, config, input_ids, attention_mask\n-            )\n-\n-    @slow\n-    def test_batch_generation(self):\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\", pad_token=\"</s>\", padding_side=\"left\")\n-        inputs = tokenizer([\"Hello this is a long string\", \"Hey\"], return_tensors=\"np\", padding=True, truncation=True)\n-\n-        model = FlaxGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n-        model.do_sample = False\n-        model.config.pad_token_id = model.config.eos_token_id\n-\n-        jit_generate = jax.jit(model.generate)\n-\n-        output_sequences = jit_generate(inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"]).sequences\n-\n-        output_string = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n-\n-        expected_string = [\n-            \"Hello this is a long string of words. I'm going to start with the first one.\\n\",\n-            \"Hey, I'm not sure if I'm going to be able to do\",\n-        ]\n-\n-        self.assertListEqual(output_string, expected_string)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"openai-community/gpt2\", from_pt=True)\n-            outputs = model(np.ones((1, 1)))\n-            self.assertIsNotNone(outputs)"
        },
        {
            "sha": "76ecd6d15bc251cca051a37571ceefadf84e3bdf",
            "filename": "tests/models/gpt2/test_modeling_tf_gpt2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 732,
            "changes": 732,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgpt2%2Ftest_modeling_tf_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgpt2%2Ftest_modeling_tf_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt2%2Ftest_modeling_tf_gpt2.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,732 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import GPT2Config, is_tf_available\n-from transformers.testing_utils import require_tf, require_tf2onnx, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-from ...utils.test_modeling_tf_core import TFCoreModelTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import GPT2Tokenizer\n-    from transformers.models.gpt2.modeling_tf_gpt2 import (\n-        TFGPT2DoubleHeadsModel,\n-        TFGPT2ForSequenceClassification,\n-        TFGPT2LMHeadModel,\n-        TFGPT2Model,\n-    )\n-    from transformers.tf_utils import shape_list\n-\n-\n-class TFGPT2ModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-    ):\n-        self.parent = parent\n-        self.batch_size = 13\n-        self.seq_length = 7\n-        self.is_training = True\n-        self.use_token_type_ids = True\n-        self.use_input_mask = True\n-        self.use_labels = True\n-        self.use_mc_token_ids = True\n-        self.vocab_size = 99\n-        self.hidden_size = 32\n-        self.num_hidden_layers = 2\n-        self.num_attention_heads = 4\n-        self.intermediate_size = 37\n-        self.hidden_act = \"gelu\"\n-        self.hidden_dropout_prob = 0.1\n-        self.attention_probs_dropout_prob = 0.1\n-        self.max_position_embeddings = 512\n-        self.type_vocab_size = 16\n-        self.type_sequence_label_size = 2\n-        self.initializer_range = 0.02\n-        self.num_labels = 3\n-        self.num_choices = 4\n-        self.scope = None\n-        self.bos_token_id = self.vocab_size - 1\n-        self.eos_token_id = self.vocab_size - 1\n-        self.pad_token_id = self.vocab_size - 1\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        mc_token_ids = None\n-        if self.use_mc_token_ids:\n-            mc_token_ids = ids_tensor([self.batch_size, self.num_choices], self.seq_length)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = GPT2Config(\n-            vocab_size=self.vocab_size,\n-            n_embd=self.hidden_size,\n-            n_layer=self.num_hidden_layers,\n-            n_head=self.num_attention_heads,\n-            # intermediate_size=self.intermediate_size,\n-            # hidden_act=self.hidden_act,\n-            # hidden_dropout_prob=self.hidden_dropout_prob,\n-            # attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            n_positions=self.max_position_embeddings,\n-            # type_vocab_size=self.type_vocab_size,\n-            # initializer_range=self.initializer_range\n-            bos_token_id=self.bos_token_id,\n-            eos_token_id=self.eos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            return_dict=True,\n-        )\n-\n-        head_mask = ids_tensor([self.num_hidden_layers, self.num_attention_heads], 2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        )\n-\n-    def prepare_config_and_inputs_for_decoder(self):\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = self.prepare_config_and_inputs()\n-\n-        encoder_hidden_states = floats_tensor([self.batch_size, self.seq_length, self.hidden_size])\n-        encoder_attention_mask = ids_tensor([self.batch_size, self.seq_length], vocab_size=2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-            encoder_hidden_states,\n-            encoder_attention_mask,\n-        )\n-\n-    def create_and_check_gpt2_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n-        model = TFGPT2Model(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-\n-        inputs = [input_ids, None, input_mask]  # None is the input for 'past'\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_gpt2_model_past(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n-        model = TFGPT2Model(config=config)\n-\n-        # first forward pass\n-        outputs = model(input_ids, token_type_ids=token_type_ids, use_cache=True)\n-        outputs_use_cache_conf = model(input_ids, token_type_ids=token_type_ids)\n-        outputs_no_past = model(input_ids, token_type_ids=token_type_ids, use_cache=False)\n-\n-        self.parent.assertTrue(len(outputs) == len(outputs_use_cache_conf))\n-        self.parent.assertTrue(len(outputs) == len(outputs_no_past) + 1)\n-\n-        output, past_key_values = outputs.to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n-        next_token_types = ids_tensor([self.batch_size, 1], self.type_vocab_size)\n-\n-        # append to next input_ids and token_type_ids\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1)\n-\n-        output_from_no_past = model(next_input_ids, token_type_ids=next_token_type_ids)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, token_type_ids=next_token_types, past_key_values=past_key_values)[\n-            \"last_hidden_state\"\n-        ]\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), shape_list(output_from_past)[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, 0, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)\n-\n-    def create_and_check_gpt2_model_attention_mask_past(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n-    ):\n-        model = TFGPT2Model(config=config)\n-\n-        # create attention mask\n-        half_seq_length = self.seq_length // 2\n-        attn_mask_begin = tf.ones((self.batch_size, half_seq_length), dtype=tf.int32)\n-        attn_mask_end = tf.zeros((self.batch_size, self.seq_length - half_seq_length), dtype=tf.int32)\n-        attn_mask = tf.concat([attn_mask_begin, attn_mask_end], axis=1)\n-\n-        # first forward pass\n-        output, past_key_values = model(input_ids, attention_mask=attn_mask).to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n-\n-        # change a random masked slice from input_ids\n-        random_seq_idx_to_change = ids_tensor((1,), half_seq_length).numpy() + 1\n-        random_other_next_tokens = ids_tensor((self.batch_size, self.seq_length), config.vocab_size)\n-        vector_condition = tf.range(self.seq_length) == (self.seq_length - random_seq_idx_to_change)\n-        condition = tf.transpose(\n-            tf.broadcast_to(tf.expand_dims(vector_condition, -1), (self.seq_length, self.batch_size))\n-        )\n-        input_ids = tf.where(condition, random_other_next_tokens, input_ids)\n-\n-        # append to next input_ids and attn_mask\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        attn_mask = tf.concat([attn_mask, tf.ones((shape_list(attn_mask)[0], 1), dtype=tf.int32)], axis=1)\n-\n-        # get two different outputs\n-        output_from_no_past = model(next_input_ids, attention_mask=attn_mask)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, past_key_values=past_key_values, attention_mask=attn_mask)[\n-            \"last_hidden_state\"\n-        ]\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), shape_list(output_from_past)[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, 0, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-12)\n-\n-    def create_and_check_gpt2_model_past_large_inputs(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n-    ):\n-        model = TFGPT2Model(config=config)\n-\n-        input_ids = input_ids[:1, :]\n-        input_mask = input_mask[:1, :]\n-        token_type_ids = token_type_ids[:1, :]\n-        self.batch_size = 1\n-\n-        # first forward pass\n-        outputs = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, use_cache=True)\n-\n-        output, past_key_values = outputs.to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_attn_mask = ids_tensor((self.batch_size, 3), 2)\n-        next_token_types = ids_tensor((self.batch_size, 3), self.type_vocab_size)\n-\n-        # append to next input_ids and token_type_ids\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        next_attention_mask = tf.concat([input_mask, next_attn_mask], axis=-1)\n-        next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids, token_type_ids=next_token_type_ids, attention_mask=next_attention_mask\n-        )[\"last_hidden_state\"]\n-        output_from_past = model(\n-            next_tokens,\n-            token_type_ids=next_token_types,\n-            attention_mask=next_attention_mask,\n-            past_key_values=past_key_values,\n-        )[\"last_hidden_state\"]\n-        self.parent.assertTrue(output_from_past.shape[1] == next_tokens.shape[1])\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), shape_list(output_from_past)[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)\n-\n-    def create_and_check_gpt2_lm_head(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n-        model = TFGPT2LMHeadModel(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_gpt2_double_head(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, mc_token_ids, *args\n-    ):\n-        model = TFGPT2DoubleHeadsModel(config=config)\n-\n-        multiple_choice_inputs_ids = tf.tile(tf.expand_dims(input_ids, 1), (1, self.num_choices, 1))\n-        multiple_choice_input_mask = tf.tile(tf.expand_dims(input_mask, 1), (1, self.num_choices, 1))\n-        multiple_choice_token_type_ids = tf.tile(tf.expand_dims(token_type_ids, 1), (1, self.num_choices, 1))\n-\n-        inputs = {\n-            \"input_ids\": multiple_choice_inputs_ids,\n-            \"mc_token_ids\": mc_token_ids,\n-            \"attention_mask\": multiple_choice_input_mask,\n-            \"token_type_ids\": multiple_choice_token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(\n-            result.logits.shape, (self.batch_size, self.num_choices, self.seq_length, self.vocab_size)\n-        )\n-        self.parent.assertEqual(result.mc_logits.shape, (self.batch_size, self.num_choices))\n-\n-    def create_and_check_gpt2_for_sequence_classification(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, mc_token_ids, sequence_labels, *args\n-    ):\n-        config.num_labels = self.num_labels\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-            \"labels\": sequence_labels,\n-        }\n-        model = TFGPT2ForSequenceClassification(config)\n-\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"token_type_ids\": token_type_ids,\n-            \"attention_mask\": input_mask,\n-        }\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFGPT2ModelTest(TFModelTesterMixin, TFCoreModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (TFGPT2Model, TFGPT2LMHeadModel, TFGPT2ForSequenceClassification, TFGPT2DoubleHeadsModel)\n-        if is_tf_available()\n-        else ()\n-    )\n-    all_generative_model_classes = (TFGPT2LMHeadModel,) if is_tf_available() else ()\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFGPT2Model,\n-            \"text-classification\": TFGPT2ForSequenceClassification,\n-            \"text-generation\": TFGPT2LMHeadModel,\n-            \"zero-shot\": TFGPT2ForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_head_masking = False\n-    test_onnx = True\n-    onnx_min_opset = 10\n-\n-    def setUp(self):\n-        self.model_tester = TFGPT2ModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=GPT2Config, n_embd=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_gpt2_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_model(*config_and_inputs)\n-\n-    def test_gpt2_model_past(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_model_past(*config_and_inputs)\n-\n-    def test_gpt2_model_att_mask_past(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_model_attention_mask_past(*config_and_inputs)\n-\n-    def test_gpt2_model_past_large_inputs(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_model_past_large_inputs(*config_and_inputs)\n-\n-    def test_gpt2_lm_head(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_lm_head(*config_and_inputs)\n-\n-    def test_gpt2_double_head(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_double_head(*config_and_inputs)\n-\n-    def test_gpt2_sequence_classification_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gpt2_for_sequence_classification(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"openai-community/gpt2\"\n-        model = TFGPT2Model.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    # overwrite from common since ONNX runtime optimization doesn't work with tf.gather() when the argument\n-    # `batch_dims` > 0\"\n-    @require_tf2onnx\n-    @slow\n-    def test_onnx_runtime_optimize(self):\n-        if not self.test_onnx:\n-            return\n-\n-        import onnxruntime\n-        import tf2onnx\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            # Skip these 2 classes which uses `tf.gather` with `batch_dims=1`\n-            if model_class in [TFGPT2ForSequenceClassification, TFGPT2DoubleHeadsModel]:\n-                continue\n-\n-            model = model_class(config)\n-            model.build_in_name_scope()\n-\n-            onnx_model_proto, _ = tf2onnx.convert.from_keras(model, opset=self.onnx_min_opset)\n-\n-            onnxruntime.InferenceSession(onnx_model_proto.SerializeToString())\n-\n-    # TODO (Joao): fix me\n-    @unittest.skip(\"Onnx compliance broke with TF 2.10\")\n-    def test_onnx_compliancy(self):\n-        pass\n-\n-\n-@require_tf\n-class TFGPT2ModelLanguageGenerationTest(unittest.TestCase):\n-    @slow\n-    def test_lm_generate_greedy_distilgpt2_batch_special(self):\n-        model = TFGPT2LMHeadModel.from_pretrained(\"distilbert/distilgpt2\")\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n-\n-        tokenizer.pad_token = tokenizer.eos_token\n-        tokenizer.padding_side = \"left\"\n-\n-        sentences = [\"Today is a beautiful day and\", \"Yesterday was\"]\n-        input_ids = tokenizer(sentences, return_tensors=\"tf\", padding=True)\n-\n-        generation_kwargs = {\n-            \"bad_words_ids\": [tokenizer(\"is\").input_ids, tokenizer(\"angry about\").input_ids],\n-            \"no_repeat_ngram_size\": 2,\n-            \"do_sample\": False,\n-            \"repetition_penalty\": 1.3,\n-        }\n-\n-        output_ids = model.generate(**input_ids, **generation_kwargs)\n-\n-        output_strings = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-        expected_output_string = [\n-            \"Today is a beautiful day and I am so happy to be able take part in this amazing event.\",\n-            \"Yesterday was a very interesting time for the world to see how much of this is\",\n-        ]\n-        self.assertListEqual(output_strings, expected_output_string)\n-\n-    @slow\n-    def test_lm_generate_sample_distilgpt2_batch_special(self):\n-        model = TFGPT2LMHeadModel.from_pretrained(\"distilbert/distilgpt2\")\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n-\n-        tokenizer.pad_token = tokenizer.eos_token\n-        tokenizer.padding_side = \"left\"\n-\n-        sentences = [\"Today is a beautiful day and\", \"Yesterday was\"]\n-        input_ids = tokenizer(sentences, return_tensors=\"tf\", padding=True)\n-\n-        generation_kwargs = {\n-            \"do_sample\": True,\n-            \"bad_words_ids\": [tokenizer(\"is\").input_ids, tokenizer(\"angry about\").input_ids],\n-            \"no_repeat_ngram_size\": 2,\n-            \"repetition_penalty\": 1.3,\n-            \"temperature\": 1.5,\n-            \"top_k\": 500,\n-            \"top_p\": 0.9,\n-            \"seed\": [42, 0],  # seed set -> deterministic sampling sequence -> deterministic generation\n-        }\n-\n-        # forces the generation to happen on CPU, to avoid GPU-related quirks\n-        with tf.device(\":/CPU:0\"):\n-            output_ids = model.generate(**input_ids, **generation_kwargs)\n-\n-        output_strings = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-\n-        expected_output_string = [\n-            \"Today is a beautiful day and we will make you feel very hot/terrific in all your\",\n-            \"Yesterday was known by national television networks as Le Big Show or Wild Dog Jeopard\",\n-        ]\n-        self.assertListEqual(output_strings, expected_output_string)\n-\n-    @slow\n-    def test_lm_generate_greedy_distilgpt2_beam_search_special(self):\n-        model = TFGPT2LMHeadModel.from_pretrained(\"distilbert/distilgpt2\")\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n-\n-        tokenizer.pad_token = tokenizer.eos_token\n-        tokenizer.padding_side = \"left\"\n-\n-        sentences = [\"Today is a beautiful day and\", \"Yesterday was\"]\n-        input_ids = tokenizer(sentences, return_tensors=\"tf\", padding=True)\n-\n-        generation_kwargs = {\n-            \"bad_words_ids\": [tokenizer(\"is\").input_ids, tokenizer(\"angry about\").input_ids],\n-            \"no_repeat_ngram_size\": 2,\n-            \"do_sample\": False,\n-            \"num_beams\": 2,\n-        }\n-\n-        output_ids = model.generate(**input_ids, **generation_kwargs)\n-\n-        output_strings = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-        expected_output_string = [\n-            \"Today is a beautiful day and a great day for all of us.\\n\\nIm\",\n-            \"Yesterday was the first time that a person has been arrested in the United States for\",\n-        ]\n-        self.assertListEqual(output_strings, expected_output_string)\n-\n-    @slow\n-    def test_lm_generate_distilgpt2_left_padding(self):\n-        \"\"\"Tests that the generated text is the same, regardless of left padding\"\"\"\n-        model = TFGPT2LMHeadModel.from_pretrained(\"distilbert/distilgpt2\")\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"distilbert/distilgpt2\")\n-\n-        tokenizer.pad_token = tokenizer.eos_token\n-        tokenizer.padding_side = \"left\"\n-\n-        generation_kwargs = {\n-            \"bad_words_ids\": [tokenizer(\"is\").input_ids, tokenizer(\"angry about\").input_ids],\n-            \"no_repeat_ngram_size\": 2,\n-            \"do_sample\": False,\n-            \"repetition_penalty\": 1.3,\n-        }\n-        expected_output_string = (\n-            \"Today is a beautiful day and I am so happy to be able take part in this amazing event.\"\n-        )\n-\n-        sentences = [\"Today is a beautiful day and\"]\n-        input_ids = tokenizer(sentences, return_tensors=\"tf\", padding=True)\n-        # using default length\n-        output_ids = model.generate(**input_ids, **generation_kwargs)\n-        output_strings = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-        self.assertEqual(output_strings[0], expected_output_string)\n-\n-        sentences = [\"Today is a beautiful day and\", \"This is a very long input that we absolutely don't care about\"]\n-        input_ids = tokenizer(sentences, return_tensors=\"tf\", padding=True)\n-        # longer max length to capture the full length (remember: it is left padded)\n-        output_ids = model.generate(**input_ids, **generation_kwargs, max_length=27)\n-        output_strings = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-        self.assertEqual(output_strings[0], expected_output_string)\n-\n-    @slow\n-    def test_lm_generate_gpt2_greedy_xla(self):\n-        model = TFGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n-\n-        tokenizer.pad_token = tokenizer.eos_token\n-        tokenizer.padding_side = \"left\"\n-\n-        sentences = [\"The dog\", \"The flying machine\"]\n-        expected_output_strings = [\n-            \"The dog was found in a field near the intersection of West and West Streets.\\n\\nThe\",\n-            \"The flying machine is a small, lightweight, and lightweight aircraft that can be used for any type of\",\n-        ]\n-        input_ids = tokenizer(sentences, return_tensors=\"tf\", padding=True)\n-\n-        output_ids = model.generate(**input_ids, do_sample=False)\n-        output_strings = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-        self.assertListEqual(output_strings, expected_output_strings)\n-\n-        xla_generate = tf.function(model.generate, jit_compile=True)\n-        output_ids = xla_generate(**input_ids, do_sample=False)\n-        output_strings = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-        self.assertListEqual(output_strings, expected_output_strings)\n-\n-    @slow\n-    def test_lm_generate_gpt2_sample_xla(self):\n-        # NOTE: due to the small numerical differences that are natural when we compile to XLA, sampling the same\n-        # output out of the same seed is far from guaranteed. We can, however, confirm that the results are sensible\n-        # and that we can seed both versions.\n-\n-        # forces the generation to happen on CPU, to avoid GPU-related quirks\n-        with tf.device(\":/CPU:0\"):\n-            model = TFGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n-            tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n-\n-            tokenizer.pad_token = tokenizer.eos_token\n-            tokenizer.padding_side = \"left\"\n-\n-            sentence = [\"The dog\", \"The flying machine\"]\n-            expected_output_string = [\n-                \"The dog owner asked why did our vet decide there needed to be extra ventilation inside because most\"\n-                \" puppies\",\n-                \"The flying machine was made by an artist who found it difficult to control it as it did not use\",\n-            ]\n-            expected_output_string_xla = [\n-                \"The dog has been named in connection with the murder of a 20-year-old man in\",\n-                \"The flying machine is a new and improved system to operate and operate a new system and system \"\n-                \"system system\",\n-            ]\n-            input_ids = tokenizer(sentence, return_tensors=\"tf\", padding=True)\n-\n-            output_ids = model.generate(**input_ids, do_sample=True, seed=[7, 0])\n-            output_strings = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-            self.assertListEqual(output_strings, expected_output_string)\n-\n-            xla_generate = tf.function(model.generate, jit_compile=True)\n-            output_ids = xla_generate(**input_ids, do_sample=True, seed=[7, 0])\n-            output_strings = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-            self.assertListEqual(output_strings, expected_output_string_xla)\n-\n-    @slow\n-    def test_lm_generate_gpt2_beam_search_xla(self):\n-        model = TFGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2\")\n-        tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2\")\n-\n-        tokenizer.pad_token = tokenizer.eos_token\n-        tokenizer.padding_side = \"left\"\n-\n-        sentences = [\"The dog\", \"The flying machine\"]\n-        expected_output_strings = [\n-            \"The dog was found in the backyard of a home in the 6500 block of South Main Street\",\n-            \"The flying machine is a very powerful machine, but it's not a very powerful machine. It's\",\n-        ]\n-        input_ids = tokenizer(sentences, return_tensors=\"tf\", padding=True)\n-\n-        output_ids = model.generate(**input_ids, do_sample=False, num_beams=2)\n-        output_strings = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-        self.assertListEqual(output_strings, expected_output_strings)\n-\n-        xla_generate = tf.function(model.generate, jit_compile=True)\n-        output_ids = xla_generate(**input_ids, do_sample=False, num_beams=2)\n-        output_strings = tokenizer.batch_decode(output_ids, skip_special_tokens=True)\n-        self.assertListEqual(output_strings, expected_output_strings)\n-\n-    @slow\n-    def test_contrastive_search_gpt2(self):\n-        article = (\n-            \"DeepMind Technologies is a British artificial intelligence subsidiary of Alphabet Inc. and research \"\n-            \"laboratory founded in 2010. DeepMind was acquired by Google in 2014. The company is based\"\n-        )\n-\n-        gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-large\")\n-        gpt2_model = TFGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-large\")\n-        input_ids = gpt2_tokenizer(article, return_tensors=\"tf\")\n-\n-        outputs = gpt2_model.generate(**input_ids, penalty_alpha=0.6, top_k=4, max_length=256)\n-\n-        generated_text = gpt2_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-\n-        self.assertListEqual(\n-            generated_text,\n-            [\n-                \"DeepMind Technologies is a British artificial intelligence subsidiary of Alphabet Inc. and research \"\n-                \"laboratory founded in 2010. DeepMind was acquired by Google in 2014. The company is based in London, \"\n-                \"United Kingdom\\n\\nGoogle has a lot of data on its users and uses it to improve its products, such as \"\n-                \"Google Now, which helps users find the information they're looking for on the web. But the company \"\n-                \"is not the only one to collect data on its users. Facebook, for example, has its own facial \"\n-                \"recognition technology, as well as a database of millions of photos that it uses to personalize its \"\n-                \"News Feed.\\n\\nFacebook's use of data is a hot topic in the tech industry, with privacy advocates \"\n-                \"concerned about the company's ability to keep users' information private. In a blog post last \"\n-                'year, Facebook CEO Mark Zuckerberg said his company would \"do our best to be transparent about our '\n-                'data use and how we use it.\"\\n\\n\"We have made it clear that we do not sell or share your data with '\n-                'third parties,\" Zuckerberg wrote. \"If you have questions or concerns, please reach out to us at '\n-                'privacy@facebook.com.\"\\n\\nGoogle declined to comment on the privacy implications of its use of data, '\n-                \"but said in a statement to The Associated Press that\"\n-            ],\n-        )\n-\n-    @slow\n-    def test_contrastive_search_gpt2_xla(self):\n-        article = (\n-            \"DeepMind Technologies is a British artificial intelligence subsidiary of Alphabet Inc. and research \"\n-            \"laboratory founded in 2010. DeepMind was acquired by Google in 2014. The company is based\"\n-        )\n-\n-        gpt2_tokenizer = GPT2Tokenizer.from_pretrained(\"openai-community/gpt2-large\")\n-        gpt2_model = TFGPT2LMHeadModel.from_pretrained(\"openai-community/gpt2-large\")\n-        input_ids = gpt2_tokenizer(article, return_tensors=\"tf\")\n-\n-        xla_generate = tf.function(gpt2_model.generate, jit_compile=True)\n-        outputs = xla_generate(**input_ids, penalty_alpha=0.6, top_k=4, max_length=256)\n-\n-        generated_text = gpt2_tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-\n-        self.assertListEqual(\n-            generated_text,\n-            [\n-                \"DeepMind Technologies is a British artificial intelligence subsidiary of Alphabet Inc. and research \"\n-                \"laboratory founded in 2010. DeepMind was acquired by Google in 2014. The company is based in London, \"\n-                \"United Kingdom\\n\\nGoogle has a lot of data on its users and uses it to improve its products, such as \"\n-                \"Google Now, which helps users find the information they're looking for on the web. But the company \"\n-                \"is not the only one to collect data on its users. Facebook, for example, has its own facial \"\n-                \"recognition technology, as well as a database of millions of photos that it uses to personalize its \"\n-                \"News Feed.\\n\\nFacebook's use of data is a hot topic in the tech industry, with privacy advocates \"\n-                \"concerned about the company's ability to keep users' information private. In a blog post last \"\n-                'year, Facebook CEO Mark Zuckerberg said his company would \"do our best to be transparent about our '\n-                'data use and how we use it.\"\\n\\n\"We have made it clear that we do not sell or share your data with '\n-                'third parties,\" Zuckerberg wrote. \"If you have questions or concerns, please reach out to us at '\n-                'privacy@facebook.com.\"\\n\\nGoogle declined to comment on the privacy implications of its use of data, '\n-                \"but said in a statement to The Associated Press that\"\n-            ],\n-        )"
        },
        {
            "sha": "abaadc2247e8d32d9da942b8ce567b1ad494b37e",
            "filename": "tests/models/gpt_neo/test_modeling_flax_gpt_neo.py",
            "status": "removed",
            "additions": 0,
            "deletions": 223,
            "changes": 223,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_flax_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_flax_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_neo%2Ftest_modeling_flax_gpt_neo.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,223 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import GPT2Tokenizer, GPTNeoConfig, is_flax_available\n-from transformers.testing_utils import require_flax, slow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor, random_attention_mask\n-\n-\n-if is_flax_available():\n-    import jax\n-    import jax.numpy as jnp\n-\n-    from transformers.models.gpt_neo.modeling_flax_gpt_neo import FlaxGPTNeoForCausalLM, FlaxGPTNeoModel\n-\n-\n-class FlaxGPTNeoModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=14,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        attention_types=[[[\"global\", \"local\"], 1]],\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        window_size=7,\n-        initializer_range=0.02,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.attention_types = attention_types\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.window_size = window_size\n-        self.initializer_range = initializer_range\n-        self.scope = None\n-        self.bos_token_id = vocab_size - 1\n-        self.eos_token_id = vocab_size - 1\n-        self.pad_token_id = vocab_size - 1\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        config = GPTNeoConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_layers=self.num_hidden_layers,\n-            num_heads=self.num_attention_heads,\n-            max_position_embeddings=self.max_position_embeddings,\n-            use_cache=False,\n-            bos_token_id=self.bos_token_id,\n-            eos_token_id=self.eos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            window_size=self.window_size,\n-            attention_types=self.attention_types,\n-        )\n-\n-        return (config, input_ids, input_mask)\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-        return config, inputs_dict\n-\n-    def check_use_cache_forward(self, model_class_name, config, input_ids, attention_mask):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        past_key_values = model.init_cache(input_ids.shape[0], max_decoder_length)\n-        attention_mask = jnp.ones((input_ids.shape[0], max_decoder_length), dtype=\"i4\")\n-\n-        position_ids = jnp.broadcast_to(\n-            jnp.arange(input_ids.shape[-1] - 1)[None, :], (input_ids.shape[0], input_ids.shape[-1] - 1)\n-        )\n-        outputs_cache = model(\n-            input_ids[:, :-1],\n-            attention_mask=attention_mask,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n-\n-        position_ids = jnp.array(input_ids.shape[0] * [[input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model(\n-            input_ids[:, -1:],\n-            attention_mask=attention_mask,\n-            past_key_values=outputs_cache.past_key_values,\n-            position_ids=position_ids,\n-        )\n-\n-        outputs = model(input_ids)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-    def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input_ids, attention_mask):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        attention_mask_cache = jnp.concatenate(\n-            [attention_mask, jnp.zeros((attention_mask.shape[0], max_decoder_length - attention_mask.shape[1]))],\n-            axis=-1,\n-        )\n-\n-        past_key_values = model.init_cache(input_ids.shape[0], max_decoder_length)\n-        position_ids = jnp.broadcast_to(\n-            jnp.arange(input_ids.shape[-1] - 1)[None, :], (input_ids.shape[0], input_ids.shape[-1] - 1)\n-        )\n-\n-        outputs_cache = model(\n-            input_ids[:, :-1],\n-            attention_mask=attention_mask_cache,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n-        position_ids = jnp.array(input_ids.shape[0] * [[input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model(\n-            input_ids[:, -1:],\n-            past_key_values=outputs_cache.past_key_values,\n-            attention_mask=attention_mask_cache,\n-            position_ids=position_ids,\n-        )\n-\n-        outputs = model(input_ids, attention_mask=attention_mask)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-\n-@require_flax\n-class FlaxGPTNeoModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (FlaxGPTNeoModel, FlaxGPTNeoForCausalLM) if is_flax_available() else ()\n-\n-    def setUp(self):\n-        self.model_tester = FlaxGPTNeoModelTester(self)\n-\n-    def test_use_cache_forward(self):\n-        for model_class_name in self.all_model_classes:\n-            config, input_ids, attention_mask = self.model_tester.prepare_config_and_inputs()\n-            self.model_tester.check_use_cache_forward(model_class_name, config, input_ids, attention_mask)\n-\n-    def test_use_cache_forward_with_attn_mask(self):\n-        for model_class_name in self.all_model_classes:\n-            config, input_ids, attention_mask = self.model_tester.prepare_config_and_inputs()\n-            self.model_tester.check_use_cache_forward_with_attn_mask(\n-                model_class_name, config, input_ids, attention_mask\n-            )\n-\n-    @slow\n-    def test_batch_generation(self):\n-        tokenizer = GPT2Tokenizer.from_pretrained(\n-            \"openai-community/gpt2\", pad_token=\"<|endoftext|>\", padding_side=\"left\"\n-        )\n-        inputs = tokenizer([\"Hello this is a long string\", \"Hey\"], return_tensors=\"np\", padding=True, truncation=True)\n-\n-        model = FlaxGPTNeoForCausalLM.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n-        model.do_sample = False\n-        model.config.pad_token_id = model.config.eos_token_id\n-\n-        jit_generate = jax.jit(model.generate)\n-\n-        output_sequences = jit_generate(\n-            inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], pad_token_id=tokenizer.pad_token_id\n-        ).sequences\n-\n-        output_string = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n-\n-        expected_string = [\n-            \"Hello this is a long string of text.\\n\\nI'm trying to get the text of the\",\n-            \"Hey, I'm a little late to the party. I'm going to\",\n-        ]\n-\n-        self.assertListEqual(output_string, expected_string)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"EleutherAI/gpt-neo-125M\")\n-            outputs = model(np.ones((1, 1)))\n-            self.assertIsNotNone(outputs)"
        },
        {
            "sha": "f92c07ab6e9574b97c0bb4ef2ac64eb4ef6d58cc",
            "filename": "tests/models/gptj/test_modeling_flax_gptj.py",
            "status": "removed",
            "additions": 0,
            "deletions": 220,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgptj%2Ftest_modeling_flax_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgptj%2Ftest_modeling_flax_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_flax_gptj.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,220 +0,0 @@\n-# Copyright 2021 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import GPT2Tokenizer, GPTJConfig, is_flax_available\n-from transformers.testing_utils import require_flax, tooslow\n-\n-from ...test_modeling_flax_common import FlaxModelTesterMixin, ids_tensor, random_attention_mask\n-\n-\n-if is_flax_available():\n-    import jax\n-    import jax.numpy as jnp\n-\n-    from transformers.models.gptj.modeling_flax_gptj import FlaxGPTJForCausalLM, FlaxGPTJModel\n-\n-\n-class FlaxGPTJModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=14,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=False,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        rotary_dim=4,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        initializer_range=0.02,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.rotary_dim = rotary_dim\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.initializer_range = initializer_range\n-        self.scope = None\n-        self.bos_token_id = vocab_size - 1\n-        self.eos_token_id = vocab_size - 1\n-        self.pad_token_id = vocab_size - 1\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        config = GPTJConfig(\n-            vocab_size=self.vocab_size,\n-            n_embd=self.hidden_size,\n-            n_layer=self.num_hidden_layers,\n-            n_head=self.num_attention_heads,\n-            n_positions=self.max_position_embeddings,\n-            use_cache=False,\n-            bos_token_id=self.bos_token_id,\n-            eos_token_id=self.eos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            rotary_dim=self.rotary_dim,\n-        )\n-\n-        return (config, input_ids, input_mask)\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n-        return config, inputs_dict\n-\n-    def check_use_cache_forward(self, model_class_name, config, input_ids, attention_mask):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        past_key_values = model.init_cache(input_ids.shape[0], max_decoder_length)\n-        attention_mask = jnp.ones((input_ids.shape[0], max_decoder_length), dtype=\"i4\")\n-\n-        position_ids = jnp.broadcast_to(\n-            jnp.arange(input_ids.shape[-1] - 1)[None, :], (input_ids.shape[0], input_ids.shape[-1] - 1)\n-        )\n-        outputs_cache = model(\n-            input_ids[:, :-1],\n-            attention_mask=attention_mask,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n-\n-        position_ids = jnp.array(input_ids.shape[0] * [[input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model(\n-            input_ids[:, -1:],\n-            attention_mask=attention_mask,\n-            past_key_values=outputs_cache.past_key_values,\n-            position_ids=position_ids,\n-        )\n-\n-        outputs = model(input_ids)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-    def check_use_cache_forward_with_attn_mask(self, model_class_name, config, input_ids, attention_mask):\n-        max_decoder_length = 20\n-        model = model_class_name(config)\n-\n-        attention_mask_cache = jnp.concatenate(\n-            [attention_mask, jnp.zeros((attention_mask.shape[0], max_decoder_length - attention_mask.shape[1]))],\n-            axis=-1,\n-        )\n-\n-        past_key_values = model.init_cache(input_ids.shape[0], max_decoder_length)\n-        position_ids = jnp.broadcast_to(\n-            jnp.arange(input_ids.shape[-1] - 1)[None, :], (input_ids.shape[0], input_ids.shape[-1] - 1)\n-        )\n-\n-        outputs_cache = model(\n-            input_ids[:, :-1],\n-            attention_mask=attention_mask_cache,\n-            past_key_values=past_key_values,\n-            position_ids=position_ids,\n-        )\n-        position_ids = jnp.array(input_ids.shape[0] * [[input_ids.shape[-1] - 1]], dtype=\"i4\")\n-        outputs_cache_next = model(\n-            input_ids[:, -1:],\n-            past_key_values=outputs_cache.past_key_values,\n-            attention_mask=attention_mask_cache,\n-            position_ids=position_ids,\n-        )\n-\n-        outputs = model(input_ids, attention_mask=attention_mask)\n-\n-        diff = np.max(np.abs(outputs_cache_next[0][:, -1, :5] - outputs[0][:, -1, :5]))\n-        self.parent.assertTrue(diff < 1e-3, msg=f\"Max diff is {diff}\")\n-\n-\n-@require_flax\n-class FlaxGPTJModelTest(FlaxModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (FlaxGPTJModel, FlaxGPTJForCausalLM) if is_flax_available() else ()\n-\n-    def setUp(self):\n-        self.model_tester = FlaxGPTJModelTester(self)\n-\n-    def test_use_cache_forward(self):\n-        for model_class_name in self.all_model_classes:\n-            config, input_ids, attention_mask = self.model_tester.prepare_config_and_inputs()\n-            self.model_tester.check_use_cache_forward(model_class_name, config, input_ids, attention_mask)\n-\n-    def test_use_cache_forward_with_attn_mask(self):\n-        for model_class_name in self.all_model_classes:\n-            config, input_ids, attention_mask = self.model_tester.prepare_config_and_inputs()\n-            self.model_tester.check_use_cache_forward_with_attn_mask(\n-                model_class_name, config, input_ids, attention_mask\n-            )\n-\n-    @tooslow\n-    def test_batch_generation(self):\n-        tokenizer = GPT2Tokenizer.from_pretrained(\n-            \"openai-community/gpt2\", pad_token=\"<|endoftext|>\", padding_side=\"left\"\n-        )\n-        inputs = tokenizer([\"Hello this is a long string\", \"Hey\"], return_tensors=\"np\", padding=True, truncation=True)\n-\n-        model = FlaxGPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\")\n-        model.do_sample = False\n-        model.config.pad_token_id = model.config.eos_token_id\n-\n-        jit_generate = jax.jit(model.generate)\n-\n-        output_sequences = jit_generate(\n-            inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], pad_token_id=tokenizer.pad_token_id\n-        ).sequences\n-\n-        output_string = tokenizer.batch_decode(output_sequences, skip_special_tokens=True)\n-\n-        expected_string = [\n-            \"Hello this is a long string of text.\\n\\nI'm trying to get the text of the\",\n-            \"Hey, I'm a little late to the party. I'm going to\",\n-        ]\n-\n-        self.assertListEqual(output_string, expected_string)\n-\n-    @tooslow\n-    def test_model_from_pretrained(self):\n-        for model_class_name in self.all_model_classes:\n-            model = model_class_name.from_pretrained(\"EleutherAI/gpt-j-6B\")\n-            outputs = model(np.ones((1, 1)))\n-            self.assertIsNotNone(outputs)"
        },
        {
            "sha": "2103dd9c267933921e8b0046908e60bdebf3a890",
            "filename": "tests/models/gptj/test_modeling_tf_gptj.py",
            "status": "removed",
            "additions": 0,
            "deletions": 468,
            "changes": 468,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgptj%2Ftest_modeling_tf_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgptj%2Ftest_modeling_tf_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgptj%2Ftest_modeling_tf_gptj.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,468 +0,0 @@\n-# Copyright 2020 The HuggingFace Team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-from transformers import AutoTokenizer, GPTJConfig, is_tf_available\n-from transformers.testing_utils import require_tf, slow, tooslow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-from ...utils.test_modeling_tf_core import TFCoreModelTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers.models.gptj.modeling_tf_gptj import (\n-        TFGPTJForCausalLM,\n-        TFGPTJForQuestionAnswering,\n-        TFGPTJForSequenceClassification,\n-        TFGPTJModel,\n-        shape_list,\n-    )\n-\n-\n-class TFGPTJModelTester:\n-    def __init__(self, parent):\n-        self.parent = parent\n-        self.batch_size = 13\n-        self.seq_length = 7\n-        self.is_training = True\n-        self.use_token_type_ids = True\n-        self.use_input_mask = True\n-        self.use_labels = True\n-        self.use_mc_token_ids = True\n-        self.vocab_size = 99\n-        self.hidden_size = 32\n-        self.rotary_dim = 4\n-        self.num_hidden_layers = 2\n-        self.num_attention_heads = 4\n-        self.intermediate_size = 37\n-        self.hidden_act = \"gelu\"\n-        self.hidden_dropout_prob = 0.1\n-        self.attention_probs_dropout_prob = 0.1\n-        self.max_position_embeddings = 512\n-        self.type_vocab_size = 16\n-        self.type_sequence_label_size = 2\n-        self.initializer_range = 0.02\n-        self.num_labels = 3\n-        self.num_choices = 4\n-        self.scope = None\n-        self.bos_token_id = self.vocab_size - 1\n-        self.eos_token_id = self.vocab_size - 1\n-        self.pad_token_id = self.vocab_size - 1\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        mc_token_ids = None\n-        if self.use_mc_token_ids:\n-            mc_token_ids = ids_tensor([self.batch_size, self.num_choices], self.seq_length)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = GPTJConfig(\n-            vocab_size=self.vocab_size,\n-            n_embd=self.hidden_size,\n-            n_layer=self.num_hidden_layers,\n-            n_head=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            n_positions=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            initializer_range=self.initializer_range,\n-            bos_token_id=self.bos_token_id,\n-            eos_token_id=self.eos_token_id,\n-            pad_token_id=self.pad_token_id,\n-            rotary_dim=self.rotary_dim,\n-            return_dict=True,\n-        )\n-\n-        head_mask = ids_tensor([self.num_hidden_layers, self.num_attention_heads], 2)\n-\n-        return (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        )\n-\n-    def create_and_check_gptj_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n-        model = TFGPTJModel(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-\n-        inputs = [input_ids, None, input_mask]  # None is the input for 'past'\n-        result = model(inputs)\n-\n-        result = model(input_ids)\n-\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-\n-    def create_and_check_gptj_model_past(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n-        model = TFGPTJModel(config=config)\n-\n-        # first forward pass\n-        outputs = model(input_ids, token_type_ids=token_type_ids, use_cache=True)\n-        outputs_use_cache_conf = model(input_ids, token_type_ids=token_type_ids)\n-        outputs_no_past = model(input_ids, token_type_ids=token_type_ids, use_cache=False)\n-\n-        self.parent.assertTrue(len(outputs) == len(outputs_use_cache_conf))\n-        self.parent.assertTrue(len(outputs) == len(outputs_no_past) + 1)\n-\n-        output, past_key_values = outputs.to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n-        next_token_types = ids_tensor([self.batch_size, 1], self.type_vocab_size)\n-\n-        # append to next input_ids and token_type_ids\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1)\n-\n-        output_from_no_past = model(next_input_ids, token_type_ids=next_token_type_ids)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, token_type_ids=next_token_types, past_key_values=past_key_values)[\n-            \"last_hidden_state\"\n-        ]\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), shape_list(output_from_past)[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, 0, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-6)\n-\n-    def create_and_check_gptj_model_attention_mask_past(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n-    ):\n-        model = TFGPTJModel(config=config)\n-\n-        # create attention mask\n-        half_seq_length = self.seq_length // 2\n-        attn_mask_begin = tf.ones((self.batch_size, half_seq_length), dtype=tf.int32)\n-        attn_mask_end = tf.zeros((self.batch_size, self.seq_length - half_seq_length), dtype=tf.int32)\n-        attn_mask = tf.concat([attn_mask_begin, attn_mask_end], axis=1)\n-\n-        # first forward pass\n-        output, past_key_values = model(input_ids, attention_mask=attn_mask).to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 1), config.vocab_size)\n-\n-        # change a random masked slice from input_ids\n-        random_seq_idx_to_change = ids_tensor((1,), half_seq_length).numpy() + 1\n-        random_other_next_tokens = ids_tensor((self.batch_size, self.seq_length), config.vocab_size)\n-        vector_condition = tf.range(self.seq_length) == (self.seq_length - random_seq_idx_to_change)\n-        condition = tf.transpose(\n-            tf.broadcast_to(tf.expand_dims(vector_condition, -1), (self.seq_length, self.batch_size))\n-        )\n-        input_ids = tf.where(condition, random_other_next_tokens, input_ids)\n-\n-        # append to next input_ids and attn_mask\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        attn_mask = tf.concat([attn_mask, tf.ones((shape_list(attn_mask)[0], 1), dtype=tf.int32)], axis=1)\n-\n-        # get two different outputs\n-        output_from_no_past = model(next_input_ids, attention_mask=attn_mask)[\"last_hidden_state\"]\n-        output_from_past = model(next_tokens, past_key_values=past_key_values, attention_mask=attn_mask)[\n-            \"last_hidden_state\"\n-        ]\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), shape_list(output_from_past)[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -1, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, 0, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-12)\n-\n-    def create_and_check_gptj_model_past_large_inputs(\n-        self, config, input_ids, input_mask, head_mask, token_type_ids, *args\n-    ):\n-        model = TFGPTJModel(config=config)\n-\n-        input_ids = input_ids[:1, :]\n-        input_mask = input_mask[:1, :]\n-        token_type_ids = token_type_ids[:1, :]\n-        self.batch_size = 1\n-\n-        # first forward pass\n-        outputs = model(input_ids, attention_mask=input_mask, token_type_ids=token_type_ids, use_cache=True)\n-\n-        output, past_key_values = outputs.to_tuple()\n-\n-        # create hypothetical next token and extent to next_input_ids\n-        next_tokens = ids_tensor((self.batch_size, 3), config.vocab_size)\n-        next_attn_mask = ids_tensor((self.batch_size, 3), 2)\n-        next_token_types = ids_tensor((self.batch_size, 3), self.type_vocab_size)\n-\n-        # append to next input_ids and token_type_ids\n-        next_input_ids = tf.concat([input_ids, next_tokens], axis=-1)\n-        next_attention_mask = tf.concat([input_mask, next_attn_mask], axis=-1)\n-        next_token_type_ids = tf.concat([token_type_ids, next_token_types], axis=-1)\n-\n-        output_from_no_past = model(\n-            next_input_ids, token_type_ids=next_token_type_ids, attention_mask=next_attention_mask\n-        )[\"last_hidden_state\"]\n-        output_from_past = model(\n-            next_tokens,\n-            token_type_ids=next_token_types,\n-            attention_mask=next_attention_mask,\n-            past_key_values=past_key_values,\n-        )[\"last_hidden_state\"]\n-        self.parent.assertTrue(output_from_past.shape[1] == next_tokens.shape[1])\n-\n-        # select random slice\n-        random_slice_idx = int(ids_tensor((1,), shape_list(output_from_past)[-1]))\n-        output_from_no_past_slice = output_from_no_past[:, -3:, random_slice_idx]\n-        output_from_past_slice = output_from_past[:, :, random_slice_idx]\n-\n-        # test that outputs are equal for slice\n-        tf.debugging.assert_near(output_from_past_slice, output_from_no_past_slice, rtol=1e-3)\n-\n-    def create_and_check_gptj_lm_head_model(self, config, input_ids, input_mask, head_mask, token_type_ids, *args):\n-        model = TFGPTJForCausalLM(config=config)\n-        inputs = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"token_type_ids\": token_type_ids,\n-        }\n-        result = model(inputs)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            head_mask,\n-            token_type_ids,\n-            mc_token_ids,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"token_type_ids\": token_type_ids,\n-            \"attention_mask\": input_mask,\n-        }\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFGPTJModelTest(TFModelTesterMixin, TFCoreModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (TFGPTJForCausalLM, TFGPTJForSequenceClassification, TFGPTJForQuestionAnswering, TFGPTJModel)\n-        if is_tf_available()\n-        else ()\n-    )\n-\n-    all_generative_model_classes = (TFGPTJForCausalLM,) if is_tf_available() else ()\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFGPTJModel,\n-            \"question-answering\": TFGPTJForQuestionAnswering,\n-            \"text-classification\": TFGPTJForSequenceClassification,\n-            \"text-generation\": TFGPTJForCausalLM,\n-            \"zero-shot\": TFGPTJForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_onnx = False\n-    test_pruning = False\n-    test_missing_keys = False\n-    test_head_masking = False\n-\n-    # TODO: Fix the failed tests\n-    def is_pipeline_test_to_skip(\n-        self,\n-        pipeline_test_case_name,\n-        config_class,\n-        model_architecture,\n-        tokenizer_name,\n-        image_processor_name,\n-        feature_extractor_name,\n-        processor_name,\n-    ):\n-        if (\n-            pipeline_test_case_name == \"QAPipelineTests\"\n-            and tokenizer_name is not None\n-            and not tokenizer_name.endswith(\"Fast\")\n-        ):\n-            # `QAPipelineTests` fails for a few models when the slower tokenizer are used.\n-            # (The slower tokenizers were never used for pipeline tests before the pipeline testing rework)\n-            # TODO: check (and possibly fix) the `QAPipelineTests` with slower tokenizer\n-            return True\n-\n-        return False\n-\n-    def setUp(self):\n-        self.model_tester = TFGPTJModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=GPTJConfig, n_embd=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_gptj_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gptj_model(*config_and_inputs)\n-\n-    def test_gptj_model_past(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gptj_model_past(*config_and_inputs)\n-\n-    def test_gptj_model_att_mask_past(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gptj_model_attention_mask_past(*config_and_inputs)\n-\n-    def test_gptj_model_past_large_inputs(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gptj_model_past_large_inputs(*config_and_inputs)\n-\n-    def test_gptj_lm_head_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_gptj_lm_head_model(*config_and_inputs)\n-\n-    @slow\n-    @unittest.skipIf(\n-        not is_tf_available() or len(tf.config.list_physical_devices(\"GPU\")) > 0,\n-        \"skip testing on GPU for now to avoid GPU OOM.\",\n-    )\n-    def test_model_from_pretrained(self):\n-        model = TFGPTJModel.from_pretrained(\"EleutherAI/gpt-j-6B\", from_pt=True)\n-        self.assertIsNotNone(model)\n-\n-    @unittest.skip(reason=\"Currently, model embeddings are going to undergo a major refactor.\")\n-    def test_resize_token_embeddings(self):\n-        super().test_resize_token_embeddings()\n-\n-\n-@require_tf\n-@tooslow\n-# Marked as @tooslow due to GPU OOM -- but still useful to run locally. Requires ~39GB of RAM.\n-class TFGPTJModelLanguageGenerationTest(unittest.TestCase):\n-    def test_lm_generate_gptj(self):\n-        model = TFGPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", from_pt=True)\n-        input_ids = tf.convert_to_tensor([[464, 3290]], dtype=tf.int32)  # The dog\n-        # The dog is a man's best friend. It is a loyal companion, and it is a friend\n-        expected_output_ids = [464, 3290, 318, 257, 582, 338, 1266, 1545, 13, 632, 318, 257, 9112, 15185, 11, 290, 340, 318, 257, 1545]  # fmt: skip\n-        output_ids = model.generate(input_ids, do_sample=False)\n-        self.assertListEqual(output_ids[0].numpy().tolist(), expected_output_ids)\n-\n-    def test_gptj_sample(self):\n-        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\")\n-        model = TFGPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", from_pt=True)\n-\n-        tokenized = tokenizer(\"Today is a nice day and\", return_tensors=\"tf\")\n-        # forces the generation to happen on CPU, to avoid GPU-related quirks\n-        with tf.device(\":/CPU:0\"):\n-            output_ids = model.generate(**tokenized, do_sample=True, seed=[42, 0])\n-        output_str = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n-\n-        EXPECTED_OUTPUT_STR = \"Today is a nice day and Im going to go for a walk. I\"\n-        self.assertEqual(output_str, EXPECTED_OUTPUT_STR)\n-\n-    def _get_beam_search_test_objects(self):\n-        model = TFGPTJForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\", from_pt=True)\n-        tokenizer = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\", revision=\"float16\")\n-\n-        tokenizer.padding_side = \"left\"\n-\n-        # Define PAD Token = EOS Token = 50256\n-        tokenizer.pad_token = tokenizer.eos_token\n-        model.config.pad_token_id = model.config.eos_token_id\n-\n-        # use different length sentences to test batching\n-        sentences = [\n-            \"Hello, my dog is a little\",\n-            \"Today, I\",\n-        ]\n-        expected_output_sentences = [\n-            \"Hello, my dog is a little over a year old and has been diagnosed with hip dysplasia\",\n-            \"Today, Im going to be talking about a topic that\",\n-        ]\n-        return model, tokenizer, sentences, expected_output_sentences\n-\n-    def test_batch_beam_search(self):\n-        # Confirms that we get the expected results with left-padded beam search\n-        model, tokenizer, sentences, expected_output_sentences = self._get_beam_search_test_objects()\n-\n-        inputs = tokenizer(sentences, return_tensors=\"tf\", padding=True)\n-        outputs = model.generate(**inputs, do_sample=False, num_beams=2)\n-        batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n-        self.assertListEqual(expected_output_sentences, batch_out_sentence)\n-\n-    def test_batch_left_padding(self):\n-        # Confirms that left-padding is working properly\n-        model, tokenizer, sentences, expected_output_sentences = self._get_beam_search_test_objects()\n-\n-        inputs = tokenizer(sentences, return_tensors=\"tf\", padding=True)\n-        inputs_non_padded = tokenizer(sentences[0], return_tensors=\"tf\")\n-        output_non_padded = model.generate(**inputs_non_padded, do_sample=False, num_beams=2)\n-        num_paddings = (\n-            shape_list(inputs_non_padded[\"input_ids\"])[-1]\n-            - tf.reduce_sum(tf.cast(inputs[\"attention_mask\"][-1], tf.int64)).numpy()\n-        )\n-        inputs_padded = tokenizer(sentences[1], return_tensors=\"tf\")\n-        output_padded = model.generate(\n-            **inputs_padded, do_sample=False, num_beams=2, max_length=model.config.max_length - num_paddings\n-        )\n-        non_padded_sentence = tokenizer.decode(output_non_padded[0], skip_special_tokens=True)\n-        padded_sentence = tokenizer.decode(output_padded[0], skip_special_tokens=True)\n-        self.assertListEqual(expected_output_sentences, [non_padded_sentence, padded_sentence])\n-\n-    def test_xla_beam_search(self):\n-        # Confirms that XLA is working properly\n-        model, tokenizer, sentences, expected_output_sentences = self._get_beam_search_test_objects()\n-\n-        inputs = tokenizer(sentences, return_tensors=\"tf\", padding=True)\n-        xla_generate = tf.function(model.generate, jit_compile=True)\n-        outputs_xla = xla_generate(**inputs, do_sample=False, num_beams=2)\n-        xla_sentence = tokenizer.batch_decode(outputs_xla, skip_special_tokens=True)\n-        self.assertListEqual(expected_output_sentences, xla_sentence)"
        },
        {
            "sha": "24ffc88a82204937053672670e37678dd067bc5f",
            "filename": "tests/models/groupvit/test_modeling_tf_groupvit.py",
            "status": "removed",
            "additions": 0,
            "deletions": 695,
            "changes": 695,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_tf_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_tf_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_tf_groupvit.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,695 +0,0 @@\n-# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Testing suite for the TensorFlow GroupViT model.\"\"\"\n-\n-from __future__ import annotations\n-\n-import inspect\n-import os\n-import random\n-import tempfile\n-import unittest\n-from importlib import import_module\n-\n-import requests\n-\n-from transformers import GroupViTConfig, GroupViTTextConfig, GroupViTVisionConfig\n-from transformers.testing_utils import (\n-    require_tensorflow_probability,\n-    require_tf,\n-    require_vision,\n-    slow,\n-)\n-from transformers.utils import is_tf_available, is_vision_available\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import TFGroupViTModel, TFGroupViTTextModel, TFGroupViTVisionModel, TFSharedEmbeddings\n-    from transformers.modeling_tf_utils import keras\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-    from transformers import CLIPProcessor\n-\n-\n-class TFGroupViTVisionModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=12,\n-        image_size=30,\n-        patch_size=2,\n-        num_channels=3,\n-        is_training=True,\n-        hidden_size=32,\n-        depths=[6, 3, 3],\n-        num_group_tokens=[64, 8, 0],\n-        num_output_groups=[64, 8, 8],\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        dropout=0.1,\n-        attention_dropout=0.1,\n-        initializer_range=0.02,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.image_size = image_size\n-        self.patch_size = patch_size\n-        self.num_channels = num_channels\n-        self.is_training = is_training\n-        self.hidden_size = hidden_size\n-        self.depths = depths\n-        self.num_hidden_layers = sum(depths)\n-        self.expected_num_hidden_layers = len(depths) + 1\n-        self.num_group_tokens = num_group_tokens\n-        self.num_output_groups = num_output_groups\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.dropout = dropout\n-        self.attention_dropout = attention_dropout\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-\n-        num_patches = (image_size // patch_size) ** 2\n-        # no [CLS] token for GroupViT\n-        self.seq_length = num_patches\n-\n-    def prepare_config_and_inputs(self):\n-        rng = random.Random(0)\n-        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size], rng=rng)\n-        config = self.get_config()\n-\n-        return config, pixel_values\n-\n-    def get_config(self):\n-        return GroupViTVisionConfig(\n-            image_size=self.image_size,\n-            patch_size=self.patch_size,\n-            num_channels=self.num_channels,\n-            hidden_size=self.hidden_size,\n-            depths=self.depths,\n-            num_group_tokens=self.num_group_tokens,\n-            num_output_groups=self.num_output_groups,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            dropout=self.dropout,\n-            attention_dropout=self.attention_dropout,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-    def create_and_check_model(self, config, pixel_values):\n-        model = TFGroupViTVisionModel(config=config)\n-        result = model(pixel_values, training=False)\n-        self.parent.assertEqual(\n-            result.last_hidden_state.shape, (self.batch_size, self.num_output_groups[-1], self.hidden_size)\n-        )\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, pixel_values = config_and_inputs\n-        inputs_dict = {\"pixel_values\": pixel_values}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFGroupViTVisionModelTest(TFModelTesterMixin, unittest.TestCase):\n-    \"\"\"\n-    Here we also overwrite some of the tests of test_modeling_common.py, as GroupViT does not use input_ids, inputs_embeds,\n-    attention_mask and seq_length.\n-    \"\"\"\n-\n-    all_model_classes = (TFGroupViTVisionModel,) if is_tf_available() else ()\n-\n-    test_pruning = False\n-    test_resize_embeddings = False\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFGroupViTVisionModelTester(self)\n-        self.config_tester = ConfigTester(\n-            self, config_class=GroupViTVisionConfig, has_text_modality=False, hidden_size=37\n-        )\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    @unittest.skip(reason=\"GroupViT does not use inputs_embeds\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    \"\"\"\n-    During saving, TensorFlow will also run with `training=True` which trigger `gumbel_softmax` that requires\n-    `tensorflow-probability`.\n-    \"\"\"\n-\n-    @require_tensorflow_probability\n-    @slow\n-    def test_saved_model_creation(self):\n-        super().test_saved_model_creation()\n-\n-    @unittest.skip(reason=\"GroupViT does not use inputs_embeds\")\n-    def test_graph_mode_with_inputs_embeds(self):\n-        pass\n-\n-    def test_model_common_attributes(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            self.assertIsInstance(model.get_input_embeddings(), (keras.layers.Layer))\n-            x = model.get_output_embeddings()\n-            self.assertTrue(x is None or isinstance(x, keras.layers.Layer))\n-\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.call)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"pixel_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_attention_outputs(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.return_dict = True\n-\n-        seq_len = getattr(self.model_tester, \"seq_length\", None)\n-\n-        expected_num_attention_outputs = sum(g > 0 for g in self.model_tester.num_group_tokens)\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_attentions\"] = True\n-            inputs_dict[\"output_hidden_states\"] = False\n-            config.return_dict = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class), training=False)\n-            attentions = outputs.attentions\n-            # GroupViT returns attention grouping of each stage\n-            self.assertEqual(len(attentions), sum(g > 0 for g in self.model_tester.num_group_tokens))\n-\n-            # check that output_attentions also work using config\n-            del inputs_dict[\"output_attentions\"]\n-            config.output_attentions = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class), training=False)\n-            attentions = outputs.attentions\n-            # GroupViT returns attention grouping of each stage\n-            self.assertEqual(len(attentions), expected_num_attention_outputs)\n-\n-            out_len = len(outputs)\n-\n-            # Check attention is always last and order is fine\n-            inputs_dict[\"output_attentions\"] = True\n-            inputs_dict[\"output_hidden_states\"] = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class), training=False)\n-\n-            added_hidden_states = 1\n-            self.assertEqual(out_len + added_hidden_states, len(outputs))\n-\n-            self_attentions = outputs.attentions\n-\n-            # GroupViT returns attention grouping of each stage\n-            self.assertEqual(len(self_attentions), expected_num_attention_outputs)\n-            for i, self_attn in enumerate(self_attentions):\n-                if self_attn is None:\n-                    continue\n-\n-                self.assertListEqual(\n-                    list(self_attentions[i].shape[-2:]),\n-                    [\n-                        self.model_tester.num_output_groups[i],\n-                        self.model_tester.num_output_groups[i - 1] if i > 0 else seq_len,\n-                    ],\n-                )\n-\n-    def test_hidden_states_output(self):\n-        def check_hidden_states_output(inputs_dict, config, model_class):\n-            model = model_class(config)\n-\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class), training=False)\n-\n-            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n-\n-            expected_num_layers = getattr(\n-                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n-            )\n-            self.assertEqual(len(hidden_states), expected_num_layers)\n-\n-            seq_length = getattr(self.model_tester, \"seq_length\", None)\n-\n-            self.assertListEqual(\n-                list(hidden_states[0].shape[-2:]),\n-                [seq_length, self.model_tester.hidden_size],\n-            )\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_hidden_states\"] = True\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-            # check that output_hidden_states also work using config\n-            del inputs_dict[\"output_hidden_states\"]\n-            config.output_hidden_states = True\n-\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"nvidia/groupvit-gcc-yfcc\"\n-        model = TFGroupViTVisionModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    @unittest.skip(\n-        \"TFGroupViTVisionModel does not convert `hidden_states` and `attentions` to tensors as they are all of\"\n-        \" different dimensions, and we get `Got a non-Tensor value` error when saving the model.\"\n-    )\n-    @slow\n-    def test_saved_model_creation_extended(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.output_hidden_states = True\n-        config.output_attentions = True\n-\n-        if hasattr(config, \"use_cache\"):\n-            config.use_cache = True\n-\n-        seq_len = getattr(self.model_tester, \"seq_length\", None)\n-\n-        for model_class in self.all_model_classes:\n-            class_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-            model = model_class(config)\n-            num_out = len(model(class_inputs_dict))\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname, saved_model=True)\n-                saved_model_dir = os.path.join(tmpdirname, \"saved_model\", \"1\")\n-                model = keras.models.load_model(saved_model_dir)\n-                outputs = model(class_inputs_dict)\n-                output_hidden_states = outputs[\"hidden_states\"]\n-                output_attentions = outputs[\"attentions\"]\n-\n-                # Check num outputs\n-                self.assertEqual(len(outputs), num_out)\n-\n-                # Check num layers\n-                expected_num_layers = getattr(\n-                    self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n-                )\n-\n-                self.assertEqual(len(output_hidden_states), expected_num_layers)\n-                self.assertEqual(len(output_attentions), self.model_tester.num_hidden_layers)\n-\n-                # Check attention outputs\n-                image_size = (self.model_tester.image_size, self.model_tester.image_size)\n-                patch_size = (self.model_tester.patch_size, self.model_tester.patch_size)\n-                num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n-                seq_len = num_patches + 1\n-\n-                self.assertListEqual(\n-                    list(output_attentions[0].shape[-3:]),\n-                    [self.model_tester.num_attention_heads, seq_len, seq_len],\n-                )\n-\n-                # Check hidden states\n-                self.assertListEqual(\n-                    list(output_hidden_states[0].shape[-2:]),\n-                    [seq_len, self.model_tester.hidden_size],\n-                )\n-\n-\n-class TFGroupViTTextModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=12,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        dropout=0.1,\n-        attention_dropout=0.1,\n-        max_position_embeddings=512,\n-        initializer_range=0.02,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.dropout = dropout\n-        self.attention_dropout = attention_dropout\n-        self.max_position_embeddings = max_position_embeddings\n-        self.initializer_range = initializer_range\n-        self.scope = scope\n-\n-    def prepare_config_and_inputs(self):\n-        rng = random.Random(0)\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size, rng=rng)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-            # make sure the first token has attention mask `1` to ensure that, after combining the causal mask, there\n-            # is still at least one token being attended to for each batch.\n-            # TODO: Change `random_attention_mask` in PT/TF/Flax common test file, after a discussion with the team.\n-            input_mask = tf.concat(\n-                [tf.ones_like(input_mask[:, :1], dtype=input_mask.dtype), input_mask[:, 1:]], axis=-1\n-            )\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, input_mask\n-\n-    def get_config(self):\n-        return GroupViTTextConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            dropout=self.dropout,\n-            attention_dropout=self.attention_dropout,\n-            max_position_embeddings=self.max_position_embeddings,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-    def create_and_check_model(self, config, input_ids, input_mask):\n-        model = TFGroupViTTextModel(config=config)\n-        result = model(input_ids, attention_mask=input_mask, training=False)\n-        result = model(input_ids, training=False)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, input_mask = config_and_inputs\n-        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFGroupViTTextModelTest(TFModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFGroupViTTextModel,) if is_tf_available() else ()\n-    test_pruning = False\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFGroupViTTextModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=GroupViTTextConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    @unittest.skip(reason=\"GroupViTTextModel does not use inputs_embeds\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"nvidia/groupvit-gcc-yfcc\"\n-        model = TFGroupViTTextModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    @slow\n-    def test_saved_model_creation_extended(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.output_hidden_states = True\n-        config.output_attentions = True\n-\n-        if hasattr(config, \"use_cache\"):\n-            config.use_cache = True\n-\n-        for model_class in self.all_model_classes:\n-            class_inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n-            model = model_class(config)\n-            num_out = len(model(class_inputs_dict))\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                model.save_pretrained(tmpdirname, saved_model=True)\n-                saved_model_dir = os.path.join(tmpdirname, \"saved_model\", \"1\")\n-                model = keras.models.load_model(saved_model_dir)\n-                outputs = model(class_inputs_dict)\n-                output_hidden_states = outputs[\"hidden_states\"]\n-                output_attentions = outputs[\"attentions\"]\n-\n-                # Check number of outputs\n-                self.assertEqual(len(outputs), num_out)\n-\n-                # Check number of layers\n-                expected_num_layers = getattr(\n-                    self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n-                )\n-\n-                # Check hidden states\n-                self.assertEqual(len(output_hidden_states), expected_num_layers)\n-                self.assertListEqual(\n-                    list(output_hidden_states[0].shape[-2:]),\n-                    [self.model_tester.seq_length, self.model_tester.hidden_size],\n-                )\n-\n-                # Check attention outputs\n-                self.assertEqual(len(output_attentions), self.model_tester.num_hidden_layers)\n-\n-                seq_length = self.model_tester.seq_length\n-                key_length = getattr(self.model_tester, \"key_length\", seq_length)\n-\n-                self.assertListEqual(\n-                    list(output_attentions[0].shape[-3:]),\n-                    [self.model_tester.num_attention_heads, seq_length, key_length],\n-                )\n-\n-\n-class TFGroupViTModelTester:\n-    def __init__(self, parent, is_training=True):\n-        self.parent = parent\n-        self.text_model_tester = TFGroupViTTextModelTester(parent)\n-        self.vision_model_tester = TFGroupViTVisionModelTester(parent)\n-        self.is_training = is_training\n-\n-    def prepare_config_and_inputs(self):\n-        text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n-        vision_config, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n-\n-        config = self.get_config()\n-\n-        return config, input_ids, attention_mask, pixel_values\n-\n-    def get_config(self):\n-        return GroupViTConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n-        )\n-\n-    def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n-        model = TFGroupViTModel(config)\n-        result = model(input_ids, pixel_values, attention_mask, training=False)\n-        self.parent.assertEqual(\n-            result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)\n-        )\n-        self.parent.assertEqual(\n-            result.logits_per_text.shape, (self.text_model_tester.batch_size, self.vision_model_tester.batch_size)\n-        )\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        config, input_ids, attention_mask, pixel_values = config_and_inputs\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": attention_mask,\n-            \"pixel_values\": pixel_values,\n-            \"return_loss\": True,\n-        }\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFGroupViTModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFGroupViTModel,) if is_tf_available() else ()\n-    pipeline_model_mapping = {\"feature-extraction\": TFGroupViTModel} if is_tf_available() else {}\n-    test_head_masking = False\n-    test_pruning = False\n-    test_resize_embeddings = False\n-    test_attention_outputs = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFGroupViTModelTester(self)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    @unittest.skip(reason=\"hidden_states are tested in individual model tests\")\n-    def test_hidden_states_output(self):\n-        pass\n-\n-    @unittest.skip(reason=\"input_embeds are tested in individual model tests\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skip(reason=\"CLIPModel does not have input/output embeddings\")\n-    def test_model_common_attributes(self):\n-        pass\n-\n-    @require_tensorflow_probability\n-    @slow\n-    def test_keras_fit(self):\n-        super().test_keras_fit()\n-\n-    # overwrite from common since `TFGroupViTModelTester` set `return_loss` to `True` and causes the preparation of\n-    # `symbolic_inputs` failed.\n-    def test_keras_save_load(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        # remove `return_loss` to make code work\n-        if self.__class__.__name__ == \"TFGroupViTModelTest\":\n-            inputs_dict.pop(\"return_loss\", None)\n-\n-        tf_main_layer_classes = {\n-            module_member\n-            for model_class in self.all_model_classes\n-            for module in (import_module(model_class.__module__),)\n-            for module_member_name in dir(module)\n-            if module_member_name.endswith(\"MainLayer\")\n-            # This condition is required, since `modeling_tf_clip.py` has 3 classes whose names end with `MainLayer`.\n-            and module_member_name[: -len(\"MainLayer\")] == model_class.__name__[: -len(\"Model\")]\n-            for module_member in (getattr(module, module_member_name),)\n-            if isinstance(module_member, type)\n-            and keras.layers.Layer in module_member.__bases__\n-            and getattr(module_member, \"_keras_serializable\", False)\n-        }\n-        for main_layer_class in tf_main_layer_classes:\n-            # T5MainLayer needs an embed_tokens parameter when called without the inputs_embeds parameter\n-            if \"T5\" in main_layer_class.__name__:\n-                # Take the same values than in TFT5ModelTester for this shared layer\n-                shared = TFSharedEmbeddings(99, 32, name=\"shared\")\n-                config.use_cache = inputs_dict.pop(\"use_cache\", None)\n-                main_layer = main_layer_class(config, embed_tokens=shared)\n-            else:\n-                main_layer = main_layer_class(config)\n-\n-            symbolic_inputs = {\n-                name: keras.Input(tensor.shape[1:], dtype=tensor.dtype) for name, tensor in inputs_dict.items()\n-            }\n-\n-            model = keras.Model(symbolic_inputs, outputs=main_layer(symbolic_inputs))\n-            outputs = model(inputs_dict)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                filepath = os.path.join(tmpdirname, \"keras_model.h5\")\n-                model.save(filepath)\n-                if \"T5\" in main_layer_class.__name__:\n-                    model = keras.models.load_model(\n-                        filepath,\n-                        custom_objects={\n-                            main_layer_class.__name__: main_layer_class,\n-                            \"TFSharedEmbeddings\": TFSharedEmbeddings,\n-                        },\n-                    )\n-                else:\n-                    model = keras.models.load_model(\n-                        filepath, custom_objects={main_layer_class.__name__: main_layer_class}\n-                    )\n-                assert isinstance(model, keras.Model)\n-                after_outputs = model(inputs_dict)\n-                self.assert_outputs_same(after_outputs, outputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"nvidia/groupvit-gcc-yfcc\"\n-        model = TFGroupViTModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    @unittest.skip(reason=\"Currently `saved_model` doesn't work with nested outputs.\")\n-    @slow\n-    def test_saved_model_creation(self):\n-        pass\n-\n-    @unittest.skip(reason=\"`saved_model` doesn't work with nested outputs so no preparation happens.\")\n-    @slow\n-    def test_prepare_serving_output(self):\n-        pass\n-\n-\n-# We will verify our results on an image of cute cats\n-def prepare_img():\n-    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-    im = Image.open(requests.get(url, stream=True).raw)\n-    return im\n-\n-\n-@require_vision\n-@require_tf\n-class TFGroupViTModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_inference(self):\n-        model_name = \"nvidia/groupvit-gcc-yfcc\"\n-        model = TFGroupViTModel.from_pretrained(model_name)\n-        processor = CLIPProcessor.from_pretrained(model_name)\n-\n-        image = prepare_img()\n-        inputs = processor(\n-            text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, padding=True, return_tensors=\"tf\"\n-        )\n-\n-        outputs = model(**inputs, training=False)\n-\n-        # verify the logits\n-        self.assertEqual(\n-            outputs.logits_per_image.shape,\n-            tf.TensorShape((inputs.pixel_values.shape[0], inputs.input_ids.shape[0])),\n-        )\n-        self.assertEqual(\n-            outputs.logits_per_text.shape,\n-            tf.TensorShape((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n-        )\n-\n-        expected_logits = tf.constant([[13.3523, 6.3629]])\n-\n-        tf.debugging.assert_near(outputs.logits_per_image, expected_logits, atol=1e-3)"
        },
        {
            "sha": "8d377ae8855c6e7cfc0eceb1d6ab5a858951f9bf",
            "filename": "tests/models/hubert/test_modeling_tf_hubert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 562,
            "changes": 562,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fhubert%2Ftest_modeling_tf_hubert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fhubert%2Ftest_modeling_tf_hubert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhubert%2Ftest_modeling_tf_hubert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,562 +0,0 @@\n-# Copyright 2021 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-\n-from __future__ import annotations\n-\n-import copy\n-import inspect\n-import math\n-import unittest\n-\n-import numpy as np\n-import pytest\n-\n-from transformers import is_tf_available\n-from transformers.testing_utils import require_soundfile, require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import HubertConfig, TFHubertForCTC, TFHubertModel, Wav2Vec2Processor\n-    from transformers.models.hubert.modeling_tf_hubert import _compute_mask_indices\n-\n-\n-@require_tf\n-class TFHubertModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=1024,\n-        is_training=False,\n-        hidden_size=16,\n-        feat_extract_norm=\"group\",\n-        feat_extract_dropout=0.0,\n-        feat_extract_activation=\"gelu\",\n-        conv_dim=(32, 32, 32),\n-        conv_stride=(4, 4, 4),\n-        conv_kernel=(8, 8, 8),\n-        conv_bias=False,\n-        num_conv_pos_embeddings=16,\n-        num_conv_pos_embedding_groups=2,\n-        num_hidden_layers=2,\n-        num_attention_heads=2,\n-        hidden_dropout_prob=0.1,  # this is most likely not correctly set yet\n-        intermediate_size=20,\n-        layer_norm_eps=1e-5,\n-        hidden_act=\"gelu\",\n-        initializer_range=0.02,\n-        vocab_size=32,\n-        do_stable_layer_norm=False,\n-        scope=None,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.hidden_size = hidden_size\n-        self.feat_extract_norm = feat_extract_norm\n-        self.feat_extract_dropout = feat_extract_dropout\n-        self.feat_extract_activation = feat_extract_activation\n-        self.conv_dim = conv_dim\n-        self.conv_stride = conv_stride\n-        self.conv_kernel = conv_kernel\n-        self.conv_bias = conv_bias\n-        self.num_conv_pos_embeddings = num_conv_pos_embeddings\n-        self.num_conv_pos_embedding_groups = num_conv_pos_embedding_groups\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.intermediate_size = intermediate_size\n-        self.layer_norm_eps = layer_norm_eps\n-        self.hidden_act = hidden_act\n-        self.initializer_range = initializer_range\n-        self.vocab_size = vocab_size\n-        self.do_stable_layer_norm = do_stable_layer_norm\n-        self.scope = scope\n-\n-        output_seq_length = self.seq_length\n-        for kernel, stride in zip(self.conv_kernel, self.conv_stride):\n-            output_seq_length = (output_seq_length - (kernel - 1)) / stride\n-        self.output_seq_length = int(math.ceil(output_seq_length))\n-        self.encoder_seq_length = self.output_seq_length\n-\n-    def prepare_config_and_inputs(self):\n-        input_values = tf.cast(ids_tensor([self.batch_size, self.seq_length], 32768), tf.float32) / 32768.0\n-        attention_mask = tf.ones_like(input_values)\n-\n-        config = HubertConfig(\n-            hidden_size=self.hidden_size,\n-            feat_extract_norm=self.feat_extract_norm,\n-            feat_extract_dropout=self.feat_extract_dropout,\n-            feat_extract_activation=self.feat_extract_activation,\n-            conv_dim=self.conv_dim,\n-            conv_stride=self.conv_stride,\n-            conv_kernel=self.conv_kernel,\n-            conv_bias=self.conv_bias,\n-            num_conv_pos_embeddings=self.num_conv_pos_embeddings,\n-            num_conv_pos_embedding_groups=self.num_conv_pos_embedding_groups,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            intermediate_size=self.intermediate_size,\n-            layer_norm_eps=self.layer_norm_eps,\n-            hidden_act=self.hidden_act,\n-            initializer_range=self.initializer_range,\n-            vocab_size=self.vocab_size,\n-            do_stable_layer_norm=self.do_stable_layer_norm,\n-        )\n-\n-        return config, input_values, attention_mask\n-\n-    def create_and_check_model(self, config, input_values, attention_mask):\n-        model = TFHubertModel(config)\n-        result = model(input_values, attention_mask=attention_mask)\n-        self.parent.assertEqual(\n-            result.last_hidden_state.shape, (self.batch_size, self.output_seq_length, self.hidden_size)\n-        )\n-\n-    def create_and_check_batch_inference(self, config, input_values, *args):\n-        # test does not pass for models making use of `group_norm`\n-        # check: https://github.com/pytorch/fairseq/issues/3227\n-        config.layerdrop = 0.0\n-        model = TFHubertModel(config)\n-\n-        input_values = input_values[:3]\n-        attention_mask = tf.ones_like(input_values)\n-\n-        input_lengths = tf.constant([input_values.shape[-1] // i for i in [4, 2, 1]])\n-        length_mask = tf.sequence_mask(input_lengths, dtype=tf.float32)\n-\n-        # convert values that are over input_lengths to padding\n-        input_values = input_values * length_mask\n-        attention_mask = attention_mask * length_mask\n-\n-        batch_outputs = model(input_values, attention_mask=attention_mask, training=False).last_hidden_state\n-\n-        for i in range(input_values.shape[0]):\n-            input_slice = input_values[i : i + 1, : input_lengths[i]]\n-            output = model(input_slice, training=False).last_hidden_state\n-\n-            batch_output = batch_outputs[i : i + 1, : output.shape[1]]\n-            self.parent.assertTrue(np.allclose(output, batch_output, atol=1e-3))\n-\n-    def check_ctc_loss(self, config, input_values, *args):\n-        model = TFHubertForCTC(config)\n-\n-        input_values = input_values[:3]\n-        attention_mask = tf.ones_like(input_values)\n-\n-        input_lengths = tf.constant([input_values.shape[-1] // i for i in [4, 2, 1]])\n-        max_length_labels = model.hubert._get_feat_extract_output_lengths(input_lengths)\n-        labels = ids_tensor((input_values.shape[0], min(max_length_labels) - 1), model.config.vocab_size)\n-\n-        length_mask = tf.sequence_mask(input_lengths, dtype=tf.float32)\n-\n-        # convert values that are over input_lengths to padding\n-        input_values = input_values * length_mask\n-        attention_mask = attention_mask * length_mask\n-\n-        model.config.ctc_loss_reduction = \"sum\"\n-        sum_loss = model(input_values, attention_mask=attention_mask, labels=labels).loss\n-\n-        model.config.ctc_loss_reduction = \"mean\"\n-        mean_loss = model(input_values, attention_mask=attention_mask, labels=labels).loss\n-\n-        self.parent.assertTrue(abs(labels.shape[0] * mean_loss - sum_loss) < 1e-2)\n-\n-    def check_training(self, config, input_values, *args):\n-        model = TFHubertForCTC(config)\n-\n-        # freeze feature encoder\n-        model.freeze_feature_encoder()\n-\n-        input_values = input_values[:3]\n-\n-        input_lengths = tf.constant([input_values.shape[-1] // i for i in [4, 2, 1]])\n-        max_length_labels = model.hubert._get_feat_extract_output_lengths(input_lengths)\n-        labels = ids_tensor((input_values.shape[0], max(max_length_labels) - 2), model.config.vocab_size)\n-\n-        length_mask = tf.sequence_mask(input_lengths, dtype=tf.float32)\n-\n-        input_values = input_values * length_mask\n-\n-        pad_size = max(max_length_labels) - labels.shape[1]\n-        labels = tf.pad(labels, ((0, 0), (0, pad_size)), constant_values=-100)\n-\n-        loss = model(input_values, labels=labels, training=True).loss\n-\n-        self.parent.assertFalse(tf.math.is_inf(loss))\n-\n-    def check_labels_out_of_vocab(self, config, input_values, *args):\n-        model = TFHubertForCTC(config)\n-        input_lengths = tf.constant([input_values.shape[-1] // i for i in [4, 2, 1]])\n-        max_length_labels = model.hubert._get_feat_extract_output_lengths(input_lengths)\n-        labels = ids_tensor((input_values.shape[0], min(max_length_labels) - 1), model.config.vocab_size + 100)\n-        with pytest.raises(ValueError):\n-            model(input_values, labels=labels)\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config, input_values, attention_mask = self.prepare_config_and_inputs()\n-        inputs_dict = {\"input_values\": input_values, \"attention_mask\": attention_mask}\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFHubertModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFHubertModel, TFHubertForCTC) if is_tf_available() else ()\n-    pipeline_model_mapping = {\"feature-extraction\": TFHubertModel} if is_tf_available() else {}\n-    test_resize_embeddings = False\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFHubertModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=HubertConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    # overwrite because input_values != input_ids\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.call)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"input_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    # overwrite because input_values != input_ids\n-    def test_keyword_and_dict_args(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            outputs_dict = model(inputs)\n-\n-            inputs_keywords = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n-            input_values = inputs_keywords.pop(\"input_values\", None)\n-            outputs_keywords = model(input_values, **inputs_keywords)\n-            output_dict = outputs_dict[0].numpy()\n-            output_keywords = outputs_keywords[0].numpy()\n-\n-            self.assertLess(np.sum(np.abs(output_dict - output_keywords)), 1e-6)\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_hidden_states_output(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        def check_hidden_states_output(config, inputs_dict, model_class):\n-            model = model_class(config)\n-            outputs = model(self._prepare_for_class(inputs_dict, model_class))\n-            expected_num_layers = getattr(\n-                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n-            )\n-\n-            hidden_states = outputs.hidden_states\n-            self.assertEqual(config.output_attentions, False)\n-            self.assertEqual(len(hidden_states), expected_num_layers)\n-            self.assertListEqual(\n-                list(hidden_states[0].shape[-2:]),\n-                [self.model_tester.output_seq_length, self.model_tester.hidden_size],\n-            )\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_hidden_states\"] = True\n-            check_hidden_states_output(config, inputs_dict, model_class)\n-\n-            del inputs_dict[\"output_hidden_states\"]\n-            config.output_hidden_states = True\n-            check_hidden_states_output(config, inputs_dict, model_class)\n-\n-    def test_ctc_loss_inference(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.check_ctc_loss(*config_and_inputs)\n-\n-    def test_train(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.check_training(*config_and_inputs)\n-\n-    def test_labels_out_of_vocab(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.check_labels_out_of_vocab(*config_and_inputs)\n-\n-    @unittest.skip(reason=\"Hubert has no input embeddings\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Hubert has no tokens embeddings\")\n-    def test_resize_tokens_embeddings(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Hubert has no input embeddings\")\n-    def test_model_common_attributes(self):\n-        pass\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model = TFHubertModel.from_pretrained(\"facebook/hubert-base-ls960\")\n-        self.assertIsNotNone(model)\n-\n-    @unittest.skip(reason=\"Fix me! Hubert hits OOM errors when loss is computed on full batch\")\n-    def test_dataset_conversion(self):\n-        # TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC\n-        pass\n-\n-    @unittest.skip(reason=\"Fix me! Hubert hits OOM errors when loss is computed on full batch\")\n-    def test_keras_fit(self):\n-        # TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC\n-        pass\n-\n-\n-@require_tf\n-class TFHubertRobustModelTest(TFModelTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFHubertModel, TFHubertForCTC) if is_tf_available() else ()\n-    test_resize_embeddings = False\n-    test_head_masking = False\n-    test_onnx = False\n-\n-    def setUp(self):\n-        self.model_tester = TFHubertModelTester(\n-            self,\n-            conv_stride=(3, 3, 3),\n-            feat_extract_norm=\"layer\",\n-            do_stable_layer_norm=True,\n-            scope=\"robust\",\n-        )\n-        self.config_tester = ConfigTester(self, config_class=HubertConfig, hidden_size=37)\n-\n-    # overwrite because input_values != input_ids\n-    def test_forward_signature(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            signature = inspect.signature(model.call)\n-            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n-            arg_names = [*signature.parameters.keys()]\n-\n-            expected_arg_names = [\"input_values\"]\n-            self.assertListEqual(arg_names[:1], expected_arg_names)\n-\n-    # overwrite because input_values != input_ids\n-    def test_keyword_and_dict_args(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            outputs_dict = model(inputs)\n-\n-            inputs_keywords = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n-            input_values = inputs_keywords.pop(\"input_values\", None)\n-            outputs_keywords = model(input_values, **inputs_keywords)\n-            output_dict = outputs_dict[0].numpy()\n-            output_keywords = outputs_keywords[0].numpy()\n-\n-            self.assertLess(np.sum(np.abs(output_dict - output_keywords)), 1e-6)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_hidden_states_output(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        def check_hidden_states_output(config, inputs_dict, model_class):\n-            model = model_class(config)\n-            outputs = model(self._prepare_for_class(inputs_dict, model_class))\n-            expected_num_layers = getattr(\n-                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n-            )\n-\n-            hidden_states = outputs.hidden_states\n-            self.assertEqual(config.output_attentions, False)\n-            self.assertEqual(len(hidden_states), expected_num_layers)\n-            self.assertListEqual(\n-                list(hidden_states[0].shape[-2:]),\n-                [self.model_tester.output_seq_length, self.model_tester.hidden_size],\n-            )\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_hidden_states\"] = True\n-            check_hidden_states_output(config, inputs_dict, model_class)\n-\n-            del inputs_dict[\"output_hidden_states\"]\n-            config.output_hidden_states = True\n-            check_hidden_states_output(config, inputs_dict, model_class)\n-\n-    def test_batched_inference(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_batch_inference(*config_and_inputs)\n-\n-    def test_ctc_loss_inference(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.check_ctc_loss(*config_and_inputs)\n-\n-    def test_train(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.check_training(*config_and_inputs)\n-\n-    def test_labels_out_of_vocab(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.check_labels_out_of_vocab(*config_and_inputs)\n-\n-    @unittest.skip(reason=\"Hubert has no input embeddings\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Hubert has no tokens embeddings\")\n-    def test_resize_tokens_embeddings(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Hubert has no input embeddings or get_input_embeddings method\")\n-    def test_model_common_attributes(self):\n-        pass\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model = TFHubertModel.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n-        self.assertIsNotNone(model)\n-\n-    @unittest.skip(reason=\"Fix me! Hubert hits OOM errors when loss is computed on full batch\")\n-    def test_dataset_conversion(self):\n-        # TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC\n-        pass\n-\n-    @unittest.skip(reason=\"Fix me! Hubert hits OOM errors when loss is computed on full batch\")\n-    def test_keras_fit(self):\n-        # TODO: (Amy) - check whether skipping CTC model resolves this issue and possible resolutions for CTC\n-        pass\n-\n-\n-@require_tf\n-class TFHubertUtilsTest(unittest.TestCase):\n-    def test_compute_mask_indices(self):\n-        batch_size = 4\n-        sequence_length = 60\n-        mask_prob = 0.5\n-        mask_length = 1\n-\n-        mask = _compute_mask_indices((batch_size, sequence_length), mask_prob, mask_length)\n-\n-        self.assertListEqual(\n-            tf.reduce_sum(mask, -1).numpy().tolist(), [mask_prob * sequence_length for _ in range(batch_size)]\n-        )\n-\n-    def test_compute_mask_indices_overlap(self):\n-        batch_size = 4\n-        sequence_length = 80\n-        mask_prob = 0.5\n-        mask_length = 4\n-\n-        mask = _compute_mask_indices((batch_size, sequence_length), mask_prob, mask_length)\n-\n-        # because of overlap mask don't have to add up exactly to `mask_prob * sequence_length`, but have to be smaller or equal\n-        for batch_sum in tf.reduce_sum(mask, -1):\n-            self.assertTrue(int(batch_sum) <= mask_prob * sequence_length)\n-\n-\n-@require_tf\n-@slow\n-@require_soundfile\n-class TFHubertModelIntegrationTest(unittest.TestCase):\n-    def _load_datasamples(self, num_samples):\n-        from datasets import load_dataset\n-\n-        ds = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation\")\n-        # automatic decoding with librispeech\n-        speech_samples = ds.sort(\"id\").filter(\n-            lambda x: x[\"id\"] in [f\"1272-141231-000{i}\" for i in range(num_samples)]\n-        )[:num_samples][\"audio\"]\n-\n-        return [x[\"array\"] for x in speech_samples]\n-\n-    def test_inference_ctc_normal(self):\n-        model = TFHubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n-        processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\", do_lower_case=True)\n-        input_speech = self._load_datasamples(1)\n-\n-        input_values = processor(input_speech, return_tensors=\"tf\", sampling_rate=16000).input_values\n-\n-        logits = model(input_values).logits\n-\n-        predicted_ids = tf.argmax(logits, axis=-1)\n-        predicted_trans = processor.batch_decode(predicted_ids)\n-\n-        EXPECTED_TRANSCRIPTIONS = [\"a man said to the universe sir i exist\"]\n-        self.assertListEqual(predicted_trans, EXPECTED_TRANSCRIPTIONS)\n-\n-    def test_inference_ctc_normal_batched(self):\n-        model = TFHubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n-        processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\", do_lower_case=True)\n-\n-        input_speech = self._load_datasamples(2)\n-\n-        input_values = processor(input_speech, return_tensors=\"tf\", padding=True, sampling_rate=16000).input_values\n-\n-        logits = model(input_values).logits\n-\n-        predicted_ids = tf.argmax(logits, axis=-1)\n-        predicted_trans = processor.batch_decode(predicted_ids)\n-\n-        EXPECTED_TRANSCRIPTIONS = [\n-            \"a man said to the universe sir i exist\",\n-            \"sweat covered brion's body trickling into the tight loin cloth that was the only garment he wore\",\n-        ]\n-        self.assertListEqual(predicted_trans, EXPECTED_TRANSCRIPTIONS)\n-\n-    def test_inference_ctc_robust_batched(self):\n-        model = TFHubertForCTC.from_pretrained(\"facebook/hubert-large-ls960-ft\")\n-        processor = Wav2Vec2Processor.from_pretrained(\"facebook/hubert-large-ls960-ft\", do_lower_case=True)\n-\n-        input_speech = self._load_datasamples(4)\n-\n-        inputs = processor(input_speech, return_tensors=\"tf\", padding=True, sampling_rate=16000)\n-\n-        input_values = inputs.input_values\n-        attention_mask = inputs.attention_mask\n-\n-        logits = model(input_values, attention_mask=attention_mask).logits\n-\n-        predicted_ids = tf.argmax(logits, axis=-1)\n-        predicted_trans = processor.batch_decode(predicted_ids)\n-\n-        EXPECTED_TRANSCRIPTIONS = [\n-            \"a man said to the universe sir i exist\",\n-            \"sweat covered brion's body trickling into the tight loin cloth that was the only garment he wore\",\n-            \"the cut on his chest still dripping blood the ache of his overstrained eyes even the soaring arena around\"\n-            \" him with the thousands of spectators were trivialities not worth thinking about\",\n-            \"his instant of panic was followed by a small sharp blow high on his chest\",\n-        ]\n-        self.assertListEqual(predicted_trans, EXPECTED_TRANSCRIPTIONS)"
        },
        {
            "sha": "bd7c9b06c29d65cf859af6094dd7e54eb1e6c7d5",
            "filename": "tests/models/idefics/test_modeling_tf_idefics.py",
            "status": "removed",
            "additions": 0,
            "deletions": 559,
            "changes": 559,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fidefics%2Ftest_modeling_tf_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fidefics%2Ftest_modeling_tf_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_tf_idefics.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,559 +0,0 @@\n-# Copyright 2023 The HuggingFace Inc. team. All rights reserved.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\"\"\"Testing suite for the TF Idefics model.\"\"\"\n-\n-import os\n-import tempfile\n-import unittest\n-from importlib import import_module\n-\n-from transformers import IdeficsConfig, is_tf_available, is_vision_available\n-from transformers.testing_utils import TestCasePlus, require_tf, require_vision, slow\n-from transformers.utils import cached_property\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers import IdeficsProcessor, TFIdeficsForVisionText2Text, TFIdeficsModel\n-    from transformers.modeling_tf_utils import keras\n-    from transformers.models.idefics.configuration_idefics import IdeficsPerceiverConfig, IdeficsVisionConfig\n-\n-if is_vision_available():\n-    from PIL import Image\n-\n-\n-IDEFICS_TINY_RANDOM_MODEL = \"HuggingFaceM4/tiny-random-idefics\"\n-\n-\n-class IdeficsModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=1,\n-        seq_length=7,\n-        image_size=30,\n-        patch_size=2,\n-        num_channels=3,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=5,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        scope=None,\n-        modality_type_vocab_size=2,\n-        vision_embed_dim=32,\n-        vision_patch_size=2,\n-        vision_image_size=30,\n-        vision_num_attention_heads=4,\n-        vision_num_hidden_layers=5,\n-        vision_intermediate_size=37,\n-        perceiver_qk_layer_norms_perceiver=False,\n-        perceiver_resampler_depth=2,\n-        perceiver_resampler_head_dim=8,\n-        perceiver_resampler_n_heads=2,\n-        perceiver_resampler_n_latents=16,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.image_size = image_size\n-        self.patch_size = patch_size\n-        self.num_channels = num_channels\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.scope = scope\n-        self.modality_type_vocab_size = modality_type_vocab_size\n-\n-        self.vision_embed_dim = vision_embed_dim\n-        self.vision_patch_size = vision_patch_size\n-        self.vision_image_size = vision_image_size\n-        self.vision_num_attention_heads = vision_num_attention_heads\n-        self.vision_num_hidden_layers = vision_num_hidden_layers\n-        self.vision_intermediate_size = vision_intermediate_size\n-\n-        self.vision_config = IdeficsVisionConfig(\n-            embed_dim=self.vision_embed_dim,\n-            patch_size=self.vision_patch_size,\n-            image_size=self.vision_image_size,\n-            num_attention_heads=self.vision_num_attention_heads,\n-            num_hidden_layers=self.vision_num_hidden_layers,\n-            intermediate_size=self.vision_intermediate_size,\n-        )\n-\n-        self.perceiver_qk_layer_norms_perceiver = perceiver_qk_layer_norms_perceiver\n-        self.perceiver_resampler_depth = perceiver_resampler_depth\n-        self.perceiver_resampler_head_dim = perceiver_resampler_head_dim\n-        self.perceiver_resampler_n_heads = perceiver_resampler_n_heads\n-        self.perceiver_resampler_n_latents = perceiver_resampler_n_latents\n-\n-        self.perceiver_config = IdeficsPerceiverConfig(\n-            qk_layer_norms_perceiver=self.perceiver_qk_layer_norms_perceiver,\n-            resampler_depth=self.perceiver_resampler_depth,\n-            resampler_head_dim=self.perceiver_resampler_head_dim,\n-            resampler_n_heads=self.perceiver_resampler_n_heads,\n-            resampler_n_latents=self.perceiver_resampler_n_latents,\n-        )\n-\n-        # we set the expected sequence length (which is used in several tests)\n-        # this is equal to the seq length of the text tokens + number of image patches + 1 for the CLS token\n-        self.expected_seq_len = self.seq_length + (self.image_size // self.patch_size) ** 2 + 1\n-\n-    def prepare_config_and_inputs(self, num_images=1, interpolate_pos_encoding=False, image_expansion=0):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        pixel_values = floats_tensor(\n-            [\n-                self.batch_size,\n-                num_images,\n-                self.num_channels,\n-                self.image_size + image_expansion,\n-                self.image_size + image_expansion,\n-            ]\n-        )\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        image_attention_mask = random_attention_mask([self.batch_size, self.seq_length, num_images])\n-\n-        config = self.get_config()\n-        return (config, input_ids, input_mask, pixel_values, image_attention_mask, interpolate_pos_encoding)\n-\n-    def get_config(self):\n-        return IdeficsConfig(\n-            image_size=self.image_size,\n-            patch_size=self.patch_size,\n-            num_channels=self.num_channels,\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            is_decoder=False,\n-            initializer_range=self.initializer_range,\n-            num_labels=self.num_labels,\n-            modality_type_vocab_size=self.modality_type_vocab_size,\n-            vision_config=self.vision_config,\n-        )\n-\n-    def create_and_check_model(\n-        self,\n-        config,\n-        input_ids,\n-        input_mask,\n-        pixel_values,\n-        image_attention_mask,\n-        interpolate_pos_encoding,\n-    ):\n-        model = TFIdeficsModel(config=config)\n-        result = model(\n-            input_ids,\n-            attention_mask=input_mask,\n-            pixel_values=pixel_values,\n-            image_attention_mask=image_attention_mask,\n-            interpolate_pos_encoding=interpolate_pos_encoding,\n-        )\n-        self.parent.assertEqual(\n-            result.last_hidden_state.shape, (self.batch_size, input_ids.shape[1], self.hidden_size)\n-        )\n-\n-    def create_and_check_model_gen(\n-        self,\n-        config,\n-        input_ids,\n-        input_mask,\n-        pixel_values,\n-        image_attention_mask,\n-        interpolate_pos_encoding,\n-    ):\n-        model = TFIdeficsForVisionText2Text(config)\n-        model.generate(\n-            input_ids,\n-            attention_mask=input_mask,\n-            pixel_values=pixel_values,\n-            image_attention_mask=image_attention_mask,\n-            interpolate_pos_encoding=interpolate_pos_encoding,\n-            max_length=self.seq_length + 2,\n-        )\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            input_mask,\n-            pixel_values,\n-            image_attention_mask,\n-            interpolate_pos_encoding,\n-        ) = config_and_inputs\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"attention_mask\": input_mask,\n-            \"pixel_values\": pixel_values,\n-            \"image_attention_mask\": image_attention_mask,\n-            \"interpolate_pos_encoding\": interpolate_pos_encoding,\n-        }\n-        return config, inputs_dict\n-\n-    def prepare_pixel_values(self):\n-        return floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n-\n-\n-@require_tf\n-class TFIdeficsModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (TFIdeficsModel, TFIdeficsForVisionText2Text) if is_tf_available() else ()\n-    pipeline_model_mapping = {\"feature-extraction\": TFIdeficsModel} if is_tf_available() else {}\n-    test_pruning = False\n-    test_headmasking = False\n-    test_onnx = False\n-    test_resize_embeddings = False\n-\n-    def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n-        inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n-        # XXX: IdeficsForVisionText2TextTest has no MODEL_FOR group yet, but it should be the same\n-        # as MODEL_FOR_CAUSAL_LM_MAPPING_NAMES, so for now manually changing to do the right thing\n-        # as super won't do it\n-        if return_labels:\n-            inputs_dict[\"labels\"] = tf.zeros(\n-                (self.model_tester.batch_size, self.model_tester.seq_length), dtype=tf.int64\n-            )\n-        return inputs_dict\n-\n-    def test_model_outputs_equivalence(self):\n-        try:\n-            orig = self.all_model_classes\n-            # IdeficsModel.forward doesn't have labels input arg - only IdeficsForVisionText2Text does\n-            self.all_model_classes = (TFIdeficsForVisionText2Text,) if is_tf_available() else ()\n-            super().test_model_outputs_equivalence()\n-        finally:\n-            self.all_model_classes = orig\n-\n-    def setUp(self):\n-        self.model_tester = IdeficsModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=IdeficsConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model_single_image(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs(\n-            num_images=1, interpolate_pos_encoding=False, image_expansion=0\n-        )\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_multiple_images(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs(\n-            num_images=2, interpolate_pos_encoding=False, image_expansion=0\n-        )\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_with_image_pos_embeddings_interpolation_single_image(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs(\n-            num_images=1, interpolate_pos_encoding=True, image_expansion=2\n-        )\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs(\n-            num_images=1, interpolate_pos_encoding=True, image_expansion=0\n-        )\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_model_with_image_pos_embeddings_interpolation_multiple_images(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs(\n-            num_images=2, interpolate_pos_encoding=True, image_expansion=2\n-        )\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs(\n-            num_images=2, interpolate_pos_encoding=True, image_expansion=0\n-        )\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_generate_with_image_pos_embeddings_interpolation_single_image(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs(\n-            num_images=1, interpolate_pos_encoding=True, image_expansion=2\n-        )\n-        self.model_tester.create_and_check_model_gen(*config_and_inputs)\n-\n-    def test_generate_with_image_pos_embeddings_interpolation_multiple_images(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs(\n-            num_images=2, interpolate_pos_encoding=True, image_expansion=2\n-        )\n-        self.model_tester.create_and_check_model_gen(*config_and_inputs)\n-\n-    def test_training_gradient_checkpointing(self):\n-        pass\n-\n-    @unittest.skip(reason=\"\"\"IDEFICS does not support retaining the gradients of the hidden states and attention\"\"\")\n-    def test_retain_grad_hidden_states_attentions(self):\n-        return\n-\n-    @unittest.skip(reason=\"IDEFICS uses out-of-bounds embeddings deliberately.\")\n-    def test_embeddings_out_of_bounds_raise_exception(self):\n-        pass\n-\n-    @unittest.skip(reason=\"IDEFICS attention weights are not extracted in scaled_dot_product_attention\")\n-    def test_prepare_serving_output(self):\n-        pass\n-\n-    def test_model_common_attributes(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            self.assertIsInstance(model.get_input_embeddings(), (tf.keras.layers.Layer))\n-            x = model.get_output_embeddings()\n-            self.assertTrue(x is None or isinstance(x, tf.keras.layers.Layer))\n-\n-    def test_attention_outputs(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.return_dict = True\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_attentions\"] = True\n-            inputs_dict[\"output_hidden_states\"] = False\n-            config.return_dict = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-            attentions = outputs.attentions\n-\n-            self.assertEqual(len(attentions), self.model_tester.num_hidden_layers)\n-\n-            # check that output_attentions also work using config\n-            del inputs_dict[\"output_attentions\"]\n-            config.output_attentions = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-            attentions = outputs.attentions\n-            # IDEFICS does not support outputting attention score because it uses SDPA under the hood\n-            self.assertTrue(attentions[0] is None)\n-            out_len = len(outputs)\n-\n-            # Check attention is always last and order is fine\n-            inputs_dict[\"output_attentions\"] = True\n-            inputs_dict[\"output_hidden_states\"] = True\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-            self.assertEqual(out_len + 1, len(outputs))\n-\n-            self_attentions = outputs.encoder_attentions if config.is_encoder_decoder else outputs.attentions\n-\n-            self.assertEqual(len(self_attentions), self.model_tester.num_hidden_layers)\n-            # IDEFICS does not support outputting attention score because it uses SDPA under the hood\n-            self.assertTrue(self_attentions[0] is None)\n-\n-    def test_hidden_states_output(self):\n-        def check_hidden_states_output(inputs_dict, config, model_class):\n-            model = model_class(config)\n-            outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n-\n-            hidden_states = outputs.encoder_hidden_states if config.is_encoder_decoder else outputs.hidden_states\n-\n-            expected_num_layers = getattr(\n-                self.model_tester, \"expected_num_hidden_layers\", self.model_tester.num_hidden_layers + 1\n-            )\n-            self.assertEqual(len(hidden_states), expected_num_layers)\n-\n-            seq_length = self.model_tester.seq_length\n-\n-            self.assertListEqual(\n-                list(hidden_states[0].shape[-2:]),\n-                [seq_length, self.model_tester.hidden_size],\n-            )\n-\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            inputs_dict[\"output_hidden_states\"] = True\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-            # check that output_hidden_states also work using config\n-            del inputs_dict[\"output_hidden_states\"]\n-            config.output_hidden_states = True\n-\n-            check_hidden_states_output(inputs_dict, config, model_class)\n-\n-    def test_keras_save_load(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        tf_main_layer_classes = {\n-            module_member\n-            for model_class in self.all_model_classes\n-            for module in (import_module(model_class.__module__),)\n-            for module_member_name in dir(module)\n-            if module_member_name.endswith(\"MainLayer\")\n-            for module_member in (getattr(module, module_member_name),)\n-            if isinstance(module_member, type)\n-            and keras.layers.Layer in module_member.__bases__\n-            and getattr(module_member, \"_keras_serializable\", False)\n-        }\n-\n-        for main_layer_class in tf_main_layer_classes:\n-            main_layer = main_layer_class(config)\n-\n-            symbolic_inputs = {\n-                name: keras.Input(tensor.shape[1:], dtype=tensor.dtype, batch_size=2)\n-                for name, tensor in inputs_dict.items()\n-                if tf.is_tensor(tensor)\n-            }\n-            model = keras.Model(symbolic_inputs, outputs=main_layer(symbolic_inputs))\n-            outputs = model(inputs_dict)\n-\n-            with tempfile.TemporaryDirectory() as tmpdirname:\n-                filepath = os.path.join(tmpdirname, \"keras_model.h5\")\n-                model.save(filepath)\n-                model = keras.models.load_model(filepath, custom_objects={main_layer_class.__name__: main_layer_class})\n-                assert isinstance(model, keras.Model)\n-                after_outputs = model(inputs_dict)\n-                self.assert_outputs_same(after_outputs, outputs)\n-\n-    @unittest.skip(reason=\"IDEFICS test_keras_fit testing done in TFIdeficsForVisionText2TextTest\")\n-    def test_keras_fit(self):\n-        pass\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model = TFIdeficsModel.from_pretrained(IDEFICS_TINY_RANDOM_MODEL, from_pt=True)\n-        self.assertIsNotNone(model)\n-\n-    @unittest.skip(reason=\"Currently `saved_model` doesn't work with nested outputs.\")\n-    def test_saved_model_creation(self):\n-        pass\n-\n-    @unittest.skip(reason=\"\"\"IDEFICS loss computation not implemented yet\"\"\")\n-    def test_loss_computation(self):\n-        pass\n-\n-\n-@require_tf\n-class TFIdeficsForVisionText2TextTest(TFIdeficsModelTest, unittest.TestCase):\n-    all_model_classes = (TFIdeficsForVisionText2Text,) if is_tf_available() else ()\n-    test_resize_embeddings = False\n-\n-    def setUp(self):\n-        self.model_tester = IdeficsModelTester(\n-            self,\n-            modality_type_vocab_size=3,\n-        )\n-        self.config_tester = ConfigTester(self, config_class=IdeficsConfig, hidden_size=37)\n-\n-    @unittest.skip(\"We only test the model that takes in multiple images\")\n-    def test_model(self):\n-        pass\n-\n-    @unittest.skip(\"We only test the model that takes in multiple images\")\n-    def test_for_token_classification(self):\n-        pass\n-\n-    @unittest.skip(reason=\"\"\"IDEFICS does not support retaining the gradients of the hidden states and attention\"\"\")\n-    def test_retain_grad_hidden_states_attentions(self):\n-        pass\n-\n-    @unittest.skip(reason=\"\"\"IDEFICS loss computation not implemented yet\"\"\")\n-    def test_loss_computation(self):\n-        pass\n-\n-    @slow\n-    def test_keras_fit(self):\n-        super().test_keras_fit()\n-\n-\n-# Below is the expected output for the integration test TFIdeficsModelIntegrationTest.\n-# Since we are using tiny-random to be able to fit it on the CI GPU,it is better to assert on the\n-# ids because the generated text is gibberish\n-\n-# fmt: off\n-EXPECTED_GENERATED_IDS = [[0, 0, 1, 4911, 29901, 32000, 32001, 32000, 20355, 915, 445, 1967, 29889, 13, 7900, 22137, 29901, 530, 1967, 310, 1023, 26361, 29889, 13, 2659, 29901, 32000, 32001, 32000, 20355, 915, 445, 1967, 29889, 13, 7900, 22137, 29901, 25519, 22326, 8071, 26357, 28004, 4428, 5916, 14383, 1033, 12358, 10536, 21834, 10447, 21201, 18102, 16886, 8875, 25388, 25914, 28304, 8558, 31048, 1322, 25952, 189, 31600, 3600, 12824, 7045, 28090, 20228, 32001, 5385, 29186, 2165, 11822, 13825, 23077, 7883, 22504, 2078, 18893, 2179, 10556, 9515, 7672, 3491, 12403, 5398, 27299, 6463, 16349, 23037, 28956, 16960, 22664, 7724, 17587, 17424, 10175, 17417, 5930, 30855, 17695, 16170, 14474, 29996, 313, 14502, 3241, 13618, 32001, 5385, 29186, 2165, 11822, 13825, 19934, 4875, 27142, 3230, 2709, 28054, 3270, 19148, 10917, 1060, 26443, 12259, 1347, 28482, 3830, 25519, 199, 12782, 9144, 12289, 1142, 18400, 21390, 19129, 7292, 28430, 24711, 5551, 30349, 30533, 13271, 17697, 4982, 8713, 5380, 17869, 12490, 5398, 27299, 11593, 19918, 15924, 29430, 10175, 17417, 5930, 30855, 17695, 16170, 14474, 19234],\n-                          [1, 4911, 29901, 32000, 32001, 32000, 20355, 915, 445, 1967, 29889, 13, 7900, 22137, 29901, 530, 1967, 310, 1023, 413, 986, 575, 29889, 13, 2659, 29901, 32000, 32001, 32000, 20355, 915, 445, 1967, 29889, 13, 7900, 22137, 29901, 25519, 22326, 8071, 26357, 28004, 4428, 17554, 20500, 21714, 27834, 4798, 12195, 30379, 5427, 20228, 10473, 14351, 8049, 15605, 14491, 212, 2711, 32000, 21714, 31259, 24368, 19036, 22970, 26083, 19394, 20372, 7672, 9939, 25388, 30533, 8200, 30271, 2114, 24749, 13224, 10603, 21118, 2179, 3759, 16515, 6587, 1287, 23998, 17793, 32001, 5385, 29186, 2165, 11822, 13825, 29732, 17503, 2729, 6722, 2943, 1221, 16043, 18244, 24965, 14383, 19840, 5980, 13488, 28531, 735, 26146, 22504, 2078, 18893, 20372, 7672, 32001, 5385, 29186, 2165, 11822, 13825, 29732, 17503, 2729, 6722, 19551, 220, 10528, 28940, 4453, 28266, 15416, 18693, 8199, 1153, 27706, 29231, 29186, 2165, 11822, 13825, 29732, 17503, 2729, 6722, 19551, 8231, 10739, 31992, 25906, 22254, 23127, 7689, 19614, 1149, 18844, 23037, 28956, 16960, 22664, 6975, 28938, 24002, 11026, 15020, 21964, 16307], ]\n-\n-@require_tf\n-@require_vision\n-class TFIdeficsModelIntegrationTest(TestCasePlus):\n-    @cached_property\n-    def default_processor(self):\n-        return IdeficsProcessor.from_pretrained(IDEFICS_TINY_RANDOM_MODEL) if is_vision_available() else None\n-\n-    @slow\n-    def test_inference_natural_language_visual_reasoning(self):\n-        cat_image_path = self.tests_dir / \"fixtures/tests_samples/COCO/000000039769.png\"\n-        cats_image_obj = Image.open(cat_image_path)  # 2 cats\n-        dogs_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures_nlvr2/raw/main/image1.jpeg\"\n-\n-        prompts = [\n-            [\n-                \"User:\",\n-                dogs_image_url,\n-                \"Describe this image.\\nAssistant: An image of two dogs.\\n\",\n-                \"User:\",\n-                cats_image_obj,\n-                \"Describe this image.\\nAssistant:\",\n-            ],\n-            [\n-                \"User:\",\n-                cats_image_obj,\n-                \"Describe this image.\\nAssistant: An image of two kittens.\\n\",\n-                \"User:\",\n-                dogs_image_url,\n-                \"Describe this image.\\nAssistant:\",\n-            ],\n-        ]\n-\n-        model = TFIdeficsForVisionText2Text.from_pretrained(IDEFICS_TINY_RANDOM_MODEL, from_pt=True)\n-        processor = self.default_processor\n-        inputs = processor(prompts, return_tensors=\"tf\")\n-        generated_ids = model.generate(**inputs, max_length=100)\n-        generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)\n-\n-        # keep for debugging\n-        for i, t in enumerate(generated_text):\n-            t = bytes(t, \"utf-8\").decode(\"unicode_escape\")\n-            print(f\"{i}:\\n{t}\\n\")\n-\n-        self.assertListEqual(EXPECTED_GENERATED_IDS[0], generated_ids[0].numpy().tolist())\n-        self.assertListEqual(EXPECTED_GENERATED_IDS[1], generated_ids[1].numpy().tolist())"
        },
        {
            "sha": "f2690b14fb071e08cdb29f2665616f0161b4b66b",
            "filename": "tests/models/layoutlm/test_modeling_tf_layoutlm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 369,
            "changes": 369,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_tf_layoutlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_tf_layoutlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlm%2Ftest_modeling_tf_layoutlm.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24",
            "patch": "@@ -1,369 +0,0 @@\n-# Copyright 2018 The Microsoft Research Asia LayoutLM Team Authors, The Hugging Face Team.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-from __future__ import annotations\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import LayoutLMConfig, is_tf_available\n-from transformers.testing_utils import require_tf, slow\n-\n-from ...test_configuration_common import ConfigTester\n-from ...test_modeling_tf_common import TFModelTesterMixin, ids_tensor, random_attention_mask\n-from ...test_pipeline_mixin import PipelineTesterMixin\n-\n-\n-if is_tf_available():\n-    import tensorflow as tf\n-\n-    from transformers.models.layoutlm.modeling_tf_layoutlm import (\n-        TFLayoutLMForMaskedLM,\n-        TFLayoutLMForQuestionAnswering,\n-        TFLayoutLMForSequenceClassification,\n-        TFLayoutLMForTokenClassification,\n-        TFLayoutLMModel,\n-    )\n-\n-\n-class TFLayoutLMModelTester:\n-    def __init__(\n-        self,\n-        parent,\n-        batch_size=13,\n-        seq_length=7,\n-        is_training=True,\n-        use_input_mask=True,\n-        use_token_type_ids=True,\n-        use_labels=True,\n-        vocab_size=99,\n-        hidden_size=32,\n-        num_hidden_layers=2,\n-        num_attention_heads=4,\n-        intermediate_size=37,\n-        hidden_act=\"gelu\",\n-        hidden_dropout_prob=0.1,\n-        attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=512,\n-        type_vocab_size=16,\n-        type_sequence_label_size=2,\n-        initializer_range=0.02,\n-        num_labels=3,\n-        num_choices=4,\n-        scope=None,\n-        range_bbox=1000,\n-    ):\n-        self.parent = parent\n-        self.batch_size = batch_size\n-        self.seq_length = seq_length\n-        self.is_training = is_training\n-        self.use_input_mask = use_input_mask\n-        self.use_token_type_ids = use_token_type_ids\n-        self.use_labels = use_labels\n-        self.vocab_size = vocab_size\n-        self.hidden_size = hidden_size\n-        self.num_hidden_layers = num_hidden_layers\n-        self.num_attention_heads = num_attention_heads\n-        self.intermediate_size = intermediate_size\n-        self.hidden_act = hidden_act\n-        self.hidden_dropout_prob = hidden_dropout_prob\n-        self.attention_probs_dropout_prob = attention_probs_dropout_prob\n-        self.max_position_embeddings = max_position_embeddings\n-        self.type_vocab_size = type_vocab_size\n-        self.type_sequence_label_size = type_sequence_label_size\n-        self.initializer_range = initializer_range\n-        self.num_labels = num_labels\n-        self.num_choices = num_choices\n-        self.scope = scope\n-        self.range_bbox = range_bbox\n-\n-    def prepare_config_and_inputs(self):\n-        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n-\n-        # convert bbox to numpy since TF does not support item assignment\n-        bbox = ids_tensor([self.batch_size, self.seq_length, 4], self.range_bbox).numpy()\n-        # Ensure that bbox is legal\n-        for i in range(bbox.shape[0]):\n-            for j in range(bbox.shape[1]):\n-                if bbox[i, j, 3] < bbox[i, j, 1]:\n-                    t = bbox[i, j, 3]\n-                    bbox[i, j, 3] = bbox[i, j, 1]\n-                    bbox[i, j, 1] = t\n-                if bbox[i, j, 2] < bbox[i, j, 0]:\n-                    t = bbox[i, j, 2]\n-                    bbox[i, j, 2] = bbox[i, j, 0]\n-                    bbox[i, j, 0] = t\n-        bbox = tf.convert_to_tensor(bbox)\n-\n-        input_mask = None\n-        if self.use_input_mask:\n-            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n-\n-        token_type_ids = None\n-        if self.use_token_type_ids:\n-            token_type_ids = ids_tensor([self.batch_size, self.seq_length], self.type_vocab_size)\n-\n-        sequence_labels = None\n-        token_labels = None\n-        choice_labels = None\n-        if self.use_labels:\n-            sequence_labels = ids_tensor([self.batch_size], self.type_sequence_label_size)\n-            token_labels = ids_tensor([self.batch_size, self.seq_length], self.num_labels)\n-            choice_labels = ids_tensor([self.batch_size], self.num_choices)\n-\n-        config = LayoutLMConfig(\n-            vocab_size=self.vocab_size,\n-            hidden_size=self.hidden_size,\n-            num_hidden_layers=self.num_hidden_layers,\n-            num_attention_heads=self.num_attention_heads,\n-            intermediate_size=self.intermediate_size,\n-            hidden_act=self.hidden_act,\n-            hidden_dropout_prob=self.hidden_dropout_prob,\n-            attention_probs_dropout_prob=self.attention_probs_dropout_prob,\n-            max_position_embeddings=self.max_position_embeddings,\n-            type_vocab_size=self.type_vocab_size,\n-            initializer_range=self.initializer_range,\n-        )\n-\n-        return config, input_ids, bbox, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-\n-    def create_and_check_model(\n-        self, config, input_ids, bbox, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFLayoutLMModel(config=config)\n-\n-        result = model(input_ids, bbox, attention_mask=input_mask, token_type_ids=token_type_ids)\n-        result = model(input_ids, bbox, token_type_ids=token_type_ids)\n-        result = model(input_ids, bbox)\n-        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n-        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n-\n-    def create_and_check_for_masked_lm(\n-        self, config, input_ids, bbox, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFLayoutLMForMaskedLM(config=config)\n-\n-        result = model(input_ids, bbox, attention_mask=input_mask, token_type_ids=token_type_ids, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.vocab_size))\n-\n-    def create_and_check_for_sequence_classification(\n-        self, config, input_ids, bbox, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFLayoutLMForSequenceClassification(config=config)\n-\n-        result = model(input_ids, bbox, attention_mask=input_mask, token_type_ids=token_type_ids)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.num_labels))\n-\n-    def create_and_check_for_token_classification(\n-        self, config, input_ids, bbox, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        config.num_labels = self.num_labels\n-        model = TFLayoutLMForTokenClassification(config=config)\n-\n-        result = model(input_ids, bbox, attention_mask=input_mask, token_type_ids=token_type_ids, labels=token_labels)\n-        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.seq_length, self.num_labels))\n-\n-    def create_and_check_for_question_answering(\n-        self, config, input_ids, bbox, token_type_ids, input_mask, sequence_labels, token_labels, choice_labels\n-    ):\n-        model = TFLayoutLMForQuestionAnswering(config=config)\n-\n-        result = model(input_ids, bbox, attention_mask=input_mask, token_type_ids=token_type_ids)\n-        self.parent.assertEqual(result.start_logits.shape, (self.batch_size, self.seq_length))\n-        self.parent.assertEqual(result.end_logits.shape, (self.batch_size, self.seq_length))\n-\n-    def prepare_config_and_inputs_for_common(self):\n-        config_and_inputs = self.prepare_config_and_inputs()\n-        (\n-            config,\n-            input_ids,\n-            bbox,\n-            token_type_ids,\n-            input_mask,\n-            sequence_labels,\n-            token_labels,\n-            choice_labels,\n-        ) = config_and_inputs\n-        inputs_dict = {\n-            \"input_ids\": input_ids,\n-            \"bbox\": bbox,\n-            \"token_type_ids\": token_type_ids,\n-            \"attention_mask\": input_mask,\n-        }\n-        return config, inputs_dict\n-\n-\n-@require_tf\n-class TFLayoutLMModelTest(TFModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (\n-        (\n-            TFLayoutLMModel,\n-            TFLayoutLMForMaskedLM,\n-            TFLayoutLMForTokenClassification,\n-            TFLayoutLMForSequenceClassification,\n-            TFLayoutLMForQuestionAnswering,\n-        )\n-        if is_tf_available()\n-        else ()\n-    )\n-    pipeline_model_mapping = (\n-        {\n-            \"feature-extraction\": TFLayoutLMModel,\n-            \"fill-mask\": TFLayoutLMForMaskedLM,\n-            \"text-classification\": TFLayoutLMForSequenceClassification,\n-            \"token-classification\": TFLayoutLMForTokenClassification,\n-            \"zero-shot\": TFLayoutLMForSequenceClassification,\n-        }\n-        if is_tf_available()\n-        else {}\n-    )\n-    test_head_masking = False\n-    test_onnx = True\n-    onnx_min_opset = 10\n-\n-    def setUp(self):\n-        self.model_tester = TFLayoutLMModelTester(self)\n-        self.config_tester = ConfigTester(self, config_class=LayoutLMConfig, hidden_size=37)\n-\n-    def test_config(self):\n-        self.config_tester.run_common_tests()\n-\n-    def test_model(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_model(*config_and_inputs)\n-\n-    def test_for_masked_lm(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_masked_lm(*config_and_inputs)\n-\n-    def test_for_sequence_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_sequence_classification(*config_and_inputs)\n-\n-    def test_for_token_classification(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_token_classification(*config_and_inputs)\n-\n-    def test_for_question_answering(self):\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        self.model_tester.create_and_check_for_question_answering(*config_and_inputs)\n-\n-    @slow\n-    def test_model_from_pretrained(self):\n-        model_name = \"microsoft/layoutlm-base-uncased\"\n-        model = TFLayoutLMModel.from_pretrained(model_name)\n-        self.assertIsNotNone(model)\n-\n-    # TODO (Joao): fix me\n-    @unittest.skip(\"Onnx compliancy broke with TF 2.10\")\n-    def test_onnx_compliancy(self):\n-        pass\n-\n-\n-def prepare_layoutlm_batch_inputs():\n-    # Here we prepare a batch of 2 sequences to test a LayoutLM forward pass on:\n-    # fmt: off\n-    input_ids = tf.convert_to_tensor([[101,1019,1014,1016,1037,12849,4747,1004,14246,2278,5439,4524,5002,2930,2193,2930,4341,3208,1005,1055,2171,2848,11300,3531,102],[101,4070,4034,7020,1024,3058,1015,1013,2861,1013,6070,19274,2772,6205,27814,16147,16147,4343,2047,10283,10969,14389,1012,2338,102]])  # noqa: E231\n-    attention_mask = tf.convert_to_tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],])  # noqa: E231\n-    bbox = tf.convert_to_tensor([[[0,0,0,0],[423,237,440,251],[427,272,441,287],[419,115,437,129],[961,885,992,912],[256,38,330,58],[256,38,330,58],[336,42,353,57],[360,39,401,56],[360,39,401,56],[411,39,471,59],[479,41,528,59],[533,39,630,60],[67,113,134,131],[141,115,209,132],[68,149,133,166],[141,149,187,164],[195,148,287,165],[195,148,287,165],[195,148,287,165],[295,148,349,165],[441,149,492,166],[497,149,546,164],[64,201,125,218],[1000,1000,1000,1000]],[[0,0,0,0],[662,150,754,166],[665,199,742,211],[519,213,554,228],[519,213,554,228],[134,433,187,454],[130,467,204,480],[130,467,204,480],[130,467,204,480],[130,467,204,480],[130,467,204,480],[314,469,376,482],[504,684,582,706],[941,825,973,900],[941,825,973,900],[941,825,973,900],[941,825,973,900],[610,749,652,765],[130,659,168,672],[176,657,237,672],[238,657,312,672],[443,653,628,672],[443,653,628,672],[716,301,825,317],[1000,1000,1000,1000]]])  # noqa: E231\n-    token_type_ids = tf.convert_to_tensor([[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0]])  # noqa: E231\n-    # these are sequence labels (i.e. at the token level)\n-    labels = tf.convert_to_tensor([[-100,10,10,10,9,1,-100,7,7,-100,7,7,4,2,5,2,8,8,-100,-100,5,0,3,2,-100],[-100,12,12,12,-100,12,10,-100,-100,-100,-100,10,12,9,-100,-100,-100,10,10,10,9,12,-100,10,-100]])  # noqa: E231\n-    # fmt: on\n-\n-    return input_ids, attention_mask, bbox, token_type_ids, labels\n-\n-\n-@require_tf\n-class TFLayoutLMModelIntegrationTest(unittest.TestCase):\n-    @slow\n-    def test_forward_pass_no_head(self):\n-        model = TFLayoutLMModel.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n-\n-        input_ids, attention_mask, bbox, token_type_ids, labels = prepare_layoutlm_batch_inputs()\n-\n-        # forward pass\n-        outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids)\n-\n-        # test the sequence output on [0, :3, :3]\n-        expected_slice = tf.convert_to_tensor(\n-            [[0.1785, -0.1947, -0.0425], [-0.3254, -0.2807, 0.2553], [-0.5391, -0.3322, 0.3364]],\n-        )\n-\n-        self.assertTrue(np.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-3))\n-\n-        # test the pooled output on [1, :3]\n-        expected_slice = tf.convert_to_tensor([-0.6580, -0.0214, 0.8552])\n-\n-        self.assertTrue(np.allclose(outputs.pooler_output[1, :3], expected_slice, atol=1e-3))\n-\n-    @slow\n-    def test_forward_pass_sequence_classification(self):\n-        # initialize model with randomly initialized sequence classification head\n-        model = TFLayoutLMForSequenceClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\", num_labels=2)\n-\n-        input_ids, attention_mask, bbox, token_type_ids, _ = prepare_layoutlm_batch_inputs()\n-\n-        # forward pass\n-        outputs = model(\n-            input_ids=input_ids,\n-            bbox=bbox,\n-            attention_mask=attention_mask,\n-            token_type_ids=token_type_ids,\n-            labels=tf.convert_to_tensor([1, 1]),\n-        )\n-\n-        # test whether we get a loss as a scalar\n-        loss = outputs.loss\n-        expected_shape = (2,)\n-        self.assertEqual(loss.shape, expected_shape)\n-\n-        # test the shape of the logits\n-        logits = outputs.logits\n-        expected_shape = (2, 2)\n-        self.assertEqual(logits.shape, expected_shape)\n-\n-    @slow\n-    def test_forward_pass_token_classification(self):\n-        # initialize model with randomly initialized token classification head\n-        model = TFLayoutLMForTokenClassification.from_pretrained(\"microsoft/layoutlm-base-uncased\", num_labels=13)\n-\n-        input_ids, attention_mask, bbox, token_type_ids, labels = prepare_layoutlm_batch_inputs()\n-\n-        # forward pass\n-        outputs = model(\n-            input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids, labels=labels\n-        )\n-\n-        # test the shape of the logits\n-        logits = outputs.logits\n-        expected_shape = tf.convert_to_tensor((2, 25, 13))\n-        self.assertEqual(logits.shape, expected_shape)\n-\n-    @slow\n-    def test_forward_pass_question_answering(self):\n-        # initialize model with randomly initialized token classification head\n-        model = TFLayoutLMForQuestionAnswering.from_pretrained(\"microsoft/layoutlm-base-uncased\")\n-\n-        input_ids, attention_mask, bbox, token_type_ids, labels = prepare_layoutlm_batch_inputs()\n-\n-        # forward pass\n-        outputs = model(input_ids=input_ids, bbox=bbox, attention_mask=attention_mask, token_type_ids=token_type_ids)\n-\n-        # test the shape of the logits\n-        expected_shape = tf.convert_to_tensor((2, 25))\n-        self.assertEqual(outputs.start_logits.shape, expected_shape)\n-        self.assertEqual(outputs.end_logits.shape, expected_shape)"
        },
        {
            "sha": "5ceea057bb47aa2082ffe1aecb80ee17d7f0bd89",
            "filename": "tests/models/layoutlmv3/test_modeling_tf_layoutlmv3.py",
            "status": "removed",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_tf_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_tf_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_modeling_tf_layoutlmv3.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "e63b376d58c9d4b01c4a55e9340f66f88f777935",
            "filename": "tests/models/led/test_modeling_tf_led.py",
            "status": "removed",
            "additions": 0,
            "deletions": 342,
            "changes": 342,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fled%2Ftest_modeling_tf_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fled%2Ftest_modeling_tf_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fled%2Ftest_modeling_tf_led.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "7091dadf5826f7683d02b86ca04de38f5c95cd15",
            "filename": "tests/models/llama/test_modeling_flax_llama.py",
            "status": "removed",
            "additions": 0,
            "deletions": 259,
            "changes": 259,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fllama%2Ftest_modeling_flax_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fllama%2Ftest_modeling_flax_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama%2Ftest_modeling_flax_llama.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "19b4197943837bc16415c394d1ab30d45d9a2a74",
            "filename": "tests/models/longformer/test_modeling_tf_longformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 736,
            "changes": 736,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Flongformer%2Ftest_modeling_tf_longformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Flongformer%2Ftest_modeling_tf_longformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongformer%2Ftest_modeling_tf_longformer.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "e4f872bd43e9410fcb793ece77c5384a078e5617",
            "filename": "tests/models/longt5/test_modeling_flax_longt5.py",
            "status": "removed",
            "additions": 0,
            "deletions": 661,
            "changes": 661,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Flongt5%2Ftest_modeling_flax_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Flongt5%2Ftest_modeling_flax_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_flax_longt5.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "27fa146fb05298f6a1e7527d29167c1c9ac9e8c5",
            "filename": "tests/models/lxmert/test_modeling_tf_lxmert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 558,
            "changes": 558,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Flxmert%2Ftest_modeling_tf_lxmert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Flxmert%2Ftest_modeling_tf_lxmert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flxmert%2Ftest_modeling_tf_lxmert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "73cd48976167b4d86213a76731cf6f02711e224e",
            "filename": "tests/models/marian/test_modeling_flax_marian.py",
            "status": "removed",
            "additions": 0,
            "deletions": 488,
            "changes": 488,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmarian%2Ftest_modeling_flax_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmarian%2Ftest_modeling_flax_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_flax_marian.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "d6f0d06405420559b99742930273f4f2dc3e18a6",
            "filename": "tests/models/marian/test_modeling_tf_marian.py",
            "status": "removed",
            "additions": 0,
            "deletions": 312,
            "changes": 312,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmarian%2Ftest_modeling_tf_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmarian%2Ftest_modeling_tf_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarian%2Ftest_modeling_tf_marian.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "eaeea3f50f99086943ac2048143c99e32091e1ec",
            "filename": "tests/models/mbart/test_modeling_flax_mbart.py",
            "status": "removed",
            "additions": 0,
            "deletions": 463,
            "changes": 463,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmbart%2Ftest_modeling_flax_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmbart%2Ftest_modeling_flax_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_flax_mbart.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "1e2986f7b531ac8df3d3ba00aaa562d8be55789e",
            "filename": "tests/models/mbart/test_modeling_tf_mbart.py",
            "status": "removed",
            "additions": 0,
            "deletions": 226,
            "changes": 226,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmbart%2Ftest_modeling_tf_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmbart%2Ftest_modeling_tf_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_tf_mbart.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "1993c5836617d2bf8f84fa5dd8c912c13a1e23ca",
            "filename": "tests/models/mistral/test_modeling_flax_mistral.py",
            "status": "removed",
            "additions": 0,
            "deletions": 241,
            "changes": 241,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmistral%2Ftest_modeling_flax_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmistral%2Ftest_modeling_flax_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_flax_mistral.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "aec7c6f23f8512b7294187e18156786424114e24",
            "filename": "tests/models/mistral/test_modeling_tf_mistral.py",
            "status": "removed",
            "additions": 0,
            "deletions": 364,
            "changes": 364,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmistral%2Ftest_modeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmistral%2Ftest_modeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_tf_mistral.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "93a0ccad1fca62a23ea824d8542ac979597e1e97",
            "filename": "tests/models/mobilebert/test_modeling_tf_mobilebert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 341,
            "changes": 341,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_tf_mobilebert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_tf_mobilebert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilebert%2Ftest_modeling_tf_mobilebert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "13e3ab092ba31adb0630e4ae13e782ec1fb5b404",
            "filename": "tests/models/mobilevit/test_modeling_tf_mobilevit.py",
            "status": "removed",
            "additions": 0,
            "deletions": 424,
            "changes": 424,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_tf_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_tf_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevit%2Ftest_modeling_tf_mobilevit.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "4a9ea86588b0e7ea37b990c52551352a5261b34e",
            "filename": "tests/models/mpnet/test_modeling_tf_mpnet.py",
            "status": "removed",
            "additions": 0,
            "deletions": 275,
            "changes": 275,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmpnet%2Ftest_modeling_tf_mpnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmpnet%2Ftest_modeling_tf_mpnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmpnet%2Ftest_modeling_tf_mpnet.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "7b98d740b31249dce2b42f0a4e95403e50b3f133",
            "filename": "tests/models/mt5/test_modeling_flax_mt5.py",
            "status": "removed",
            "additions": 0,
            "deletions": 62,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmt5%2Ftest_modeling_flax_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmt5%2Ftest_modeling_flax_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_flax_mt5.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "f7b77014d409285c1d8cd81da8313086e3cf6bac",
            "filename": "tests/models/mt5/test_modeling_tf_mt5.py",
            "status": "removed",
            "additions": 0,
            "deletions": 57,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmt5%2Ftest_modeling_tf_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fmt5%2Ftest_modeling_tf_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_tf_mt5.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "44dc5a4b6300cd9a6f485d5ccf6d2a192283d4fb",
            "filename": "tests/models/openai/test_modeling_tf_openai.py",
            "status": "removed",
            "additions": 0,
            "deletions": 296,
            "changes": 296,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fopenai%2Ftest_modeling_tf_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fopenai%2Ftest_modeling_tf_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopenai%2Ftest_modeling_tf_openai.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "d922775628a3e2f805ccbc412f298a3d3e0ca086",
            "filename": "tests/models/opt/test_modeling_flax_opt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 404,
            "changes": 404,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_flax_opt.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "d36fa885c9e7ccabb2634090a8580734632afbf6",
            "filename": "tests/models/opt/test_modeling_tf_opt.py",
            "status": "removed",
            "additions": 0,
            "deletions": 405,
            "changes": 405,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fopt%2Ftest_modeling_tf_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fopt%2Ftest_modeling_tf_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fopt%2Ftest_modeling_tf_opt.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "9201bdf5d33b0f0888a9d5155b6af422062f72b7",
            "filename": "tests/models/pegasus/test_modeling_flax_pegasus.py",
            "status": "removed",
            "additions": 0,
            "deletions": 336,
            "changes": 336,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fpegasus%2Ftest_modeling_flax_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fpegasus%2Ftest_modeling_flax_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_flax_pegasus.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "61f5fec2ef511c118c9ed194ce5c44c50254246a",
            "filename": "tests/models/pegasus/test_modeling_tf_pegasus.py",
            "status": "removed",
            "additions": 0,
            "deletions": 249,
            "changes": 249,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fpegasus%2Ftest_modeling_tf_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fpegasus%2Ftest_modeling_tf_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_tf_pegasus.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "ed15cfd7b612534e5f0ee18160598814c1b7603c",
            "filename": "tests/models/rag/test_modeling_tf_rag.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1091,
            "changes": 1091,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Frag%2Ftest_modeling_tf_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Frag%2Ftest_modeling_tf_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frag%2Ftest_modeling_tf_rag.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "f7158efdfeb720a03aa3d105a97e0b83c16d18a5",
            "filename": "tests/models/regnet/test_modeling_flax_regnet.py",
            "status": "removed",
            "additions": 0,
            "deletions": 236,
            "changes": 236,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fregnet%2Ftest_modeling_flax_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fregnet%2Ftest_modeling_flax_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fregnet%2Ftest_modeling_flax_regnet.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "df017fb4161ac325a4ef642c3ac599ce0d68d50d",
            "filename": "tests/models/regnet/test_modeling_tf_regnet.py",
            "status": "removed",
            "additions": 0,
            "deletions": 288,
            "changes": 288,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fregnet%2Ftest_modeling_tf_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fregnet%2Ftest_modeling_tf_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fregnet%2Ftest_modeling_tf_regnet.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "b07d84d6074ff3883e9b4a544e643261fd038e5a",
            "filename": "tests/models/rembert/test_modeling_tf_rembert.py",
            "status": "removed",
            "additions": 0,
            "deletions": 727,
            "changes": 727,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Frembert%2Ftest_modeling_tf_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Frembert%2Ftest_modeling_tf_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frembert%2Ftest_modeling_tf_rembert.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "7399405f00c15d5dbe591bd5fe1bd0a8f284e1ca",
            "filename": "tests/models/resnet/test_modeling_flax_resnet.py",
            "status": "removed",
            "additions": 0,
            "deletions": 228,
            "changes": 228,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fresnet%2Ftest_modeling_flax_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fresnet%2Ftest_modeling_flax_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fresnet%2Ftest_modeling_flax_resnet.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "a64a3479ed723e299124892730298b7a4b515a01",
            "filename": "tests/models/resnet/test_modeling_tf_resnet.py",
            "status": "removed",
            "additions": 0,
            "deletions": 249,
            "changes": 249,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fresnet%2Ftest_modeling_tf_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fresnet%2Ftest_modeling_tf_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fresnet%2Ftest_modeling_tf_resnet.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "b9a877d2bdddad1dc3db57d4058ab003c888a911",
            "filename": "tests/models/roberta/test_modeling_flax_roberta.py",
            "status": "removed",
            "additions": 0,
            "deletions": 159,
            "changes": 159,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froberta%2Ftest_modeling_flax_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froberta%2Ftest_modeling_flax_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_flax_roberta.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "d2dbc30928104292243da7a0679d70042960bf99",
            "filename": "tests/models/roberta/test_modeling_tf_roberta.py",
            "status": "removed",
            "additions": 0,
            "deletions": 700,
            "changes": 700,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froberta%2Ftest_modeling_tf_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froberta%2Ftest_modeling_tf_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_tf_roberta.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "d464e28640aee80ecc20273b5a0cb97abd279b10",
            "filename": "tests/models/roberta_prelayernorm/test_modeling_flax_roberta_prelayernorm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 192,
            "changes": 192,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_flax_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_flax_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_flax_roberta_prelayernorm.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "835a6d3e3a94592ce9ea14032a149869cbdc5665",
            "filename": "tests/models/roberta_prelayernorm/test_modeling_tf_roberta_prelayernorm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 691,
            "changes": 691,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_tf_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_tf_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_tf_roberta_prelayernorm.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "856ed906060b950a72ca472acb6c02f7a01c9373",
            "filename": "tests/models/roformer/test_modeling_flax_roformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 163,
            "changes": 163,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froformer%2Ftest_modeling_flax_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froformer%2Ftest_modeling_flax_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_flax_roformer.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "c3796724306e48f6e69ab8726e88ae304a6994ad",
            "filename": "tests/models/roformer/test_modeling_tf_roformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 430,
            "changes": 430,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froformer%2Ftest_modeling_tf_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Froformer%2Ftest_modeling_tf_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_tf_roformer.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "6b4cd75467f83c5742795e02a6b08b608ea6214b",
            "filename": "tests/models/sam/test_modeling_tf_sam.py",
            "status": "removed",
            "additions": 0,
            "deletions": 858,
            "changes": 858,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsam%2Ftest_modeling_tf_sam.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "8e61e3e0015f3286e96f4036f3cfa224fcfa25f2",
            "filename": "tests/models/segformer/test_modeling_tf_segformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 499,
            "changes": 499,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fsegformer%2Ftest_modeling_tf_segformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fsegformer%2Ftest_modeling_tf_segformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsegformer%2Ftest_modeling_tf_segformer.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "649ac3e292d7910d76c49411239213916fba1d13",
            "filename": "tests/models/speech_encoder_decoder/test_modeling_flax_speech_encoder_decoder.py",
            "status": "removed",
            "additions": 0,
            "deletions": 621,
            "changes": 621,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_flax_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_flax_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_flax_speech_encoder_decoder.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "613081a82e07502668d49d920ce6175b47e4df2a",
            "filename": "tests/models/speech_to_text/test_modeling_tf_speech_to_text.py",
            "status": "removed",
            "additions": 0,
            "deletions": 490,
            "changes": 490,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_modeling_tf_speech_to_text.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "161e5db7536008f613b9c8dcff25855497ffbb77",
            "filename": "tests/models/swiftformer/test_modeling_tf_swiftformer.py",
            "status": "removed",
            "additions": 0,
            "deletions": 270,
            "changes": 270,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_tf_swiftformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_tf_swiftformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswiftformer%2Ftest_modeling_tf_swiftformer.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "b72369480dfe0434c472223fd1edad57fb65c668",
            "filename": "tests/models/swin/test_modeling_tf_swin.py",
            "status": "removed",
            "additions": 0,
            "deletions": 405,
            "changes": 405,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fswin%2Ftest_modeling_tf_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fswin%2Ftest_modeling_tf_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin%2Ftest_modeling_tf_swin.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "d0eb9e0f5064fb6e3157ac74b57a68ae3a95f377",
            "filename": "tests/models/t5/test_modeling_flax_t5.py",
            "status": "removed",
            "additions": 0,
            "deletions": 922,
            "changes": 922,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_flax_t5.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "8dd06d6e25de93ef79ad7ceddd8650043c10d5fe",
            "filename": "tests/models/t5/test_modeling_tf_t5.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1030,
            "changes": 1030,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Ft5%2Ftest_modeling_tf_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Ft5%2Ftest_modeling_tf_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_tf_t5.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "ab00f70673ef8ae4ce03db6c5301efaac1bcaa2f",
            "filename": "tests/models/tapas/test_modeling_tf_tapas.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1072,
            "changes": 1072,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Ftapas%2Ftest_modeling_tf_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Ftapas%2Ftest_modeling_tf_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_modeling_tf_tapas.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "3c70f5a81e225d275c1259de18a49a008cc8989d",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_flax_vision_encoder_decoder.py",
            "status": "removed",
            "additions": 0,
            "deletions": 410,
            "changes": 410,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_flax_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_flax_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_flax_vision_encoder_decoder.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "0810789ce046c472aede9b40426702d966932f5d",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_tf_vision_encoder_decoder.py",
            "status": "removed",
            "additions": 0,
            "deletions": 648,
            "changes": 648,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_tf_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_tf_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_tf_vision_encoder_decoder.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "cc0a0fa212667365ba35257852bb308781c80c29",
            "filename": "tests/models/vision_text_dual_encoder/test_modeling_flax_vision_text_dual_encoder.py",
            "status": "removed",
            "additions": 0,
            "deletions": 297,
            "changes": 297,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_flax_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_flax_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_flax_vision_text_dual_encoder.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "b24a5dfb67f07b186533ce713b6ae42a68d35b74",
            "filename": "tests/models/vision_text_dual_encoder/test_modeling_tf_vision_text_dual_encoder.py",
            "status": "removed",
            "additions": 0,
            "deletions": 419,
            "changes": 419,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_tf_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_tf_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_modeling_tf_vision_text_dual_encoder.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "2e352056635d935474c0e42d85190bd3f4d18e5c",
            "filename": "tests/models/vit/test_modeling_flax_vit.py",
            "status": "removed",
            "additions": 0,
            "deletions": 190,
            "changes": 190,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvit%2Ftest_modeling_flax_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvit%2Ftest_modeling_flax_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit%2Ftest_modeling_flax_vit.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "6fa3d93d41b06b8cac73fb9251ea0d2c6172f3c0",
            "filename": "tests/models/vit/test_modeling_tf_vit.py",
            "status": "removed",
            "additions": 0,
            "deletions": 253,
            "changes": 253,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvit%2Ftest_modeling_tf_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvit%2Ftest_modeling_tf_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit%2Ftest_modeling_tf_vit.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "3154e13106836dcb0601ccf4ae2e45506ecba620",
            "filename": "tests/models/vit_mae/test_modeling_tf_vit_mae.py",
            "status": "removed",
            "additions": 0,
            "deletions": 471,
            "changes": 471,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_tf_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_tf_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_tf_vit_mae.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "aa55557691b5e76789e9fea22c42359b9be84570",
            "filename": "tests/models/wav2vec2/test_modeling_flax_wav2vec2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 633,
            "changes": 633,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_flax_wav2vec2.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "b6585baa119a65ee6b7e83c974a80a7cc2515b0f",
            "filename": "tests/models/wav2vec2/test_modeling_tf_wav2vec2.py",
            "status": "removed",
            "additions": 0,
            "deletions": 817,
            "changes": 817,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_tf_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_tf_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_modeling_tf_wav2vec2.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "ee583c87191caf1920538efc701d3347e82f39ed",
            "filename": "tests/models/whisper/test_modeling_flax_whisper.py",
            "status": "removed",
            "additions": 0,
            "deletions": 805,
            "changes": 805,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_flax_whisper.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "e978fe2fe1009a366d5bd0a5c47bf52df94d3ad2",
            "filename": "tests/models/whisper/test_modeling_tf_whisper.py",
            "status": "removed",
            "additions": 0,
            "deletions": 950,
            "changes": 950,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_tf_whisper.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "8e0d64488518b986a843070bdb4841c116d15222",
            "filename": "tests/models/xglm/test_modeling_flax_xglm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 217,
            "changes": 217,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_flax_xglm.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "1fa58bd3d3590a7fe2f53d80f0fd0e149c4a36d8",
            "filename": "tests/models/xglm/test_modeling_tf_xglm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 259,
            "changes": 259,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "b0a20ce0c6130a64e77d6781a610037a325788b1",
            "filename": "tests/models/xlm/test_modeling_tf_xlm.py",
            "status": "removed",
            "additions": 0,
            "deletions": 403,
            "changes": 403,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxlm%2Ftest_modeling_tf_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxlm%2Ftest_modeling_tf_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm%2Ftest_modeling_tf_xlm.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "17ae593b16454e72febe1f547f04098102e75589",
            "filename": "tests/models/xlm_roberta/test_modeling_flax_xlm_roberta.py",
            "status": "removed",
            "additions": 0,
            "deletions": 47,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_flax_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_flax_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_flax_xlm_roberta.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "f28038bc224c0a4991fb7011529fa8df9203e30b",
            "filename": "tests/models/xlm_roberta/test_modeling_tf_xlm_roberta.py",
            "status": "removed",
            "additions": 0,
            "deletions": 58,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_tf_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_tf_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlm_roberta%2Ftest_modeling_tf_xlm_roberta.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "e9708a5807c7c376dfc6b746bb651dccc17d558e",
            "filename": "tests/models/xlnet/test_modeling_tf_xlnet.py",
            "status": "removed",
            "additions": 0,
            "deletions": 540,
            "changes": 540,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxlnet%2Ftest_modeling_tf_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fmodels%2Fxlnet%2Ftest_modeling_tf_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxlnet%2Ftest_modeling_tf_xlnet.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "87e96268261d4ffaae4733c461a9143ff047849d",
            "filename": "tests/sagemaker/scripts/tensorflow/run_tf_dist.py",
            "status": "removed",
            "additions": 0,
            "deletions": 192,
            "changes": 192,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fsagemaker%2Fscripts%2Ftensorflow%2Frun_tf_dist.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Fsagemaker%2Fscripts%2Ftensorflow%2Frun_tf_dist.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2Fscripts%2Ftensorflow%2Frun_tf_dist.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "97aedcb8e4e66e8e38d99040897db7cab2f4ae79",
            "filename": "tests/test_modeling_flax_common.py",
            "status": "removed",
            "additions": 0,
            "deletions": 840,
            "changes": 840,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Ftest_modeling_flax_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Ftest_modeling_flax_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_flax_common.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "8b9d7dc3919aff613bcadbf6d071f06688fd9929",
            "filename": "tests/test_modeling_tf_common.py",
            "status": "removed",
            "additions": 0,
            "deletions": 1340,
            "changes": 1340,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Ftest_modeling_tf_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Ftest_modeling_tf_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_tf_common.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "7a2c516132b8e720617802aa084c33f8751b9b7f",
            "filename": "tests/utils/test_modeling_flax_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 285,
            "changes": 285,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Futils%2Ftest_modeling_flax_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Futils%2Ftest_modeling_flax_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_flax_utils.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "e15a804ef31b93ce939a5aeabc21e0315fdb99f2",
            "filename": "tests/utils/test_modeling_tf_core.py",
            "status": "removed",
            "additions": 0,
            "deletions": 403,
            "changes": 403,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Futils%2Ftest_modeling_tf_core.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Futils%2Ftest_modeling_tf_core.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_tf_core.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        },
        {
            "sha": "079b385b51cf6d03027b8fb8370a7e3757eb32ff",
            "filename": "tests/utils/test_modeling_tf_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 662,
            "changes": 662,
            "blob_url": "https://github.com/huggingface/transformers/blob/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Futils%2Ftest_modeling_tf_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/337757cbd5bcaa5e44f13acd2600346129188a24/tests%2Futils%2Ftest_modeling_tf_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_tf_utils.py?ref=337757cbd5bcaa5e44f13acd2600346129188a24"
        }
    ],
    "stats": {
        "total": 50057,
        "additions": 27,
        "deletions": 50030
    }
}