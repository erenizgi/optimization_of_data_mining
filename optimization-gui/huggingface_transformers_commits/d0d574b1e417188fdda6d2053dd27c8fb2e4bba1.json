{
    "author": "remi-or",
    "message": "Modernbert fix (#41056)\n\n* Add FA to docker\n\n* Fixed padding for mdernbert\n\n* Fixed logits and hidden states extraction in ModernBertForMultipleChoice\n\n* Added a test for ModernBertForMultipleChoice\n\n* fixes\n\n* More fixes and GREEN CI\n\n* consistency\n\n* moar consistency",
    "sha": "d0d574b1e417188fdda6d2053dd27c8fb2e4bba1",
    "files": [
        {
            "sha": "00fbe19c3a63c7932d8710b38466d6720a4d46a3",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 35,
            "deletions": 5,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0d574b1e417188fdda6d2053dd27c8fb2e4bba1/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0d574b1e417188fdda6d2053dd27c8fb2e4bba1/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=d0d574b1e417188fdda6d2053dd27c8fb2e4bba1",
            "patch": "@@ -893,6 +893,15 @@ def forward(\n                     _pad_modernbert_output(inputs=hs, indices=indices, batch=batch_size, seqlen=seq_len)\n                     for hs in all_hidden_states\n                 )\n+        # If the attention implementation is FA2 and there is no need for repadding, there might still be the batch\n+        # dimension missing\n+        elif (\n+            self.config._attn_implementation == \"flash_attention_2\"\n+            and all_hidden_states is not None\n+            and all_hidden_states[-1].dim() == 2\n+        ):\n+            hidden_states = hidden_states.unsqueeze(0)\n+            all_hidden_states = tuple(hs.unsqueeze(0) for hs in all_hidden_states)\n \n         if not return_dict:\n             return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n@@ -1075,8 +1084,19 @@ def forward(\n             loss = self.loss_function(logits, labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if self.config._attn_implementation == \"flash_attention_2\":\n+            # Logits padding\n             with nullcontext() if self.config.repad_logits_with_grad or labels is None else torch.no_grad():\n                 logits = _pad_modernbert_output(inputs=logits, indices=indices, batch=batch_size, seqlen=seq_len)\n+            # Hidden states padding\n+            if getattr(outputs, \"hidden_states\", None) is not None:\n+                padded_hidden_states = []\n+                for hs in outputs.hidden_states:\n+                    if hs.dim() == 3 and hs.shape[0] == 1:\n+                        hs = hs.squeeze(0)\n+                    padded_hidden_states.append(\n+                        _pad_modernbert_output(inputs=hs, indices=indices, batch=batch_size, seqlen=seq_len)\n+                    )\n+                outputs.hidden_states = tuple(padded_hidden_states)\n \n         if not return_dict:\n             output = (logits,)\n@@ -1499,14 +1519,24 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n         )\n-        last_hidden_state = outputs[0]\n+        last_hidden_state = outputs[0]  # shape (num_choices, seq_len, hidden_size)\n \n+        # If classifier_pooling is \"cls\", isolate the <cls> token\n         if self.config.classifier_pooling == \"cls\":\n-            last_hidden_state = last_hidden_state[:, 0]\n+            indices_0 = torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device)\n+            # for left or right padding, <cls> is the first non-pad token\n+            if attention_mask is not None:\n+                cls_mask = attention_mask.argmax(dim=-1).to(last_hidden_state.device)\n+            # if no pad, <cls> is the first token\n+            else:\n+                cls_mask = torch.tensor(0, dtype=torch.long, device=last_hidden_state.device)\n+            # extract the <cls> token for the logits\n+            last_hidden_state = last_hidden_state[indices_0, cls_mask]\n+\n+        # If classifier_pooling is \"mean\", pool the hidden states by averaging over the sequence length\n         elif self.config.classifier_pooling == \"mean\":\n-            last_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(\n-                dim=1, keepdim=True\n-            )\n+            num_non_pad_tokens = attention_mask.sum(dim=1, keepdim=True)\n+            last_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(dim=1) / num_non_pad_tokens\n \n         pooled_output = self.head(last_hidden_state)\n         pooled_output = self.drop(pooled_output)"
        },
        {
            "sha": "5ac298f0959669c9f4201216d796df5291464e29",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 35,
            "deletions": 5,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0d574b1e417188fdda6d2053dd27c8fb2e4bba1/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0d574b1e417188fdda6d2053dd27c8fb2e4bba1/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=d0d574b1e417188fdda6d2053dd27c8fb2e4bba1",
            "patch": "@@ -1018,6 +1018,15 @@ def forward(\n                     _pad_modernbert_output(inputs=hs, indices=indices, batch=batch_size, seqlen=seq_len)\n                     for hs in all_hidden_states\n                 )\n+        # If the attention implementation is FA2 and there is no need for repadding, there might still be the batch\n+        # dimension missing\n+        elif (\n+            self.config._attn_implementation == \"flash_attention_2\"\n+            and all_hidden_states is not None\n+            and all_hidden_states[-1].dim() == 2\n+        ):\n+            hidden_states = hidden_states.unsqueeze(0)\n+            all_hidden_states = tuple(hs.unsqueeze(0) for hs in all_hidden_states)\n \n         if not return_dict:\n             return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)\n@@ -1200,8 +1209,19 @@ def forward(\n             loss = self.loss_function(logits, labels, vocab_size=self.config.vocab_size, **kwargs)\n \n         if self.config._attn_implementation == \"flash_attention_2\":\n+            # Logits padding\n             with nullcontext() if self.config.repad_logits_with_grad or labels is None else torch.no_grad():\n                 logits = _pad_modernbert_output(inputs=logits, indices=indices, batch=batch_size, seqlen=seq_len)\n+            # Hidden states padding\n+            if getattr(outputs, \"hidden_states\", None) is not None:\n+                padded_hidden_states = []\n+                for hs in outputs.hidden_states:\n+                    if hs.dim() == 3 and hs.shape[0] == 1:\n+                        hs = hs.squeeze(0)\n+                    padded_hidden_states.append(\n+                        _pad_modernbert_output(inputs=hs, indices=indices, batch=batch_size, seqlen=seq_len)\n+                    )\n+                outputs.hidden_states = tuple(padded_hidden_states)\n \n         if not return_dict:\n             output = (logits,)\n@@ -1624,14 +1644,24 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n         )\n-        last_hidden_state = outputs[0]\n+        last_hidden_state = outputs[0]  # shape (num_choices, seq_len, hidden_size)\n \n+        # If classifier_pooling is \"cls\", isolate the <cls> token\n         if self.config.classifier_pooling == \"cls\":\n-            last_hidden_state = last_hidden_state[:, 0]\n+            indices_0 = torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device)\n+            # for left or right padding, <cls> is the first non-pad token\n+            if attention_mask is not None:\n+                cls_mask = attention_mask.argmax(dim=-1).to(last_hidden_state.device)\n+            # if no pad, <cls> is the first token\n+            else:\n+                cls_mask = torch.tensor(0, dtype=torch.long, device=last_hidden_state.device)\n+            # extract the <cls> token for the logits\n+            last_hidden_state = last_hidden_state[indices_0, cls_mask]\n+\n+        # If classifier_pooling is \"mean\", pool the hidden states by averaging over the sequence length\n         elif self.config.classifier_pooling == \"mean\":\n-            last_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(dim=1) / attention_mask.sum(\n-                dim=1, keepdim=True\n-            )\n+            num_non_pad_tokens = attention_mask.sum(dim=1, keepdim=True)\n+            last_hidden_state = (last_hidden_state * attention_mask.unsqueeze(-1)).sum(dim=1) / num_non_pad_tokens\n \n         pooled_output = self.head(last_hidden_state)\n         pooled_output = self.drop(pooled_output)"
        },
        {
            "sha": "4787ad8b8535f138ec080fa24faf5d1ef86b04ab",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 169,
            "deletions": 1,
            "changes": 170,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0d574b1e417188fdda6d2053dd27c8fb2e4bba1/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0d574b1e417188fdda6d2053dd27c8fb2e4bba1/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=d0d574b1e417188fdda6d2053dd27c8fb2e4bba1",
            "patch": "@@ -11,6 +11,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import copy\n import json\n import os\n import tempfile\n@@ -19,7 +20,7 @@\n import pytest\n from packaging import version\n \n-from transformers import AutoTokenizer, ModernBertConfig, is_torch_available\n+from transformers import AutoTokenizer, ModernBertConfig, PreTrainedModel, is_torch_available\n from transformers.models.auto import get_values\n from transformers.testing_utils import (\n     CaptureLogger,\n@@ -409,6 +410,137 @@ def test_flash_attention_dispatches_by_default(self):\n             model = model_class(config=config)\n             self.assertTrue(model.config._attn_implementation == \"flash_attention_2\")\n \n+    # This is overloaded because the model handles padding / unpadding on its own, thus ModernBertForMultipleChoice has\n+    # a different hidden states shape when using FA2.\n+    def flash_attn_inference_equivalence(\n+        self, attn_implementation: str, padding_side: str, atol: float = 4e-2, rtol: float = 4e-2\n+    ):\n+        r\"\"\"\n+        Tests the equivalence between the eager and flash attention implementations.\n+        This test is only for inference and runs with `dtype=torch.bfloat16`.\n+        \"\"\"\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        # This flag is used to know if the test was skipped for all `self.all_model_classes` or not\n+        _has_run_at_least_one_model = False\n+\n+        for model_class in self.all_model_classes:\n+            # Custom kernel which needs the mask interface to be properly usable on these models\n+            if not model_class._supports_attention_backend and not attn_implementation.startswith(\"flash_attention\"):\n+                continue\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            # flash attention variants does not always support arbitrary headim\n+            config = self._prepare_config_headdim(config, 16)\n+\n+            # forcing the prefill size to go over sliding window size to check for SWA correctness\n+            if getattr(config, \"sliding_window\", None):\n+                config.sliding_window = 2\n+\n+            model = model_class(config)\n+            if not all(\n+                submodel._supports_flash_attn for submodel in model.modules() if isinstance(submodel, PreTrainedModel)\n+            ):\n+                continue\n+\n+            # If we end up here, at least one model class was not skipped\n+            _has_run_at_least_one_model = True\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                # Save the model so we can reload with correct attention\n+                model.save_pretrained(tmpdirname)\n+\n+                # Create first inputs without attention mask\n+                main_input = inputs_dict[model.main_input_name]\n+                # Only keep first batch sequence\n+                if isinstance(main_input, torch.Tensor):\n+                    main_input = main_input[:1]\n+                    # Fix the dtype\n+                    if torch.is_floating_point(main_input):\n+                        main_input = main_input.to(torch.bfloat16)\n+                first_inputs = {model.main_input_name: main_input, \"output_hidden_states\": True}\n+                # Some models have main input name which is different from input_ids, but require input_ids... e.g. BarkFine\n+                if model.main_input_name != \"input_ids\" and \"input_ids\" in inputs_dict:\n+                    first_inputs[\"input_ids\"] = inputs_dict[\"input_ids\"][:1]\n+                # If we have some pixel values, use them as well\n+                if model.main_input_name != \"pixel_values\" and \"pixel_values\" in inputs_dict:\n+                    # NOTE: this fixes qwen2_5_vl/omni because test break w/ pixel values\n+                    if \"image_grid_thw\" in inputs_dict:\n+                        continue\n+                    first_inputs[\"pixel_values\"] = inputs_dict[\"pixel_values\"][:1].to(torch.bfloat16)\n+                if model.config.is_encoder_decoder:\n+                    decoder_input_ids = inputs_dict.get(\"decoder_input_ids\", first_inputs.get(\"input_ids\"))\n+                    if decoder_input_ids is not None:\n+                        first_inputs[\"decoder_input_ids\"] = decoder_input_ids[:1]\n+\n+                # Create attention mask with padding\n+                dummy_attention_mask = inputs_dict.get(\"attention_mask\", None)\n+                if dummy_attention_mask is not None:\n+                    dummy_attention_mask = dummy_attention_mask[:1]\n+                    if padding_side == \"left\":\n+                        dummy_attention_mask[:, 1:] = 1\n+                        dummy_attention_mask[:, 0] = 0\n+                    else:\n+                        dummy_attention_mask[:, :-1] = 1\n+                        dummy_attention_mask[:, -1] = 0\n+\n+                # Create second inputs with attention mask and padding\n+                second_inputs = copy.deepcopy(first_inputs)\n+                if dummy_attention_mask is not None:\n+                    second_inputs[\"attention_mask\"] = dummy_attention_mask\n+                    if model.config.is_encoder_decoder:\n+                        second_inputs[\"decoder_attention_mask\"] = dummy_attention_mask\n+\n+                # Use prepare for class to account for special attributes (e.g. in QnA models)\n+                first_inputs = self._prepare_for_class(first_inputs, model_class)\n+                first_inputs = {\n+                    k: v.to(torch_device) if isinstance(v, torch.Tensor) else v for k, v in first_inputs.items()\n+                }\n+                second_inputs = self._prepare_for_class(second_inputs, model_class)\n+                second_inputs = {\n+                    k: v.to(torch_device) if isinstance(v, torch.Tensor) else v for k, v in second_inputs.items()\n+                }\n+\n+                model = model_class.from_pretrained(\n+                    tmpdirname, dtype=torch.bfloat16, attn_implementation=\"eager\", device_map=torch_device\n+                )\n+\n+                # First run without attention mask\n+                outputs = model(**first_inputs)\n+                retrieve_logits = model_class == ModernBertForMultipleChoice\n+                logits_1_eager = outputs.logits if retrieve_logits else outputs.hidden_states[-1]\n+                # Second run with attention mask and padding\n+                outputs = model(**second_inputs)\n+                logits_2_eager = outputs.logits if retrieve_logits else outputs.hidden_states[-1]\n+\n+                # Switch to FA\n+                del model\n+                model = model_class.from_pretrained(\n+                    tmpdirname, dtype=torch.bfloat16, attn_implementation=attn_implementation, device_map=torch_device\n+                )\n+                outputs = model(**first_inputs)\n+                logits_1_fa = outputs.logits if retrieve_logits else outputs.hidden_states[-1]\n+                # Second run with attention mask and padding\n+                outputs = model(**second_inputs)\n+                logits_2_fa = outputs.logits if retrieve_logits else outputs.hidden_states[-1]\n+\n+                # Check the results\n+                torch.testing.assert_close(logits_1_eager, logits_1_fa, atol=atol, rtol=rtol)\n+                if padding_side == \"left\":\n+                    torch.testing.assert_close(logits_2_eager[1:], logits_2_fa[1:], atol=atol, rtol=rtol)\n+                    # Check it can run in training mode\n+                    model.train()\n+                    _ = model(**second_inputs)\n+                else:\n+                    torch.testing.assert_close(logits_2_eager[:-1], logits_2_fa[:-1], atol=atol, rtol=rtol)\n+\n+        # In this case, the test should appear as skipped, not successful\n+        if not _has_run_at_least_one_model:\n+            self.skipTest(\n+                f\"Model architecture does not support {attn_implementation}, or setting its attention dynamically\"\n+            )\n+\n \n @require_torch\n class ModernBertModelIntegrationTest(unittest.TestCase):\n@@ -541,3 +673,39 @@ def test_export(self):\n         result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n         ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n         self.assertEqual(eg_predicted_mask, ep_predicted_mask)\n+\n+    @slow\n+    def test_inference_multiple_choice(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n+        model = (\n+            ModernBertForMultipleChoice.from_pretrained(\n+                \"netique/ModernBertForMultipleChoice\",\n+                reference_compile=False,\n+                attn_implementation=\"sdpa\",\n+            )\n+            .eval()\n+            .to(torch_device)\n+        )\n+\n+        prompt = \"In Italy, pizza served in formal settings, such as at a restaurant, is presented unsliced.\"\n+        choices = [\n+            \"It is eaten with a fork and a knife.\",\n+            \"It is eaten while held in the hand.\",\n+            \"It also walks on the sidewalks.\",\n+            \"It is a common drink.\",\n+        ]\n+        labels = torch.tensor([0], device=torch_device)\n+\n+        encoding = tokenizer([prompt for _ in choices], choices, return_tensors=\"pt\", padding=True)\n+        outputs = model(**{k: v.unsqueeze(0).to(torch_device) for k, v in encoding.items()}, labels=labels)\n+\n+        expected_logits = torch.tensor([[0.1973, 0.2041, 0.1835, 0.1896]])\n+        logits = outputs.logits.to(\"cpu\")\n+\n+        self.assertTrue(\n+            torch.allclose(logits, expected_logits, atol=1e-4, rtol=1e-4),\n+            f\"Logits: {logits.tolist()}\\nExpected: {expected_logits.tolist()}\",\n+        )"
        }
    ],
    "stats": {
        "total": 250,
        "additions": 239,
        "deletions": 11
    }
}