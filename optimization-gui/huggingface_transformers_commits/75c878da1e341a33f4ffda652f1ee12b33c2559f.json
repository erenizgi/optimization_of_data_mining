{
    "author": "ydshieh",
    "message": "Update daily ci to use new cluster (#33627)\n\n* update\r\n\r\n* re-enable daily CI\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "75c878da1e341a33f4ffda652f1ee12b33c2559f",
    "files": [
        {
            "sha": "001e2c531d9bc894b6628656780ab230adb5c162",
            "filename": ".github/workflows/model_jobs.yml",
            "status": "modified",
            "additions": 27,
            "deletions": 9,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/75c878da1e341a33f4ffda652f1ee12b33c2559f/.github%2Fworkflows%2Fmodel_jobs.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/75c878da1e341a33f4ffda652f1ee12b33c2559f/.github%2Fworkflows%2Fmodel_jobs.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fmodel_jobs.yml?ref=75c878da1e341a33f4ffda652f1ee12b33c2559f",
            "patch": "@@ -41,7 +41,8 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(inputs.folder_slices)[inputs.slice_id] }}\n-    runs-on: ['${{ inputs.machine_type }}', nvidia-gpu, t4, '${{ inputs.runner }}']\n+    runs-on:\n+      group: '${{ inputs.machine_type }}'\n     container:\n       image: ${{ inputs.docker }}\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -97,25 +98,42 @@ jobs:\n         working-directory: /transformers\n         run: pip freeze\n \n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          echo \"${{ inputs.machine_type }}\"\n+\n+          if [ \"${{ inputs.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ inputs.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ inputs.machine_type }}\n+          fi\n+\n+          echo \"$machine_type\"\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+\n       - name: Run all tests on GPU\n         working-directory: /transformers\n-        run: python3 -m pytest -rsfE -v --make-reports=${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n+        run: python3 -m pytest -rsfE -v --make-reports=${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n+        run: cat /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n \n       - name: Run test\n         shell: bash\n         run: |\n-          mkdir -p /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n-          echo \"hello\" > /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/hello.txt\n-          echo \"${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\"\n+          mkdir -p /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n+          echo \"hello\" > /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports/hello.txt\n+          echo \"${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\"\n \n-      - name: \"Test suite reports artifacts: ${{ inputs.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ inputs.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ inputs.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports\n+          name: ${{ env.machine_type }}_run_models_gpu_${{ env.matrix_folders }}_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_run_models_gpu_${{ matrix.folders }}_test_reports"
        },
        {
            "sha": "75ea3bb24bc7fa2acda33e001780bafc26bc5f0b",
            "filename": ".github/workflows/self-scheduled-caller.yml",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/75c878da1e341a33f4ffda652f1ee12b33c2559f/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/75c878da1e341a33f4ffda652f1ee12b33c2559f/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-caller.yml?ref=75c878da1e341a33f4ffda652f1ee12b33c2559f",
            "patch": "@@ -2,6 +2,9 @@ name: Self-hosted runner (scheduled)\n \n \n on:\n+  repository_dispatch:\n+  schedule:\n+    - cron: \"17 2 * * *\"\n   push:\n     branches:\n       - run_scheduled_ci*"
        },
        {
            "sha": "1a6f4a485430d41caadfccfadcfc92e35276bc3d",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 129,
            "deletions": 38,
            "changes": 167,
            "blob_url": "https://github.com/huggingface/transformers/blob/75c878da1e341a33f4ffda652f1ee12b33c2559f/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/75c878da1e341a33f4ffda652f1ee12b33c2559f/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=75c878da1e341a33f4ffda652f1ee12b33c2559f",
            "patch": "@@ -50,8 +50,9 @@ jobs:\n     name: Setup\n     strategy:\n       matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, '${{ inputs.runner }}']\n+        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-all-latest-gpu\n       options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -102,7 +103,7 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [single-gpu, multi-gpu]\n+        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n         slice_id: ${{ fromJSON(needs.setup.outputs.slice_ids) }}\n     uses: ./.github/workflows/model_jobs.yml\n     with:\n@@ -119,8 +120,9 @@ jobs:\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, '${{ inputs.runner }}']\n+        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-pytorch-gpu\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -146,31 +148,49 @@ jobs:\n         working-directory: /transformers\n         run: pip freeze\n \n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          echo \"${{ matrix.machine_type }}\"\n+\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+\n+          echo \"$machine_type\"\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+\n       - name: Run all pipeline tests on GPU\n         working-directory: /transformers\n         run: |\n-          python3 -m pytest -n 1 -v --dist=loadfile --make-reports=${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports tests/pipelines\n+          python3 -m pytest -n 1 -v --dist=loadfile --make-reports=${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports tests/pipelines\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports/failures_short.txt\n+        run: cat /transformers/reports/${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports/failures_short.txt\n \n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\n+          name: ${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports\n \n   run_pipelines_tf_gpu:\n     if: ${{ inputs.job == 'run_pipelines_tf_gpu' }}\n     name: TensorFlow pipelines\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, '${{ inputs.runner }}']\n+        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-tensorflow-gpu\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -197,31 +217,49 @@ jobs:\n         working-directory: /transformers\n         run: pip freeze\n \n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          echo \"${{ matrix.machine_type }}\"\n+\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+\n+          echo \"$machine_type\"\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+\n       - name: Run all pipeline tests on GPU\n         working-directory: /transformers\n         run: |\n-          python3 -m pytest -n 1 -v --dist=loadfile --make-reports=${{ matrix.machine_type }}_run_pipelines_tf_gpu_test_reports tests/pipelines\n+          python3 -m pytest -n 1 -v --dist=loadfile --make-reports=${{ env.machine_type }}_run_pipelines_tf_gpu_test_reports tests/pipelines\n \n       - name: Failure short reports\n         if: ${{ always() }}\n         run: |\n-          cat /transformers/reports/${{ matrix.machine_type }}_run_pipelines_tf_gpu_test_reports/failures_short.txt\n+          cat /transformers/reports/${{ env.machine_type }}_run_pipelines_tf_gpu_test_reports/failures_short.txt\n \n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_pipelines_tf_gpu_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_pipelines_tf_gpu_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ matrix.machine_type }}_run_pipelines_tf_gpu_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_pipelines_tf_gpu_test_reports\n+          name: ${{ env.machine_type }}_run_pipelines_tf_gpu_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_run_pipelines_tf_gpu_test_reports\n \n   run_examples_gpu:\n     if: ${{ inputs.job == 'run_examples_gpu' }}\n     name: Examples directory\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [single-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, '${{ inputs.runner }}']\n+        machine_type: [aws-g4dn-2xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-all-latest-gpu\n       options: --gpus 0 --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -247,32 +285,50 @@ jobs:\n         working-directory: /transformers\n         run: pip freeze\n \n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          echo \"${{ matrix.machine_type }}\"\n+\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+\n+          echo \"$machine_type\"\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+\n       - name: Run examples tests on GPU\n         working-directory: /transformers\n         run: |\n           pip install -r examples/pytorch/_tests_requirements.txt\n-          python3 -m pytest -v --make-reports=${{ matrix.machine_type }}_run_examples_gpu_test_reports examples/pytorch\n+          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_examples_gpu_test_reports examples/pytorch\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_examples_gpu_test_reports/failures_short.txt\n+        run: cat /transformers/reports/${{ env.machine_type }}_run_examples_gpu_test_reports/failures_short.txt\n \n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_examples_gpu_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_examples_gpu_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ matrix.machine_type }}_run_examples_gpu_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_examples_gpu_test_reports\n+          name: ${{ env.machine_type }}_run_examples_gpu_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_run_examples_gpu_test_reports\n \n   run_torch_cuda_extensions_gpu:\n     if: ${{ inputs.job == 'run_torch_cuda_extensions_gpu' }}\n     name: Torch CUDA extension tests\n     strategy:\n       fail-fast: false\n       matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, '${{ inputs.runner }}']\n+        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n     container:\n       image: ${{ inputs.docker }}\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -326,22 +382,39 @@ jobs:\n         working-directory: ${{ inputs.working-directory-prefix }}/transformers\n         run: pip freeze\n \n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          echo \"${{ matrix.machine_type }}\"\n+\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+\n+          echo \"$machine_type\"\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+\n       - name: Run all tests on GPU\n         working-directory: ${{ inputs.working-directory-prefix }}/transformers\n         run: |\n-          python3 -m pytest -v --make-reports=${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n+          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat ${{ inputs.working-directory-prefix }}/transformers/reports/${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n+        run: cat ${{ inputs.working-directory-prefix }}/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n \n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-          path: ${{ inputs.working-directory-prefix }}/transformers/reports/${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n+          name: ${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n+          path: ${{ inputs.working-directory-prefix }}/transformers/reports/${{ env.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n \n   run_quantization_torch_gpu:\n     if: ${{ inputs.job == 'run_quantization_torch_gpu' }}\n@@ -352,8 +425,9 @@ jobs:\n       fail-fast: false\n       matrix:\n         folders: ${{ fromJson(needs.setup.outputs.quantization_matrix) }}\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', nvidia-gpu, t4, '${{ inputs.runner }}']\n+        machine_type: [aws-g4dn-2xlarge-cache, aws-g4dn-12xlarge-cache]\n+    runs-on:\n+      group: '${{ matrix.machine_type }}'\n     container:\n       image: huggingface/transformers-quantization-latest-gpu\n       options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n@@ -388,22 +462,39 @@ jobs:\n         working-directory: /transformers\n         run: pip freeze\n \n+      - name: Set `machine_type` for report and artifact names\n+        working-directory: /transformers\n+        shell: bash\n+        run: |\n+          echo \"${{ matrix.machine_type }}\"\n+\n+          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-2xlarge-cache\" ]; then\n+            machine_type=single-gpu\n+          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n+            machine_type=multi-gpu\n+          else\n+            machine_type=${{ matrix.machine_type }}\n+          fi\n+\n+          echo \"$machine_type\"\n+          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n+\n       - name: Run quantization tests on GPU\n         working-directory: /transformers\n         run: |\n-          python3 -m pytest -v --make-reports=${{ matrix.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n+          python3 -m pytest -v --make-reports=${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports tests/${{ matrix.folders }}\n \n       - name: Failure short reports\n         if: ${{ failure() }}\n         continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n+        run: cat /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports/failures_short.txt\n \n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\"\n+      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\"\n         if: ${{ always() }}\n         uses: actions/upload-artifact@v4\n         with:\n-          name: ${{ matrix.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports\n+          name: ${{ env.machine_type }}_run_quantization_torch_gpu_${{ env.matrix_folders }}_test_reports\n+          path: /transformers/reports/${{ env.machine_type }}_run_quantization_torch_gpu_${{ matrix.folders }}_test_reports\n \n   run_extract_warnings:\n     # Let's only do this for the job `run_models_gpu` to simplify the (already complex) logic."
        }
    ],
    "stats": {
        "total": 206,
        "additions": 159,
        "deletions": 47
    }
}