{
    "author": "fabxoe",
    "message": "ğŸŒ [i18n-KO] Translated `model_doc/deberta.md` to Korean (#33967)\n\n* docs: ko: model_doc/deberta.md\r\n\r\n* feat: nmt draft\r\n\r\n* fix: resolve suggestions\r\n\r\nCo-authored-by: Chaewon Song <chaewon1019@ewhain.net>\r\n\r\n* fix: resolve suggestions\r\n\r\n* fix: resolve suggestions\r\n\r\n---------\r\n\r\nCo-authored-by: Chaewon Song <chaewon1019@ewhain.net>",
    "sha": "13929a0ec686988b56a9298b8035897215f4d2d7",
    "files": [
        {
            "sha": "1bae938cedd9fe9bb1c741b384424998278f7d0e",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/13929a0ec686988b56a9298b8035897215f4d2d7/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/13929a0ec686988b56a9298b8035897215f4d2d7/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=13929a0ec686988b56a9298b8035897215f4d2d7",
            "patch": "@@ -366,8 +366,8 @@\n         title: (ë²ˆì—­ì¤‘) CTRL\n       - local: model_doc/dbrx\n         title: DBRX\n-      - local: in_translation\n-        title: (ë²ˆì—­ì¤‘) DeBERTa\n+      - local: model_doc/deberta\n+        title: DeBERTa\n       - local: model_doc/deberta-v2\n         title: DeBERTa-v2\n       - local: in_translation"
        },
        {
            "sha": "6f82bed033a0e063b80210a71325741868fdc994",
            "filename": "docs/source/ko/model_doc/deberta.md",
            "status": "added",
            "additions": 152,
            "deletions": 0,
            "changes": 152,
            "blob_url": "https://github.com/huggingface/transformers/blob/13929a0ec686988b56a9298b8035897215f4d2d7/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/13929a0ec686988b56a9298b8035897215f4d2d7/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fdeberta.md?ref=13929a0ec686988b56a9298b8035897215f4d2d7",
            "patch": "@@ -0,0 +1,152 @@\n+<!--Copyright 2020 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# DeBERTa[[deberta]]\n+\n+## ê°œìš”[[overview]]\n+\n+\n+DeBERTa ëª¨ë¸ì€ Pengcheng He, Xiaodong Liu, Jianfeng Gao, Weizhu Chenì´ ì‘ì„±í•œ [DeBERTa: ë¶„ë¦¬ëœ ì–´í…ì…˜ì„ í™œìš©í•œ ë””ì½”ë”© ê°•í™” BERT](https://arxiv.org/abs/2006.03654)ì´ë¼ëŠ” ë…¼ë¬¸ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ì´ ëª¨ë¸ì€ 2018ë…„ Googleì´ ë°œí‘œí•œ BERT ëª¨ë¸ê³¼ 2019ë…„ Facebookì´ ë°œí‘œí•œ RoBERTa ëª¨ë¸ì„ ê¸°ë°˜ìœ¼ë¡œ í•©ë‹ˆë‹¤.\n+DeBERTaëŠ” RoBERTaì—ì„œ ì‚¬ìš©ëœ ë°ì´í„°ì˜ ì ˆë°˜ë§Œì„ ì‚¬ìš©í•˜ì—¬ ë¶„ë¦¬ëœ(disentangled) ì–´í…ì…˜ê³¼ í–¥ìƒëœ ë§ˆìŠ¤í¬ ë””ì½”ë” í•™ìŠµì„ í†µí•´ RoBERTaë¥¼ ê°œì„ í–ˆìŠµë‹ˆë‹¤.\n+\n+ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+*ì‚¬ì „ í•™ìŠµëœ ì‹ ê²½ë§ ì–¸ì–´ ëª¨ë¸ì˜ ìµœê·¼ ë°œì „ì€ ë§ì€ ìì—°ì–´ ì²˜ë¦¬(NLP) ì‘ì—…ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œì¼°ìŠµë‹ˆë‹¤. ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ë‘ ê°€ì§€ ìƒˆë¡œìš´ ê¸°ìˆ ì„ ì‚¬ìš©í•˜ì—¬ BERTì™€ RoBERTa ëª¨ë¸ì„ ê°œì„ í•œ ìƒˆë¡œìš´ ëª¨ë¸ êµ¬ì¡°ì¸ DeBERTaë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ì²« ë²ˆì§¸ëŠ” ë¶„ë¦¬ëœ ì–´í…ì…˜ ë©”ì»¤ë‹ˆì¦˜ìœ¼ë¡œ, ê° ë‹¨ì–´ê°€ ë‚´ìš©ê³¼ ìœ„ì¹˜ë¥¼ ê°ê° ì¸ì½”ë”©í•˜ëŠ” ë‘ ê°œì˜ ë²¡í„°ë¡œ í‘œí˜„ë˜ë©°, ë‹¨ì–´ë“¤ ê°„ì˜ ì–´í…ì…˜ ê°€ì¤‘ì¹˜ëŠ” ë‚´ìš©ê³¼ ìƒëŒ€ì  ìœ„ì¹˜ì— ëŒ€í•œ ë¶„ë¦¬ëœ í–‰ë ¬ì„ ì‚¬ìš©í•˜ì—¬ ê³„ì‚°ë©ë‹ˆë‹¤. ë‘ ë²ˆì§¸ë¡œ, ëª¨ë¸ ì‚¬ì „ í•™ìŠµì„ ìœ„í•´ ë§ˆìŠ¤í‚¹ëœ í† í°ì„ ì˜ˆì¸¡í•˜ëŠ” ì¶œë ¥ ì†Œí”„íŠ¸ë§¥ìŠ¤ ì¸µì„ ëŒ€ì²´í•˜ëŠ” í–¥ìƒëœ ë§ˆìŠ¤í¬ ë””ì½”ë”ê°€ ì‚¬ìš©ë©ë‹ˆë‹¤. ìš°ë¦¬ëŠ” ì´ ë‘ ê°€ì§€ ê¸°ìˆ ì´ ëª¨ë¸ ì‚¬ì „ í•™ìŠµì˜ íš¨ìœ¨ì„±ê³¼ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì‘ì—…ì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¨ë‹¤ëŠ” ê²ƒì„ ë³´ì—¬ì¤ë‹ˆë‹¤. RoBERTa-Largeì™€ ë¹„êµí–ˆì„ ë•Œ, ì ˆë°˜ì˜ í•™ìŠµ ë°ì´í„°ë¡œ í•™ìŠµëœ DeBERTa ëª¨ë¸ì€ ê´‘ë²”ìœ„í•œ NLP ì‘ì—…ì—ì„œ ì¼ê´€ë˜ê²Œ ë” ë‚˜ì€ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ë©°, MNLIì—ì„œ +0.9%(90.2% vs 91.1%), SQuAD v2.0ì—ì„œ +2.3%(88.4% vs 90.7%), RACEì—ì„œ +3.6%(83.2% vs 86.8%)ì˜ ì„±ëŠ¥ í–¥ìƒì„ ë‹¬ì„±í–ˆìŠµë‹ˆë‹¤. DeBERTa ì½”ë“œì™€ ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì€ https://github.com/microsoft/DeBERTa ì—ì„œ ê³µê°œë  ì˜ˆì •ì…ë‹ˆë‹¤.*\n+\n+[DeBERTa](https://huggingface.co/DeBERTa) ëª¨ë¸ì˜ í…ì„œí”Œë¡œ 2.0 êµ¬í˜„ì€ [kamalkraj](https://huggingface.co/kamalkraj)ê°€ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì½”ë“œëŠ” [ì´ê³³](https://github.com/microsoft/DeBERTa)ì—ì„œ í™•ì¸í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ë¦¬ì†ŒìŠ¤[[resources]]\n+\n+\n+DeBERTaë¥¼ ì‹œì‘í•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ” Hugging Faceì™€ community ìë£Œ ëª©ë¡(ğŸŒë¡œ í‘œì‹œë¨) ì…ë‹ˆë‹¤. ì—¬ê¸°ì— í¬í•¨ë  ìë£Œë¥¼ ì œì¶œí•˜ê³  ì‹¶ìœ¼ì‹œë‹¤ë©´ PR(Pull Request)ë¥¼ ì—´ì–´ì£¼ì„¸ìš”. ë¦¬ë·°í•´ ë“œë¦¬ê² ìŠµë‹ˆë‹¤! ìë£ŒëŠ” ê¸°ì¡´ ìë£Œë¥¼ ë³µì œí•˜ëŠ” ëŒ€ì‹  ìƒˆë¡œìš´ ë‚´ìš©ì„ ë‹´ê³  ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n+\n+\n+<PipelineTag pipeline=\"text-classification\"/>\n+\n+- DeBERTaì™€ [DeepSpeedë¥¼ ì´ìš©í•´ì„œ ëŒ€í˜• ëª¨ë¸ í•™ìŠµì„ ê°€ì†ì‹œí‚¤ëŠ”](https://huggingface.co/blog/accelerate-deepspeed) ë°©ë²•ì— ëŒ€í•œ í¬ìŠ¤íŠ¸.\n+- DeBERTaì™€ [ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ í•œì¸µ í–¥ìƒëœ ê³ ê° ì„œë¹„ìŠ¤](https://huggingface.co/blog/supercharge-customer-service-with-machine-learning)ì— ëŒ€í•œ ë¸”ë¡œê·¸ í¬ìŠ¤íŠ¸.\n+- [`DebertaForSequenceClassification`]ëŠ” ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/text-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [`TFDebertaForSequenceClassification`]ëŠ” ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/text-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb)ì—ì„œ ì§€ì›ë©ë‹ˆë‹¤.\n+- [í…ìŠ¤íŠ¸ ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/sequence_classification)\n+\n+<PipelineTag pipeline=\"token-classification\" />\n+\n+- [`DebertaForTokenClassification`]ëŠ” ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/token-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification.ipynb)ì—ì„œ ì§€ì›í•©ë‹ˆë‹¤.\n+- [`TFDebertaForTokenClassification`]ëŠ” ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/token-classification)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/token_classification-tf.ipynb)ì—ì„œ ì§€ì›í•©ë‹ˆë‹¤.\n+- ğŸ¤— Hugging Face ì½”ìŠ¤ì˜ [í† í° ë¶„ë¥˜](https://huggingface.co/course/chapter7/2?fw=pt) ì¥.\n+- ğŸ¤— Hugging Face ì½”ìŠ¤ì˜ [BPE(Byte-Pair Encoding) í† í°í™”](https://huggingface.co/course/chapter6/5?fw=pt) ì¥.\n+- [í† í° ë¶„ë¥˜ ì‘ì—… ê°€ì´ë“œ](../tasks/token_classification)\n+\n+<PipelineTag pipeline=\"fill-mask\"/>\n+\n+- [`DebertaForMaskedLM`]ëŠ” ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/language-modeling#robertabertdistilbert-and-masked-language-modeling)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)ì—ì„œ ì§€ì›í•©ë‹ˆë‹¤.\n+- [`TFDebertaForMaskedLM`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/language-modeling#run_mlmpy)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb)ì—ì„œ ì§€ì›í•©ë‹ˆë‹¤.\n+- ğŸ¤— Hugging Face ì½”ìŠ¤ì˜ [ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë§](https://huggingface.co/course/chapter7/3?fw=pt) ì¥.\n+- [ë§ˆìŠ¤í¬ ì–¸ì–´ ëª¨ë¸ë§ ì‘ì—… ê°€ì´ë“œ](../tasks/masked_language_modeling)\n+\n+<PipelineTag pipeline=\"question-answering\"/>\n+\n+- [`DebertaForQuestionAnswering`]ì€ ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/pytorch/question-answering)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering.ipynb)ì—ì„œ ì§€ì›í•©ë‹ˆë‹¤.\n+- [`TFDebertaForQuestionAnswering`]ëŠ” ì´ [ì˜ˆì œ ìŠ¤í¬ë¦½íŠ¸](https://github.com/huggingface/transformers/tree/main/examples/tensorflow/question-answering)ì™€ [ë…¸íŠ¸ë¶](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/question_answering-tf.ipynb)ì—ì„œ ì§€ì›í•©ë‹ˆë‹¤.\n+- ğŸ¤— Hugging Face ì½”ìŠ¤ì˜ [ì§ˆì˜ì‘ë‹µ(Question answering)](https://huggingface.co/course/chapter7/7?fw=pt) ì¥.\n+- [ì§ˆì˜ì‘ë‹µ ì‘ì—… ê°€ì´ë“œ](../tasks/question_answering)\n+\n+## DebertaConfig[[transformers.DebertaConfig]]\n+\n+[[autodoc]] DebertaConfig\n+\n+## DebertaTokenizer[[transformers.DebertaTokenizer]]\n+\n+[[autodoc]] DebertaTokenizer\n+    - build_inputs_with_special_tokens\n+    - get_special_tokens_mask\n+    - create_token_type_ids_from_sequences\n+    - save_vocabulary\n+\n+## DebertaTokenizerFast[[transformers.DebertaTokenizerFast]]\n+\n+[[autodoc]] DebertaTokenizerFast\n+    - build_inputs_with_special_tokens\n+    - create_token_type_ids_from_sequences\n+\n+<frameworkcontent>\n+<pt>\n+\n+## DebertaModel[[transformers.DebertaModel]]\n+\n+[[autodoc]] DebertaModel\n+    - forward\n+\n+## DebertaPreTrainedModel[[transformers.DebertaPreTrainedModel]]\n+\n+[[autodoc]] DebertaPreTrainedModel\n+\n+## DebertaForMaskedLM[[transformers.DebertaForMaskedLM]]\n+\n+[[autodoc]] DebertaForMaskedLM\n+    - forward\n+\n+## DebertaForSequenceClassification[[transformers.DebertaForSequenceClassification]]\n+\n+[[autodoc]] DebertaForSequenceClassification\n+    - forward\n+\n+## DebertaForTokenClassification[[transformers.DebertaForTokenClassification]]\n+\n+[[autodoc]] DebertaForTokenClassification\n+    - forward\n+\n+## DebertaForQuestionAnswering[[transformers.DebertaForQuestionAnswering]]\n+\n+[[autodoc]] DebertaForQuestionAnswering\n+    - forward\n+\n+</pt>\n+<tf>\n+\n+## TFDebertaModel[[transformers.TFDebertaModel]]\n+\n+[[autodoc]] TFDebertaModel\n+    - call\n+\n+## TFDebertaPreTrainedModel[[transformers.TFDebertaPreTrainedModel]]\n+\n+[[autodoc]] TFDebertaPreTrainedModel\n+    - call\n+\n+## TFDebertaForMaskedLM[[transformers.TFDebertaForMaskedLM]]\n+\n+[[autodoc]] TFDebertaForMaskedLM\n+    - call\n+\n+## TFDebertaForSequenceClassification[[transformers.TFDebertaForSequenceClassification]]\n+\n+[[autodoc]] TFDebertaForSequenceClassification\n+    - call\n+\n+## TFDebertaForTokenClassification[[transformers.TFDebertaForTokenClassification]]\n+\n+[[autodoc]] TFDebertaForTokenClassification\n+    - call\n+\n+## TFDebertaForQuestionAnswering[[transformers.TFDebertaForQuestionAnswering]]\n+\n+[[autodoc]] TFDebertaForQuestionAnswering\n+    - call\n+\n+</tf>\n+</frameworkcontent>\n+"
        }
    ],
    "stats": {
        "total": 156,
        "additions": 154,
        "deletions": 2
    }
}