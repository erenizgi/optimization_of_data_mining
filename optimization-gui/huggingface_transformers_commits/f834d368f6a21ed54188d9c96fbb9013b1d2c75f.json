{
    "author": "zucchini-nlp",
    "message": "[gemma3] fix bidirectional attention mask (#38080)\n\n* fix attn mask\n\n* attn viz doesn't show yello cubes between images\n\n* bucketize made it hard with different number of crops\n\n* fixup",
    "sha": "f834d368f6a21ed54188d9c96fbb9013b1d2c75f",
    "files": [
        {
            "sha": "9aeba5f18aa7986b564767569c4074cb98af3467",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/f834d368f6a21ed54188d9c96fbb9013b1d2c75f/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f834d368f6a21ed54188d9c96fbb9013b1d2c75f/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=f834d368f6a21ed54188d9c96fbb9013b1d2c75f",
            "patch": "@@ -1062,10 +1062,21 @@ def _update_causal_mask(\n         if token_type_ids is not None and sequence_length != 1:\n             token_type_mask = token_type_ids.unsqueeze(1) == token_type_ids.unsqueeze(2)\n             token_type_mask[token_type_ids == 0] = False  # if text token do not change anything\n-            token_type_mask = token_type_mask.unsqueeze(1).to(causal_mask.device, dtype=torch.bool)\n+\n+            # Find where a new image block starts: 1 if image and previous not image\n+            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+            is_image = token_type_ids == 1\n+            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n+            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n+            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+\n+            same_image_mask = image_group_ids.unsqueeze(1) == image_group_ids.unsqueeze(2)\n+            same_image_mask[image_group_ids == -1] = False  # remove non-image\n+            image_mask = (token_type_mask & same_image_mask).unsqueeze(1).to(causal_mask.device, dtype=torch.bool)\n+\n             causal_mask = causal_mask.clone()\n             causal_mask[:, :, :, :sequence_length] = causal_mask[:, :, :, :sequence_length].masked_fill(\n-                token_type_mask, 0.0\n+                image_mask, 0.0\n             )\n \n         if attention_mask is not None:"
        },
        {
            "sha": "495fe167d79cedad7d1982c34d5ea08538cca399",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 2,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/f834d368f6a21ed54188d9c96fbb9013b1d2c75f/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f834d368f6a21ed54188d9c96fbb9013b1d2c75f/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=f834d368f6a21ed54188d9c96fbb9013b1d2c75f",
            "patch": "@@ -781,10 +781,21 @@ def _update_causal_mask(\n         if token_type_ids is not None and sequence_length != 1:\n             token_type_mask = token_type_ids.unsqueeze(1) == token_type_ids.unsqueeze(2)\n             token_type_mask[token_type_ids == 0] = False  # if text token do not change anything\n-            token_type_mask = token_type_mask.unsqueeze(1).to(causal_mask.device, dtype=torch.bool)\n+\n+            # Find where a new image block starts: 1 if image and previous not image\n+            # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+            is_image = token_type_ids == 1\n+            new_image_start = is_image & ~nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n+            image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n+            image_group_ids = torch.where(is_image, image_group_ids, torch.full_like(token_type_ids, -1))\n+\n+            same_image_mask = image_group_ids.unsqueeze(1) == image_group_ids.unsqueeze(2)\n+            same_image_mask[image_group_ids == -1] = False  # remove non-image\n+            image_mask = (token_type_mask & same_image_mask).unsqueeze(1).to(causal_mask.device, dtype=torch.bool)\n+\n             causal_mask = causal_mask.clone()\n             causal_mask[:, :, :, :sequence_length] = causal_mask[:, :, :, :sequence_length].masked_fill(\n-                token_type_mask, 0.0\n+                image_mask, 0.0\n             )\n \n         if attention_mask is not None:"
        },
        {
            "sha": "ac52dd46ce4ee36ee85ac0f1b68be1cb9e21915c",
            "filename": "src/transformers/utils/attention_visualizer.py",
            "status": "modified",
            "additions": 18,
            "deletions": 5,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/f834d368f6a21ed54188d9c96fbb9013b1d2c75f/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f834d368f6a21ed54188d9c96fbb9013b1d2c75f/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fattention_visualizer.py?ref=f834d368f6a21ed54188d9c96fbb9013b1d2c75f",
            "patch": "@@ -36,7 +36,9 @@\n WHITE_SQUARE = \"â¬š\"\n \n \n-def generate_attention_matrix_from_mask(words, mask, img_token=\"<img>\", sliding_window=None, token_type_ids=None):\n+def generate_attention_matrix_from_mask(\n+    words, mask, img_token=\"<img>\", sliding_window=None, token_type_ids=None, image_seq_length=None\n+):\n     \"\"\"\n     Generates an attention matrix from a given attention mask.\n \n@@ -80,6 +82,14 @@ def generate_attention_matrix_from_mask(words, mask, img_token=\"<img>\", sliding_\n         for j in range(n)\n     )\n \n+    if token_type_ids is not None:\n+        is_special = token_type_ids == 1\n+        token_type_buckets = torch.where(\n+            (token_type_ids.cumsum(-1) % 5 + is_special).bool(), token_type_ids.cumsum(-1), 0\n+        )\n+        boundaries = torch.arange(0, image_seq_length + 1, image_seq_length)\n+        token_type_buckets = torch.bucketize(token_type_buckets, boundaries=boundaries)\n+\n     # Print headers\n     legend = f\"{GREEN}{BLACK_SQUARE}{RESET}: i == j (diagonal)   {YELLOW}{BLACK_SQUARE}{RESET}: token_type_ids\"\n     output.append(\" \" + legend)\n@@ -103,7 +113,6 @@ def generate_attention_matrix_from_mask(words, mask, img_token=\"<img>\", sliding_\n             if sliding_window is not None\n             else \"\"\n         )\n-\n     for i, word in enumerate(words):\n         word_repr = repr(word).ljust(max_word_length)\n         colored_word = f\"{YELLOW}{word_repr}{RESET}\" if img_token in word else word_repr\n@@ -121,7 +130,9 @@ def generate_attention_matrix_from_mask(words, mask, img_token=\"<img>\", sliding_\n         if sliding_window is not None:\n             sliding_window_row = \" \".join(\n                 f\"{YELLOW}{BLACK_SQUARE}{RESET}\"\n-                if img_token in words[j] and img_token in words[i]\n+                if img_token in words[j]\n+                and img_token in words[i]\n+                and token_type_buckets[0, i] == token_type_buckets[0, j]\n                 else f\"{GREEN}{BLACK_SQUARE}{RESET}\"\n                 if i == j\n                 else BLACK_SQUARE\n@@ -170,7 +181,8 @@ def visualize_attention_mask(self, input_sentence: str, suffix=\"\"):\n         if self.config.model_type in PROCESSOR_MAPPING_NAMES:\n             img = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg?download=true\"\n             img = Image.open(requests.get(img, stream=True).raw)\n-            processor = AutoProcessor.from_pretrained(self.repo_id, image_seq_length=5)\n+            image_seq_length = 5\n+            processor = AutoProcessor.from_pretrained(self.repo_id, image_seq_length=image_seq_length)\n             if hasattr(processor, \"image_token\"):\n                 image_token = processor.image_token\n             else:\n@@ -179,7 +191,7 @@ def visualize_attention_mask(self, input_sentence: str, suffix=\"\"):\n             if image_token:\n                 input_sentence = input_sentence.replace(\"<img>\", image_token)\n \n-            inputs = processor(img, input_sentence, suffix=suffix, return_tensors=\"pt\")\n+            inputs = processor(images=img, text=input_sentence, suffix=suffix, return_tensors=\"pt\")\n \n             self.image_token = processor.tokenizer.convert_ids_to_tokens([processor.image_token_id])[0]\n \n@@ -223,6 +235,7 @@ def visualize_attention_mask(self, input_sentence: str, suffix=\"\"):\n             img_token=self.image_token,\n             sliding_window=getattr(self.config, \"sliding_window\", None),\n             token_type_ids=kwargs.get(\"token_type_ids\", None),\n+            image_seq_length=image_seq_length,\n         )\n         print(f_string)\n         print(f\"{top_bottom_border}\")"
        }
    ],
    "stats": {
        "total": 53,
        "additions": 44,
        "deletions": 9
    }
}