{
    "author": "stevhliu",
    "message": "[docs] Format fix (#38414)\n\nfix table",
    "sha": "78d771c3c21922642fc9546ccb973cc7a182ab34",
    "files": [
        {
            "sha": "4190cefdb8a18ee319ba6d074b14087e058393ee",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/78d771c3c21922642fc9546ccb973cc7a182ab34/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/78d771c3c21922642fc9546ccb973cc7a182ab34/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=78d771c3c21922642fc9546ccb973cc7a182ab34",
            "patch": "@@ -56,10 +56,10 @@ Attention is calculated independently in each layer of the model, and caching is\n \n Refer to the table below to compare how caching improves efficiency.\n \n-| without caching | with caching |  |  |  |\n-|---|---|---|---|---|\n-| for each step, recompute all previous `K` and `V`  | for each step, only compute current `K` and `V` |  |  |  |\n-| attention cost per step is **quadratic** with sequence length | attention cost per step is **linear** with sequence length (memory grows linearly, but compute/token remains low) |  |  |  |\n+| without caching | with caching |\n+|---|---|\n+| for each step, recompute all previous `K` and `V`  | for each step, only compute current `K` and `V` \n+| attention cost per step is **quadratic** with sequence length | attention cost per step is **linear** with sequence length (memory grows linearly, but compute/token remains low) |\n \n \n "
        }
    ],
    "stats": {
        "total": 8,
        "additions": 4,
        "deletions": 4
    }
}