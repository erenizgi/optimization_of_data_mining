{
    "author": "MekkCyber",
    "message": "[Quantization] fix dequant when block size is none & static quantization (#42545)\n\n* fix\n\n* style",
    "sha": "bb09a30f5ad57fe837526b60d72981a41f08c0fd",
    "files": [
        {
            "sha": "ffaa4f3d8c19c004efc3450bbe1baa6eefaa5ca2",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb09a30f5ad57fe837526b60d72981a41f08c0fd/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb09a30f5ad57fe837526b60d72981a41f08c0fd/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=bb09a30f5ad57fe837526b60d72981a41f08c0fd",
            "patch": "@@ -686,7 +686,7 @@ def convert(\n         missing_keys=None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:\n-        if len(input_dict) != 2:\n+        if len(input_dict) < 2:\n             # in case of no scales, the weights are not quantized, so we return the weights as is\n             return {\n                 full_layer_name: input_dict[\"weight$\"][0]\n@@ -702,15 +702,18 @@ def convert(\n \n         rows, cols = quantized.shape[-2:]\n         block_size = self.hf_quantizer.quantization_config.weight_block_size\n+        if block_size is None:\n+            block_size = (quantized.shape[-2], quantized.shape[-1])\n \n         block_m, block_n = block_size\n+\n         if rows % block_m != 0 or cols % block_n != 0:\n             raise ValueError(\n                 f\"Matrix dimensions ({rows}, {cols}) must be divisible by block sizes ({block_m}, {block_n}).\"\n             )\n-\n+        quantized = quantized.to(scales.dtype)\n         reshaped = quantized.reshape(-1, rows // block_m, block_m, cols // block_n, block_n)\n-        expanded_scales = scales.to(torch.float32).reshape(-1, rows // block_m, cols // block_n)\n+        expanded_scales = scales.reshape(-1, rows // block_m, cols // block_n)\n         expanded_scales = expanded_scales.unsqueeze(-1).unsqueeze(2)\n         dequantized = reshaped * expanded_scales\n "
        },
        {
            "sha": "49acc9c5007b1d61648e7fb43bef5af0c8033299",
            "filename": "src/transformers/quantizers/quantizer_finegrained_fp8.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bb09a30f5ad57fe837526b60d72981a41f08c0fd/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bb09a30f5ad57fe837526b60d72981a41f08c0fd/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_finegrained_fp8.py?ref=bb09a30f5ad57fe837526b60d72981a41f08c0fd",
            "patch": "@@ -246,8 +246,9 @@ def get_weight_conversions(self):\n         if self.pre_quantized and self.quantization_config.dequantize:\n             return [\n                 # either use the dollar sign, or permute the source patterns to start matching against the scales first\n+                # We also collect the activation scales, they will not be used\n                 WeightConverter(\n-                    source_patterns=[\"weight$\", \"weight_scale_inv\"],\n+                    source_patterns=[\"weight$\", \"weight_scale_inv\", \"activation_scale\"],\n                     target_patterns=\"weight\",\n                     operations=[Fp8Dequantize(self)],\n                 )"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 8,
        "deletions": 4
    }
}