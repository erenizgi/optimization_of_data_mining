{
    "author": "Taha1506",
    "message": "Added error when sequence length is bigger than max_position_embeddings (#32156)\n\n* Added error when sequence length is bigger than max_position_embeddings\r\n\r\n* Fixed formatting\r\n\r\n* Fixed bug\r\n\r\n* Changed copies to match\r\n\r\n* Fixed bug\r\n\r\n* Applied suggestions\r\n\r\n* Removed redundant code\r\n\r\n* Fixed bugs\r\n\r\n* Bug fix\r\n\r\n* Bug fix\r\n\r\n* Added requested Changes\r\n\r\n* Fixed bug\r\n\r\n* Fixed unwanted change\r\n\r\n* Fixed unwanated changes\r\n\r\n* Fixed formatting",
    "sha": "0aaf124fb934c314fe28fa9d1fa935c39d69f911",
    "files": [
        {
            "sha": "154d6eb9f8225df839580a4aaafd82a16be72cfe",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=0aaf124fb934c314fe28fa9d1fa935c39d69f911",
            "patch": "@@ -309,6 +309,13 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n     ) -> torch.Tensor:\n         seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n+        max_position_embedding = self.position_embedding.weight.shape[0]\n+\n+        if seq_length > max_position_embedding:\n+            raise ValueError(\n+                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n+                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n+            )\n \n         if position_ids is None:\n             position_ids = self.position_ids[:, :seq_length]"
        },
        {
            "sha": "1818c6bb0963c0cd2e83cb609d463c3c8dd1f54b",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=0aaf124fb934c314fe28fa9d1fa935c39d69f911",
            "patch": "@@ -277,6 +277,13 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n     ) -> torch.Tensor:\n         seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n+        max_position_embedding = self.position_embedding.weight.shape[0]\n+\n+        if seq_length > max_position_embedding:\n+            raise ValueError(\n+                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n+                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n+            )\n \n         if position_ids is None:\n             position_ids = self.position_ids[:, :seq_length]"
        },
        {
            "sha": "2d88746b771370c038c306a7cfb152c780c76338",
            "filename": "src/transformers/models/clipseg/modeling_clipseg.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fmodeling_clipseg.py?ref=0aaf124fb934c314fe28fa9d1fa935c39d69f911",
            "patch": "@@ -244,6 +244,13 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n     ) -> torch.Tensor:\n         seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n+        max_position_embedding = self.position_embedding.weight.shape[0]\n+\n+        if seq_length > max_position_embedding:\n+            raise ValueError(\n+                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n+                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n+            )\n \n         if position_ids is None:\n             position_ids = self.position_ids[:, :seq_length]"
        },
        {
            "sha": "889b200552d51ab8c59369e5bdd06f6456ce8424",
            "filename": "src/transformers/models/groupvit/modeling_groupvit.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fmodeling_groupvit.py?ref=0aaf124fb934c314fe28fa9d1fa935c39d69f911",
            "patch": "@@ -446,6 +446,13 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n     ) -> torch.Tensor:\n         seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n+        max_position_embedding = self.position_embedding.weight.shape[0]\n+\n+        if seq_length > max_position_embedding:\n+            raise ValueError(\n+                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n+                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n+            )\n \n         if position_ids is None:\n             position_ids = self.position_ids[:, :seq_length]"
        },
        {
            "sha": "d8a317493a1013d5438073ace2ec9ba82dccbbc3",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=0aaf124fb934c314fe28fa9d1fa935c39d69f911",
            "patch": "@@ -340,6 +340,13 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n     ) -> torch.Tensor:\n         seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n+        max_position_embedding = self.position_embedding.weight.shape[0]\n+\n+        if seq_length > max_position_embedding:\n+            raise ValueError(\n+                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n+                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n+            )\n \n         if position_ids is None:\n             position_ids = self.position_ids[:, :seq_length]"
        },
        {
            "sha": "4cf2af1da5c0ef0c720c6b646a278c3e259f9d83",
            "filename": "src/transformers/models/x_clip/modeling_x_clip.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0aaf124fb934c314fe28fa9d1fa935c39d69f911/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fmodeling_x_clip.py?ref=0aaf124fb934c314fe28fa9d1fa935c39d69f911",
            "patch": "@@ -203,6 +203,13 @@ def forward(\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n     ) -> torch.Tensor:\n         seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n+        max_position_embedding = self.position_embedding.weight.shape[0]\n+\n+        if seq_length > max_position_embedding:\n+            raise ValueError(\n+                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n+                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n+            )\n \n         if position_ids is None:\n             position_ids = self.position_ids[:, :seq_length]"
        }
    ],
    "stats": {
        "total": 42,
        "additions": 42,
        "deletions": 0
    }
}