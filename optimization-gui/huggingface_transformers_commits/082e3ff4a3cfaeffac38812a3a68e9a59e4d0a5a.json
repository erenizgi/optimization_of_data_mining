{
    "author": "zucchini-nlp",
    "message": "Add cross links for model contribution (#42207)\n\n* add cross links\n\n* a few nits\n\n* last bit\n\n* Update CONTRIBUTING.md\n\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\n\n* Update docs/source/en/transformers_as_backend.md\n\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>",
    "sha": "082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a",
    "files": [
        {
            "sha": "c5e6a74297a319dc42b054ac684a434851030d48",
            "filename": "CONTRIBUTING.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a/CONTRIBUTING.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a/CONTRIBUTING.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/CONTRIBUTING.md?ref=082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a",
            "patch": "@@ -125,8 +125,9 @@ If you're contributing a **vision-language model** (or any multimodal model that\n All new models should use the modular architecture pattern. Create a `modular_<model_name>.py` file using the modular model converter:\n \n - Use the CLI, [`transformers add-new-model-like`](https://github.com/huggingface/transformers/blob/main/src/transformers/cli/add_new_model_like.py) to generate a modular skeleton and get started\n-- All code should be in the modular file if possible. Modeling must be in it, it's better if configuration is in it as well. \n+- All code should be in the modular file if possible. Modeling must be in it, it's better if configuration is in it as well. [Modular guide](./modular_transformers#implementing-a-modular-file) shows a quick way to set up a modular file.\n - Reuse existing patterns from similar models as much as possible\n+- You can make the model compatible with inference engines such as vLLM or SGLang, and enable zero-effort integration. See specific requirements for model implementation in [\"Transformers modeling backend\"](./transformers_as_backend#multimodal-models)\n \n To verify your modular file is correct, run:\n "
        },
        {
            "sha": "4590b4ddefbf417035a0dd0c17be38ed494907dd",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a",
            "patch": "@@ -118,7 +118,7 @@\n   - local: tools\n     title: Tools\n   - local: transformers_as_backend\n-    title: Inference server backends\n+    title: Transformers as modeling backend\n   - local: continuous_batching\n     title: Continuous Batching\n   title: Inference"
        },
        {
            "sha": "b1883f10d86a0dae3cb276011eabe80548892b32",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a",
            "patch": "@@ -1,6 +1,6 @@\n # Contributing a new model to Transformers\n \n-Modular Transformers lowers the bar for contributing models and significantly reduces the code required to add a model by allowing imports and inheritance.\n+Modular Transformers lowers the bar for contributing models and significantly reduces the code required to add a model by allowing imports and inheritance. We recommend to go through [general contribution guidelines for new models](./contributing#do-you-want-to-implement-a-new-model) before diving into the details here. \n \n One of Transformers' core design feature is the [single model, single file](https://huggingface.co/blog/transformers-design-philosophy) policy. Model components - such as attention layers - are repeated across many files and any independent implementations tend to diverge as fixes and changes are applied to specific parts of the code.\n "
        },
        {
            "sha": "984964d38fe5b04651026e20ed2f18ee64ac38c7",
            "filename": "docs/source/en/transformers_as_backend.md",
            "status": "modified",
            "additions": 8,
            "deletions": 52,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a/docs%2Fsource%2Fen%2Ftransformers_as_backend.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a/docs%2Fsource%2Fen%2Ftransformers_as_backend.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftransformers_as_backend.md?ref=082e3ff4a3cfaeffac38812a3a68e9a59e4d0a5a",
            "patch": "@@ -14,9 +14,9 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Inference server backends\n+# Transformers as modeling backend\n \n-Transformers' models are compatible with different inference servers like vLLM and SGLang. Instead of implementing a model for each inference server, you only need one model, which can be plugged into any inference server. It simplifies maintenance and makes it easy for users to use different inference servers for different use cases.\n+Transformers' models are compatible with different inference servers like vLLM and SGLang. Instead of implementing a new model architecture from scratch for each inference server, you only need a model definition in `transformers`, which can be plugged into any inference server. It simplifies maintenance and makes it easy for users to use different inference servers for different use cases.\n \n With Transformers as a backend, you can also serve any model - including custom and Hub-hosted models - without waiting for native support.\n \n@@ -157,57 +157,13 @@ class MyConfig(PreTrainedConfig):\n \n ### Multimodal models\n \n-For multimodal models, you need to include a few more changes on top of the general recommendations. These rules ensure that your model integrates properly with multimodal data.\n+For multimodal models, you need to include a few more changes on top of the general recommendations outlined in [\"contribuiting a model\"](./contributing#vision-language-model-contribution-checklist). These rules ensure that your model integrates properly and enables processing multimodal data.\n \n-1. A multimodal model requires a base `MyMultiModalModel` class to handle multimodal fusion without a language modeling head and a separate generative class that adds a head.\n+1. A multimodal model's processing class must have the `self.image_token` and `self.image_token_ids` attributes. These are placeholder tokens used to indicate image positions in the input. This placeholder token is the same token used in the input prompt to denote images and used in model code to scatter image features.\n \n-    The base model needs to implement the `get_image_features()` method to accept image pixel values and return encoded outputs. These are later merged with the language embeddings and don't require any postprocessing. The shape of the returned features must match the number of input images. If a vision encoder returns variable-length outputs (patch-based), return a list of 2D tensors of size `(image_seq_len, image_dim)` for each image.\n+2. The processing class needs `self._get_num_multimodal_tokens` method to compute the number of placeholder tokens needed for multimodal inputs with given sizes and to return a [`MultiModalData`] object. The placeholders between `<image>` tokens such as row or column tokens don't count as image placeholders. Only tokens that are actually replaced by image features later in modeling should be counted!\n \n-Expand the code below for an example.\n-\n-<details>\n-<summary>modeling_my_multimodal_model.py</summary>\n-\n-```python\n-from transformers.generation import GenerationMixin\n-\n-class MyMultimodalModel(MyMultimodalPreTrainedModel):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.language_model = AutoModel.from_config(config.text_config)\n-        self.vision_tower = AutoModel.from_config(config.vision_config)\n-        self.multimodal_projection = nn.Linear(vision_dim, text_dim)\n-    \n-    def get_image_features(self, pixel_values):\n-        return self.vision_tower(pixel_values).last_hidden_states\n-    \n-    def forward(self, input_ids, pixel_values, **kwargs):\n-        # process your inputs\n-        return MyModelOutputWithPast(\n-            last_hidden_state=last_hidden_state,\n-            image_hidden_states=image_features,\n-            [...]\n-        )\n-\n-class MyMultimodalModelForConditionalGeneration(MyMultimodalPreTrainedModel, GenerationMixin):\n-    def __init__(self, config):\n-        super().__init__(config)\n-        self.model = MyMultimodalModel(config)\n-        self.lm_head = nn.Linear(hidden_dim, vocab_size)\n-```\n-\n-</details>\n-\n-2. A multimodal model config must be nested with the following fields.\n-    * text_config: decoder language model config\n-    * vision_config: vision encoder config\n-    * image_token_id: ID of the image placeholder token used in the input to indicate image position\n-\n-3. A multimodal model's processing class must have the `self.image_token` and `self.image_token_ids` attributes. These are placeholder tokens used to indicate image positions in the input. The placeholder token is the same token used in the input prompt and to mask scatter image features.\n-\n-   The processing class also needs `self._get_num_multimodal_tokens` method to compute the number of placeholder tokens needed for multimodal inputs with given sizes and to return a [`MultiModalData`] object. The placeholder for row and column tokens don't count as image placeholders. Only the tokens that are actually replaced by image features are computed.\n-\n-Finally, when `return_mm_token_type_ids=True`, the class has to return `mm_token_type_ids` to indicate whether each position is a text token (`0`) or image placeholder token (`1`). Each image's token type IDs must be contiguous with no breaks between consecutive ones.\n+3. The processor needs to check the value of `return_mm_token_type_ids` and return `mm_token_type_ids` to indicate whether each position is a text token (`0`), image placeholder token (`1`) or video placeholder token (`2`). Each multimodal token type ID sequence must be contiguous without breaks between consecutive tokens, therefore special tokens for begin/end/row/column must be treated as placeholders.\n \n Expand the code below for an example.\n \n@@ -246,5 +202,5 @@ class MyMultimodalProcessor(ProcessorMixin):\n \n ## Resources\n \n-* Read the [Transformers backend integration in vLLM](https://blog.vllm.ai/2025/04/11/transformers-backend.html) blog post for more details about the Transformers backend in vLLM.\n-* Read the [Transformers backend integration in SGLang](https://huggingface.co/blog/transformers-backend-sglang) blog post for more details about the Transformers backend in SGLang.\n+* Read the [Transformers modeling backend integration in vLLM](https://blog.vllm.ai/2025/04/11/transformers-backend.html) blog post for more details about the Transformers modeling backend in vLLM.\n+* Read the [Transformers modeling  backend integration in SGLang](https://huggingface.co/blog/transformers-backend-sglang) blog post for more details about the Transformers modeling backend in SGLang."
        }
    ],
    "stats": {
        "total": 67,
        "additions": 12,
        "deletions": 55
    }
}