{
    "author": "sbucaille",
    "message": "ðŸš¨ðŸš¨ðŸš¨ [SuperPoint] Fix keypoint coordinate output and add post processing (#33200)\n\n* feat: Added int conversion and unwrapping\r\n\r\n* test: added tests for post_process_keypoint_detection of SuperPointImageProcessor\r\n\r\n* docs: changed docs to include post_process_keypoint_detection method and switched from opencv to matplotlib\r\n\r\n* test: changed test to not depend on SuperPointModel forward\r\n\r\n* test: added missing require_torch decorator\r\n\r\n* docs: changed pyplot parameters for the keypoints to be more visible in the example\r\n\r\n* tests: changed import torch location to make test_flax and test_tf\r\n\r\n* Revert \"tests: changed import torch location to make test_flax and test_tf\"\r\n\r\nThis reverts commit 39b32a2f69500bc7af01715fc7beae2260549afe.\r\n\r\n* tests: fixed import\r\n\r\n* chore: applied suggestions from code review\r\n\r\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\r\n\r\n* tests: fixed import\r\n\r\n* tests: fixed import (bis)\r\n\r\n* tests: fixed import (ter)\r\n\r\n* feat: added choice of type for target_size and changed tests accordingly\r\n\r\n* docs: updated code snippet to reflect the addition of target size type choice in post process method\r\n\r\n* tests: fixed imports (...)\r\n\r\n* tests: fixed imports (...)\r\n\r\n* style: formatting file\r\n\r\n* docs: fixed typo from image[0] to image.size[0]\r\n\r\n* docs: added output image and fixed some tests\r\n\r\n* Update docs/source/en/model_doc/superpoint.md\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n* fix: included SuperPointKeypointDescriptionOutput in TYPE_CHECKING if statement and changed tests results to reflect changes to SuperPoint from absolute keypoints coordinates to relative\r\n\r\n* docs: changed SuperPoint's docs to print output instead of just accessing\r\n\r\n* style: applied make style\r\n\r\n* docs: added missing output type and precision in docstring of post_process_keypoint_detection\r\n\r\n* perf: deleted loop to perform keypoint conversion in one statement\r\n\r\n* fix: moved keypoint conversion at the end of model forward\r\n\r\n* docs: changed SuperPointInterestPointDecoder to SuperPointKeypointDecoder class name and added relative (x, y) coordinates information to its method\r\n\r\n* fix: changed type hint\r\n\r\n* refactor: removed unnecessary brackets\r\n\r\n* revert: SuperPointKeypointDecoder to SuperPointInterestPointDecoder\r\n\r\n* Update docs/source/en/model_doc/superpoint.md\r\n\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\r\n\r\n---------\r\n\r\nCo-authored-by: Steven Bucaille <steven.bucaille@buawei.com>\r\nCo-authored-by: NielsRogge <48327001+NielsRogge@users.noreply.github.com>\r\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "a1835195d134f5a244aed1212342be94fa27b40c",
    "files": [
        {
            "sha": "59e451adceb8173b2577ba752ca2d2502d54d878",
            "filename": "docs/source/en/model_doc/superpoint.md",
            "status": "modified",
            "additions": 23,
            "deletions": 14,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1835195d134f5a244aed1212342be94fa27b40c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1835195d134f5a244aed1212342be94fa27b40c/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsuperpoint.md?ref=a1835195d134f5a244aed1212342be94fa27b40c",
            "patch": "@@ -86,24 +86,32 @@ model = SuperPointForKeypointDetection.from_pretrained(\"magic-leap-community/sup\n \n inputs = processor(images, return_tensors=\"pt\")\n outputs = model(**inputs)\n-\n-for i in range(len(images)):\n-    image_mask = outputs.mask[i]\n-    image_indices = torch.nonzero(image_mask).squeeze()\n-    image_keypoints = outputs.keypoints[i][image_indices]\n-    image_scores = outputs.scores[i][image_indices]\n-    image_descriptors = outputs.descriptors[i][image_indices]\n+image_sizes = [(image.height, image.width) for image in images]\n+outputs = processor.post_process_keypoint_detection(outputs, image_sizes)\n+\n+for output in outputs:\n+    for keypoints, scores, descriptors in zip(output[\"keypoints\"], output[\"scores\"], output[\"descriptors\"]):\n+        print(f\"Keypoints: {keypoints}\")\n+        print(f\"Scores: {scores}\")\n+        print(f\"Descriptors: {descriptors}\")\n ```\n \n-You can then print the keypoints on the image to visualize the result :\n+You can then print the keypoints on the image of your choice to visualize the result:\n ```python\n-import cv2\n-for keypoint, score in zip(image_keypoints, image_scores):\n-    keypoint_x, keypoint_y = int(keypoint[0].item()), int(keypoint[1].item())\n-    color = tuple([score.item() * 255] * 3)\n-    image = cv2.circle(image, (keypoint_x, keypoint_y), 2, color)\n-cv2.imwrite(\"output_image.png\", image)\n+import matplotlib.pyplot as plt\n+\n+plt.axis(\"off\")\n+plt.imshow(image_1)\n+plt.scatter(\n+    outputs[0][\"keypoints\"][:, 0],\n+    outputs[0][\"keypoints\"][:, 1],\n+    c=outputs[0][\"scores\"] * 100,\n+    s=outputs[0][\"scores\"] * 50,\n+    alpha=0.8\n+)\n+plt.savefig(f\"output_image.png\")\n ```\n+![image/png](https://cdn-uploads.huggingface.co/production/uploads/632885ba1558dac67c440aa8/ZtFmphEhx8tcbEQqOolyE.png)\n \n This model was contributed by [stevenbucaille](https://huggingface.co/stevenbucaille).\n The original code can be found [here](https://github.com/magicleap/SuperPointPretrainedNetwork).\n@@ -123,6 +131,7 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n [[autodoc]] SuperPointImageProcessor\n \n - preprocess\n+- post_process_keypoint_detection\n \n ## SuperPointForKeypointDetection\n "
        },
        {
            "sha": "65309b1c1826f24283f258e837c8d8634bc3d009",
            "filename": "src/transformers/models/superpoint/image_processing_superpoint.py",
            "status": "modified",
            "additions": 57,
            "deletions": 2,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1835195d134f5a244aed1212342be94fa27b40c/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1835195d134f5a244aed1212342be94fa27b40c/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fimage_processing_superpoint.py?ref=a1835195d134f5a244aed1212342be94fa27b40c",
            "patch": "@@ -13,11 +13,11 @@\n # limitations under the License.\n \"\"\"Image processor class for SuperPoint.\"\"\"\n \n-from typing import Dict, Optional, Union\n+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n \n import numpy as np\n \n-from ... import is_vision_available\n+from ... import is_torch_available, is_vision_available\n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_size_dict\n from ...image_transforms import resize, to_channel_dimension_format\n from ...image_utils import (\n@@ -32,6 +32,12 @@\n from ...utils import TensorType, logging, requires_backends\n \n \n+if is_torch_available():\n+    import torch\n+\n+if TYPE_CHECKING:\n+    from .modeling_superpoint import SuperPointKeypointDescriptionOutput\n+\n if is_vision_available():\n     import PIL\n \n@@ -270,3 +276,52 @@ def preprocess(\n         data = {\"pixel_values\": images}\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+    def post_process_keypoint_detection(\n+        self, outputs: \"SuperPointKeypointDescriptionOutput\", target_sizes: Union[TensorType, List[Tuple]]\n+    ) -> List[Dict[str, \"torch.Tensor\"]]:\n+        \"\"\"\n+        Converts the raw output of [`SuperPointForKeypointDetection`] into lists of keypoints, scores and descriptors\n+        with coordinates absolute to the original image sizes.\n+\n+        Args:\n+            outputs ([`SuperPointKeypointDescriptionOutput`]):\n+                Raw outputs of the model containing keypoints in a relative (x, y) format, with scores and descriptors.\n+            target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`):\n+                Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n+                `(height, width)` of each image in the batch. This must be the original\n+                image size (before any processing).\n+        Returns:\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the keypoints in absolute format according\n+            to target_sizes, scores and descriptors for an image in the batch as predicted by the model.\n+        \"\"\"\n+        if len(outputs.mask) != len(target_sizes):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the mask\")\n+\n+        if isinstance(target_sizes, List):\n+            image_sizes = torch.tensor(target_sizes)\n+        else:\n+            if target_sizes.shape[1] != 2:\n+                raise ValueError(\n+                    \"Each element of target_sizes must contain the size (h, w) of each image of the batch\"\n+                )\n+            image_sizes = target_sizes\n+\n+        # Flip the image sizes to (width, height) and convert keypoints to absolute coordinates\n+        image_sizes = torch.flip(image_sizes, [1])\n+        masked_keypoints = outputs.keypoints * image_sizes[:, None]\n+\n+        # Convert masked_keypoints to int\n+        masked_keypoints = masked_keypoints.to(torch.int32)\n+\n+        results = []\n+        for image_mask, keypoints, scores, descriptors in zip(\n+            outputs.mask, masked_keypoints, outputs.scores, outputs.descriptors\n+        ):\n+            indices = torch.nonzero(image_mask).squeeze(1)\n+            keypoints = keypoints[indices]\n+            scores = scores[indices]\n+            descriptors = descriptors[indices]\n+            results.append({\"keypoints\": keypoints, \"scores\": scores, \"descriptors\": descriptors})\n+\n+        return results"
        },
        {
            "sha": "1075de299a9f406fccd50b5df287fc954a6531b4",
            "filename": "src/transformers/models/superpoint/modeling_superpoint.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1835195d134f5a244aed1212342be94fa27b40c/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1835195d134f5a244aed1212342be94fa27b40c/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperpoint%2Fmodeling_superpoint.py?ref=a1835195d134f5a244aed1212342be94fa27b40c",
            "patch": "@@ -239,7 +239,10 @@ def _get_pixel_scores(self, encoded: torch.Tensor) -> torch.Tensor:\n         return scores\n \n     def _extract_keypoints(self, scores: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n-        \"\"\"Based on their scores, extract the pixels that represent the keypoints that will be used for descriptors computation\"\"\"\n+        \"\"\"\n+        Based on their scores, extract the pixels that represent the keypoints that will be used for descriptors computation.\n+        The keypoints are in the form of relative (x, y) coordinates.\n+        \"\"\"\n         _, height, width = scores.shape\n \n         # Threshold keypoints by score value\n@@ -447,7 +450,7 @@ def forward(\n \n         pixel_values = self.extract_one_channel_pixel_values(pixel_values)\n \n-        batch_size = pixel_values.shape[0]\n+        batch_size, _, height, width = pixel_values.shape\n \n         encoder_outputs = self.encoder(\n             pixel_values,\n@@ -485,6 +488,9 @@ def forward(\n             descriptors[i, : _descriptors.shape[0]] = _descriptors\n             mask[i, : _scores.shape[0]] = 1\n \n+        # Convert to relative coordinates\n+        keypoints = keypoints / torch.tensor([width, height], device=keypoints.device)\n+\n         hidden_states = encoder_outputs[1] if output_hidden_states else None\n         if not return_dict:\n             return tuple(v for v in [loss, keypoints, scores, descriptors, mask, hidden_states] if v is not None)"
        },
        {
            "sha": "c2eae872004c774af9524506cb38f7eb1ee3a146",
            "filename": "tests/models/superpoint/test_image_processing_superpoint.py",
            "status": "modified",
            "additions": 53,
            "deletions": 1,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1835195d134f5a244aed1212342be94fa27b40c/tests%2Fmodels%2Fsuperpoint%2Ftest_image_processing_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1835195d134f5a244aed1212342be94fa27b40c/tests%2Fmodels%2Fsuperpoint%2Ftest_image_processing_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperpoint%2Ftest_image_processing_superpoint.py?ref=a1835195d134f5a244aed1212342be94fa27b40c",
            "patch": "@@ -16,14 +16,19 @@\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n+from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_image_processing_common import (\n     ImageProcessingTestMixin,\n     prepare_image_inputs,\n )\n \n \n+if is_torch_available():\n+    import torch\n+\n+    from transformers.models.superpoint.modeling_superpoint import SuperPointKeypointDescriptionOutput\n+\n if is_vision_available():\n     from transformers import SuperPointImageProcessor\n \n@@ -70,6 +75,23 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n             torchify=torchify,\n         )\n \n+    def prepare_keypoint_detection_output(self, pixel_values):\n+        max_number_keypoints = 50\n+        batch_size = len(pixel_values)\n+        mask = torch.zeros((batch_size, max_number_keypoints))\n+        keypoints = torch.zeros((batch_size, max_number_keypoints, 2))\n+        scores = torch.zeros((batch_size, max_number_keypoints))\n+        descriptors = torch.zeros((batch_size, max_number_keypoints, 16))\n+        for i in range(batch_size):\n+            random_number_keypoints = np.random.randint(0, max_number_keypoints)\n+            mask[i, :random_number_keypoints] = 1\n+            keypoints[i, :random_number_keypoints] = torch.rand((random_number_keypoints, 2))\n+            scores[i, :random_number_keypoints] = torch.rand((random_number_keypoints,))\n+            descriptors[i, :random_number_keypoints] = torch.rand((random_number_keypoints, 16))\n+        return SuperPointKeypointDescriptionOutput(\n+            loss=None, keypoints=keypoints, scores=scores, descriptors=descriptors, mask=mask, hidden_states=None\n+        )\n+\n \n @require_torch\n @require_vision\n@@ -110,3 +132,33 @@ def test_input_image_properly_converted_to_grayscale(self):\n         pre_processed_images = image_processor.preprocess(image_inputs)\n         for image in pre_processed_images[\"pixel_values\"]:\n             self.assertTrue(np.all(image[0, ...] == image[1, ...]) and np.all(image[1, ...] == image[2, ...]))\n+\n+    @require_torch\n+    def test_post_processing_keypoint_detection(self):\n+        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n+        image_inputs = self.image_processor_tester.prepare_image_inputs()\n+        pre_processed_images = image_processor.preprocess(image_inputs, return_tensors=\"pt\")\n+        outputs = self.image_processor_tester.prepare_keypoint_detection_output(**pre_processed_images)\n+\n+        def check_post_processed_output(post_processed_output, image_size):\n+            for post_processed_output, image_size in zip(post_processed_output, image_size):\n+                self.assertTrue(\"keypoints\" in post_processed_output)\n+                self.assertTrue(\"descriptors\" in post_processed_output)\n+                self.assertTrue(\"scores\" in post_processed_output)\n+                keypoints = post_processed_output[\"keypoints\"]\n+                all_below_image_size = torch.all(keypoints[:, 0] <= image_size[1]) and torch.all(\n+                    keypoints[:, 1] <= image_size[0]\n+                )\n+                all_above_zero = torch.all(keypoints[:, 0] >= 0) and torch.all(keypoints[:, 1] >= 0)\n+                self.assertTrue(all_below_image_size)\n+                self.assertTrue(all_above_zero)\n+\n+        tuple_image_sizes = [(image.size[0], image.size[1]) for image in image_inputs]\n+        tuple_post_processed_outputs = image_processor.post_process_keypoint_detection(outputs, tuple_image_sizes)\n+\n+        check_post_processed_output(tuple_post_processed_outputs, tuple_image_sizes)\n+\n+        tensor_image_sizes = torch.tensor([image.size for image in image_inputs]).flip(1)\n+        tensor_post_processed_outputs = image_processor.post_process_keypoint_detection(outputs, tensor_image_sizes)\n+\n+        check_post_processed_output(tensor_post_processed_outputs, tensor_image_sizes)"
        },
        {
            "sha": "8db435502ca5655f570e6be8a3a4d4aa0fceba73",
            "filename": "tests/models/superpoint/test_modeling_superpoint.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a1835195d134f5a244aed1212342be94fa27b40c/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a1835195d134f5a244aed1212342be94fa27b40c/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperpoint%2Ftest_modeling_superpoint.py?ref=a1835195d134f5a244aed1212342be94fa27b40c",
            "patch": "@@ -260,7 +260,7 @@ def test_inference(self):\n         inputs = preprocessor(images=images, return_tensors=\"pt\").to(torch_device)\n         with torch.no_grad():\n             outputs = model(**inputs)\n-        expected_number_keypoints_image0 = 567\n+        expected_number_keypoints_image0 = 568\n         expected_number_keypoints_image1 = 830\n         expected_max_number_keypoints = max(expected_number_keypoints_image0, expected_number_keypoints_image1)\n         expected_keypoints_shape = torch.Size((len(images), expected_max_number_keypoints, 2))\n@@ -275,11 +275,13 @@ def test_inference(self):\n         self.assertEqual(outputs.keypoints.shape, expected_keypoints_shape)\n         self.assertEqual(outputs.scores.shape, expected_scores_shape)\n         self.assertEqual(outputs.descriptors.shape, expected_descriptors_shape)\n-        expected_keypoints_image0_values = torch.tensor([[480.0, 9.0], [494.0, 9.0], [489.0, 16.0]]).to(torch_device)\n+        expected_keypoints_image0_values = torch.tensor([[0.75, 0.0188], [0.7719, 0.0188], [0.7641, 0.0333]]).to(\n+            torch_device\n+        )\n         expected_scores_image0_values = torch.tensor(\n-            [0.0064, 0.0137, 0.0589, 0.0723, 0.5166, 0.0174, 0.1515, 0.2054, 0.0334]\n+            [0.0064, 0.0139, 0.0591, 0.0727, 0.5170, 0.0175, 0.1526, 0.2057, 0.0335]\n         ).to(torch_device)\n-        expected_descriptors_image0_value = torch.tensor(-0.1096).to(torch_device)\n+        expected_descriptors_image0_value = torch.tensor(-0.1095).to(torch_device)\n         predicted_keypoints_image0_values = outputs.keypoints[0, :3]\n         predicted_scores_image0_values = outputs.scores[0, :9]\n         predicted_descriptors_image0_value = outputs.descriptors[0, 0, 0]"
        }
    ],
    "stats": {
        "total": 170,
        "additions": 147,
        "deletions": 23
    }
}