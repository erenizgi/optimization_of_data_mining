{
    "author": "cyang49",
    "message": "Fix Mamba2 Grouped SSD Support in the torch_forward Path (#37533)\n\n* Fix mamba2 grouped support in bamba torch path\n\n* patch zamba2 and mamba2\n\n* Add a unit test for grouped SSD\n\n* add comment for the new unit test\n\n* add output_size arg value to repeat_interleave calls\n\n* Add comment",
    "sha": "4005730044dd4de1575052eec24c1860ef97ccf9",
    "files": [
        {
            "sha": "0fb696c1f7364fc403d24196a5c872d7cd6376f0",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4005730044dd4de1575052eec24c1860ef97ccf9/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4005730044dd4de1575052eec24c1860ef97ccf9/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=4005730044dd4de1575052eec24c1860ef97ccf9",
            "patch": "@@ -783,8 +783,8 @@ def torch_forward(\n             hidden_states = hidden_states.reshape(batch_size, seq_len, -1, self.head_dim).float()\n             B = B.reshape(batch_size, seq_len, -1, self.ssm_state_size).float()\n             C = C.reshape(batch_size, seq_len, -1, self.ssm_state_size).float()\n-            B = B.repeat(1, 1, self.num_heads // self.n_groups, 1)\n-            C = C.repeat(1, 1, self.num_heads // self.n_groups, 1)\n+            B = B.repeat_interleave(self.num_heads // self.n_groups, dim=2, output_size=self.num_heads)\n+            C = C.repeat_interleave(self.num_heads // self.n_groups, dim=2, output_size=self.num_heads)\n             pad_size = (self.chunk_size - seq_len % self.chunk_size) % self.chunk_size\n \n             D_residual = self.D[..., None] * pad_tensor_by_size(hidden_states, pad_size)"
        },
        {
            "sha": "3e0eb513d5cdec54aefef1fe50e0ecf4a86696b6",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4005730044dd4de1575052eec24c1860ef97ccf9/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4005730044dd4de1575052eec24c1860ef97ccf9/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=4005730044dd4de1575052eec24c1860ef97ccf9",
            "patch": "@@ -580,8 +580,8 @@ def torch_forward(\n             hidden_states = hidden_states.reshape(batch_size, seq_len, -1, self.head_dim).float()\n             B = B.reshape(batch_size, seq_len, -1, self.ssm_state_size).float()\n             C = C.reshape(batch_size, seq_len, -1, self.ssm_state_size).float()\n-            B = B.repeat(1, 1, self.num_heads // self.n_groups, 1)\n-            C = C.repeat(1, 1, self.num_heads // self.n_groups, 1)\n+            B = B.repeat_interleave(self.num_heads // self.n_groups, dim=2, output_size=self.num_heads)\n+            C = C.repeat_interleave(self.num_heads // self.n_groups, dim=2, output_size=self.num_heads)\n             pad_size = (self.chunk_size - seq_len % self.chunk_size) % self.chunk_size\n \n             D_residual = self.D[..., None] * pad_tensor_by_size(hidden_states, pad_size)"
        },
        {
            "sha": "a1ca8d095c2563b96cababbdbc7fe51d67ab2d6d",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4005730044dd4de1575052eec24c1860ef97ccf9/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4005730044dd4de1575052eec24c1860ef97ccf9/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=4005730044dd4de1575052eec24c1860ef97ccf9",
            "patch": "@@ -572,8 +572,8 @@ def torch_forward(self, input_states, cache_params: Optional[Mamba2Cache]=None,\n             hidden_states = hidden_states.reshape(batch_size, seq_len, -1, self.head_dim).float()\n             B = B.reshape(batch_size, seq_len, -1, self.ssm_state_size).float()\n             C = C.reshape(batch_size, seq_len, -1, self.ssm_state_size).float()\n-            B = B.repeat(1, 1, self.num_heads // self.n_groups, 1)\n-            C = C.repeat(1, 1, self.num_heads // self.n_groups, 1)\n+            B = B.repeat_interleave(self.num_heads // self.n_groups, dim=2, output_size=self.num_heads)\n+            C = C.repeat_interleave(self.num_heads // self.n_groups, dim=2, output_size=self.num_heads)\n             pad_size = (self.chunk_size - seq_len % self.chunk_size) % self.chunk_size\n \n             D_residual = self.D[..., None] * pad_tensor_by_size(hidden_states, pad_size)"
        },
        {
            "sha": "ad6ca0f872e44e88c3f93ad54467e7a5319c663e",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4005730044dd4de1575052eec24c1860ef97ccf9/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4005730044dd4de1575052eec24c1860ef97ccf9/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=4005730044dd4de1575052eec24c1860ef97ccf9",
            "patch": "@@ -860,8 +860,8 @@ def torch_forward(self, input_states, cache_params: Optional[Zamba2HybridDynamic\n             hidden_states = hidden_states.reshape(batch_size, seq_len, -1, self.head_dim).float()\n             B = B.reshape(batch_size, seq_len,  -1, self.ssm_state_size).float()\n             C = C.reshape(batch_size, seq_len, -1, self.ssm_state_size).float()\n-            B = B.repeat(1, 1, self.num_heads // self.n_groups, 1)\n-            C = C.repeat(1, 1, self.num_heads // self.n_groups, 1)\n+            B = B.repeat_interleave(self.num_heads // self.n_groups, dim=2, output_size=self.num_heads)\n+            C = C.repeat_interleave(self.num_heads // self.n_groups, dim=2, output_size=self.num_heads)\n             pad_size = (self.chunk_size - seq_len % self.chunk_size) % self.chunk_size\n \n             D_residual = self.D[..., None] * pad_tensor_by_size(hidden_states, pad_size)"
        },
        {
            "sha": "cd0af82658a50753d13e3c3e5c0c2edb20637098",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4005730044dd4de1575052eec24c1860ef97ccf9/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4005730044dd4de1575052eec24c1860ef97ccf9/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=4005730044dd4de1575052eec24c1860ef97ccf9",
            "patch": "@@ -630,8 +630,8 @@ def torch_forward(self, input_states, cache_params: Optional[Zamba2HybridDynamic\n             hidden_states = hidden_states.reshape(batch_size, seq_len, -1, self.head_dim).float()\n             B = B.reshape(batch_size, seq_len,  -1, self.ssm_state_size).float()\n             C = C.reshape(batch_size, seq_len, -1, self.ssm_state_size).float()\n-            B = B.repeat(1, 1, self.num_heads // self.n_groups, 1)\n-            C = C.repeat(1, 1, self.num_heads // self.n_groups, 1)\n+            B = B.repeat_interleave(self.num_heads // self.n_groups, dim=2, output_size=self.num_heads)\n+            C = C.repeat_interleave(self.num_heads // self.n_groups, dim=2, output_size=self.num_heads)\n             pad_size = (self.chunk_size - seq_len % self.chunk_size) % self.chunk_size\n \n             D_residual = self.D[..., None] * pad_tensor_by_size(hidden_states, pad_size)"
        },
        {
            "sha": "31565bf23d89df8aed66948033b67c5ae0b08db6",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4005730044dd4de1575052eec24c1860ef97ccf9/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4005730044dd4de1575052eec24c1860ef97ccf9/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=4005730044dd4de1575052eec24c1860ef97ccf9",
            "patch": "@@ -238,6 +238,14 @@ def test_mamba2_slow_vs_fast_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_mamba2_slow_vs_fast_forward(*config_and_inputs)\n \n+    # This test adjusts n_groups to half the original setting and effectively\n+    # creates a grouped SSD configuration in the mamba2 layers\n+    # See https://github.com/huggingface/transformers/pull/37533/\n+    def test_mamba2_slow_vs_fast_forward_grouped(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        config_and_inputs[0].n_groups //= 2\n+        self.model_tester.create_and_check_mamba2_slow_vs_fast_forward(*config_and_inputs)\n+\n     def test_initialization(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n "
        }
    ],
    "stats": {
        "total": 28,
        "additions": 18,
        "deletions": 10
    }
}