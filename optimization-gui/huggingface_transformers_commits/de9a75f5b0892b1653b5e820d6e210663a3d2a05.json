{
    "author": "The5cheduler",
    "message": "fix(trainer): Avoid moving model with device_map (#41032)\n\n* fix(trainer): Avoid moving model with device_map\n\nWhen a model is loaded with `device_map=\"auto\"` and is too large to fit on a single GPU, `accelerate` will offload some layers to the CPU or disk. The `Trainer` would previously attempt to move the entire model to the specified device, causing a `RuntimeError` because a model dispatched with `accelerate` hooks cannot be moved.\n\nThis commit fixes the issue by adding a check in `_move_model_to_device` to see if the model has an `hf_device_map` attribute. If it does, the device placement is assumed to be handled by `accelerate`, and the `model.to(device)` call is skipped.\n\nA regression test is added to ensure the `Trainer` can be initialized with a model that has a `hf_device_map` that simulates offloading without raising an error.\n\n* Added the logger warning for the move model\n\n---------\n\nCo-authored-by: google-labs-jules[bot] <161369871+google-labs-jules[bot]@users.noreply.github.com>",
    "sha": "de9a75f5b0892b1653b5e820d6e210663a3d2a05",
    "files": [
        {
            "sha": "00e71e37ef0ff90400efdab019d3fe11fb7e84ad",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/de9a75f5b0892b1653b5e820d6e210663a3d2a05/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de9a75f5b0892b1653b5e820d6e210663a3d2a05/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=de9a75f5b0892b1653b5e820d6e210663a3d2a05",
            "patch": "@@ -894,6 +894,11 @@ def remove_callback(self, callback):\n         self.callback_handler.remove_callback(callback)\n \n     def _move_model_to_device(self, model, device):\n+        if getattr(model, \"hf_device_map\", None) is not None:\n+            logger.warning(\n+                \"The model is already on multiple devices. Skipping the move to device specified in `args`.\"\n+            )\n+            return\n         model = model.to(device)\n         # Moving a model to an XLA device disconnects the tied weights, so we have to retie them.\n         if self.args.parallel_mode == ParallelMode.TPU and hasattr(model, \"tie_weights\"):"
        },
        {
            "sha": "97e6b569e452a1bf247291cbbbc29b2527a35647",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 18,
            "deletions": 0,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/de9a75f5b0892b1653b5e820d6e210663a3d2a05/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de9a75f5b0892b1653b5e820d6e210663a3d2a05/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=de9a75f5b0892b1653b5e820d6e210663a3d2a05",
            "patch": "@@ -1428,6 +1428,24 @@ def test_trainer_works_with_dict(self):\n         _ = trainer.evaluate()\n         _ = trainer.predict(eval_dataset)\n \n+    def test_init_with_offloaded_model(self):\n+        # Test that Trainer can be initialized with a model that has been offloaded to CPU\n+        config = RegressionModelConfig(a=1.5, b=2.5)\n+        model = RegressionPreTrainedModel(config)\n+\n+        # Simulate a model with some parts offloaded to CPU\n+        device_map = {\"a\": \"cpu\"}\n+        if torch.cuda.is_available():\n+            device_map[\"b\"] = \"cuda:0\"\n+        else:\n+            device_map[\"b\"] = \"cpu\"\n+        model.hf_device_map = device_map\n+\n+        with tempfile.TemporaryDirectory() as tmp_dir:\n+            args = TrainingArguments(output_dir=tmp_dir, report_to=\"none\")\n+            # This should not raise an error.\n+            _ = Trainer(model, args=args, train_dataset=RegressionDataset())\n+\n     def test_training_arguments_are_left_untouched(self):\n         tmp_dir = self.get_auto_remove_tmp_dir()\n         trainer = get_regression_trainer(output_dir=tmp_dir)"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 23,
        "deletions": 0
    }
}