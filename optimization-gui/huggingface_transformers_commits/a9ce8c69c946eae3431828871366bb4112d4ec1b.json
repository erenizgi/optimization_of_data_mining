{
    "author": "yao-matrix",
    "message": "align xpu's autocast behavior w/ cuda by using device agnostic torch APIs (#38284)\n\n* siwtch to device agnostic autocast in nemotron to align xpu behavior w/\ncuda\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* fix issue\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* use torch.cast as other modeling code for decision_transformer&gpt2&imagegpt\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* refine\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\n\n* update get_autocast_gpu_dtype to device agnostic one\n\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\n\n* fix comments\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: Matrix Yao <matrix.yao@intel.com>\nSigned-off-by: Matrix YAO <matrix.yao@intel.com>\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "a9ce8c69c946eae3431828871366bb4112d4ec1b",
    "files": [
        {
            "sha": "64148e2457f0db856d76bbdf7a3e50bccba840a2",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -100,7 +100,7 @@ def forward(self, x, position_ids):\n         # Force float32 since bfloat16 loses precision on long contexts\n         # See https://github.com/huggingface/transformers/pull/29285\n         device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        device_type = device_type if device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)"
        },
        {
            "sha": "f1fee43a2391cadaa34556a41a9c759ddd8ee4bd",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -64,7 +64,7 @@ def forward(self, x, position_ids, seq_len=None):\n         # Force float32 since bfloat16 loses precision on long contexts\n         # See https://github.com/huggingface/transformers/pull/29285\n         device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        device_type = device_type if device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n@@ -387,9 +387,14 @@ def forward(\n         # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n         # in fp32. (LlamaRMSNorm handles it correctly)\n         input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "a436ce6d2c03fc3698f68e789013f9ab419ac3ce",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -219,7 +219,7 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n             scale_factor /= float(self.layer_idx + 1)\n \n         # Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))\n-        with torch.amp.autocast(query.device.type, enabled=False):\n+        with torch.autocast(query.device.type, enabled=False):\n             q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(-1, dk, k_seq_len)\n             attn_weights = torch.baddbmm(attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)\n             attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)"
        },
        {
            "sha": "fae9f2dbb95cab56a4ec8fea53daf821798cf4c9",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -306,9 +306,14 @@ def forward(\n         # in fp32. (DiffLlamaRMSNorm handles it correctly)\n \n         input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "0ff0465c793781d7a382441c23a15bc9166b8c43",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -239,9 +239,14 @@ def forward(\n         # in fp32. (DiffLlamaRMSNorm handles it correctly)\n \n         input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "28cec74fb3d1d0a4e1b2bd8f93c2ca6a064886b4",
            "filename": "src/transformers/models/distilbert/modeling_distilbert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdistilbert%2Fmodeling_distilbert.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -289,9 +289,14 @@ def reshape(x: torch.Tensor) -> torch.Tensor:\n         # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n         # in fp32. (LlamaRMSNorm handles it correctly)\n \n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if query_states.dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "10db78a67cb11fecd829829518f36da3c51b4af6",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -459,9 +459,14 @@ def forward(\n         # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n         # in fp32.\n         input_dtype = query_layer.dtype\n+        device_type = query_layer.device.type if query_layer.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "644fec2e1a32f9e8d7be755a6eff46096d6e3b5e",
            "filename": "src/transformers/models/esm/modeling_esmfold.py",
            "status": "modified",
            "additions": 13,
            "deletions": 6,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esmfold.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -133,9 +133,14 @@ class EsmForProteinFoldingOutput(ModelOutput):\n     max_predicted_aligned_error: Optional[torch.FloatTensor] = None\n \n \n-def is_fp16_enabled():\n+def is_fp16_enabled(device_type):\n     # Autocast world\n-    fp16_enabled = torch.get_autocast_gpu_dtype() == torch.float16\n+    autocast_dtype = (\n+        torch.get_autocast_dtype(device_type)\n+        if hasattr(torch, \"get_autocast_dtype\")\n+        else torch.get_autocast_gpu_dtype()\n+    )\n+    fp16_enabled = autocast_dtype == torch.float16\n     fp16_enabled = fp16_enabled and torch.is_autocast_enabled()\n \n     return fp16_enabled\n@@ -885,8 +890,9 @@ def forward(\n         b = b * self.sigmoid(self.linear_b_g(z))\n         b = b * self.linear_b_p(z)\n \n-        if is_fp16_enabled():\n-            with torch.cuda.amp.autocast(enabled=False):\n+        device_type = a.device.type if a.device.type != \"mps\" else \"cpu\"\n+        if is_fp16_enabled(device_type):\n+            with torch.autocast(device_type=device_type, enabled=False):\n                 x = self._combine_projections(a.float(), b.float())\n         else:\n             x = self._combine_projections(a, b)\n@@ -1499,8 +1505,9 @@ def forward(\n             z[0] = z[0].cpu()\n \n         # [*, H, N_res, N_res]\n-        if is_fp16_enabled():\n-            with torch.cuda.amp.autocast(enabled=False):\n+        device_type = q.device.type if q.device.type != \"mps\" else \"cpu\"\n+        if is_fp16_enabled(device_type):\n+            with torch.autocast(device_type=device_type, enabled=False):\n                 a = torch.matmul(\n                     permute_final_dims(q.float(), (1, 0, 2)),  # [*, H, N_res, C_hidden]\n                     permute_final_dims(k.float(), (1, 2, 0)),  # [*, H, C_hidden, N_res]"
        },
        {
            "sha": "d6634662f30b0bd626fecf5fc5e701cfa30036ff",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -488,9 +488,14 @@ def forward(\n         # therefore the input hidden states gets silently casted in float32. Hence, we need\n         # cast them back in float16 just to be sure everything works as expected.\n         input_dtype = query_layer.dtype\n+        device_type = query_layer.device.type if query_layer.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "fa98bc3614e5a4994f83d1eb41dc7fa3e703e6ca",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -229,7 +229,7 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n             scale_factor /= float(self.layer_idx + 1)\n \n         # Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))\n-        with torch.amp.autocast(query.device.type, enabled=False):\n+        with torch.autocast(query.device.type, enabled=False):\n             q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(-1, dk, k_seq_len)\n             attn_weights = torch.baddbmm(attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)\n             attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)"
        },
        {
            "sha": "b90fdfe8acc159017439973b2526bb466d558956",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -343,9 +343,14 @@ def forward(\n         # therefore the input hidden states gets silently casted in float32. Hence, we need\n         # cast them back in float16 just to be sure everything works as expected.\n         input_dtype = query.dtype\n+        device_type = query.device.type if query.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "8ac65c7d1ae86bb28c0e5e9953424d6270c8a190",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -323,9 +323,14 @@ def forward(\n         # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n         # in fp32. (LlamaRMSNorm handles it correctly)\n \n+        device_type = query.device.type if query.device.type != \"mps\" else \"cpu\"\n         if query.dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "4388fad01f60911c555fb1c24c588375ace27da1",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -355,9 +355,14 @@ def forward(\n         # in fp32. (LlamaRMSNorm handles it correctly)\n \n         input_dtype = query.dtype\n+        device_type = query.device.type if query.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "65d5cbc3df213814df6dbc773644a0a1eeef9656",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -22,7 +22,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.cuda.amp import autocast\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n@@ -280,7 +279,7 @@ def _upcast_and_reordered_attn(self, query, key, value, attention_mask=None, hea\n             scale_factor /= float(self.layer_idx + 1)\n \n         # Upcast (turn off autocast) and reorder (Scale K by 1 / root(dk))\n-        with autocast(enabled=False):\n+        with torch.autocast(query.device.type, enabled=False):\n             q, k = query.reshape(-1, q_seq_len, dk), key.transpose(-1, -2).reshape(-1, dk, k_seq_len)\n             attn_weights = torch.baddbmm(attn_weights, q.float(), k.float(), beta=0, alpha=scale_factor)\n             attn_weights = attn_weights.reshape(bsz, num_heads, q_seq_len, k_seq_len)"
        },
        {
            "sha": "0b93d4484c9fe277212b08215d07277fbd240eea",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -412,9 +412,14 @@ def forward(\n         # therefore the input hidden states gets silently casted in float32. Hence, we need\n         # cast them back in float16 just to be sure everything works as expected.\n         input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "4b6fefbf9e92192280108d050fbecaaa1df0b6ae",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -710,9 +710,14 @@ def forward(\n         # in fp32. (LlamaRMSNorm handles it correctly)\n \n         input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "45c307b6136726c066ca7f51ff2cb58c5895bd17",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -661,9 +661,14 @@ def forward(\n         # in fp32. (MimiRMSNorm handles it correctly)\n \n         input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "6eca81ef49775a42be130b7202bf5a190ffe8138",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -594,9 +594,14 @@ def forward(\n         # in fp32. (MoshiRMSNorm handles it correctly)\n \n         input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "d9d0248a33d458f7f24e45ed9e8f5d5c92df126e",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 20,
            "deletions": 5,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -51,11 +51,17 @@\n logger = logging.get_logger(__name__)\n \n \n-def _cast_if_autocast_enabled(*args):\n+def _cast_if_autocast_enabled(device_type, *args):\n     if not torch.is_autocast_enabled():\n         return args\n     else:\n-        return torch.cuda.amp.autocast_mode._cast(args, torch.get_autocast_gpu_dtype())\n+        # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n+        target_dtype = (\n+            torch.get_autocast_dtype(device_type)\n+            if hasattr(torch, \"get_autocast_dtype\")\n+            else torch.get_autocast_gpu_dtype()\n+        )\n+        return torch.amp.autocast_mode._cast(args, device_type, target_dtype)\n \n \n class NemotronLayerNorm1P(nn.LayerNorm):\n@@ -71,8 +77,11 @@ def __init__(\n         super().__init__(normalized_shape, eps, elementwise_affine, bias, device, dtype)\n \n     def forward(self, input: Tensor) -> Tensor:\n-        args = _cast_if_autocast_enabled(input, self.normalized_shape, self.weight + 1, self.bias, self.eps)\n-        with torch.amp.autocast(input.device.type, enabled=False):\n+        device_type = input.device.type if input.device.type != \"mps\" else \"cpu\"\n+        args = _cast_if_autocast_enabled(\n+            device_type, input, self.normalized_shape, self.weight + 1, self.bias, self.eps\n+        )\n+        with torch.autocast(device_type=input.device.type, enabled=False):\n             return F.layer_norm(*args)\n \n \n@@ -344,9 +353,15 @@ def forward(\n         # in fp32. (NemotronRMSNorm handles it correctly)\n \n         input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                # NOTE: `torch.get_autocast_dtype` is there starting from PyTorch 2.4\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "a9f2a08124e5bd010ffe183872203c10706188c2",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -421,9 +421,14 @@ def forward(\n         # in fp32. (OlmoeRMSNorm handles it correctly)\n \n         input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "b1651f467b41ba63f5d341869f9f8d87285d8cb8",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -369,9 +369,14 @@ def forward(\n         # therefore the input hidden states gets silently casted in float32. Hence, we need\n         # cast them back in float16 just to be sure everything works as expected.\n         input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "1ccc6ea0bfbc697a90b8e18bea61a96bb30daa68",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -2638,7 +2638,7 @@ def forward(self, x):\n         batch_size, seq_len = x.shape[0], x.shape[1]\n         t = torch.arange(seq_len, device=x.device)\n         device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        device_type = device_type if device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = t.unsqueeze(1).float() @ self.inv_freq.unsqueeze(0).float()\n             freqs = torch.stack((freqs, freqs), dim=-1)"
        },
        {
            "sha": "3779a2ad06157c5b0600e192ae473ffb49f2a538",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -2906,7 +2906,7 @@ def forward(self, x):\n         batch_size, seq_len = x.shape[0], x.shape[1]\n         t = torch.arange(seq_len, device=x.device)\n         device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        device_type = device_type if device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = t.unsqueeze(1).float() @ self.inv_freq.unsqueeze(0).float()\n             freqs = torch.stack((freqs, freqs), dim=-1)"
        },
        {
            "sha": "cc617533582d9e6bb615469f4defd3bc195daa07",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -418,9 +418,14 @@ def forward(\n         # therefore the input hidden states gets silently casted in float32. Hence, we need\n         # cast them back in float16 just to be sure everything works as expected.\n         input_dtype = query_states.dtype\n+        device_type = query_states.device.type if query_states.device.type != \"mps\" else \"cpu\"\n         if input_dtype == torch.float32:\n             if torch.is_autocast_enabled():\n-                target_dtype = torch.get_autocast_gpu_dtype()\n+                target_dtype = (\n+                    torch.get_autocast_dtype(device_type)\n+                    if hasattr(torch, \"get_autocast_dtype\")\n+                    else torch.get_autocast_gpu_dtype()\n+                )\n             # Handle the case where the model is quantized\n             elif hasattr(self.config, \"_pre_quantization_dtype\"):\n                 target_dtype = self.config._pre_quantization_dtype"
        },
        {
            "sha": "6723c520f2829d34f571f664e63cbb3c55a821ca",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -78,7 +78,7 @@ def forward(self, x, position_ids, seq_len=None):\n         # Force float32 since bfloat16 loses precision on long contexts\n         # See https://github.com/huggingface/transformers/pull/29285\n         device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n+        device_type = device_type if device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)"
        },
        {
            "sha": "dfa8bca69ef4a48d43afa3e40e74d26a70dd11df",
            "filename": "tests/models/mamba2/test_modeling_mamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a9ce8c69c946eae3431828871366bb4112d4ec1b/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a9ce8c69c946eae3431828871366bb4112d4ec1b/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmamba2%2Ftest_modeling_mamba2.py?ref=a9ce8c69c946eae3431828871366bb4112d4ec1b",
            "patch": "@@ -462,7 +462,7 @@ def test_mamba2_mixer_train_vs_eval_equivalence(self):\n         config = Mamba2Config(num_heads=24, head_dim=64, hidden_size=768, expand=2, n_groups=1)\n \n         torch.manual_seed(42)\n-        with torch.amp.autocast(device_type=torch_device, dtype=dtype):\n+        with torch.autocast(device_type=torch_device, dtype=dtype):\n             with torch.no_grad():\n                 mixer = Mamba2Mixer(config, layer_idx=0).to(torch_device)\n                 hidden_states = torch.rand(size=(B, T, D), dtype=dtype, device=torch_device)"
        }
    ],
    "stats": {
        "total": 175,
        "additions": 138,
        "deletions": 37
    }
}