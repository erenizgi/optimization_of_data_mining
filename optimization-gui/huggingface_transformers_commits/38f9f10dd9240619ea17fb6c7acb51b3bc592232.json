{
    "author": "gante",
    "message": "Cache: revert DynamicCache init for BC (#33861)\n\n* tmp commit\r\n\r\n* tmp commit\r\n\r\n* make fixup\r\n\r\n* missing removal\r\n\r\n* fix condition\r\n\r\n* fix end-to-end compilation\r\n\r\n* if -> elif\r\n\r\n* BC\r\n\r\n* BC\r\n\r\n* use @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\r\n\r\n* wups the import\r\n\r\n* ðŸ¥´\r\n\r\n---------\r\n\r\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>",
    "sha": "38f9f10dd9240619ea17fb6c7acb51b3bc592232",
    "files": [
        {
            "sha": "2ee20aea5568a0a8aaadcd1614b4020fe936c6b8",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 41,
            "deletions": 30,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/38f9f10dd9240619ea17fb6c7acb51b3bc592232/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38f9f10dd9240619ea17fb6c7acb51b3bc592232/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=38f9f10dd9240619ea17fb6c7acb51b3bc592232",
            "patch": "@@ -16,6 +16,7 @@\n     is_torchdynamo_compiling,\n     logging,\n )\n+from .utils.deprecation import deprecate_kwarg\n \n \n if is_hqq_available():\n@@ -361,15 +362,12 @@ class DynamicCache(Cache):\n         ```\n     \"\"\"\n \n+    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n     def __init__(self, num_hidden_layers: Optional[int] = None) -> None:\n         super().__init__()\n-        if num_hidden_layers is None:\n-            self.key_cache: List[torch.Tensor] = []\n-            self.value_cache: List[torch.Tensor] = []\n-        else:\n-            self.key_cache: List[torch.Tensor] = [[] for _ in range(num_hidden_layers)]\n-            self.value_cache: List[torch.Tensor] = [[] for _ in range(num_hidden_layers)]\n         self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n+        self.key_cache: List[torch.Tensor] = []\n+        self.value_cache: List[torch.Tensor] = []\n \n     def __getitem__(self, layer_idx: int) -> List[Tuple[torch.Tensor]]:\n         \"\"\"\n@@ -425,11 +423,13 @@ def update(\n \n         # Update the cache\n         if len(self.key_cache) <= layer_idx:\n+            # There may be skipped layers, fill them with empty lists\n+            for _ in range(len(self.key_cache), layer_idx):\n+                self.key_cache.append([])\n+                self.value_cache.append([])\n             self.key_cache.append(key_states)\n             self.value_cache.append(value_states)\n-        # content on layer cache can be a tensor and checking not tensor causes errors\n-        # so we explicitly check for the empty list\n-        elif self.key_cache[layer_idx] == []:\n+        elif len(self.key_cache[layer_idx]) == 0:  # fills previously skipped layers; checking for tensor causes errors\n             self.key_cache[layer_idx] = key_states\n             self.value_cache[layer_idx] = value_states\n         else:\n@@ -441,9 +441,13 @@ def update(\n     def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n         \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n         # TODO: deprecate this function in favor of `cache_position`\n-        if len(self.key_cache) <= layer_idx or (len(self.key_cache) > layer_idx and self.key_cache[layer_idx] == []):\n-            return 0\n-        return self.key_cache[layer_idx].shape[-2]\n+        is_empty_layer = (\n+            len(self.key_cache) == 0  # no cache in any layer\n+            or len(self.key_cache) <= layer_idx  # skipped `layer_idx` and hasn't run a layer with cache after it\n+            or len(self.key_cache[layer_idx]) == 0  # the layer has no cache\n+        )\n+        layer_seq_length = self.key_cache[layer_idx].shape[-2] if not is_empty_layer else 0\n+        return layer_seq_length\n \n     def get_max_length(self) -> Optional[int]:\n         \"\"\"Returns the maximum sequence length of the cached states. DynamicCache does not have a maximum length.\"\"\"\n@@ -458,12 +462,13 @@ def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n         return legacy_cache\n \n     @classmethod\n+    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n     def from_legacy_cache(\n         cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, num_hidden_layers: int = None\n     ) -> \"DynamicCache\":\n         \"\"\"Converts a cache in the legacy cache format into an equivalent `DynamicCache`. Used for\n         backward compatibility.\"\"\"\n-        cache = cls(num_hidden_layers)\n+        cache = cls()\n         if past_key_values is not None:\n             for layer_idx in range(len(past_key_values)):\n                 key_states, value_states = past_key_values[layer_idx]\n@@ -486,23 +491,27 @@ def crop(self, max_length: int):\n                 self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n                 self.value_cache[idx] = self.value_cache[idx][..., :max_length, :]\n \n-    def batch_split(self, full_batch_size: int, split_size: int, num_hidden_layers: int) -> List[\"DynamicCache\"]:\n+    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n+    def batch_split(\n+        self, full_batch_size: int, split_size: int, num_hidden_layers: int = None\n+    ) -> List[\"DynamicCache\"]:\n         \"\"\"Split the current instance into a list of `DynamicCache` by the batch size. This will be used by\n         `_split_model_inputs()` in `generation.utils`\"\"\"\n         out = []\n         for i in range(0, full_batch_size, split_size):\n-            current_split = DynamicCache(num_hidden_layers)\n+            current_split = DynamicCache()\n             current_split._seen_tokens = self._seen_tokens\n             current_split.key_cache = [tensor[i : i + split_size] for tensor in self.key_cache]\n             current_split.value_cache = [tensor[i : i + split_size] for tensor in self.value_cache]\n             out.append(current_split)\n         return out\n \n     @classmethod\n-    def from_batch_splits(cls, splits: List[\"DynamicCache\"], num_hidden_layers: int) -> \"DynamicCache\":\n+    @deprecate_kwarg(\"num_hidden_layers\", version=\"4.47.0\")\n+    def from_batch_splits(cls, splits: List[\"DynamicCache\"], num_hidden_layers: int = None) -> \"DynamicCache\":\n         \"\"\"This is the opposite of the above `batch_split()` method. This will be used by `stack_model_outputs` in\n         `generation.utils`\"\"\"\n-        cache = cls(num_hidden_layers)\n+        cache = cls()\n         for idx in range(len(splits[0])):\n             key_cache = [current.key_cache[idx] for current in splits if current.key_cache[idx] != []]\n             value_cache = [current.key_cache[idx] for current in splits if current.key_cache[idx] != []]\n@@ -618,7 +627,9 @@ def update(\n             self._seen_tokens += key_states.shape[-2]\n \n         # Update the cache\n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) < layer_idx:\n+            raise ValueError(\"OffloadedCache does not support model usage where layers are skipped. Use DynamicCache.\")\n+        elif len(self.key_cache) == layer_idx:\n             self.key_cache.append(key_states)\n             self.value_cache.append(value_states)\n             self.original_device.append(key_states.device)\n@@ -677,7 +688,9 @@ def update(\n         if layer_idx == 0:\n             self._seen_tokens += key_states.shape[-2]\n \n-        if len(self.key_cache) <= layer_idx:\n+        if len(self.key_cache) < layer_idx:\n+            raise ValueError(\"QuantizedCache does not support model usage where layers are skipped. Use DynamicCache.\")\n+        elif len(self.key_cache) == layer_idx:\n             self._quantized_key_cache.append(self._quantize(key_states.contiguous(), axis=self.axis_key))\n             self._quantized_value_cache.append(self._quantize(value_states.contiguous(), axis=self.axis_value))\n             self.key_cache.append(torch.zeros(0, dtype=key_states.dtype, device=key_states.device))\n@@ -1430,12 +1443,12 @@ def to_legacy_cache(self) -> Tuple[Tuple[torch.Tensor], Tuple[torch.Tensor]]:\n \n     @classmethod\n     def from_legacy_cache(\n-        cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None, num_hidden_layers: int = None\n+        cls, past_key_values: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     ) -> \"EncoderDecoderCache\":\n         \"\"\"Converts a cache in the legacy cache format into an equivalent `EncoderDecoderCache`.\"\"\"\n         cache = cls(\n-            self_attention_cache=DynamicCache(num_hidden_layers),\n-            cross_attention_cache=DynamicCache(num_hidden_layers),\n+            self_attention_cache=DynamicCache(),\n+            cross_attention_cache=DynamicCache(),\n         )\n         if past_key_values is not None:\n             for layer_idx in range(len(past_key_values)):\n@@ -1493,26 +1506,24 @@ def crop(self, maximum_length: int):\n         self.check_dynamic_cache(self.crop.__name__)\n         self.self_attention_cache.crop(maximum_length)\n \n-    def batch_split(\n-        self, full_batch_size: int, split_size: int, num_hidden_layers: int\n-    ) -> \"List[EncoderDecoderCache]\":\n+    def batch_split(self, full_batch_size: int, split_size: int) -> \"List[EncoderDecoderCache]\":\n         \"\"\"Split the current instance into a list of `DynamicCache` by the batch size. This will be used by\n         `_split_model_inputs()` in `generation.utils`\"\"\"\n         self.check_dynamic_cache(self.batch_split.__name__)\n-        self_attention_cache = self.self_attention_cache.batch_split(full_batch_size, split_size, num_hidden_layers)\n-        cross_attention_cache = self.cross_attention_cache.batch_split(full_batch_size, split_size, num_hidden_layers)\n+        self_attention_cache = self.self_attention_cache.batch_split(full_batch_size, split_size)\n+        cross_attention_cache = self.cross_attention_cache.batch_split(full_batch_size, split_size)\n \n         out = []\n         for self_attn, cross_attn in zip(self_attention_cache, cross_attention_cache):\n             out.append(EncoderDecoderCache(self_attn, cross_attn))\n         return out\n \n     @classmethod\n-    def from_batch_splits(cls, splits: List[\"EncoderDecoderCache\"], num_hidden_layers: int) -> \"EncoderDecoderCache\":\n+    def from_batch_splits(cls, splits: List[\"EncoderDecoderCache\"]) -> \"EncoderDecoderCache\":\n         \"\"\"This is the opposite of the above `batch_split()` method. This will be used by `stack_model_outputs` in\n         `generation.utils`\"\"\"\n-        self_attention_cache = DynamicCache(num_hidden_layers)\n-        cross_attention_cache = DynamicCache(num_hidden_layers)\n+        self_attention_cache = DynamicCache()\n+        cross_attention_cache = DynamicCache()\n         for idx in range(len(splits[0])):\n             layer_keys = torch.cat([current.self_attention_cache.key_cache[idx] for current in splits], dim=0)\n             layer_values = torch.cat([current.self_attention_cache.value_cache[idx] for current in splits], dim=0)"
        },
        {
            "sha": "e00d0e41556f8a8470bc2d21385d08269ab2bce1",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/38f9f10dd9240619ea17fb6c7acb51b3bc592232/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38f9f10dd9240619ea17fb6c7acb51b3bc592232/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=38f9f10dd9240619ea17fb6c7acb51b3bc592232",
            "patch": "@@ -1697,11 +1697,10 @@ def _prepare_cache_for_generation(\n         # Use DynamicCache() instance by default. This will avoid back and forth from legacy format that\n         # keeps copying the cache thus using much more memory\n         else:\n-            num_hidden_layers = self.config.get_text_config().num_hidden_layers\n             model_kwargs[cache_name] = (\n-                DynamicCache(num_hidden_layers)\n+                DynamicCache()\n                 if not requires_cross_attention_cache\n-                else EncoderDecoderCache(DynamicCache(num_hidden_layers), DynamicCache(num_hidden_layers))\n+                else EncoderDecoderCache(DynamicCache(), DynamicCache())\n             )\n \n     def _supports_num_logits_to_keep(self) -> bool:"
        },
        {
            "sha": "59192be876971ba728969c479f6bf6311a5b3253",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 25,
            "deletions": 2,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/38f9f10dd9240619ea17fb6c7acb51b3bc592232/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38f9f10dd9240619ea17fb6c7acb51b3bc592232/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=38f9f10dd9240619ea17fb6c7acb51b3bc592232",
            "patch": "@@ -1776,13 +1776,13 @@ def test_new_cache_format(self, num_beams, do_sample):\n             set_seed(seed)\n             legacy_results = model.generate(**generation_kwargs, **inputs_dict)\n             set_seed(seed)\n-            num_hidden_layers = config.get_text_config().num_hidden_layers\n             if config.is_encoder_decoder:\n                 cache_cls = EncoderDecoderCache\n-                past_key_values = cache_cls(DynamicCache(num_hidden_layers), DynamicCache(num_hidden_layers))\n+                past_key_values = cache_cls(DynamicCache(), DynamicCache())\n             else:\n                 cache_cls = DynamicCache\n                 past_key_values = cache_cls()\n+\n             new_results = model.generate(past_key_values=past_key_values, **generation_kwargs, **inputs_dict)\n \n             # The two sets of generated sequences must match, despite the cache format between forward passes being\n@@ -3725,6 +3725,29 @@ def test_padding_input_contrastive_search_t5(self):\n         self.assertEqual(generated_text_no_padding, generated_text_with_padding)\n         self.assertEqual(generated_text_no_padding, \"Ich muss diese Aufgabe vor Ende des Tages beenden.\")\n \n+    def test_generate_compile_fullgraph_tiny(self):\n+        \"\"\"\n+        Tests that we can call end-to-end generation with a tiny model (i.e. doesn't crash)\n+        NOTE: this test is quite slow (~20s on a consumer desktop), but it is important that we keep it as part of the\n+        non-slow tests to prevent regressions!\n+        \"\"\"\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"hf-internal-testing/tiny-random-LlamaForCausalLM\", torch_dtype=torch.bfloat16, device_map=\"auto\"\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-LlamaForCausalLM\")\n+\n+        # compile generate\n+        compiled_generate = torch.compile(model.generate, fullgraph=True, mode=\"reduce-overhead\")\n+\n+        # compiled generate does NOT accept parameterization except a) model inputs b) a generation config\n+        generation_config = copy.deepcopy(model.generation_config)\n+        generation_config.pad_token_id = model.config.eos_token_id\n+\n+        model_inputs = tokenizer([\"Write a poem about the market crashing in summer\"], return_tensors=\"pt\")\n+        model_inputs = model_inputs.to(model.device)\n+        gen_out = compiled_generate(**model_inputs, generation_config=generation_config)\n+        self.assertTrue(gen_out.shape[1] > model_inputs[\"input_ids\"].shape[1])  # some text was generated\n+\n \n @require_torch\n class TokenHealingTestCase(unittest.TestCase):"
        },
        {
            "sha": "85e54f707d7d2e3f8bb201c3cd64ea5701aebc91",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 40,
            "deletions": 12,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/38f9f10dd9240619ea17fb6c7acb51b3bc592232/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38f9f10dd9240619ea17fb6c7acb51b3bc592232/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=38f9f10dd9240619ea17fb6c7acb51b3bc592232",
            "patch": "@@ -383,45 +383,73 @@ def test_assisted_decoding_with_num_logits_to_keep(self):\n         pass\n \n     @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_beam_sample_generate_dict_output():\n+    def test_beam_sample_generate_dict_output(self):\n         pass\n \n     @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_beam_search_generate_dict_output():\n+    def test_beam_search_generate_dict_output(self):\n         pass\n \n     @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_constrained_beam_search_generate_dict_output():\n+    def test_constrained_beam_search_generate_dict_output(self):\n         pass\n \n     @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_dola_decoding_sample():\n+    def test_dola_decoding_sample(self):\n         pass\n \n     @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_generate_methods_with_num_logits_to_keep():\n+    def test_generate_methods_with_num_logits_to_keep(self):\n         pass\n \n     @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_greedy_generate_dict_outputs():\n+    def test_greedy_generate_dict_outputs(self):\n         pass\n \n     @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_group_beam_search_generate_dict_output():\n+    def test_group_beam_search_generate_dict_output(self):\n         pass\n \n     @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_model_parallel_beam_search():\n+    def test_model_parallel_beam_search(self):\n         pass\n \n-    @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_new_cache_format_2():\n-        pass\n+    @is_flaky()  # TODO (joao, raushan) - investigate why this test is flaky (probably depends on the model initialization)\n+    def test_new_cache_format_0(self):\n+        super().test_new_cache_format_0()\n+\n+    @is_flaky()  # TODO (joao, raushan) - investigate why this test is flaky (probably depends on the model initialization)\n+    def test_new_cache_format_1(self):\n+        super().test_new_cache_format_1()\n+\n+    @is_flaky()  # TODO (joao, raushan) - investigate why this test is flaky (probably depends on the model initialization)\n+    def test_new_cache_format_2(self):\n+        super().test_new_cache_format_2()\n \n     @unittest.skip(reason=\"Failing test, need to fix\")\n-    def test_sample_generate_dict_output():\n+    def test_sample_generate_dict_output(self):\n         pass\n \n+    def test_generate_text_only_with_cache(self):\n+        \"\"\"\n+        Tests that our cached generation with text-only inputs works. When mllama was introduced, this feature\n+        required cache modifications (because layers are skipped in practice). This test should prevent regressions.\n+        \"\"\"\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+            input_ids = inputs[\"input_ids\"]\n+            del inputs[\"input_ids\"]\n+            del inputs[\"pixel_values\"]\n+\n+            model.generate(input_ids, use_cache=True)\n+\n \n @require_torch\n class MllamaForConditionalGenerationIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "4a6dae67cbc807ed76f054b8d3b59cc876cee694",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/38f9f10dd9240619ea17fb6c7acb51b3bc592232/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/38f9f10dd9240619ea17fb6c7acb51b3bc592232/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=38f9f10dd9240619ea17fb6c7acb51b3bc592232",
            "patch": "@@ -53,7 +53,7 @@ class CacheTest(unittest.TestCase):\n     def test_dynamic_cache_retrocompatibility(self):\n         \"\"\"Tests that we can convert back and forth between the legacy cache format and DynamicCache\"\"\"\n         legacy_cache = ()\n-        new_cache = DynamicCache(num_hidden_layers=10)\n+        new_cache = DynamicCache()\n \n         # Creates a new cache with 10 layers in both formats\n         for layer_idx in range(10):\n@@ -83,7 +83,7 @@ def test_dynamic_cache_retrocompatibility(self):\n                 )\n \n         # Test 1: We can convert from legacy to new with no changes\n-        from_legacy = DynamicCache.from_legacy_cache(legacy_cache, num_hidden_layers=10)\n+        from_legacy = DynamicCache.from_legacy_cache(legacy_cache)\n         for layer_idx in range(10):\n             for key_value_idx in range(2):\n                 self.assertTrue(\n@@ -103,7 +103,7 @@ def test_reorder_cache_retrocompatibility(self):\n         legacy_reorder_fn = GPT2LMHeadModel._reorder_cache  # An example of a legacy `_reorder_cache` function\n \n         legacy_cache = ()\n-        new_cache = DynamicCache(num_hidden_layers=10)\n+        new_cache = DynamicCache()\n \n         # Creates a new cache with 10 layers in both formats\n         for layer_idx in range(10):\n@@ -240,9 +240,7 @@ def test_dynamic_cache_hard(self):\n         set_seed(0)\n         gen_out_legacy = model.generate(**inputs, do_sample=True, max_new_tokens=256)\n         set_seed(0)\n-        gen_out = model.generate(\n-            **inputs, do_sample=True, max_new_tokens=256, past_key_values=DynamicCache(model.config.num_hidden_layers)\n-        )\n+        gen_out = model.generate(**inputs, do_sample=True, max_new_tokens=256, past_key_values=DynamicCache())\n         self.assertListEqual(gen_out_legacy.tolist(), gen_out.tolist())\n \n         decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n@@ -270,9 +268,7 @@ def test_dynamic_cache_batched(self):\n             model.device\n         )\n \n-        gen_out = model.generate(\n-            **inputs, do_sample=False, max_new_tokens=10, past_key_values=DynamicCache(model.config.num_hidden_layers)\n-        )\n+        gen_out = model.generate(**inputs, do_sample=False, max_new_tokens=10, past_key_values=DynamicCache())\n         decoded = tokenizer.batch_decode(gen_out, skip_special_tokens=True)\n         expected_text = [\"A sequence: 1, 2, 3, 4, 5, 6, 7, 8,\", \"A sequence: A, B, C, D, E, F, G, H\"]\n         self.assertListEqual(decoded, expected_text)"
        }
    ],
    "stats": {
        "total": 169,
        "additions": 113,
        "deletions": 56
    }
}