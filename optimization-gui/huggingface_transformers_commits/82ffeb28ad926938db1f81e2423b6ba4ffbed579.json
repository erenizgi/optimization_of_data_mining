{
    "author": "ErfanBaghaei",
    "message": "Add Top-H decoding (entropy-bounded truncation) as a LogitsWarper for text generation (#40837)\n\n* init\n\n* added TopH\n\n* Update TopH logits_process.py\n\n* Update logits_process.py\n\n* Update test_logits_process.py\n\n* Update test_logits_process.py\n\n* added test No. 4\n\n* Resolving __init__.py issues\n\n* Resolving configuration_utils.py Issues\n\n* Resolving logits_process.py Issues\n\n* Resolving utils.py Issues\n\n* Resolving test_logits_process.py Issues\n\n* Resolving __init__.py issues\n\n* Resolving logits_process.py Issues\n\n* Resolving __init__.py issues\n\n* Updated Docs\n\n* Updated Docstring\n\n* style: autoformat with make fixup\n\n* Fixing Docstring\n\n* Update logits_process.py removed defaults\n\n* Variable H name -> cumulative_entropy\n\n* Using torch.distributions.Categorical\n\n* Improve torch_dtype checks (#40808)\n\n* Improve torch_dtype checks\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Apply suggestions from code review\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Add VideoProcessors to auto-backend requirements (#40843)\n\n* add it\n\n* fix existing ones\n\n* add perception to auto_mapping...\n\n* Adds Causal Conv 1D kernel for mamba models (#40765)\n\n* add kernel\n\n* make style\n\n* keep causal-conv1d\n\n* small fix\n\n* small fix\n\n* fix modular converter\n\n* modular fix + lazy loading\n\n* revert changes modular\n\n* nit\n\n* hub kernels update\n\n* update\n\n* small nit\n\n* Update no split modules in T5Gemma model (#40810)\n\n* Update no split modules in T5Gemma model\n\n* Update no_split_modules also for T5Gemma modular\n\n* Remove model_split_percents from test cases\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Replace image classification loss functions to `self.loss_function` (#40764)\n\n* Fix the misalignment between the l2norm in GDN of Qwen3-Next and the implementation in the FLA library. (#40842)\n\n* align torch implementation of gdn with fla.\n\n* fix fla import.\n\n* fix\n\n* remove unused attr\n\n* fixes\n\n* strictly align l2norm in Qwen3-Next with FLA implementation.\n\n---------\n\nCo-authored-by: bozheng-hit <dsoul0621@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* Fixes for continuous batching (#40828)\n\n* Fix for CB attn mask and refactor\n\n* Tests for CB (not all passing)\n\n* Passing tests and a logger fix\n\n* Fixed the KV metrics that were broken when we moved to hybrid alloc\n\n* Fix circular import and style\n\n* Added tests for FA\n\n* Unfolded test to have device expectations\n\n* Fixes for H100\n\n* more fixes for h100\n\n* H100 are good\n\n* Style\n\n* Adding some comments from #40831\n\n* Rename test\n\n* Avoid 1 letter variables\n\n* Dictonnary is only removed during kwargs\n\n* Test for supported sample\n\n* Fix a unvoluntary slice\n\n* Fixes for non-sliced inputs and small example improvments\n\n* Slice inputs is more understandabe\n\n* Style\n\n* [tests] re-enable aria fast tests (#40846)\n\n* rise from the dead\n\n* test\n\n* [SAM2] Fix inconsistent results with original implementation with input boxes (#40800)\n\n* Fix inconsistencies with box input inference with original repo\n\n* remove print\n\n* always pad\n\n* fix modular\n\n* [Sam2Video] Fix video inference with batched boxes and add test (#40797)\n\nfix video inference with batched boxes and add test\n\n* add: differential privacy research model (#40851)\n\n* VaultGemma\n\n* Removing Sequence and Token classification models. Removing integration tests for now\n\n* Remove pass-only modular code. style fixes\n\n* Update vaultgemma.md\n\n* Update docs/source/en/model_doc/vaultgemma.md\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/vaultgemma.md\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Add links to model doc\n\n* Correct model doc usage examples\n\n* Updating model doc to describe differences from Gemma 2\n\n* Update model_doc links\n\n* Adding integration tests\n\n* style fixes\n\n* repo consistency\n\n* attribute exception\n\n---------\n\nCo-authored-by: Amer <amersinha@gmail.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* [test] Fix test_eager_matches_sdpa incorrectly skipped (#40852)\n\n* ouput_attentions in typed kwargs\n\n* correct typing in GenericForTokenClassification\n\n* improve\n\n* [tests] move generative tests away from `test_modeling_common.py` (#40854)\n\nmove tests\n\n* [generate] Always use decoder config to init cache (#40772)\n\n* mega derp\n\n* fix\n\n* always use the decoder\n\n* Use checkpoint in auto_class_docstring (#40844)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix TrainingArguments.parallelism_config NameError with accelerate<1.10.1 (#40818)\n\nFix ParallelismConfig type for accelerate < 1.10.1\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Redirect MI355 CI results to dummy dataset (#40862)\n\n* [Bug fix #40813] Fix base_model_tp_plan of Starcoder2 model. (#40814)\n\nSigned-off-by: greg-kwasniewski1 <213329731+greg-kwasniewski1@users.noreply.github.com>\n\n* [docstrings / type hints] Update outdated annotations for `past_key_values`  (#40803)\n\n* some fixes\n\n* nits\n\n* indentation\n\n* indentation\n\n* a bunch of type hints\n\n* bulk changes\n\n* fix florence kwargs  (#40826)\n\n* fix: XIELU act parameters not being casted to correct dtype (#40812)\n\n* Update model tags and integration references in bug report (#40881)\n\n* [Qwen3 Next] Use numerically stable `rsqrt` (#40848)\n\nuse numerically stable inverse\n\n* Adding Support for Qwen3-VL Series (#40795)\n\n* add qwen3vl series\n\n* make fixup\n\n* fix import\n\n* re-protect import\n\n* fix it finally (need to merge main into the branch)\n\n* skip processor test (need the checkpoint)\n\n* oups typo\n\n* simplify modular\n\n* remove unecesary attr\n\n* fix layer\n\n* remove unused rope_deltas args\n\n* reuse image def\n\n* remove unnesesary imports\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\n\n* [`VaultGemma`] Update expectations in integration tests (#40855)\n\n* fix tests\n\n* style\n\n* Fix modular consistency (#40883)\n\n* reapply modular\n\n* add missing one\n\n* üî¥ Move variable output controls to `_prepare_generation_config ` (#40715)\n\n* move checks to validate steps where possible\n\n* fix csm and other models that override _sample\n\n* ops dia you again\n\n* opsie\n\n* joao review\n\n* Move variable output controls to `prepare_inputs_for_generation`\n\n* fix a bunch of models\n\n* back to basics\n\n* final touches\n\n* Clarify passing is_causal in sdpa_attention_paged_forward (#40838)\n\n* Correctly pass is_causal in sdpa_attention_paged_forward\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Improve typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Add comment\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Improve comments\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Revert typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Use torch.expm1 and torch.log1p for better numerical results (#40860)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Add Fast PromptDepthAnything Processor (#40602)\n\n* Test & import setup\n\n* First version passing tests\n\n* Ruff\n\n* Dummy post processing\n\n* Add numerical test\n\n* Adjust\n\n* Doc\n\n* Ruff\n\n* remove unused arg\n\n* Refine interpolation method and push test script\n\n* update bench\n\n* Comments\n\n* Update src/transformers/models/auto/image_processing_auto.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* Remove benchmrk script\n\n* Update docstrings\n\n* Update src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* Update src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* doc\n\n* further process kwargs\n\n* remove it\n\n* remove\n\n* Remove to dict\n\n* remove crop middle\n\n* Remove param specific handling\n\n* Update testing logic\n\n* remove ensure multiple of as kwargs\n\n* fix formatting\n\n* Remove none default and get image size\n\n* Move stuff to _preprocess_image_like_inputs and refacto\n\n* Clean\n\n* ruff\n\n* End of file & comments\n\n* ruff again\n\n* Padding fixed\n\n* Remove comments to pass tests\n\n* Remove prompt depth from kwargs\n\n* Adjust output_size logic\n\n* Docstring for preprocess\n\n* auto_docstring for preprocess\n\n* pass as an arg\n\n* update test batched\n\n* stack images\n\n* remove prompt scale to meter\n\n* return tensors back in preprocess\n\n* remove copying of images\n\n* Update behavior to match old processoer\n\n* Fix batch size of tests\n\n* fix test and fast\n\n* Fix slow processor\n\n* Put tests back to pytorch\n\n* remove check and modify batched tests\n\n* test do_pad + slow processor fix\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\n\n* Fix deta loading & dataclass (#40878)\n\n* fix\n\n* fix 2\n\n* Remove dict branch of attention_mask in sdpa_attention_paged_forward (#40882)\n\nRemove dict branch of attention_mask\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* üåê [i18n-KO] Translated smolvlm.md to Korean (#40414)\n\n* fix: manual edits\n\n* Apply suggestions from code review\n\n* Update docs/source/ko/model_doc/smolvlm.md\n\n* Update docs/source/ko/model_doc/smolvlm.md\n\n* Update docs/source/ko/model_doc/smolvlm.md\n\n* Update docs/source/ko/model_doc/smolvlm.md\n\n* Update docs/source/ko/_toctree.yml\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* üåê [i18n-KO] Translated `imageprocessor.md` to Korean (#39557)\n\n* feat: manual translation\n\n* docs: fix ko/_toctree.yml\n\n* Apply suggestions from code review\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\n\n* Update docs/source/ko/image_processors.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* [generate] remove docs of a feature that no longer exists (#40895)\n\n* Make debugging failing tests (check and update expect output values) easier üî•  (#40727)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Fixing the call to kernelize (#40628)\n\n* fix\n\n* style\n\n* overload train and eval\n\n* add getter and setter\n\n* Fix getter  regression (#40824)\n\n* test things\n\n* style\n\n* move tests to a sane place\n\n* Fix flaky `Gemma3nAudioFeatureExtractionTest::test_dither` (#40902)\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* [cache] Merge static sliding and static chunked layer (#40893)\n\n* merge\n\n* get rid of tensors in get_mask_sizes!!\n\n* remove branch\n\n* add comment explanation\n\n* re-add the class with deprecation cycle\n\n* Harmonize CacheLayer names (#40892)\n\n* unify naming\n\n* style\n\n* doc as well\n\n* post rebase fix\n\n* style\n\n* style\n\n* revert\n\n* [cache] Only use scalars in `get_mask_sizes` (#40907)\n\n* remove tensor ops\n\n* style\n\n* style\n\n* Set seed for `Glm4vIntegrationTest` (#40905)\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Add Olmo3 model (#40778)\n\n* transformers add-new-model-like for Olmo3\n\n* Implement modular Olmo3\n\n* Update Olmo3 tests\n\n* Copy Olmo2 weight converter to Olmo3\n\n* Implement Olmo3 weight converter\n\n* Fix code quality errors\n\n* Remove unused import\n\n* Address rope-related PR comments\n\n* Update Olmo3 model doc with minimal details\n\n* Fix Olmo3 rope test failure\n\n* Fix 7B integration test\n\n* remove dummy EncodingFast (#40864)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Improve module name handling for local custom code (#40809)\n\n* Improve module name handling for local custom code\n\n* Use `%lazy` in logging messages\n\n* Revert \"Use `%lazy` in logging messages\"\n\nThis reverts commit 5848755d5805e67177c5218f351c0ac852df9340.\n\n* Add notes for sanitization rule in docstring\n\n* Remove too many underscores\n\n* Update src/transformers/dynamic_module_utils.py\n\n* Update src/transformers/dynamic_module_utils.py\n\n---------\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* Remove `runner_map` (#40880)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* disable `test_fast_is_faster_than_slow` (#40909)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* [gemma3] `Gemma3ForConditionalGeneration` compatible with assisted generation (#40791)\n\n* gemma3vision compatible with assisted generation\n\n* docstring\n\n* BC\n\n* docstring\n\n* failing checks\n\n* make fixup\n\n* apply changes to modular\n\n* misc fixes\n\n* is_initialized\n\n* fix poor rebase\n\n* [generate] misc fixes (#40906)\n\nmisc fixes\n\n* üî¥Make `center_crop` fast equivalent to slow (#40856)\n\nmake center_crop fast equivalent to slow\n\n* Fix dtype in Paligemma (#40912)\n\n* fix dtypes\n\n* fix copies\n\n* delete unused attr\n\n* [Docs] Adding documentation of MXFP4 Quantization (#40885)\n\n* adding mxfp4 quantization docs\n\n* review suggestions\n\n* Apply suggestions from code review\n\nCo-authored-by: vb <vaibhavs10@gmail.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: vb <vaibhavs10@gmail.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Processor load with multi-processing (#40786)\n\npush\n\n* [Llama4] Remove `image_sizes` arg and deprecate `vision_feature_layer` (#40832)\n\n* Remove unused arg\n\n* deprecate\n\n* revrt one change\n\n* get set go\n\n* version correction\n\n* fix\n\n* make style\n\n* comment\n\n* Fix #40067: Add dedicated UMT5 support to GGUF loader (config, tokenizer, test) (#40218)\n\n* Fix #40067 : add UMT5 support in GGUF loader (config, tokenizer, test)\n\n* chore: fix code formatting and linting issues\n\n* refactor: move UMT5 GGUF test to quantization directory and clean up comments\n\n* chore: trigger CI pipeline\n\n* refactor(tests): Move UMT5 Encoder GGUF test to GgufModelTests. This consolidates the new test into the main class for consistency.\n\n* Add regression check to UMT5 encoder GGUF test\n\nVerify encoder output against reference tensor values with appropriate tolerances for stability.\n\n* Update tests/quantization/ggml/test_ggml.py\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Update tests/quantization/ggml/test_ggml.py\r\n\r\nremove comments\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* [torchao safetensors] renaming get_state_dict function (#40774)\n\nrenaming get_state_dict function\n\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\n\n* Adding activation kernels (#40890)\n\n* first commit\n\n* add mode\n\n* revert modeling\n\n* add compile\n\n* rm print\n\n* Minor fix for #40727 (#40929)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Add support for Florence-2 training (#40914)\n\n* Support training florence2\n\n* update doc and testing model to florence-community\n\n* fix florence-2 test, use head dim 16 instead of 8 for fa2\n\n* skip test_sdpa_can_dispatch_on_flash\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\n\n* Add LongCat-Flash (#40730)\n\n* working draft for LongCat\n\n* BC changes to deepseek_v3 for modular\n\n* format\n\n* various modularities\n\n* better tp plan\n\n* better init\n\n* minor changes\n\n* make modular better\n\n* clean up patterns\n\n* Revert a couple of modular commits, because we won't convert in the end\n\n* make things explicit.\n\n* draft test\n\n* toctree, tests and imports\n\n* drop\n\n* woops\n\n* make better things\n\n* update test\n\n* update\n\n* fixes\n\n* style and CI\n\n* convert stuff\n\n* up\n\n* ah, yes, that\n\n* enable gen tests\n\n* fix cache shape in test (sum of 2 things)\n\n* fix tests\n\n* comments\n\n* re-Identitise\n\n* minimize changes\n\n* better defaults\n\n* modular betterment\n\n* fix configuration, add documentation\n\n* fix init\n\n* add integration tests\n\n* add info\n\n* simplify\n\n* update slow tests\n\n* fix\n\n* style\n\n* some additional long tests\n\n* cpu-only long test\n\n* fix last tests?\n\n* urg\n\n* cleaner tests why not\n\n* fix\n\n* improve slow tests, no skip\n\n* style\n\n* don't upcast\n\n* one skip\n\n* finally fix parallelism\n\n* [DOC] Add missing dates in model cards (#40922)\n\nadd missing dates\n\n* [models] remove unused `import torch.utils.checkpoint`  (#40934)\n\n* Intel CPU dockerfile (#40806)\n\n* upload intel cpu dockerfile\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update cpu dockerfile\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* update label name\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* docs(i18n): Correct the descriptive text in the README_zh-hans.md (#40941)\n\n* Fix trainer tests (#40823)\n\n* fix liger\n\n* fix\n\n* more\n\n* fix\n\n* fix hp\n\n* fix\n\n---------\n\nCo-authored-by: Matej Sirovatka <54212263+S1ro1@users.noreply.github.com>\n\n* Fix `Glm4vMoeIntegrationTest` (#40930)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Raise error instead of warning when using meta device in from_pretrained (#40942)\n\n* raise instead of warning\n\n* add timm\n\n* remove\n\n* Consistent naming for images kwargs (#40834)\n\n* use consistent naming for padding\n\n* no validation on pad size\n\n* add warnings\n\n* fix\n\n* fox copies\n\n* another fix\n\n* fix some tests\n\n* fix more tests\n\n* fix lasts tests\n\n* fix copies\n\n* better docstring\n\n* delete print\n\n* Remove nested import logic for torchvision (#40940)\n\n* remove nested import logic for torchvision\n\n* remove unnecessary protected imports\n\n* remove unnecessarry protected import in modular (and modeling)\n\n* fix wrongly remove protected imports\n\n* Fix `Glm4vModelTest::test_eager_matches_fa2_generate` (#40947)\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Update expected values for some `test_speculative_generation` (#40949)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Standardize audio embedding function name for audio multimodal models (#40919)\n\n* Standardize audio embedding function name for audio multimodal models\n\n* PR review\n\n* Add FlexOlmo model (#40921)\n\n* transformers add-new-model-like\n\n* Add FlexOlmo implementation\n\n* Update FlexOlmo docs\n\n* Set default tokenization for flex olmo\n\n* Update FlexOlmo tests\n\n* Update attention comment\n\n* Remove unneeded use of `sliding_window`\n\n* Don't list dropout in eager_paged_attention_forward (#40924)\n\nRemove dropout argument\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Update expected values for one more `test_speculative_generation` after #40949 (#40967)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* FIX(trainer): ensure final checkpoint is saved when resuming training (#40347)\n\n* fix(trainer): ensure final checkpoint is saved when resuming training\n\n* add test\n\n* make style && slight fix of test\n\n* make style again\n\n* move test code to test_trainer\n\n* remove outdated test file\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: rangehow <rangehow@foxmail.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* Add new model LFM2-VL (#40624)\n\n* Add LFM2-VL support\n\n* add tests\n\n* linting, formatting, misc review changes\n\n* add siglip2 to auto config and instantiate it in lfm2-vl configuration\n\n* decouple image processor from processor\n\n* remove torch import from configuration\n\n* replace | with Optional\n\n* remove layer truncation from modeling file\n\n* fix copies\n\n* update everything\n\n* fix test case to use tiny model\n\n* update the test cases\n\n* fix finally the image processor and add slow tests\n\n* fixup\n\n* typo in docs\n\n* fix tests\n\n* the doc name uses underscore\n\n* address comments from Yoni\n\n* delete tests and unsuffling\n\n* relative import\n\n* do we really handle imports better now?\n\n* fix test\n\n* slow tests\n\n* found a bug in ordering + slow tests\n\n* fix copies\n\n* dont run compile test\n\n---------\n\nCo-authored-by: Anna <anna@liquid.ai>\nCo-authored-by: Anna Banaszak <48625325+ankke@users.noreply.github.com>\n\n* Fix outdated version checks of accelerator (#40969)\n\n* Fix outdated version checks of accelerator\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix outdated version checks of accelerator\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Use `skip_predictor=True` in vjepa2 `get_vision_features` (#40966)\n\nuse skip_predictor in vjepa2 `get_vision_features`\n\n* [Trainer] Fix DP loss (#40799)\n\n* fix\n\n* style\n\n* Fix fp16\n\n* style\n\n---------\n\nCo-authored-by: Matej Sirovatka <54212263+S1ro1@users.noreply.github.com>\n\n* [timm_wrapper] better handling of \"Unknown model\" exception in timm (#40951)\n\n* fix(timm): Add exception handling for unknown Gemma3n model\n\n* nit: Let‚Äôs cater to this specific issue\n\n* nit: Simplify error handling\n\n* Fix Issue #39030: AutoTokenizer.from_pretrained does not propagate token (#40956)\n\n* fix merge conflicts\n\n* change token typing\n\n---------\n\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-27-253.ec2.internal>\n\n* [tests] Really use small models in all fast tests (#40945)\n\n* start\n\n* xcodec\n\n* chameleon\n\n* start\n\n* layoutlm2\n\n* layoutlm\n\n* remove skip\n\n* oups\n\n* timm_wrapper\n\n* add default\n\n* doc\n\n* consistency\n\n* Add captured actual outputs to CI artifacts (#40965)\n\n* fix\n\n* fix\n\n* Remove `# TODO: ???` as it make me `???`\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Revert change in `compile_friendly_resize` (#40645)\n\nfix\n\n* Track the CI (model) jobs that don't produce test output files (process being killed etc.) (#40981)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Using torch.distributions.Categorical\n\n* Remove `set_model_tester_for_less_flaky_tests` (#40982)\n\nremove\n\n* Benchmarking v2 GH workflows (#40716)\n\n* WIP benchmark v2 workflow\n\n* Container was missing\n\n* Change to sandbox branch name\n\n* Wrong place for image name\n\n* Variable declarations\n\n* Remove references to file logging\n\n* Remove unnecessary step\n\n* Fix deps install\n\n* Syntax\n\n* Add workdir\n\n* Add upload feature\n\n* typo\n\n* No need for hf_transfer\n\n* Pass in runner\n\n* Runner config\n\n* Runner config\n\n* Runner config\n\n* Runner config\n\n* Runner config\n\n* mi325 caller\n\n* Name workflow runs properly\n\n* Copy-paste error\n\n* Add final repo IDs and schedule\n\n* Review comments\n\n* Remove wf params\n\n* Remove parametrization from worfkflow files\n\n* Fix callers\n\n* Change push trigger to pull_request + label\n\n* Add back schedule event\n\n* Push to the same dataset\n\n* Simplify parameter description\n\n* üî¥[`Attention`] Bert-based Models Attention Refactor (#38301)\n\n* clean start to bert refactor\n\n* some test fixes\n\n* style\n\n* fix last tests\n\n* be strict on positional embeddings, fixup according tests\n\n* cache support\n\n* more cache fixes, new causal API\n\n* simplify masks, fix tests for gen\n\n* flex attn, static cache support, round of fixes\n\n* ?\n\n* this time\n\n* style\n\n* fix flash attention tests, flex attention requires torch 2.7.x to work with multiple classes (as recompile strats force a size call which is wrongly interpreted before)\n\n* roberta\n\n* fixup sdpa remains\n\n* attention split, simplify args and kwargs, better typing\n\n* fix encoder decoder\n\n* fix test\n\n* modular roberta\n\n* albert\n\n* data2vectext, making it modular tomorrow\n\n* modular data2vec text\n\n* tmp disable\n\n* xmod + cache position fixes\n\n* whoops\n\n* electra + markuplm, small fixes\n\n* remove wrong copy\n\n* xlm_roberta + some embedding fixes\n\n* roberta prelayernorm\n\n* RemBert: remove copy, maybe doing it later\n\n* ernie\n\n* fix roberta offloading\n\n* camembert\n\n* copy fixes\n\n* bert generation + fixes on eager\n\n* xlm roberta xl\n\n* bridgetower (text) + seamlessv2 copy fixes\n\n* rocbert + small fixes\n\n* whoops\n\n* small round of fixups\n\n* NOTE: kernels didnt load with an earlier version, some fixup (needs another look bc cross deps)\n\n* the end of the tunnel?\n\n* fixup nllbmoe + style\n\n* we dont need this anymore\n\n* megatron bert is barely used, low prio skip for now\n\n* Modernize bert (template for others)\n\nNOTE: trying to push this through, might be overdue if not in time possible\n\n* check inputs for all others (if checkmarked)\n\n* fix bridgetower\n\n* style\n\n* fix encoder decoder (partially but cause found and fix also, just needs to be done for everything else)\n\n* proper fix for bert to force intermediate dict outputs\n\n* propagate to others\n\n* style\n\n* xlm roberta xl investigation, its the layernorm...\n\n* mobile bert\n\n* revert this, might cause issues with composed models\n\n* review\n\n* style\n\n* Remove [[autodoc]] refs to TF/Flax objects (#40996)\n\n* remove refs\n\n* more\n\n* ENH: Enable readline support for transformers chat (#40911)\n\nENH Enable readline support for chat\n\nThis small change enables GNU readline support for the transformers chat\ncommand. This includes, among others:\n\n- advanced navigation and editing: ctrl + a ctrl + e alt + b alt + f\n  ctrl + k alt + d etc.\n- navigate and search history: arrow up/down ctrl + p ctrl + n  ctrl + r\n- undo: ctrl + _\n- clear screen: ctrl + l\n\nImplementation\n\nAlthough it may look strange, just importing readline is enough to\nenable it in Python, see:\n\nhttps://docs.python.org/3/library/functions.html#input\n\nAs readline is not available on some\nplatforms (https://docs.python.org/3/library/readline.html), the import\nis guarded.\n\nReadline should work on Linux, MacOS, and with WSL, I'm not sure about\nWindows though. Ideally, someone can give it a try. It's possible that\nWindows users would have to install\npyreadline (https://pypi.org/project/pyreadline3/).\n\n* [testing] test `num_hidden_layers` being small in model tester (#40992)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* blt wip (#38579)\n\n* blt wip\n\n* cpu version\n\n* cpu friendly with full entropy model (real time patching)\n\n* adding config file instead of args file\n\n* enable MPS\n\n* refactoring unused code\n\n* single config class in config file\n\n* inherit from PreTrainedModel\n\n* refactor LMTransformer --> BLTPatcher\n\n* add conversion script\n\n* load from new checkpoing with form_pretrained\n\n* fixed demo from_pretrained\n\n* clean up\n\n* clean a few comments\n\n* cleanup folder\n\n* clean up dir\n\n* cleaned up modeling further\n\n* rename classes\n\n* adding transformers Attention class and RotaryEmbedding class\n\n* exchanged blt modules for transformers modules: attention, rotary_emb, create_causal_mask, etc\n\n* seperate out patcher config, update modeling and conversion script\n\n* rename vars to be more transformers-like\n\n* rm unused functions\n\n* adding cross attention from transformers\n\n* pass arg\n\n* rename weights\n\n* updated conversion script\n\n* overwritten commit! fixing PR\n\n* apply feedback\n\n* adding BLTRMSNorm like Llama\n\n* add repeat_kv and eager_attention_forward copied from\n\n* BLTMLP identical to MllamTextMLP\n\n* clean up some args'\n\n* more like mllama, but busier inits\n\n* BLTTransformerLayer config\n\n* decoder, encoder, global configs\n\n* wip working on modular file\n\n* cleaning up patch and configs\n\n* clean up patcher helpers\n\n* clean up patcher helpers further\n\n* clean up\n\n* some config renaming\n\n* clean up unused configs\n\n* clean up configs\n\n* clean up configs\n\n* update modular\n\n* clean\n\n* update demo\n\n* config more like mllama, seperated subconfigs from subdicts\n\n* read from config instead of self args\n\n* update demo file\n\n* model weights to causal lm weights\n\n* missed file\n\n* added tied weights keys\n\n* BLTForCausalLM\n\n* adding files after add-new-model-like\n\n* update demo\n\n* working on tests\n\n* first running integration tests\n\n* added integration tests\n\n* adding tokenization tests, integration tests, and cleaned up tokenization file, + ruff\n\n* tokenizer clean up\n\n* modular file\n\n* fixing rebase\n\n* ruff\n\n* adding correct basemodel output and updating config with checkpoint vals (for testing)\n\n* BLTModelTests git status\n\n* enabling inputs_embeds, although won't be equal to input_ids since need ids for patching logic\n\n* fix sdpa == causal tests\n\n* fix small model test and some gradient checkpointing\n\n* skip training GC tests\n\n* fix test\n\n* updated modular\n\n* update modular\n\n* ruff\n\n* adding modular + modeling\n\n* modular\n\n* more modern is_casual check\n\n* cleaning up modular\n\n* more modular reduction\n\n* ruff\n\n* modular fix\n\n* fix styling\n\n* return 2\n\n* return 2\n\n* fix some tests\n\n* fix bltcrossattention after modular break\n\n* some fixes / feedback\n\n* try cache generate fix\n\n* try cache generate fix\n\n* fix generate tests\n\n* attn_impl workaround\n\n* refactoring to use recent TransformersKwargs changes\n\n* fix hidden_states shape test\n\n* refactor to new outputs\n\n* simplify outputs a bit\n\n* rm unneeded decoderlayer overwriting\n\n* rename blt\n\n* forgot tokenizer test renamed\n\n* Reorder\n\n* Reorder\n\n* working on modular\n\n* updates from modular\n\n* new modular\n\n* ruff and such\n\n* update pretrainedmodel modular\n\n* using cohere2 apply_rotary_pos_emb\n\n* small changes\n\n* apply feedback r2\n\n* fix cross_attention\n\n* apply more feedback\n\n* update modeling fix\n\n* load submodules from pretrainedmodel\n\n* set initializer_range to subconfigs\n\n* rm cross_attnetion_states pass when not needed\n\n* add 7b projection layer support\n\n* check repo\n\n* make copies\n\n* lost cohere2 rotate_half\n\n* ruff\n\n* copies?\n\n* don't tie weights for submodules\n\n* tie weights setting\n\n* check docstrings\n\n* apply feedback\n\n* rebase\n\n* rebased modeling\n\n* update docs\n\n* applying feedback\n\n* few more fixes\n\n* fix can_record_outputs\n\n* fast tokenizer\n\n* no more modulelist\n\n* tok auto\n\n* rm tokenizersss\n\n* fix docs\n\n* ruff\n\n* fix after rebase\n\n* fix test, configs are not subscriptable\n\n---------\n\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-168-30.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-161-103.ec2.internal>\nCo-authored-by: Lysandre <hi@lysand.re>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-174-36.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-164-45.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-173-121.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-160-103.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-161-178.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-162-79.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-169-239.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-167-111.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-160-100.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-161-153.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-166-15.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-165-131.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-161-138.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-174-215.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-172-142.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-172-147.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-164-0.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-163-58.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-165-202.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-166-244.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-174-186.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-160-192.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-162-14.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-171-249.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-164-75.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-161-78.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-163-134.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-162-180.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-175-241.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-160-225.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-167-9.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-168-34.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-166-68.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-167-175.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-170-160.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-168-95.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-172-73.ec2.internal>\n\n* [docs] rm stray tf/flax autodocs references (#40999)\n\nrm tf references\n\n* [`RMSNorm`] Fix rms norm init for models that center around 1 (#40796)\n\n* fix\n\n* fixup inits\n\n* oops\n\n* fixup gemma\n\n* fixup modular order\n\n* how does this keep happen lol\n\n* vaultgemma is new i forgot\n\n* remove init check\n\n* Make `EfficientLoFTRModelTest` faster (#41000)\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Fix typoes in src and tests (#40845)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix more dates in model cards and wrong modalities in _toctree.yml (#40955)\n\n* Fix model cards and modalities in toctree\n\n* fix new models\n\n* RUFF fix on CI scripts (#40805)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* fix dict like init for ModelOutput (#41002)\n\n* fix dict like init\n\n* style\n\n* üö® [v5] remove generate output retrocompatibility aliases (#40998)\n\nremove old type aliases\n\n* [tests] update `test_left_padding_compatibility` (and minimize overwrites) (#40980)\n\n* update test (and overwrites)\n\n* better test comment\n\n* 0 as a default for\n\n* Patch more `unittest.case.TestCase.assertXXX` methods (#41008)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* üö® [v5] remove deprecated entry point (#40997)\n\n* remove old entry point\n\n* update references to transformers-cli\n\n* üö® [lightglue] fix: matches order changed because of early stopped indices (#40859)\n\n* fix: bug that made early stop change order of matches\n\n* fix: applied code suggestion\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* fix: applied code suggestion to modular\n\n* fix: integration tests\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Fix `PhimoeIntegrationTest` (#41007)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Fix Glm4v test (#41011)\n\nfix\n\n* Update after #41007 (#41014)\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Fix benchmark runner argument name (#41012)\n\n* Adding support for Qwen3Omni (#41025)\n\n* Add Qwen3Omni\n\n* make fix-copies, import properly\n\n* nit\n\n* fix wrong setup. Why was audio_token_id renamed ?\n\n* upds\n\n* more processing fixes\n\n* yup\n\n* fix more generation tests\n\n* down to 1?\n\n* fix import issue\n\n* style, update check repo\n\n* up\n\n* fix quality at my best\n\n* final quality?\n\n* fix doc building\n\n* FINAL COMMIT: SKIP IMPORTANT BUT FAILING TESTS FOR MERGE\n\n* SKIP THE TEMPLATE ONE\n\n---------\n\nCo-authored-by: lvyuanjun.lyj <lvyuanjun.lyj@alibaba-inc.com>\nCo-authored-by: Arthur <arthur.zucker@gmail.com>\n\n* Making compute_loss_func always take priority in Trainer (#40632)\n\n* logger warn, if-else logic improved\n\n* redundant if condition fix\n\n* Modify Qwen3Omni parameter name since VL changed it (#41045)\n\nModify parameter name since VL changed it\n\nCo-authored-by: lvyuanjun.lyj <lvyuanjun.lyj@alibaba-inc.com>\n\n* Fix Qwen video tests (#41049)\n\nfix test\n\n* [testing] Fix `qwen2_audio` (#41018)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Fix typing of tuples (#41028)\n\n* Fix tuple typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Remove optax (#41030)\n\nRemove optax dep\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix typos in English/Chinese documentation (#41031)\n\n* Fix typos and formatting in English docs\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix typos and formatting in Chinese docs\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Use torch.autocast (#40975)\n\n* Use torch.autocast\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Format code\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* docs: improved RoPE function Docstrings (#41004)\n\n* docs: improved RoPE functuon docstrings\n\n* Update src/transformers/modeling_rope_utils.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Fix condition for emitting warning when generation exceeds max model length (#40775)\n\ncorrect warning when generation exceeds max model length\n\nSigned-off-by: Yannick Schnider <yannick.schnider1@ibm.com>\n\n* Fix outdated torch version check (#40925)\n\nUpdate torch minimum version check to 2.2\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Remove doc of tf and flax (#41029)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Add Whole Word Masking and Padding Strategy to DataCollatorForLanguageModeling (#39485)\n\n* Add whole word masking\n\n* Vectorize whole word masking functions\n\n* Unit test whole word masking\n\n* Remove support for TF in whole word masking\n\n* [testing] Fix `seed_oss` (#41052)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* Update tests/models/seed_oss/test_modeling_seed_oss.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Remove repeated import (#40937)\n\n* Remove repeated import\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix conflict\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Simplify unnecessary Optional typing (#40839)\n\nRemove Optional\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Add write token for uploading benchmark results to the Hub (#41047)\n\n* Separate write token for Hub upload\n\n* Address review comments\n\n* Address review comments\n\n* Ci utils (#40978)\n\n* Add CI reports dir to gitignore\n\n* Add utils to run local CI\n\n* Review compliance\n\n* Style\n\n* License\n\n* Remove <frameworkcontent> and <pt> tags from documentation (#41055)\n\n* Remove <frameworkcontent> and <pt> tags\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Revert changes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Update docs/source/en/model_doc/madlad-400.md\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* Fix CI jobs being all red üî¥ (false positive) (#41059)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Update quantization CI (#41068)\n\n* fix\n\n* new everything\n\n* fix\n\n* [i18n-bn] Add Bengali language README file (#40935)\n\n* [i18n-bn] Add Bengali language README file and update links in existing language files\n\n* Update Bengali README for clarity and consistency in model descriptions\n\n* Improve documentation and errors in Mamba2-based models (#41063)\n\n* fix bug in Mamba2 docs\n\n* correct 'because on of' issue\n\n* link to other Mamba2 model types\n\n* github URL is not changed\n\n* update error message in generated files\n\n* Update team member list for some CI workflows (#41094)\n\n* update list\n\n* update list\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* fix crash when using chat to send 2+ request to gptoss (#40536)\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\n\n* Minor addition, no split modules for VideoMAEE (#41051)\n\n* added no split modules\n\n* fixed typo\n\n---------\n\nCo-authored-by: Raushan Turganbay <raushan@huggingface.co>\n\n* Switch to `python:3.10-slim` for CircleCI docker images (#41067)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Fix argument name in benchmarking script (#41086)\n\n* Fix argument name in benchmarking script\n\n* Adjust vars\n\n* Remove mention of TensorFlow/Flax/JAX from English documentation (#41058)\n\nRemove mention of TensorFlow from English documentation\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix typos in documentation (#41087)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix typing (#40788)\n\n* Fix optional typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix optional typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix schema typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix typing\n\n* Fix typing\n\n* Fix typing\n\n* Fix typing\n\n* Use np.ndarray\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Format code\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Use np.ndarray\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Improve typing\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix quote string of np.ndarray\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix code\n\n* Format\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Remove unused arguments (#40916)\n\n* Fix unused arguments\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Remove tf and flax from Chinese documentation (#41057)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* fix wrong height and width when read video use torchvision (#41091)\n\n* docs: Fix Tool Use links and remove dead RAG links (#41104)\n\ndocs: Fix tool use links. Remove dead RAG links. Fix style\n\n* üö® [generate] update paligemma mask updates (and other assisted generation-related fixes) (#40917)\n\n* tmp\n\n* fix modular inheritance\n\n* nit\n\n* paligemma 1 doesn't have swa\n\n* use same pattern as in models with hybrid layers\n\n* PR comments\n\n* helium also needs layer_typed (bc it relies on gemma)\n\n* paligemma/gemma3: same mask creation fn in fwd and generate\n\n* propagate changes to helium (gemma-based)\n\n* tmp commit\n\n* slow paligemma tests passing, let's see what breaks\n\n* fix test_left_padding_compatibility\n\n* tmp commit\n\n* tmp commit\n\n* rebase error\n\n* docs\n\n* reduce diff\n\n* like this?\n\n* t5gemma\n\n* better comment\n\n* shorter diff\n\n* exception\n\n* ffs type\n\n* optional\n\n* shorter modular_gemma.py\n\n* helium model actually needs no changes -- the tester is the issue\n\n* t5gemma modular config\n\n* a few more modular; paligemma BC\n\n* fix processor issues?\n\n* rm config exception\n\n* lift warning in gemma\n\n* [tests] gpt2 + `CausalLMModelTester` (#41003)\n\n* tmp commit\n\n* tmp commit\n\n* tmp commit\n\n* rm old GPT2ModelTester\n\n* nit bug\n\n* add facilities for encoder-decoder tests; add comments on ALL overwrites/extra fns\n\n* vision_encoder_decoder\n\n* Fix `_get_test_info` for inherited tests (#41106)\n\n* fix _get_test_info\n\n* fix patched\n\n* add comment\n\n* ruff\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Remove bad test skips (#41109)\n\n* remove bad skips\n\n* remove more\n\n* fix inits\n\n* Format empty lines and white space in markdown files. (#41100)\n\n* Remove additional white space and empty lines from markdown files\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Add empty lines around code\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Update ruff to 0.13.1 + target Python 3.10 + apply fixes (#37809)\n\nUpdate ruff to 0.13.1 target it to Python 3.10 and apply its fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\n\n* üö® [V5] Remove deprecated training arguments  (#41017)\n\n* Remove deprecated training arguments from V5\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Remove deprecated training arguments from V5\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix comments\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix code\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Support loading LFM2 GGUF (#41111)\n\n* add gguf config mapping for lfm2\n\n* add lfm2 tensor process to unsqueeze conv weights\n\n* adjust values from gguf config to HF config\n\n* add test for lfm2 gguf\n\n* ruff\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* [torchao safetensors] integrate torchao safetensors support with transformers  (#40735)\n\n* enable torchao safetensors\n\n* enable torchao safetensors support\n\n* add more version checking\n\n* [Qwen3-next] Fix dimension mismatch in torch_chunk_gated_delta_rule and torch_recurrent_gated_delta_rule (#40963) (#41036)\n\n* fix mismatched dims for qwen3 next\n\n* propagate changes\n\n* chore: renamed tot_heads to total_sequence_length\n\n* Apply suggestion from @vasqu\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* minor fix to modular qwen3 next file\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Fix the error where a keyword argument appearing before *args (#41099)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Fix broken `` expressions in markdown files (#41113)\n\nFix broken expressions in markdown files\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Remove self-assignment (#41062)\n\n* Remove self-assignment\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Update src/transformers/integrations/flash_paged.py\n\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* Clear pass\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Clear pass\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Clear pass\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\n\n* üö®Refactor: Update text2text generation pipelines to use max_new_tokens‚Ä¶ (#40928)\n\n* Refactor: Update text2text generation pipelines to use max_new_tokens and resolve max_length warning\n\n* docs(text2text_generation): Êõ¥Êñ∞ÂèÇÊï∞Ê≥®Èáä‰ª•ÂèçÊò†Áé∞‰ª£ÁîüÊàêÂÆûË∑µ\n\nÂ∞Ümax_lengthÂèÇÊï∞Ê≥®ÈáäÊõ¥Êñ∞‰∏∫max_new_tokensÔºå‰ª•Á¨¶ÂêàÁé∞‰ª£ÁîüÊàêÂÆûË∑µ‰∏≠ÊåáÂÆöÁîüÊàêÊñ∞tokenÊï∞ÈáèÁöÑÊ†áÂáÜÂÅöÊ≥ï\n\n* refactor(text2text_generation): Remove outdated input validation logic\n\n* docs(text2text_generation): Revert incorrectly modified comment\n\n* docs(text2text_generation): Revert incorrectly modified comment\n\n* Fixed MXFP4 model storage issue (#41118)\n\n* Fixed loading LongT5 from legacy checkpoints (#40724)\n\n* Fixed loading LongT5 from legacy checkpoints\n\n* Adapted the fix to work with missing lm_head\n\n* dummy commit (#41133)\n\n* dummy commit, nothing interesting\n\n* dummy commit, nothing interesting\n\n* dummy commit, nothing interesting\n\n* dummy commit, nothing interesting\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Fix loading logic flaw with regards to unexpected and missing keys (#40850)\n\n* Unexpected keys should be ignored at load with device map\n\n* remove them all\n\n* fix logic flaw\n\n* fix\n\n* simplify\n\n* style\n\n* fix\n\n* revert caching allocator change\n\n* add other test\n\n* add nice doc\n\n---------\n\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\n\n* Using torch.distributions.Categorical\n\n* Resolving logits_process.py Issues\n\n* style: autoformat with make fixup\n\n* Update logits_process.py removed defaults\n\n* Variable H name -> cumulative_entropy\n\n* Resolving format error\n\n* Correction of the loop variables in logit processor\n\n* Vectorized the loop in logits_process\n\n* formatted  logits_process\n\n* paper reference and stopping rule comment logits_process\n\n* Trigger CI rerun\n\n* Update logits_process.py\n\n* added test_TopH_example_integration\n\n* added test_TopH_example_integration\n\n* Update README.md\n\n* Restore CI config to match main (remove accidental changes)\n\n* Restore CI config to match upstream main (no diffs)\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\nSigned-off-by: greg-kwasniewski1 <213329731+greg-kwasniewski1@users.noreply.github.com>\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\nSigned-off-by: Yannick Schnider <yannick.schnider1@ibm.com>\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\nCo-authored-by: ArminAzizi98 <147081650+ArminAzizi98@users.noreply.github.com>\nCo-authored-by: Yuanyuan Chen <cyyever@outlook.com>\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@huggingface.co>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\nCo-authored-by: Yuchao Zhang <418121364@qq.com>\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\nCo-authored-by: Bo Zheng <368586905@qq.com>\nCo-authored-by: bozheng-hit <dsoul0621@gmail.com>\nCo-authored-by: Cyril Vallez <cyril.vallez@gmail.com>\nCo-authored-by: R√©mi Ouazan <83456801+remi-or@users.noreply.github.com>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\nCo-authored-by: Amer <amersinha@gmail.com>\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\nCo-authored-by: Albert Villanova del Moral <8515462+albertvillanova@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: √Åkos Hadnagy <akos@ahadnagy.com>\nCo-authored-by: Grzegorz Kwasniewski <213329731+greg-kwasniewski1@users.noreply.github.com>\nCo-authored-by: NanoCode012 <nano@axolotl.ai>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: ËâæÂäõÂèØ <178652170+thalahors@users.noreply.github.com>\nCo-authored-by: JJJYmmm <92386084+JJJYmmm@users.noreply.github.com>\nCo-authored-by: Manuel de Prada Corral <6536835+manueldeprada@users.noreply.github.com>\nCo-authored-by: Samuel Barry <127697809+SamuelBarryCS@users.noreply.github.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: HyunZ118 <156191095+HyunZ118@users.noreply.github.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\nCo-authored-by: Shane A <shanea@allenai.org>\nCo-authored-by: Xuehai Pan <XuehaiPan@pku.edu.cn>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\nCo-authored-by: Raushan Turganbay <raushan@huggingface.co>\nCo-authored-by: Aritra Roy Gosthipaty <aritra.born2fly@gmail.com>\nCo-authored-by: vb <vaibhavs10@gmail.com>\nCo-authored-by: Yaswanth Gali <82788246+yaswanth19@users.noreply.github.com>\nCo-authored-by: Akshay Babbar <priv.akshay@outlook.com>\nCo-authored-by: liangel-02 <liangel@meta.com>\nCo-authored-by: Duc-Viet Hoang <vietyb00@gmail.com>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\nCo-authored-by: jiqing-feng <jiqing.feng@intel.com>\nCo-authored-by: lilin-1 <256404019@qq.com>\nCo-authored-by: Matej Sirovatka <54212263+S1ro1@users.noreply.github.com>\nCo-authored-by: Jack <32371937+jackzhxng@users.noreply.github.com>\nCo-authored-by: Rangehow <88258534+rangehow@users.noreply.github.com>\nCo-authored-by: rangehow <rangehow@foxmail.com>\nCo-authored-by: Anna <anna@liquid.ai>\nCo-authored-by: Anna Banaszak <48625325+ankke@users.noreply.github.com>\nCo-authored-by: Hamish Scott <41787553+hamishs@users.noreply.github.com>\nCo-authored-by: Harshal Janjani <75426551+harshaljanjani@users.noreply.github.com>\nCo-authored-by: Branden <brandenkmurray@gmail.com>\nCo-authored-by: Ubuntu <ubuntu@ip-172-31-27-253.ec2.internal>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Ita Zaporozhets <31893021+itazap@users.noreply.github.com>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-168-30.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-161-103.ec2.internal>\nCo-authored-by: Lysandre <hi@lysand.re>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-174-36.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-164-45.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-173-121.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-160-103.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-161-178.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-162-79.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-169-239.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-167-111.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-160-100.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-161-153.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-166-15.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-165-131.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-161-138.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-174-215.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-172-142.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-172-147.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-164-0.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-163-58.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-165-202.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-166-244.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-174-186.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-160-192.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-162-14.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-171-249.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-164-75.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-161-78.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-163-134.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-162-180.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-175-241.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-160-225.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-167-9.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-168-34.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-166-68.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-167-175.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-170-160.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-168-95.ec2.internal>\nCo-authored-by: ita.zaporozhets@huggingface.co <ita_zaporozhets@ip-26-0-172-73.ec2.internal>\nCo-authored-by: StevenBucaille <steven.bucaille@gmail.com>\nCo-authored-by: BakerBunker <17872844+BakerBunker@users.noreply.github.com>\nCo-authored-by: lvyuanjun.lyj <lvyuanjun.lyj@alibaba-inc.com>\nCo-authored-by: Arthur <arthur.zucker@gmail.com>\nCo-authored-by: Ayush <ayushtanwar1729@gmail.com>\nCo-authored-by: Ryan Mullins <ryan@ryanmullins.org>\nCo-authored-by: Yannick Schnider <Yannick.Schnider1@ibm.com>\nCo-authored-by: Ralph Gleaton <70818603+rjgleaton@users.noreply.github.com>\nCo-authored-by: Saidur Rahman Pulok <59414463+saidurpulok@users.noreply.github.com>\nCo-authored-by: Nick Doiron <ndoiron@mapmeld.com>\nCo-authored-by: Wang, Yi <yi.a.wang@intel.com>\nCo-authored-by: Duygu Altinok <duygu.altinok12@gmail.com>\nCo-authored-by: Jinde.Song <juude.song@gmail.com>\nCo-authored-by: hbenoit <60629420+HaroldBenoit@users.noreply.github.com>\nCo-authored-by: nnul <107971634+notkisk@users.noreply.github.com>\nCo-authored-by: YangKai0616 <kai.yang@intel.com>\nCo-authored-by: Karol Szustakowski <61427290+Szustarol@users.noreply.github.com>\nCo-authored-by: souvikku <107592858+souvikku@users.noreply.github.com>",
    "sha": "82ffeb28ad926938db1f81e2423b6ba4ffbed579",
    "files": [
        {
            "sha": "aa27aa366b1975f7c48eb7b094bd52661fa9c7d6",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/82ffeb28ad926938db1f81e2423b6ba4ffbed579/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/82ffeb28ad926938db1f81e2423b6ba4ffbed579/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=82ffeb28ad926938db1f81e2423b6ba4ffbed579",
            "patch": "@@ -153,6 +153,9 @@ generation.\n [[autodoc]] TemperatureLogitsWarper\n     - __call__\n \n+[[autodoc]] TopHLogitsWarper\n+    - __call__\n+\n [[autodoc]] TopKLogitsWarper\n     - __call__\n "
        },
        {
            "sha": "72b62bdd3cfd3245a4a1485bbbeb4ddb6c2bbfd4",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/82ffeb28ad926938db1f81e2423b6ba4ffbed579/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82ffeb28ad926938db1f81e2423b6ba4ffbed579/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=82ffeb28ad926938db1f81e2423b6ba4ffbed579",
            "patch": "@@ -422,6 +422,7 @@\n             \"SynthIDTextWatermarkingConfig\",\n             \"SynthIDTextWatermarkLogitsProcessor\",\n             \"TemperatureLogitsWarper\",\n+            \"TopHLogitsWarper\",\n             \"TopKLogitsWarper\",\n             \"TopPLogitsWarper\",\n             \"TypicalLogitsWarper\",\n@@ -586,6 +587,7 @@\n     from .generation import TemperatureLogitsWarper as TemperatureLogitsWarper\n     from .generation import TextIteratorStreamer as TextIteratorStreamer\n     from .generation import TextStreamer as TextStreamer\n+    from .generation import TopHLogitsWarper as TopHLogitsWarper\n     from .generation import TopKLogitsWarper as TopKLogitsWarper\n     from .generation import TopPLogitsWarper as TopPLogitsWarper\n     from .generation import TypicalLogitsWarper as TypicalLogitsWarper"
        },
        {
            "sha": "92ef3184e773cb73a498135ea4b211fe591ef8df",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/82ffeb28ad926938db1f81e2423b6ba4ffbed579/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82ffeb28ad926938db1f81e2423b6ba4ffbed579/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=82ffeb28ad926938db1f81e2423b6ba4ffbed579",
            "patch": "@@ -67,6 +67,7 @@\n         \"SuppressTokensAtBeginLogitsProcessor\",\n         \"SynthIDTextWatermarkLogitsProcessor\",\n         \"TemperatureLogitsWarper\",\n+        \"TopHLogitsWarper\",\n         \"TopKLogitsWarper\",\n         \"TopPLogitsWarper\",\n         \"TypicalLogitsWarper\",\n@@ -153,6 +154,7 @@\n             SuppressTokensLogitsProcessor,\n             SynthIDTextWatermarkLogitsProcessor,\n             TemperatureLogitsWarper,\n+            TopHLogitsWarper,\n             TopKLogitsWarper,\n             TopPLogitsWarper,\n             TypicalLogitsWarper,"
        },
        {
            "sha": "35b98b22a8e7895e00ad43d441557b86b2ce9240",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/82ffeb28ad926938db1f81e2423b6ba4ffbed579/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82ffeb28ad926938db1f81e2423b6ba4ffbed579/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=82ffeb28ad926938db1f81e2423b6ba4ffbed579",
            "patch": "@@ -165,6 +165,12 @@ class GenerationConfig(PushToHubMixin):\n             Minimum token probability, which will be scaled by the probability of the most likely token. It must be a\n             value between 0 and 1. Typical values are in the 0.01-0.2 range, comparably selective as setting `top_p` in\n             the 0.99-0.8 range (use the opposite of normal `top_p` values).\n+        top_h (`float`, *optional*):\n+            Entropy budget scaling factor, which controls how much of the distribution‚Äôs entropy is preserved when sampling.\n+            Must be a value between 0 and 1. At each step, tokens are sorted by probability, and the smallest prefix of tokens\n+            is kept whose *renormalized* entropy is less than or equal to `top_h` times the entropy of the full distribution.\n+            Smaller values (e.g., 0.2‚Äì0.5) lead to more focused, deterministic outputs, while values closer to 1.0 allow more\n+            randomness and diversity. Typical values are in the 0.3‚Äì0.6 range.\n         typical_p (`float`, *optional*, defaults to 1.0):\n             Local typicality measures how similar the conditional probability of predicting a target token next is to\n             the expected conditional probability of predicting a random token next, given the partial text already\n@@ -354,6 +360,7 @@ def __init__(self, **kwargs):\n         self.top_k = kwargs.pop(\"top_k\", 50)\n         self.top_p = kwargs.pop(\"top_p\", 1.0)\n         self.min_p = kwargs.pop(\"min_p\", None)\n+        self.top_h = kwargs.pop(\"top_h\", None)\n         self.typical_p = kwargs.pop(\"typical_p\", 1.0)\n         self.epsilon_cutoff = kwargs.pop(\"epsilon_cutoff\", 0.0)\n         self.eta_cutoff = kwargs.pop(\"eta_cutoff\", 0.0)\n@@ -578,6 +585,8 @@ def validate(self, strict=False):\n                 minor_issues[\"top_p\"] = greedy_wrong_parameter_msg.format(flag_name=\"top_p\", flag_value=self.top_p)\n             if self.min_p is not None:\n                 minor_issues[\"min_p\"] = greedy_wrong_parameter_msg.format(flag_name=\"min_p\", flag_value=self.min_p)\n+            if self.top_h is not None:\n+                minor_issues[\"top_h\"] = greedy_wrong_parameter_msg.format(flag_name=\"top_h\", flag_value=self.top_h)\n             if self.typical_p is not None and self.typical_p != 1.0:\n                 minor_issues[\"typical_p\"] = greedy_wrong_parameter_msg.format(\n                     flag_name=\"typical_p\", flag_value=self.typical_p"
        },
        {
            "sha": "63940b17d8197f4c9c20ef0472a838413241a08f",
            "filename": "src/transformers/generation/logits_process.py",
            "status": "modified",
            "additions": 106,
            "deletions": 0,
            "changes": 106,
            "blob_url": "https://github.com/huggingface/transformers/blob/82ffeb28ad926938db1f81e2423b6ba4ffbed579/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82ffeb28ad926938db1f81e2423b6ba4ffbed579/src%2Ftransformers%2Fgeneration%2Flogits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Flogits_process.py?ref=82ffeb28ad926938db1f81e2423b6ba4ffbed579",
            "patch": "@@ -581,6 +581,112 @@ def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> to\n         return scores_processed\n \n \n+class TopHLogitsWarper(LogitsProcessor):\n+    \"\"\"\n+    [`LogitsProcessor`] that implements Top-H sampling, a decoding method which adaptively selects a subset of\n+    high-probability tokens based on entropy and cumulative probability constraints.\n+\n+    This method dynamically determines how many tokens to keep by analyzing the entropy difference of the selected\n+    distribution, thereby balancing exploration and exploitation. It ensures that generated text maintains both\n+    diversity and coherence.\n+\n+    Reference:\n+    For details, see *Top-H Decoding: Adapting the Creativity and Coherence with Bounded Entropy in Text Generation*\n+    (NeurIPS 2025): https://arxiv.org/abs/2509.02510\n+\n+    Args:\n+        top_h (`float`):\n+            Scaling coefficient for the entropy-based threshold (`tau`). Must be in the range `(0, 1]`.\n+\n+        filter_value (`float`, *optional*, defaults to -inf):\n+            All filtered values will be set to this float value.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import AutoTokenizer, AutoModelForCausalLM\n+\n+    >>> model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n+    >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-3.1-8B\")\n+\n+    >>> inputs = tokenizer(\"A sequence: 1, 2\", return_tensors=\"pt\")\n+\n+    >>> outputs = model.generate(**inputs, do_sample=True, top_h=0.4)\n+    >>> print(tokenizer.batch_decode(outputs, skip_special_tokens=True)[0])\n+    A sequence: 1, 2, 3, 4, 5, 6, 7, 8, 9\n+    ```\n+    \"\"\"\n+\n+    def __init__(self, top_h: float, filter_value: float = -float(\"Inf\")):\n+        super().__init__()\n+\n+        # input checks\n+        if not (0 < top_h <= 1):\n+            raise ValueError(\"`top_h` must be in the range (0, 1].\")\n+\n+        # Maximum number of top tokens to consider before applying the entropy-based filter.\n+        # Acts as a cap for efficiency and numerical stability ‚Äî increasing this allows more\n+        # tokens to be evaluated but may slow down generation. Default is 100.\n+        self.top_n = 100\n+\n+        self.top_h = top_h\n+        self.filter_value = filter_value\n+\n+    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor) -> torch.FloatTensor:\n+        \"\"\"\n+        Filters logits using Top-H sampling.\n+\n+        Args:\n+            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+                Input token IDs.\n+            scores (`torch.FloatTensor` of shape `(batch_size, vocab_size)`):\n+                Raw logits from the model.\n+\n+        Return:\n+            `torch.FloatTensor` of shape `(batch_size, vocab_size)`:\n+                Processed logits where invalid tokens are masked with `-inf`.\n+        \"\"\"\n+        batch_size, vocab_size = scores.shape\n+        device = scores.device\n+        keep_mask = torch.zeros((batch_size, vocab_size), dtype=torch.bool, device=device)\n+        top_n = min(self.top_n, vocab_size)\n+\n+        # 1. Get top-k logits and indices for the whole batch\n+        top_logits, top_idx = torch.topk(scores, top_n, dim=-1, largest=True, sorted=True)\n+\n+        # 2. Create a batch of categorical distributions\n+        dist = torch.distributions.Categorical(logits=top_logits)\n+        probs = dist.probs\n+        log_probs = torch.log(probs)  # dist.log_prob(idx)\n+\n+        # 3. Calculate the entropy-based threshold tau for the whole batch\n+        # We unsqueeze tau to enable broadcasting against the cumulative entropy tensor.\n+        tau = (dist.entropy() * self.top_h).unsqueeze(-1)\n+\n+        # 4. Calculate cumulative entropy using torch.cumsum\n+        # The individual entropy terms (-p * log(p)) are calculated for all top_n tokens at once.\n+        entropy_terms = -probs * log_probs\n+        cumulative_entropy = torch.cumsum(entropy_terms, dim=-1)\n+\n+        # 5. Determine which tokens to keep based on the stopping condition\n+        # Create a boolean mask for the top_n tokens.\n+        # Stopping rule: keep adding tokens in order of probability until the cumulative entropy\n+        # exceeds the threshold œÑ = H(p) * top_h. This ensures diversity (via entropy) while\n+        # guaranteeing at least the most probable token is always included.\n+        selection_mask = cumulative_entropy <= tau\n+        selection_mask[:, 0] = True\n+\n+        # 6. Update the final keep_mask for the entire batch in one operation\n+        # The scatter_ operation efficiently updates the keep_mask at the indices\n+        # specified by top_idx with the boolean values from selection_mask.\n+        keep_mask.scatter_(dim=1, index=top_idx, src=selection_mask)\n+\n+        # apply filtering\n+        scores_processed = scores.clone()\n+        scores_processed[~keep_mask] = self.filter_value\n+        return scores_processed\n+\n+\n class MinPLogitsWarper(LogitsProcessor):\n     \"\"\"\n     [`LogitsProcessor`] that performs min-p, i.e. keeps all tokens that are above a minimum probability, scaled by the"
        },
        {
            "sha": "4ab2f77db5793ae7346bbe45a8bb7748e409e4d9",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/82ffeb28ad926938db1f81e2423b6ba4ffbed579/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82ffeb28ad926938db1f81e2423b6ba4ffbed579/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=82ffeb28ad926938db1f81e2423b6ba4ffbed579",
            "patch": "@@ -93,6 +93,7 @@\n     SuppressTokensAtBeginLogitsProcessor,\n     SuppressTokensLogitsProcessor,\n     TemperatureLogitsWarper,\n+    TopHLogitsWarper,\n     TopKLogitsWarper,\n     TopPLogitsWarper,\n     TypicalLogitsWarper,\n@@ -1243,6 +1244,8 @@ def _get_logits_processor(\n             # all samplers can be found in `generation_utils_samplers.py`\n             if generation_config.temperature is not None and generation_config.temperature != 1.0:\n                 processors.append(TemperatureLogitsWarper(generation_config.temperature))\n+            if generation_config.top_h is not None:\n+                processors.append(TopHLogitsWarper(top_h=generation_config.top_h))\n             if generation_config.top_k is not None and generation_config.top_k != 0:\n                 processors.append(\n                     TopKLogitsWarper(top_k=generation_config.top_k, min_tokens_to_keep=min_tokens_to_keep)"
        },
        {
            "sha": "06531b52f5a5027b569220dfbc9ea9613d120f9f",
            "filename": "tests/generation/test_logits_process.py",
            "status": "modified",
            "additions": 90,
            "deletions": 0,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/82ffeb28ad926938db1f81e2423b6ba4ffbed579/tests%2Fgeneration%2Ftest_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82ffeb28ad926938db1f81e2423b6ba4ffbed579/tests%2Fgeneration%2Ftest_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_logits_process.py?ref=82ffeb28ad926938db1f81e2423b6ba4ffbed579",
            "patch": "@@ -49,6 +49,7 @@\n         SequenceBiasLogitsProcessor,\n         SynthIDTextWatermarkLogitsProcessor,\n         TemperatureLogitsWarper,\n+        TopHLogitsWarper,\n         TopKLogitsWarper,\n         TopPLogitsWarper,\n         TypicalLogitsWarper,\n@@ -394,6 +395,95 @@ def test_top_p_dist_warper(self):\n         # first batch should keep three tokens, second batch would keep only 1, but due to `min_tokens_to_keep=2` keeps 2.\n         self.assertListEqual((filtered_dist != 0.0).to(torch.long).sum(dim=-1).tolist(), [3, 2])\n \n+    def test_top_h_dist_warper(self):\n+        \"\"\"\n+        We construct small distributions where the expected kept set is obvious for a given alpha.\n+        We pass *log-probabilities* as \"scores\" so that softmax(scores) == original probabilities,\n+        matching the style in other warper tests (e.g., MinP).\n+        \"\"\"\n+\n+        input_ids = None\n+\n+        # --- Case 1: Highly peaked distribution -> small alpha keeps only the top-1\n+        dist1 = torch.log(\n+            torch.tensor(\n+                [[0.97, 0.01, 0.01, 0.01]],\n+                device=torch_device,\n+                dtype=torch.float,\n+            )\n+        )\n+        top_h_warp = TopHLogitsWarper(top_h=0.3)\n+        filtered_logits = top_h_warp(input_ids, dist1.clone())\n+        filtered_dist = torch.exp(filtered_logits)  # exp(-inf) -> 0\n+\n+        EXPECTED1 = torch.tensor(\n+            [[0.97, 0.0, 0.0, 0.0]],\n+            device=torch_device,\n+            dtype=torch.float,\n+        )\n+        torch.testing.assert_close(filtered_dist, EXPECTED1, rtol=1e-3, atol=1e-3)\n+\n+        # --- Case 2: Moderately skewed distribution -> alpha large enough to keep exactly top-2\n+        dist2 = torch.log(\n+            torch.tensor(\n+                [[0.4, 0.3, 0.2, 0.1]],  # entropy budget with alpha=0.7 yields 2-token prefix\n+                device=torch_device,\n+                dtype=torch.float,\n+            )\n+        )\n+        top_h_warp = TopHLogitsWarper(top_h=0.7)\n+        filtered_logits = top_h_warp(input_ids, dist2.clone())\n+        filtered_dist = torch.exp(filtered_logits)\n+\n+        EXPECTED2 = torch.tensor(\n+            [[0.4, 0.3, 0.0, 0.0]],\n+            device=torch_device,\n+            dtype=torch.float,\n+        )\n+        torch.testing.assert_close(filtered_dist, EXPECTED2, rtol=1e-3, atol=1e-3)\n+\n+        # --- Case 3: Uniform distribution -> alpha=1.0 keeps all tokens\n+        dist3 = torch.log(\n+            torch.tensor(\n+                [[0.25, 0.25, 0.25, 0.25]],\n+                device=torch_device,\n+                dtype=torch.float,\n+            )\n+        )\n+        top_h_warp = TopHLogitsWarper(top_h=1.0)\n+        filtered_logits = top_h_warp(input_ids, dist3.clone())\n+        filtered_dist = torch.exp(filtered_logits)\n+\n+        EXPECTED3 = torch.tensor(\n+            [[0.25, 0.25, 0.25, 0.25]],\n+            device=torch_device,\n+            dtype=torch.float,\n+        )\n+        torch.testing.assert_close(filtered_dist, EXPECTED3, rtol=1e-3, atol=1e-3)\n+\n+        # --- Case 4: Probabilities including 0 value\n+        dist4 = torch.log(\n+            torch.tensor(\n+                [[0.75, 0.25, 0.0, 0.0]],\n+                device=torch_device,\n+                dtype=torch.float,\n+            )\n+        )\n+        top_h_warp = TopHLogitsWarper(top_h=0.4)\n+        filtered_logits = top_h_warp(input_ids, dist4.clone())\n+        filtered_dist = torch.exp(filtered_logits)\n+\n+        EXPECTED4 = torch.tensor(\n+            [[0.75, 0.0, 0.0, 0.0]],\n+            device=torch_device,\n+            dtype=torch.float,\n+        )\n+        torch.testing.assert_close(filtered_dist, EXPECTED4, rtol=1e-3, atol=1e-3)\n+        # Processor should not change logits in-place\n+        top_h_warp = TopHLogitsWarper(top_h=0.5)\n+        out_again = top_h_warp(input_ids, dist3)\n+        assert not torch.all(out_again == dist3)\n+\n     def test_min_p_dist_warper(self):\n         input_ids = None\n         vocab_size = 10"
        },
        {
            "sha": "ed734b1d14a78bfd4b57951bbe32afaa02db43ee",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/82ffeb28ad926938db1f81e2423b6ba4ffbed579/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82ffeb28ad926938db1f81e2423b6ba4ffbed579/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=82ffeb28ad926938db1f81e2423b6ba4ffbed579",
            "patch": "@@ -3030,6 +3030,34 @@ def test_synthid_text_watermark_generation_mean_expected_bias(self):\n         )\n         self.assertTrue(torch.all(is_close))\n \n+    @slow\n+    def test_TopH_example_integration(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen2.5-3B\")\n+        model = AutoModelForCausalLM.from_pretrained(\"Qwen/Qwen2.5-3B\")\n+        tokenizer.pad_token = tokenizer.eos_token\n+        model.config.pad_token_id = tokenizer.pad_token_id\n+        encoder_input_str = \"Tell me a joke about a monkey.\"\n+        input_ids = tokenizer(encoder_input_str, return_tensors=\"pt\")\n+\n+        torch.manual_seed(0)\n+\n+        outputs = model.generate(\n+            **input_ids,\n+            eos_token_id=model.config.eos_token_id,\n+            do_sample=True,\n+            temperature=1.0,\n+            top_h=0.4,\n+            max_new_tokens=32,\n+            pad_token_id=tokenizer.pad_token_id,\n+        )\n+        outputs = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+        self.assertListEqual(\n+            outputs,\n+            [\n+                'Tell me a joke about a monkey. Why did the monkey go to the doctor? Because he was feeling a little \"tropic\"!'\n+            ],\n+        )\n+\n     @slow\n     def test_beam_search_example_integration(self):\n         # exactly the example provided in the docstrings of beam search, which previously"
        }
    ],
    "stats": {
        "total": 243,
        "additions": 243,
        "deletions": 0
    }
}