{
    "author": "matthewdouglas",
    "message": "docs: update bitsandbytes platform support (#41266)",
    "sha": "9f2d5666f8fda5b647e5f64dfc8ba1edd7a87a1e",
    "files": [
        {
            "sha": "81238c0707e7690872dcf182b4fd220678c38d0f",
            "filename": "docs/source/en/quantization/bitsandbytes.md",
            "status": "modified",
            "additions": 17,
            "deletions": 15,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f2d5666f8fda5b647e5f64dfc8ba1edd7a87a1e/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f2d5666f8fda5b647e5f64dfc8ba1edd7a87a1e/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md?ref=9f2d5666f8fda5b647e5f64dfc8ba1edd7a87a1e",
            "patch": "@@ -16,7 +16,7 @@ rendered properly in your Markdown viewer.\n \n # Bitsandbytes\n \n-The [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes) library provides quantization tools for LLMs through a lightweight Python wrapper around CUDA functions. It enables working with large models using limited computational resources by reducing their memory footprint.\n+The [bitsandbytes](https://github.com/bitsandbytes-foundation/bitsandbytes) library provides quantization tools for LLMs through a lightweight Python wrapper around hardware accelerator functions. It enables working with large models using limited computational resources by reducing their memory footprint.\n \n At its core, bitsandbytes provides:\n \n@@ -41,27 +41,29 @@ pip install --upgrade transformers accelerate bitsandbytes\n To compile from source, follow the instructions in the [bitsandbytes installation guide](https://huggingface.co/docs/bitsandbytes/main/en/installation).\n \n ## Hardware Compatibility\n-bitsandbytes is currently only supported on CUDA GPUs for CUDA versions 11.0 - 12.8. However, there's an ongoing multi-backend effort under development, which is currently in alpha. If you're interested in providing feedback or testing, check out the [bitsandbytes repository](https://github.com/bitsandbytes-foundation/bitsandbytes) for more information.\n+bitsandbytes is supported on NVIDIA GPUs for CUDA versions 11.8 - 13.0, Intel XPU, Intel Gaudi (HPU), and CPU. There is an ongoing effort to support additional platforms. If you're interested in providing feedback or testing, check out the [bitsandbytes repository](https://github.com/bitsandbytes-foundation/bitsandbytes) for more information.\n \n-### CUDA\n+### NVIDIA GPUs (CUDA)\n+\n+This backend is supported on Linux x86-64, Linux aarch64, and Windows platforms.\n \n | Feature | Minimum Hardware Requirement |\n |---------|-------------------------------|\n-| 8-bit optimizers | NVIDIA Maxwell (GTX 900 series, TITAN X, M40) or newer GPUs * |\n-| LLM.int8() | NVIDIA Turing (RTX 20 series, T4) or newer GPUs |\n-| NF4/FP4 quantization | NVIDIA Maxwell (GTX 900 series, TITAN X, M40) or newer GPUs * |\n+| 8-bit optimizers | NVIDIA Pascal (GTX 10X0 series, P100) or newer GPUs * |\n+| LLM.int8() | NVIDIA Turing (RTX 20X0 series, T4) or newer GPUs |\n+| NF4/FP4 quantization | NVIDIA Pascal (GTX 10X0 series, P100) or newer GPUs * |\n+\n+### Intel GPUs (XPU)\n+\n+This backend is supported on Linux x86-64 and Windows x86-64 platforms.\n+\n+### Intel Gaudi (HPU)\n \n-### Multi-backend\n+This backend is supported on Linux x86-64 for Gaudi2 and Gaudi3.\n \n-| Backend | Supported Versions | Python versions | Architecture Support | Status |\n-|---------|-------------------|----------------|---------------------|---------|\n-| AMD ROCm | 6.1+ | 3.10+ | minimum CDNA - gfx90a, RDNA - gfx1100 | Alpha |\n-| Apple Silicon (MPS) | WIP | 3.10+ | M1/M2 chips | Planned |\n-| Intel CPU | v2.4.0+ (ipex) | 3.10+ | Intel CPU | Alpha |\n-| Intel GPU | v2.4.0+ (ipex) | 3.10+ | Intel GPU | Experimental |\n-| Ascend NPU | 2.1.0+ (torch_npu) | 3.10+ | Ascend NPU | Experimental |\n+### CPU\n \n-> **Note:** Bitsandbytes is moving away from the multi-backend approach towards using [Pytorch Custom Operators](https://pytorch.org/tutorials/advanced/custom_ops_landing_page.html), as the main mechanism for supporting new hardware, and dispatching to the correct backend.\n+This backend is supported on Linux x86-64, Linux aarch64, and Windows x86-64 platforms.\n \n ## Quantization Examples\n "
        },
        {
            "sha": "0a8dee1e33aead398f9179cbefde73d3db915e02",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f2d5666f8fda5b647e5f64dfc8ba1edd7a87a1e/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f2d5666f8fda5b647e5f64dfc8ba1edd7a87a1e/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=9f2d5666f8fda5b647e5f64dfc8ba1edd7a87a1e",
            "patch": "@@ -27,7 +27,7 @@ Use the Space below to help you pick a quantization method depending on your har\n | [AQLM](./aqlm)                            | 游댮                   | 游릭              |     游릭     | 游댮        | 游댮                                 | 游릭              | 游릭              | 1/2          | 游릭               | 游릭                          | 游릭                      | https://github.com/Vahe1994/AQLM            |\n | [AutoRound](./auto_round)                 | 游댮                   | 游릭               | 游릭          |   游댮        |   游댮                                |   游릭              |   游댮               | 2/3/4/8      |    游댮              |       游릭                      |    游릭                       |      https://github.com/intel/auto-round                                       |\n | [AWQ](./awq)                              | 游댮                   | 游릭              | 游릭        | 游릭        | 游댮                                 | 游릭              | ?               | 4            | 游릭               | 游릭                          | 游릭                      | https://github.com/casper-hansen/AutoAWQ    |\n-| [bitsandbytes](./bitsandbytes)            | 游릭                   | 游리 |     游릭     | 游리 | 游댮                    | 游리 | 游릭 | 4/8          | 游릭               | 游릭                          | 游릭                      | https://github.com/bitsandbytes-foundation/bitsandbytes |\n+| [bitsandbytes](./bitsandbytes)            | 游릭                   | 游릭 |     游릭     | 游리 | 游리                    | 游릭 | 游릭 | 4/8          | 游릭               | 游릭                          | 游릭                      | https://github.com/bitsandbytes-foundation/bitsandbytes |\n | [compressed-tensors](./compressed_tensors) | 游댮                   | 游릭              |     游릭     | 游릭        | 游댮                                 | 游댮              | 游댮              | 1/8          | 游릭               | 游릭                          | 游릭                      | https://github.com/neuralmagic/compressed-tensors |\n | [EETQ](./eetq)                            | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | ?               | 8            | 游릭               | 游릭                          | 游릭                      | https://github.com/NetEase-FuXi/EETQ        |\n | [FP-Quant](./fp_quant)                          | 游릭                   | 游댮              | 游릭        | 游댮        | 游댮                                 | 游댮              | 游릭              | 4           | 游댮               | 游릭                          | 游릭                      | https://github.com/IST-DASLab/FP-Quant      |"
        },
        {
            "sha": "e2c7bdf27076031693e034f070f5761e489c0137",
            "filename": "docs/source/en/quantization/selecting.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f2d5666f8fda5b647e5f64dfc8ba1edd7a87a1e/docs%2Fsource%2Fen%2Fquantization%2Fselecting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f2d5666f8fda5b647e5f64dfc8ba1edd7a87a1e/docs%2Fsource%2Fen%2Fquantization%2Fselecting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fselecting.md?ref=9f2d5666f8fda5b647e5f64dfc8ba1edd7a87a1e",
            "patch": "@@ -26,7 +26,7 @@ Consider the quantization methods below for inference.\n \n | quantization method | use case |\n |---|---|\n-| bitsandbytes | ease of use and QLoRA fine-tuning on NVIDIA GPUs |\n+| bitsandbytes | ease of use and QLoRA fine-tuning on NVIDIA and Intel GPUs |\n | compressed-tensors | loading specific quantized formats (FP8, Sparse) |\n | GPTQModel or AWQ | good 4-bit accuracy with upfront calibration |\n | HQQ | fast on the fly quantization without calibration |"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 19,
        "deletions": 17
    }
}