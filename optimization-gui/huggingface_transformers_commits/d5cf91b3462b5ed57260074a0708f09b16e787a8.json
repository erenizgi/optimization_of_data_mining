{
    "author": "Rocketknight1",
    "message": "Separate chat templates into a single file (#33957)\n\n* Initial draft\r\n\r\n* Add .jinja file loading for processors\r\n\r\n* Add processor saving of naked chat template files\r\n\r\n* make fixup\r\n\r\n* Add save-load test for tokenizers\r\n\r\n* Add save-load test for tokenizers\r\n\r\n* stash commit\r\n\r\n* Try popping the file\r\n\r\n* make fixup\r\n\r\n* Pop the arg correctly\r\n\r\n* Pop the arg correctly\r\n\r\n* Add processor test\r\n\r\n* Fix processor code\r\n\r\n* stash commit\r\n\r\n* Processor clobbers child tokenizer's chat template\r\n\r\n* Processor clobbers child tokenizer's chat template\r\n\r\n* make fixup\r\n\r\n* Split processor/tokenizer files to avoid interactions\r\n\r\n* fix test\r\n\r\n* Expand processor tests\r\n\r\n* Rename arg to \"save_raw_chat_template\" across all classes\r\n\r\n* Update processor warning\r\n\r\n* Move templates to single file\r\n\r\n* Move templates to single file\r\n\r\n* Improve testing for processor/tokenizer clashes\r\n\r\n* Improve testing for processor/tokenizer clashes\r\n\r\n* Extend saving test\r\n\r\n* Test file priority correctly\r\n\r\n* make fixup\r\n\r\n* Don't pop the chat template file before the slow tokenizer gets a look\r\n\r\n* Remove breakpoint\r\n\r\n* make fixup\r\n\r\n* Fix error",
    "sha": "d5cf91b3462b5ed57260074a0708f09b16e787a8",
    "files": [
        {
            "sha": "c26463003670337a5e620d073ffe4dd53f6de934",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 38,
            "deletions": 13,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5cf91b3462b5ed57260074a0708f09b16e787a8/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5cf91b3462b5ed57260074a0708f09b16e787a8/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=d5cf91b3462b5ed57260074a0708f09b16e787a8",
            "patch": "@@ -44,7 +44,6 @@\n     TruncationStrategy,\n )\n from .utils import (\n-    CHAT_TEMPLATE_NAME,\n     PROCESSOR_NAME,\n     PushToHubMixin,\n     TensorType,\n@@ -527,18 +526,24 @@ def save_pretrained(self, save_directory, push_to_hub: bool = False, **kwargs):\n         # If we save using the predefined names, we can load using `from_pretrained`\n         # plus we save chat_template in its own file\n         output_processor_file = os.path.join(save_directory, PROCESSOR_NAME)\n-        output_chat_template_file = os.path.join(save_directory, CHAT_TEMPLATE_NAME)\n+        output_raw_chat_template_file = os.path.join(save_directory, \"chat_template.jinja\")\n+        output_chat_template_file = os.path.join(save_directory, \"chat_template.json\")\n \n         processor_dict = self.to_dict()\n         # Save `chat_template` in its own file. We can't get it from `processor_dict` as we popped it in `to_dict`\n         # to avoid serializing chat template in json config file. So let's get it from `self` directly\n         if self.chat_template is not None:\n-            chat_template_json_string = (\n-                json.dumps({\"chat_template\": self.chat_template}, indent=2, sort_keys=True) + \"\\n\"\n-            )\n-            with open(output_chat_template_file, \"w\", encoding=\"utf-8\") as writer:\n-                writer.write(chat_template_json_string)\n-            logger.info(f\"chat template saved in {output_chat_template_file}\")\n+            if kwargs.get(\"save_raw_chat_template\", False):\n+                with open(output_raw_chat_template_file, \"w\", encoding=\"utf-8\") as writer:\n+                    writer.write(self.chat_template)\n+                logger.info(f\"chat template saved in {output_raw_chat_template_file}\")\n+            else:\n+                chat_template_json_string = (\n+                    json.dumps({\"chat_template\": self.chat_template}, indent=2, sort_keys=True) + \"\\n\"\n+                )\n+                with open(output_chat_template_file, \"w\", encoding=\"utf-8\") as writer:\n+                    writer.write(chat_template_json_string)\n+                logger.info(f\"chat template saved in {output_chat_template_file}\")\n \n         # For now, let's not save to `processor_config.json` if the processor doesn't have extra attributes and\n         # `auto_map` is not specified.\n@@ -601,21 +606,23 @@ def get_processor_dict(\n         is_local = os.path.isdir(pretrained_model_name_or_path)\n         if os.path.isdir(pretrained_model_name_or_path):\n             processor_file = os.path.join(pretrained_model_name_or_path, PROCESSOR_NAME)\n-            chat_template_file = os.path.join(pretrained_model_name_or_path, \"chat_template.json\")\n \n         if os.path.isfile(pretrained_model_name_or_path):\n             resolved_processor_file = pretrained_model_name_or_path\n             # cant't load chat-template when given a file as pretrained_model_name_or_path\n             resolved_chat_template_file = None\n+            resolved_raw_chat_template_file = None\n             is_local = True\n         elif is_remote_url(pretrained_model_name_or_path):\n             processor_file = pretrained_model_name_or_path\n             resolved_processor_file = download_url(pretrained_model_name_or_path)\n             # can't load chat-template when given a file url as pretrained_model_name_or_path\n             resolved_chat_template_file = None\n+            resolved_raw_chat_template_file = None\n         else:\n             processor_file = PROCESSOR_NAME\n-            chat_template_file = CHAT_TEMPLATE_NAME\n+            chat_template_file = \"chat_template.json\"\n+            raw_chat_template_file = \"chat_template.jinja\"\n             try:\n                 # Load from local folder or from cache or download from model Hub and cache\n                 resolved_processor_file = cached_file(\n@@ -650,6 +657,21 @@ def get_processor_dict(\n                     subfolder=subfolder,\n                     _raise_exceptions_for_missing_entries=False,\n                 )\n+\n+                resolved_raw_chat_template_file = cached_file(\n+                    pretrained_model_name_or_path,\n+                    raw_chat_template_file,\n+                    cache_dir=cache_dir,\n+                    force_download=force_download,\n+                    proxies=proxies,\n+                    resume_download=resume_download,\n+                    local_files_only=local_files_only,\n+                    token=token,\n+                    user_agent=user_agent,\n+                    revision=revision,\n+                    subfolder=subfolder,\n+                    _raise_exceptions_for_missing_entries=False,\n+                )\n             except EnvironmentError:\n                 # Raise any environment error raise by `cached_file`. It will have a helpful error message adapted to\n                 # the original exception.\n@@ -664,8 +686,11 @@ def get_processor_dict(\n                 )\n \n         # Add chat template as kwarg before returning because most models don't have processor config\n-        chat_template = None\n-        if resolved_chat_template_file is not None:\n+        if resolved_raw_chat_template_file is not None:\n+            with open(resolved_raw_chat_template_file, \"r\", encoding=\"utf-8\") as reader:\n+                chat_template = reader.read()\n+            kwargs[\"chat_template\"] = chat_template\n+        elif resolved_chat_template_file is not None:\n             with open(resolved_chat_template_file, \"r\", encoding=\"utf-8\") as reader:\n                 text = reader.read()\n             chat_template = json.loads(text)[\"chat_template\"]\n@@ -696,7 +721,7 @@ def get_processor_dict(\n \n         if \"chat_template\" in processor_dict and processor_dict[\"chat_template\"] is not None:\n             logger.warning_once(\n-                \"Chat templates should be in a 'chat_template.json' file but found key='chat_template' \"\n+                \"Chat templates should be in a 'chat_template.jinja' file but found key='chat_template' \"\n                 \"in the processor's config. Make sure to move your template to its own file.\"\n             )\n "
        },
        {
            "sha": "0bfcc4aa30366540aeb58023842a37f564d09b5e",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5cf91b3462b5ed57260074a0708f09b16e787a8/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5cf91b3462b5ed57260074a0708f09b16e787a8/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=d5cf91b3462b5ed57260074a0708f09b16e787a8",
            "patch": "@@ -145,6 +145,7 @@ class EncodingFast:\n SPECIAL_TOKENS_MAP_FILE = \"special_tokens_map.json\"\n ADDED_TOKENS_FILE = \"added_tokens.json\"\n TOKENIZER_CONFIG_FILE = \"tokenizer_config.json\"\n+CHAT_TEMPLATE_FILE = \"chat_template.jinja\"\n \n # Fast tokenizers (provided by HuggingFace tokenizer's library) can be saved in a single file\n FULL_TOKENIZER_FILE = \"tokenizer.json\"\n@@ -1941,6 +1942,7 @@ def from_pretrained(\n                     \"tokenizer_config_file\": TOKENIZER_CONFIG_FILE,\n                     # tokenizer_file used to initialize a slow from a fast. Properly copy the `addedTokens` instead of adding in random orders\n                     \"tokenizer_file\": FULL_TOKENIZER_FILE,\n+                    \"chat_template_file\": CHAT_TEMPLATE_FILE,\n                 }\n                 vocab_files = {**cls.vocab_files_names, **additional_files_names}\n                 if \"tokenizer_file\" in vocab_files:\n@@ -2097,6 +2099,12 @@ def _from_pretrained(\n             config_tokenizer_class = None\n             init_kwargs = init_configuration\n \n+        # If an independent chat template file exists, it takes priority over template entries in the tokenizer config\n+        chat_template_file = resolved_vocab_files.pop(\"chat_template_file\", None)\n+        if chat_template_file is not None:\n+            with open(chat_template_file) as chat_template_handle:\n+                init_kwargs[\"chat_template\"] = chat_template_handle.read()  # Clobbers any template in the config\n+\n         if not _is_local:\n             if \"auto_map\" in init_kwargs:\n                 # For backward compatibility with odl format.\n@@ -2396,6 +2404,9 @@ def save_pretrained(\n         tokenizer_config_file = os.path.join(\n             save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + TOKENIZER_CONFIG_FILE\n         )\n+        chat_template_file = os.path.join(\n+            save_directory, (filename_prefix + \"-\" if filename_prefix else \"\") + CHAT_TEMPLATE_FILE\n+        )\n \n         tokenizer_config = copy.deepcopy(self.init_kwargs)\n \n@@ -2418,7 +2429,15 @@ def save_pretrained(\n             if isinstance(self.chat_template, dict):\n                 # Chat template dicts are saved to the config as lists of dicts with fixed key names.\n                 # They will be reconstructed as a single dict during loading.\n+                # We're trying to discourage chat template dicts, and they are always\n+                # saved in the config, never as single files.\n                 tokenizer_config[\"chat_template\"] = [{\"name\": k, \"template\": v} for k, v in self.chat_template.items()]\n+            elif kwargs.get(\"save_raw_chat_template\", False):\n+                with open(chat_template_file, \"w\", encoding=\"utf-8\") as f:\n+                    f.write(self.chat_template)\n+                logger.info(f\"chat template saved in {chat_template_file}\")\n+                if \"chat_template\" in tokenizer_config:\n+                    tokenizer_config.pop(\"chat_template\")  # To ensure it doesn't somehow end up in the config too\n             else:\n                 tokenizer_config[\"chat_template\"] = self.chat_template\n "
        },
        {
            "sha": "4c4f6fac49813fe6fb131a7fe6085fa5b0e93496",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5cf91b3462b5ed57260074a0708f09b16e787a8/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5cf91b3462b5ed57260074a0708f09b16e787a8/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=d5cf91b3462b5ed57260074a0708f09b16e787a8",
            "patch": "@@ -18,6 +18,7 @@\n import json\n import random\n import tempfile\n+from pathlib import Path\n from typing import Optional\n \n import numpy as np\n@@ -519,3 +520,27 @@ def test_prepare_and_validate_optional_call_args(self):\n             processor.prepare_and_validate_optional_call_args(\n                 *(f\"optional_{i}\" for i in range(num_optional_call_args + 1))\n             )\n+\n+    def test_chat_template_save_loading(self):\n+        processor = self.get_processor()\n+        existing_tokenizer_template = getattr(processor.tokenizer, \"chat_template\", None)\n+        processor.chat_template = \"test template\"\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            processor.save_pretrained(tmpdirname)\n+            self.assertTrue(Path(tmpdirname, \"chat_template.json\").is_file())\n+            self.assertFalse(Path(tmpdirname, \"chat_template.jinja\").is_file())\n+            reloaded_processor = self.processor_class.from_pretrained(tmpdirname)\n+            self.assertEqual(processor.chat_template, reloaded_processor.chat_template)\n+            # When we don't use single-file chat template saving, processor and tokenizer chat templates\n+            # should remain separate\n+            self.assertEqual(getattr(reloaded_processor.tokenizer, \"chat_template\", None), existing_tokenizer_template)\n+\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            processor.save_pretrained(tmpdirname, save_raw_chat_template=True)\n+            self.assertTrue(Path(tmpdirname, \"chat_template.jinja\").is_file())\n+            self.assertFalse(Path(tmpdirname, \"chat_template.json\").is_file())\n+            reloaded_processor = self.processor_class.from_pretrained(tmpdirname)\n+            self.assertEqual(processor.chat_template, reloaded_processor.chat_template)\n+            # When we save as single files, tokenizers and processors share a chat template, which means\n+            # the reloaded tokenizer should get the chat template as well\n+            self.assertEqual(reloaded_processor.chat_template, reloaded_processor.tokenizer.chat_template)"
        },
        {
            "sha": "ed09d800ad6dd51c7d34c68551148b2f0d47feef",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 53,
            "deletions": 14,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/d5cf91b3462b5ed57260074a0708f09b16e787a8/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d5cf91b3462b5ed57260074a0708f09b16e787a8/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=d5cf91b3462b5ed57260074a0708f09b16e787a8",
            "patch": "@@ -25,6 +25,7 @@\n import unittest\n from collections import OrderedDict\n from itertools import takewhile\n+from pathlib import Path\n from typing import TYPE_CHECKING, Any, Dict, List, Tuple, Union\n \n from parameterized import parameterized\n@@ -1107,13 +1108,29 @@ def test_chat_template(self):\n \n                 with tempfile.TemporaryDirectory() as tmp_dir_name:\n                     tokenizer.save_pretrained(tmp_dir_name)\n-                    tokenizer = tokenizer.from_pretrained(tmp_dir_name)\n+                    new_tokenizer = tokenizer.from_pretrained(tmp_dir_name)\n \n-                self.assertEqual(tokenizer.chat_template, dummy_template)  # Test template has persisted\n-                output = tokenizer.apply_chat_template(dummy_conversation, tokenize=False, return_dict=False)\n+                self.assertEqual(new_tokenizer.chat_template, dummy_template)  # Test template has persisted\n+                output = new_tokenizer.apply_chat_template(dummy_conversation, tokenize=False, return_dict=False)\n                 self.assertEqual(output, expected_output)  # Test output is the same after reloading\n                 # Check that no error raised\n-                tokenizer.apply_chat_template(dummy_conversation, tokenize=True, return_dict=False)\n+                new_tokenizer.apply_chat_template(dummy_conversation, tokenize=True, return_dict=False)\n+\n+                with tempfile.TemporaryDirectory() as tmp_dir_name:\n+                    tokenizer.save_pretrained(tmp_dir_name, save_raw_chat_template=True)\n+                    chat_template_file = Path(tmp_dir_name) / \"chat_template.jinja\"\n+                    self.assertTrue(chat_template_file.is_file())\n+                    self.assertEqual(chat_template_file.read_text(), dummy_template)\n+                    config_dict = json.loads((Path(tmp_dir_name) / \"tokenizer_config.json\").read_text())\n+                    # Assert the chat template is not in the config when it's saved as a separate file\n+                    self.assertNotIn(\"chat_template\", config_dict)\n+                    new_tokenizer = tokenizer.from_pretrained(tmp_dir_name)\n+\n+                self.assertEqual(new_tokenizer.chat_template, dummy_template)  # Test template has persisted\n+                output = new_tokenizer.apply_chat_template(dummy_conversation, tokenize=False, return_dict=False)\n+                self.assertEqual(output, expected_output)  # Test output is the same after reloading\n+                # Check that no error raised\n+                new_tokenizer.apply_chat_template(dummy_conversation, tokenize=True, return_dict=False)\n \n     @require_jinja\n     def test_chat_template_batched(self):\n@@ -1526,18 +1543,40 @@ def test_chat_template_dict_saving(self):\n         tokenizers = self.get_tokenizers()\n         for tokenizer in tokenizers:\n             with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n-                tokenizer.chat_template = {\"template1\": dummy_template_1, \"template2\": dummy_template_2}\n+                for save_raw_chat_template in (True, False):\n+                    tokenizer.chat_template = {\"template1\": dummy_template_1, \"template2\": dummy_template_2}\n+                    with tempfile.TemporaryDirectory() as tmp_dir_name:\n+                        # Test that save_raw_chat_template is ignored when there's a dict of multiple templates\n+                        tokenizer.save_pretrained(tmp_dir_name, save_raw_chat_template=save_raw_chat_template)\n+                        config_dict = json.load(open(os.path.join(tmp_dir_name, \"tokenizer_config.json\")))\n+                        # Assert that chat templates are correctly serialized as lists of dictionaries\n+                        self.assertEqual(\n+                            config_dict[\"chat_template\"],\n+                            [\n+                                {\"name\": \"template1\", \"template\": \"{{'a'}}\"},\n+                                {\"name\": \"template2\", \"template\": \"{{'b'}}\"},\n+                            ],\n+                        )\n+                        self.assertFalse(os.path.exists(os.path.join(tmp_dir_name, \"chat_template.jinja\")))\n+                        new_tokenizer = tokenizer.from_pretrained(tmp_dir_name)\n+                    # Assert that the serialized list is correctly reconstructed as a single dict\n+                    self.assertEqual(new_tokenizer.chat_template, tokenizer.chat_template)\n+\n+    @require_jinja\n+    def test_chat_template_file_priority(self):\n+        dummy_template1 = \"a\"\n+        dummy_template2 = \"b\"\n+        tokenizers = self.get_tokenizers()\n+        for tokenizer in tokenizers:\n+            with self.subTest(f\"{tokenizer.__class__.__name__}\"):\n                 with tempfile.TemporaryDirectory() as tmp_dir_name:\n-                    tokenizer.save_pretrained(tmp_dir_name)\n-                    config_dict = json.load(open(os.path.join(tmp_dir_name, \"tokenizer_config.json\")))\n-                    # Assert that chat templates are correctly serialized as lists of dictionaries\n-                    self.assertEqual(\n-                        config_dict[\"chat_template\"],\n-                        [{\"name\": \"template1\", \"template\": \"{{'a'}}\"}, {\"name\": \"template2\", \"template\": \"{{'b'}}\"}],\n-                    )\n+                    tokenizer.chat_template = dummy_template1\n+                    tokenizer.save_pretrained(tmp_dir_name, save_raw_chat_template=False)\n+                    with Path(tmp_dir_name, \"chat_template.jinja\").open(\"w\") as f:\n+                        f.write(dummy_template2)\n                     new_tokenizer = tokenizer.from_pretrained(tmp_dir_name)\n-                # Assert that the serialized list is correctly reconstructed as a single dict\n-                self.assertEqual(new_tokenizer.chat_template, tokenizer.chat_template)\n+                # Assert the file template clobbers any template in the config\n+                self.assertEqual(new_tokenizer.chat_template, dummy_template2)\n \n     def test_number_of_added_tokens(self):\n         tokenizers = self.get_tokenizers(do_lower_case=False)"
        }
    ],
    "stats": {
        "total": 162,
        "additions": 135,
        "deletions": 27
    }
}