{
    "author": "ChanderG",
    "message": "Setup loss_type in config at model init time (#34616)\n\n* setup loss_type in config at model init time\n\nensures no additional graph break introduced when torch.compile'ed\n\nfixes #34615\n\nSigned-off-by: ChanderG <mail@chandergovind.org>\n\n* lookup loss mapping at init time instead of manual setup\n\nSigned-off-by: ChanderG <mail@chandergovind.org>\n\n* remove redundant lookup at loss_function time\n\n* overwride losstype at init time\n\n---------\n\nSigned-off-by: ChanderG <mail@chandergovind.org>\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>",
    "sha": "4adc415b6de5b899a48379b6cba01bd930cb587d",
    "files": [
        {
            "sha": "8eb2d7439ef3c2abf0616a66619268c98f62ebb6",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 12,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/4adc415b6de5b899a48379b6cba01bd930cb587d/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4adc415b6de5b899a48379b6cba01bd930cb587d/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4adc415b6de5b899a48379b6cba01bd930cb587d",
            "patch": "@@ -1319,6 +1319,17 @@ def __init__(self, config: PretrainedConfig, *inputs, **kwargs):\n             )\n         self.config = config\n \n+        # for initialization of the loss\n+        loss_type = self.__class__.__name__\n+        if loss_type not in LOSS_MAPPING:\n+            loss_groups = f\"({'|'.join(LOSS_MAPPING)})\"\n+            loss_type = re.findall(loss_groups, self.__class__.__name__)\n+            if len(loss_type) > 0:\n+                loss_type = loss_type[0]\n+            else:\n+                loss_type = None\n+        self.loss_type = loss_type\n+\n         self.name_or_path = config.name_or_path\n         self.warnings_issued = {}\n         self.generation_config = GenerationConfig.from_model_config(config) if self.can_generate() else None\n@@ -5110,18 +5121,9 @@ def tplize(mod: torch.nn.Module) -> None:\n \n     @property\n     def loss_function(self):\n-        if getattr(self.config, \"loss_type\", None) is not None:\n-            loss_type = self.config.loss_type\n-        else:\n-            loss_type = self.__class__.__name__\n-            if loss_type not in LOSS_MAPPING:\n-                loss_groups = f\"({'|'.join(LOSS_MAPPING)})\"\n-                loss_type = re.findall(loss_groups, self.__class__.__name__)\n-                if len(loss_type) > 0:\n-                    loss_type = loss_type[0]\n-                else:\n-                    loss_type = None\n-        if loss_type is None or loss_type not in LOSS_MAPPING and getattr(self.config, \"loss_type\", None) is not None:\n+        loss_type = getattr(self, \"loss_type\", None)\n+\n+        if loss_type is None or loss_type not in LOSS_MAPPING:\n             logger.warning_once(\n                 f\"`loss_type={loss_type}` was set in the config but it is unrecognised.\"\n                 f\"Using the default loss: `ForCausalLMLoss`.\""
        }
    ],
    "stats": {
        "total": 26,
        "additions": 14,
        "deletions": 12
    }
}