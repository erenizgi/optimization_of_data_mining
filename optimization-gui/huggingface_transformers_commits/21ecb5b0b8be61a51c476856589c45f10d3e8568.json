{
    "author": "LysandreJik",
    "message": "Fix transformers serve following chat template output update",
    "sha": "21ecb5b0b8be61a51c476856589c45f10d3e8568",
    "files": [
        {
            "sha": "0f69fc15126c6f5f1cf7a5d21781ee50808dfe64",
            "filename": "src/transformers/cli/serve.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/21ecb5b0b8be61a51c476856589c45f10d3e8568/src%2Ftransformers%2Fcli%2Fserve.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/21ecb5b0b8be61a51c476856589c45f10d3e8568/src%2Ftransformers%2Fcli%2Fserve.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcli%2Fserve.py?ref=21ecb5b0b8be61a51c476856589c45f10d3e8568",
            "patch": "@@ -813,7 +813,7 @@ def continuous_batching_chat_completion(self, req: dict, request_id: str) -> Str\n         # TODO (Joao, Lysandre): this should also work with tool support\n         inputs = processor.apply_chat_template(req[\"messages\"], return_tensors=\"pt\", add_generation_prompt=True).to(\n             model.device\n-        )[0]\n+        )[\"input_ids\"][0]\n \n         def stream_chat_completion(request_id, decode_stream):\n             try:\n@@ -1237,7 +1237,7 @@ def generate_response(self, req: dict) -> Generator[str, None, None]:\n         else:\n             raise TypeError(\"inputs should be a list, dict, or str\")\n \n-        inputs = processor.apply_chat_template(inputs, add_generation_prompt=True, return_tensors=\"pt\")\n+        inputs = processor.apply_chat_template(inputs, add_generation_prompt=True, return_tensors=\"pt\")[\"input_ids\"]\n         inputs = inputs.to(model.device)\n         request_id = req.get(\"previous_response_id\", \"req_0\")\n \n@@ -1541,7 +1541,7 @@ def generate_response_non_streaming(self, req: dict) -> dict:\n         else:\n             raise ValueError(\"inputs should be a list, dict, or str\")\n \n-        inputs = processor.apply_chat_template(inputs, add_generation_prompt=True, return_tensors=\"pt\")\n+        inputs = processor.apply_chat_template(inputs, add_generation_prompt=True, return_tensors=\"pt\")[\"input_ids\"]\n         inputs = inputs.to(model.device)\n         request_id = req.get(\"previous_response_id\", \"req_0\")\n "
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}