{
    "author": "tomaarsen",
    "message": "Fix: Only call Trainer.align_special_tokens if model has \"config\" attribute (#40322)\n\n* Only call Trainer.align_special_tokens if model has \"config\" attribute\n\n* Add efficient test for training a model without model.config\n\n* Reformat",
    "sha": "5c88d8fbccd9e61fce8cdde32878539e60b15f1c",
    "files": [
        {
            "sha": "44266526da9d3ddc70e7bf9fd08c417d52d30bde",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c88d8fbccd9e61fce8cdde32878539e60b15f1c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c88d8fbccd9e61fce8cdde32878539e60b15f1c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=5c88d8fbccd9e61fce8cdde32878539e60b15f1c",
            "patch": "@@ -2237,7 +2237,9 @@ def train(\n         self.is_in_train = True\n \n         # If the model uses a tokenizer, it may have a new tokens for fine-tuning purposes.\n-        if isinstance(self.processing_class, (PreTrainedTokenizerBase, ProcessorMixin)):\n+        if isinstance(self.processing_class, (PreTrainedTokenizerBase, ProcessorMixin)) and hasattr(\n+            self.model, \"config\"\n+        ):\n             self._align_special_tokens()\n \n         # Attach NEFTune hooks if necessary"
        },
        {
            "sha": "0e5918742d525d2b4e70334b4adc5661c0e3985a",
            "filename": "tests/trainer/test_trainer.py",
            "status": "modified",
            "additions": 44,
            "deletions": 2,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c88d8fbccd9e61fce8cdde32878539e60b15f1c/tests%2Ftrainer%2Ftest_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c88d8fbccd9e61fce8cdde32878539e60b15f1c/tests%2Ftrainer%2Ftest_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftrainer%2Ftest_trainer.py?ref=5c88d8fbccd9e61fce8cdde32878539e60b15f1c",
            "patch": "@@ -516,11 +516,15 @@ def __init__(self, vocab_size, hidden_size):\n             self.lstm = nn.LSTM(hidden_size, hidden_size, batch_first=True)\n             self.fc = nn.Linear(hidden_size, vocab_size)\n \n-        def forward(self, input_ids, **kwargs):\n+        def forward(self, input_ids, labels=None, **kwargs):\n             embedded = self.embedding(input_ids)\n             lstm_out, _ = self.lstm(embedded)\n             logits = self.fc(lstm_out)\n-            return logits\n+            if labels is None:\n+                return logits\n+\n+            loss = nn.functional.cross_entropy(logits.view(-1, logits.size(-1)), labels.view(-1))\n+            return loss, logits\n \n     def create_dummy_dataset_for_text_generation(vocab_size, seq_length, num_samples):\n         import numpy as np\n@@ -5021,6 +5025,44 @@ def test_special_token_aligment(self):\n             self.assertEqual(trainer.model.config.pad_token_id, tokenizer.pad_token_id)\n             self.assertEqual(trainer.model.config.bos_token_id, tokenizer.bos_token_id)\n \n+    def test_trainer_works_without_model_config(self):\n+        \"\"\"\n+        Tests that models without a `config` parameter can still be trained.\n+        This is useful for preserving compatibility with third parties that train different models using the\n+        transformers Trainer.\n+\n+        If this test fails, it doesn't imply that there's issues with transformers, but perhaps with third\n+        parties.\n+        \"\"\"\n+\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-LlamaForCausalLM\")\n+        model = BasicTextGenerationModel(vocab_size=tokenizer.vocab_size, hidden_size=32)\n+        # Note that this class does not have a config attribute\n+\n+        train_dataset = LineByLineTextDataset(\n+            tokenizer=tokenizer,\n+            file_path=PATH_SAMPLE_TEXT,\n+            block_size=tokenizer.max_len_single_sentence,\n+        )\n+        for example in train_dataset.examples:\n+            example[\"labels\"] = example[\"input_ids\"]\n+\n+        with tempfile.TemporaryDirectory() as tmpdir:\n+            training_args = TrainingArguments(\n+                output_dir=tmpdir,\n+                report_to=\"none\",\n+                max_steps=5,\n+                per_device_train_batch_size=1,\n+                remove_unused_columns=False,\n+            )\n+            trainer = Trainer(\n+                model=model,\n+                args=training_args,\n+                processing_class=tokenizer,\n+                train_dataset=train_dataset,\n+            )\n+            trainer.train()\n+\n \n @require_torch\n @is_staging_test"
        }
    ],
    "stats": {
        "total": 50,
        "additions": 47,
        "deletions": 3
    }
}