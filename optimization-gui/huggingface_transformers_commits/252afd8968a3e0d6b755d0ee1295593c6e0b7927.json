{
    "author": "juliendenize",
    "message": "Fix convert_tekken_tokenizer (#42592)\n\n* Fix convert_tekken_tokenizer\n\n* quality\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "252afd8968a3e0d6b755d0ee1295593c6e0b7927",
    "files": [
        {
            "sha": "3256c9839acda91970f1bfb3f7668047c04067b1",
            "filename": "src/transformers/integrations/mistral.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/252afd8968a3e0d6b755d0ee1295593c6e0b7927/src%2Ftransformers%2Fintegrations%2Fmistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252afd8968a3e0d6b755d0ee1295593c6e0b7927/src%2Ftransformers%2Fintegrations%2Fmistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fmistral.py?ref=252afd8968a3e0d6b755d0ee1295593c6e0b7927",
            "patch": "@@ -77,6 +77,7 @@ def convert_tekken_tokenizer(tokenizer_file: str):\n     \"\"\"Convert a \"tekken\" tokenizer to a fast Tokenizer.\"\"\"\n     # Tekken format -- need to use the Converter\n \n+    from mistral_common.tokens.tokenizers.base import SpecialTokens\n     from mistral_common.tokens.tokenizers.mistral import MistralTokenizer\n \n     # Load directly using their lib\n@@ -106,4 +107,15 @@ def convert_tekken_tokenizer(tokenizer_file: str):\n     # Post-process\n     tokenizer.add_special_tokens({\"additional_special_tokens\": all_special})\n \n+    MAP_SPECAL = {\n+        \"bos_token\": SpecialTokens.bos.value,\n+        \"eos_token\": SpecialTokens.eos.value,\n+        \"pad_token\": SpecialTokens.pad.value,\n+        \"unk_token\": SpecialTokens.unk.value,\n+    }\n+\n+    for special_key, special_token in MAP_SPECAL.items():\n+        if special_token in all_special:\n+            tokenizer.add_special_tokens({special_key: special_token})\n+\n     return tokenizer"
        },
        {
            "sha": "6709fa25dd734b6c4972b4341e5ae2a29c6c8369",
            "filename": "src/transformers/models/ministral3/convert_ministral3_weights_to_hf.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/252afd8968a3e0d6b755d0ee1295593c6e0b7927/src%2Ftransformers%2Fmodels%2Fministral3%2Fconvert_ministral3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252afd8968a3e0d6b755d0ee1295593c6e0b7927/src%2Ftransformers%2Fmodels%2Fministral3%2Fconvert_ministral3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fministral3%2Fconvert_ministral3_weights_to_hf.py?ref=252afd8968a3e0d6b755d0ee1295593c6e0b7927",
            "patch": "@@ -273,7 +273,6 @@ def convert_and_write_processor_and_tokenizer(\n \n     tokenizer_file = os.path.join(input_dir, \"tekken.json\")\n     tokenizer = convert_tekken_tokenizer(tokenizer_file)\n-    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n \n     # No vision\n     if isinstance(model_config, Ministral3Config):"
        },
        {
            "sha": "2ad75bf4602af1a3c7337697b2cfeabb5239d543",
            "filename": "src/transformers/models/mistral3/convert_mistral3_weights_to_hf.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/252afd8968a3e0d6b755d0ee1295593c6e0b7927/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconvert_mistral3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/252afd8968a3e0d6b755d0ee1295593c6e0b7927/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconvert_mistral3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fconvert_mistral3_weights_to_hf.py?ref=252afd8968a3e0d6b755d0ee1295593c6e0b7927",
            "patch": "@@ -192,7 +192,7 @@ def convert_and_write_processor(input_dir: str, output_dir: str):\n     \"\"\"Convert the tokenizer and save it.\"\"\"\n     tokenizer_file = os.path.join(input_dir, \"tekken.json\")\n     tokenizer = convert_tekken_tokenizer(tokenizer_file)\n-    tokenizer.add_special_tokens({\"pad_token\": \"<pad>\"})\n+\n     chat_template = '{%- if messages[0][\"role\"] == \"system\" %}{%- set system_message = messages[0][\"content\"] %}{%- set loop_messages = messages[1:] %}\\n{%- else %}{%- set loop_messages = messages %}{%- endif %}{{- bos_token }}{%- for message in loop_messages %}{%- if (message[\\'role\\'] == \\'user\\') != (loop.index0 % 2 == 0) %}{{- raise_exception(\\'After the optional system message, conversation roles must alternate user/assistant/user/assistant/...\\') }}{%- endif %}{%- if message[\"role\"] == \"user\" %}{%- if loop.last and system_message is defined %}{{- \"[INST]\" + system_message + \"\\n\\n\" }}{%- else %}{{ \"[INST]\" }}{%- endif %}{%- endif %}{%- if message[\"content\"] is not string %}{%- for chunk in message[\"content\"] %}{%- if chunk[\"type\"] == \"text\" %}{%- if \"content\" in chunk %}{{- chunk[\"content\"] }}{%- elif \"text\" in chunk %}{{- chunk[\"text\"] }}{%- endif %}{%- elif chunk[\"type\"] == \"image\" %}{{- \"[IMG]\" }}{%- else %}{{- raise_exception(\"Unrecognized content type!\") }}{%- endif %}{%- endfor %}{%- else %}{{- message[\"content\"] }}{%- endif %}{%- if message[\"role\"] == \"user\" %}{{- \"[/INST]\" }}{%- elif message[\"role\"] == \"assistant\" %}{{- eos_token}}{%- else %}{{- raise_exception(\"Only user and assistant roles are supported, with the exception of an initial optional system message!\") }}{%- endif %}{%- endfor %}'\n \n     config = read_json(os.path.join(input_dir, \"params.json\"))"
        }
    ],
    "stats": {
        "total": 15,
        "additions": 13,
        "deletions": 2
    }
}