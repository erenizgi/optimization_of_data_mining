{
    "author": "Cyrilvallez",
    "message": "Fix some tests (#35682)\n\n* cohere tests\r\n\r\n* glm tests\r\n\r\n* cohere2 model name\r\n\r\n* create decorator\r\n\r\n* update\r\n\r\n* fix cohere2 completions\r\n\r\n* style\r\n\r\n* style\r\n\r\n* style\r\n\r\n* add cuda in comments",
    "sha": "ab1afd56f57245525a86e894c934c60df561cfdb",
    "files": [
        {
            "sha": "bba4e646599fcca57a7d3446b95a7f1d6f66efa1",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab1afd56f57245525a86e894c934c60df561cfdb/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab1afd56f57245525a86e894c934c60df561cfdb/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=ab1afd56f57245525a86e894c934c60df561cfdb",
            "patch": "@@ -595,7 +595,7 @@ class DiffLlamaPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n-    _supports_flex_attn = True\n+    _supports_flex_attn = False\n     _supports_cache_class = True\n     _supports_quantized_cache = True\n     _supports_static_cache = True"
        },
        {
            "sha": "2c8c846706525fc801944e4877fdb01009a4f1d3",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab1afd56f57245525a86e894c934c60df561cfdb/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab1afd56f57245525a86e894c934c60df561cfdb/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=ab1afd56f57245525a86e894c934c60df561cfdb",
            "patch": "@@ -431,7 +431,7 @@ def __init__(self, config: DiffLlamaConfig, layer_idx: int):\n \n \n class DiffLlamaPreTrainedModel(LlamaPreTrainedModel):\n-    pass\n+    _supports_flex_attn = False\n \n \n class DiffLlamaModel(LlamaModel):"
        },
        {
            "sha": "da2a39f462f556b11183d7dfbf4a705149f532e8",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab1afd56f57245525a86e894c934c60df561cfdb/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab1afd56f57245525a86e894c934c60df561cfdb/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=ab1afd56f57245525a86e894c934c60df561cfdb",
            "patch": "@@ -988,6 +988,17 @@ def require_torch_gpu(test_case):\n     return unittest.skipUnless(torch_device == \"cuda\", \"test requires CUDA\")(test_case)\n \n \n+def require_torch_large_gpu(test_case, memory: float = 20):\n+    \"\"\"Decorator marking a test that requires a CUDA GPU with more than `memory` GiB of memory.\"\"\"\n+    if torch_device != \"cuda\":\n+        return unittest.skip(reason=f\"test requires a CUDA GPU with more than {memory} GiB of memory\")(test_case)\n+\n+    return unittest.skipUnless(\n+        torch.cuda.get_device_properties(0).total_memory / 1024**3 > memory,\n+        f\"test requires a GPU with more than {memory} GiB of memory\",\n+    )(test_case)\n+\n+\n def require_torch_gpu_if_bnb_not_multi_backend_enabled(test_case):\n     \"\"\"\n     Decorator marking a test that requires a GPU if bitsandbytes multi-backend feature is not enabled."
        },
        {
            "sha": "89e3b6898948682a4f29916d460f5a1c6363dc9c",
            "filename": "tests/models/cohere/test_modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab1afd56f57245525a86e894c934c60df561cfdb/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab1afd56f57245525a86e894c934c60df561cfdb/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere%2Ftest_modeling_cohere.py?ref=ab1afd56f57245525a86e894c934c60df561cfdb",
            "patch": "@@ -347,7 +347,7 @@ def test_batched_small_model_logits(self):\n                 [[0.0000, 0.1866, -0.1997], [0.0000, -0.0736, 0.1785], [0.0000, -0.1965, -0.0569]],\n                 [[0.0000, -0.0302, 0.1488], [0.0000, -0.0402, 0.1351], [0.0000, -0.0341, 0.1116]],\n             ]\n-        ).to(torch_device)\n+        ).to(device=torch_device, dtype=torch.float16)\n \n         tokenizer = AutoTokenizer.from_pretrained(model_id)\n         model = CohereForCausalLM.from_pretrained(model_id, low_cpu_mem_usage=True, torch_dtype=torch.float16).to("
        },
        {
            "sha": "1d201eb1e2873e55f02eacc305aecc289d26841e",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 33,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab1afd56f57245525a86e894c934c60df561cfdb/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab1afd56f57245525a86e894c934c60df561cfdb/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=ab1afd56f57245525a86e894c934c60df561cfdb",
            "patch": "@@ -26,7 +26,7 @@\n     require_flash_attn,\n     require_read_token,\n     require_torch,\n-    require_torch_gpu,\n+    require_torch_large_gpu,\n     slow,\n     torch_device,\n )\n@@ -182,7 +182,8 @@ def test_sdpa_equivalence(self):\n \n \n @slow\n-@require_torch_gpu\n+@require_read_token\n+@require_torch_large_gpu\n class Cohere2IntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n     # This variable is used to determine which CUDA device are we using for our runners (A10 or T4)\n@@ -195,12 +196,11 @@ def setUpClass(cls):\n             # 8 is for A100 / A10 and 7 for T4\n             cls.cuda_compute_capability_major_version = torch.cuda.get_device_capability()[0]\n \n-    @require_read_token\n     def test_model_bf16(self):\n-        model_id = \"CohereForAI/command-r7b-12-2024\"\n+        model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n         EXPECTED_TEXTS = [\n-            \"<BOS_TOKEN>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many\",\n-            \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to be talking about the history of the United States. The United States of America\",\n+            \"<BOS_TOKEN>Hello I am doing a project for a school assignment and I need to create a website for a fictional company. I have\",\n+            \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to show you how to make a simple and easy to make a chocolate cake.\\n\",\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n@@ -215,12 +215,11 @@ def test_model_bf16(self):\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n-    @require_read_token\n     def test_model_fp16(self):\n-        model_id = \"CohereForAI/command-r7b-12-2024\"\n+        model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n         EXPECTED_TEXTS = [\n-            \"<BOS_TOKEN>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many\",\n-            \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to be talking about the history of the United States. The United States of America\",\n+            \"<BOS_TOKEN>Hello I am doing a project for a school assignment and I need to create a website for a fictional company. I have\",\n+            \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to show you how to make a simple and easy to make a chocolate cake.\\n\",\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n@@ -235,14 +234,13 @@ def test_model_fp16(self):\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n-    @require_read_token\n     def test_model_pipeline_bf16(self):\n         # See https://github.com/huggingface/transformers/pull/31747 -- pipeline was broken for Cohere2 before this PR\n-        model_id = \"CohereForAI/command-r7b-12-2024\"\n+        model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n         # EXPECTED_TEXTS should match the same non-pipeline test, minus the special tokens\n         EXPECTED_TEXTS = [\n-            \"Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many\",\n-            \"Hi today I'm going to be talking about the history of the United States. The United States of America\",\n+            \"Hello I am doing a project for a school assignment and I need to create a website for a fictional company. I have\",\n+            \"Hi today I'm going to show you how to make a simple and easy to make a chocolate cake.\\n\",\n         ]\n \n         model = AutoModelForCausalLM.from_pretrained(\n@@ -256,17 +254,14 @@ def test_model_pipeline_bf16(self):\n         self.assertEqual(output[0][0][\"generated_text\"], EXPECTED_TEXTS[0])\n         self.assertEqual(output[1][0][\"generated_text\"], EXPECTED_TEXTS[1])\n \n-    @require_read_token\n     @require_flash_attn\n-    @require_torch_gpu\n     @mark.flash_attn_test\n-    @slow\n     def test_model_flash_attn(self):\n         # See https://github.com/huggingface/transformers/issues/31953 --- flash attn was generating garbage for Gemma2, especially in long context\n-        model_id = \"CohereForAI/command-r7b-12-2024\"\n+        model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n         EXPECTED_TEXTS = [\n-            '<BOS_TOKEN>Hello I am doing a project on the 1918 flu pandemic and I am trying to find out how many people died in the United States. I have found a few sites that say 500,000 but I am not sure if that is correct. I have also found a site that says 675,000 but I am not sure if that is correct either. I am trying to find out how many people died in the United States. I have found a few',\n-            \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to be talking about the history of the United States. The United States of America is a country in North America. It is the third largest country in the world by total area and the third most populous country with over 320 million people. The United States is a federal republic consisting of 50 states and a federal district. The 48 contiguous states and the district of Columbia are in central North America between Canada and Mexico. The state of Alaska is in the\"\n+            '<BOS_TOKEN>Hello I am doing a project for my school and I need to create a website for a fictional company. I have the logo and the name of the company. I need a website that is simple and easy to navigate. I need a home page, about us, services, contact us, and a gallery. I need the website to be responsive and I need it to be able to be hosted on a server. I need the website to be done in a week. I need the website to be done in HTML,',\n+            \"<PAD><PAD><BOS_TOKEN>Hi today I'm going to show you how to make a simple and easy to make a chocolate cake.\\n\\nThis recipe is very simple and easy to make.\\n\\nYou will need:\\n\\n* 2 cups of flour\\n* 1 cup of sugar\\n* 1/2 cup of cocoa powder\\n* 1 teaspoon of baking powder\\n* 1 teaspoon of baking soda\\n* 1/2 teaspoon of salt\\n* 2 eggs\\n* 1 cup of milk\\n\",\n         ]  # fmt: skip\n \n         model = AutoModelForCausalLM.from_pretrained(\n@@ -280,8 +275,6 @@ def test_model_flash_attn(self):\n \n         self.assertEqual(output_text, EXPECTED_TEXTS)\n \n-    @slow\n-    @require_read_token\n     def test_export_static_cache(self):\n         if version.parse(torch.__version__) < version.parse(\"2.5.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.5 to run.\")\n@@ -291,42 +284,38 @@ def test_export_static_cache(self):\n             convert_and_export_with_cache,\n         )\n \n-        tokenizer = AutoTokenizer.from_pretrained(\n-            \"CohereForAI/command-r7b-12-2024\", pad_token=\"<PAD>\", padding_side=\"right\"\n-        )\n+        model_id = \"CohereForAI/c4ai-command-r7b-12-2024\"\n         EXPECTED_TEXT_COMPLETION = [\n-            \"Hello I am doing a project for my school and I need to know how to make a program that will take a number\",\n+            \"Hello I am doing a project on the effects of social media on mental health. I have a few questions. 1. What is the relationship\",\n         ]\n-        max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n-            \"input_ids\"\n-        ].shape[-1]\n \n+        tokenizer = AutoTokenizer.from_pretrained(model_id, pad_token=\"<PAD>\", padding_side=\"right\")\n         # Load model\n         device = \"cpu\"\n         dtype = torch.bfloat16\n         cache_implementation = \"static\"\n         attn_implementation = \"sdpa\"\n         batch_size = 1\n         model = AutoModelForCausalLM.from_pretrained(\n-            \"CohereForAI/command-r7b-12-2024\",\n+            \"CohereForAI/c4ai-command-r7b-12-2024\",\n             device_map=device,\n             torch_dtype=dtype,\n             attn_implementation=attn_implementation,\n             generation_config=GenerationConfig(\n                 use_cache=True,\n                 cache_implementation=cache_implementation,\n-                max_length=max_generation_length,\n+                max_length=30,\n                 cache_config={\n                     \"batch_size\": batch_size,\n-                    \"max_cache_len\": max_generation_length,\n+                    \"max_cache_len\": 30,\n                 },\n             ),\n         )\n \n         prompts = [\"Hello I am doing\"]\n         prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n         prompt_token_ids = prompt_tokens[\"input_ids\"]\n-        max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n+        max_new_tokens = 30 - prompt_token_ids.shape[-1]\n \n         # Static Cache + export\n         exported_program = convert_and_export_with_cache(model)"
        },
        {
            "sha": "8642fe826ca2089ce4ae09216ea302308fb0f2c0",
            "filename": "tests/models/glm/test_modeling_glm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ab1afd56f57245525a86e894c934c60df561cfdb/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ab1afd56f57245525a86e894c934c60df561cfdb/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm%2Ftest_modeling_glm.py?ref=ab1afd56f57245525a86e894c934c60df561cfdb",
            "patch": "@@ -23,7 +23,7 @@\n     is_flaky,\n     require_flash_attn,\n     require_torch,\n-    require_torch_accelerator,\n+    require_torch_large_gpu,\n     require_torch_sdpa,\n     slow,\n     torch_device,\n@@ -418,7 +418,7 @@ def test_custom_4d_attention_mask(self):\n \n \n @slow\n-@require_torch_accelerator\n+@require_torch_large_gpu\n class GlmIntegrationTest(unittest.TestCase):\n     input_text = [\"Hello I am doing\", \"Hi today\"]\n     model_id = \"THUDM/glm-4-9b\""
        }
    ],
    "stats": {
        "total": 76,
        "additions": 38,
        "deletions": 38
    }
}