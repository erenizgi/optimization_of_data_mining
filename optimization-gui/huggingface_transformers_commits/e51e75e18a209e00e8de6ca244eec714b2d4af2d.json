{
    "author": "SunMarc",
    "message": "remove tokenizer warning (#42483)\n\n* fix warning\n\n* fix\n\n* remove",
    "sha": "e51e75e18a209e00e8de6ca244eec714b2d4af2d",
    "files": [
        {
            "sha": "5d0dc48e5aee9a2fa8423ca1e45702898d037379",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 51,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/e51e75e18a209e00e8de6ca244eec714b2d4af2d/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e51e75e18a209e00e8de6ca244eec714b2d4af2d/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=e51e75e18a209e00e8de6ca244eec714b2d4af2d",
            "patch": "@@ -1827,16 +1827,14 @@ def _from_pretrained(\n         if tokenizer_config_file is not None:\n             with open(tokenizer_config_file, encoding=\"utf-8\") as tokenizer_config_handle:\n                 init_kwargs = json.load(tokenizer_config_handle)\n-            # First attempt. We get tokenizer_class from tokenizer_config to check mismatch between tokenizers.\n-            config_tokenizer_class = init_kwargs.get(\"tokenizer_class\")\n+            # used in the past to check if the tokenizer class matches the class in the repo\n             init_kwargs.pop(\"tokenizer_class\", None)\n             if not has_tokenizer_file:\n                 init_kwargs.get(\"tokenizer_file\", None)\n             saved_init_inputs = init_kwargs.pop(\"init_inputs\", ())\n             if not init_inputs:\n                 init_inputs = saved_init_inputs\n         else:\n-            config_tokenizer_class = None\n             init_kwargs = init_configuration\n \n         # If independent chat template file(s) exist, they take priority over template entries in the tokenizer config\n@@ -1864,54 +1862,6 @@ def _from_pretrained(\n                 if isinstance(init_kwargs[\"auto_map\"], (tuple, list)):\n                     init_kwargs[\"auto_map\"] = {\"AutoTokenizer\": init_kwargs[\"auto_map\"]}\n \n-        if config_tokenizer_class is None:\n-            # Matt: This entire block is only used to decide if the tokenizer class matches the class in the repo.\n-            #       If not, it raises a warning, but otherwise continues. Since we mostly load tokenizers with\n-            #       AutoTokenizer these days, it seems like a lot of work (and a source of bugs) for little gain.\n-            #       Maybe we can just remove this entirely?\n-            from .models.auto.configuration_auto import AutoConfig  # tests_ignore\n-\n-            # Second attempt. If we have not yet found tokenizer_class, let's try to use the config.\n-            try:\n-                config = AutoConfig.from_pretrained(\n-                    pretrained_model_name_or_path,\n-                    token=token,\n-                    cache_dir=cache_dir,\n-                    local_files_only=local_files_only,\n-                    trust_remote_code=trust_remote_code,\n-                    _commit_hash=_commit_hash,\n-                )\n-                config_tokenizer_class = config.tokenizer_class\n-            except (OSError, ValueError, KeyError):\n-                # skip if an error occurred.\n-                config = None\n-            if config_tokenizer_class is None:\n-                # Third attempt. If we have not yet found the original type of the tokenizer,\n-                # we are loading we see if we can infer it from the type of the configuration file\n-                from .models.auto.tokenization_auto import TOKENIZER_MAPPING_NAMES  # tests_ignore\n-\n-                if hasattr(config, \"model_type\"):\n-                    model_type = config.model_type\n-                else:\n-                    # Fallback: use pattern matching on the string.\n-                    model_type = None\n-                    for pattern in TOKENIZER_MAPPING_NAMES:\n-                        if pattern in str(pretrained_model_name_or_path):\n-                            model_type = pattern\n-                            break\n-\n-                if model_type is not None:\n-                    config_tokenizer_class = TOKENIZER_MAPPING_NAMES.get(model_type)\n-\n-        if config_tokenizer_class is not None:\n-            if cls.__name__.replace(\"Fast\", \"\") != config_tokenizer_class.replace(\"Fast\", \"\"):\n-                logger.warning(\n-                    \"The tokenizer class you load from this checkpoint is not the same type as the class this\"\n-                    \" function is called from. It may result in unexpected tokenization. \\nThe tokenizer class you\"\n-                    f\" load from this checkpoint is '{config_tokenizer_class}'. \\nThe class this function is called\"\n-                    f\" from is '{cls.__name__}'.\"\n-                )\n-\n         # Preserve extra_special_tokens from tokenizer_config.json before updating with kwargs\n         # extra_special_tokens should be a list (user-defined extra tokens)\n         extra_special_tokens_from_config = init_kwargs.get(\"extra_special_tokens\")"
        }
    ],
    "stats": {
        "total": 52,
        "additions": 1,
        "deletions": 51
    }
}