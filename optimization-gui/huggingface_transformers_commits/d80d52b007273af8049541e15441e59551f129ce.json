{
    "author": "MHRDYN7",
    "message": "addressing the issue #34611 to make FlaxDinov2 compatible with any batch size (#35138)\n\nfixed the batch_size error, all tests are passing\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "d80d52b007273af8049541e15441e59551f129ce",
    "files": [
        {
            "sha": "8093b3a0b799ba555a209d2c5e1d5a8eebe79aa2",
            "filename": "src/transformers/models/dinov2/modeling_flax_dinov2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d80d52b007273af8049541e15441e59551f129ce/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d80d52b007273af8049541e15441e59551f129ce/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_flax_dinov2.py?ref=d80d52b007273af8049541e15441e59551f129ce",
            "patch": "@@ -185,9 +185,11 @@ def interpolate_pos_encoding(self, config, hidden_states, height, width, positio\n             antialias=False,\n         )\n         patch_pos_embed = patch_pos_embed.astype(target_dtype)\n-        patch_pos_embed = jnp.transpose(patch_pos_embed, (0, 2, 3, 1)).reshape((hidden_states.shape[0], -1, dim))\n+        patch_pos_embed = jnp.transpose(patch_pos_embed, (0, 2, 3, 1)).reshape((position_embeddings.shape[0], -1, dim))\n+        patch_pos_embed_expanded = jnp.tile(patch_pos_embed, (hidden_states.shape[0], 1, 1))\n+        class_pos_embed_expanded = jnp.tile(class_pos_embed, (hidden_states.shape[0], 1, 1))\n \n-        return jnp.concatenate((class_pos_embed[jnp.newaxis, :], patch_pos_embed), axis=1)\n+        return jnp.concatenate((class_pos_embed_expanded, patch_pos_embed_expanded), axis=1)\n \n     def __call__(self, pixel_values, deterministic=True):\n         batch_size = pixel_values.shape[0]\n@@ -778,7 +780,7 @@ class FlaxDinov2ForImageClassification(FlaxDinov2PreTrainedModel):\n     >>> image = Image.open(requests.get(url, stream=True).raw)\n \n     >>> image_processor = AutoImageProcessor.from_pretrained(\"facebook/dinov2-base-imagenet1k-1-layer\")\n-    >>> model = FlaxDinov2ForImageClassification.from_pretrained(\"facebook/dinov2-base-imagenet1k-1-layer\")\n+    >>> model = FlaxDinov2ForImageClassification.from_pretrained(\"facebook/dinov2-base-imagenet1k-1-layer\", from_pt=True)\n \n     >>> inputs = image_processor(images=image, return_tensors=\"np\")\n     >>> outputs = model(**inputs)"
        },
        {
            "sha": "09ce20611a6de2a3c8c218889357f0d503914ab9",
            "filename": "tests/models/dinov2/test_modeling_flax_dinov2.py",
            "status": "modified",
            "additions": 18,
            "deletions": 10,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/d80d52b007273af8049541e15441e59551f129ce/tests%2Fmodels%2Fdinov2%2Ftest_modeling_flax_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d80d52b007273af8049541e15441e59551f129ce/tests%2Fmodels%2Fdinov2%2Ftest_modeling_flax_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2%2Ftest_modeling_flax_dinov2.py?ref=d80d52b007273af8049541e15441e59551f129ce",
            "patch": "@@ -202,7 +202,7 @@ def test_model_from_pretrained(self):\n # We will verify our results on an image of cute cats\n def prepare_img():\n     image = Image.open(\"./tests/fixtures/tests_samples/COCO/000000039769.png\")\n-    return image\n+    return [image, image]\n \n \n @require_vision\n@@ -224,18 +224,25 @@ def test_inference_no_head(self):\n         outputs = model(pixel_values=pixel_values)\n \n         # verify the logits\n-        expected_shape = (1, 257, 768)\n+        expected_shape = (2, 257, 768)\n         self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n \n         expected_slice = np.array(\n             [\n-                [-2.1629121, -0.46566057, 1.0925977],\n-                [-3.5971704, -1.0283585, -1.1780515],\n-                [-2.900407, 1.1334689, -0.74357724],\n+                [\n+                    [-2.1629121, -0.46566057, 1.0925977],\n+                    [-3.5971704, -1.0283585, -1.1780515],\n+                    [-2.900407, 1.1334689, -0.74357724],\n+                ],\n+                [\n+                    [-2.1629121, -0.46566057, 1.0925977],\n+                    [-3.5971704, -1.0283585, -1.1780515],\n+                    [-2.900407, 1.1334689, -0.74357724],\n+                ],\n             ]\n         )\n \n-        self.assertTrue(np.allclose(outputs.last_hidden_state[0, :3, :3], expected_slice, atol=1e-4))\n+        self.assertTrue(np.allclose(outputs.last_hidden_state[:2, :3, :3], expected_slice, atol=1e-4))\n \n     @slow\n     def test_inference_image_classification_head_imagenet_1k(self):\n@@ -252,12 +259,13 @@ def test_inference_image_classification_head_imagenet_1k(self):\n         logits = outputs.logits\n \n         # verify the logits\n-        expected_shape = (1, 1000)\n+        expected_shape = (2, 1000)\n         self.assertEqual(logits.shape, expected_shape)\n \n-        expected_slice = np.array([-2.1776447, 0.36716992, 0.13870952])\n+        expected_slice = np.array([[-2.1776447, 0.36716992, 0.13870952], [-2.1776447, 0.36716992, 0.13870952]])\n \n-        self.assertTrue(np.allclose(logits[0, :3], expected_slice, atol=1e-4))\n+        self.assertTrue(np.allclose(logits[:2, :3], expected_slice, atol=1e-3))\n \n         expected_class_idx = 281\n-        self.assertEqual(logits.argmax(-1).item(), expected_class_idx)\n+        self.assertEqual(logits[0].argmax(-1).item(), expected_class_idx)\n+        self.assertEqual(logits[1].argmax(-1).item(), expected_class_idx)"
        }
    ],
    "stats": {
        "total": 36,
        "additions": 23,
        "deletions": 13
    }
}