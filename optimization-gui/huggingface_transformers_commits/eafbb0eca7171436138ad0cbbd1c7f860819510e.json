{
    "author": "CISC",
    "message": "Implement AsyncTextIteratorStreamer for asynchronous streaming (#34931)\n\n* Add AsyncTextIteratorStreamer class\n\n* export AsyncTextIteratorStreamer\n\n* export AsyncTextIteratorStreamer\n\n* improve docs\n\n* missing import\n\n* missing import\n\n* doc example fix\n\n* doc example output fix\n\n* add pytest-asyncio\n\n* first attempt at tests\n\n* missing import\n\n* add pytest-asyncio\n\n* fallback to wait_for and raise TimeoutError on timeout\n\n* check for TimeoutError\n\n* autodoc\n\n* reorder imports\n\n* fix style\n\n---------\n\nCo-authored-by: Arthur Zucker <arthur.zucker@gmail.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>",
    "sha": "eafbb0eca7171436138ad0cbbd1c7f860819510e",
    "files": [
        {
            "sha": "d8931342ee45f85fc12b84ddbb26a28569216724",
            "filename": "docs/source/en/internal/generation_utils.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eafbb0eca7171436138ad0cbbd1c7f860819510e/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/eafbb0eca7171436138ad0cbbd1c7f860819510e/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fgeneration_utils.md?ref=eafbb0eca7171436138ad0cbbd1c7f860819510e",
            "patch": "@@ -352,6 +352,8 @@ A [`Constraint`] can be used to force the generation to include specific tokens\n \n [[autodoc]] TextIteratorStreamer\n \n+[[autodoc]] AsyncTextIteratorStreamer\n+\n ## Caches\n \n [[autodoc]] Cache"
        },
        {
            "sha": "9e678db9978b145bbf9806daf59fc255968518e3",
            "filename": "setup.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/eafbb0eca7171436138ad0cbbd1c7f860819510e/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eafbb0eca7171436138ad0cbbd1c7f860819510e/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=eafbb0eca7171436138ad0cbbd1c7f860819510e",
            "patch": "@@ -148,6 +148,7 @@\n     \"pyyaml>=5.1\",\n     \"pydantic\",\n     \"pytest>=7.2.0,<8.0.0\",\n+    \"pytest-asyncio\",\n     \"pytest-timeout\",\n     \"pytest-xdist\",\n     \"python>=3.9.0\",\n@@ -319,6 +320,7 @@ def run(self):\n extras[\"testing\"] = (\n     deps_list(\n         \"pytest\",\n+        \"pytest-asyncio\",\n         \"pytest-rich\",\n         \"pytest-xdist\",\n         \"timeout-decorator\","
        },
        {
            "sha": "5510ac6c8ad512933b6485043534d3bfee4a8372",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/eafbb0eca7171436138ad0cbbd1c7f860819510e/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eafbb0eca7171436138ad0cbbd1c7f860819510e/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=eafbb0eca7171436138ad0cbbd1c7f860819510e",
            "patch": "@@ -122,6 +122,7 @@\n     \"feature_extraction_utils\": [\"BatchFeature\", \"FeatureExtractionMixin\"],\n     \"file_utils\": [],\n     \"generation\": [\n+        \"AsyncTextIteratorStreamer\",\n         \"CompileConfig\",\n         \"GenerationConfig\",\n         \"TextIteratorStreamer\",\n@@ -5055,7 +5056,14 @@\n     from .feature_extraction_utils import BatchFeature, FeatureExtractionMixin\n \n     # Generation\n-    from .generation import CompileConfig, GenerationConfig, TextIteratorStreamer, TextStreamer, WatermarkingConfig\n+    from .generation import (\n+        AsyncTextIteratorStreamer,\n+        CompileConfig,\n+        GenerationConfig,\n+        TextIteratorStreamer,\n+        TextStreamer,\n+        WatermarkingConfig,\n+    )\n     from .hf_argparser import HfArgumentParser\n \n     # Integrations"
        },
        {
            "sha": "c370c7a5d7c18c352cfddc161f36ef6fddcff654",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/eafbb0eca7171436138ad0cbbd1c7f860819510e/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eafbb0eca7171436138ad0cbbd1c7f860819510e/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=eafbb0eca7171436138ad0cbbd1c7f860819510e",
            "patch": "@@ -54,6 +54,7 @@\n     \"pyyaml\": \"pyyaml>=5.1\",\n     \"pydantic\": \"pydantic\",\n     \"pytest\": \"pytest>=7.2.0,<8.0.0\",\n+    \"pytest-asyncio\": \"pytest-asyncio\",\n     \"pytest-timeout\": \"pytest-timeout\",\n     \"pytest-xdist\": \"pytest-xdist\",\n     \"python\": \"python>=3.9.0\","
        },
        {
            "sha": "d3eb10c1e6b355875c6cddb86ca53022549a8333",
            "filename": "src/transformers/generation/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/eafbb0eca7171436138ad0cbbd1c7f860819510e/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eafbb0eca7171436138ad0cbbd1c7f860819510e/src%2Ftransformers%2Fgeneration%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2F__init__.py?ref=eafbb0eca7171436138ad0cbbd1c7f860819510e",
            "patch": "@@ -26,7 +26,7 @@\n         \"SynthIDTextWatermarkingConfig\",\n         \"WatermarkingConfig\",\n     ],\n-    \"streamers\": [\"TextIteratorStreamer\", \"TextStreamer\"],\n+    \"streamers\": [\"AsyncTextIteratorStreamer\", \"TextIteratorStreamer\", \"TextStreamer\"],\n }\n \n try:\n@@ -199,7 +199,7 @@\n         SynthIDTextWatermarkingConfig,\n         WatermarkingConfig,\n     )\n-    from .streamers import TextIteratorStreamer, TextStreamer\n+    from .streamers import AsyncTextIteratorStreamer, TextIteratorStreamer, TextStreamer\n \n     try:\n         if not is_torch_available():"
        },
        {
            "sha": "c78e259db38be8e9cd74e792a13baba4126e3716",
            "filename": "src/transformers/generation/streamers.py",
            "status": "modified",
            "additions": 89,
            "deletions": 0,
            "changes": 89,
            "blob_url": "https://github.com/huggingface/transformers/blob/eafbb0eca7171436138ad0cbbd1c7f860819510e/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eafbb0eca7171436138ad0cbbd1c7f860819510e/src%2Ftransformers%2Fgeneration%2Fstreamers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstreamers.py?ref=eafbb0eca7171436138ad0cbbd1c7f860819510e",
            "patch": "@@ -13,6 +13,7 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import asyncio\n from queue import Queue\n from typing import TYPE_CHECKING, Optional\n \n@@ -225,3 +226,91 @@ def __next__(self):\n             raise StopIteration()\n         else:\n             return value\n+\n+\n+class AsyncTextIteratorStreamer(TextStreamer):\n+    \"\"\"\n+    Streamer that stores print-ready text in a queue, to be used by a downstream application as an async iterator.\n+    This is useful for applications that benefit from acessing the generated text asynchronously (e.g. in an\n+    interactive Gradio demo).\n+\n+    <Tip warning={true}>\n+\n+    The API for the streamer classes is still under development and may change in the future.\n+\n+    </Tip>\n+\n+    Parameters:\n+        tokenizer (`AutoTokenizer`):\n+            The tokenized used to decode the tokens.\n+        skip_prompt (`bool`, *optional*, defaults to `False`):\n+            Whether to skip the prompt to `.generate()` or not. Useful e.g. for chatbots.\n+        timeout (`float`, *optional*):\n+            The timeout for the text queue. If `None`, the queue will block indefinitely. Useful to handle exceptions\n+            in `.generate()`, when it is called in a separate thread.\n+        decode_kwargs (`dict`, *optional*):\n+            Additional keyword arguments to pass to the tokenizer's `decode` method.\n+\n+    Raises:\n+        TimeoutError: If token generation time exceeds timeout value.\n+\n+    Examples:\n+\n+        ```python\n+        >>> from transformers import AutoModelForCausalLM, AutoTokenizer, AsyncTextIteratorStreamer\n+        >>> from threading import Thread\n+        >>> import asyncio\n+\n+        >>> tok = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n+        >>> model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n+        >>> inputs = tok([\"An increasing sequence: one,\"], return_tensors=\"pt\")\n+\n+        >>> # Run the generation in a separate thread, so that we can fetch the generated text in a non-blocking way.\n+        >>> async def main():\n+        ...     # Important: AsyncTextIteratorStreamer must be initialized inside a coroutine!\n+        ...     streamer = AsyncTextIteratorStreamer(tok)\n+        ...     generation_kwargs = dict(inputs, streamer=streamer, max_new_tokens=20)\n+        ...     thread = Thread(target=model.generate, kwargs=generation_kwargs)\n+        ...     thread.start()\n+        ...     generated_text = \"\"\n+        ...     async for new_text in streamer:\n+        ...         generated_text += new_text\n+        >>>     print(generated_text)\n+        >>> asyncio.run(main())\n+        An increasing sequence: one, two, three, four, five, six, seven, eight, nine, ten, eleven,\n+        ```\n+    \"\"\"\n+\n+    def __init__(\n+        self, tokenizer: \"AutoTokenizer\", skip_prompt: bool = False, timeout: float | None = None, **decode_kwargs\n+    ):\n+        super().__init__(tokenizer, skip_prompt, **decode_kwargs)\n+        self.text_queue = asyncio.Queue()\n+        self.stop_signal = None\n+        self.timeout = timeout\n+        self.loop = asyncio.get_running_loop()\n+        self.has_asyncio_timeout = hasattr(asyncio, \"timeout\")\n+\n+    def on_finalized_text(self, text: str, stream_end: bool = False):\n+        \"\"\"Put the new text in the queue. If the stream is ending, also put a stop signal in the queue.\"\"\"\n+        self.loop.call_soon_threadsafe(self.text_queue.put_nowait, text)\n+        if stream_end:\n+            self.loop.call_soon_threadsafe(self.text_queue.put_nowait, self.stop_signal)\n+\n+    def __aiter__(self):\n+        return self\n+\n+    async def __anext__(self):\n+        try:\n+            if self.has_asyncio_timeout:\n+                async with asyncio.timeout(self.timeout):\n+                    value = await self.text_queue.get()\n+            else:\n+                value = await asyncio.wait_for(self.text_queue.get(), timeout=self.timeout)\n+        except asyncio.TimeoutError:\n+            raise TimeoutError()\n+        else:\n+            if value == self.stop_signal:\n+                raise StopAsyncIteration()\n+            else:\n+                return value"
        },
        {
            "sha": "be8c37334d02fcd3a6f0ce8bf2634ded46cb99e7",
            "filename": "tests/generation/test_streamers.py",
            "status": "modified",
            "additions": 49,
            "deletions": 1,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/eafbb0eca7171436138ad0cbbd1c7f860819510e/tests%2Fgeneration%2Ftest_streamers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/eafbb0eca7171436138ad0cbbd1c7f860819510e/tests%2Fgeneration%2Ftest_streamers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_streamers.py?ref=eafbb0eca7171436138ad0cbbd1c7f860819510e",
            "patch": "@@ -17,7 +17,15 @@\n from queue import Empty\n from threading import Thread\n \n-from transformers import AutoTokenizer, TextIteratorStreamer, TextStreamer, is_torch_available\n+import pytest\n+\n+from transformers import (\n+    AsyncTextIteratorStreamer,\n+    AutoTokenizer,\n+    TextIteratorStreamer,\n+    TextStreamer,\n+    is_torch_available,\n+)\n from transformers.testing_utils import CaptureStdout, require_torch, torch_device\n \n from ..test_modeling_common import ids_tensor\n@@ -120,3 +128,43 @@ def test_iterator_streamer_timeout(self):\n             streamer_text = \"\"\n             for new_text in streamer:\n                 streamer_text += new_text\n+\n+\n+@require_torch\n+@pytest.mark.asyncio(loop_scope=\"class\")\n+class AsyncStreamerTester(unittest.IsolatedAsyncioTestCase):\n+    async def test_async_iterator_streamer_matches_non_streaming(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n+        model.config.eos_token_id = -1\n+\n+        input_ids = ids_tensor((1, 5), vocab_size=model.config.vocab_size).to(torch_device)\n+        greedy_ids = model.generate(input_ids, max_new_tokens=10, do_sample=False)\n+        greedy_text = tokenizer.decode(greedy_ids[0])\n+\n+        streamer = AsyncTextIteratorStreamer(tokenizer)\n+        generation_kwargs = {\"input_ids\": input_ids, \"max_new_tokens\": 10, \"do_sample\": False, \"streamer\": streamer}\n+        thread = Thread(target=model.generate, kwargs=generation_kwargs)\n+        thread.start()\n+        streamer_text = \"\"\n+        async for new_text in streamer:\n+            streamer_text += new_text\n+\n+        self.assertEqual(streamer_text, greedy_text)\n+\n+    async def test_async_iterator_streamer_timeout(self):\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n+        model.config.eos_token_id = -1\n+\n+        input_ids = ids_tensor((1, 5), vocab_size=model.config.vocab_size).to(torch_device)\n+        streamer = AsyncTextIteratorStreamer(tokenizer, timeout=0.001)\n+        generation_kwargs = {\"input_ids\": input_ids, \"max_new_tokens\": 10, \"do_sample\": False, \"streamer\": streamer}\n+        thread = Thread(target=model.generate, kwargs=generation_kwargs)\n+        thread.start()\n+\n+        # The streamer will timeout after 0.001 seconds, so TimeoutError will be raised\n+        with self.assertRaises(TimeoutError):\n+            streamer_text = \"\"\n+            async for new_text in streamer:\n+                streamer_text += new_text"
        }
    ],
    "stats": {
        "total": 158,
        "additions": 154,
        "deletions": 4
    }
}