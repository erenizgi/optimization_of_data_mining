{
    "author": "Cyrilvallez",
    "message": "Offloading need to add the prefix into the offload_index (#42624)\n\n* add prefix\n\n* stupide typing rules\n\n* peft integration as well",
    "sha": "20890e3b901b264ffa2a5c31b4c26304bc3a5e61",
    "files": [
        {
            "sha": "5d44ae8fa04ffe5261a4f91995b5cf5696bcbb0a",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/20890e3b901b264ffa2a5c31b4c26304bc3a5e61/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20890e3b901b264ffa2a5c31b4c26304bc3a5e61/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=20890e3b901b264ffa2a5c31b4c26304bc3a5e61",
            "patch": "@@ -466,10 +466,10 @@ def expand_device_map(device_map, param_names):\n \n \n def accelerate_disk_offload(\n+    model: \"PreTrainedModel\",\n     disk_offload_folder: str | None,\n     checkpoint_files: list[str] | None,\n     device_map: dict,\n-    expected_keys: list[str],\n     sharded_metadata: dict | None,\n     dtype: torch.dtype | None,\n     weight_mapping=None,\n@@ -493,7 +493,8 @@ def accelerate_disk_offload(\n     # In this case, the offload index is simply the existing safetensors (except if using custom weight loading\n     # Operation, e.g. the MoE models, where we need to resave the weights that were changed at loading time)\n     if is_offloaded_safetensors:\n-        param_device_map = expand_device_map(device_map, expected_keys)\n+        meta_state_dict = model.state_dict()\n+        param_device_map = expand_device_map(device_map, meta_state_dict.keys())\n         str_dtype = str(dtype).replace(\"torch.\", \"\") if dtype is not None else \"float32\"\n         if sharded_metadata is None:\n             weight_map = dict.fromkeys(safe_open(checkpoint_files[0], framework=\"pt\").keys(), checkpoint_files[0])\n@@ -502,7 +503,9 @@ def accelerate_disk_offload(\n             weight_map = {k: os.path.join(folder, v) for k, v in sharded_metadata[\"weight_map\"].items()}\n \n         # Update the weight names according to the `weight_mapping`\n-        weight_renaming_map = {rename_source_key(k, renamings, [])[0]: k for k in weight_map}\n+        weight_renaming_map = {\n+            rename_source_key(k, renamings, [], model.base_model_prefix, meta_state_dict)[0]: k for k in weight_map\n+        }\n \n         # Prepare the index using existing safetensors files\n         disk_offload_index = {"
        },
        {
            "sha": "b06eb0cc1173559d00fe0cd2cd1f59e55db6adbb",
            "filename": "src/transformers/integrations/peft.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/20890e3b901b264ffa2a5c31b4c26304bc3a5e61/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20890e3b901b264ffa2a5c31b4c26304bc3a5e61/src%2Ftransformers%2Fintegrations%2Fpeft.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fpeft.py?ref=20890e3b901b264ffa2a5c31b4c26304bc3a5e61",
            "patch": "@@ -299,13 +299,14 @@ def load_adapter(\n             renamings = [entry for entry in key_mapping if isinstance(entry, WeightRenaming)]\n         processed_adapter_state_dict = {}\n         prefix = \"base_model.model.\"\n+        state_dict = self.state_dict()\n         for key, value in adapter_state_dict.items():\n             if key.startswith(prefix):\n                 new_key = key[len(prefix) :]\n             else:\n                 new_key = key\n \n-            new_key = rename_source_key(new_key, renamings, [])[0]\n+            new_key = rename_source_key(new_key, renamings, [], self.base_model_prefix, state_dict)[0]\n \n             # For hotswapping, we need the adapter name to be present in the state dict keys\n             if hotswap:"
        },
        {
            "sha": "e46f66ffb0f6ea1e23cb998060a23ba5694d301f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/20890e3b901b264ffa2a5c31b4c26304bc3a5e61/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/20890e3b901b264ffa2a5c31b4c26304bc3a5e61/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=20890e3b901b264ffa2a5c31b4c26304bc3a5e61",
            "patch": "@@ -4089,10 +4089,10 @@ def _load_pretrained_model(\n         # Prepare parameters offloading if needed\n         if device_map is not None and \"disk\" in device_map.values():\n             disk_offload_index = accelerate_disk_offload(\n+                model,\n                 disk_offload_folder,\n                 checkpoint_files,\n                 device_map,\n-                expected_keys,\n                 sharded_metadata,\n                 dtype,\n                 weight_mapping,"
        }
    ],
    "stats": {
        "total": 14,
        "additions": 9,
        "deletions": 5
    }
}