{
    "author": "NielsRogge",
    "message": "Add MetaCLIP 2 (#39826)\n\n* First draft\n\n* Make fixup\n\n* Use eos_token_id\n\n* Improve tests\n\n* Update clip\n\n* Make fixup\n\n* Fix processor tests\n\n* Add conversion script\n\n* Update docs\n\n* Update tokenization_auto\n\n* Make fixup\n\n* Use check_model_inputs\n\n* Rename to lowercase\n\n* Undo CLIP changes\n\n* Address comment\n\n* Convert all checkpoints\n\n* Update auto files\n\n* Rename checkpoints",
    "sha": "1d4609173780c973c2f52ee21249856724277922",
    "files": [
        {
            "sha": "4694e0c41c04e10d8656d0840686a491873a21fb",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -1065,6 +1065,8 @@\n         title: LXMERT\n       - local: model_doc/matcha\n         title: MatCha\n+      - local: model_doc/metaclip_2\n+        title: MetaCLIP 2\n       - local: model_doc/mgp-str\n         title: MGP-STR\n       - local: model_doc/mistral3"
        },
        {
            "sha": "b69f069c8df5143be597dbabf584f869e5489ae7",
            "filename": "docs/source/en/model_doc/metaclip_2.md",
            "status": "added",
            "additions": 134,
            "deletions": 0,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmetaclip_2.md?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -0,0 +1,134 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-07-31.*\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# MetaCLIP 2\n+\n+## Overview\n+\n+MetaCLIP 2 is a replication of the original CLIP model trained on 300+ languages. It achieves state-of-the-art (SOTA) results on multilingual benchmarks (e.g., XM3600, CVQA, Babelâ€‘ImageNet), surpassing previous SOTA such as [mSigLIP](siglip) and [SigLIPâ€‘2](siglip2). The authors show that English and non-English worlds can mutually benefit and elevate each other.\n+\n+This model was contributed by [nielsr](https://huggingface.co/nielsr).\n+The original code can be found [here](https://github.com/facebookresearch/MetaCLIP).\n+\n+You can find all the MetaCLIP 2 checkpoints under the [Meta](https://huggingface.co/facebook?search_models=metaclip-2) organization.\n+\n+> [!TIP]\n+> Click on the MetaCLIP 2 models in the right sidebar for more examples of how to apply MetaCLIP 2 to different image and language tasks.\n+\n+The example below demonstrates how to calculate similarity scores between multiple text descriptions and an image with [`Pipeline`] or the [`AutoModel`] class. Usage of the MetaCLIP 2 models is identical to the CLIP models, you just need the `MetaClip2Model` class instead of `CLIPModel`.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+clip = pipeline(\n+   task=\"zero-shot-image-classification\",\n+   model=\"facebook/metaclip-2-worldwide-huge-quickgelu\",\n+   torch_dtype=torch.bfloat16,\n+   device=0\n+)\n+labels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]\n+clip(\"http://images.cocodataset.org/val2017/000000039769.jpg\", candidate_labels=labels)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import requests\n+import torch\n+from PIL import Image\n+from transformers import AutoProcessor, AutoModel\n+\n+model = AutoModel.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\", torch_dtype=torch.bfloat16, attn_implementation=\"sdpa\")\n+processor = AutoProcessor.from_pretrained(\"facebook/metaclip-2-worldwide-huge-quickgelu\")\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+labels = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]\n+\n+inputs = processor(text=labels, images=image, return_tensors=\"pt\", padding=True)\n+\n+outputs = model(**inputs)\n+logits_per_image = outputs.logits_per_image\n+probs = logits_per_image.softmax(dim=1)\n+most_likely_idx = probs.argmax(dim=1).item()\n+most_likely_label = labels[most_likely_idx]\n+print(f\"Most likely label: {most_likely_label} with probability: {probs[0][most_likely_idx].item():.3f}\")\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## MetaClip2Config\n+\n+[[autodoc]] MetaClip2Config\n+    - from_text_vision_configs\n+\n+## MetaClip2TextConfig\n+\n+[[autodoc]] MetaClip2TextConfig\n+\n+## MetaClip2VisionConfig\n+\n+[[autodoc]] MetaClip2VisionConfig\n+\n+## MetaClip2Model\n+\n+[[autodoc]] MetaClip2Model\n+    - forward\n+    - get_text_features\n+    - get_image_features\n+\n+## MetaClip2TextModel\n+\n+[[autodoc]] MetaClip2TextModel\n+    - forward\n+\n+## MetaClip2TextModelWithProjection\n+\n+[[autodoc]] MetaClip2TextModelWithProjection\n+    - forward\n+\n+## MetaClip2VisionModelWithProjection\n+\n+[[autodoc]] MetaClip2VisionModelWithProjection\n+    - forward\n+\n+## MetaClip2VisionModel\n+\n+[[autodoc]] MetaClip2VisionModel\n+    - forward\n+\n+## MetaClip2ForImageClassification\n+\n+[[autodoc]] MetaClip2ForImageClassification\n+    - forward\n+\n+</pt>\n+<tf>"
        },
        {
            "sha": "453fe3ec38f76f328b88c3de91dec0964032d15f",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -242,6 +242,7 @@\n         (\"mctct\", \"MCTCTConfig\"),\n         (\"mega\", \"MegaConfig\"),\n         (\"megatron-bert\", \"MegatronBertConfig\"),\n+        (\"metaclip_2\", \"MetaClip2Config\"),\n         (\"mgp-str\", \"MgpstrConfig\"),\n         (\"mimi\", \"MimiConfig\"),\n         (\"minimax\", \"MiniMaxConfig\"),\n@@ -667,6 +668,7 @@\n         (\"mega\", \"MEGA\"),\n         (\"megatron-bert\", \"Megatron-BERT\"),\n         (\"megatron_gpt2\", \"Megatron-GPT2\"),\n+        (\"metaclip_2\", \"MetaCLIP 2\"),\n         (\"mgp-str\", \"MGP-STR\"),\n         (\"mimi\", \"Mimi\"),\n         (\"minimax\", \"MiniMax\"),"
        },
        {
            "sha": "1240a677d97c49a93311d598b14051de3d88ca88",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -128,6 +128,7 @@\n             (\"llava_onevision\", (\"LlavaOnevisionImageProcessor\", \"LlavaOnevisionImageProcessorFast\")),\n             (\"mask2former\", (\"Mask2FormerImageProcessor\", \"Mask2FormerImageProcessorFast\")),\n             (\"maskformer\", (\"MaskFormerImageProcessor\", \"MaskFormerImageProcessorFast\")),\n+            (\"metaclip_2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n             (\"mgp-str\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"mistral3\", (\"PixtralImageProcessor\", \"PixtralImageProcessorFast\")),\n             (\"mlcd\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),"
        },
        {
            "sha": "2dde76b467e4d01394abda77948cb1b370f2e456",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -242,6 +242,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"mctct\", \"MCTCTModel\"),\n         (\"mega\", \"MegaModel\"),\n         (\"megatron-bert\", \"MegatronBertModel\"),\n+        (\"metaclip_2\", \"MetaClip2Model\"),\n         (\"mgp-str\", \"MgpstrForSceneTextRecognition\"),\n         (\"mimi\", \"MimiModel\"),\n         (\"minimax\", \"MiniMaxModel\"),\n@@ -849,6 +850,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n             \"levit\",\n             (\"LevitForImageClassification\", \"LevitForImageClassificationWithTeacher\"),\n         ),\n+        (\"metaclip_2\", \"MetaClip2ForImageClassification\"),\n         (\"mobilenet_v1\", \"MobileNetV1ForImageClassification\"),\n         (\"mobilenet_v2\", \"MobileNetV2ForImageClassification\"),\n         (\"mobilevit\", \"MobileViTForImageClassification\"),\n@@ -1616,6 +1618,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"chinese_clip\", \"ChineseCLIPModel\"),\n         (\"clip\", \"CLIPModel\"),\n         (\"clipseg\", \"CLIPSegModel\"),\n+        (\"metaclip_2\", \"MetaClip2Model\"),\n         (\"siglip\", \"SiglipModel\"),\n         (\"siglip2\", \"Siglip2Model\"),\n     ]"
        },
        {
            "sha": "5b9b6105df4d9d9f22a1fce94a2825897e2e32e5",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -99,6 +99,7 @@\n         (\"llava_onevision\", \"LlavaOnevisionProcessor\"),\n         (\"markuplm\", \"MarkupLMProcessor\"),\n         (\"mctct\", \"MCTCTProcessor\"),\n+        (\"metaclip_2\", \"CLIPProcessor\"),\n         (\"mgp-str\", \"MgpstrProcessor\"),\n         (\"mistral3\", \"PixtralProcessor\"),\n         (\"mllama\", \"MllamaProcessor\"),"
        },
        {
            "sha": "39dbe89483fb574655ded519c68cadc7253319c6",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -405,6 +405,13 @@\n         ),\n         (\"mega\", (\"RobertaTokenizer\", \"RobertaTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"megatron-bert\", (\"BertTokenizer\", \"BertTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\n+            \"metaclip_2\",\n+            (\n+                \"XLMRobertaTokenizer\",\n+                \"XLMRobertaTokenizerFast\" if is_tokenizers_available() else None,\n+            ),\n+        ),\n         (\"mgp-str\", (\"MgpstrTokenizer\", None)),\n         (\n             \"minimax\","
        },
        {
            "sha": "8ad8cc5edcc7795fd967d2c55a60b8262ceb9e19",
            "filename": "src/transformers/models/clip/processing_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -32,13 +32,13 @@ class CLIPProcessor(ProcessorMixin):\n     Args:\n         image_processor ([`CLIPImageProcessor`], *optional*):\n             The image processor is a required input.\n-        tokenizer ([`CLIPTokenizerFast`], *optional*):\n+        tokenizer ([`AutoTokenizer`], *optional*):\n             The tokenizer is a required input.\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")\n-    tokenizer_class = (\"CLIPTokenizer\", \"CLIPTokenizerFast\")\n+    tokenizer_class = \"AutoTokenizer\"\n \n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         feature_extractor = None"
        },
        {
            "sha": "5ea828a839b2f37e109fce22e330d1b430551c40",
            "filename": "src/transformers/models/metaclip_2/__init__.py",
            "status": "added",
            "additions": 27,
            "deletions": 0,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2F__init__.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -0,0 +1,27 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_metaclip_2 import *\n+    from .modeling_metaclip_2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "32b21a193e8d3b95b72d886c610304d9cd5eea99",
            "filename": "src/transformers/models/metaclip_2/configuration_metaclip_2.py",
            "status": "added",
            "additions": 346,
            "deletions": 0,
            "changes": 346,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconfiguration_metaclip_2.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -0,0 +1,346 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/metaclip_2/modular_metaclip_2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_metaclip_2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class MetaClip2TextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MetaClip2TextModel`]. It is used to instantiate a METACLIP_2\n+    text encoder according to the specified arguments, defining the model architecture. Instantiating a configuration\n+    with the defaults will yield a similar configuration to that of the text encoder of the METACLIP_2\n+    [openai/metaclip_2-vit-base-patch32](https://huggingface.co/openai/metaclip_2-vit-base-patch32) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 49408):\n+            Vocabulary size of the METACLIP_2 text model. Defines the number of different tokens that can be represented by\n+            the `inputs_ids` passed when calling [`MetaClip2Model`].\n+        hidden_size (`int`, *optional*, defaults to 512):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 2048):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        projection_dim (`int`, *optional*, defaults to 512):\n+            Dimensionality of text and vision projection layers.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 8):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 77):\n+            The maximum sequence length that this model might ever be used with. Typically set this to something large\n+            just in case (e.g., 512 or 1024 or 2048).\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"quick_gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_factor (`float`, *optional*, defaults to 1.0):\n+            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n+            testing).\n+        pad_token_id (`int`, *optional*, defaults to 1):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*, defaults to 49406):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 49407):\n+            End of stream token id.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import MetaClip2TextConfig, MetaClip2TextModel\n+\n+    >>> # Initializing a MetaClip2TextConfig with openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> configuration = MetaClip2TextConfig()\n+\n+    >>> # Initializing a MetaClip2TextModel (with random weights) from the openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> model = MetaClip2TextModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"metaclip_2_text_model\"\n+    base_config_key = \"text_config\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=49408,\n+        hidden_size=512,\n+        intermediate_size=2048,\n+        projection_dim=512,\n+        num_hidden_layers=12,\n+        num_attention_heads=8,\n+        max_position_embeddings=77,\n+        hidden_act=\"quick_gelu\",\n+        layer_norm_eps=1e-5,\n+        attention_dropout=0.0,\n+        initializer_range=0.02,\n+        initializer_factor=1.0,\n+        # This differs from `MetaClip2Tokenizer`'s default and from openai/metaclip_2\n+        # See https://github.com/huggingface/transformers/pull/24773#issuecomment-1632287538\n+        pad_token_id=1,\n+        bos_token_id=49406,\n+        eos_token_id=49407,\n+        **kwargs,\n+    ):\n+        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n+\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.projection_dim = projection_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.max_position_embeddings = max_position_embeddings\n+        self.layer_norm_eps = layer_norm_eps\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.initializer_factor = initializer_factor\n+        self.attention_dropout = attention_dropout\n+\n+\n+class MetaClip2VisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`MetaClip2VisionModel`]. It is used to instantiate a\n+    METACLIP_2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the vision encoder of the METACLIP_2\n+    [openai/metaclip_2-vit-base-patch32](https://huggingface.co/openai/metaclip_2-vit-base-patch32) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 3072):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        projection_dim (`int`, *optional*, defaults to 512):\n+            Dimensionality of text and vision projection layers.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            The number of input channels.\n+        image_size (`int`, *optional*, defaults to 224):\n+            The size (resolution) of each image.\n+        patch_size (`int`, *optional*, defaults to 32):\n+            The size (resolution) of each patch.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"quick_gelu\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        initializer_factor (`float`, *optional*, defaults to 1.0):\n+            A factor for initializing all weight matrices (should be kept to 1, used internally for initialization\n+            testing).\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import MetaClip2VisionConfig, MetaClip2VisionModel\n+\n+    >>> # Initializing a MetaClip2VisionConfig with openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> configuration = MetaClip2VisionConfig()\n+\n+    >>> # Initializing a MetaClip2VisionModel (with random weights) from the openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> model = MetaClip2VisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"metaclip_2_vision_model\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=768,\n+        intermediate_size=3072,\n+        projection_dim=512,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        num_channels=3,\n+        image_size=224,\n+        patch_size=32,\n+        hidden_act=\"quick_gelu\",\n+        layer_norm_eps=1e-5,\n+        attention_dropout=0.0,\n+        initializer_range=0.02,\n+        initializer_factor=1.0,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.projection_dim = projection_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.patch_size = patch_size\n+        self.image_size = image_size\n+        self.initializer_range = initializer_range\n+        self.initializer_factor = initializer_factor\n+        self.attention_dropout = attention_dropout\n+        self.layer_norm_eps = layer_norm_eps\n+        self.hidden_act = hidden_act\n+\n+\n+class MetaClip2Config(PretrainedConfig):\n+    r\"\"\"\n+    [`MetaClip2Config`] is the configuration class to store the configuration of a [`MetaClip2Model`]. It is used to instantiate\n+    a METACLIP_2 model according to the specified arguments, defining the text model and vision model configs. Instantiating\n+    a configuration with the defaults will yield a similar configuration to that of the METACLIP_2\n+    [openai/metaclip_2-vit-base-patch32](https://huggingface.co/openai/metaclip_2-vit-base-patch32) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`MetaClip2TextConfig`].\n+        vision_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`MetaClip2VisionConfig`].\n+        projection_dim (`int`, *optional*, defaults to 512):\n+            Dimensionality of text and vision projection layers.\n+        logit_scale_init_value (`float`, *optional*, defaults to 2.6592):\n+            The initial value of the *logit_scale* parameter. Default is used as per the original METACLIP_2 implementation.\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import MetaClip2Config, MetaClip2Model\n+\n+    >>> # Initializing a MetaClip2Config with openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> configuration = MetaClip2Config()\n+\n+    >>> # Initializing a MetaClip2Model (with random weights) from the openai/metaclip_2-vit-base-patch32 style configuration\n+    >>> model = MetaClip2Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+\n+    >>> # We can also initialize a MetaClip2Config from a MetaClip2TextConfig and a MetaClip2VisionConfig\n+    >>> from transformers import MetaClip2TextConfig, MetaClip2VisionConfig\n+\n+    >>> # Initializing a MetaClip2Text and MetaClip2Vision configuration\n+    >>> config_text = MetaClip2TextConfig()\n+    >>> config_vision = MetaClip2VisionConfig()\n+\n+    >>> config = MetaClip2Config.from_text_vision_configs(config_text, config_vision)\n+    ```\"\"\"\n+\n+    model_type = \"metaclip_2\"\n+    sub_configs = {\"text_config\": MetaClip2TextConfig, \"vision_config\": MetaClip2VisionConfig}\n+\n+    def __init__(\n+        self, text_config=None, vision_config=None, projection_dim=512, logit_scale_init_value=2.6592, **kwargs\n+    ):\n+        # If `_config_dict` exist, we use them for the backward compatibility.\n+        # We pop out these 2 attributes before calling `super().__init__` to avoid them being saved (which causes a lot\n+        # of confusion!).\n+        text_config_dict = kwargs.pop(\"text_config_dict\", None)\n+        vision_config_dict = kwargs.pop(\"vision_config_dict\", None)\n+\n+        super().__init__(**kwargs)\n+\n+        # Instead of simply assigning `[text|vision]_config_dict` to `[text|vision]_config`, we use the values in\n+        # `[text|vision]_config_dict` to update the values in `[text|vision]_config`. The values should be same in most\n+        # cases, but we don't want to break anything regarding `_config_dict` that existed before commit `8827e1b2`.\n+        if text_config_dict is not None:\n+            if text_config is None:\n+                text_config = {}\n+\n+            # This is the complete result when using `text_config_dict`.\n+            _text_config_dict = MetaClip2TextConfig(**text_config_dict).to_dict()\n+\n+            # Give a warning if the values exist in both `_text_config_dict` and `text_config` but being different.\n+            for key, value in _text_config_dict.items():\n+                if key in text_config and value != text_config[key] and key not in [\"transformers_version\"]:\n+                    # If specified in `text_config_dict`\n+                    if key in text_config_dict:\n+                        message = (\n+                            f\"`{key}` is found in both `text_config_dict` and `text_config` but with different values. \"\n+                            f'The value `text_config_dict[\"{key}\"]` will be used instead.'\n+                        )\n+                    # If inferred from default argument values (just to be super careful)\n+                    else:\n+                        message = (\n+                            f\"`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The \"\n+                            f'value `text_config[\"{key}\"]` will be overridden.'\n+                        )\n+                    logger.info(message)\n+\n+            # Update all values in `text_config` with the ones in `_text_config_dict`.\n+            text_config.update(_text_config_dict)\n+\n+        if vision_config_dict is not None:\n+            if vision_config is None:\n+                vision_config = {}\n+\n+            # This is the complete result when using `vision_config_dict`.\n+            _vision_config_dict = MetaClip2VisionConfig(**vision_config_dict).to_dict()\n+            # convert keys to string instead of integer\n+            if \"id2label\" in _vision_config_dict:\n+                _vision_config_dict[\"id2label\"] = {\n+                    str(key): value for key, value in _vision_config_dict[\"id2label\"].items()\n+                }\n+\n+            # Give a warning if the values exist in both `_vision_config_dict` and `vision_config` but being different.\n+            for key, value in _vision_config_dict.items():\n+                if key in vision_config and value != vision_config[key] and key not in [\"transformers_version\"]:\n+                    # If specified in `vision_config_dict`\n+                    if key in vision_config_dict:\n+                        message = (\n+                            f\"`{key}` is found in both `vision_config_dict` and `vision_config` but with different \"\n+                            f'values. The value `vision_config_dict[\"{key}\"]` will be used instead.'\n+                        )\n+                    # If inferred from default argument values (just to be super careful)\n+                    else:\n+                        message = (\n+                            f\"`vision_config_dict` is provided which will be used to initialize `CLIPVisionConfig`. \"\n+                            f'The value `vision_config[\"{key}\"]` will be overridden.'\n+                        )\n+                    logger.info(message)\n+\n+            # Update all values in `vision_config` with the ones in `_vision_config_dict`.\n+            vision_config.update(_vision_config_dict)\n+\n+        if text_config is None:\n+            text_config = {}\n+            logger.info(\"`text_config` is `None`. Initializing the `MetaClip2TextConfig` with default values.\")\n+\n+        if vision_config is None:\n+            vision_config = {}\n+            logger.info(\"`vision_config` is `None`. initializing the `MetaClip2VisionConfig` with default values.\")\n+\n+        self.text_config = MetaClip2TextConfig(**text_config)\n+        self.vision_config = MetaClip2VisionConfig(**vision_config)\n+\n+        self.projection_dim = projection_dim\n+        self.logit_scale_init_value = logit_scale_init_value\n+        self.initializer_factor = 1.0\n+\n+\n+__all__ = [\"MetaClip2Config\", \"MetaClip2TextConfig\", \"MetaClip2VisionConfig\"]"
        },
        {
            "sha": "21a0a1462fff6bab4569caba3821c925c2c8446b",
            "filename": "src/transformers/models/metaclip_2/convert_metaclip_2_to_hf.py",
            "status": "added",
            "additions": 426,
            "deletions": 0,
            "changes": 426,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconvert_metaclip_2_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconvert_metaclip_2_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fconvert_metaclip_2_to_hf.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -0,0 +1,426 @@\n+\"\"\"\n+This script allows you to convert MetaCLIP 2 (worldwide) checkpoints from the\n+original repository to the Hugging Face format.\n+\n+URL: https://github.com/facebookresearch/MetaCLIP\n+\n+To convert:\n+1. git clone the MetaCLIP repository\n+2. place it in the same directory as this script\n+3. move the conversion script to the MetaCLIP repository.\n+\n+Then run the script with:\n+\n+```bash\n+cd MetaCLIP\n+python convert_metaclip_2_to_hf.py --checkpoint_path /path/to/checkpoint --model_name ViT-H-14-quickgelu-worldwide\n+```\n+\"\"\"\n+\n+import argparse\n+import os\n+from typing import Optional\n+\n+import torch\n+from PIL import Image\n+\n+# Import MetaCLIP modules\n+from src.mini_clip.factory import create_model_and_transforms\n+from transformers import (\n+    AutoTokenizer,\n+    CLIPImageProcessor,\n+    CLIPProcessor,\n+    MetaClip2Config,\n+    MetaClip2Model,\n+)\n+\n+\n+def load_metaclip2_checkpoint(checkpoint_path: str, model_name: str) -> torch.nn.Module:\n+    \"\"\"Load MetaCLIP 2 model from checkpoint.\"\"\"\n+    print(f\"Loading MetaCLIP 2 model: {model_name}\")\n+\n+    # For worldwide models, use WorldWideCLIP class\n+    model_name_with_class = model_name\n+    if \"worldwide\" in model_name.lower():\n+        model_name_with_class = f\"{model_name}@WorldWideCLIP\"\n+        print(\"Using WorldWideCLIP class for worldwide model\")\n+\n+    # Create model using the factory\n+    model, _, preprocess = create_model_and_transforms(model_name_with_class, pretrained=checkpoint_path, device=\"cpu\")\n+    model.eval()\n+    return model, preprocess\n+\n+\n+def create_hf_config(tokenizer: AutoTokenizer, model_name: str) -> tuple[MetaClip2Config, int]:\n+    \"\"\"Create Hugging Face MetaClip2Config from MetaCLIP model.\n+\n+    This is based on the configs found at https://github.com/facebookresearch/MetaCLIP/tree/main/src/mini_clip/model_configs.\n+    \"\"\"\n+    print(\"Creating Hugging Face config...\")\n+\n+    # Vision config\n+    vision_configs = {\n+        \"ViT-H-14-quickgelu-worldwide\": {\n+            \"image_size\": 224,\n+            \"patch_size\": 14,\n+            \"hidden_size\": 1280,\n+            \"intermediate_size\": 1280 * 4,\n+            \"num_attention_heads\": 16,\n+            \"num_hidden_layers\": 32,\n+            \"hidden_act\": \"quick_gelu\",\n+            \"projection_dim\": 1024,\n+        },\n+        \"ViT-H-14-378-worldwide\": {\n+            \"image_size\": 378,\n+            \"patch_size\": 14,\n+            \"hidden_size\": 1280,\n+            \"intermediate_size\": 1280 * 4,\n+            \"num_attention_heads\": 16,\n+            \"num_hidden_layers\": 32,\n+            \"hidden_act\": \"gelu\",\n+            \"projection_dim\": 1024,\n+        },\n+        \"ViT-bigG-14-worldwide\": {\n+            \"image_size\": 224,\n+            \"patch_size\": 14,\n+            \"hidden_size\": 1664,\n+            \"intermediate_size\": 8192,\n+            \"num_attention_heads\": 16,\n+            \"num_hidden_layers\": 48,\n+            \"hidden_act\": \"gelu\",\n+            \"projection_dim\": 1280,\n+        },\n+        \"ViT-bigG-14-378-worldwide\": {\n+            \"image_size\": 378,\n+            \"patch_size\": 14,\n+            \"hidden_size\": 1664,\n+            \"intermediate_size\": 8192,\n+            \"num_attention_heads\": 16,\n+            \"num_hidden_layers\": 48,\n+            \"hidden_act\": \"gelu\",\n+            \"projection_dim\": 1280,\n+        },\n+    }\n+\n+    vision_config = vision_configs[model_name]\n+    image_size = vision_config[\"image_size\"]\n+\n+    # Text config\n+    text_configs = {\n+        \"ViT-H-14-quickgelu-worldwide\": {\n+            \"hidden_size\": 1024,\n+            \"intermediate_size\": 1024 * 4,\n+            \"num_attention_heads\": 16,\n+            \"num_hidden_layers\": 24,\n+            \"max_position_embeddings\": 77,\n+            \"vocab_size\": 901629,\n+            \"eos_token_id\": tokenizer.eos_token_id,\n+            \"hidden_act\": \"quick_gelu\",\n+            \"projection_dim\": 1024,\n+        },\n+        \"ViT-H-14-378-worldwide\": {\n+            \"hidden_size\": 1024,\n+            \"intermediate_size\": 1024 * 4,\n+            \"num_attention_heads\": 16,\n+            \"num_hidden_layers\": 24,\n+            \"max_position_embeddings\": 77,\n+            \"vocab_size\": 901629,\n+            \"eos_token_id\": tokenizer.eos_token_id,\n+            \"hidden_act\": \"gelu\",\n+            \"projection_dim\": 1024,\n+        },\n+        \"ViT-bigG-14-worldwide\": {\n+            \"hidden_size\": 1280,\n+            \"intermediate_size\": 1280 * 4,\n+            \"num_attention_heads\": 20,\n+            \"num_hidden_layers\": 32,\n+            \"max_position_embeddings\": 77,\n+            \"vocab_size\": 901629,\n+            \"eos_token_id\": tokenizer.eos_token_id,\n+            \"hidden_act\": \"gelu\",\n+            \"projection_dim\": 1280,\n+        },\n+        \"ViT-bigG-14-378-worldwide\": {\n+            \"hidden_size\": 1280,\n+            \"intermediate_size\": 1280 * 4,\n+            \"num_attention_heads\": 20,\n+            \"num_hidden_layers\": 32,\n+            \"max_position_embeddings\": 77,\n+            \"vocab_size\": 901629,\n+            \"eos_token_id\": tokenizer.eos_token_id,\n+            \"hidden_act\": \"gelu\",\n+            \"projection_dim\": 1280,\n+        },\n+    }\n+\n+    text_config = text_configs[model_name]\n+    projection_dim = text_config[\"projection_dim\"]\n+\n+    # Create config\n+    config = MetaClip2Config(\n+        vision_config=vision_config,\n+        text_config=text_config,\n+        projection_dim=projection_dim,\n+    )\n+\n+    return config, image_size\n+\n+\n+def convert_state_dict(metaclip_state_dict: dict[str, torch.Tensor]) -> dict[str, torch.Tensor]:\n+    \"\"\"Convert MetaCLIP state dict to Hugging Face format.\"\"\"\n+    print(\"Converting state dict...\")\n+\n+    hf_state_dict = {}\n+\n+    for key, value in metaclip_state_dict.items():\n+        new_key = key\n+\n+        # Handle specific mappings first before general prefix replacements\n+        if key == \"visual.proj\":\n+            new_key = \"visual_projection.weight\"\n+            # Don't transpose! MetaCLIP: x @ proj, HF: Linear(x) = x @ weight.T\n+            # So we want weight.T = proj, which means weight = proj.T\n+            # But since we're storing proj as weight, we need proj.T\n+            value = value.T  # This gives us the correct orientation for Linear layer\n+        elif key == \"text_projection\":\n+            new_key = \"text_projection.weight\"\n+            # Same logic as visual projection\n+            value = value.T\n+        elif key == \"token_embedding.weight\":\n+            new_key = \"text_model.embeddings.token_embedding.weight\"\n+        elif key == \"positional_embedding\":\n+            new_key = \"text_model.embeddings.position_embedding.weight\"\n+        elif key == \"ln_final.weight\":\n+            new_key = \"text_model.final_layer_norm.weight\"\n+        elif key == \"ln_final.bias\":\n+            new_key = \"text_model.final_layer_norm.bias\"\n+        # Vision encoder mappings\n+        elif key.startswith(\"visual.\"):\n+            new_key = key.replace(\"visual.\", \"vision_model.\")\n+\n+            # Handle specific vision model components\n+            if \"conv1\" in new_key:\n+                new_key = new_key.replace(\"conv1\", \"embeddings.patch_embedding\")\n+            elif \"class_embedding\" in new_key:\n+                new_key = new_key.replace(\"class_embedding\", \"embeddings.class_embedding\")\n+            elif \"positional_embedding\" in new_key:\n+                new_key = new_key.replace(\"positional_embedding\", \"embeddings.position_embedding.weight\")\n+            elif \"ln_pre\" in new_key:\n+                new_key = new_key.replace(\"ln_pre\", \"pre_layrnorm\")\n+            elif \"ln_post\" in new_key:\n+                new_key = new_key.replace(\"ln_post\", \"post_layernorm\")\n+            elif \"transformer.resblocks\" in new_key:\n+                new_key = new_key.replace(\"transformer.resblocks\", \"encoder.layers\")\n+                # Handle attention and MLP mappings within transformer blocks\n+                if \"attn.in_proj\" in new_key:\n+                    # Split the in_proj into q, k, v projections\n+                    if \"weight\" in new_key:\n+                        # We'll handle this later in a special case\n+                        continue\n+                    elif \"bias\" in new_key:\n+                        continue\n+                elif \"attn.out_proj\" in new_key:\n+                    new_key = new_key.replace(\"attn.out_proj\", \"self_attn.out_proj\")\n+                elif \"ln_1\" in new_key:\n+                    new_key = new_key.replace(\"ln_1\", \"layer_norm1\")\n+                elif \"ln_2\" in new_key:\n+                    new_key = new_key.replace(\"ln_2\", \"layer_norm2\")\n+                elif \"mlp.c_fc\" in new_key:\n+                    new_key = new_key.replace(\"mlp.c_fc\", \"mlp.fc1\")\n+                elif \"mlp.c_proj\" in new_key:\n+                    new_key = new_key.replace(\"mlp.c_proj\", \"mlp.fc2\")\n+\n+        # Text encoder mappings\n+        elif key.startswith(\"transformer.\"):\n+            new_key = key.replace(\"transformer.\", \"text_model.encoder.\")\n+\n+            if \"resblocks\" in new_key:\n+                new_key = new_key.replace(\"resblocks\", \"layers\")\n+                # Similar mappings as vision transformer\n+                if \"attn.in_proj\" in new_key:\n+                    continue  # Handle separately\n+                elif \"attn.out_proj\" in new_key:\n+                    new_key = new_key.replace(\"attn.out_proj\", \"self_attn.out_proj\")\n+                elif \"ln_1\" in new_key:\n+                    new_key = new_key.replace(\"ln_1\", \"layer_norm1\")\n+                elif \"ln_2\" in new_key:\n+                    new_key = new_key.replace(\"ln_2\", \"layer_norm2\")\n+                elif \"mlp.c_fc\" in new_key:\n+                    new_key = new_key.replace(\"mlp.c_fc\", \"mlp.fc1\")\n+                elif \"mlp.c_proj\" in new_key:\n+                    new_key = new_key.replace(\"mlp.c_proj\", \"mlp.fc2\")\n+\n+        hf_state_dict[new_key] = value\n+\n+    # Handle in_proj weights separately (split into q, k, v)\n+    for key, value in metaclip_state_dict.items():\n+        if \"attn.in_proj_weight\" in key:\n+            # Split the combined qkv weight into separate q, k, v weights\n+            dim = value.shape[0] // 3\n+            q_weight = value[:dim]\n+            k_weight = value[dim : 2 * dim]\n+            v_weight = value[2 * dim :]\n+\n+            base_key = key.replace(\"attn.in_proj_weight\", \"\")\n+            if key.startswith(\"visual.\"):\n+                base_key = base_key.replace(\"visual.transformer.resblocks\", \"vision_model.encoder.layers\")\n+            else:\n+                base_key = base_key.replace(\"transformer.resblocks\", \"text_model.encoder.layers\")\n+\n+            hf_state_dict[f\"{base_key}self_attn.q_proj.weight\"] = q_weight\n+            hf_state_dict[f\"{base_key}self_attn.k_proj.weight\"] = k_weight\n+            hf_state_dict[f\"{base_key}self_attn.v_proj.weight\"] = v_weight\n+\n+        elif \"attn.in_proj_bias\" in key:\n+            # Split the combined qkv bias into separate q, k, v biases\n+            dim = value.shape[0] // 3\n+            q_bias = value[:dim]\n+            k_bias = value[dim : 2 * dim]\n+            v_bias = value[2 * dim :]\n+\n+            base_key = key.replace(\"attn.in_proj_bias\", \"\")\n+            if key.startswith(\"visual.\"):\n+                base_key = base_key.replace(\"visual.transformer.resblocks\", \"vision_model.encoder.layers\")\n+            else:\n+                base_key = base_key.replace(\"transformer.resblocks\", \"text_model.encoder.layers\")\n+\n+            hf_state_dict[f\"{base_key}self_attn.q_proj.bias\"] = q_bias\n+            hf_state_dict[f\"{base_key}self_attn.k_proj.bias\"] = k_bias\n+            hf_state_dict[f\"{base_key}self_attn.v_proj.bias\"] = v_bias\n+\n+    return hf_state_dict\n+\n+\n+def verify_conversion(\n+    original_model, hf_model, preprocess, image_processor, tokenizer, test_image_path: Optional[str] = None\n+) -> bool:\n+    \"\"\"Verify that the conversion produces the same outputs.\"\"\"\n+    print(\"Verifying conversion...\")\n+\n+    # Create test image\n+    if test_image_path and os.path.exists(test_image_path):\n+        image = Image.open(test_image_path)\n+    else:\n+        # Create a dummy image\n+        image = Image.new(\"RGB\", (224, 224), color=\"red\")\n+\n+    # Verify image processor\n+    processed_image = preprocess(image).unsqueeze(0)\n+    pixel_values = image_processor(image, return_tensors=\"pt\").pixel_values\n+    print(\"Shape of pixel_values:\", pixel_values.shape)\n+    print(\"Shape of processed_image:\", processed_image.shape)\n+    assert torch.allclose(pixel_values, processed_image)\n+\n+    # Use tokenizer to get input_ids\n+    texts = [\"a cat\", \"a dog\", \"a bird\"]\n+    token_inputs = tokenizer(texts, return_tensors=\"pt\", padding=\"max_length\", truncation=True, max_length=77)\n+    input_ids = token_inputs.input_ids\n+\n+    print(f\"Processed text shape: {input_ids.shape}\")\n+    print(f\"Processed image shape: {processed_image.shape}\")\n+\n+    with torch.no_grad():\n+        # Original model outputs\n+        orig_image_features = original_model.encode_image(processed_image)\n+        orig_text_features = original_model.encode_text(input_ids)\n+\n+        # Normalize and compute logits\n+        orig_image_features = orig_image_features / orig_image_features.norm(dim=-1, keepdim=True)\n+        orig_text_features = orig_text_features / orig_text_features.norm(dim=-1, keepdim=True)\n+        orig_logits = original_model.logit_scale.exp() * orig_image_features @ orig_text_features.T\n+\n+        print(f\"Original text features: {orig_text_features[0][:5].tolist()}\")\n+        print(f\"Original image features: {orig_image_features[0][:5].tolist()}\")\n+\n+    with torch.no_grad():\n+        hf_outputs = hf_model(input_ids=input_ids, pixel_values=pixel_values)\n+        hf_logits = hf_outputs.logits_per_image\n+\n+        # Debug: Check HF model features\n+        print(f\"HF text features: {hf_outputs.text_embeds[0][:5].tolist()}\")\n+        print(f\"HF image features: {hf_outputs.image_embeds[0][:5].tolist()}\")\n+        print(f\"HF model EOS token ID: {hf_model.config.text_config.eos_token_id}\")\n+\n+    # Compare outputs\n+    print(f\"Original logits: {orig_logits}\")\n+    print(f\"HF logits: {hf_logits}\")\n+    print(f\"Logit scale - Original: {original_model.logit_scale.exp():.6f}, HF: {hf_model.logit_scale.exp():.6f}\")\n+\n+    # Check if they're close\n+    if orig_logits.shape == hf_logits.shape and torch.allclose(orig_logits, hf_logits, atol=1e-4):\n+        print(\"âœ… Conversion verified! Outputs match.\")\n+        return True\n+    else:\n+        print(\"âŒ Conversion failed! Outputs don't match.\")\n+        if orig_logits.numel() > 0 and hf_logits.numel() > 0:\n+            print(f\"Max difference: {(orig_logits - hf_logits).abs().max()}\")\n+        return False\n+\n+\n+def push_to_hub(hf_model: MetaClip2Model, processor: CLIPProcessor, repo_name: str):\n+    \"\"\"Push the converted model to Hugging Face Hub.\"\"\"\n+    print(f\"Pushing to hub: {repo_name}\")\n+\n+    try:\n+        hf_model.push_to_hub(repo_name)\n+        processor.push_to_hub(repo_name)\n+        print(f\"âœ… Successfully pushed to {repo_name}\")\n+    except Exception as e:\n+        print(f\"âŒ Failed to push to hub: {e}\")\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser(description=\"Convert MetaCLIP 2 to Hugging Face format\")\n+    parser.add_argument(\"--checkpoint_path\", required=True, help=\"Path to MetaCLIP 2 checkpoint\")\n+    parser.add_argument(\"--model_name\", required=True, help=\"MetaCLIP model name (e.g., ViT-H-14-quickgelu-worldwide)\")\n+    parser.add_argument(\"--output_dir\", default=\"./converted_models\", help=\"Output directory for converted model\")\n+    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Push to Hugging Face Hub\")\n+    parser.add_argument(\"--hub_repo_name\", help=\"Hub repository name\")\n+    parser.add_argument(\"--test_image\", help=\"Path to test image for verification\")\n+\n+    args = parser.parse_args()\n+\n+    # Load original model\n+    original_model, preprocess = load_metaclip2_checkpoint(args.checkpoint_path, args.model_name)\n+\n+    # Create HF config\n+    # Requires the tokenizer for the eos token id\n+    tokenizer = AutoTokenizer.from_pretrained(\"facebook/xlm-v-base\")\n+    config, image_size = create_hf_config(tokenizer=tokenizer, model_name=args.model_name)\n+\n+    # Create processor\n+    image_processor = CLIPImageProcessor(\n+        size={\"height\": image_size, \"width\": image_size}, crop_size={\"height\": image_size, \"width\": image_size}\n+    )\n+    processor = CLIPProcessor(image_processor=image_processor, tokenizer=tokenizer)\n+\n+    # Create HF model\n+    hf_model = MetaClip2Model(config)\n+\n+    # Convert state dict\n+    converted_state_dict = convert_state_dict(original_model.state_dict())\n+\n+    for name, param in hf_model.named_parameters():\n+        print(name, param.shape)\n+\n+    # Load converted weights\n+    hf_model.load_state_dict(converted_state_dict)\n+\n+    # Verify conversion\n+    if not verify_conversion(original_model, hf_model, preprocess, image_processor, tokenizer, args.test_image):\n+        print(\"Conversion verification failed. Please check the conversion logic.\")\n+        return\n+\n+    # Save model locally\n+    if args.output_dir:\n+        os.makedirs(args.output_dir, exist_ok=True)\n+        hf_model.save_pretrained(args.output_dir)\n+        processor.save_pretrained(args.output_dir)\n+\n+    # Push to hub if requested\n+    if args.push_to_hub and args.hub_repo_name:\n+        push_to_hub(hf_model, processor, args.hub_repo_name)\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "0fe3f56f5c48ff3134342a4e4cc27c173254cd97",
            "filename": "src/transformers/models/metaclip_2/modeling_metaclip_2.py",
            "status": "added",
            "additions": 1248,
            "deletions": 0,
            "changes": 1248,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodeling_metaclip_2.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -0,0 +1,1248 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/metaclip_2/modular_metaclip_2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_metaclip_2.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+from dataclasses import dataclass\n+from typing import Any, Callable, Optional, Union\n+\n+import torch\n+from torch import nn\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from ...activations import ACT2FN\n+from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import ModelOutput, TransformersKwargs, auto_docstring, can_return_tuple, logging, torch_int\n+from ...utils.generic import check_model_inputs\n+from .configuration_metaclip_2 import MetaClip2Config, MetaClip2TextConfig, MetaClip2VisionConfig\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class MetaClip2TextEmbeddings(nn.Module):\n+    def __init__(self, config: MetaClip2TextConfig):\n+        super().__init__()\n+        embed_dim = config.hidden_size\n+\n+        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n+        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n+\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+    ) -> torch.Tensor:\n+        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n+        max_position_embedding = self.position_embedding.weight.shape[0]\n+\n+        if seq_length > max_position_embedding:\n+            raise ValueError(\n+                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n+                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n+            )\n+\n+        if position_ids is None:\n+            position_ids = self.position_ids[:, :seq_length]\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.token_embedding(input_ids)\n+\n+        position_embeddings = self.position_embedding(position_ids)\n+        embeddings = inputs_embeds + position_embeddings\n+\n+        return embeddings\n+\n+\n+class MetaClip2VisionEmbeddings(nn.Module):\n+    def __init__(self, config: MetaClip2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.image_size = config.image_size\n+        self.patch_size = config.patch_size\n+\n+        self.class_embedding = nn.Parameter(torch.randn(self.embed_dim))\n+\n+        self.patch_embedding = nn.Conv2d(\n+            in_channels=config.num_channels,\n+            out_channels=self.embed_dim,\n+            kernel_size=self.patch_size,\n+            stride=self.patch_size,\n+            bias=False,\n+        )\n+\n+        self.num_patches = (self.image_size // self.patch_size) ** 2\n+        self.num_positions = self.num_patches + 1\n+        self.position_embedding = nn.Embedding(self.num_positions, self.embed_dim)\n+        self.register_buffer(\"position_ids\", torch.arange(self.num_positions).expand((1, -1)), persistent=False)\n+\n+    def interpolate_pos_encoding(self, embeddings: torch.Tensor, height: int, width: int) -> torch.Tensor:\n+        \"\"\"\n+        This method allows to interpolate the pre-trained position encodings, to be able to use the model on higher resolution\n+        images. This method is also adapted to support torch.jit tracing.\n+\n+        Adapted from:\n+        - https://github.com/facebookresearch/dino/blob/de9ee3df6cf39fac952ab558447af1fa1365362a/vision_transformer.py#L174-L194, and\n+        - https://github.com/facebookresearch/dinov2/blob/e1277af2ba9496fbadf7aec6eba56e8d882d1e35/dinov2/models/vision_transformer.py#L179-L211\n+        \"\"\"\n+\n+        num_patches = embeddings.shape[1] - 1\n+        position_embedding = self.position_embedding.weight.unsqueeze(0)\n+        num_positions = position_embedding.shape[1] - 1\n+\n+        # always interpolate when tracing to ensure the exported model works for dynamic input shapes\n+        if not torch.jit.is_tracing() and num_patches == num_positions and height == width:\n+            return self.position_embedding(self.position_ids)\n+\n+        class_pos_embed = position_embedding[:, :1]\n+        patch_pos_embed = position_embedding[:, 1:]\n+\n+        dim = embeddings.shape[-1]\n+\n+        new_height = height // self.patch_size\n+        new_width = width // self.patch_size\n+\n+        sqrt_num_positions = torch_int(num_positions**0.5)\n+        patch_pos_embed = patch_pos_embed.reshape(1, sqrt_num_positions, sqrt_num_positions, dim)\n+        patch_pos_embed = patch_pos_embed.permute(0, 3, 1, 2)\n+\n+        patch_pos_embed = nn.functional.interpolate(\n+            patch_pos_embed,\n+            size=(new_height, new_width),\n+            mode=\"bicubic\",\n+            align_corners=False,\n+        )\n+\n+        patch_pos_embed = patch_pos_embed.permute(0, 2, 3, 1).view(1, -1, dim)\n+\n+        return torch.cat((class_pos_embed, patch_pos_embed), dim=1)\n+\n+    def forward(self, pixel_values: torch.FloatTensor, interpolate_pos_encoding=False) -> torch.Tensor:\n+        batch_size, _, height, width = pixel_values.shape\n+        if not interpolate_pos_encoding and (height != self.image_size or width != self.image_size):\n+            raise ValueError(\n+                f\"Input image size ({height}*{width}) doesn't match model ({self.image_size}*{self.image_size}).\"\n+            )\n+        target_dtype = self.patch_embedding.weight.dtype\n+        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))  # shape = [*, width, grid, grid]\n+        patch_embeds = patch_embeds.flatten(2).transpose(1, 2)\n+\n+        class_embeds = self.class_embedding.expand(batch_size, 1, -1)\n+        embeddings = torch.cat([class_embeds, patch_embeds], dim=1)\n+        if interpolate_pos_encoding:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+        else:\n+            embeddings = embeddings + self.position_embedding(self.position_ids)\n+        return embeddings\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    output_attentions: bool = True,\n+    **kwargs,\n+):\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+    if attention_mask is not None:\n+        attn_weights = attn_weights + attention_mask\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+    if not output_attentions:\n+        attn_weights = None\n+    return attn_output, attn_weights\n+\n+\n+class MetaClip2Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Union[MetaClip2VisionConfig, MetaClip2TextConfig]):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+        self.is_causal = False\n+\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        causal_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, seq_length, embed_dim = hidden_states.shape\n+\n+        queries = self.q_proj(hidden_states)\n+        keys = self.k_proj(hidden_states)\n+        values = self.v_proj(hidden_states)\n+\n+        queries = queries.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        keys = keys.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        values = values.view(batch_size, seq_length, -1, self.head_dim).transpose(1, 2)\n+        # METACLIP_2 text model uses both `causal_attention_mask` and `attention_mask`\n+        # in case FA2 kernel is called, `is_causal` should be inferred from `causal_attention_mask`\n+        if self.config._attn_implementation == \"flash_attention_2\":\n+            self.is_causal = causal_attention_mask is not None\n+        else:\n+            if attention_mask is not None and causal_attention_mask is not None:\n+                attention_mask = attention_mask + causal_attention_mask\n+            elif causal_attention_mask is not None:\n+                attention_mask = causal_attention_mask\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            queries,\n+            keys,\n+            values,\n+            attention_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scale,\n+            dropout=0.0 if not self.training else self.dropout,\n+            output_attentions=output_attentions,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, seq_length, embed_dim).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+        return attn_output, attn_weights\n+\n+\n+class MetaClip2MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+@auto_docstring\n+class MetaClip2PreTrainedModel(PreTrainedModel):\n+    config: MetaClip2Config\n+    base_model_prefix = \"metaclip_2\"\n+    supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        factor = self.config.initializer_factor\n+        if isinstance(module, MetaClip2TextEmbeddings):\n+            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+        elif isinstance(module, MetaClip2VisionEmbeddings):\n+            factor = self.config.initializer_factor\n+            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+        elif isinstance(module, MetaClip2Attention):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            out_proj_std = (module.embed_dim**-0.5) * factor\n+            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+        elif isinstance(module, MetaClip2MLP):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n+            nn.init.normal_(module.fc1.weight, std=fc_std)\n+            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+        elif isinstance(module, MetaClip2Model):\n+            nn.init.normal_(\n+                module.text_projection.weight,\n+                std=module.text_embed_dim**-0.5 * self.config.initializer_factor,\n+            )\n+            nn.init.normal_(\n+                module.visual_projection.weight,\n+                std=module.vision_embed_dim**-0.5 * self.config.initializer_factor,\n+            )\n+        elif isinstance(module, MetaClip2VisionModelWithProjection):\n+            nn.init.normal_(\n+                module.visual_projection.weight,\n+                std=self.config.hidden_size**-0.5 * self.config.initializer_factor,\n+            )\n+        elif isinstance(module, MetaClip2TextModelWithProjection):\n+            nn.init.normal_(\n+                module.text_projection.weight,\n+                std=self.config.hidden_size**-0.5 * self.config.initializer_factor,\n+            )\n+        elif isinstance(module, MetaClip2ForImageClassification):\n+            nn.init.normal_(\n+                module.classifier.weight,\n+                std=self.config.vision_config.hidden_size**-0.5 * self.config.initializer_factor,\n+            )\n+\n+        if isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        if isinstance(module, nn.Linear) and module.bias is not None:\n+            module.bias.data.zero_()\n+\n+\n+class MetaClip2EncoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Union[MetaClip2VisionConfig, MetaClip2TextConfig]):\n+        super().__init__()\n+        self.embed_dim = config.hidden_size\n+        self.self_attn = MetaClip2Attention(config)\n+        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = MetaClip2MLP(config)\n+        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        causal_attention_mask: torch.Tensor,\n+        output_attentions: Optional[bool] = False,\n+    ) -> tuple[torch.FloatTensor]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`): input to the layer of shape `(batch, seq_len, embed_dim)`\n+            attention_mask (`torch.FloatTensor`): attention mask of size\n+                `(batch, 1, tgt_len, src_len)` where padding elements are indicated by very large negative values.\n+                `(config.encoder_attention_heads,)`.\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            causal_attention_mask=causal_attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class MetaClip2Encoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`MetaClip2EncoderLayer`].\n+\n+    Args:\n+        config: MetaClip2Config\n+    \"\"\"\n+\n+    def __init__(self, config: MetaClip2Config):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([MetaClip2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        causal_attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutput:\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+                than the model's internal embedding lookup matrix.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            causal_attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Causal mask for the text model. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        hidden_states = inputs_embeds\n+        for idx, encoder_layer in enumerate(self.layers):\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+            layer_outputs = encoder_layer(\n+                hidden_states,\n+                attention_mask,\n+                causal_attention_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states,\n+            hidden_states=encoder_states,\n+            attentions=all_attentions,\n+        )\n+\n+\n+class MetaClip2TextTransformer(nn.Module):\n+    def __init__(self, config: MetaClip2TextConfig):\n+        super().__init__()\n+        self.config = config\n+        embed_dim = config.hidden_size\n+        self.embeddings = MetaClip2TextEmbeddings(config)\n+        self.encoder = MetaClip2Encoder(config)\n+        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+\n+        # For `pooled_output` computation\n+        self.eos_token_id = config.eos_token_id\n+\n+        # For attention mask, it differs between `flash_attention_2` and other attention implementations\n+        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPooling:\n+        input_shape = input_ids.size()\n+        input_ids = input_ids.view(-1, input_shape[-1])\n+\n+        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n+\n+        # CLIP's text model uses causal mask, prepare it here.\n+        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n+        causal_attention_mask = _create_4d_causal_attention_mask(\n+            input_shape, hidden_states.dtype, device=hidden_states.device\n+        )\n+\n+        # expand attention_mask\n+        if attention_mask is not None and not self._use_flash_attention_2:\n+            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n+            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+\n+        encoder_outputs: BaseModelOutput = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            causal_attention_mask=causal_attention_mask,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = encoder_outputs.last_hidden_state\n+        last_hidden_state = self.final_layer_norm(last_hidden_state)\n+\n+        # Use robust pooling like CLIP - finds the first EOS token position per sequence\n+        pooled_output = last_hidden_state[\n+            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n+            (input_ids.to(dtype=torch.int, device=last_hidden_state.device) == self.eos_token_id).int().argmax(dim=-1),\n+        ]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The text model from METACLIP_2 without any head or projection on top.\n+    \"\"\"\n+)\n+class MetaClip2TextModel(MetaClip2PreTrainedModel):\n+    config: MetaClip2TextConfig\n+\n+    _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n+\n+    def __init__(self, config: MetaClip2TextConfig):\n+        super().__init__(config)\n+        self.text_model = MetaClip2TextTransformer(config)\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.text_model.embeddings.token_embedding\n+\n+    def set_input_embeddings(self, value):\n+        self.text_model.embeddings.token_embedding = value\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> BaseModelOutputWithPooling:\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, MetaClip2TextModel\n+\n+        >>> model = MetaClip2TextModel.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+\n+        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> last_hidden_state = outputs.last_hidden_state\n+        >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n+        ```\"\"\"\n+\n+        return self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for text model's outputs that also contains a pooling of the last hidden states.\n+    \"\"\"\n+)\n+class MetaClip2TextModelOutput(ModelOutput):\n+    r\"\"\"\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The text embeddings obtained by applying the projection layer to the pooler_output.\n+    \"\"\"\n+\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@auto_docstring\n+class MetaClip2TextModelWithProjection(MetaClip2PreTrainedModel):\n+    config: MetaClip2TextConfig\n+\n+    _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\"]\n+\n+    def __init__(self, config: MetaClip2TextConfig):\n+        super().__init__(config)\n+\n+        text_model = MetaClip2TextModel._from_config(config)\n+        self.text_model = text_model.text_model\n+\n+        self.text_projection = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.text_model.embeddings.token_embedding\n+\n+    def set_input_embeddings(self, value):\n+        self.text_model.embeddings.token_embedding = value\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> MetaClip2TextModelOutput:\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, MetaClip2TextModelWithProjection\n+\n+        >>> model = MetaClip2TextModelWithProjection.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+\n+        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> text_embeds = outputs.text_embeds\n+        ```\"\"\"\n+\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+        pooled_output = text_outputs.pooler_output\n+        text_embeds = self.text_projection(pooled_output)\n+\n+        return MetaClip2TextModelOutput(\n+            text_embeds=text_embeds,\n+            last_hidden_state=text_outputs.last_hidden_state,\n+            hidden_states=text_outputs.hidden_states,\n+            attentions=text_outputs.attentions,\n+        )\n+\n+\n+@dataclass\n+@auto_docstring\n+class MetaClip2Output(ModelOutput):\n+    r\"\"\"\n+    loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+        Contrastive loss for image-text similarity.\n+    logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+        The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+        similarity scores.\n+    logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+        The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+        similarity scores.\n+    text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The text embeddings obtained by applying the projection layer to the pooled output of [`MetaClip2TextModel`].\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+        The image embeddings obtained by applying the projection layer to the pooled output of [`MetaClip2VisionModel`].\n+    text_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`MetaClip2TextModel`].\n+    vision_model_output (`BaseModelOutputWithPooling`):\n+        The output of the [`MetaClip2VisionModel`].\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits_per_image: Optional[torch.FloatTensor] = None\n+    logits_per_text: Optional[torch.FloatTensor] = None\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    image_embeds: Optional[torch.FloatTensor] = None\n+    text_model_output: BaseModelOutputWithPooling = None\n+    vision_model_output: BaseModelOutputWithPooling = None\n+\n+    def to_tuple(self) -> tuple[Any]:\n+        return tuple(\n+            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n+            for k in self.keys()\n+        )\n+\n+\n+class MetaClip2VisionTransformer(nn.Module):\n+    def __init__(self, config: MetaClip2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = MetaClip2VisionEmbeddings(config)\n+        self.pre_layrnorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.encoder = MetaClip2Encoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: Optional[bool] = False,\n+    ) -> BaseModelOutputWithPooling:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        if pixel_values is None:\n+            raise ValueError(\"You have to specify pixel_values\")\n+\n+        hidden_states = self.embeddings(pixel_values, interpolate_pos_encoding=interpolate_pos_encoding)\n+        hidden_states = self.pre_layrnorm(hidden_states)\n+\n+        encoder_outputs: BaseModelOutput = self.encoder(\n+            inputs_embeds=hidden_states,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        last_hidden_state = encoder_outputs.last_hidden_state\n+        pooled_output = last_hidden_state[:, 0, :]\n+        pooled_output = self.post_layernorm(pooled_output)\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+# contrastive loss function, adapted from\n+# https://sachinruk.github.io/blog/2021-03-07-metaclip_2.html\n+def contrastive_loss(logits: torch.Tensor) -> torch.Tensor:\n+    return nn.functional.cross_entropy(logits, torch.arange(len(logits), device=logits.device))\n+\n+\n+def metaclip_2_loss(similarity: torch.Tensor) -> torch.Tensor:\n+    caption_loss = contrastive_loss(similarity)\n+    image_loss = contrastive_loss(similarity.t())\n+    return (caption_loss + image_loss) / 2.0\n+\n+\n+def _get_vector_norm(tensor: torch.Tensor) -> torch.Tensor:\n+    \"\"\"\n+    This method is equivalent to tensor.norm(p=2, dim=-1, keepdim=True) and used to make\n+    model `executorch` exportable. See issue https://github.com/pytorch/executorch/issues/3566\n+    \"\"\"\n+    square_tensor = torch.pow(tensor, 2)\n+    sum_tensor = torch.sum(square_tensor, dim=-1, keepdim=True)\n+    normed_tensor = torch.pow(sum_tensor, 0.5)\n+    return normed_tensor\n+\n+\n+@auto_docstring\n+class MetaClip2Model(MetaClip2PreTrainedModel):\n+    config: MetaClip2Config\n+    _no_split_modules = [\"MetaClip2TextEmbeddings\", \"MetaClip2EncoderLayer\", \"MetaClip2VisionEmbeddings\"]\n+\n+    def __init__(self, config: MetaClip2Config):\n+        super().__init__(config)\n+\n+        if not isinstance(config.text_config, MetaClip2TextConfig):\n+            raise TypeError(\n+                \"config.text_config is expected to be of type MetaClip2TextConfig but is of type\"\n+                f\" {type(config.text_config)}.\"\n+            )\n+\n+        if not isinstance(config.vision_config, MetaClip2VisionConfig):\n+            raise TypeError(\n+                \"config.vision_config is expected to be of type MetaClip2VisionConfig but is of type\"\n+                f\" {type(config.vision_config)}.\"\n+            )\n+\n+        text_config = config.text_config\n+        vision_config = config.vision_config\n+\n+        self.projection_dim = config.projection_dim\n+        self.text_embed_dim = text_config.hidden_size\n+        self.vision_embed_dim = vision_config.hidden_size\n+\n+        text_model = MetaClip2TextModel._from_config(text_config)\n+        self.text_model = text_model.text_model\n+\n+        vision_model = MetaClip2VisionModel._from_config(vision_config)\n+        self.vision_model = vision_model.vision_model\n+\n+        self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)\n+        self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)\n+        self.logit_scale = nn.Parameter(torch.tensor(self.config.logit_scale_init_value))\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @auto_docstring\n+    def get_text_features(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> torch.FloatTensor:\n+        r\"\"\"\n+        Returns:\n+            text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\n+            applying the projection layer to the pooled output of [`MetaClip2TextModel`].\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, MetaClip2Model\n+\n+        >>> model = MetaClip2Model.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+\n+        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=True, return_tensors=\"pt\")\n+        >>> text_features = model.get_text_features(**inputs)\n+        ```\"\"\"\n+        # Use METACLIP_2 model's config for some fields (if specified) instead of those of vision & text components.\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        pooled_output = text_outputs.pooler_output\n+        text_features = self.text_projection(pooled_output)\n+\n+        return text_features\n+\n+    @auto_docstring\n+    def get_image_features(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ) -> torch.FloatTensor:\n+        r\"\"\"\n+        Returns:\n+            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n+            applying the projection layer to the pooled output of [`MetaClip2VisionModel`].\n+\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, MetaClip2Model\n+\n+        >>> model = MetaClip2Model.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> processor = AutoProcessor.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> image_features = model.get_image_features(**inputs)\n+        ```\"\"\"\n+        # Use METACLIP_2 model's config for some fields (if specified) instead of those of vision & text components.\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n+\n+        pooled_output = vision_outputs.pooler_output\n+        image_features = self.visual_projection(pooled_output)\n+\n+        return image_features\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        return_loss: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ) -> MetaClip2Output:\n+        r\"\"\"\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n+\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, MetaClip2Model\n+\n+        >>> model = MetaClip2Model.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> processor = AutoProcessor.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(\n+        ...     text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, return_tensors=\"pt\", padding=True\n+        ... )\n+\n+        >>> outputs = model(**inputs)\n+        >>> logits_per_image = outputs.logits_per_image  # this is the image-text similarity score\n+        >>> probs = logits_per_image.softmax(dim=1)  # we can take the softmax to get the label probabilities\n+        ```\"\"\"\n+        # Use METACLIP_2 model's config for some fields (if specified) instead of those of vision & text components.\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n+\n+        text_outputs: BaseModelOutputWithPooling = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        image_embeds = vision_outputs.pooler_output\n+        image_embeds = self.visual_projection(image_embeds)\n+\n+        text_embeds = text_outputs.pooler_output\n+        text_embeds = self.text_projection(text_embeds)\n+\n+        # normalized features\n+        image_embeds = image_embeds / _get_vector_norm(image_embeds)\n+        text_embeds = text_embeds / _get_vector_norm(text_embeds)\n+\n+        # cosine similarity as logits\n+        logits_per_text = torch.matmul(text_embeds, image_embeds.t().to(text_embeds.device))\n+        logits_per_text = logits_per_text * self.logit_scale.exp().to(text_embeds.device)\n+\n+        logits_per_image = logits_per_text.t()\n+\n+        loss = None\n+        if return_loss:\n+            loss = metaclip_2_loss(logits_per_text)\n+\n+        return MetaClip2Output(\n+            loss=loss,\n+            logits_per_image=logits_per_image,\n+            logits_per_text=logits_per_text,\n+            text_embeds=text_embeds,\n+            image_embeds=image_embeds,\n+            text_model_output=text_outputs,\n+            vision_model_output=vision_outputs,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    The vision model from METACLIP_2 without any head or projection on top.\n+    \"\"\"\n+)\n+class MetaClip2VisionModel(MetaClip2PreTrainedModel):\n+    config: MetaClip2VisionConfig\n+    main_input_name = \"pixel_values\"\n+    _no_split_modules = [\"MetaClip2EncoderLayer\"]\n+\n+    def __init__(self, config: MetaClip2VisionConfig):\n+        super().__init__(config)\n+        self.vision_model = MetaClip2VisionTransformer(config)\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.vision_model.embeddings.patch_embedding\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ) -> BaseModelOutputWithPooling:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, MetaClip2VisionModel\n+\n+        >>> model = MetaClip2VisionModel.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> processor = AutoProcessor.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> last_hidden_state = outputs.last_hidden_state\n+        >>> pooled_output = outputs.pooler_output  # pooled CLS states\n+        ```\"\"\"\n+\n+        return self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n+\n+\n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n+    \"\"\"\n+)\n+class MetaClip2VisionModelOutput(ModelOutput):\n+    r\"\"\"\n+    image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+        The image embeddings obtained by applying the projection layer to the pooler_output.\n+    \"\"\"\n+\n+    image_embeds: Optional[torch.FloatTensor] = None\n+    last_hidden_state: Optional[torch.FloatTensor] = None\n+    hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@auto_docstring\n+class MetaClip2VisionModelWithProjection(MetaClip2PreTrainedModel):\n+    config: MetaClip2VisionConfig\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(self, config: MetaClip2VisionConfig):\n+        super().__init__(config)\n+\n+        vision_model = MetaClip2VisionModel._from_config(config)\n+        self.vision_model = vision_model.vision_model\n+\n+        self.visual_projection = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.vision_model.embeddings.patch_embedding\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+    ) -> MetaClip2VisionModelOutput:\n+        r\"\"\"\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, MetaClip2VisionModelWithProjection\n+\n+        >>> model = MetaClip2VisionModelWithProjection.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+        >>> processor = AutoProcessor.from_pretrained(\"openai/metaclip_2-vit-base-patch32\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> image_embeds = outputs.image_embeds\n+        ```\"\"\"\n+\n+        vision_outputs: BaseModelOutputWithPooling = self.vision_model(\n+            pixel_values=pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+        )\n+        pooled_output = vision_outputs.pooler_output\n+        image_embeds = self.visual_projection(pooled_output)\n+\n+        return MetaClip2VisionModelOutput(\n+            image_embeds=image_embeds,\n+            last_hidden_state=vision_outputs.last_hidden_state,\n+            hidden_states=vision_outputs.hidden_states,\n+            attentions=vision_outputs.attentions,\n+        )\n+\n+\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    METACLIP_2 vision encoder with an image classification head on top (a linear layer on top of the pooled final hidden states of\n+    the patch tokens) e.g. for ImageNet.\n+    \"\"\"\n+)\n+class MetaClip2ForImageClassification(MetaClip2PreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(self, config: MetaClip2Config) -> None:\n+        super().__init__(config)\n+\n+        self.num_labels = config.num_labels\n+        vision_model = MetaClip2VisionModel._from_config(config.vision_config)\n+        self.vision_model = vision_model.vision_model\n+\n+        # Classifier head\n+        self.classifier = (\n+            nn.Linear(config.vision_config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+    ) -> ImageClassifierOutput:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+\n+        outputs: BaseModelOutputWithPooling = self.vision_model(\n+            pixel_values,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+        )\n+\n+        sequence_output = outputs.last_hidden_state\n+\n+        # average pool the patch tokens\n+        sequence_output = torch.mean(sequence_output[:, 1:, :], dim=1)\n+        # apply classifier\n+        logits = self.classifier(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(logits.device)\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        return ImageClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"MetaClip2Model\",\n+    \"MetaClip2PreTrainedModel\",\n+    \"MetaClip2TextModel\",\n+    \"MetaClip2TextModelWithProjection\",\n+    \"MetaClip2VisionModel\",\n+    \"MetaClip2VisionModelWithProjection\",\n+    \"MetaClip2ForImageClassification\",\n+]"
        },
        {
            "sha": "d4c259849e69ec0bcbbe1c734a55ab8225036839",
            "filename": "src/transformers/models/metaclip_2/modular_metaclip_2.py",
            "status": "added",
            "additions": 246,
            "deletions": 0,
            "changes": 246,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmetaclip_2%2Fmodular_metaclip_2.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -0,0 +1,246 @@\n+from typing import Optional\n+\n+import torch\n+from torch import nn\n+\n+from ...modeling_attn_mask_utils import _create_4d_causal_attention_mask, _prepare_4d_attention_mask\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import TransformersKwargs, auto_docstring, logging\n+from ...utils.generic import check_model_inputs\n+from ..clip.configuration_clip import CLIPConfig, CLIPTextConfig, CLIPVisionConfig\n+from ..clip.modeling_clip import (\n+    CLIPMLP,\n+    CLIPAttention,\n+    CLIPForImageClassification,\n+    CLIPModel,\n+    CLIPTextEmbeddings,\n+    CLIPTextModel,\n+    CLIPTextModelWithProjection,\n+    CLIPTextTransformer,\n+    CLIPVisionEmbeddings,\n+    CLIPVisionModel,\n+    CLIPVisionModelWithProjection,\n+)\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class MetaClip2TextConfig(CLIPTextConfig):\n+    pass\n+\n+\n+class MetaClip2VisionConfig(CLIPVisionConfig):\n+    pass\n+\n+\n+class MetaClip2Config(CLIPConfig):\n+    pass\n+\n+\n+class MetaClip2TextEmbeddings(CLIPTextEmbeddings):\n+    pass\n+\n+\n+class MetaClip2VisionEmbeddings(CLIPVisionEmbeddings):\n+    pass\n+\n+\n+class MetaClip2Attention(CLIPAttention):\n+    pass\n+\n+\n+class MetaClip2MLP(CLIPMLP):\n+    pass\n+\n+\n+@auto_docstring\n+class MetaClip2PreTrainedModel(PreTrainedModel):\n+    config: MetaClip2Config\n+    base_model_prefix = \"metaclip_2\"\n+    supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n+    _supports_flash_attn = True\n+    _supports_flex_attn = True\n+    _supports_attention_backend = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        factor = self.config.initializer_factor\n+        if isinstance(module, MetaClip2TextEmbeddings):\n+            module.token_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+            module.position_embedding.weight.data.normal_(mean=0.0, std=factor * 0.02)\n+        elif isinstance(module, MetaClip2VisionEmbeddings):\n+            factor = self.config.initializer_factor\n+            nn.init.normal_(module.class_embedding, mean=0.0, std=module.embed_dim**-0.5 * factor)\n+            nn.init.normal_(module.patch_embedding.weight, std=module.config.initializer_range * factor)\n+            nn.init.normal_(module.position_embedding.weight, std=module.config.initializer_range * factor)\n+        elif isinstance(module, MetaClip2Attention):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.embed_dim**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            out_proj_std = (module.embed_dim**-0.5) * factor\n+            nn.init.normal_(module.q_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.k_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.v_proj.weight, std=in_proj_std)\n+            nn.init.normal_(module.out_proj.weight, std=out_proj_std)\n+        elif isinstance(module, MetaClip2MLP):\n+            factor = self.config.initializer_factor\n+            in_proj_std = (module.config.hidden_size**-0.5) * ((2 * module.config.num_hidden_layers) ** -0.5) * factor\n+            fc_std = (2 * module.config.hidden_size) ** -0.5 * factor\n+            nn.init.normal_(module.fc1.weight, std=fc_std)\n+            nn.init.normal_(module.fc2.weight, std=in_proj_std)\n+        elif isinstance(module, MetaClip2Model):\n+            nn.init.normal_(\n+                module.text_projection.weight,\n+                std=module.text_embed_dim**-0.5 * self.config.initializer_factor,\n+            )\n+            nn.init.normal_(\n+                module.visual_projection.weight,\n+                std=module.vision_embed_dim**-0.5 * self.config.initializer_factor,\n+            )\n+        elif isinstance(module, MetaClip2VisionModelWithProjection):\n+            nn.init.normal_(\n+                module.visual_projection.weight,\n+                std=self.config.hidden_size**-0.5 * self.config.initializer_factor,\n+            )\n+        elif isinstance(module, MetaClip2TextModelWithProjection):\n+            nn.init.normal_(\n+                module.text_projection.weight,\n+                std=self.config.hidden_size**-0.5 * self.config.initializer_factor,\n+            )\n+        elif isinstance(module, MetaClip2ForImageClassification):\n+            nn.init.normal_(\n+                module.classifier.weight,\n+                std=self.config.vision_config.hidden_size**-0.5 * self.config.initializer_factor,\n+            )\n+\n+        if isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+        if isinstance(module, nn.Linear) and module.bias is not None:\n+            module.bias.data.zero_()\n+\n+\n+class MetaClip2TextTransformer(CLIPTextTransformer):\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPooling:\n+        input_shape = input_ids.size()\n+        input_ids = input_ids.view(-1, input_shape[-1])\n+\n+        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n+\n+        # CLIP's text model uses causal mask, prepare it here.\n+        # https://github.com/openai/CLIP/blob/cfcffb90e69f37bf2ff1e988237a0fbe41f33c04/clip/model.py#L324\n+        causal_attention_mask = _create_4d_causal_attention_mask(\n+            input_shape, hidden_states.dtype, device=hidden_states.device\n+        )\n+\n+        # expand attention_mask\n+        if attention_mask is not None and not self._use_flash_attention_2:\n+            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n+            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+\n+        encoder_outputs: BaseModelOutput = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            causal_attention_mask=causal_attention_mask,\n+            **kwargs,\n+        )\n+\n+        last_hidden_state = encoder_outputs.last_hidden_state\n+        last_hidden_state = self.final_layer_norm(last_hidden_state)\n+\n+        # Use robust pooling like CLIP - finds the first EOS token position per sequence\n+        pooled_output = last_hidden_state[\n+            torch.arange(last_hidden_state.shape[0], device=last_hidden_state.device),\n+            (input_ids.to(dtype=torch.int, device=last_hidden_state.device) == self.eos_token_id).int().argmax(dim=-1),\n+        ]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+class MetaClip2TextModel(CLIPTextModel):\n+    def __init__(self, config: MetaClip2TextConfig):\n+        super().__init__(config)\n+        self.text_model = MetaClip2TextTransformer(config)\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+\n+class MetaClip2TextModelWithProjection(CLIPTextModelWithProjection):\n+    def __init__(self, config: MetaClip2TextConfig):\n+        super().__init__(config)\n+\n+        text_model = MetaClip2TextModel._from_config(config)\n+        self.text_model = text_model.text_model\n+\n+        self.text_projection = nn.Linear(config.hidden_size, config.projection_dim, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+\n+class MetaClip2Model(CLIPModel):\n+    def __init__(self, config: MetaClip2Config):\n+        super().__init__(config)\n+\n+        text_config = config.text_config\n+        vision_config = config.vision_config\n+\n+        self.projection_dim = config.projection_dim\n+        self.text_embed_dim = text_config.hidden_size\n+        self.vision_embed_dim = vision_config.hidden_size\n+\n+        text_model = MetaClip2TextModel._from_config(text_config)\n+        self.text_model = text_model.text_model\n+\n+        vision_model = MetaClip2VisionModel._from_config(vision_config)\n+        self.vision_model = vision_model.vision_model\n+\n+        self.visual_projection = nn.Linear(self.vision_embed_dim, self.projection_dim, bias=False)\n+        self.text_projection = nn.Linear(self.text_embed_dim, self.projection_dim, bias=False)\n+        self.logit_scale = nn.Parameter(torch.tensor(self.config.logit_scale_init_value))\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+\n+class MetaClip2VisionModel(CLIPVisionModel):\n+    pass\n+\n+\n+class MetaClip2VisionModelWithProjection(CLIPVisionModelWithProjection):\n+    pass\n+\n+\n+class MetaClip2ForImageClassification(CLIPForImageClassification):\n+    pass\n+\n+\n+__all__ = [\n+    \"MetaClip2Config\",\n+    \"MetaClip2TextConfig\",\n+    \"MetaClip2VisionConfig\",\n+    \"MetaClip2Model\",\n+    \"MetaClip2PreTrainedModel\",\n+    \"MetaClip2TextModel\",\n+    \"MetaClip2TextModelWithProjection\",\n+    \"MetaClip2VisionModel\",\n+    \"MetaClip2VisionModelWithProjection\",\n+    \"MetaClip2ForImageClassification\",\n+]"
        },
        {
            "sha": "710959323e45b865c6261da4ff2a72333d11021b",
            "filename": "tests/models/clip/test_processing_clip.py",
            "status": "modified",
            "additions": 12,
            "deletions": 30,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/tests%2Fmodels%2Fclip%2Ftest_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/tests%2Fmodels%2Fclip%2Ftest_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_processing_clip.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -12,18 +12,15 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import json\n-import os\n import shutil\n import tempfile\n import unittest\n \n import pytest\n \n-from transformers import CLIPTokenizer, CLIPTokenizerFast\n-from transformers.models.clip.tokenization_clip import VOCAB_FILES_NAMES\n+from transformers import AutoTokenizer, CLIPTokenizer, CLIPTokenizerFast\n from transformers.testing_utils import require_vision\n-from transformers.utils import IMAGE_PROCESSOR_NAME, is_vision_available\n+from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n@@ -32,38 +29,23 @@\n     from transformers import CLIPImageProcessor, CLIPProcessor\n \n \n+TEST_MODEL_PATH = \"openai/clip-vit-base-patch32\"\n+\n+\n @require_vision\n class CLIPProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = CLIPProcessor\n \n     @classmethod\n     def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n-\n-        vocab = [\"l\", \"o\", \"w\", \"e\", \"r\", \"s\", \"t\", \"i\", \"d\", \"n\", \"lo\", \"l</w>\", \"w</w>\", \"r</w>\", \"t</w>\", \"low</w>\", \"er</w>\", \"lowest</w>\", \"newer</w>\", \"wider\", \"<unk>\", \"<|startoftext|>\", \"<|endoftext|>\"]  # fmt: skip\n-        vocab_tokens = dict(zip(vocab, range(len(vocab))))\n-        merges = [\"#version: 0.2\", \"l o\", \"lo w</w>\", \"e r</w>\", \"\"]\n-        cls.special_tokens_map = {\"unk_token\": \"<unk>\"}\n-\n-        cls.vocab_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"vocab_file\"])\n-        cls.merges_file = os.path.join(cls.tmpdirname, VOCAB_FILES_NAMES[\"merges_file\"])\n-        with open(cls.vocab_file, \"w\", encoding=\"utf-8\") as fp:\n-            fp.write(json.dumps(vocab_tokens) + \"\\n\")\n-        with open(cls.merges_file, \"w\", encoding=\"utf-8\") as fp:\n-            fp.write(\"\\n\".join(merges))\n-\n-        image_processor_map = {\n-            \"do_resize\": True,\n-            \"size\": 20,\n-            \"do_center_crop\": True,\n-            \"crop_size\": 18,\n-            \"do_normalize\": True,\n-            \"image_mean\": [0.48145466, 0.4578275, 0.40821073],\n-            \"image_std\": [0.26862954, 0.26130258, 0.27577711],\n-        }\n-        cls.image_processor_file = os.path.join(cls.tmpdirname, IMAGE_PROCESSOR_NAME)\n-        with open(cls.image_processor_file, \"w\", encoding=\"utf-8\") as fp:\n-            json.dump(image_processor_map, fp)\n+        tokenizer = AutoTokenizer.from_pretrained(TEST_MODEL_PATH)\n+        image_processor = CLIPImageProcessor.from_pretrained(TEST_MODEL_PATH)\n+        processor = CLIPProcessor(\n+            image_processor=image_processor,\n+            tokenizer=tokenizer,\n+        )\n+        processor.save_pretrained(cls.tmpdirname)\n \n     @classmethod\n     def get_tokenizer(cls, **kwargs):"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/metaclip_2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/tests%2Fmodels%2Fmetaclip_2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/tests%2Fmodels%2Fmetaclip_2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmetaclip_2%2F__init__.py?ref=1d4609173780c973c2f52ee21249856724277922"
        },
        {
            "sha": "26308bc9c413de3102d48a8a3cb997794aa252f9",
            "filename": "tests/models/metaclip_2/test_modeling_metaclip_2.py",
            "status": "added",
            "additions": 927,
            "deletions": 0,
            "changes": 927,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmetaclip_2%2Ftest_modeling_metaclip_2.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -0,0 +1,927 @@\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch MetaClip2 model.\"\"\"\n+\n+import inspect\n+import os\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+import requests\n+from parameterized import parameterized\n+from pytest import mark\n+\n+from transformers import MetaClip2Config, MetaClip2TextConfig, MetaClip2VisionConfig\n+from transformers.testing_utils import (\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_gpu,\n+    require_torch_sdpa,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import (\n+    is_torch_available,\n+    is_vision_available,\n+)\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION,\n+    ModelTesterMixin,\n+    _config_zero_init,\n+    floats_tensor,\n+    ids_tensor,\n+    is_flaky,\n+    random_attention_mask,\n+)\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import (\n+        MetaClip2ForImageClassification,\n+        MetaClip2Model,\n+        MetaClip2TextModel,\n+        MetaClip2TextModelWithProjection,\n+        MetaClip2VisionModel,\n+        MetaClip2VisionModelWithProjection,\n+    )\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import CLIPProcessor\n+\n+\n+class MetaClip2VisionModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=12,\n+        image_size=30,\n+        patch_size=2,\n+        num_channels=3,\n+        is_training=True,\n+        hidden_size=32,\n+        projection_dim=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        dropout=0.1,\n+        attention_dropout=0.1,\n+        initializer_range=0.02,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.image_size = image_size\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.hidden_size = hidden_size\n+        self.projection_dim = projection_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.scope = scope\n+\n+        # in ViT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n+        num_patches = (image_size // patch_size) ** 2\n+        self.seq_length = num_patches + 1\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def get_config(self):\n+        return MetaClip2VisionConfig(\n+            image_size=self.image_size,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            projection_dim=self.projection_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+            initializer_range=self.initializer_range,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values):\n+        model = MetaClip2VisionModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n+        image_size = (self.image_size, self.image_size)\n+        patch_size = (self.patch_size, self.patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n+        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n+\n+    def create_and_check_model_with_projection(self, config, pixel_values):\n+        model = MetaClip2VisionModelWithProjection(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values)\n+        # expected sequence length = num_patches + 1 (we add 1 for the [CLS] token)\n+        image_size = (self.image_size, self.image_size)\n+        patch_size = (self.patch_size, self.patch_size)\n+        num_patches = (image_size[1] // patch_size[1]) * (image_size[0] // patch_size[0])\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, num_patches + 1, self.hidden_size))\n+        self.parent.assertEqual(result.image_embeds.shape, (self.batch_size, self.projection_dim))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n+\n+\n+class MetaClip2ModelTesterMixin(ModelTesterMixin):\n+    \"\"\"\n+    Subclass of ModelTesterMixin with methods specific to testing MetaClip2 models.\n+    The SDPA equivalence test is overridden here because MetaClip2 models may have test/vision/text+vision inputs,\n+    different output logits, and are not supposed to be used or tested with padding_side=\"left\".\n+    \"\"\"\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # Load the model with SDPA (it is the default, but we explicit it for clarity)\n+                model_sdpa = model_class.from_pretrained(tmpdirname, attn_implementation=\"sdpa\")\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                # Load model with eager attention\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    attn_implementation=\"eager\",\n+                )\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+            if hasattr(model_sdpa, \"vision_model\"):\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+\n+            if hasattr(model_sdpa, \"text_model\"):\n+                self.assertTrue(model_sdpa.text_model.config._attn_implementation == \"sdpa\")\n+                self.assertTrue(model_eager.text_model.config._attn_implementation == \"eager\")\n+\n+            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+\n+@require_torch\n+class MetaClip2VisionModelTest(MetaClip2ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as MetaClip2 does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (MetaClip2VisionModel, MetaClip2VisionModelWithProjection) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+\n+    def setUp(self):\n+        self.model_tester = MetaClip2VisionModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self, config_class=MetaClip2VisionConfig, has_text_modality=False, hidden_size=37\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"MetaClip2 does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_model_with_projection(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model_with_projection(*config_and_inputs)\n+\n+    @unittest.skip\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"facebook/metaclip-2-worldwide-huge-quickgelu\"\n+        model = MetaClip2VisionModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+    @slow\n+    def test_model_with_projection_from_pretrained(self):\n+        model_name = \"facebook/metaclip-2-worldwide-huge-quickgelu\"\n+        model = MetaClip2VisionModelWithProjection.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+        self.assertTrue(hasattr(model, \"visual_projection\"))\n+\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    @is_flaky()\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        # adding only flaky decorator here and call the parent test method\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n+\n+class MetaClip2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=12,\n+        seq_length=7,\n+        is_training=True,\n+        use_input_mask=True,\n+        use_labels=True,\n+        vocab_size=99,\n+        hidden_size=32,\n+        projection_dim=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        dropout=0.1,\n+        attention_dropout=0.1,\n+        max_position_embeddings=512,\n+        initializer_range=0.02,\n+        eos_token_id=2,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.use_input_mask = use_input_mask\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.projection_dim = projection_dim\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.eos_token_id = eos_token_id\n+        self.scope = scope\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+        # ensure that the last token is the eos token\n+        input_ids[:, -1] = self.eos_token_id\n+\n+        input_mask = None\n+        if self.use_input_mask:\n+            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n+\n+        if input_mask is not None:\n+            batch_size, seq_length = input_mask.shape\n+            rnd_start_indices = np.random.randint(1, seq_length - 1, size=(batch_size,))\n+            for batch_idx, start_index in enumerate(rnd_start_indices):\n+                input_mask[batch_idx, :start_index] = 1\n+                input_mask[batch_idx, start_index:] = 0\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, input_mask\n+\n+    def get_config(self):\n+        return MetaClip2TextConfig(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            projection_dim=self.projection_dim,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+            max_position_embeddings=self.max_position_embeddings,\n+            initializer_range=self.initializer_range,\n+            eos_token_id=self.eos_token_id,\n+        )\n+\n+    def create_and_check_model(self, config, input_ids, input_mask):\n+        model = MetaClip2TextModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(input_ids, attention_mask=input_mask)\n+            result = model(input_ids)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n+\n+    def create_and_check_model_with_projection(self, config, input_ids, input_mask):\n+        model = MetaClip2TextModelWithProjection(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(input_ids, attention_mask=input_mask)\n+            result = model(input_ids)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+        self.parent.assertEqual(result.text_embeds.shape, (self.batch_size, self.projection_dim))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, input_mask = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class MetaClip2TextModelTest(MetaClip2ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (MetaClip2TextModel, MetaClip2TextModelWithProjection) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_head_masking = False\n+    model_split_percents = [0.5, 0.8, 0.9]\n+\n+    def setUp(self):\n+        self.model_tester = MetaClip2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=MetaClip2TextConfig, hidden_size=37)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_model_with_projection(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model_with_projection(*config_and_inputs)\n+\n+    @unittest.skip\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n+    )\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"MetaClip2 does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"facebook/metaclip-2-worldwide-huge-quickgelu\"\n+        model = MetaClip2TextModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+    @slow\n+    def test_model_with_projection_from_pretrained(self):\n+        model_name = \"facebook/metaclip-2-worldwide-huge-quickgelu\"\n+        model = MetaClip2TextModelWithProjection.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+        self.assertTrue(hasattr(model, \"text_projection\"))\n+\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    @slow\n+    @is_flaky()\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        # adding only flaky decorator here and call the parent test method\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        self.skipTest(\n+            reason=\"MetaClip2TextModel has two attention masks: `causal_attention_mask` and `attention_mask`\"\n+        )\n+\n+\n+class MetaClip2ModelTester:\n+    def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n+        if text_kwargs is None:\n+            text_kwargs = {}\n+        if vision_kwargs is None:\n+            vision_kwargs = {}\n+\n+        self.parent = parent\n+        self.text_model_tester = MetaClip2TextModelTester(parent, **text_kwargs)\n+        self.vision_model_tester = MetaClip2VisionModelTester(parent, **vision_kwargs)\n+        self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n+        self.is_training = is_training\n+\n+    def prepare_config_and_inputs(self):\n+        text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n+        vision_config, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, attention_mask, pixel_values\n+\n+    def get_config(self):\n+        return MetaClip2Config(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n+        )\n+\n+    def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n+        model = MetaClip2Model(config).to(torch_device).eval()\n+        with torch.no_grad():\n+            result = model(input_ids, pixel_values, attention_mask)\n+        self.parent.assertEqual(\n+            result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)\n+        )\n+        self.parent.assertEqual(\n+            result.logits_per_text.shape, (self.text_model_tester.batch_size, self.vision_model_tester.batch_size)\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, attention_mask, pixel_values = config_and_inputs\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"attention_mask\": attention_mask,\n+            \"pixel_values\": pixel_values,\n+            \"return_loss\": True,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class MetaClip2ModelTest(MetaClip2ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (MetaClip2Model,) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"feature-extraction\": MetaClip2Model, \"image-feature-extraction\": MetaClip2VisionModel}\n+        if is_torch_available()\n+        else {}\n+    )\n+    additional_model_inputs = [\"pixel_values\"]\n+    fx_compatible = False\n+    test_head_masking = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_attention_outputs = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = MetaClip2ModelTester(self)\n+        common_properties = [\"projection_dim\", \"logit_scale_init_value\"]\n+        self.config_tester = ConfigTester(\n+            self, config_class=MetaClip2Config, has_text_modality=False, common_properties=common_properties\n+        )\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Inputs_embeds is tested in individual model tests\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"MetaClip2Model does not have input/output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    # override as the `logit_scale` parameter initialization is different for MetaClip2\n+    def test_initialization(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        configs_no_init = _config_zero_init(config)\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            for name, param in model.named_parameters():\n+                if param.requires_grad:\n+                    # check if `logit_scale` is initialized as per the original implementation\n+                    if name == \"logit_scale\":\n+                        self.assertAlmostEqual(\n+                            param.data.item(),\n+                            np.log(1 / 0.07),\n+                            delta=1e-3,\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+                    else:\n+                        self.assertIn(\n+                            ((param.data.mean() * 1e9).round() / 1e9).item(),\n+                            [0.0, 1.0],\n+                            msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n+                        )\n+\n+    def _create_and_check_torchscript(self, config, inputs_dict):\n+        if not self.test_torchscript:\n+            self.skipTest(reason=\"test_torchscript is set to False\")\n+\n+        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n+        configs_no_init.torchscript = True\n+        configs_no_init.return_dict = False\n+        for model_class in self.all_model_classes:\n+            model = model_class(config=configs_no_init)\n+            model.to(torch_device)\n+            model.eval()\n+\n+            try:\n+                input_ids = inputs_dict[\"input_ids\"]\n+                pixel_values = inputs_dict[\"pixel_values\"]  # MetaClip2 needs pixel_values\n+                traced_model = torch.jit.trace(model, (input_ids, pixel_values))\n+            except RuntimeError:\n+                self.fail(\"Couldn't trace module.\")\n+\n+            with tempfile.TemporaryDirectory() as tmp_dir_name:\n+                pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n+\n+                try:\n+                    torch.jit.save(traced_model, pt_file_name)\n+                except Exception:\n+                    self.fail(\"Couldn't save module.\")\n+\n+                try:\n+                    loaded_model = torch.jit.load(pt_file_name)\n+                except Exception:\n+                    self.fail(\"Couldn't load module.\")\n+\n+            model.to(torch_device)\n+            model.eval()\n+\n+            loaded_model.to(torch_device)\n+            loaded_model.eval()\n+\n+            model_state_dict = model.state_dict()\n+            loaded_model_state_dict = loaded_model.state_dict()\n+\n+            non_persistent_buffers = {}\n+            for key in loaded_model_state_dict:\n+                if key not in model_state_dict:\n+                    non_persistent_buffers[key] = loaded_model_state_dict[key]\n+\n+            loaded_model_state_dict = {\n+                key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n+            }\n+\n+            self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n+\n+            model_buffers = list(model.buffers())\n+            for non_persistent_buffer in non_persistent_buffers.values():\n+                found_buffer = False\n+                for i, model_buffer in enumerate(model_buffers):\n+                    if torch.equal(non_persistent_buffer, model_buffer):\n+                        found_buffer = True\n+                        break\n+\n+                self.assertTrue(found_buffer)\n+                model_buffers.pop(i)\n+\n+            models_equal = True\n+            for layer_name, p1 in model_state_dict.items():\n+                p2 = loaded_model_state_dict[layer_name]\n+                if p1.data.ne(p2.data).sum() > 0:\n+                    models_equal = False\n+\n+            self.assertTrue(models_equal)\n+\n+    def test_load_vision_text_config(self):\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # Save MetaClip2Config and check if we can load MetaClip2VisionConfig from it\n+        with tempfile.TemporaryDirectory() as tmp_dir_name:\n+            config.save_pretrained(tmp_dir_name)\n+            vision_config = MetaClip2VisionConfig.from_pretrained(tmp_dir_name)\n+            self.assertDictEqual(config.vision_config.to_dict(), vision_config.to_dict())\n+\n+        # Save MetaClip2Config and check if we can load MetaClip2TextConfig from it\n+        with tempfile.TemporaryDirectory() as tmp_dir_name:\n+            config.save_pretrained(tmp_dir_name)\n+            text_config = MetaClip2TextConfig.from_pretrained(tmp_dir_name)\n+            self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"facebook/metaclip-2-worldwide-huge-quickgelu\"\n+        model = MetaClip2Model.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    @slow\n+    @is_flaky()\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        # adding only flaky decorator here and call the parent test method\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        self.skipTest(\n+            reason=\"MetaClip2 text tower has two attention masks: `causal_attention_mask` and `attention_mask`\"\n+        )\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_compile_dynamic(self):\n+        self.skipTest(reason=\"MetaClip2 model can't be compiled dynamic, error in metaclip_2_loss`\")\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_inference_equivalence(self):\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.bfloat16)\n+                model.to(torch_device)\n+\n+                dummy_pixel_values = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n+                dummy_input_ids = inputs_dict[\"input_ids\"]\n+\n+                outputs = model(pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True)\n+                outputs_fa = model_fa(\n+                    pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True\n+                )\n+\n+                self.assertTrue(\n+                    torch.allclose(outputs.logits_per_image, outputs_fa.logits_per_image, atol=4e-2, rtol=4e-2),\n+                    f\"Image logits max diff: {torch.max(torch.abs(outputs.logits_per_image - outputs_fa.logits_per_image))}\",\n+                )\n+                self.assertTrue(\n+                    torch.allclose(outputs.logits_per_text, outputs_fa.logits_per_text, atol=4e-2, rtol=4e-2),\n+                    f\"Text logits max diff: {torch.max(torch.abs(outputs.logits_per_text - outputs_fa.logits_per_text))}\",\n+                )\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"eager\"\n+                )\n+                model.to(torch_device)\n+\n+                dummy_pixel_values = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n+                dummy_input_ids = inputs_dict[\"input_ids\"]\n+                dummy_pixel_mask = inputs_dict[\"attention_mask\"]\n+\n+                # right padding\n+                dummy_pixel_mask[:] = 1\n+                dummy_pixel_mask[:, -1:] = 0\n+\n+                outputs = model(pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True)\n+                outputs_fa = model_fa(\n+                    pixel_values=dummy_pixel_values, input_ids=dummy_input_ids, output_hidden_states=True\n+                )\n+\n+                logits_per_image_eager = outputs.logits_per_image[:, :-1]\n+                logits_per_text_eager = outputs.logits_per_text[:, :-1]\n+\n+                logits_per_image_sdpa = outputs_fa.logits_per_image[:, :-1]\n+                logits_per_text_sdpa = outputs_fa.logits_per_text[:, :-1]\n+\n+                self.assertTrue(\n+                    torch.allclose(logits_per_image_eager, logits_per_image_sdpa, atol=4e-2, rtol=4e-2),\n+                    f\"Image logits max diff: {torch.max(torch.abs(logits_per_image_eager - logits_per_image_sdpa))}\",\n+                )\n+                self.assertTrue(\n+                    torch.allclose(logits_per_text_eager, logits_per_text_sdpa, atol=4e-2, rtol=4e-2),\n+                    f\"Text logits max diff: {torch.max(torch.abs(logits_per_text_eager - logits_per_text_sdpa))}\",\n+                )\n+\n+\n+class MetaClip2ForImageClassificationModelTester(MetaClip2ModelTester):\n+    def __init__(self, parent):\n+        super().__init__(parent)\n+        self.batch_size = self.vision_model_tester.batch_size\n+        self.num_hidden_layers = self.vision_model_tester.num_hidden_layers\n+        self.hidden_size = self.vision_model_tester.hidden_size\n+        self.seq_length = self.vision_model_tester.seq_length\n+\n+    def prepare_config_and_inputs(self):\n+        _, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n+        config = self.get_config()\n+\n+        return config, pixel_values\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values = config_and_inputs\n+        inputs_dict = {\"pixel_values\": pixel_values}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class MetaClip2ForImageClassificationModelTest(MetaClip2ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (MetaClip2ForImageClassification,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-classification\": MetaClip2ForImageClassification} if is_torch_available() else {}\n+    fx_compatible = False\n+    test_head_masking = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_attention_outputs = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = MetaClip2ForImageClassificationModelTester(self)\n+\n+    @unittest.skip(reason=\"MetaClip2ForImageClassification does not support inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"MetaClip2ForImageClassification does not support inputs_embeds\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"MetaClip2ForImageClassification does not support gradient checkpointing yet\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"MetaClip2ForImageClassification does not support gradient checkpointing yet\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"MetaClip2ForImageClassification does not support gradient checkpointing yet\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"MetaClip2 uses the same initialization scheme as the Flax original implementation\")\n+    def test_initialization(self):\n+        pass\n+\n+    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n+    @require_torch_sdpa\n+    @slow\n+    @is_flaky()\n+    def test_eager_matches_sdpa_inference(self, *args):\n+        # adding only flaky decorator here and call the parent test method\n+        return getattr(ModelTesterMixin, self._testMethodName)(self)\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n+\n+# We will verify our results on an image of cute cats\n+def prepare_img():\n+    url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+    im = Image.open(requests.get(url, stream=True).raw)\n+    return im\n+\n+\n+@require_vision\n+@require_torch\n+class MetaClip2ModelIntegrationTest(unittest.TestCase):\n+    @slow\n+    def test_inference(self):\n+        model_name = \"facebook/metaclip-2-worldwide-huge-quickgelu\"\n+        model = MetaClip2Model.from_pretrained(model_name, attn_implementation=\"sdpa\").to(torch_device)\n+        processor = CLIPProcessor.from_pretrained(model_name)\n+\n+        image = prepare_img()\n+        inputs = processor(\n+            text=[\"a photo of a cat\", \"a photo of a dog\"], images=image, padding=True, return_tensors=\"pt\"\n+        ).to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        # verify the logits\n+        self.assertEqual(\n+            outputs.logits_per_image.shape,\n+            torch.Size((inputs.pixel_values.shape[0], inputs.input_ids.shape[0])),\n+        )\n+        self.assertEqual(\n+            outputs.logits_per_text.shape,\n+            torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n+        )\n+\n+        expected_logits = torch.tensor([[19.9799, 13.6169]], device=torch_device)\n+\n+        torch.testing.assert_close(outputs.logits_per_image, expected_logits, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "a4022b9a83e148fd62b804042951ab4f264673bf",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -3297,6 +3297,7 @@ def test_mismatched_shapes_have_properly_initialized_weights(self):\n                 \"wav2vec2.masked_spec_embed\",\n                 \"Wav2Vec2ForSequenceClassification\",\n                 \"CLIPForImageClassification\",\n+                \"MetaClip2ForImageClassification\",\n                 \"Siglip2ForImageClassification\",\n                 \"RegNetForImageClassification\",\n                 \"ResNetForImageClassification\","
        },
        {
            "sha": "1fc03fe71525d9526bda726c0b456c1604af0f7c",
            "filename": "utils/add_dates.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/utils%2Fadd_dates.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/utils%2Fadd_dates.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fadd_dates.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -220,7 +220,7 @@ def insert_dates(model_card_list: list[str]):\n \n         # If the dates info line does not exist, add it\n         else:\n-            paper_link = get_paper_link(path=file_path)\n+            paper_link = get_paper_link(model_card=model_card, path=file_path)\n             release_date = \"\"\n \n             if not (paper_link == \"No_paper\" or paper_link == \"blog\"):"
        },
        {
            "sha": "07955b3901853e6c5c15aff6c21cf64e68239e42",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/1d4609173780c973c2f52ee21249856724277922/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1d4609173780c973c2f52ee21249856724277922/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=1d4609173780c973c2f52ee21249856724277922",
            "patch": "@@ -375,6 +375,10 @@\n     \"ChameleonVQVAE\",  # no autoclass for VQ-VAE models\n     \"VitPoseForPoseEstimation\",\n     \"CLIPTextModel\",\n+    \"MetaClip2TextModel\",\n+    \"MetaClip2TextModelWithProjection\",\n+    \"MetaClip2VisionModel\",\n+    \"MetaClip2VisionModelWithProjection\",\n     \"MoshiForConditionalGeneration\",  # no auto class for speech-to-speech\n     \"Emu3VQVAE\",  # no autoclass for VQ-VAE models\n     \"Emu3TextModel\",  # Building part of bigger (tested) model"
        }
    ],
    "stats": {
        "total": 3423,
        "additions": 3390,
        "deletions": 33
    }
}