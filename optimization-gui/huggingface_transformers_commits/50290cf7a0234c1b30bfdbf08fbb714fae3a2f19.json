{
    "author": "molbap",
    "message": "Uniformize model processors (#31368)\n\n* add initial design for uniform processors + align model\r\n\r\n* add uniform processors for altclip + chinese_clip\r\n\r\n* add uniform processors for blip + blip2\r\n\r\n* fix mutable default :eyes:\r\n\r\n* add configuration test\r\n\r\n* handle structured kwargs w defaults + add test\r\n\r\n* protect torch-specific test\r\n\r\n* fix style\r\n\r\n* fix\r\n\r\n* rebase\r\n\r\n* update processor to generic kwargs + test\r\n\r\n* fix style\r\n\r\n* add sensible kwargs merge\r\n\r\n* update test\r\n\r\n* fix assertEqual\r\n\r\n* move kwargs merging to processing common\r\n\r\n* rework kwargs for type hinting\r\n\r\n* just get Unpack from extensions\r\n\r\n* run-slow[align]\r\n\r\n* handle kwargs passed as nested dict\r\n\r\n* add from_pretrained test for nested kwargs handling\r\n\r\n* [run-slow]align\r\n\r\n* update documentation + imports\r\n\r\n* update audio inputs\r\n\r\n* protect audio types, silly\r\n\r\n* try removing imports\r\n\r\n* make things simpler\r\n\r\n* simplerer\r\n\r\n* move out kwargs test to common mixin\r\n\r\n* [run-slow]align\r\n\r\n* skip tests for old processors\r\n\r\n* [run-slow]align, clip\r\n\r\n* !$#@!! protect imports, darn it\r\n\r\n* [run-slow]align, clip\r\n\r\n* [run-slow]align, clip\r\n\r\n* update common processor testing\r\n\r\n* add altclip\r\n\r\n* add chinese_clip\r\n\r\n* add pad_size\r\n\r\n* [run-slow]align, clip, chinese_clip, altclip\r\n\r\n* remove duplicated tests\r\n\r\n* fix\r\n\r\n* add blip, blip2, bridgetower\r\n\r\nAdded tests for bridgetower which override common. Also modified common\r\ntests to force center cropping if existing\r\n\r\n* fix\r\n\r\n* update doc\r\n\r\n* improve documentation for default values\r\n\r\n* add model_max_length testing\r\n\r\nThis parameter depends on tokenizers received.\r\n\r\n* Raise if kwargs are specified in two places\r\n\r\n* fix\r\n\r\n* removed copied from\r\n\r\n* match defaults\r\n\r\n* force padding\r\n\r\n* fix tokenizer test\r\n\r\n* clean defaults\r\n\r\n* move tests to common\r\n\r\n* add missing import\r\n\r\n* fix\r\n\r\n* adapt bridgetower tests to shortest edge\r\n\r\n* uniformize donut processor + tests\r\n\r\n* add wav2vec2\r\n\r\n* extend common testing to audio processors\r\n\r\n* add testing + bert version\r\n\r\n* propagate common kwargs to different modalities\r\n\r\n* BC order of arguments\r\n\r\n* check py version\r\n\r\n* revert kwargs merging\r\n\r\n* add draft overlap test\r\n\r\n* update\r\n\r\n* fix blip2 and wav2vec due to updates\r\n\r\n* fix copies\r\n\r\n* ensure overlapping kwargs do not disappear\r\n\r\n* replace .pop by .get to handle duplicated kwargs\r\n\r\n* fix copies\r\n\r\n* fix missing import\r\n\r\n* add clearly wav2vec2_bert to uniformized models\r\n\r\n* fix copies\r\n\r\n* increase number of features\r\n\r\n* fix style\r\n\r\n* [run-slow] blip, blip2, bridgetower, donut, wav2vec2, wav2vec2_bert\r\n\r\n* [run-slow] blip, blip_2, bridgetower, donut, wav2vec2, wav2vec2_bert\r\n\r\n* fix concatenation\r\n\r\n* [run-slow] blip, blip_2, bridgetower, donut, wav2vec2, wav2vec2_bert\r\n\r\n* Update tests/test_processing_common.py\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* :broom:\r\n\r\n* address comments\r\n\r\n* clean up + tests\r\n\r\n* [run-slow] instructblip, blip, blip_2, bridgetower, donut, wav2vec2, wav2vec2_bert\r\n\r\n---------\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
    "files": [
        {
            "sha": "78e1aa58ef044357f13de447a36bec009ed6b55c",
            "filename": "src/transformers/models/blip/processing_blip.py",
            "status": "modified",
            "additions": 54,
            "deletions": 69,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -19,9 +19,25 @@\n from typing import List, Optional, Union\n \n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+\n+\n+class BlipProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": False,\n+            \"stride\": 0,\n+            \"return_overflowing_tokens\": False,\n+            \"return_special_tokens_mask\": False,\n+            \"return_offsets_mapping\": False,\n+            \"return_token_type_ids\": False,\n+            \"return_length\": False,\n+            \"verbose\": True,\n+        },\n+        \"images_kwargs\": {},\n+    }\n \n \n class BlipProcessor(ProcessorMixin):\n@@ -51,84 +67,53 @@ def __init__(self, image_processor, tokenizer, **kwargs):\n     def __call__(\n         self,\n         images: ImageInput = None,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n-        add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_overflowing_tokens: bool = False,\n-        return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n-        return_token_type_ids: bool = False,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs,\n+        text: Optional[Union[str, List[str], TextInput, PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[BlipProcessorKwargs],\n     ) -> BatchEncoding:\n         \"\"\"\n         This method uses [`BlipImageProcessor.__call__`] method to prepare image(s) for the model, and\n         [`BertTokenizerFast.__call__`] to prepare text for the model.\n \n         Please refer to the docstring of the above two methods for more information.\n+        Args:\n+            images (`ImageInput`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                    - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                    - `'np'`: Return NumPy `np.ndarray` objects.\n+                    - `'jax'`: Return JAX `jnp.ndarray` objects.\n         \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You have to specify either images or text.\")\n \n-        # Get only text\n-        if images is None:\n-            self.current_processor = self.tokenizer\n-            text_encoding = self.tokenizer(\n-                text=text,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_token_type_ids=return_token_type_ids,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=return_tensors,\n-                **kwargs,\n-            )\n-            return text_encoding\n-\n-        # add pixel_values\n-        encoding_image_processor = self.image_processor(images, return_tensors=return_tensors)\n+        text_encoding = None\n \n+        # add pixel_values encoding. If we also have text_encoding, update image encoding and return it.\n+        # else, return the text encoding.\n+        output_kwargs = self._merge_kwargs(\n+            BlipProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n         if text is not None:\n-            text_encoding = self.tokenizer(\n-                text=text,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_token_type_ids=return_token_type_ids,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=return_tensors,\n-                **kwargs,\n-            )\n-        else:\n-            text_encoding = None\n-\n-        if text_encoding is not None:\n-            encoding_image_processor.update(text_encoding)\n-\n-        return encoding_image_processor\n+            text_encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+        if images is not None:\n+            encoding_image_processor = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+\n+            if text_encoding is not None:\n+                encoding_image_processor.update(text_encoding)\n+            return encoding_image_processor\n+\n+        return text_encoding\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "606aadc1eab45feb23c0e5ea75c143cc95b49a7b",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 61,
            "deletions": 73,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -18,22 +18,38 @@\n \n from typing import List, Optional, Union\n \n+from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessorMixin\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import (\n     AddedToken,\n     BatchEncoding,\n-    PaddingStrategy,\n     PreTokenizedInput,\n     TextInput,\n-    TruncationStrategy,\n )\n-from ...utils import TensorType, logging\n+from ...utils import logging\n \n \n logger = logging.get_logger(__name__)\n \n \n+class Blip2ProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": False,\n+            \"stride\": 0,\n+            \"return_overflowing_tokens\": False,\n+            \"return_special_tokens_mask\": False,\n+            \"return_offsets_mapping\": False,\n+            \"return_token_type_ids\": False,\n+            \"return_length\": False,\n+            \"verbose\": True,\n+        },\n+        \"images_kwargs\": {},\n+    }\n+\n+\n class Blip2Processor(ProcessorMixin):\n     r\"\"\"\n     Constructs a BLIP-2 processor which wraps a BLIP image processor and an OPT/T5 tokenizer into a single processor.\n@@ -67,83 +83,55 @@ def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n     def __call__(\n         self,\n         images: ImageInput = None,\n-        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n-        add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_overflowing_tokens: bool = False,\n-        return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n-        return_token_type_ids: bool = False,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs,\n+        text: Optional[Union[str, List[str], TextInput, PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[Blip2ProcessorKwargs],\n     ) -> BatchEncoding:\n         \"\"\"\n         This method uses [`BlipImageProcessor.__call__`] method to prepare image(s) for the model, and\n         [`BertTokenizerFast.__call__`] to prepare text for the model.\n \n         Please refer to the docstring of the above two methods for more information.\n+        Args:\n+            images (`ImageInput`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+                    - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                    - `'np'`: Return NumPy `np.ndarray` objects.\n+                    - `'jax'`: Return JAX `jnp.ndarray` objects.\n         \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You have to specify either images or text.\")\n-\n-        # Get only text\n-        if images is None:\n-            self.current_processor = self.tokenizer\n-            text_encoding = self.tokenizer(\n-                text=text,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_token_type_ids=return_token_type_ids,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=return_tensors,\n-                **kwargs,\n-            )\n-            return text_encoding\n-\n-        # add pixel_values\n-        encoding_image_processor = self.image_processor(images, return_tensors=return_tensors)\n-\n+        output_kwargs = self._merge_kwargs(\n+            Blip2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        # BC for explicit return_tensors\n+        if \"return_tensors\" in output_kwargs[\"common_kwargs\"]:\n+            return_tensors = output_kwargs[\"common_kwargs\"].pop(\"return_tensors\", None)\n+        else:\n+            return_tensors = None\n+        encoding = BatchFeature(tensor_type=return_tensors)\n         if text is not None:\n             if isinstance(text, str):\n                 text = [text]\n             elif not isinstance(text, list) and not isinstance(text[0], str):\n                 raise ValueError(\"Invalid input text. Please provide a string, or a list of strings\")\n \n             text_encoding = {}\n-            _text_encoding = self.tokenizer(\n-                text=text,\n-                add_special_tokens=add_special_tokens,\n-                padding=padding,\n-                truncation=truncation,\n-                max_length=max_length,\n-                stride=stride,\n-                pad_to_multiple_of=pad_to_multiple_of,\n-                return_attention_mask=return_attention_mask,\n-                return_overflowing_tokens=return_overflowing_tokens,\n-                return_special_tokens_mask=return_special_tokens_mask,\n-                return_offsets_mapping=return_offsets_mapping,\n-                return_token_type_ids=return_token_type_ids,\n-                return_length=return_length,\n-                verbose=verbose,\n-                return_tensors=None,  # hardcode \"None\" here for prepending image tokens\n-                **kwargs,\n-            )\n+\n+            return_tensors = output_kwargs[\"text_kwargs\"].pop(\"return_tensors\", None)\n+            _text_encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"], return_tensors=None)\n+            output_kwargs[\"text_kwargs\"][\"return_tensors\"] = return_tensors\n \n             # if we know how many query tokens, expand text inside processor. We need this hacky manipulation\n             # because BLIP expects image tokens to be at the beginning even before BOS token\n@@ -164,14 +152,14 @@ def __call__(\n                 )\n \n             # cast to desired return tensors type\n-            text_encoding = BatchEncoding(text_encoding, tensor_type=return_tensors)\n-        else:\n-            text_encoding = None\n-\n-        if text_encoding is not None:\n-            encoding_image_processor.update(text_encoding)\n-\n-        return encoding_image_processor\n+            encoding.update(BatchEncoding(text_encoding, tensor_type=return_tensors))\n+        # add pixel_values encoding. If we also have text_encoding, update image encoding and return it.\n+        # else, return the text encoding.\n+\n+        if images is not None:\n+            image_encoding = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+            encoding.update(image_encoding)\n+        return encoding\n \n     # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer\n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "177eb12051654df63bcd99ce6a582f7d596ad778",
            "filename": "src/transformers/models/bridgetower/processing_bridgetower.py",
            "status": "modified",
            "additions": 30,
            "deletions": 38,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -16,11 +16,29 @@\n Processor class for BridgeTower.\n \"\"\"\n \n-from typing import List, Optional, Union\n+from typing import List, Union\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+\n+\n+class BridgeTowerProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"add_special_tokens\": True,\n+            \"padding\": False,\n+            \"stride\": 0,\n+            \"return_overflowing_tokens\": False,\n+            \"return_special_tokens_mask\": False,\n+            \"return_offsets_mapping\": False,\n+            \"return_length\": False,\n+            \"verbose\": True,\n+        },\n+        \"images_kwargs\": {\n+            \"do_normalize\": True,\n+            \"do_center_crop\": True,\n+        },\n+    }\n \n \n class BridgeTowerProcessor(ProcessorMixin):\n@@ -50,50 +68,24 @@ def __call__(\n         self,\n         images,\n         text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n-        add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy] = None,\n-        max_length: Optional[int] = None,\n-        stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_overflowing_tokens: bool = False,\n-        return_special_tokens_mask: bool = False,\n-        return_offsets_mapping: bool = False,\n-        return_length: bool = False,\n-        verbose: bool = True,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[BridgeTowerProcessorKwargs],\n     ) -> BatchEncoding:\n         \"\"\"\n         This method uses [`BridgeTowerImageProcessor.__call__`] method to prepare image(s) for the model, and\n         [`RobertaTokenizerFast.__call__`] to prepare text for the model.\n \n         Please refer to the docstring of the above two methods for more information.\n         \"\"\"\n-        encoding = self.tokenizer(\n-            text=text,\n-            add_special_tokens=add_special_tokens,\n-            padding=padding,\n-            truncation=truncation,\n-            max_length=max_length,\n-            stride=stride,\n-            pad_to_multiple_of=pad_to_multiple_of,\n-            return_token_type_ids=return_token_type_ids,\n-            return_attention_mask=return_attention_mask,\n-            return_overflowing_tokens=return_overflowing_tokens,\n-            return_special_tokens_mask=return_special_tokens_mask,\n-            return_offsets_mapping=return_offsets_mapping,\n-            return_length=return_length,\n-            verbose=verbose,\n-            return_tensors=return_tensors,\n+        output_kwargs = self._merge_kwargs(\n+            BridgeTowerProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n         )\n+        encoding = self.tokenizer(text=text, **output_kwargs[\"text_kwargs\"])\n         # add pixel_values + pixel_mask\n-        encoding_image_processor = self.image_processor(\n-            images, return_tensors=return_tensors, do_normalize=True, do_center_crop=True, **kwargs\n-        )\n+        encoding_image_processor = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n         encoding.update(encoding_image_processor)\n \n         return encoding"
        },
        {
            "sha": "9552d323ac57c0e6af5dc838688df34687401b8e",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 27,
            "deletions": 12,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -19,8 +19,15 @@\n import re\n import warnings\n from contextlib import contextmanager\n+from typing import List, Optional, Union\n \n-from ...processing_utils import ProcessorMixin\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+class DonutProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {}\n \n \n class DonutProcessor(ProcessorMixin):\n@@ -63,7 +70,14 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         self.current_processor = self.image_processor\n         self._in_target_context_manager = False\n \n-    def __call__(self, *args, **kwargs):\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Optional[Union[str, List[str], TextInput, PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[DonutProcessorKwargs],\n+    ):\n         \"\"\"\n         When used in normal mode, this method forwards all its arguments to AutoImageProcessor's\n         [`~AutoImageProcessor.__call__`] and returns its output. If used in the context\n@@ -72,28 +86,29 @@ def __call__(self, *args, **kwargs):\n         \"\"\"\n         # For backward compatibility\n         if self._in_target_context_manager:\n-            return self.current_processor(*args, **kwargs)\n-\n-        images = kwargs.pop(\"images\", None)\n-        text = kwargs.pop(\"text\", None)\n-        if len(args) > 0:\n-            images = args[0]\n-            args = args[1:]\n+            return self.current_processor(images, text, **kwargs)\n \n         if images is None and text is None:\n             raise ValueError(\"You need to specify either an `images` or `text` input to process.\")\n \n+        output_kwargs = self._merge_kwargs(\n+            DonutProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n         if images is not None:\n-            inputs = self.image_processor(images, *args, **kwargs)\n+            inputs = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n         if text is not None:\n-            encodings = self.tokenizer(text, **kwargs)\n+            encodings = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n \n         if text is None:\n             return inputs\n         elif images is None:\n             return encodings\n         else:\n-            inputs[\"labels\"] = encodings[\"input_ids\"]\n+            inputs[\"labels\"] = encodings[\"input_ids\"]  # for BC\n+            inputs[\"input_ids\"] = encodings[\"input_ids\"]\n             return inputs\n \n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "1fc9a25fd8e9a22ea3ce4c291fde2b2b3562fb07",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -17,24 +17,18 @@\n \"\"\"\n \n import re\n-import sys\n from typing import TYPE_CHECKING, Dict, List, Optional, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image, load_image\n-from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AddedToken, BatchEncoding, TextInput\n from ...utils import logging\n \n \n if TYPE_CHECKING:\n     from ...tokenization_utils_base import PreTokenizedInput\n \n-if sys.version_info >= (3, 11):\n-    from typing import Unpack\n-else:\n-    from typing_extensions import Unpack\n-\n logger = logging.get_logger(__name__)\n \n "
        },
        {
            "sha": "dbfea1a90deab8ca99cf9ae17ff22858f42076ba",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -19,19 +19,9 @@\n \n import numpy as np\n \n-\n-try:\n-    from typing import Unpack\n-except ImportError:\n-    from typing_extensions import Unpack\n-\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import (\n-    ImagesKwargs,\n-    ProcessingKwargs,\n-    ProcessorMixin,\n-)\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import (\n     BatchEncoding,\n     PreTokenizedInput,"
        },
        {
            "sha": "e4e2e5197f2e2c5c4969e4aabb132bec4ad96a15",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -16,13 +16,12 @@\n Processor class for OmDet-Turbo.\n \"\"\"\n \n-import sys\n from typing import List, Optional, Tuple, Union\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_transforms import center_to_corners_format\n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import (\n     TensorType,\n@@ -31,12 +30,6 @@\n )\n \n \n-if sys.version_info >= (3, 11):\n-    from typing import Unpack\n-else:\n-    from typing_extensions import Unpack\n-\n-\n class OmDetTurboTextKwargs(TextKwargs, total=False):\n     task: Optional[Union[str, List[str], TextInput, PreTokenizedInput]]\n "
        },
        {
            "sha": "4bd4255315fdc51e6f700478d3316e44508af8c2",
            "filename": "src/transformers/models/wav2vec2/processing_wav2vec2.py",
            "status": "modified",
            "additions": 31,
            "deletions": 14,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -18,12 +18,18 @@\n \n import warnings\n from contextlib import contextmanager\n+from typing import List, Optional, Union\n \n-from ...processing_utils import ProcessorMixin\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import AudioInput, PreTokenizedInput, TextInput\n from .feature_extraction_wav2vec2 import Wav2Vec2FeatureExtractor\n from .tokenization_wav2vec2 import Wav2Vec2CTCTokenizer\n \n \n+class Wav2Vec2ProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {}\n+\n+\n class Wav2Vec2Processor(ProcessorMixin):\n     r\"\"\"\n     Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor and a Wav2Vec2 CTC tokenizer into a single\n@@ -66,35 +72,46 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n \n             return cls(feature_extractor=feature_extractor, tokenizer=tokenizer)\n \n-    def __call__(self, *args, **kwargs):\n+    def __call__(\n+        self,\n+        audio: AudioInput = None,\n+        text: Optional[Union[str, List[str], TextInput, PreTokenizedInput]] = None,\n+        images=None,\n+        videos=None,\n+        **kwargs: Unpack[Wav2Vec2ProcessorKwargs],\n+    ):\n         \"\"\"\n         When used in normal mode, this method forwards all its arguments to Wav2Vec2FeatureExtractor's\n         [`~Wav2Vec2FeatureExtractor.__call__`] and returns its output. If used in the context\n         [`~Wav2Vec2Processor.as_target_processor`] this method forwards all its arguments to PreTrainedTokenizer's\n         [`~PreTrainedTokenizer.__call__`]. Please refer to the docstring of the above two methods for more information.\n         \"\"\"\n-        # For backward compatibility\n-        if self._in_target_context_manager:\n-            return self.current_processor(*args, **kwargs)\n \n         if \"raw_speech\" in kwargs:\n             warnings.warn(\"Using `raw_speech` as a keyword argument is deprecated. Use `audio` instead.\")\n             audio = kwargs.pop(\"raw_speech\")\n-        else:\n-            audio = kwargs.pop(\"audio\", None)\n-        sampling_rate = kwargs.pop(\"sampling_rate\", None)\n-        text = kwargs.pop(\"text\", None)\n-        if len(args) > 0:\n-            audio = args[0]\n-            args = args[1:]\n \n         if audio is None and text is None:\n             raise ValueError(\"You need to specify either an `audio` or `text` input to process.\")\n \n+        output_kwargs = self._merge_kwargs(\n+            Wav2Vec2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+        # For backward compatibility\n+        if self._in_target_context_manager:\n+            return self.current_processor(\n+                audio,\n+                **output_kwargs[\"audio_kwargs\"],\n+                **output_kwargs[\"text_kwargs\"],\n+                **output_kwargs[\"common_kwargs\"],\n+            )\n+\n         if audio is not None:\n-            inputs = self.feature_extractor(audio, *args, sampling_rate=sampling_rate, **kwargs)\n+            inputs = self.feature_extractor(audio, **output_kwargs[\"audio_kwargs\"])\n         if text is not None:\n-            encodings = self.tokenizer(text, **kwargs)\n+            encodings = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n \n         if text is None:\n             return inputs"
        },
        {
            "sha": "8b09e92419ae9732285915e6a72b03a6895cc494",
            "filename": "src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py",
            "status": "modified",
            "additions": 27,
            "deletions": 13,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -17,12 +17,18 @@\n \"\"\"\n \n import warnings\n+from typing import List, Optional, Union\n \n-from ...processing_utils import ProcessorMixin\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import AudioInput, PreTokenizedInput, TextInput\n from ..seamless_m4t.feature_extraction_seamless_m4t import SeamlessM4TFeatureExtractor\n from ..wav2vec2.tokenization_wav2vec2 import Wav2Vec2CTCTokenizer\n \n \n+class Wav2Vec2BertProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {}\n+\n+\n class Wav2Vec2BertProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a Wav2Vec2-BERT processor which wraps a Wav2Vec2-BERT feature extractor and a Wav2Vec2 CTC tokenizer into a single\n@@ -63,25 +69,30 @@ def from_pretrained(cls, pretrained_model_name_or_path, **kwargs):\n \n             return cls(feature_extractor=feature_extractor, tokenizer=tokenizer)\n \n-    def __call__(self, audio=None, text=None, **kwargs):\n+    def __call__(\n+        self,\n+        audio: AudioInput = None,\n+        text: Optional[Union[str, List[str], TextInput, PreTokenizedInput]] = None,\n+        images=None,\n+        videos=None,\n+        **kwargs: Unpack[Wav2Vec2BertProcessorKwargs],\n+    ):\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `audio`\n         and `kwargs` arguments to SeamlessM4TFeatureExtractor's [`~SeamlessM4TFeatureExtractor.__call__`] if `audio` is not\n         `None` to pre-process the audio. To prepare the target sequences(s), this method forwards the `text` and `kwargs` arguments to\n         PreTrainedTokenizer's [`~PreTrainedTokenizer.__call__`] if `text` is not `None`. Please refer to the doctsring of the above two methods for more information.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n             audio (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n                 The audio or batch of audios to be prepared. Each audio can be NumPy array or PyTorch tensor. In case\n                 of a NumPy array/PyTorch tensor, each audio should be of shape (C, T), where C is a number of channels,\n                 and T the sample length of the audio.\n-            kwargs (*optional*):\n-                Remaining dictionary of keyword arguments that will be passed to the feature extractor and/or the\n-                tokenizer.\n+\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n         Returns:\n             [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n             - **input_features** -- Audio input features to be fed to a model. Returned when `audio` is not `None`.\n@@ -91,15 +102,18 @@ def __call__(self, audio=None, text=None, **kwargs):\n             - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None` and `audio` is `None`.\n         \"\"\"\n \n-        sampling_rate = kwargs.pop(\"sampling_rate\", None)\n-\n         if audio is None and text is None:\n             raise ValueError(\"You need to specify either an `audio` or `text` input to process.\")\n+        output_kwargs = self._merge_kwargs(\n+            Wav2Vec2BertProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n \n         if audio is not None:\n-            inputs = self.feature_extractor(audio, sampling_rate=sampling_rate, **kwargs)\n+            inputs = self.feature_extractor(audio, **output_kwargs[\"audio_kwargs\"])\n         if text is not None:\n-            encodings = self.tokenizer(text, **kwargs)\n+            encodings = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n \n         if text is None:\n             return inputs"
        },
        {
            "sha": "062dfe311c1dca96572548778560c0736aa7ee32",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 7,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -820,6 +820,8 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n             \"common_kwargs\": {},\n         }\n \n+        used_keys = set()\n+\n         # get defaults from set model processor kwargs if they exist\n         for modality in default_kwargs:\n             default_kwargs[modality] = ModelProcessorKwargs._defaults.get(modality, {}).copy()\n@@ -846,18 +848,29 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n                             f\"in a dictionary for {modality} and as a **kwarg.\"\n                         )\n                 elif modality_key in kwargs:\n-                    kwarg_value = kwargs.pop(modality_key, \"__empty__\")\n+                    # we get a modality_key instead of popping it because modality-specific processors\n+                    # can have overlapping kwargs\n+                    kwarg_value = kwargs.get(modality_key, \"__empty__\")\n                 else:\n                     kwarg_value = \"__empty__\"\n                 if kwarg_value != \"__empty__\":\n                     output_kwargs[modality][modality_key] = kwarg_value\n-        # if something remains in kwargs, it belongs to common after flattening\n-        if set(kwargs) & set(default_kwargs):\n-            # here kwargs is dictionary-based since it shares keys with default set\n-            [output_kwargs[\"common_kwargs\"].update(subdict) for _, subdict in kwargs.items()]\n+                    used_keys.add(modality_key)\n+\n+        # Determine if kwargs is a flat dictionary or contains nested dictionaries\n+        if any(key in default_kwargs for key in kwargs):\n+            # kwargs is dictionary-based, and some keys match modality names\n+            for modality, subdict in kwargs.items():\n+                if modality in default_kwargs:\n+                    for subkey, subvalue in subdict.items():\n+                        if subkey not in used_keys:\n+                            output_kwargs[modality][subkey] = subvalue\n+                            used_keys.add(subkey)\n         else:\n-            # here it's a flat dict\n-            output_kwargs[\"common_kwargs\"].update(kwargs)\n+            # kwargs is a flat dictionary\n+            for key in kwargs:\n+                if key not in used_keys:\n+                    output_kwargs[\"common_kwargs\"][key] = kwargs[key]\n \n         # all modality-specific kwargs are updated with common kwargs\n         for modality in output_kwargs:"
        },
        {
            "sha": "5b290efb1150310f19347fff01107c395fe14672",
            "filename": "tests/models/altclip/test_processor_altclip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Faltclip%2Ftest_processor_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Faltclip%2Ftest_processor_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_processor_altclip.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -17,17 +17,12 @@\n import tempfile\n import unittest\n \n-from transformers import XLMRobertaTokenizer, XLMRobertaTokenizerFast\n+from transformers import AltCLIPProcessor, CLIPImageProcessor, XLMRobertaTokenizer, XLMRobertaTokenizerFast\n from transformers.testing_utils import require_vision\n-from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n \n \n-if is_vision_available():\n-    from transformers import AltCLIPProcessor, CLIPImageProcessor\n-\n-\n @require_vision\n class AltClipProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     processor_class = AltCLIPProcessor"
        },
        {
            "sha": "4d22c6527c07b14115ba14ec7b5a5b0e9aba29f6",
            "filename": "tests/models/blip/test_processor_blip.py",
            "status": "modified",
            "additions": 27,
            "deletions": 1,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_processor_blip.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers.testing_utils import require_vision\n+from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -139,3 +139,29 @@ def test_model_input_names(self):\n \n         # For now the processor supports only ['pixel_values', 'input_ids', 'attention_mask']\n         self.assertListEqual(list(inputs.keys()), [\"pixel_values\", \"input_ids\", \"attention_mask\"])\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            crop_size={\"height\": 214, \"width\": 214},\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 24)"
        },
        {
            "sha": "7151be8ac71200945631699db0180897ce13c8af",
            "filename": "tests/models/blip_2/test_processor_blip_2.py",
            "status": "modified",
            "additions": 31,
            "deletions": 4,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_processor_blip_2.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -17,7 +17,7 @@\n \n import pytest\n \n-from transformers.testing_utils import require_vision\n+from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -94,7 +94,7 @@ def test_tokenizer(self):\n         encoded_tok = tokenizer(input_str, return_token_type_ids=False)\n \n         for key in encoded_tok.keys():\n-            self.assertListEqual(encoded_tok[key], encoded_processor[key])\n+            self.assertListEqual(encoded_tok[key], encoded_processor[key][0])\n \n     def test_processor(self):\n         image_processor = self.get_image_processor()\n@@ -107,7 +107,7 @@ def test_processor(self):\n \n         inputs = processor(text=input_str, images=image_input)\n \n-        self.assertListEqual(list(inputs.keys()), [\"pixel_values\", \"input_ids\", \"attention_mask\"])\n+        self.assertCountEqual(list(inputs.keys()), [\"input_ids\", \"pixel_values\", \"attention_mask\"])\n \n         # test if it raises when no input is passed\n         with pytest.raises(ValueError):\n@@ -138,4 +138,31 @@ def test_model_input_names(self):\n         inputs = processor(text=input_str, images=image_input)\n \n         # For now the processor supports only ['pixel_values', 'input_ids', 'attention_mask']\n-        self.assertListEqual(list(inputs.keys()), [\"pixel_values\", \"input_ids\", \"attention_mask\"])\n+        self.assertCountEqual(list(inputs.keys()), [\"input_ids\", \"pixel_values\", \"attention_mask\"])\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            crop_size={\"height\": 214, \"width\": 214},\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 11)"
        },
        {
            "sha": "19902a1cc57f3b44abe979d97fed3e9960b716ef",
            "filename": "tests/models/bridgetower/test_processing_bridgetower.py",
            "status": "added",
            "additions": 218,
            "deletions": 0,
            "changes": 218,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fbridgetower%2Ftest_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fbridgetower%2Ftest_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_processing_bridgetower.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -0,0 +1,218 @@\n+# Copyright 2023 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import shutil\n+import tempfile\n+import unittest\n+\n+import numpy as np\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import (\n+        AutoProcessor,\n+        BridgeTowerImageProcessor,\n+        BridgeTowerProcessor,\n+        RobertaTokenizerFast,\n+    )\n+\n+\n+@require_vision\n+class Blip2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = BridgeTowerProcessor\n+\n+    def setUp(self):\n+        self.tmpdirname = tempfile.mkdtemp()\n+\n+        image_processor = BridgeTowerImageProcessor()\n+        tokenizer = RobertaTokenizerFast.from_pretrained(\"BridgeTower/bridgetower-large-itm-mlm-itc\")\n+\n+        processor = BridgeTowerProcessor(image_processor, tokenizer)\n+\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n+\n+    def get_image_processor(self, **kwargs):\n+        return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n+\n+    def prepare_image_inputs(self):\n+        \"\"\"This function prepares a list of PIL images, or a list of numpy arrays if one specifies numpify=True,\n+        or a list of PyTorch tensors if one specifies torchify=True.\n+        \"\"\"\n+\n+        image_inputs = [np.random.randint(255, size=(3, 30, 400), dtype=np.uint8)]\n+\n+        image_inputs = [Image.fromarray(np.moveaxis(x, 0, -1)) for x in image_inputs]\n+\n+        return image_inputs\n+\n+    # Some kwargs tests are overriden from common tests to handle shortest_edge\n+    # and size_divisor behaviour\n+\n+    @require_torch\n+    @require_vision\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\n+            \"image_processor\",\n+            crop_size={\"shortest_edge\": 234, \"longest_edge\": 234},\n+        )\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 234)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested_from_dict(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\n+                \"crop_size\": {\"shortest_edge\": 214},\n+            },\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", crop_size={\"shortest_edge\": 234})\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(text=input_str, images=image_input, crop_size={\"shortest_edge\": 224})\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 224)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            crop_size={\"shortest_edge\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            crop_size={\"shortest_edge\": 214},\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_nested(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"crop_size\": {\"shortest_edge\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)"
        },
        {
            "sha": "87cdb41a02c7bbed2b9931f0b6fde11ecc312b60",
            "filename": "tests/models/donut/test_processing_donut.py",
            "status": "modified",
            "additions": 45,
            "deletions": 2,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fdonut%2Ftest_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fdonut%2Ftest_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdonut%2Ftest_processing_donut.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -14,16 +14,32 @@\n # limitations under the License.\n \n \n+import tempfile\n import unittest\n \n-from transformers import DonutProcessor\n+from transformers import DonutImageProcessor, DonutProcessor, XLMRobertaTokenizerFast\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_vision,\n+)\n \n+from ...test_processing_common import ProcessorTesterMixin\n \n-class DonutProcessorTest(unittest.TestCase):\n+\n+class DonutProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n     from_pretrained_id = \"naver-clova-ix/donut-base\"\n+    processor_class = DonutProcessor\n \n     def setUp(self):\n         self.processor = DonutProcessor.from_pretrained(self.from_pretrained_id)\n+        self.tmpdirname = tempfile.mkdtemp()\n+\n+        image_processor = DonutImageProcessor()\n+        tokenizer = XLMRobertaTokenizerFast.from_pretrained(self.from_pretrained_id)\n+\n+        processor = DonutProcessor(image_processor, tokenizer)\n+\n+        processor.save_pretrained(self.tmpdirname)\n \n     def test_token2json(self):\n         expected_json = {\n@@ -49,3 +65,30 @@ def test_token2json(self):\n         actual_json = self.processor.token2json(sequence)\n \n         self.assertDictEqual(actual_json, expected_json)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            crop_size={\"height\": 214, \"width\": 214},\n+            size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 7)"
        },
        {
            "sha": "30c9243e8e4f5538cb3f5134bf22179a62abf31a",
            "filename": "tests/models/wav2vec2/test_processor_wav2vec2.py",
            "status": "modified",
            "additions": 25,
            "deletions": 2,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fwav2vec2%2Ftest_processor_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fwav2vec2%2Ftest_processor_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_processor_wav2vec2.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -18,14 +18,19 @@\n import tempfile\n import unittest\n \n+import numpy as np\n+\n from transformers.models.wav2vec2 import Wav2Vec2CTCTokenizer, Wav2Vec2FeatureExtractor, Wav2Vec2Processor\n from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES\n from transformers.utils import FEATURE_EXTRACTOR_NAME\n \n+from ...test_processing_common import ProcessorTesterMixin\n from .test_feature_extraction_wav2vec2 import floats_list\n \n \n-class Wav2Vec2ProcessorTest(unittest.TestCase):\n+class Wav2Vec2ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Wav2Vec2Processor\n+\n     def setUp(self):\n         vocab = \"<pad> <s> </s> <unk> | E T A O N I H S R D L U M W C F G Y P B V K ' X J Q Z\".split(\" \")\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n@@ -53,6 +58,9 @@ def setUp(self):\n         with open(self.feature_extraction_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(feature_extractor_map) + \"\\n\")\n \n+        tokenizer = self.get_tokenizer()\n+        tokenizer.save_pretrained(self.tmpdirname)\n+\n     def get_tokenizer(self, **kwargs_init):\n         kwargs = self.add_kwargs_tokens_map.copy()\n         kwargs.update(kwargs_init)\n@@ -117,14 +125,29 @@ def test_tokenizer(self):\n         processor = Wav2Vec2Processor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n \n         input_str = \"This is a test string\"\n-\n         encoded_processor = processor(text=input_str)\n \n         encoded_tok = tokenizer(input_str)\n \n         for key in encoded_tok.keys():\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n+    def test_padding_argument_not_ignored(self):\n+        # padding, or any other overlap arg between audio extractor and tokenizer\n+        # should be passed to both text and audio and not ignored\n+\n+        feature_extractor = self.get_feature_extractor()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = Wav2Vec2Processor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        batch_duration_in_seconds = [1, 3, 2, 6]\n+        input_features = [np.random.random(16_000 * s) for s in batch_duration_in_seconds]\n+\n+        # padding = True should not raise an error and will if the audio processor popped its value to None\n+        _ = processor(\n+            input_features, padding=True, sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\"\n+        )\n+\n     def test_tokenizer_decode(self):\n         feature_extractor = self.get_feature_extractor()\n         tokenizer = self.get_tokenizer()"
        },
        {
            "sha": "704d087a56a8e3a73b30320e81443312be2aee92",
            "filename": "tests/models/wav2vec2_bert/test_processor_wav2vec2_bert.py",
            "status": "modified",
            "additions": 26,
            "deletions": 4,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processor_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processor_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processor_wav2vec2_bert.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -18,17 +18,21 @@\n import tempfile\n import unittest\n \n+import numpy as np\n+\n from transformers.models.seamless_m4t import SeamlessM4TFeatureExtractor\n from transformers.models.wav2vec2 import Wav2Vec2CTCTokenizer\n from transformers.models.wav2vec2.tokenization_wav2vec2 import VOCAB_FILES_NAMES\n from transformers.models.wav2vec2_bert import Wav2Vec2BertProcessor\n from transformers.utils import FEATURE_EXTRACTOR_NAME\n \n+from ...test_processing_common import ProcessorTesterMixin\n from ..wav2vec2.test_feature_extraction_wav2vec2 import floats_list\n \n \n-# Copied from tests.models.wav2vec2.test_processor_wav2vec2.Wav2Vec2ProcessorTest with Wav2Vec2FeatureExtractor->SeamlessM4TFeatureExtractor, Wav2Vec2Processor->Wav2Vec2BertProcessor\n-class Wav2Vec2BertProcessorTest(unittest.TestCase):\n+class Wav2Vec2BertProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = Wav2Vec2BertProcessor\n+\n     def setUp(self):\n         vocab = \"<pad> <s> </s> <unk> | E T A O N I H S R D L U M W C F G Y P B V K ' X J Q Z\".split(\" \")\n         vocab_tokens = dict(zip(vocab, range(len(vocab))))\n@@ -40,7 +44,7 @@ def setUp(self):\n             \"eos_token\": \"</s>\",\n         }\n         feature_extractor_map = {\n-            \"feature_size\": 1,\n+            \"feature_size\": 80,\n             \"padding_value\": 0.0,\n             \"sampling_rate\": 16000,\n             \"return_attention_mask\": False,\n@@ -56,6 +60,9 @@ def setUp(self):\n         with open(self.feature_extraction_file, \"w\", encoding=\"utf-8\") as fp:\n             fp.write(json.dumps(feature_extractor_map) + \"\\n\")\n \n+        tokenizer = self.get_tokenizer()\n+        tokenizer.save_pretrained(self.tmpdirname)\n+\n     def get_tokenizer(self, **kwargs_init):\n         kwargs = self.add_kwargs_tokens_map.copy()\n         kwargs.update(kwargs_init)\n@@ -122,14 +129,29 @@ def test_tokenizer(self):\n         processor = Wav2Vec2BertProcessor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n \n         input_str = \"This is a test string\"\n-\n         encoded_processor = processor(text=input_str)\n \n         encoded_tok = tokenizer(input_str)\n \n         for key in encoded_tok.keys():\n             self.assertListEqual(encoded_tok[key], encoded_processor[key])\n \n+    def test_padding_argument_not_ignored(self):\n+        # padding, or any other overlap arg between audio extractor and tokenizer\n+        # should be passed to both text and audio and not ignored\n+        feature_extractor = self.get_feature_extractor()\n+        tokenizer = self.get_tokenizer()\n+\n+        processor = Wav2Vec2BertProcessor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        batch_duration_in_seconds = [1, 3, 2, 6]\n+        input_features = [np.random.random(16_000 * s) for s in batch_duration_in_seconds]\n+\n+        # padding = True should not raise an error and will if the audio processor popped its value to None\n+        # processor(input_features, padding=True, sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\")\n+        _ = processor(\n+            input_features, padding=True, sampling_rate=processor.feature_extractor.sampling_rate, return_tensors=\"pt\"\n+        )\n+\n     def test_tokenizer_decode(self):\n         feature_extractor = self.get_feature_extractor()\n         tokenizer = self.get_tokenizer()"
        },
        {
            "sha": "9f0d88089129b89c2dc1faa9177422165db1b7d2",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 146,
            "deletions": 5,
            "changes": 151,
            "blob_url": "https://github.com/huggingface/transformers/blob/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/50290cf7a0234c1b30bfdbf08fbb714fae3a2f19/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=50290cf7a0234c1b30bfdbf08fbb714fae3a2f19",
            "patch": "@@ -16,6 +16,7 @@\n \n import inspect\n import json\n+import random\n import tempfile\n from typing import Optional\n \n@@ -31,11 +32,7 @@\n from transformers.utils import is_vision_available\n \n \n-try:\n-    from typing import Unpack\n-except ImportError:\n-    from typing_extensions import Unpack\n-\n+global_rng = random.Random()\n \n if is_vision_available():\n     from PIL import Image\n@@ -48,6 +45,21 @@ def prepare_image_inputs():\n     return image_inputs\n \n \n+# Copied from tests.models.whisper.test_feature_extraction_whisper.floats_list\n+def floats_list(shape, scale=1.0, rng=None, name=None):\n+    \"\"\"Creates a random float32 tensor\"\"\"\n+    if rng is None:\n+        rng = global_rng\n+\n+    values = []\n+    for batch_idx in range(shape[0]):\n+        values.append([])\n+        for _ in range(shape[1]):\n+            values[-1].append(rng.random() * scale)\n+\n+    return values\n+\n+\n @require_torch\n @require_vision\n class ProcessorTesterMixin:\n@@ -333,6 +345,135 @@ def test_structured_kwargs_nested_from_dict(self):\n         self.assertLessEqual(inputs[self.images_input_name][0][0].mean(), 0)\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n \n+    #  text + audio kwargs testing\n+    @require_torch\n+    def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n+        if \"feature_extractor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+        feature_extractor = self.get_component(\"feature_extractor\")\n+        if hasattr(self, \"get_tokenizer\"):\n+            tokenizer = self.get_tokenizer(max_length=117, padding=\"max_length\")\n+        elif hasattr(self, \"get_component\"):\n+            tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+        else:\n+            self.assertTrue(False, \"Processor doesn't have get_tokenizer or get_component defined\")\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        raw_speech = floats_list((3, 1000))\n+        inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\")\n+        if \"input_ids\" in inputs:\n+            self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n+        elif \"labels\" in inputs:\n+            self.assertEqual(len(inputs[\"labels\"][0]), 117)\n+\n+    @require_torch\n+    def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n+        if \"feature_extractor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+        feature_extractor = self.get_component(\"feature_extractor\")\n+        if hasattr(self, \"get_tokenizer\"):\n+            tokenizer = self.get_tokenizer(max_length=117)\n+        elif hasattr(self, \"get_component\"):\n+            tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        raw_speech = floats_list((3, 1000))\n+        inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\", max_length=112, padding=\"max_length\")\n+        if \"input_ids\" in inputs:\n+            self.assertEqual(len(inputs[\"input_ids\"][0]), 112)\n+        elif \"labels\" in inputs:\n+            self.assertEqual(len(inputs[\"labels\"][0]), 112)\n+\n+    @require_torch\n+    def test_unstructured_kwargs_audio(self):\n+        if \"feature_extractor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+        feature_extractor = self.get_component(\"feature_extractor\")\n+        if hasattr(self, \"get_tokenizer\"):\n+            tokenizer = self.get_tokenizer(max_length=117)\n+        elif hasattr(self, \"get_component\"):\n+            tokenizer = self.get_component(\"tokenizer\", max_length=117)\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        raw_speech = floats_list((3, 1000))\n+        inputs = processor(\n+            text=input_str,\n+            audio=raw_speech,\n+            return_tensors=\"pt\",\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        if \"input_ids\" in inputs:\n+            self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+        elif \"labels\" in inputs:\n+            self.assertEqual(len(inputs[\"labels\"][0]), 76)\n+\n+    @require_torch\n+    def test_doubly_passed_kwargs_audio(self):\n+        if \"feature_extractor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+        feature_extractor = self.get_component(\"feature_extractor\")\n+        if hasattr(self, \"get_tokenizer\"):\n+            tokenizer = self.get_tokenizer()\n+        elif hasattr(self, \"get_component\"):\n+            tokenizer = self.get_component(\"tokenizer\")\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\"]\n+        raw_speech = floats_list((3, 1000))\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                audio=raw_speech,\n+                audio_kwargs={\"padding\": \"max_length\"},\n+                padding=\"max_length\",\n+            )\n+\n+    @require_torch\n+    @require_vision\n+    def test_structured_kwargs_audio_nested(self):\n+        if \"feature_extractor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+        feature_extractor = self.get_component(\"feature_extractor\")\n+        if hasattr(self, \"get_tokenizer\"):\n+            tokenizer = self.get_tokenizer()\n+        elif hasattr(self, \"get_component\"):\n+            tokenizer = self.get_component(\"tokenizer\")\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        processor = self.processor_class(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\"]\n+        raw_speech = floats_list((3, 1000))\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+            \"audio_kwargs\": {\"padding\": \"max_length\", \"max_length\": 66},\n+        }\n+\n+        inputs = processor(text=input_str, audio=raw_speech, **all_kwargs)\n+        if \"input_ids\" in inputs:\n+            self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+        elif \"labels\" in inputs:\n+            self.assertEqual(len(inputs[\"labels\"][0]), 76)\n+\n     # TODO: the same test, but for audio + text processors that have strong overlap in kwargs\n     # TODO (molbap) use the same structure of attribute kwargs for other tests to avoid duplication\n     def test_overlapping_text_kwargs_handling(self):"
        }
    ],
    "stats": {
        "total": 1048,
        "additions": 772,
        "deletions": 276
    }
}