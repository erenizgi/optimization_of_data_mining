{
    "author": "zucchini-nlp",
    "message": "Consistent naming for images kwargs (#40834)\n\n* use consistent naming for padding\n\n* no validation on pad size\n\n* add warnings\n\n* fix\n\n* fox copies\n\n* another fix\n\n* fix some tests\n\n* fix more tests\n\n* fix lasts tests\n\n* fix copies\n\n* better docstring\n\n* delete print",
    "sha": "8e837f6ae244cd5be0d9629e695f60b054f16cb5",
    "files": [
        {
            "sha": "4028c38ff227beb1896e3ba338ec9270bb668874",
            "filename": "src/transformers/image_processing_utils_fast.py",
            "status": "modified",
            "additions": 85,
            "deletions": 6,
            "changes": 91,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fimage_processing_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_utils_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -79,8 +79,6 @@ def validate_fast_preprocess_arguments(\n     do_normalize: Optional[bool] = None,\n     image_mean: Optional[Union[float, list[float]]] = None,\n     image_std: Optional[Union[float, list[float]]] = None,\n-    do_pad: Optional[bool] = None,\n-    size_divisibility: Optional[int] = None,\n     do_center_crop: Optional[bool] = None,\n     crop_size: Optional[SizeDict] = None,\n     do_resize: Optional[bool] = None,\n@@ -99,8 +97,6 @@ def validate_fast_preprocess_arguments(\n         do_normalize=do_normalize,\n         image_mean=image_mean,\n         image_std=image_std,\n-        do_pad=do_pad,\n-        size_divisibility=size_divisibility,\n         do_center_crop=do_center_crop,\n         crop_size=crop_size,\n         do_resize=do_resize,\n@@ -181,6 +177,8 @@ class DefaultFastImageProcessorKwargs(TypedDict, total=False):\n     do_normalize: Optional[bool]\n     image_mean: Optional[Union[float, list[float]]]\n     image_std: Optional[Union[float, list[float]]]\n+    do_pad: Optional[bool]\n+    pad_size: Optional[dict[str, int]]\n     do_convert_rgb: Optional[bool]\n     return_tensors: Optional[Union[str, TensorType]]\n     data_format: Optional[ChannelDimension]\n@@ -199,6 +197,8 @@ class BaseImageProcessorFast(BaseImageProcessor):\n     crop_size = None\n     do_resize = None\n     do_center_crop = None\n+    do_pad = None\n+    pad_size = None\n     do_rescale = None\n     rescale_factor = 1 / 255\n     do_normalize = None\n@@ -222,6 +222,9 @@ def __init__(self, **kwargs: Unpack[DefaultFastImageProcessorKwargs]):\n         )\n         crop_size = kwargs.pop(\"crop_size\", self.crop_size)\n         self.crop_size = get_size_dict(crop_size, param_name=\"crop_size\") if crop_size is not None else None\n+        pad_size = kwargs.pop(\"pad_size\", self.pad_size)\n+        self.pad_size = get_size_dict(size=pad_size, param_name=\"pad_size\") if pad_size is not None else None\n+\n         for key in self.valid_kwargs.__annotations__:\n             kwarg = kwargs.pop(key, None)\n             if kwarg is not None:\n@@ -239,6 +242,74 @@ def is_fast(self) -> bool:\n         \"\"\"\n         return True\n \n+    def pad(\n+        self,\n+        images: \"torch.Tensor\",\n+        pad_size: SizeDict = None,\n+        fill_value: Optional[int] = 0,\n+        padding_mode: Optional[str] = \"constant\",\n+        return_mask: Optional[bool] = False,\n+        disable_grouping: Optional[bool] = False,\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pads images to `(pad_size[\"height\"], pad_size[\"width\"])` or to the largest size in the batch.\n+\n+        Args:\n+            images (`torch.Tensor`):\n+                Images to pad.\n+            pad_size (`SizeDict`, *optional*):\n+                Dictionary in the format `{\"height\": int, \"width\": int}` specifying the size of the output image.\n+            fill_value (`int`, *optional*, defaults to `0`):\n+                The constant value used to fill the padded area.\n+            padding_mode (`str`, *optional*, defaults to \"constant\"):\n+                The padding mode to use. Can be any of the modes supported by\n+                `torch.nn.functional.pad` (e.g. constant, reflection, replication).\n+            return_mask (`bool`, *optional*, defaults to `False`):\n+                Whether to return a pixel mask to denote padded regions.\n+            disable_grouping (`bool`, *optional*, defaults to `False`):\n+                Whether to disable grouping of images by size.\n+\n+        Returns:\n+            `torch.Tensor`: The resized image.\n+        \"\"\"\n+        if pad_size is not None:\n+            if not (pad_size.height and pad_size.width):\n+                raise ValueError(f\"Pad size must contain 'height' and 'width' keys only. Got pad_size={pad_size}.\")\n+            pad_size = (pad_size.height, pad_size.width)\n+        else:\n+            pad_size = get_max_height_width(images)\n+\n+        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n+        processed_images_grouped = {}\n+        processed_masks_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            image_size = stacked_images.shape[-2:]\n+            padding_height = pad_size[0] - image_size[0]\n+            padding_width = pad_size[1] - image_size[1]\n+            if padding_height < 0 or padding_width < 0:\n+                raise ValueError(\n+                    f\"Padding dimensions are negative. Please make sure that the `pad_size` is larger than the \"\n+                    f\"image size. Got pad_size={pad_size}, image_size={image_size}.\"\n+                )\n+            if image_size != pad_size:\n+                padding = (0, 0, padding_width, padding_height)\n+                stacked_images = F.pad(stacked_images, padding, fill=fill_value, padding_mode=padding_mode)\n+            processed_images_grouped[shape] = stacked_images\n+\n+            if return_mask:\n+                # keep only one from the channel dimension in pixel mask\n+                stacked_masks = torch.zeros_like(stacked_images, dtype=torch.int64)[..., 0, :, :]\n+                stacked_masks[..., : image_size[0], : image_size[1]] = 1\n+                processed_masks_grouped[shape] = stacked_masks\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        if return_mask:\n+            processed_masks = reorder_images(processed_masks_grouped, grouped_images_index)\n+            return processed_images, processed_masks\n+\n+        return processed_images\n+\n     def resize(\n         self,\n         image: \"torch.Tensor\",\n@@ -577,6 +648,7 @@ def _further_process_kwargs(\n         self,\n         size: Optional[SizeDict] = None,\n         crop_size: Optional[SizeDict] = None,\n+        pad_size: Optional[SizeDict] = None,\n         default_to_square: Optional[bool] = None,\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n@@ -593,6 +665,8 @@ def _further_process_kwargs(\n             size = SizeDict(**get_size_dict(size=size, default_to_square=default_to_square))\n         if crop_size is not None:\n             crop_size = SizeDict(**get_size_dict(crop_size, param_name=\"crop_size\"))\n+        if pad_size is not None:\n+            pad_size = SizeDict(**get_size_dict(size=pad_size, param_name=\"pad_size\"))\n         if isinstance(image_mean, list):\n             image_mean = tuple(image_mean)\n         if isinstance(image_std, list):\n@@ -602,6 +676,7 @@ def _further_process_kwargs(\n \n         kwargs[\"size\"] = size\n         kwargs[\"crop_size\"] = crop_size\n+        kwargs[\"pad_size\"] = pad_size\n         kwargs[\"image_mean\"] = image_mean\n         kwargs[\"image_std\"] = image_std\n         kwargs[\"data_format\"] = data_format\n@@ -714,6 +789,8 @@ def _preprocess(\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n+        do_pad: Optional[bool],\n+        pad_size: Optional[SizeDict],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n@@ -739,10 +816,12 @@ def _preprocess(\n                 stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n             )\n             processed_images_grouped[shape] = stacked_images\n-\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n \n+        if do_pad:\n+            processed_images = self.pad(processed_images, pad_size=pad_size, disable_grouping=disable_grouping)\n+\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n         return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n \n     def to_dict(self):"
        },
        {
            "sha": "2079c21f3b0c45419b8bbcb006ac509dd5caa06b",
            "filename": "src/transformers/image_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fimage_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fimage_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_utils.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -525,7 +525,7 @@ def validate_preprocess_arguments(\n     image_mean: Optional[Union[float, list[float]]] = None,\n     image_std: Optional[Union[float, list[float]]] = None,\n     do_pad: Optional[bool] = None,\n-    size_divisibility: Optional[int] = None,\n+    pad_size: Optional[Union[dict[str, int], int]] = None,\n     do_center_crop: Optional[bool] = None,\n     crop_size: Optional[dict[str, int]] = None,\n     do_resize: Optional[bool] = None,\n@@ -544,10 +544,15 @@ def validate_preprocess_arguments(\n     if do_rescale and rescale_factor is None:\n         raise ValueError(\"`rescale_factor` must be specified if `do_rescale` is `True`.\")\n \n-    if do_pad and size_divisibility is None:\n-        # Here, size_divisor might be passed as the value of size\n+    if do_pad and pad_size is None:\n+        # Processors pad images using different args depending on the model, so the below check is pointless\n+        # but we keep it for BC for now. TODO: remove in v5\n+        # Usually padding can be called with:\n+        #   - \"pad_size/size\" if we're padding to specific values\n+        #   - \"size_divisor\" if we're padding to any value divisible by X\n+        #   - \"None\" if we're padding to the maximum size image in batch\n         raise ValueError(\n-            \"Depending on the model, `size_divisibility`, `size_divisor`, `pad_size` or `size` must be specified if `do_pad` is `True`.\"\n+            \"Depending on the model, `size_divisor` or `pad_size` or `size` must be specified if `do_pad` is `True`.\"\n         )\n \n     if do_normalize and (image_mean is None or image_std is None):"
        },
        {
            "sha": "cb39ed0975610f65f2b44725ffbbbb43a0c18f3f",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -480,8 +480,6 @@ def preprocess(\n             do_normalize=do_normalize,\n             image_mean=image_mean,\n             image_std=image_std,\n-            do_pad=do_pad,\n-            size_divisibility=size_divisor,\n             do_center_crop=do_center_crop,\n             crop_size=crop_size,\n             do_resize=do_resize,"
        },
        {
            "sha": "4a7450c8449850cae77e00b3e00ebb092f3a7968",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 59,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -25,7 +25,6 @@\n     SizeDict,\n     TensorType,\n     Unpack,\n-    get_max_height_width,\n     group_images_by_shape,\n     reorder_images,\n )\n@@ -99,13 +98,9 @@ class BridgeTowerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         size_divisor (`int`, *optional*, defaults to 32):\n             The size by which to make sure both the height and width can be divided. Only has an effect if `do_resize`\n             is set to `True`. Can be overridden by the `size_divisor` parameter in the `preprocess` method.\n-        do_pad (`bool`, *optional*, defaults to `True`):\n-            Whether to pad the image to the `(max_height, max_width)` of the images in the batch. Can be overridden by\n-            the `do_pad` parameter in the `preprocess` method.\n     \"\"\"\n \n     size_divisor: Optional[int]\n-    do_pad: Optional[bool]\n \n \n @auto_docstring\n@@ -224,59 +219,6 @@ def _pad_image(\n         )\n         return padded_image\n \n-    def pad(\n-        self,\n-        images: list[\"torch.Tensor\"],\n-        constant_values: Union[float, Iterable[float]] = 0,\n-        return_pixel_mask: bool = True,\n-        disable_grouping: Optional[bool] = False,\n-    ) -> tuple:\n-        \"\"\"\n-        Pads a batch of images to the bottom and right of the image with zeros to the size of largest height and width\n-        in the batch and optionally returns their corresponding pixel mask.\n-\n-        Args:\n-            image (`torch.Tensor`):\n-                Image to pad.\n-            constant_values (`float` or `Iterable[float]`, *optional*):\n-                The value to use for the padding if `mode` is `\"constant\"`.\n-            return_pixel_mask (`bool`, *optional*, defaults to `True`):\n-                Whether to return a pixel mask.\n-            disable_grouping (`bool`, *optional*, defaults to `False`):\n-                Whether to disable grouping of images by size.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return. Can be one of:\n-                    - Unset: Return a list of `np.ndarray`.\n-                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n-                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n-                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n-        \"\"\"\n-        pad_size = get_max_height_width(images)\n-\n-        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n-        processed_images_grouped = {}\n-        processed_masks_grouped = {}\n-        for shape, stacked_images in grouped_images.items():\n-            stacked_images = self._pad_image(\n-                stacked_images,\n-                pad_size,\n-                constant_values=constant_values,\n-            )\n-            processed_images_grouped[shape] = stacked_images\n-\n-            if return_pixel_mask:\n-                stacked_masks = make_pixel_mask(image=stacked_images, output_size=pad_size)\n-                processed_masks_grouped[shape] = stacked_masks\n-\n-        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-\n-        processed_masks = None\n-        if return_pixel_mask:\n-            processed_masks = reorder_images(processed_masks_grouped, grouped_images_index)\n-\n-        return processed_images, processed_masks\n-\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n@@ -325,7 +267,7 @@ def _preprocess(\n         data = {}\n         if do_pad:\n             processed_images, processed_masks = self.pad(\n-                processed_images, return_pixel_mask=True, disable_grouping=disable_grouping\n+                processed_images, return_mask=True, disable_grouping=disable_grouping\n             )\n             processed_masks = torch.stack(processed_masks, dim=0) if return_tensors else processed_masks\n             data[\"pixel_mask\"] = processed_masks"
        },
        {
            "sha": "6d7059c4c5a5dfef87ca20117d60a4b6e9fb5f72",
            "filename": "src/transformers/models/bridgetower/processing_bridgetower.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -16,10 +16,17 @@\n Processor class for BridgeTower.\n \"\"\"\n \n-from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+from typing import Optional\n+\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin\n+\n+\n+class BridgeTowerImagesKwargs(ImagesKwargs):\n+    size_divisor: Optional[int]\n \n \n class BridgeTowerProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: BridgeTowerImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"add_special_tokens\": True,"
        },
        {
            "sha": "afe76134bc8d856e7ac50a29ae3007043554e888",
            "filename": "src/transformers/models/cohere2_vision/image_processing_cohere2_vision_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fimage_processing_cohere2_vision_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -227,6 +227,7 @@ def _preprocess(\n         image_std: Optional[Union[float, list[float]]],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         if crop_to_patches:\n             grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "86e51f2b4a6024cf22ac5e81aeb9a6cd9d4aa882",
            "filename": "src/transformers/models/conditional_detr/image_processing_conditional_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fimage_processing_conditional_detr_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -74,23 +74,12 @@ class ConditionalDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         Controls whether to convert the annotations to the format expected by the CONDITIONAL_DETR model. Converts the\n         bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n         Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-        If `pad_size` is provided, the image will be padded to the specified dimensions.\n-        Otherwise, the image will be padded to the maximum height and width of the batch.\n-    pad_size (`dict[str, int]`, *optional*):\n-        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-        height and width in the batch.\n     return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n         Whether to return segmentation masks.\n     \"\"\"\n \n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[dict[str, int]]\n     return_segmentation_masks: Optional[bool]\n \n \n@@ -629,7 +618,7 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n-        pad_size: Optional[dict[str, int]],\n+        pad_size: Optional[SizeDict],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n@@ -698,7 +687,7 @@ def _preprocess(\n         if do_pad:\n             # depends on all resized image shapes so we need another loop\n             if pad_size is not None:\n-                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+                padded_size = (pad_size.height, pad_size.width)\n             else:\n                 padded_size = get_max_height_width(images)\n "
        },
        {
            "sha": "0866b230a52e3da4e1a2ffe26856e8900acf478c",
            "filename": "src/transformers/models/convnext/image_processing_convnext_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconvnext%2Fimage_processing_convnext_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -155,6 +155,7 @@ def _preprocess(\n         image_std: Optional[Union[float, list[float]]],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "7ab4e98012ac3ad5230e4b51b2e2f905384e3c81",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl.py",
            "status": "modified",
            "additions": 23,
            "deletions": 10,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -89,6 +89,8 @@ class DeepseekVLImageProcessor(BaseImageProcessor):\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_convert_rgb (`bool`, *optional*, defaults to `True`):\n             Whether to convert the image to RGB.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Whether to pad the image to square or not.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n@@ -105,6 +107,7 @@ def __init__(\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: Optional[bool] = None,\n+        do_pad: Optional[bool] = True,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -121,6 +124,7 @@ def __init__(\n         self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n         self.do_convert_rgb = do_convert_rgb\n \n+        self.do_pad = do_pad\n         self.min_size = min_size\n         if image_mean is None:\n             self.background_color = (127, 127, 127)\n@@ -131,7 +135,6 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: Union[dict[str, int], int],\n-        background_color: Optional[tuple[int, int, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -145,8 +148,6 @@ def resize(\n                 Image to resize.\n             size (`dict[str, int]` or `int`):\n                 The size to resize the image to. If a dictionary, it should have the keys `\"height\"` and `\"width\"`.\n-            background_color (`tuple[int, int, int]`):\n-                The background color to use for the padding.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n             data_format (`ChannelDimension` or `str`, *optional*):\n@@ -165,7 +166,6 @@ def resize(\n         Returns:\n             `np.ndarray`: The resized image.\n         \"\"\"\n-        background_color = background_color if background_color is not None else self.background_color\n         if input_data_format is None:\n             input_data_format = infer_channel_dimension_format(image)\n \n@@ -194,12 +194,6 @@ def resize(\n             input_data_format=input_data_format,\n             **kwargs,\n         )\n-        # Expand and pad the images to obtain a square image of dimensions `size x size`\n-        image = self.pad_to_square(\n-            image=image,\n-            background_color=background_color,\n-            input_data_format=input_data_format,\n-        )\n         return image\n \n     @filter_out_non_signature_kwargs()\n@@ -216,6 +210,8 @@ def preprocess(\n         image_std: Optional[Union[float, list[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         do_convert_rgb: Optional[bool] = None,\n+        background_color: Optional[Union[int, tuple[int, int, int]]] = None,\n+        do_pad: Optional[bool] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> PIL.Image.Image:\n@@ -247,6 +243,10 @@ def preprocess(\n                 Image standard deviation to normalize the image by if `do_normalize` is set to `True`.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                 Whether to convert the image to RGB.\n+            background_color (`tuple[int, int, int]`):\n+                The background color to use for the padding.\n+            do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n+                Whether to pad the image to square or not.\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n                     - Unset: Return a list of `np.ndarray`.\n@@ -274,6 +274,8 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        do_pad = do_pad if do_pad is not None else self.do_pad\n+        background_color = background_color if background_color is not None else self.background_color\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n@@ -319,6 +321,17 @@ def preprocess(\n                 for image in images\n             ]\n \n+        if do_pad:\n+            # Expand and pad the images to obtain a square image of dimensions `size x size`\n+            images = [\n+                self.pad_to_square(\n+                    image=image,\n+                    background_color=background_color,\n+                    input_data_format=input_data_format,\n+                )\n+                for image in images\n+            ]\n+\n         if do_rescale:\n             images = [\n                 self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)"
        },
        {
            "sha": "7764a82501598e7019eccb2ab4bc8f620f514066",
            "filename": "src/transformers/models/deepseek_vl/image_processing_deepseek_vl_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fimage_processing_deepseek_vl_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -62,6 +62,7 @@ class DeepseekVLImageProcessorFast(BaseImageProcessorFast):\n     do_resize = True\n     do_rescale = True\n     do_normalize = True\n+    do_pad = True\n     valid_kwargs = DeepseekVLFastImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[DeepseekVLFastImageProcessorKwargs]):"
        },
        {
            "sha": "7c7d6df82424487c5ba7388a822a68679753e6f8",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 25,
            "deletions": 12,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -102,6 +102,8 @@ class DeepseekVLHybridImageProcessor(BaseImageProcessor):\n             number of channels in the image. Can be overridden by the `high_res_image_std` parameter in the `preprocess` method.\n         do_convert_rgb (`bool`, *optional*, defaults to `True`):\n             Whether to convert the image to RGB.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Whether to pad the image to square or not.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"high_res_pixel_values\"]\n@@ -122,6 +124,7 @@ def __init__(\n         high_res_image_mean: Optional[Union[float, list[float]]] = None,\n         high_res_image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: Optional[bool] = None,\n+        do_pad: bool = True,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -147,6 +150,7 @@ def __init__(\n         self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n         self.do_convert_rgb = do_convert_rgb\n \n+        self.do_pad = do_pad\n         self.min_size = min_size\n         if image_mean is None:\n             self.background_color = (127, 127, 127)\n@@ -162,7 +166,6 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: Union[dict[str, int], int],\n-        background_color: Optional[tuple[int, int, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -176,8 +179,6 @@ def resize(\n                 Image to resize.\n             size (`dict[str, int]` or `int`):\n                 The size to resize the image to. If a dictionary, it should have the keys `\"height\"` and `\"width\"`.\n-            background_color (`tuple[int, int, int]`):\n-                The background color to use for the padding.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n             data_format (`ChannelDimension` or `str`, *optional*):\n@@ -196,7 +197,6 @@ def resize(\n         Returns:\n             `np.ndarray`: The resized image.\n         \"\"\"\n-        background_color = background_color if background_color is not None else self.background_color\n         if input_data_format is None:\n             input_data_format = infer_channel_dimension_format(image)\n \n@@ -225,12 +225,6 @@ def resize(\n             input_data_format=input_data_format,\n             **kwargs,\n         )\n-        # Expand and pad the images to obtain a square image of dimensions `size x size`\n-        image = self.pad_to_square(\n-            image=image,\n-            background_color=background_color,\n-            input_data_format=input_data_format,\n-        )\n         return image\n \n     @filter_out_non_signature_kwargs()\n@@ -253,6 +247,8 @@ def preprocess(\n         data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n         do_convert_rgb: Optional[bool] = None,\n+        do_pad: Optional[bool] = None,\n+        background_color: Optional[tuple[int, int, int]] = None,\n     ) -> PIL.Image.Image:\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -309,6 +305,10 @@ def preprocess(\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                 Whether to convert the image to RGB.\n+            do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n+                Whether to pad the image to square or not.\n+            background_color (`tuple[int, int, int]`):\n+                The background color to use for the padding.\n         \"\"\"\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n@@ -321,6 +321,8 @@ def preprocess(\n         high_res_image_mean = high_res_image_mean if high_res_image_mean is not None else self.high_res_image_mean\n         high_res_image_std = high_res_image_std if high_res_image_std is not None else self.high_res_image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        do_pad = do_pad if do_pad is not None else self.do_pad\n+        background_color = background_color if background_color is not None else self.background_color\n \n         size = size if size is not None else self.size\n         size_dict = get_size_dict(size)\n@@ -372,17 +374,28 @@ def preprocess(\n                 high_res_image = self.resize(\n                     image=high_res_image,\n                     size=high_res_size_dict,\n-                    background_color=self.high_res_background_color,\n                     resample=high_res_resample,\n                     input_data_format=input_data_format,\n                 )\n+                if do_pad:\n+                    # Expand and pad the images to obtain a square image of dimensions `size x size`\n+                    high_res_image = self.pad_to_square(\n+                        image=high_res_image,\n+                        background_color=background_color,\n+                        input_data_format=input_data_format,\n+                    )\n                 image = self.resize(\n                     image=high_res_image,\n                     size=size_dict,\n-                    background_color=self.background_color,\n                     resample=resample,\n                     input_data_format=input_data_format,\n                 )\n+                if do_pad:\n+                    image = self.pad_to_square(\n+                        image=image,\n+                        background_color=background_color,\n+                        input_data_format=input_data_format,\n+                    )\n \n             if do_rescale:\n                 image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)"
        },
        {
            "sha": "3770cf18303e2c37ef0653765eaaca0ebd313327",
            "filename": "src/transformers/models/deepseek_vl_hybrid/image_processing_deepseek_vl_hybrid_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fimage_processing_deepseek_vl_hybrid_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -86,6 +86,7 @@ class DeepseekVLHybridImageProcessorFast(BaseImageProcessorFast):\n     do_resize = True\n     do_rescale = True\n     do_normalize = True\n+    do_pad = True\n     valid_kwargs = DeepseekVLHybridFastImageProcessorKwargs\n     high_res_image_mean = OPENAI_CLIP_MEAN\n     high_res_image_std = OPENAI_CLIP_STD"
        },
        {
            "sha": "c6cf71b096132017004e8d469b42038f39f66e64",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 25,
            "deletions": 2,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -488,6 +488,8 @@ class DeepseekVLHybridImageProcessor(DeepseekVLImageProcessor):\n             number of channels in the image. Can be overridden by the `high_res_image_std` parameter in the `preprocess` method.\n         do_convert_rgb (`bool`, *optional*, defaults to `True`):\n             Whether to convert the image to RGB.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Whether to pad the image to square or not.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\", \"high_res_pixel_values\"]\n@@ -508,6 +510,7 @@ def __init__(\n         high_res_image_mean: Optional[Union[float, list[float]]] = None,\n         high_res_image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: Optional[bool] = None,\n+        do_pad: bool = True,\n         **kwargs,\n     ) -> None:\n         high_res_size = high_res_size if high_res_size is not None else {\"height\": 1024, \"width\": 1024}\n@@ -531,6 +534,7 @@ def __init__(\n             image_mean=image_mean,\n             image_std=image_std,\n             do_convert_rgb=do_convert_rgb,\n+            do_pad=do_pad,\n             **kwargs,\n         )\n \n@@ -559,6 +563,8 @@ def preprocess(\n         data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n         do_convert_rgb: Optional[bool] = None,\n+        do_pad: Optional[bool] = None,\n+        background_color: Optional[tuple[int, int, int]] = None,\n     ):\n         \"\"\"\n         Preprocess an image or batch of images.\n@@ -615,6 +621,10 @@ def preprocess(\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                 Whether to convert the image to RGB.\n+            do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n+                Whether to pad the image to square or not.\n+            background_color (`tuple[int, int, int]`):\n+                The background color to use for the padding.\n         \"\"\"\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n@@ -627,6 +637,8 @@ def preprocess(\n         high_res_image_mean = high_res_image_mean if high_res_image_mean is not None else self.high_res_image_mean\n         high_res_image_std = high_res_image_std if high_res_image_std is not None else self.high_res_image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        do_pad = do_pad if do_pad is not None else self.do_pad\n+        background_color = background_color if background_color is not None else self.background_color\n \n         size = size if size is not None else self.size\n         size_dict = get_size_dict(size)\n@@ -678,17 +690,28 @@ def preprocess(\n                 high_res_image = self.resize(\n                     image=high_res_image,\n                     size=high_res_size_dict,\n-                    background_color=self.high_res_background_color,\n                     resample=high_res_resample,\n                     input_data_format=input_data_format,\n                 )\n+                if do_pad:\n+                    # Expand and pad the images to obtain a square image of dimensions `size x size`\n+                    high_res_image = self.pad_to_square(\n+                        image=high_res_image,\n+                        background_color=background_color,\n+                        input_data_format=input_data_format,\n+                    )\n                 image = self.resize(\n                     image=high_res_image,\n                     size=size_dict,\n-                    background_color=self.background_color,\n                     resample=resample,\n                     input_data_format=input_data_format,\n                 )\n+                if do_pad:\n+                    image = self.pad_to_square(\n+                        image=image,\n+                        background_color=background_color,\n+                        input_data_format=input_data_format,\n+                    )\n \n             if do_rescale:\n                 image = self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)"
        },
        {
            "sha": "2bfbedddc5d0ac9c108966b526f8a7d9a0bc4f8e",
            "filename": "src/transformers/models/deformable_detr/image_processing_deformable_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fimage_processing_deformable_detr_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -65,23 +65,12 @@ class DeformableDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         Controls whether to convert the annotations to the format expected by the DEFORMABLE_DETR model. Converts the\n         bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n         Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-        If `pad_size` is provided, the image will be padded to the specified dimensions.\n-        Otherwise, the image will be padded to the maximum height and width of the batch.\n-    pad_size (`dict[str, int]`, *optional*):\n-        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-        height and width in the batch.\n     return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n         Whether to return segmentation masks.\n     \"\"\"\n \n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[dict[str, int]]\n     return_segmentation_masks: Optional[bool]\n \n \n@@ -620,7 +609,7 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n-        pad_size: Optional[dict[str, int]],\n+        pad_size: Optional[SizeDict],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n@@ -689,7 +678,7 @@ def _preprocess(\n         if do_pad:\n             # depends on all resized image shapes so we need another loop\n             if pad_size is not None:\n-                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+                padded_size = (pad_size.height, pad_size.width)\n             else:\n                 padded_size = get_max_height_width(images)\n "
        },
        {
            "sha": "d27220c3d2be8879b53917c56d9f4c8caef551e5",
            "filename": "src/transformers/models/depth_pro/image_processing_depth_pro_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_pro%2Fimage_processing_depth_pro_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -78,6 +78,7 @@ def _preprocess(\n         image_std: Optional[Union[float, list[float]]],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched scaling\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "ba216a6f2d49509f57f4a584ee981a38fc81205e",
            "filename": "src/transformers/models/detr/image_processing_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fimage_processing_detr_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -286,23 +286,12 @@ class DetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         Controls whether to convert the annotations to the format expected by the DETR model. Converts the\n         bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n         Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-        If `pad_size` is provided, the image will be padded to the specified dimensions.\n-        Otherwise, the image will be padded to the maximum height and width of the batch.\n-    pad_size (`dict[str, int]`, *optional*):\n-        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-        height and width in the batch.\n     return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n         Whether to return segmentation masks.\n     \"\"\"\n \n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[dict[str, int]]\n     return_segmentation_masks: Optional[bool]\n \n \n@@ -641,7 +630,7 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n-        pad_size: Optional[dict[str, int]],\n+        pad_size: Optional[SizeDict],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n@@ -710,7 +699,7 @@ def _preprocess(\n         if do_pad:\n             # depends on all resized image shapes so we need another loop\n             if pad_size is not None:\n-                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+                padded_size = (pad_size.height, pad_size.width)\n             else:\n                 padded_size = get_max_height_width(images)\n "
        },
        {
            "sha": "fba0d30894384e6eaef5aa787013316a9056835c",
            "filename": "src/transformers/models/dinov3_vit/image_processing_dinov3_vit_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_vit%2Fimage_processing_dinov3_vit_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -70,6 +70,7 @@ def _preprocess(\n         image_std: Optional[Union[float, list[float]]],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "7dec96422c5d82e51fe9c4291930d9827bd54e11",
            "filename": "src/transformers/models/donut/image_processing_donut.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -215,10 +215,6 @@ def pad_image(\n         padding = ((pad_top, pad_bottom), (pad_left, pad_right))\n         return pad(image, padding, data_format=data_format, input_data_format=input_data_format)\n \n-    def pad(self, *args, **kwargs):\n-        logger.info(\"pad is deprecated and will be removed in version 4.27. Please use pad_image instead.\")\n-        return self.pad_image(*args, **kwargs)\n-\n     def thumbnail(\n         self,\n         image: np.ndarray,\n@@ -412,8 +408,6 @@ def preprocess(\n             do_normalize=do_normalize,\n             image_mean=image_mean,\n             image_std=image_std,\n-            do_pad=do_pad,\n-            size_divisibility=size,  # There is no pad divisibility in this processor, but pad requires the size arg.\n             do_resize=do_resize,\n             size=size,\n             resample=resample,"
        },
        {
            "sha": "23714affe1e8625cbf2b9dddd994509518082292",
            "filename": "src/transformers/models/donut/image_processing_donut_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 5,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fimage_processing_donut_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -49,15 +49,10 @@ class DonutFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n             Whether to resize the image using thumbnail method.\n         do_align_long_axis (`bool`, *optional*, defaults to `self.do_align_long_axis`):\n             Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n-        do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n-            Whether to pad the image. If `random_padding` is set to `True`, each image is padded with a random\n-            amount of padding on each size, up to the largest image size in the batch. Otherwise, all images are\n-            padded to the largest image size in the batch.\n     \"\"\"\n \n     do_thumbnail: Optional[bool]\n     do_align_long_axis: Optional[bool]\n-    do_pad: Optional[bool]\n \n \n @auto_docstring"
        },
        {
            "sha": "9b28950d2ded2d046e6c39d54d246f893a50d122",
            "filename": "src/transformers/models/dpt/image_processing_dpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -541,8 +541,6 @@ def preprocess(\n             do_normalize=do_normalize,\n             image_mean=image_mean,\n             image_std=image_std,\n-            do_pad=do_pad,\n-            size_divisibility=size_divisor,\n             do_resize=do_resize,\n             size=size,\n             resample=resample,"
        },
        {
            "sha": "7fce8a9f64db79efac9bad62eb1915890b55f473",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -64,9 +64,6 @@ class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     ensure_multiple_of (`int`, *optional*, defaults to 1):\n         If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overridden\n         by `ensure_multiple_of` in `preprocess`.\n-    do_pad (`bool`, *optional*, defaults to `False`):\n-        Whether to apply center padding. This was introduced in the DINOv2 paper, which uses the model in\n-        combination with DPT.\n     size_divisor (`int`, *optional*):\n         If `do_pad` is `True`, pads the image dimensions to be divisible by this value. This was introduced in the\n         DINOv2 paper, which uses the model in combination with DPT.\n@@ -81,7 +78,6 @@ class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n \n     ensure_multiple_of: Optional[int]\n     size_divisor: Optional[int]\n-    do_pad: Optional[bool]\n     keep_aspect_ratio: Optional[bool]\n     do_reduce_labels: Optional[bool]\n "
        },
        {
            "sha": "7ae6bb40c3afb1649c216d85837e8cb39f3f6d21",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -94,9 +94,6 @@ class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     ensure_multiple_of (`int`, *optional*, defaults to 1):\n         If `do_resize` is `True`, the image is resized to a size that is a multiple of this value. Can be overridden\n         by `ensure_multiple_of` in `preprocess`.\n-    do_pad (`bool`, *optional*, defaults to `False`):\n-        Whether to apply center padding. This was introduced in the DINOv2 paper, which uses the model in\n-        combination with DPT.\n     size_divisor (`int`, *optional*):\n         If `do_pad` is `True`, pads the image dimensions to be divisible by this value. This was introduced in the\n         DINOv2 paper, which uses the model in combination with DPT.\n@@ -111,7 +108,6 @@ class DPTFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n \n     ensure_multiple_of: Optional[int]\n     size_divisor: Optional[int]\n-    do_pad: Optional[bool]\n     keep_aspect_ratio: Optional[bool]\n     do_reduce_labels: Optional[bool]\n "
        },
        {
            "sha": "e52d9dc8ee913baf048206137dfcfc69eff305e0",
            "filename": "src/transformers/models/fuyu/image_processing_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fimage_processing_fuyu.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -455,8 +455,6 @@ def preprocess(\n             do_normalize=do_normalize,\n             image_mean=image_mean,\n             image_std=image_std,\n-            do_pad=do_pad,\n-            size_divisibility=size,  # There is no pad divisibility in this processor, but pad requires the size arg.\n             do_resize=do_resize,\n             size=size,\n             resample=resample,"
        },
        {
            "sha": "3826f40bd997ab8681a3e83df207ae6293ded72a",
            "filename": "src/transformers/models/gemma3/image_processing_gemma3_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fimage_processing_gemma3_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -194,15 +194,14 @@ def _preprocess(\n         pan_and_scan_max_num_crops: Optional[int],\n         pan_and_scan_min_ratio_to_activate: Optional[float],\n         interpolation: Optional[\"F.InterpolationMode\"],\n-        do_center_crop: bool,\n-        crop_size: SizeDict,\n         do_rescale: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched processing\n         processed_images_grouped = {}"
        },
        {
            "sha": "38b87aed623f0677b98b042202e894b81f5f9a07",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -173,6 +173,7 @@ def _preprocess(\n         image_std: Optional[Union[float, list[float]]],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         if crop_to_patches:\n             grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "59866c9a410e13cacc55b9c9c3546f7b0674ea15",
            "filename": "src/transformers/models/grounding_dino/image_processing_grounding_dino_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fimage_processing_grounding_dino_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -68,23 +68,12 @@ class GroundingDinoFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         Controls whether to convert the annotations to the format expected by the GROUNDING_DINO model. Converts the\n         bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n         Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-        If `pad_size` is provided, the image will be padded to the specified dimensions.\n-        Otherwise, the image will be padded to the maximum height and width of the batch.\n-    pad_size (`dict[str, int]`, *optional*):\n-        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-        height and width in the batch.\n     return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n         Whether to return segmentation masks.\n     \"\"\"\n \n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[dict[str, int]]\n     return_segmentation_masks: Optional[bool]\n \n \n@@ -651,7 +640,7 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n-        pad_size: Optional[dict[str, int]],\n+        pad_size: Optional[SizeDict],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n@@ -720,7 +709,7 @@ def _preprocess(\n         if do_pad:\n             # depends on all resized image shapes so we need another loop\n             if pad_size is not None:\n-                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+                padded_size = (pad_size.height, pad_size.width)\n             else:\n                 padded_size = get_max_height_width(images)\n "
        },
        {
            "sha": "5348bda389edb5b3ac2831cfa52264cd799334d1",
            "filename": "src/transformers/models/idefics2/image_processing_idefics2_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fimage_processing_idefics2_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -109,12 +109,9 @@ class Idefics2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     do_image_splitting (`bool`, *optional*, defaults to `False`):\n         Whether to split the image into a sequence 4 equal sub-images concatenated with the original image.\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Whether to pad images to the largest height and width in the batch.\n     \"\"\"\n \n     do_image_splitting: Optional[bool]\n-    do_pad: Optional[bool]\n \n \n @auto_docstring"
        },
        {
            "sha": "5b0c0e6180f9841e3bd25bf0eec5ea19a262d49c",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -171,9 +171,6 @@ def make_pixel_mask(image: \"torch.Tensor\", output_size: tuple[int, int]) -> \"tor\n \n class Idefics3FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n-    do_pad (`bool`, *optional*):\n-        Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n-        number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n     do_image_splitting (`bool`, *optional*, defaults to `True`):\n         Whether to split the image into sub-images concatenated with the original image. They are split into patches\n         such that each patch has a size of `max_image_size[\"height\"]` x `max_image_size[\"width\"]`.\n@@ -183,7 +180,6 @@ class Idefics3FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         Whether to return the row and column information of the images.\n     \"\"\"\n \n-    do_pad: Optional[bool]\n     do_image_splitting: Optional[bool]\n     max_image_size: Optional[dict[str, int]]\n     return_row_col_info: Optional[bool]"
        },
        {
            "sha": "a2cd3cf351d2fcfbc33d4d4ba6fba7e29fae29b1",
            "filename": "src/transformers/models/instructblipvideo/video_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fvideo_processing_instructblipvideo.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -61,12 +61,10 @@ def _preprocess(\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n-        size_divisor: Optional[int],\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_center_crop: bool,\n         crop_size: SizeDict,\n         do_rescale: bool,\n-        do_pad: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n@@ -81,9 +79,7 @@ def _preprocess(\n             if do_convert_rgb:\n                 stacked_videos = self.convert_to_rgb(stacked_videos)\n             if do_resize:\n-                stacked_videos = self.resize(\n-                    stacked_videos, size=size, size_divisor=size_divisor, interpolation=interpolation\n-                )\n+                stacked_videos = self.resize(stacked_videos, size=size, interpolation=interpolation)\n             resized_videos_grouped[shape] = stacked_videos\n         resized_videos = reorder_videos(resized_videos_grouped, grouped_videos_index)\n "
        },
        {
            "sha": "a2e06d3b7ec466c4286f76235580d3f8d9c02427",
            "filename": "src/transformers/models/internvl/video_processing_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fvideo_processing_internvl.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -110,12 +110,10 @@ def _preprocess(\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n-        size_divisor: Optional[int],\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_center_crop: bool,\n         crop_size: SizeDict,\n         do_rescale: bool,\n-        do_pad: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n@@ -130,9 +128,7 @@ def _preprocess(\n             if do_convert_rgb:\n                 stacked_videos = self.convert_to_rgb(stacked_videos)\n             if do_resize:\n-                stacked_videos = self.resize(\n-                    stacked_videos, size=size, size_divisor=size_divisor, interpolation=interpolation\n-                )\n+                stacked_videos = self.resize(stacked_videos, size=size, interpolation=interpolation)\n             resized_videos_grouped[shape] = stacked_videos\n         resized_videos = reorder_videos(resized_videos_grouped, grouped_videos_index)\n "
        },
        {
            "sha": "16659bd853542e75442e976a4b2746453a926b3b",
            "filename": "src/transformers/models/janus/image_processing_janus.py",
            "status": "modified",
            "additions": 23,
            "deletions": 10,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -86,6 +86,8 @@ class JanusImageProcessor(BaseImageProcessor):\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_convert_rgb (`bool`, *optional*, defaults to `True`):\n             Whether to convert the image to RGB.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Whether to pad the image to square or not.\n     \"\"\"\n \n     model_input_names = [\"pixel_values\"]\n@@ -102,6 +104,7 @@ def __init__(\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: Optional[bool] = None,\n+        do_pad: Optional[bool] = True,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -118,6 +121,7 @@ def __init__(\n         self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n         self.do_convert_rgb = do_convert_rgb\n \n+        self.do_pad = do_pad\n         self.min_size = min_size\n         if image_mean is None:\n             self.background_color = (127, 127, 127)\n@@ -128,7 +132,6 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: Union[dict[str, int], int],\n-        background_color: Optional[tuple[int, int, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -142,8 +145,6 @@ def resize(\n                 Image to resize.\n             size (`dict[str, int]` or `int`):\n                 The size to resize the image to. If a dictionary, it should have the keys `\"height\"` and `\"width\"`.\n-            background_color (`tuple[int, int, int]`):\n-                The background color to use for the padding.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n             data_format (`ChannelDimension` or `str`, *optional*):\n@@ -162,7 +163,6 @@ def resize(\n         Returns:\n             `np.ndarray`: The resized image.\n         \"\"\"\n-        background_color = background_color if background_color is not None else self.background_color\n         if input_data_format is None:\n             input_data_format = infer_channel_dimension_format(image)\n \n@@ -191,12 +191,6 @@ def resize(\n             input_data_format=input_data_format,\n             **kwargs,\n         )\n-        # Expand and pad the images to obtain a square image of dimensions `size x size`\n-        image = self.pad_to_square(\n-            image=image,\n-            background_color=background_color,\n-            input_data_format=input_data_format,\n-        )\n         return image\n \n     @filter_out_non_signature_kwargs()\n@@ -213,6 +207,8 @@ def preprocess(\n         image_std: Optional[Union[float, list[float]]] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         do_convert_rgb: Optional[bool] = None,\n+        background_color: Optional[Union[int, tuple[int, int, int]]] = None,\n+        do_pad: Optional[bool] = None,\n         data_format: ChannelDimension = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n     ) -> PIL.Image.Image:\n@@ -244,6 +240,10 @@ def preprocess(\n                 Image standard deviation to normalize the image by if `do_normalize` is set to `True`.\n             do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n                 Whether to convert the image to RGB.\n+            background_color (`tuple[int, int, int]`):\n+                The background color to use for the padding.\n+            do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n+                Whether to pad the image to square or not.\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n                     - Unset: Return a list of `np.ndarray`.\n@@ -271,6 +271,8 @@ def preprocess(\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n         do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        do_pad = do_pad if do_pad is not None else self.do_pad\n+        background_color = background_color if background_color is not None else self.background_color\n \n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)\n@@ -316,6 +318,17 @@ def preprocess(\n                 for image in images\n             ]\n \n+        if do_pad:\n+            # Expand and pad the images to obtain a square image of dimensions `size x size`\n+            images = [\n+                self.pad_to_square(\n+                    image=image,\n+                    background_color=background_color,\n+                    input_data_format=input_data_format,\n+                )\n+                for image in images\n+            ]\n+\n         if do_rescale:\n             images = [\n                 self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)"
        },
        {
            "sha": "3e9483f21bfede191b70ab1085a34a50203036da",
            "filename": "src/transformers/models/janus/image_processing_janus_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fimage_processing_janus_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -68,6 +68,7 @@ class JanusImageProcessorFast(BaseImageProcessorFast):\n     do_resize = True\n     do_rescale = True\n     do_normalize = True\n+    do_pad = True\n     valid_kwargs = JanusFastImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[JanusFastImageProcessorKwargs]):"
        },
        {
            "sha": "0541854200a5fc5d375abc7fa7730721e156ca14",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 166,
            "deletions": 11,
            "changes": 177,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -29,23 +29,28 @@\n from ...generation import ClassifierFreeGuidanceLogitsProcessor, GenerationMixin, GenerationMode, LogitsProcessorList\n from ...generation.utils import GenerateDecoderOnlyOutput\n from ...image_processing_utils import BatchFeature, get_size_dict\n-from ...image_transforms import resize, to_channel_dimension_format\n+from ...image_transforms import convert_to_rgb, resize, to_channel_dimension_format\n from ...image_utils import (\n     ChannelDimension,\n     ImageInput,\n     PILImageResampling,\n     get_image_size,\n     infer_channel_dimension_format,\n+    is_scaled_image,\n     make_flat_list_of_images,\n     to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n )\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n+    TensorType,\n     TransformersKwargs,\n     auto_docstring,\n     can_return_tuple,\n+    filter_out_non_signature_kwargs,\n     is_torch_available,\n     is_vision_available,\n     logging,\n@@ -1328,6 +1333,8 @@ class JanusImageProcessor(BlipImageProcessor):\n             Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_convert_rgb (`bool`, *optional*, defaults to `True`):\n             Whether to convert the image to RGB.\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Whether to pad the image to square or not.\n     \"\"\"\n \n     def __init__(\n@@ -1342,10 +1349,12 @@ def __init__(\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n         do_convert_rgb: Optional[bool] = None,\n+        do_pad: Optional[bool] = True,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n \n+        self.do_pad = do_pad\n         self.min_size = min_size\n         if image_mean is None:\n             self.background_color = (127, 127, 127)\n@@ -1430,7 +1439,6 @@ def resize(\n         self,\n         image: np.ndarray,\n         size: Union[dict[str, int], int],\n-        background_color: Optional[tuple[int, int, int]] = None,\n         resample: PILImageResampling = PILImageResampling.BICUBIC,\n         data_format: Optional[Union[str, ChannelDimension]] = None,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -1444,8 +1452,6 @@ def resize(\n                 Image to resize.\n             size (`dict[str, int]` or `int`):\n                 The size to resize the image to. If a dictionary, it should have the keys `\"height\"` and `\"width\"`.\n-            background_color (`tuple[int, int, int]`):\n-                The background color to use for the padding.\n             resample (`PILImageResampling`, *optional*, defaults to `PILImageResampling.BICUBIC`):\n                 `PILImageResampling` filter to use when resizing the image e.g. `PILImageResampling.BICUBIC`.\n             data_format (`ChannelDimension` or `str`, *optional*):\n@@ -1464,7 +1470,6 @@ def resize(\n         Returns:\n             `np.ndarray`: The resized image.\n         \"\"\"\n-        background_color = background_color if background_color is not None else self.background_color\n         if input_data_format is None:\n             input_data_format = infer_channel_dimension_format(image)\n \n@@ -1493,14 +1498,164 @@ def resize(\n             input_data_format=input_data_format,\n             **kwargs,\n         )\n-        # Expand and pad the images to obtain a square image of dimensions `size x size`\n-        image = self.pad_to_square(\n-            image=image,\n-            background_color=background_color,\n-            input_data_format=input_data_format,\n-        )\n         return image\n \n+    @filter_out_non_signature_kwargs()\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        size: Optional[dict[str, int]] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, list[float]]] = None,\n+        image_std: Optional[Union[float, list[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        background_color: Optional[Union[int, tuple[int, int, int]]] = None,\n+        do_pad: Optional[bool] = None,\n+        data_format: ChannelDimension = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+    ) -> PIL.Image.Image:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`dict[str, int]`, *optional*, defaults to `self.size`):\n+                Controls the size of the image after `resize`. The shortest edge of the image is resized to\n+                `size[\"shortest_edge\"]` whilst preserving the aspect ratio. If the longest edge of this resized image\n+                is > `int(size[\"shortest_edge\"] * (1333 / 800))`, then the image is resized again to make the longest\n+                edge equal to `int(size[\"shortest_edge\"] * (1333 / 800))`.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. Only has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image values between [0 - 1].\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `list[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to normalize the image by if `do_normalize` is set to `True`.\n+            image_std (`float` or `list[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to normalize the image by if `do_normalize` is set to `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            background_color (`tuple[int, int, int]`):\n+                The background color to use for the padding.\n+            do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n+                Whether to pad the image to square or not.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                    - Unset: Return a list of `np.ndarray`.\n+                    - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                    - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                    - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        do_pad = do_pad if do_pad is not None else self.do_pad\n+        background_color = background_color if background_color is not None else self.background_color\n+\n+        size = size if size is not None else self.size\n+        size = get_size_dict(size, default_to_square=False)\n+        images = self.fetch_images(images)\n+        images = make_flat_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+        # PIL RGBA images are converted to RGB\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if do_rescale and is_scaled_image(images[0]):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        if do_resize:\n+            images = [\n+                self.resize(image=image, size=size, resample=resample, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_pad:\n+            # Expand and pad the images to obtain a square image of dimensions `size x size`\n+            images = [\n+                self.pad_to_square(\n+                    image=image,\n+                    background_color=background_color,\n+                    input_data_format=input_data_format,\n+                )\n+                for image in images\n+            ]\n+\n+        if do_rescale:\n+            images = [\n+                self.rescale(image=image, scale=rescale_factor, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        if do_normalize:\n+            images = [\n+                self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=input_data_format)\n+                for image in images\n+            ]\n+\n+        images = [\n+            to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images\n+        ]\n+\n+        encoded_outputs = BatchFeature(data={\"pixel_values\": images}, tensor_type=return_tensors)\n+\n+        return encoded_outputs\n+\n     def postprocess(\n         self,\n         images: ImageInput,"
        },
        {
            "sha": "cf62f250bc2ff206847fe02507e1c3c46f48a306",
            "filename": "src/transformers/models/llava/image_processing_llava_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fimage_processing_llava_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -56,14 +56,7 @@\n         from torchvision.transforms import functional as F\n \n \n-class LlavaFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    \"\"\"\n-    Args:\n-        do_pad (`bool`, *optional*):\n-            Whether to pad the image to a square based on the longest edge.\n-    \"\"\"\n-\n-    do_pad: Optional[bool]\n+class LlavaFastImageProcessorKwargs(DefaultFastImageProcessorKwargs): ...\n \n \n @auto_docstring\n@@ -147,6 +140,7 @@ def _preprocess(\n         image_std: Optional[Union[float, list[float]]],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         # Group images by size for batched resizing\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "201a652605890d80fb2f1200968a8c657ae1ce74",
            "filename": "src/transformers/models/llava_next/image_processing_llava_next_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fimage_processing_llava_next_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -59,13 +59,9 @@ class LlavaNextFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n         based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n         method.\n-    do_pad (`bool`, *optional*):\n-        Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n-        number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n     \"\"\"\n \n     image_grid_pinpoints: Optional[list[list[int]]]\n-    do_pad: Optional[bool]\n \n \n @auto_docstring"
        },
        {
            "sha": "4392d64e9ebfb6f4d5ea91e96bb3bf84feed71a5",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -56,13 +56,9 @@ class LlavaOnevisionFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n         based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n         method.\n-    do_pad (`bool`, *optional*):\n-        Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n-        number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n     \"\"\"\n \n     image_grid_pinpoints: Optional[list[list[int]]]\n-    do_pad: Optional[bool]\n \n \n @auto_docstring"
        },
        {
            "sha": "45dfac3b37ef4daf7bea6834ad13cab23aa3638d",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -72,13 +72,9 @@ class LlavaOnevisionFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         A list of possible resolutions to use for processing high resolution images. The best resolution is selected\n         based on the original size of the image. Can be overridden by `image_grid_pinpoints` in the `preprocess`\n         method.\n-    do_pad (`bool`, *optional*):\n-        Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n-        number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n     \"\"\"\n \n     image_grid_pinpoints: Optional[list[list[int]]]\n-    do_pad: Optional[bool]\n \n \n class LlavaOnevisionImageProcessorFast(LlavaNextImageProcessorFast):"
        },
        {
            "sha": "c61d531eb07786b8c353f9a9da01c46851a9a207",
            "filename": "src/transformers/models/mask2former/image_processing_mask2former_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fimage_processing_mask2former_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -84,23 +84,12 @@ class Mask2FormerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         The background label will be replaced by `ignore_index`.\n     num_labels (`int`, *optional*):\n         The number of labels in the segmentation map.\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-        If `pad_size` is provided, the image will be padded to the specified dimensions.\n-        Otherwise, the image will be padded to the maximum height and width of the batch.\n-    pad_size (`Dict[str, int]`, *optional*):\n-        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-        height and width in the batch.\n     \"\"\"\n \n     size_divisor: Optional[int]\n     ignore_index: Optional[int]\n     do_reduce_labels: Optional[bool]\n     num_labels: Optional[int]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[dict[str, int]]\n \n \n def convert_segmentation_map_to_binary_masks_fast(\n@@ -334,8 +323,8 @@ def _preprocess(\n         segmentation_maps: Optional[\"torch.Tensor\"],\n         instance_id_to_semantic_id: Optional[dict[int, int]],\n         do_resize: Optional[bool],\n-        size: Optional[dict[str, int]],\n-        pad_size: Optional[dict[str, int]],\n+        size: Optional[SizeDict],\n+        pad_size: Optional[SizeDict],\n         size_divisor: Optional[int],\n         interpolation: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]],\n         do_rescale: Optional[bool],\n@@ -383,7 +372,7 @@ def _preprocess(\n                 resized_segmentation_maps_grouped, grouped_segmentation_maps_index\n             )\n         if pad_size is not None:\n-            padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+            padded_size = (pad_size.height, pad_size.width)\n         else:\n             padded_size = get_max_height_width(resized_images)\n "
        },
        {
            "sha": "0b1c95aa1012d7259f1f3f3767d48b31c887815f",
            "filename": "src/transformers/models/maskformer/image_processing_maskformer_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 14,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fimage_processing_maskformer_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -120,23 +120,12 @@ class MaskFormerFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         The background label will be replaced by `ignore_index`.\n     num_labels (`int`, *optional*):\n         The number of labels in the segmentation map.\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-        If `pad_size` is provided, the image will be padded to the specified dimensions.\n-        Otherwise, the image will be padded to the maximum height and width of the batch.\n-    pad_size (`Dict[str, int]`, *optional*):\n-        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-        height and width in the batch.\n     \"\"\"\n \n     size_divisor: Optional[int]\n     ignore_index: Optional[int]\n     do_reduce_labels: Optional[bool]\n     num_labels: Optional[int]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[dict[str, int]]\n \n \n @auto_docstring\n@@ -335,8 +324,8 @@ def _preprocess(\n         segmentation_maps: Optional[\"torch.Tensor\"],\n         instance_id_to_semantic_id: Optional[dict[int, int]],\n         do_resize: Optional[bool],\n-        size: Optional[dict[str, int]],\n-        pad_size: Optional[dict[str, int]],\n+        size: Optional[SizeDict],\n+        pad_size: Optional[SizeDict],\n         size_divisor: Optional[int],\n         interpolation: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]],\n         do_rescale: Optional[bool],\n@@ -384,7 +373,7 @@ def _preprocess(\n                 resized_segmentation_maps_grouped, grouped_segmentation_maps_index\n             )\n         if pad_size is not None:\n-            padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+            padded_size = (pad_size.height, pad_size.width)\n         else:\n             padded_size = get_max_height_width(resized_images)\n "
        },
        {
            "sha": "0c0a51464b43ad953bf3aeaab74dd2b7901dc8c3",
            "filename": "src/transformers/models/nougat/image_processing_nougat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -461,8 +461,6 @@ def preprocess(\n             do_normalize=do_normalize,\n             image_mean=image_mean,\n             image_std=image_std,\n-            do_pad=do_pad,\n-            size_divisibility=size,  # There is no pad divisibility in this processor, but pad requires the size arg.\n             do_resize=do_resize,\n             size=size,\n             resample=resample,"
        },
        {
            "sha": "ebe37389f3f678b5812d2a0e02680a85fe8102b2",
            "filename": "src/transformers/models/nougat/image_processing_nougat_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fimage_processing_nougat_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -63,14 +63,11 @@ class NougatFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n             Whether to resize the image using thumbnail method.\n     do_align_long_axis (`bool`, *optional*, defaults to `False`):\n             Whether to align the long axis of the image with the long axis of `size` by rotating by 90 degrees.\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-            Whether to pad the images to the largest image size in the batch.\n     \"\"\"\n \n     do_crop_margin: Optional[bool]\n     do_thumbnail: Optional[bool]\n     do_align_long_axis: Optional[bool]\n-    do_pad: Optional[bool]\n \n \n @auto_docstring"
        },
        {
            "sha": "10869f50f622655b519c15549f3b96820dc5a3a6",
            "filename": "src/transformers/models/oneformer/image_processing_oneformer_fast.py",
            "status": "modified",
            "additions": 3,
            "deletions": 18,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fimage_processing_oneformer_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -530,24 +530,9 @@ def pad(\n         Returns:\n             `BatchFeature`: Padded images and optional pixel masks.\n         \"\"\"\n-        pad_size = get_max_height_width(images)\n-\n-        padded_images = []\n-        pixel_masks = []\n-\n-        for image in images:\n-            padded_image = self._pad_image_fast(\n-                image=image,\n-                output_size=pad_size,\n-                constant_values=0,\n-            )\n-            padded_images.append(padded_image)\n-\n-            if return_pixel_mask:\n-                input_height, input_width = image.shape[1], image.shape[2]\n-                mask = torch.zeros(pad_size, dtype=torch.int64, device=image.device)\n-                mask[:input_height, :input_width] = 1\n-                pixel_masks.append(mask)\n+        outputs = super().pad(images, return_mask=return_pixel_mask)\n+        padded_images = outputs[0] if return_pixel_mask else outputs\n+        pixel_masks = outputs[1] if return_pixel_mask else None\n \n         if return_tensors:\n             padded_images = torch.stack(padded_images, dim=0)"
        },
        {
            "sha": "f12a9c70ee57e15e8639314b655c942148d94e24",
            "filename": "src/transformers/models/ovis2/image_processing_ovis2_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fimage_processing_ovis2_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -202,6 +202,7 @@ def _preprocess(\n         image_std: Optional[Union[float, list[float]]],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         if crop_to_patches and max_patches > 1:\n             grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)"
        },
        {
            "sha": "926da9b27ffcdae4a183cb748ed8a2fd2141233f",
            "filename": "src/transformers/models/owlv2/image_processing_owlv2_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fimage_processing_owlv2_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -60,14 +60,7 @@\n     from .image_processing_owlv2 import _scale_boxes, box_iou\n \n \n-class Owlv2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-        method. If `True`, padding will be applied to the bottom and right of the image with grey pixels.\n-    \"\"\"\n-\n-    do_pad: Optional[bool]\n+class Owlv2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs): ...\n \n \n @auto_docstring\n@@ -289,7 +282,12 @@ def pad(\n         images: list[\"torch.Tensor\"],\n         disable_grouping: Optional[bool],\n         constant_value: float = 0.5,\n+        **kwargs,\n     ) -> list[\"torch.Tensor\"]:\n+        \"\"\"\n+        Unlike the Base class `self.pad` where all images are padded to the maximum image size,\n+        Owlv2 pads an image to square.\n+        \"\"\"\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n@@ -389,7 +387,7 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n         if do_pad:\n-            processed_images = self.pad(processed_images, disable_grouping=disable_grouping)\n+            processed_images = self.pad(processed_images, constant_value=0.5, disable_grouping=disable_grouping)\n \n         grouped_images, grouped_images_index = group_images_by_shape(\n             processed_images, disable_grouping=disable_grouping"
        },
        {
            "sha": "7fe4d75ee9ead4e8154228606ac5b8ab0c61edd5",
            "filename": "src/transformers/models/owlv2/modular_owlv2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -52,14 +52,7 @@\n     from torchvision.transforms import functional as F\n \n \n-class Owlv2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n-    r\"\"\"\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-        method. If `True`, padding will be applied to the bottom and right of the image with grey pixels.\n-    \"\"\"\n-\n-    do_pad: Optional[bool]\n+class Owlv2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs): ...\n \n \n @auto_docstring\n@@ -102,7 +95,12 @@ def pad(\n         images: list[\"torch.Tensor\"],\n         disable_grouping: Optional[bool],\n         constant_value: float = 0.5,\n+        **kwargs,\n     ) -> list[\"torch.Tensor\"]:\n+        \"\"\"\n+        Unlike the Base class `self.pad` where all images are padded to the maximum image size,\n+        Owlv2 pads an image to square.\n+        \"\"\"\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n@@ -202,7 +200,7 @@ def _preprocess(\n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n \n         if do_pad:\n-            processed_images = self.pad(processed_images, disable_grouping=disable_grouping)\n+            processed_images = self.pad(processed_images, constant_value=0.5, disable_grouping=disable_grouping)\n \n         grouped_images, grouped_images_index = group_images_by_shape(\n             processed_images, disable_grouping=disable_grouping"
        },
        {
            "sha": "5854056270233e51799e6c16ad75f8330cbed4e1",
            "filename": "src/transformers/models/pixtral/image_processing_pixtral_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fimage_processing_pixtral_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -162,6 +162,7 @@ def _preprocess(\n         image_std: Optional[Union[float, list[float]]],\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n     ) -> BatchFeature:\n         patch_size = get_size_dict(patch_size, default_to_square=True)\n         patch_size = SizeDict(**patch_size)"
        },
        {
            "sha": "4f0f68240f9a1ebe52ca016192dd0ab593390656",
            "filename": "src/transformers/models/prompt_depth_anything/image_processing_prompt_depth_anything.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fimage_processing_prompt_depth_anything.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -378,8 +378,6 @@ def preprocess(\n             do_normalize=do_normalize,\n             image_mean=image_mean,\n             image_std=image_std,\n-            do_pad=do_pad,\n-            size_divisibility=size_divisor,\n             do_resize=do_resize,\n             size=size,\n             resample=resample,"
        },
        {
            "sha": "eefc45bf9f9a78e9906f78a8e34e4124a2de5e17",
            "filename": "src/transformers/models/rt_detr/image_processing_rt_detr_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fimage_processing_rt_detr_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -59,23 +59,12 @@ class RTDetrFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         Controls whether to convert the annotations to the format expected by the RT_DETR model. Converts the\n         bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n         Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-        If `pad_size` is provided, the image will be padded to the specified dimensions.\n-        Otherwise, the image will be padded to the maximum height and width of the batch.\n-    pad_size (`dict[str, int]`, *optional*):\n-        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-        height and width in the batch.\n     return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n         Whether to return segmentation masks.\n     \"\"\"\n \n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[dict[str, int]]\n     return_segmentation_masks: Optional[bool]\n \n \n@@ -424,7 +413,7 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n-        pad_size: Optional[dict[str, int]],\n+        pad_size: Optional[SizeDict],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n@@ -483,7 +472,7 @@ def _preprocess(\n         if do_pad:\n             # depends on all resized image shapes so we need another loop\n             if pad_size is not None:\n-                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+                padded_size = (pad_size.height, pad_size.width)\n             else:\n                 padded_size = get_max_height_width(images)\n "
        },
        {
            "sha": "938f070d3672c6e647fbe8623521550f1fa10cd0",
            "filename": "src/transformers/models/rt_detr/modular_rt_detr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -175,7 +175,7 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n-        pad_size: Optional[dict[str, int]],\n+        pad_size: Optional[SizeDict],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n@@ -234,7 +234,7 @@ def _preprocess(\n         if do_pad:\n             # depends on all resized image shapes so we need another loop\n             if pad_size is not None:\n-                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+                padded_size = (pad_size.height, pad_size.width)\n             else:\n                 padded_size = get_max_height_width(images)\n "
        },
        {
            "sha": "c9b54f561fb6e4342038176a0d9d9d8264664510",
            "filename": "src/transformers/models/sam/image_processing_sam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -516,8 +516,6 @@ def preprocess(\n             do_normalize=do_normalize,\n             image_mean=image_mean,\n             image_std=image_std,\n-            do_pad=do_pad,\n-            size_divisibility=pad_size,  # Here _preprocess needs do_pad and pad_size.\n             do_resize=do_resize,\n             size=size,\n             resample=resample,"
        },
        {
            "sha": "1bfb6adf52343a4c16753ba1b97148fb2822a080",
            "filename": "src/transformers/models/sam/image_processing_sam_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 66,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fimage_processing_sam_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -26,8 +26,6 @@\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     DefaultFastImageProcessorKwargs,\n-    group_images_by_shape,\n-    reorder_images,\n )\n from ...image_utils import (\n     IMAGENET_DEFAULT_MEAN,\n@@ -40,7 +38,6 @@\n )\n from ...processing_utils import Unpack\n from ...utils import (\n-    TensorType,\n     auto_docstring,\n     is_torch_available,\n     is_torchvision_available,\n@@ -62,12 +59,6 @@\n \n class SamFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     r\"\"\"\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-    pad_size (`dict[str, int]`, *optional*):\n-        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-        provided for preprocessing.\n     mask_size (`dict[str, int]`, *optional*):\n         The size `{\"longest_edge\": int}` to resize the segmentation maps to.\n     mask_pad_size (`dict[str, int]`, *optional*):\n@@ -76,8 +67,6 @@ class SamFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n \n     mask_size: Optional[dict[str, int]]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[dict[str, int]]\n     mask_pad_size: Optional[dict[str, int]]\n \n \n@@ -102,15 +91,6 @@ class SamImageProcessorFast(BaseImageProcessorFast):\n     def __init__(self, **kwargs: Unpack[SamFastImageProcessorKwargs]):\n         super().__init__(**kwargs)\n \n-    def pad_image(self, images: \"torch.Tensor\", pad_size: SizeDict):\n-        \"\"\"Pad images to the specified size.\"\"\"\n-        output_height, output_width = pad_size.height, pad_size.width\n-        input_height, input_width = images.shape[-2:]\n-        pad_width = output_width - input_width\n-        pad_height = output_height - input_height\n-        padding = (0, 0, pad_width, pad_height)\n-        return F_t.pad(images, padding)\n-\n     def _get_preprocess_shape(self, old_shape: tuple[int, int], longest_edge: int):\n         \"\"\"\n         Compute the output size given input size and target long side length.\n@@ -231,7 +211,7 @@ def _preprocess_image_like_inputs(\n         )\n         original_sizes = [image.shape[-2:] for image in images]\n         images_kwargs = kwargs.copy()\n-        pixel_values = self._preprocess(images, **images_kwargs)\n+        pixel_values = self._preprocess(images, **images_kwargs)[\"pixel_values\"]\n         reshaped_input_sizes = [image.shape[-2:] for image in images]\n         data = {\n             \"pixel_values\": pixel_values,\n@@ -262,54 +242,10 @@ def _preprocess_image_like_inputs(\n             processed_segmentation_maps = self._preprocess(\n                 images=processed_segmentation_maps, **segmentation_maps_kwargs\n             )\n-            data[\"labels\"] = processed_segmentation_maps.squeeze(1).to(torch.int64)\n+            data[\"labels\"] = processed_segmentation_maps[\"pixel_values\"].squeeze(1).to(torch.int64)\n \n         return BatchFeature(data=data, tensor_type=kwargs[\"return_tensors\"])\n \n-    def _preprocess(\n-        self,\n-        images: list[\"torch.Tensor\"],\n-        do_resize: bool,\n-        size: SizeDict,\n-        interpolation: Optional[\"F_t.InterpolationMode\"],\n-        do_rescale: bool,\n-        rescale_factor: float,\n-        do_normalize: bool,\n-        image_mean: Optional[Union[float, list[float]]],\n-        image_std: Optional[Union[float, list[float]]],\n-        do_pad: bool,\n-        pad_size: SizeDict,\n-        disable_grouping: Optional[bool],\n-        return_tensors: Optional[Union[str, TensorType]],\n-        **kwargs,\n-    ) -> Union[\"torch.Tensor\", list[\"torch.Tensor\"]]:\n-        # Group images by size for batched resizing\n-        grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n-        resized_images_grouped = {}\n-        for shape, stacked_images in grouped_images.items():\n-            if do_resize:\n-                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n-            resized_images_grouped[shape] = stacked_images\n-        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n-\n-        # Group images by size for further processing\n-        # Needed in case do_resize is False, or resize returns images with different sizes\n-        grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n-        processed_images_grouped = {}\n-        for shape, stacked_images in grouped_images.items():\n-            # Fused rescale and normalize\n-            stacked_images = self.rescale_and_normalize(\n-                stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n-            )\n-            if do_pad:\n-                stacked_images = self.pad_image(stacked_images, pad_size)\n-            processed_images_grouped[shape] = stacked_images\n-\n-        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n-\n-        return processed_images\n-\n     def generate_crop_boxes(\n         self,\n         image: \"torch.Tensor\","
        },
        {
            "sha": "8cb5381f09770f7e78af9a3a119f0ea7955b3271",
            "filename": "src/transformers/models/sam2/image_processing_sam2_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fimage_processing_sam2_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -504,14 +504,6 @@ def _preprocess_image_like_inputs(\n \n         return BatchFeature(data=data, tensor_type=kwargs[\"return_tensors\"])\n \n-    def _preprocess(\n-        self,\n-        images: list[\"torch.Tensor\"],\n-        return_tensors: Optional[Union[str, TensorType]],\n-        **kwargs,\n-    ) -> \"torch.Tensor\":\n-        return super()._preprocess(images, return_tensors=return_tensors, **kwargs).pixel_values\n-\n     def generate_crop_boxes(\n         self,\n         image: \"torch.Tensor\",\n@@ -713,6 +705,17 @@ def post_process_for_mask_generation(self, all_masks, all_scores, all_boxes, cro\n         \"\"\"\n         return _post_process_for_mask_generation(all_masks, all_scores, all_boxes, crops_nms_thresh)\n \n+    def pad_image(self):\n+        raise NotImplementedError(\"No pad_image for SAM 2.\")\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> \"torch.Tensor\":\n+        return super()._preprocess(images, return_tensors=return_tensors, **kwargs).pixel_values\n+\n     def _apply_non_overlapping_constraints(self, pred_masks: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Apply non-overlapping constraints to the object scores in pred_masks. Here we"
        },
        {
            "sha": "4e24bc2795437cfa3d4c41a0d024e5f4ac82d681",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -52,9 +52,6 @@\n \n class SmolVLMFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n-    do_pad (`bool`, *optional*):\n-        Whether to pad the image. If `True`, will pad the patch dimension of the images in the batch to the largest\n-        number of patches in the batch. Padding will be applied to the bottom and right with zeros.\n     do_image_splitting (`bool`, *optional*, defaults to `True`):\n         Whether to split the image into sub-images concatenated with the original image. They are split into patches\n         such that each patch has a size of `max_image_size[\"height\"]` x `max_image_size[\"width\"]`.\n@@ -64,7 +61,6 @@ class SmolVLMFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         Whether to return the row and column information of the images.\n     \"\"\"\n \n-    do_pad: Optional[bool]\n     do_image_splitting: Optional[bool]\n     max_image_size: Optional[dict[str, int]]\n     return_row_col_info: Optional[bool]"
        },
        {
            "sha": "eda3bdb1c8115747c81bac8ee7124cc2135342a7",
            "filename": "src/transformers/models/smolvlm/video_processing_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fvideo_processing_smolvlm.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -98,7 +98,8 @@ def get_resize_output_image_size(\n \n \n class SmolVLMVideoProcessorInitKwargs(VideosKwargs):\n-    max_image_size: dict[str, int] = None\n+    max_image_size: Optional[dict[str, int]]\n+    do_pad: Optional[bool]\n \n \n class SmolVLMVideoProcessor(BaseVideoProcessor):"
        },
        {
            "sha": "76c5e907da1cd7141dc52eebb4d7b19adc2f7fbb",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr.py",
            "status": "modified",
            "additions": 23,
            "deletions": 8,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -31,6 +31,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n \n \n logger = logging.get_logger(__name__)\n@@ -56,15 +57,30 @@ def __init__(\n         do_rescale: bool = True,\n         rescale_factor: Union[int, float] = 1 / 255,\n         do_pad: bool = True,\n-        pad_size: int = 8,\n+        size_divisor: int = 8,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n \n         self.do_rescale = do_rescale\n         self.rescale_factor = rescale_factor\n         self.do_pad = do_pad\n-        self.pad_size = pad_size\n+        pad_size = kwargs.get(\"pad_size\")\n+        self.size_divisor = size_divisor if size_divisor is not None else pad_size\n+\n+    @property\n+    def pad_size(self):\n+        logger.warning(\n+            \"`self.pad_size` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\",\n+        )\n+        return self.size_divisor\n+\n+    @pad_size.setter\n+    def pad_size(self, value):\n+        logger.warning(\n+            \"`self.pad_size` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\",\n+        )\n+        self.size_divisor = value\n \n     def pad(\n         self,\n@@ -108,13 +124,14 @@ def pad(\n         )\n \n     @filter_out_non_signature_kwargs()\n+    @deprecate_kwarg(\"pad_size\", version=\"v5\", new_name=\"size_divisor\")\n     def preprocess(\n         self,\n         images: ImageInput,\n         do_rescale: Optional[bool] = None,\n         rescale_factor: Optional[float] = None,\n         do_pad: Optional[bool] = None,\n-        pad_size: Optional[int] = None,\n+        size_divisor: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -132,7 +149,7 @@ def preprocess(\n                 Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n             do_pad (`bool`, *optional*, defaults to `True`):\n                 Whether to pad the image to make the height and width divisible by `window_size`.\n-            pad_size (`int`, *optional*, defaults to 32):\n+            size_divisor (`int`, *optional*, defaults to 32):\n                 The size of the sliding window for the local attention.\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n@@ -157,7 +174,7 @@ def preprocess(\n         do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n         rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n         do_pad = do_pad if do_pad is not None else self.do_pad\n-        pad_size = pad_size if pad_size is not None else self.pad_size\n+        size_divisor = size_divisor if size_divisor is not None else self.size_divisor\n \n         images = make_flat_list_of_images(images)\n \n@@ -169,8 +186,6 @@ def preprocess(\n         validate_preprocess_arguments(\n             do_rescale=do_rescale,\n             rescale_factor=rescale_factor,\n-            do_pad=do_pad,\n-            size_divisibility=pad_size,  # Here the pad function simply requires pad_size.\n         )\n \n         # All transformations expect numpy arrays.\n@@ -193,7 +208,7 @@ def preprocess(\n             ]\n \n         if do_pad:\n-            images = [self.pad(image, size=pad_size, input_data_format=input_data_format) for image in images]\n+            images = [self.pad(image, size=size_divisor, input_data_format=input_data_format) for image in images]\n \n         images = [\n             to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format) for image in images"
        },
        {
            "sha": "f99ab99274f5c4b6d29737a8bf53c8de8318ef3e",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr_fast.py",
            "status": "modified",
            "additions": 34,
            "deletions": 14,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -31,9 +31,13 @@\n     is_torch_available,\n     is_torchvision_available,\n     is_torchvision_v2_available,\n+    logging,\n )\n+from ...utils.deprecation import deprecate_kwarg\n \n \n+logger = logging.get_logger(__name__)\n+\n if is_torch_available():\n     import torch\n \n@@ -46,60 +50,76 @@\n \n class Swin2SRFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Whether to pad the image to make the height and width divisible by `window_size`.\n-    pad_size (`int`, *optional*, defaults to `8`):\n-        The size of the sliding window for the local attention.\n+    size_divisor (`int`, *optional*, defaults to `8`):\n+        The size of the sliding window for the local attention. It will be used to pad the image\n+        to the size divisible by `size_divisor`\n     \"\"\"\n \n-    do_pad: Optional[bool]\n-    pad_size: Optional[int]\n+    size_divisor: Optional[int]\n \n \n @auto_docstring\n class Swin2SRImageProcessorFast(BaseImageProcessorFast):\n     do_rescale = True\n     rescale_factor = 1 / 255\n     do_pad = True\n-    pad_size = 8\n+    size_divisor = 8\n     valid_kwargs = Swin2SRFastImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[Swin2SRFastImageProcessorKwargs]):\n+        pad_size = kwargs.pop(\"pad_size\", None)\n+        kwargs.setdefault(\"size_divisor\", pad_size)\n         super().__init__(**kwargs)\n \n+    @property\n+    def pad_size(self):\n+        logger.warning(\n+            \"`self.pad_size` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\",\n+        )\n+        return self.size_divisor\n+\n+    @pad_size.setter\n+    def pad_size(self, value):\n+        logger.warning(\n+            \"`self.pad_size` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\",\n+        )\n+        self.size_divisor = value\n+\n     def preprocess(self, images: ImageInput, **kwargs: Unpack[Swin2SRFastImageProcessorKwargs]) -> BatchFeature:\n         return super().preprocess(images, **kwargs)\n \n-    def pad(self, images: \"torch.Tensor\", size: int) -> \"torch.Tensor\":\n+    @deprecate_kwarg(\"size\", version=\"v5\", new_name=\"size_divisor\")\n+    def pad(self, images: \"torch.Tensor\", size_divisor: int) -> \"torch.Tensor\":\n         \"\"\"\n-        Pad an image to make the height and width divisible by `size`.\n+        Pad an image to make the height and width divisible by `size_divisor`.\n \n         Args:\n             images (`torch.Tensor`):\n                 Images to pad.\n-            size (`int`):\n+            size_divisor (`int`):\n                 The size to make the height and width divisible by.\n \n         Returns:\n             `torch.Tensor`: The padded images.\n         \"\"\"\n         height, width = get_image_size(images, ChannelDimension.FIRST)\n-        pad_height = (height // size + 1) * size - height\n-        pad_width = (width // size + 1) * size - width\n+        pad_height = (height // size_divisor + 1) * size_divisor - height\n+        pad_width = (width // size_divisor + 1) * size_divisor - width\n \n         return F.pad(\n             images,\n             (0, 0, pad_width, pad_height),\n             padding_mode=\"symmetric\",\n         )\n \n+    @deprecate_kwarg(\"pad_size\", version=\"v5\", new_name=\"size_divisor\")\n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n         do_rescale: bool,\n         rescale_factor: float,\n         do_pad: bool,\n-        pad_size: int,\n+        size_divisor: int,\n         disable_grouping: Optional[bool],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n@@ -110,7 +130,7 @@ def _preprocess(\n             if do_rescale:\n                 stacked_images = self.rescale(stacked_images, scale=rescale_factor)\n             if do_pad:\n-                stacked_images = self.pad(stacked_images, size=pad_size)\n+                stacked_images = self.pad(stacked_images, size_divisor=size_divisor)\n             processed_image_grouped[shape] = stacked_images\n         processed_images = reorder_images(processed_image_grouped, grouped_images_index)\n         processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images"
        },
        {
            "sha": "d3f698873d55c2eef94c9b0894bce4c5f76ea869",
            "filename": "src/transformers/models/tvp/image_processing_tvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -294,8 +294,6 @@ def _preprocess_image(\n             do_normalize=do_normalize,\n             image_mean=image_mean,\n             image_std=image_std,\n-            do_pad=do_pad,\n-            size_divisibility=pad_size,  # here the pad() method simply requires the pad_size argument.\n             do_center_crop=do_center_crop,\n             crop_size=crop_size,\n             do_resize=do_resize,"
        },
        {
            "sha": "b96e4991f6193a79fbb074f5f9ee6c4dadc04224",
            "filename": "src/transformers/models/tvp/image_processing_tvp_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 49,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fimage_processing_tvp_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -16,7 +16,7 @@\n \n from typing import Optional, Union\n \n-from ...image_processing_utils import BatchFeature, get_size_dict\n+from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import (\n     BaseImageProcessorFast,\n     DefaultFastImageProcessorKwargs,\n@@ -55,19 +55,13 @@ class TvpFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     r\"\"\"\n     do_flip_channel_order (`bool`, *optional*):\n         Whether to flip the channel order of the image from RGB to BGR.\n-    do_pad (`bool`, *optional*):\n-        Whether to pad the image.\n-    pad_size (`Dict[str, int]` or `SizeDict`, *optional*):\n-        Size dictionary specifying the desired height and width for padding.\n     constant_values (`float` or `List[float]`, *optional*):\n         Value used to fill the padding area when `pad_mode` is `'constant'`.\n     pad_mode (`str`, *optional*):\n         Padding mode to use  `'constant'`, `'edge'`, `'reflect'`, or `'symmetric'`.\n     \"\"\"\n \n     do_flip_channel_order: Optional[bool]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[SizeDict]\n     constant_values: Optional[Union[float, list[float]]]\n     pad_mode: Optional[str]\n \n@@ -103,21 +97,6 @@ def preprocess(\n     ) -> BatchFeature:\n         return super().preprocess(videos, **kwargs)\n \n-    def _further_process_kwargs(\n-        self,\n-        pad_size: Optional[SizeDict] = None,\n-        **kwargs,\n-    ) -> dict:\n-        \"\"\"\n-        Update kwargs that need further processing before being validated\n-        Can be overridden by subclasses to customize the processing of kwargs.\n-        \"\"\"\n-        if pad_size is not None:\n-            pad_size = SizeDict(**get_size_dict(pad_size, param_name=\"pad_size\"))\n-        kwargs[\"pad_size\"] = pad_size\n-\n-        return super()._further_process_kwargs(**kwargs)\n-\n     def _prepare_images_structure(\n         self,\n         images: ImageInput,\n@@ -135,31 +114,6 @@ def _prepare_images_structure(\n         \"\"\"\n         return make_nested_list_of_images(images, **kwargs)\n \n-    def _pad_frames(\n-        self,\n-        frames: \"torch.Tensor\",\n-        pad_size: Union[SizeDict, dict],\n-        constant_values: Union[float, list[float]],\n-        pad_mode: str,\n-    ) -> \"torch.Tensor\":\n-        \"\"\"Pad frames to the specified size.\"\"\"\n-        height, width = pad_size.height, pad_size.width\n-\n-        if frames.shape[-2:] == (height, width):\n-            return frames\n-\n-        # Calculate padding\n-        current_height, current_width = frames.shape[-2:]\n-        pad_bottom = height - current_height\n-        pad_right = width - current_width\n-\n-        if pad_bottom < 0 or pad_right < 0:\n-            raise ValueError(\"The padding size must be greater than frame size\")\n-\n-        # Apply padding\n-        padding = [0, 0, pad_right, pad_bottom]  # [left, top, right, bottom]\n-        return F.pad(frames, padding, fill=constant_values, padding_mode=pad_mode)\n-\n     def resize(\n         self,\n         image: \"torch.Tensor\",\n@@ -238,7 +192,7 @@ def _preprocess(\n         do_rescale: bool,\n         rescale_factor: float,\n         do_pad: bool,\n-        pad_size: Union[SizeDict, dict],\n+        pad_size: SizeDict,\n         constant_values: Union[float, list[float]],\n         pad_mode: str,\n         do_normalize: bool,\n@@ -275,7 +229,8 @@ def _preprocess(\n \n             # Pad if needed\n             if do_pad:\n-                stacked_frames = self._pad_frames(stacked_frames, pad_size, constant_values, pad_mode)\n+                stacked_frames = self.pad(stacked_frames, pad_size, fill_value=constant_values, pad_mode=pad_mode)\n+                stacked_frames = torch.stack(stacked_frames, dim=0)\n \n             # Flip channel order if needed (RGB to BGR)\n             if do_flip_channel_order:"
        },
        {
            "sha": "1c169994ba3f8a358e7126b8e1c791c6cc8513ac",
            "filename": "src/transformers/models/vilt/image_processing_vilt_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fimage_processing_vilt_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -51,16 +51,12 @@\n class ViltFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n     Args:\n-        do_pad (`bool`, *optional*, defaults to `True`):\n-            Whether to pad the image. If `True`, will pad the images in the batch to the largest height and width\n-            in the batch. Padding will be applied to the bottom and right with zeros.\n         size_divisor (`int`, *optional*, defaults to 32):\n             The size to make the height and width divisible by.\n         rescale_factor (`float`, *optional*, defaults to 1/255):\n             The factor to rescale the image by.\n     \"\"\"\n \n-    do_pad: Optional[bool]\n     size_divisor: Optional[int]\n     rescale_factor: Optional[float]\n "
        },
        {
            "sha": "f4f9fc9a746d750da64753642cb8779ce2698dc3",
            "filename": "src/transformers/models/vilt/processing_vilt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -17,11 +17,17 @@\n \"\"\"\n \n import warnings\n+from typing import Optional\n \n-from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin\n+\n+\n+class ViltImagesKwargs(ImagesKwargs):\n+    size_divisor: Optional[int]\n \n \n class ViltProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: ViltImagesKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"add_special_tokens\": True,"
        },
        {
            "sha": "6e65a634d23db50ca64f7b44ccd59f0805060026",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte.py",
            "status": "modified",
            "additions": 25,
            "deletions": 10,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -34,6 +34,7 @@\n     validate_preprocess_arguments,\n )\n from ...utils import TensorType, filter_out_non_signature_kwargs, logging\n+from ...utils.deprecation import deprecate_kwarg\n \n \n logger = logging.get_logger(__name__)\n@@ -60,9 +61,9 @@ class VitMatteImageProcessor(BaseImageProcessor):\n             Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n             number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n         do_pad (`bool`, *optional*, defaults to `True`):\n-            Whether to pad the image to make the width and height divisible by `size_divisibility`. Can be overridden\n+            Whether to pad the image to make the width and height divisible by `size_divisor`. Can be overridden\n             by the `do_pad` parameter in the `preprocess` method.\n-        size_divisibility (`int`, *optional*, defaults to 32):\n+        size_divisor (`int`, *optional*, defaults to 32):\n             The width and height of the image will be padded to be divisible by this number.\n     \"\"\"\n \n@@ -76,7 +77,7 @@ def __init__(\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n         do_pad: bool = True,\n-        size_divisibility: int = 32,\n+        size_divisor: int = 32,\n         **kwargs,\n     ) -> None:\n         super().__init__(**kwargs)\n@@ -86,7 +87,22 @@ def __init__(\n         self.rescale_factor = rescale_factor\n         self.image_mean = image_mean if image_mean is not None else IMAGENET_STANDARD_MEAN\n         self.image_std = image_std if image_std is not None else IMAGENET_STANDARD_STD\n-        self.size_divisibility = size_divisibility\n+        size_divisibility = kwargs.get(\"size_divisibility\")\n+        self.size_divisor = size_divisibility if size_divisibility is not None else size_divisor\n+\n+    @property\n+    def size_divisibility(self):\n+        logger.warning(\n+            \"`self.size_divisibility` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\"\n+        )\n+        return self.size_divisor\n+\n+    @size_divisibility.setter\n+    def size_divisibility(self, value):\n+        logger.warning(\n+            \"`self.size_divisibility` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\"\n+        )\n+        self.size_divisor = value\n \n     def pad_image(\n         self,\n@@ -130,6 +146,7 @@ def pad_image(\n         return image\n \n     @filter_out_non_signature_kwargs()\n+    @deprecate_kwarg(\"size_divisibility\", version=\"v5\", new_name=\"size_divisor\")\n     def preprocess(\n         self,\n         images: ImageInput,\n@@ -140,7 +157,7 @@ def preprocess(\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n         do_pad: Optional[bool] = None,\n-        size_divisibility: Optional[int] = None,\n+        size_divisor: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         data_format: Union[str, ChannelDimension] = ChannelDimension.FIRST,\n         input_data_format: Optional[Union[str, ChannelDimension]] = None,\n@@ -166,7 +183,7 @@ def preprocess(\n                 Image standard deviation to use if `do_normalize` is set to `True`.\n             do_pad (`bool`, *optional*, defaults to `self.do_pad`):\n                 Whether to pad the image.\n-            size_divisibility (`int`, *optional*, defaults to `self.size_divisibility`):\n+            size_divisor (`int`, *optional*, defaults to `self.size_divisor`):\n                 The size divisibility to pad the image to if `do_pad` is set to `True`.\n             return_tensors (`str` or `TensorType`, *optional*):\n                 The type of tensors to return. Can be one of:\n@@ -193,7 +210,7 @@ def preprocess(\n         rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n         image_mean = image_mean if image_mean is not None else self.image_mean\n         image_std = image_std if image_std is not None else self.image_std\n-        size_divisibility = size_divisibility if size_divisibility is not None else self.size_divisibility\n+        size_divisor = size_divisor if size_divisor is not None else self.size_divisor\n \n         images = make_flat_list_of_images(images)\n         trimaps = make_flat_list_of_images(trimaps, expected_ndims=2)\n@@ -215,8 +232,6 @@ def preprocess(\n             do_normalize=do_normalize,\n             image_mean=image_mean,\n             image_std=image_std,\n-            do_pad=do_pad,\n-            size_divisibility=size_divisibility,\n         )\n \n         # All transformations expect numpy arrays.\n@@ -258,7 +273,7 @@ def preprocess(\n \n         if do_pad:\n             images = [\n-                self.pad_image(image, size_divisibility=size_divisibility, input_data_format=input_data_format)\n+                self.pad_image(image, size_divisibility=size_divisor, input_data_format=input_data_format)\n                 for image in images\n             ]\n "
        },
        {
            "sha": "014a6939af5c9901029b6c37a503eb4043ebd2d4",
            "filename": "src/transformers/models/vitmatte/image_processing_vitmatte_fast.py",
            "status": "modified",
            "additions": 21,
            "deletions": 10,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fimage_processing_vitmatte_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -57,15 +57,11 @@\n \n class VitMatteFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Whether to pad the image to make the width and height divisible by `size_divisibility`. Can be overridden\n-        by the `do_pad` parameter in the `preprocess` method.\n-    size_divisibility (`int`, *optional*, defaults to 32):\n+    size_divisor (`int`, *optional*, defaults to 32):\n         The width and height of the image will be padded to be divisible by this number.\n     \"\"\"\n \n-    do_pad: Optional[bool]\n-    size_divisibility: int\n+    size_divisor: Optional[int]\n \n \n @auto_docstring\n@@ -76,12 +72,28 @@ class VitMatteImageProcessorFast(BaseImageProcessorFast):\n     image_mean: Optional[Union[float, list[float]]] = IMAGENET_STANDARD_MEAN\n     image_std: Optional[Union[float, list[float]]] = IMAGENET_STANDARD_STD\n     do_pad: bool = True\n-    size_divisibility: int = 32\n+    size_divisor: int = 32\n     valid_kwargs = VitMatteFastImageProcessorKwargs\n \n     def __init__(self, **kwargs: Unpack[VitMatteFastImageProcessorKwargs]) -> None:\n+        size_divisibility = kwargs.pop(\"size_divisibility\", None)\n+        kwargs.setdefault(\"size_divisor\", size_divisibility)\n         super().__init__(**kwargs)\n \n+    @property\n+    def size_divisibility(self):\n+        logger.warning(\n+            \"`self.size_divisibility` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\"\n+        )\n+        return self.size_divisor\n+\n+    @size_divisibility.setter\n+    def size_divisibility(self, value):\n+        logger.warning(\n+            \"`self.size_divisibility` attribute is deprecated and will be removed in v5. Use `self.size_divisor` instead\"\n+        )\n+        self.size_divisor = value\n+\n     def _pad_image(\n         self,\n         images: \"torch.tensor\",\n@@ -150,10 +162,9 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]] = None,\n         image_std: Optional[Union[float, list[float]]] = None,\n         do_pad: Optional[bool] = None,\n-        size_divisibility: Optional[int] = None,\n+        size_divisor: Optional[int] = None,\n         disable_grouping: Optional[bool] = None,\n         return_tensors: Optional[Union[str, TensorType]] = None,\n-        **kwargs,\n     ) -> BatchFeature:\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         grouped_trimaps, grouped_trimaps_index = group_images_by_shape(trimaps, disable_grouping=disable_grouping)\n@@ -170,7 +181,7 @@ def _preprocess(\n             )\n             stacked_images = torch.cat([stacked_images, stacked_trimaps], dim=1)\n             if do_pad:\n-                stacked_images = self._pad_image(stacked_images, self.size_divisibility)\n+                stacked_images = self._pad_image(stacked_images, size_divisor)\n             processed_images_grouped[shape] = stacked_images\n \n         processed_images = reorder_images(processed_images_grouped, grouped_images_index)"
        },
        {
            "sha": "81fb0b008e0d0b884402ddb555fa10a91963ab2c",
            "filename": "src/transformers/models/yolos/image_processing_yolos_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fimage_processing_yolos_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -64,23 +64,12 @@ class YolosFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         Controls whether to convert the annotations to the format expected by the YOLOS model. Converts the\n         bounding boxes to the format `(center_x, center_y, width, height)` and in the range `[0, 1]`.\n         Can be overridden by the `do_convert_annotations` parameter in the `preprocess` method.\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Controls whether to pad the image. Can be overridden by the `do_pad` parameter in the `preprocess`\n-        method. If `True`, padding will be applied to the bottom and right of the image with zeros.\n-        If `pad_size` is provided, the image will be padded to the specified dimensions.\n-        Otherwise, the image will be padded to the maximum height and width of the batch.\n-    pad_size (`dict[str, int]`, *optional*):\n-        The size `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n-        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n-        height and width in the batch.\n     return_segmentation_masks (`bool`, *optional*, defaults to `False`):\n         Whether to return segmentation masks.\n     \"\"\"\n \n     format: Optional[Union[str, AnnotationFormat]]\n     do_convert_annotations: Optional[bool]\n-    do_pad: Optional[bool]\n-    pad_size: Optional[dict[str, int]]\n     return_segmentation_masks: Optional[bool]\n \n \n@@ -668,7 +657,7 @@ def _preprocess(\n         image_mean: Optional[Union[float, list[float]]],\n         image_std: Optional[Union[float, list[float]]],\n         do_pad: bool,\n-        pad_size: Optional[dict[str, int]],\n+        pad_size: Optional[SizeDict],\n         format: Optional[Union[str, AnnotationFormat]],\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n@@ -737,7 +726,7 @@ def _preprocess(\n         if do_pad:\n             # depends on all resized image shapes so we need another loop\n             if pad_size is not None:\n-                padded_size = (pad_size[\"height\"], pad_size[\"width\"])\n+                padded_size = (pad_size.height, pad_size.width)\n             else:\n                 padded_size = get_max_height_width(images)\n "
        },
        {
            "sha": "c89ec8b2ebf11b5c4e78ac9f8a346e36c69dce6c",
            "filename": "src/transformers/models/zoedepth/image_processing_zoedepth_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fimage_processing_zoedepth_fast.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -70,8 +70,6 @@\n \n class ZoeDepthFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n     \"\"\"\n-    do_pad (`bool`, *optional*, defaults to `True`):\n-        Whether to apply pad the input.\n     keep_aspect_ratio (`bool`, *optional*, defaults to `True`):\n         If `True`, the image is resized by choosing the smaller of the height and width scaling factors and using it\n         for both dimensions. This ensures that the image is scaled down as little as possible while still fitting\n@@ -85,7 +83,6 @@ class ZoeDepthFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n         Can be overridden by `ensure_multiple_of` in `preprocess`.\n     \"\"\"\n \n-    do_pad: Optional[bool]\n     keep_aspect_ratio: Optional[bool]\n     ensure_multiple_of: Optional[int]\n "
        },
        {
            "sha": "86cdb372034cbcef266ddf92ae7b37b2f4b9d1d3",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 9,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -168,8 +168,6 @@ class methods and docstrings.\n             Whether to resize the image.\n         size (`dict[str, int]`, *optional*):\n             Resize the shorter side of the input to `size[\"shortest_edge\"]`.\n-        size_divisor (`int`, *optional*):\n-            The size by which to make sure both the height and width can be divided.\n         crop_size (`dict[str, int]`, *optional*):\n             Desired output size when applying center-cropping.\n         resample (`PILImageResampling`, *optional*):\n@@ -200,7 +198,6 @@ class methods and docstrings.\n \n     do_resize: Optional[bool]\n     size: Optional[dict[str, int]]\n-    size_divisor: Optional[int]\n     crop_size: Optional[dict[str, int]]\n     resample: Optional[Union[\"PILImageResampling\", int]]\n     do_rescale: Optional[bool]\n@@ -229,8 +226,6 @@ class VideosKwargs(TypedDict, total=False):\n             Resize the shorter side of the input to `size[\"shortest_edge\"]`.\n         default_to_square (`bool`, *optional*, defaults to `self.default_to_square`):\n             Whether to default to a square when resizing, if size is an int.\n-        size_divisor (`int`, *optional*):\n-            The size by which to make sure both the height and width can be divided.\n         resample (`PILImageResampling`, *optional*):\n             Resampling filter to use if resizing the video.\n         do_rescale (`bool`, *optional*):\n@@ -243,8 +238,6 @@ class VideosKwargs(TypedDict, total=False):\n             Mean to use if normalizing the video.\n         image_std (`float` or `list[float]`, *optional*):\n             Standard deviation to use if normalizing the video.\n-        do_pad (`bool`, *optional*):\n-            Whether to pad the video to the `(max_height, max_width)` of the videos in the batch.\n         do_center_crop (`bool`, *optional*):\n             Whether to center crop the video.\n         do_sample_frames (`bool`, *optional*):\n@@ -268,15 +261,13 @@ class VideosKwargs(TypedDict, total=False):\n     do_convert_rgb: Optional[bool]\n     do_resize: Optional[bool]\n     size: Optional[dict[str, int]]\n-    size_divisor: Optional[int]\n     default_to_square: Optional[bool]\n     resample: Optional[\"PILImageResampling\"]\n     do_rescale: Optional[bool]\n     rescale_factor: Optional[float]\n     do_normalize: Optional[bool]\n     image_mean: Optional[Union[float, list[float]]]\n     image_std: Optional[Union[float, list[float]]]\n-    do_pad: Optional[bool]\n     do_center_crop: Optional[bool]\n     crop_size: Optional[dict[str, int]]\n     data_format: Optional[ChannelDimension]\n@@ -655,6 +646,18 @@ def to_dict(self, legacy_serialization=True) -> dict[str, Any]:\n         if \"chat_template\" in output:\n             del output[\"chat_template\"]\n \n+        def cast_array_to_list(dictionary):\n+            \"\"\"\n+            Numpy arrays are not serialiazable but can be in pre-processing dicts.\n+            This function casts arrays to list, recusring through the nested configs as well.\n+            \"\"\"\n+            for key, value in dictionary.items():\n+                if isinstance(value, np.ndarray):\n+                    dictionary[key] = value.tolist()\n+                elif isinstance(value, dict):\n+                    dictionary[key] = cast_array_to_list(value)\n+            return dictionary\n+\n         # Serialize attributes as a dict\n         output = {\n             k: v.to_dict() if isinstance(v, PushToHubMixin) else v\n@@ -667,6 +670,7 @@ def to_dict(self, legacy_serialization=True) -> dict[str, Any]:\n                 )  # remove `PushToHubMixin` objects\n             )\n         }\n+        output = cast_array_to_list(output)\n \n         # Special case, add `audio_tokenizer` dict which points to model weights and path\n         if not legacy_serialization and \"audio_tokenizer\" in output:"
        },
        {
            "sha": "0847859450ea19ce41787d38939ac5b1a9abbceb",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -131,6 +131,23 @@ class ImageProcessorArgs:\n         \"shape\": None,\n     }\n \n+    do_pad = {\n+        \"description\": \"\"\"\n+    Whether to pad the image. Padding is done either to the largest size in the batch\n+    or to a fixed square size per image. The exact padding strategy depends on the model.\n+    \"\"\",\n+        \"shape\": None,\n+    }\n+\n+    pad_size = {\n+        \"description\": \"\"\"\n+    The size in `{\"height\": int, \"width\" int}` to pad the images to. Must be larger than any image size\n+        provided for preprocessing. If `pad_size` is not provided, images will be padded to the largest\n+        height and width in the batch. Applied only when `do_pad=True.`\n+    \"\"\",\n+        \"shape\": None,\n+    }\n+\n     do_rescale = {\n         \"description\": \"\"\"\n     Whether to rescale the image."
        },
        {
            "sha": "9f6545ebe10ece592eb37ca9842160c9f6b2168d",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 8,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -95,8 +95,6 @@\n         do_center_crop (`bool`, *optional*, defaults to `self.do_center_crop`):\n             Whether to center crop the video to the specified `crop_size`. Can be overridden by `do_center_crop` in the\n             `preprocess` method.\n-        do_pad (`bool`, *optional*):\n-            Whether to pad the video to the `(max_height, max_width)` of the videos in the batch.\n         crop_size (`dict[str, int]` *optional*, defaults to `self.crop_size`):\n             Size of the output video after applying `center_crop`. Can be overridden by `crop_size` in the `preprocess`\n             method.\n@@ -164,7 +162,6 @@ class BaseVideoProcessor(BaseImageProcessorFast):\n     crop_size = None\n     do_resize = None\n     do_center_crop = None\n-    do_pad = None\n     do_rescale = None\n     rescale_factor = 1 / 255\n     do_normalize = None\n@@ -401,12 +398,10 @@ def _preprocess(\n         do_convert_rgb: bool,\n         do_resize: bool,\n         size: SizeDict,\n-        size_divisor: Optional[int],\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_center_crop: bool,\n         crop_size: SizeDict,\n         do_rescale: bool,\n-        do_pad: bool,\n         rescale_factor: float,\n         do_normalize: bool,\n         image_mean: Optional[Union[float, list[float]]],\n@@ -421,9 +416,7 @@ def _preprocess(\n             if do_convert_rgb:\n                 stacked_videos = self.convert_to_rgb(stacked_videos)\n             if do_resize:\n-                stacked_videos = self.resize(\n-                    stacked_videos, size=size, size_divisor=size_divisor, interpolation=interpolation\n-                )\n+                stacked_videos = self.resize(stacked_videos, size=size, interpolation=interpolation)\n             resized_videos_grouped[shape] = stacked_videos\n         resized_videos = reorder_videos(resized_videos_grouped, grouped_videos_index)\n "
        },
        {
            "sha": "2fbe7e79d3e52117be1a56c7954b1a8a92472287",
            "filename": "tests/models/gemma3n/test_processing_gemma3n.py",
            "status": "modified",
            "additions": 2,
            "deletions": 8,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -66,15 +66,12 @@ def test_save_load_pretrained_default(self):\n             tokenizer=tokenizer, feature_extractor=feature_extractor, image_processor=image_processor\n         )\n \n-        processor.save_pretrained(self.tmpdirname)\n+        processor.save_pretrained(self.tmpdirname, legacy_serialization=False)\n         processor = Gemma3nProcessor.from_pretrained(self.tmpdirname)\n \n         self.assertIsInstance(processor.tokenizer, GemmaTokenizerFast)\n         self.assertEqual(processor.tokenizer.get_vocab(), tokenizer.get_vocab())\n \n-        # `disable_grouping` is a new attribute that got added on main while gemma3n was being released - so was\n-        # not part of the saved processor\n-        del processor.feature_extractor.disable_grouping\n         self.assertIsInstance(processor.feature_extractor, Gemma3nAudioFeatureExtractor)\n         self.assertEqual(processor.feature_extractor.to_json_string(), feature_extractor.to_json_string())\n \n@@ -86,7 +83,7 @@ def test_save_load_pretrained_additional_features(self):\n         processor = Gemma3nProcessor(\n             tokenizer=tokenizer, feature_extractor=feature_extractor, image_processor=image_processor\n         )\n-        processor.save_pretrained(self.tmpdirname)\n+        processor.save_pretrained(self.tmpdirname, legacy_serialization=False)\n \n         tokenizer_add_kwargs = self.get_tokenizer(bos_token=\"(BOS-BOS)\", eos_token=\"(EOS-EOS)\")\n         feature_extractor_add_kwargs = self.get_feature_extractor(dither=5.0, padding_value=1.0)\n@@ -98,9 +95,6 @@ def test_save_load_pretrained_additional_features(self):\n         self.assertEqual(processor.tokenizer.get_vocab(), tokenizer_add_kwargs.get_vocab())\n         self.assertIsInstance(processor.tokenizer, GemmaTokenizerFast)\n \n-        # `disable_grouping` is a new attribute that got added on main while gemma3n was being released - so was\n-        # not part of the saved processor\n-        del processor.feature_extractor.disable_grouping\n         self.assertEqual(processor.feature_extractor.to_json_string(), feature_extractor_add_kwargs.to_json_string())\n         self.assertIsInstance(processor.feature_extractor, Gemma3nAudioFeatureExtractor)\n "
        },
        {
            "sha": "73212e3ec4b3eea9e3c71a6875366ef22de52b37",
            "filename": "tests/models/janus/test_processing_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -457,7 +457,7 @@ def test_processor_postprocess(self):\n         orig_image_input = self.prepare_image_inputs()\n         orig_image = np.array(orig_image_input).transpose(2, 0, 1)\n \n-        inputs = processor(text=input_str, images=orig_image, do_resize=False, return_tensors=\"np\")\n+        inputs = processor(text=input_str, images=orig_image, do_resize=False, do_pad=False, return_tensors=\"np\")\n         normalized_image_input = inputs.pixel_values\n         unnormalized_images = processor.postprocess(normalized_image_input, return_tensors=\"np\")[\"pixel_values\"]\n "
        },
        {
            "sha": "2cf3edaf4386bb9397de55d562e029263f6ab181",
            "filename": "tests/models/swin2sr/test_image_processing_swin2sr.py",
            "status": "modified",
            "additions": 8,
            "deletions": 7,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -48,7 +48,7 @@ def __init__(\n         do_rescale=True,\n         rescale_factor=1 / 255,\n         do_pad=True,\n-        pad_size=8,\n+        size_divisor=8,\n     ):\n         self.parent = parent\n         self.batch_size = batch_size\n@@ -59,14 +59,14 @@ def __init__(\n         self.do_rescale = do_rescale\n         self.rescale_factor = rescale_factor\n         self.do_pad = do_pad\n-        self.pad_size = pad_size\n+        self.size_divisor = size_divisor\n \n     def prepare_image_processor_dict(self):\n         return {\n             \"do_rescale\": self.do_rescale,\n             \"rescale_factor\": self.rescale_factor,\n             \"do_pad\": self.do_pad,\n-            \"pad_size\": self.pad_size,\n+            \"size_divisor\": self.size_divisor,\n         }\n \n     def expected_output_image_shape(self, images):\n@@ -79,8 +79,8 @@ def expected_output_image_shape(self, images):\n         else:\n             input_height, input_width = img.shape[-2:]\n \n-        pad_height = (input_height // self.pad_size + 1) * self.pad_size - input_height\n-        pad_width = (input_width // self.pad_size + 1) * self.pad_size - input_width\n+        pad_height = (input_height // self.size_divisor + 1) * self.size_divisor - input_height\n+        pad_width = (input_width // self.size_divisor + 1) * self.size_divisor - input_width\n \n         return self.num_channels, input_height + pad_height, input_width + pad_width\n \n@@ -116,11 +116,12 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n             self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n             self.assertTrue(hasattr(image_processing, \"do_pad\"))\n-            self.assertTrue(hasattr(image_processing, \"pad_size\"))\n+            self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n+            self.assertTrue(hasattr(image_processing, \"pad_size\"))  # deprecated but should be available\n \n     def calculate_expected_size(self, image):\n         old_height, old_width = get_image_size(image)\n-        size = self.image_processor_tester.pad_size\n+        size = self.image_processor_tester.size_divisor\n \n         pad_height = (old_height // size + 1) * size - old_height\n         pad_width = (old_width // size + 1) * size - old_width"
        },
        {
            "sha": "c2c8b81dfc0a4e2f132671c1654567e33ff2a6d0",
            "filename": "tests/models/tvp/test_image_processing_tvp.py",
            "status": "modified",
            "additions": 16,
            "deletions": 16,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -222,31 +222,31 @@ def test_call_numpy(self):\n             # Test not batched input\n             expected_height, expected_width = self.image_processor_tester.get_expected_values(video_inputs)\n             encoded_videos = image_processing(test_inputs[0], return_tensors=\"pt\").pixel_values\n-            self.assertEqual(\n-                encoded_videos.shape,\n-                (\n+            self.assertListEqual(\n+                list(encoded_videos.shape),\n+                [\n                     1,\n                     self.image_processor_tester.num_frames,\n                     self.image_processor_tester.num_channels,\n                     expected_height,\n                     expected_width,\n-                ),\n+                ],\n             )\n \n             # Test batched\n             expected_height, expected_width = self.image_processor_tester.get_expected_values(\n                 video_inputs, batched=True\n             )\n             encoded_videos = image_processing(test_inputs, return_tensors=\"pt\").pixel_values\n-            self.assertEqual(\n-                encoded_videos.shape,\n-                (\n+            self.assertListEqual(\n+                list(encoded_videos.shape),\n+                [\n                     self.image_processor_tester.batch_size,\n                     self.image_processor_tester.num_frames,\n                     self.image_processor_tester.num_channels,\n                     expected_height,\n                     expected_width,\n-                ),\n+                ],\n             )\n \n     def test_call_numpy_4_channels(self):\n@@ -276,15 +276,15 @@ def test_call_numpy_4_channels(self):\n             encoded_videos = image_processing(\n                 test_inputs[0], return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n             ).pixel_values\n-            self.assertEqual(\n-                encoded_videos.shape,\n-                (\n+            self.assertListEqual(\n+                list(encoded_videos.shape),\n+                [\n                     1,\n                     self.image_processor_tester.num_frames,\n                     self.image_processor_tester.num_channels,\n                     expected_height,\n                     expected_width,\n-                ),\n+                ],\n             )\n \n             # Test batched\n@@ -294,15 +294,15 @@ def test_call_numpy_4_channels(self):\n             encoded_videos = image_processing(\n                 test_inputs, return_tensors=\"pt\", image_mean=0, image_std=1, input_data_format=\"channels_first\"\n             ).pixel_values\n-            self.assertEqual(\n-                encoded_videos.shape,\n-                (\n+            self.assertListEqual(\n+                list(encoded_videos.shape),\n+                [\n                     self.image_processor_tester.batch_size,\n                     self.image_processor_tester.num_frames,\n                     self.image_processor_tester.num_channels,\n                     expected_height,\n                     expected_width,\n-                ),\n+                ],\n             )\n         self.image_processor_tester.num_channels = 3\n "
        },
        {
            "sha": "a103c33a9cca8c1785f29abfd0e9ffaead6d6d3a",
            "filename": "tests/models/vitmatte/test_image_processing_vitmatte.py",
            "status": "modified",
            "additions": 15,
            "deletions": 13,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e837f6ae244cd5be0d9629e695f60b054f16cb5/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e837f6ae244cd5be0d9629e695f60b054f16cb5/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py?ref=8e837f6ae244cd5be0d9629e695f60b054f16cb5",
            "patch": "@@ -60,7 +60,7 @@ def __init__(\n         do_rescale=True,\n         rescale_factor=0.5,\n         do_pad=True,\n-        size_divisibility=10,\n+        size_divisor=10,\n         do_normalize=True,\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n@@ -74,7 +74,7 @@ def __init__(\n         self.do_rescale = do_rescale\n         self.rescale_factor = rescale_factor\n         self.do_pad = do_pad\n-        self.size_divisibility = size_divisibility\n+        self.size_divisor = size_divisor\n         self.do_normalize = do_normalize\n         self.image_mean = image_mean\n         self.image_std = image_std\n@@ -87,7 +87,7 @@ def prepare_image_processor_dict(self):\n             \"do_rescale\": self.do_rescale,\n             \"rescale_factor\": self.rescale_factor,\n             \"do_pad\": self.do_pad,\n-            \"size_divisibility\": self.size_divisibility,\n+            \"size_divisor\": self.size_divisor,\n         }\n \n     def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n@@ -125,6 +125,8 @@ def test_image_processor_properties(self):\n             self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n             self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n             self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"size_divisor\"))\n+            # Check size_divisibility for BC, the image proccessor has to have an atribute\n             self.assertTrue(hasattr(image_processing, \"size_divisibility\"))\n \n     def test_call_numpy(self):\n@@ -141,8 +143,8 @@ def test_call_numpy(self):\n             encoded_images = image_processing(images=image, trimaps=trimap, return_tensors=\"pt\").pixel_values\n \n             # Verify that width and height can be divided by size_divisibility and that correct dimensions got merged\n-            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n-            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisor == 0)\n+            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisor == 0)\n             self.assertTrue(encoded_images.shape[-3] == 4)\n \n     def test_call_pytorch(self):\n@@ -160,8 +162,8 @@ def test_call_pytorch(self):\n             encoded_images = image_processing(images=image, trimaps=trimap, return_tensors=\"pt\").pixel_values\n \n             # Verify that width and height can be divided by size_divisibility and that correct dimensions got merged\n-            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n-            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisor == 0)\n+            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisor == 0)\n             self.assertTrue(encoded_images.shape[-3] == 4)\n \n         # create batched tensors\n@@ -180,8 +182,8 @@ def test_call_pytorch(self):\n             encoded_images = image_processing(images=image, trimaps=trimap, return_tensors=\"pt\").pixel_values\n \n             # Verify that width and height can be divided by size_divisibility and that correct dimensions got merged\n-            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n-            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisor == 0)\n+            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisor == 0)\n             self.assertTrue(encoded_images.shape[-3] == 4)\n \n     def test_call_pil(self):\n@@ -198,8 +200,8 @@ def test_call_pil(self):\n             encoded_images = image_processing(images=image, trimaps=trimap, return_tensors=\"pt\").pixel_values\n \n             # Verify that width and height can be divided by size_divisibility and that correct dimensions got merged\n-            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n-            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisor == 0)\n+            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisor == 0)\n             self.assertTrue(encoded_images.shape[-3] == 4)\n \n     def test_call_numpy_4_channels(self):\n@@ -224,8 +226,8 @@ def test_call_numpy_4_channels(self):\n             ).pixel_values\n \n             # Verify that width and height can be divided by size_divisibility and that correct dimensions got merged\n-            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisibility == 0)\n-            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisibility == 0)\n+            self.assertTrue(encoded_images.shape[-1] % self.image_processor_tester.size_divisor == 0)\n+            self.assertTrue(encoded_images.shape[-2] % self.image_processor_tester.size_divisor == 0)\n             self.assertTrue(encoded_images.shape[-3] == 5)\n \n     def test_padding_slow(self):"
        }
    ],
    "stats": {
        "total": 1193,
        "additions": 619,
        "deletions": 574
    }
}