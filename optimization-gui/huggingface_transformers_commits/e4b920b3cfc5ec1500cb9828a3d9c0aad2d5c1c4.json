{
    "author": "eustlb",
    "message": "[Parakeet] add output_attention_mask (#41694)\n\n* add output_attention_mask\n\n* style",
    "sha": "e4b920b3cfc5ec1500cb9828a3d9c0aad2d5c1c4",
    "files": [
        {
            "sha": "8ca7b7ff37d8299bc471768af52dc3318962ae57",
            "filename": "src/transformers/models/parakeet/modeling_parakeet.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4b920b3cfc5ec1500cb9828a3d9c0aad2d5c1c4/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4b920b3cfc5ec1500cb9828a3d9c0aad2d5c1c4/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodeling_parakeet.py?ref=e4b920b3cfc5ec1500cb9828a3d9c0aad2d5c1c4",
            "patch": "@@ -37,6 +37,16 @@\n from .configuration_parakeet import ParakeetCTCConfig, ParakeetEncoderConfig\n \n \n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Extends [~modeling_outputs.BaseModelOutput] to include the output attention mask since sequence length is not preserved in the model's forward.\n+    \"\"\"\n+)\n+class ParakeetEncoderModelOutput(BaseModelOutput):\n+    attention_mask: Optional[torch.Tensor] = None\n+\n+\n class ParakeetEncoderRelPositionalEncoding(nn.Module):\n     \"\"\"Relative positional encoding for Parakeet.\"\"\"\n \n@@ -513,9 +523,13 @@ def forward(\n         self,\n         input_features: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n+        output_attention_mask: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n+        output_attention_mask (`bool`, *optional*):\n+            Whether to return the output attention mask.\n+\n         Example:\n \n         ```python\n@@ -546,8 +560,8 @@ def forward(\n         )\n \n         if attention_mask is not None:\n-            attention_mask = self._get_output_attention_mask(attention_mask, target_length=hidden_states.shape[1])\n-            attention_mask = attention_mask.unsqueeze(1).expand(-1, hidden_states.shape[1], -1)\n+            output_mask = self._get_output_attention_mask(attention_mask, target_length=hidden_states.shape[1])\n+            attention_mask = output_mask.unsqueeze(1).expand(-1, hidden_states.shape[1], -1)\n             attention_mask = attention_mask & attention_mask.transpose(1, 2)\n             attention_mask = attention_mask.unsqueeze(1)\n \n@@ -567,7 +581,9 @@ def forward(\n                     **kwargs,\n                 )\n \n-        return BaseModelOutput(last_hidden_state=hidden_states)\n+        return ParakeetEncoderModelOutput(\n+            last_hidden_state=hidden_states, attention_mask=output_mask.int() if output_attention_mask else None\n+        )\n \n \n @dataclass"
        },
        {
            "sha": "6b597e1b50a3978aec33e2ef4f8c344467ede191",
            "filename": "src/transformers/models/parakeet/modular_parakeet.py",
            "status": "modified",
            "additions": 19,
            "deletions": 3,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/e4b920b3cfc5ec1500cb9828a3d9c0aad2d5c1c4/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e4b920b3cfc5ec1500cb9828a3d9c0aad2d5c1c4/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fmodular_parakeet.py?ref=e4b920b3cfc5ec1500cb9828a3d9c0aad2d5c1c4",
            "patch": "@@ -34,6 +34,16 @@\n from .configuration_parakeet import ParakeetCTCConfig, ParakeetEncoderConfig\n \n \n+@dataclass\n+@auto_docstring(\n+    custom_intro=\"\"\"\n+    Extends [~modeling_outputs.BaseModelOutput] to include the output attention mask since sequence length is not preserved in the model's forward.\n+    \"\"\"\n+)\n+class ParakeetEncoderModelOutput(BaseModelOutput):\n+    attention_mask: Optional[torch.Tensor] = None\n+\n+\n class ParakeetEncoderRelPositionalEncoding(nn.Module):\n     \"\"\"Relative positional encoding for Parakeet.\"\"\"\n \n@@ -399,9 +409,13 @@ def forward(\n         self,\n         input_features: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n+        output_attention_mask: Optional[bool] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> BaseModelOutput:\n         r\"\"\"\n+        output_attention_mask (`bool`, *optional*):\n+            Whether to return the output attention mask.\n+\n         Example:\n \n         ```python\n@@ -432,8 +446,8 @@ def forward(\n         )\n \n         if attention_mask is not None:\n-            attention_mask = self._get_output_attention_mask(attention_mask, target_length=hidden_states.shape[1])\n-            attention_mask = attention_mask.unsqueeze(1).expand(-1, hidden_states.shape[1], -1)\n+            output_mask = self._get_output_attention_mask(attention_mask, target_length=hidden_states.shape[1])\n+            attention_mask = output_mask.unsqueeze(1).expand(-1, hidden_states.shape[1], -1)\n             attention_mask = attention_mask & attention_mask.transpose(1, 2)\n             attention_mask = attention_mask.unsqueeze(1)\n \n@@ -453,7 +467,9 @@ def forward(\n                     **kwargs,\n                 )\n \n-        return BaseModelOutput(last_hidden_state=hidden_states)\n+        return ParakeetEncoderModelOutput(\n+            last_hidden_state=hidden_states, attention_mask=output_mask.int() if output_attention_mask else None\n+        )\n \n \n @dataclass"
        }
    ],
    "stats": {
        "total": 44,
        "additions": 38,
        "deletions": 6
    }
}