{
    "author": "zucchini-nlp",
    "message": "[emu3] fix conversion script (#38297)\n\n* fix conversion script and update weights\n\n* fixup\n\n* remove commented line",
    "sha": "b01984a51daa00337c6f0b7018f9569f51517e1b",
    "files": [
        {
            "sha": "ddc907adb4c9710c16b25a233d0907a920e7983d",
            "filename": "src/transformers/models/emu3/convert_emu3_weights_to_hf.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/b01984a51daa00337c6f0b7018f9569f51517e1b/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b01984a51daa00337c6f0b7018f9569f51517e1b/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fconvert_emu3_weights_to_hf.py?ref=b01984a51daa00337c6f0b7018f9569f51517e1b",
            "patch": "@@ -211,14 +211,13 @@ def convert_tiktoken(tokenizer, output_dir):\n \n \n KEYS_TO_MODIFY_MAPPING = {\n+    \"^model\": \"model.text_model\",\n     \"^encoder\": \"model.vqmodel.encoder\",\n     \"^decoder\": \"model.vqmodel.decoder\",\n     \"^post_quant_conv\": \"model.vqmodel.post_quant_conv\",\n     \"^quant_conv\": \"model.vqmodel.quant_conv\",\n     \"^quantize\": \"model.vqmodel.quantize\",\n-    \"^model\": \"text_model.model\",\n-    r\"lm_head\\.weight\": \"text_model.lm_head.weight\",\n-    r\"^text_model\\.model\\.vqmodel\": \"vqmodel\",\n+    r\"lm_head\\.weight\": \"lm_head.weight\",\n     # rename QKV proj for the VQ-VAE model because we use SiglipAttention\n     r\"\\.q\\.\": \".q_proj.\",\n     r\"\\.k\\.\": \".k_proj.\","
        },
        {
            "sha": "6e7b2e419a0798ff689e98b082da8b2b7661dfd2",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b01984a51daa00337c6f0b7018f9569f51517e1b/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b01984a51daa00337c6f0b7018f9569f51517e1b/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=b01984a51daa00337c6f0b7018f9569f51517e1b",
            "patch": "@@ -1598,6 +1598,13 @@ def text_model(self):\n     def vqmodel(self):\n         return self.model.vqmodel\n \n+    @property\n+    def vocabulary_mapping(self):\n+        return self.model.vocabulary_mapping\n+\n+    def decode_image_tokens(self, **kwargs):\n+        return self.model.decode_image_tokens(**kwargs)\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "9a141d61dba7cd22d99c6bc43930cd8b859f8887",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/b01984a51daa00337c6f0b7018f9569f51517e1b/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b01984a51daa00337c6f0b7018f9569f51517e1b/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=b01984a51daa00337c6f0b7018f9569f51517e1b",
            "patch": "@@ -1077,6 +1077,13 @@ def text_model(self):\n     def vqmodel(self):\n         return self.model.vqmodel\n \n+    @property\n+    def vocabulary_mapping(self):\n+        return self.model.vocabulary_mapping\n+\n+    def decode_image_tokens(self, **kwargs):\n+        return self.model.decode_image_tokens(**kwargs)\n+\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        }
    ],
    "stats": {
        "total": 19,
        "additions": 16,
        "deletions": 3
    }
}