{
    "author": "qubvel",
    "message": "Refactor Attention implementation for ViT-based models (#36545)\n\n* Refactor vit attention\n\n* Refactor ViT-based models\n\n* ðŸš¨ðŸš¨ðŸš¨ Fix prefix for DPT\n\n* Update params order\n\n* trigger tests\n\n* Fix Dinov2 attention\n\n* Fix DPT attention impl propagation for backbone config\n\n* Common test fix: config is modif. inplace - avoid it\n\n* view->reshape\n\n* Fixup\n\n* Fixup\n\n* Enable IJepa FA2\n\n* Add FA2 in corresponding model docs",
    "sha": "66291778dd7cea6622219257bf890b20835a6de9",
    "files": [
        {
            "sha": "14669ce0fb1b02b689ee99c0e49810cdd0b0cd70",
            "filename": "docs/source/en/model_doc/audio-spectrogram-transformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -18,6 +18,7 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n "
        },
        {
            "sha": "57cfee1f11c5e906c9cc90128247755cd6361852",
            "filename": "docs/source/en/model_doc/deit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -19,6 +19,7 @@ rendered properly in your Markdown viewer.\n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n "
        },
        {
            "sha": "acf7b2060038dcbbdc7f29db76002b8b41daad36",
            "filename": "docs/source/en/model_doc/dinov2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -16,6 +16,7 @@ specific language governing permissions and limitations under the License.\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n \">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n "
        },
        {
            "sha": "3b12d314a5a0d4acd5a71ae4a78cc7eb377738fd",
            "filename": "docs/source/en/model_doc/dinov2_with_registers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov2_with_registers.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -11,6 +11,7 @@ specific language governing permissions and limitations under the License.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n "
        },
        {
            "sha": "95e422dee862a3b9bde9b5cbcdd7adac50ef7bb7",
            "filename": "docs/source/en/model_doc/dpt.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdpt.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -18,6 +18,8 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n \n ## Overview"
        },
        {
            "sha": "d35011478182318575d3fcd797a512837b4e7bcd",
            "filename": "docs/source/en/model_doc/ijepa.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fijepa.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -18,6 +18,7 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n "
        },
        {
            "sha": "be048d5b73a678f4dae45b8c92ea29a8ded1214e",
            "filename": "docs/source/en/model_doc/videomae.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -18,6 +18,7 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n "
        },
        {
            "sha": "05c724ff7bb6fe57877f6bbc9d4161c69c713eaa",
            "filename": "docs/source/en/model_doc/vit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -21,6 +21,7 @@ rendered properly in your Markdown viewer.\n <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n <img alt=\"Flax\" src=\"https://img.shields.io/badge/Flax-29a79b.svg?style=flat&logo=data:image/png;base64,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\n \">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n "
        },
        {
            "sha": "893490cf013b292a47d207911b447a95f0e8c0fc",
            "filename": "docs/source/en/model_doc/vit_mae.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_mae.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -19,6 +19,7 @@ rendered properly in your Markdown viewer.\n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n <img alt=\"TensorFlow\" src=\"https://img.shields.io/badge/TensorFlow-FF6F00?style=flat&logo=tensorflow&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n "
        },
        {
            "sha": "a3aadef0e9bf41da18df97ed7653fd13fc3c1ac6",
            "filename": "docs/source/en/model_doc/vit_msn.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -18,6 +18,7 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n "
        },
        {
            "sha": "a2cba9793e82bf3cda36e4c64b106d12d8731bbf",
            "filename": "docs/source/en/model_doc/vivit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -14,6 +14,7 @@ specific language governing permissions and limitations under the License.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n "
        },
        {
            "sha": "2a0f5d23fa47ded19ba6b8f6f9c35e05ff6d2255",
            "filename": "docs/source/en/model_doc/yolos.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fyolos.md?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -18,6 +18,7 @@ rendered properly in your Markdown viewer.\n \n <div class=\"flex flex-wrap space-x-1\">\n <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n </div>\n "
        },
        {
            "sha": "4e27421574a6c5660a09899e211143b4b5ad8ea8",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -2098,7 +2098,9 @@ def _autoset_attn_implementation(\n                 if not isinstance(requested_attn_implementation, dict)\n                 else requested_attn_implementation.get(key, None)\n             )\n-            sub_config._attn_implementation_internal = curr_attn_implementation\n+            # For models with backbone sub-config might be not initialized\n+            if sub_config is not None:\n+                sub_config._attn_implementation_internal = curr_attn_implementation\n \n         if use_flash_attention_2:\n             logger.warning_once("
        },
        {
            "sha": "e9e029cf53f3bed057bb419dade96223d43c79f7",
            "filename": "src/transformers/models/audio_spectrogram_transformer/modeling_audio_spectrogram_transformer.py",
            "status": "modified",
            "additions": 59,
            "deletions": 87,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faudio_spectrogram_transformer%2Fmodeling_audio_spectrogram_transformer.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -14,8 +14,7 @@\n # limitations under the License.\n \"\"\"PyTorch Audio Spectrogram Transformer (AST) model.\"\"\"\n \n-import math\n-from typing import Dict, List, Optional, Set, Tuple, Union\n+from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -24,7 +23,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, SequenceClassifierOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import add_code_sample_docstrings, add_start_docstrings, add_start_docstrings_to_model_forward, logging\n from .configuration_audio_spectrogram_transformer import ASTConfig\n@@ -108,6 +107,37 @@ def forward(self, input_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n+# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->AST\n class ASTSelfAttention(nn.Module):\n     def __init__(self, config: ASTConfig) -> None:\n@@ -118,16 +148,18 @@ def __init__(self, config: ASTConfig) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -136,85 +168,37 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaSelfAttention with ViT->AST\n-class ASTSdpaSelfAttention(ASTSelfAttention):\n-    def __init__(self, config: ASTConfig) -> None:\n-        super().__init__(config)\n-        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        if output_attentions or head_mask is not None:\n-            logger.warning_once(\n-                \"`ASTSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n-                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                head_mask=head_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+        context_layer, attention_probs = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n             head_mask,\n-            self.attention_probs_dropout_prob if self.training else 0.0,\n-            is_causal=False,\n-            scale=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n         )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n \n-        return context_layer, None\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->AST\n@@ -276,13 +260,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaAttention with ViT->AST\n-class ASTSdpaAttention(ASTAttention):\n-    def __init__(self, config: ASTConfig) -> None:\n-        super().__init__(config)\n-        self.attention = ASTSdpaSelfAttention(config)\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTIntermediate with ViT->AST\n class ASTIntermediate(nn.Module):\n     def __init__(self, config: ASTConfig) -> None:\n@@ -316,12 +293,6 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-AST_ATTENTION_CLASSES = {\n-    \"eager\": ASTAttention,\n-    \"sdpa\": ASTSdpaAttention,\n-}\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->AST,VIT->AST\n class ASTLayer(nn.Module):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n@@ -330,7 +301,7 @@ def __init__(self, config: ASTConfig) -> None:\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = AST_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = ASTAttention(config)\n         self.intermediate = ASTIntermediate(config)\n         self.output = ASTOutput(config)\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -428,6 +399,7 @@ class ASTPreTrainedModel(PreTrainedModel):\n     main_input_name = \"input_values\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     # Copied from transformers.models.deit.modeling_deit.DeiTPreTrainedModel._init_weights\n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:"
        },
        {
            "sha": "3c70ae132f0c5297e7cdc03a2445e23d672abe8d",
            "filename": "src/transformers/models/deit/modeling_deit.py",
            "status": "modified",
            "additions": 59,
            "deletions": 87,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeit%2Fmodeling_deit.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -15,9 +15,8 @@\n \"\"\"PyTorch DeiT model.\"\"\"\n \n import collections.abc\n-import math\n from dataclasses import dataclass\n-from typing import Optional, Set, Tuple, Union\n+from typing import Callable, Optional, Set, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -31,7 +30,7 @@\n     ImageClassifierOutput,\n     MaskedImageModelingOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     ModelOutput,\n@@ -180,6 +179,37 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return x\n \n \n+# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->DeiT\n class DeiTSelfAttention(nn.Module):\n     def __init__(self, config: DeiTConfig) -> None:\n@@ -190,16 +220,18 @@ def __init__(self, config: DeiTConfig) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -208,85 +240,37 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaSelfAttention with ViT->DeiT\n-class DeiTSdpaSelfAttention(DeiTSelfAttention):\n-    def __init__(self, config: DeiTConfig) -> None:\n-        super().__init__(config)\n-        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        if output_attentions or head_mask is not None:\n-            logger.warning_once(\n-                \"`DeiTSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n-                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                head_mask=head_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+        context_layer, attention_probs = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n             head_mask,\n-            self.attention_probs_dropout_prob if self.training else 0.0,\n-            is_causal=False,\n-            scale=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n         )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n \n-        return context_layer, None\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->DeiT\n@@ -348,13 +332,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaAttention with ViT->DeiT\n-class DeiTSdpaAttention(DeiTAttention):\n-    def __init__(self, config: DeiTConfig) -> None:\n-        super().__init__(config)\n-        self.attention = DeiTSdpaSelfAttention(config)\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTIntermediate with ViT->DeiT\n class DeiTIntermediate(nn.Module):\n     def __init__(self, config: DeiTConfig) -> None:\n@@ -388,12 +365,6 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-DEIT_ATTENTION_CLASSES = {\n-    \"eager\": DeiTAttention,\n-    \"sdpa\": DeiTSdpaAttention,\n-}\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->DeiT,VIT->DEIT\n class DeiTLayer(nn.Module):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n@@ -402,7 +373,7 @@ def __init__(self, config: DeiTConfig) -> None:\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = DEIT_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = DeiTAttention(config)\n         self.intermediate = DeiTIntermediate(config)\n         self.output = DeiTOutput(config)\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -501,6 +472,7 @@ class DeiTPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DeiTLayer\"]\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "17b79134adbe6f02c1e1c48caac3d84cd83bef7e",
            "filename": "src/transformers/models/depth_anything/modeling_depth_anything.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fmodeling_depth_anything.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -240,7 +240,8 @@ def forward(self, hidden_states, size=None):\n         return fused_hidden_states\n \n \n-# Copied from transformers.models.dpt.modeling_dpt.DPTPreTrainedModel with DPT->DepthAnything,dpt->depth_anything\n+# Modified from transformers.models.dpt.modeling_dpt.DPTPreTrainedModel with DPT->DepthAnything,dpt->depth_anything\n+# avoiding sdpa and flash_attn_2 support, it's done in the backend\n class DepthAnythingPreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained"
        },
        {
            "sha": "2e11d3a76c6cf3f7cea9d5abc1e828d1021d7aed",
            "filename": "src/transformers/models/dinov2/modeling_dinov2.py",
            "status": "modified",
            "additions": 59,
            "deletions": 80,
            "changes": 139,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2%2Fmodeling_dinov2.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -15,8 +15,7 @@\n \"\"\"PyTorch DINOv2 model.\"\"\"\n \n import collections.abc\n-import math\n-from typing import Dict, List, Optional, Set, Tuple, Union\n+from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -30,7 +29,7 @@\n     BaseModelOutputWithPooling,\n     ImageClassifierOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -172,6 +171,37 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n+# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->Dinov2\n class Dinov2SelfAttention(nn.Module):\n     def __init__(self, config: Dinov2Config) -> None:\n@@ -182,16 +212,18 @@ def __init__(self, config: Dinov2Config) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -200,78 +232,37 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-class Dinov2SdpaSelfAttention(Dinov2SelfAttention):\n-    def __init__(self, config: Dinov2Config) -> None:\n-        super().__init__(config)\n-        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Dinov2Model is using Dinov2SdpaSelfAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states, head_mask=head_mask, output_attentions=output_attentions\n-            )\n-\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+        context_layer, attention_probs = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n             head_mask,\n-            self.attention_probs_dropout_prob if self.training else 0.0,\n-            is_causal=False,\n-            scale=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n         )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n-        return context_layer, None\n+        return outputs\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->Dinov2\n@@ -333,13 +324,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaAttention with ViT->Dinov2\n-class Dinov2SdpaAttention(Dinov2Attention):\n-    def __init__(self, config: Dinov2Config) -> None:\n-        super().__init__(config)\n-        self.attention = Dinov2SdpaSelfAttention(config)\n-\n-\n class Dinov2LayerScale(nn.Module):\n     def __init__(self, config) -> None:\n         super().__init__()\n@@ -421,20 +405,14 @@ def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n         return self.weights_out(hidden)\n \n \n-DINOV2_ATTENTION_CLASSES = {\n-    \"eager\": Dinov2Attention,\n-    \"sdpa\": Dinov2SdpaAttention,\n-}\n-\n-\n class Dinov2Layer(nn.Module):\n     \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n \n     def __init__(self, config: Dinov2Config) -> None:\n         super().__init__()\n \n         self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.attention = DINOV2_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = Dinov2Attention(config)\n         self.layer_scale1 = Dinov2LayerScale(config)\n         self.drop_path = Dinov2DropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n \n@@ -542,6 +520,7 @@ class Dinov2PreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Dinov2SwiGLUFFN\"]\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "c7c48dadb73f26c2186d1fedddcbb69b12d54b3b",
            "filename": "src/transformers/models/dinov2_with_registers/modeling_dinov2_with_registers.py",
            "status": "modified",
            "additions": 58,
            "deletions": 79,
            "changes": 137,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov2_with_registers%2Fmodeling_dinov2_with_registers.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -21,16 +21,15 @@\n # limitations under the License.\n \n import collections.abc\n-import math\n-from typing import Dict, List, Optional, Set, Tuple, Union\n+from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n \n import torch\n from torch import nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BackboneOutput, BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -185,6 +184,36 @@ def forward(self, pixel_values: torch.Tensor, bool_masked_pos: Optional[torch.Te\n         return embeddings\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class Dinov2WithRegistersSelfAttention(nn.Module):\n     def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n         super().__init__()\n@@ -194,16 +223,18 @@ def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -212,78 +243,37 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-class Dinov2WithRegistersSdpaSelfAttention(Dinov2WithRegistersSelfAttention):\n-    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n-        super().__init__(config)\n-        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        if output_attentions:\n-            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n-            logger.warning_once(\n-                \"Dinov2WithRegistersModel is using Dinov2WithRegistersSdpaSelfAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n-                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states, head_mask=head_mask, output_attentions=output_attentions\n-            )\n-\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+        context_layer, attention_probs = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n             head_mask,\n-            self.attention_probs_dropout_prob if self.training else 0.0,\n-            is_causal=False,\n-            scale=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n         )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n-        return context_layer, None\n+        return outputs\n \n \n class Dinov2WithRegistersSelfOutput(nn.Module):\n@@ -343,12 +333,6 @@ def forward(\n         return outputs\n \n \n-class Dinov2WithRegistersSdpaAttention(Dinov2WithRegistersAttention):\n-    def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n-        super().__init__(config)\n-        self.attention = Dinov2WithRegistersSdpaSelfAttention(config)\n-\n-\n class Dinov2WithRegistersLayerScale(nn.Module):\n     def __init__(self, config) -> None:\n         super().__init__()\n@@ -428,20 +412,14 @@ def forward(self, hidden_state: torch.Tensor) -> torch.Tensor:\n         return self.weights_out(hidden)\n \n \n-DINOV2_WITH_REGISTERS_ATTENTION_CLASSES = {\n-    \"eager\": Dinov2WithRegistersAttention,\n-    \"sdpa\": Dinov2WithRegistersSdpaAttention,\n-}\n-\n-\n class Dinov2WithRegistersLayer(nn.Module):\n     \"\"\"This corresponds to the Block class in the original implementation.\"\"\"\n \n     def __init__(self, config: Dinov2WithRegistersConfig) -> None:\n         super().__init__()\n \n         self.norm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n-        self.attention = DINOV2_WITH_REGISTERS_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = Dinov2WithRegistersAttention(config)\n         self.layer_scale1 = Dinov2WithRegistersLayerScale(config)\n         self.drop_path = (\n             Dinov2WithRegistersDropPath(config.drop_path_rate) if config.drop_path_rate > 0.0 else nn.Identity()\n@@ -550,6 +528,7 @@ class Dinov2WithRegistersPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"Dinov2WithRegistersSwiGLUFFN\"]\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "32015924cf045735d2529df380c56aabae8c07ad",
            "filename": "src/transformers/models/dpt/configuration_dpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fconfiguration_dpt.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -282,5 +282,9 @@ def to_dict(self):\n         output[\"model_type\"] = self.__class__.model_type\n         return output\n \n+    @property\n+    def sub_configs(self):\n+        return {\"backbone_config\": type(self.backbone_config)} if self.backbone_config is not None else {}\n+\n \n __all__ = [\"DPTConfig\"]"
        },
        {
            "sha": "66c779294a30d31e9a989923d03593dc54da6002",
            "filename": "src/transformers/models/dpt/modeling_dpt.py",
            "status": "modified",
            "additions": 63,
            "deletions": 29,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodeling_dpt.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -20,9 +20,8 @@\n \"\"\"\n \n import collections.abc\n-import math\n from dataclasses import dataclass\n-from typing import List, Optional, Set, Tuple, Union\n+from typing import Callable, List, Optional, Set, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -37,7 +36,7 @@\n     replace_return_docstrings,\n )\n from ...modeling_outputs import BaseModelOutput, DepthEstimatorOutput, SemanticSegmenterOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import ModelOutput, logging, torch_int\n from ...utils.backbone_utils import load_backbone\n@@ -295,8 +294,39 @@ def forward(self, pixel_values):\n         return embeddings\n \n \n+# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->DPT\n-class DPTViTSelfAttention(nn.Module):\n+class DPTSelfAttention(nn.Module):\n     def __init__(self, config: DPTConfig) -> None:\n         super().__init__()\n         if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, \"embedding_size\"):\n@@ -305,16 +335,18 @@ def __init__(self, config: DPTConfig) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -323,33 +355,33 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+        context_layer, attention_probs = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            head_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n+        )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n \n         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n@@ -378,7 +410,7 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n class DPTViTAttention(nn.Module):\n     def __init__(self, config: DPTConfig) -> None:\n         super().__init__()\n-        self.attention = DPTViTSelfAttention(config)\n+        self.attention = DPTSelfAttention(config)\n         self.output = DPTViTSelfOutput(config)\n         self.pruned_heads = set()\n \n@@ -809,6 +841,8 @@ class DPTPreTrainedModel(PreTrainedModel):\n     base_model_prefix = \"dpt\"\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "5c738fe07be52742ce109c907b3abcdd6ff04eb0",
            "filename": "src/transformers/models/ijepa/modeling_ijepa.py",
            "status": "modified",
            "additions": 58,
            "deletions": 86,
            "changes": 144,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodeling_ijepa.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -5,16 +5,15 @@\n #                          modular_ijepa.py file directly. One of our CI enforces this.\n #                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n import collections.abc\n-import math\n-from typing import Dict, List, Optional, Set, Tuple, Union\n+from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n \n import torch\n import torch.nn as nn\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -167,6 +166,7 @@ class IJepaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\"\n@@ -189,6 +189,36 @@ def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> No\n             ).to(module.position_embeddings.dtype)\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class IJepaSelfAttention(nn.Module):\n     def __init__(self, config: IJepaConfig) -> None:\n         super().__init__()\n@@ -198,16 +228,18 @@ def __init__(self, config: IJepaConfig) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -216,84 +248,37 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-class IJepaSdpaSelfAttention(IJepaSelfAttention):\n-    def __init__(self, config: IJepaConfig) -> None:\n-        super().__init__(config)\n-        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        if output_attentions or head_mask is not None:\n-            logger.warning_once(\n-                \"`IJepaSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n-                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                head_mask=head_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+        context_layer, attention_probs = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n             head_mask,\n-            self.attention_probs_dropout_prob if self.training else 0.0,\n-            is_causal=False,\n-            scale=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n         )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n-        return context_layer, None\n+        return outputs\n \n \n class IJepaSelfOutput(nn.Module):\n@@ -353,12 +338,6 @@ def forward(\n         return outputs\n \n \n-class IJepaSdpaAttention(IJepaAttention):\n-    def __init__(self, config: IJepaConfig) -> None:\n-        super().__init__(config)\n-        self.attention = IJepaSdpaSelfAttention(config)\n-\n-\n class IJepaIntermediate(nn.Module):\n     def __init__(self, config: IJepaConfig) -> None:\n         super().__init__()\n@@ -390,20 +369,14 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-IJEPA_ATTENTION_CLASSES = {\n-    \"eager\": IJepaAttention,\n-    \"sdpa\": IJepaSdpaAttention,\n-}\n-\n-\n class IJepaLayer(nn.Module):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: IJepaConfig) -> None:\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = IJEPA_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = IJepaAttention(config)\n         self.intermediate = IJepaIntermediate(config)\n         self.output = IJepaOutput(config)\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -531,7 +504,6 @@ def forward(self, hidden_states):\n \n _EXPECTED_OUTPUT_SHAPE = [1, 256, 1280]\n \n-\n IJEPA_START_DOCSTRING = r\"\"\"\n     This model is a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass. Use it\n     as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage and"
        },
        {
            "sha": "447347a4eca87ae61695aaee5a9b76013e992d70",
            "filename": "src/transformers/models/ijepa/modular_ijepa.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fijepa%2Fmodular_ijepa.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -108,6 +108,7 @@ class IJepaPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"IJepaEmbeddings\", \"IJepaLayer\"]\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "8174a430b72e27840f8700d52207a772631297f9",
            "filename": "src/transformers/models/videomae/modeling_videomae.py",
            "status": "modified",
            "additions": 57,
            "deletions": 67,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideomae%2Fmodeling_videomae.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -15,10 +15,9 @@\n \"\"\"PyTorch VideoMAE (masked autoencoder) model.\"\"\"\n \n import collections.abc\n-import math\n from copy import deepcopy\n from dataclasses import dataclass\n-from typing import Optional, Set, Tuple, Union\n+from typing import Callable, Optional, Set, Tuple, Union\n \n import numpy as np\n import torch\n@@ -28,7 +27,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     ModelOutput,\n@@ -196,6 +195,37 @@ def forward(self, pixel_values):\n         return embeddings\n \n \n+# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class VideoMAESelfAttention(nn.Module):\n     def __init__(self, config: VideoMAEConfig) -> None:\n         super().__init__()\n@@ -204,10 +234,13 @@ def __init__(self, config: VideoMAEConfig) -> None:\n                 f\"The hidden size {config.hidden_size} is not a multiple of the number of attention \"\n                 f\"heads {config.num_attention_heads}.\"\n             )\n-\n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=False)\n@@ -220,8 +253,6 @@ def __init__(self, config: VideoMAEConfig) -> None:\n             self.q_bias = None\n             self.v_bias = None\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -239,65 +270,33 @@ def forward(\n         value_layer = self.transpose_for_scores(values)\n         query_layer = self.transpose_for_scores(queries)\n \n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-class VideoMAESdpaSelfAttention(VideoMAESelfAttention):\n-    def __init__(self, config: VideoMAEConfig) -> None:\n-        super().__init__(config)\n-        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        k_bias = torch.zeros_like(self.v_bias, requires_grad=False) if self.q_bias is not None else None\n-        keys = nn.functional.linear(input=hidden_states, weight=self.key.weight, bias=k_bias)\n-        values = nn.functional.linear(input=hidden_states, weight=self.value.weight, bias=self.v_bias)\n-        queries = nn.functional.linear(input=hidden_states, weight=self.query.weight, bias=self.q_bias)\n-\n-        key_layer = self.transpose_for_scores(keys)\n-        value_layer = self.transpose_for_scores(values)\n-        query_layer = self.transpose_for_scores(queries)\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+        context_layer, attention_probs = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n             head_mask,\n-            self.attention_probs_dropout_prob if self.training else 0.0,\n-            is_causal=False,\n-            scale=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n         )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n \n-        return context_layer, None\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->VideoMAE\n@@ -359,13 +358,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaAttention with ViT->VideoMAE\n-class VideoMAESdpaAttention(VideoMAEAttention):\n-    def __init__(self, config: VideoMAEConfig) -> None:\n-        super().__init__(config)\n-        self.attention = VideoMAESdpaSelfAttention(config)\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTIntermediate ViT->VideoMAE\n class VideoMAEIntermediate(nn.Module):\n     def __init__(self, config: VideoMAEConfig) -> None:\n@@ -399,9 +391,6 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-VIDEOMAE_ATTENTION_CLASSES = {\"eager\": VideoMAEAttention, \"sdpa\": VideoMAESdpaAttention}\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->VideoMAE,VIT->VIDEOMAE\n class VideoMAELayer(nn.Module):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n@@ -410,7 +399,7 @@ def __init__(self, config: VideoMAEConfig) -> None:\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = VIDEOMAE_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = VideoMAEAttention(config)\n         self.intermediate = VideoMAEIntermediate(config)\n         self.output = VideoMAEOutput(config)\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -508,6 +497,7 @@ class VideoMAEPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "2d392ccaf9cddbdc0aa8fc377a9e4131e9eee143",
            "filename": "src/transformers/models/vit/modeling_vit.py",
            "status": "modified",
            "additions": 58,
            "deletions": 84,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit%2Fmodeling_vit.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -16,7 +16,7 @@\n \n import collections.abc\n import math\n-from typing import Dict, List, Optional, Set, Tuple, Union\n+from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -30,7 +30,7 @@\n     ImageClassifierOutput,\n     MaskedImageModelingOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -184,6 +184,36 @@ def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = F\n         return embeddings\n \n \n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n class ViTSelfAttention(nn.Module):\n     def __init__(self, config: ViTConfig) -> None:\n         super().__init__()\n@@ -193,16 +223,18 @@ def __init__(self, config: ViTConfig) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -211,84 +243,37 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-class ViTSdpaSelfAttention(ViTSelfAttention):\n-    def __init__(self, config: ViTConfig) -> None:\n-        super().__init__(config)\n-        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        if output_attentions or head_mask is not None:\n-            logger.warning_once(\n-                \"`ViTSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n-                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                head_mask=head_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+        context_layer, attention_probs = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n             head_mask,\n-            self.attention_probs_dropout_prob if self.training else 0.0,\n-            is_causal=False,\n-            scale=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n         )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n \n-        return context_layer, None\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n \n \n class ViTSelfOutput(nn.Module):\n@@ -348,12 +333,6 @@ def forward(\n         return outputs\n \n \n-class ViTSdpaAttention(ViTAttention):\n-    def __init__(self, config: ViTConfig) -> None:\n-        super().__init__(config)\n-        self.attention = ViTSdpaSelfAttention(config)\n-\n-\n class ViTIntermediate(nn.Module):\n     def __init__(self, config: ViTConfig) -> None:\n         super().__init__()\n@@ -385,20 +364,14 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-VIT_ATTENTION_CLASSES = {\n-    \"eager\": ViTAttention,\n-    \"sdpa\": ViTSdpaAttention,\n-}\n-\n-\n class ViTLayer(nn.Module):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n \n     def __init__(self, config: ViTConfig) -> None:\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = VIT_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = ViTAttention(config)\n         self.intermediate = ViTIntermediate(config)\n         self.output = ViTOutput(config)\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -496,6 +469,7 @@ class ViTPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ViTEmbeddings\", \"ViTLayer\"]\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "c002c41ca068953a0312e3c180b9a9e2b13f2465",
            "filename": "src/transformers/models/vit_mae/modeling_vit_mae.py",
            "status": "modified",
            "additions": 59,
            "deletions": 87,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_mae%2Fmodeling_vit_mae.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -15,10 +15,9 @@\n \"\"\"PyTorch ViT MAE (masked autoencoder) model.\"\"\"\n \n import collections.abc\n-import math\n from copy import deepcopy\n from dataclasses import dataclass\n-from typing import Optional, Set, Tuple, Union\n+from typing import Callable, Optional, Set, Tuple, Union\n \n import numpy as np\n import torch\n@@ -27,7 +26,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     ModelOutput,\n@@ -356,6 +355,37 @@ def forward(self, pixel_values, interpolate_pos_encoding: bool = False):\n         return x\n \n \n+# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention ViT->ViTMAE\n class ViTMAESelfAttention(nn.Module):\n     def __init__(self, config: ViTMAEConfig) -> None:\n@@ -366,16 +396,18 @@ def __init__(self, config: ViTMAEConfig) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -384,85 +416,37 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaSelfAttention ViT->ViTMAE\n-class ViTMAESdpaSelfAttention(ViTMAESelfAttention):\n-    def __init__(self, config: ViTMAEConfig) -> None:\n-        super().__init__(config)\n-        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        if output_attentions or head_mask is not None:\n-            logger.warning_once(\n-                \"`ViTMAESdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n-                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                head_mask=head_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+        context_layer, attention_probs = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n             head_mask,\n-            self.attention_probs_dropout_prob if self.training else 0.0,\n-            is_causal=False,\n-            scale=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n         )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n \n-        return context_layer, None\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->ViTMAE\n@@ -524,13 +508,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaAttention with ViT->ViTMAE\n-class ViTMAESdpaAttention(ViTMAEAttention):\n-    def __init__(self, config: ViTMAEConfig) -> None:\n-        super().__init__(config)\n-        self.attention = ViTMAESdpaSelfAttention(config)\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTIntermediate ViT->ViTMAE\n class ViTMAEIntermediate(nn.Module):\n     def __init__(self, config: ViTMAEConfig) -> None:\n@@ -564,12 +541,6 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-VITMAE_ATTENTION_CLASSES = {\n-    \"eager\": ViTMAEAttention,\n-    \"sdpa\": ViTMAESdpaAttention,\n-}\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->ViTMAE,VIT->VITMAE\n class ViTMAELayer(nn.Module):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n@@ -578,7 +549,7 @@ def __init__(self, config: ViTMAEConfig) -> None:\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = VITMAE_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = ViTMAEAttention(config)\n         self.intermediate = ViTMAEIntermediate(config)\n         self.output = ViTMAEOutput(config)\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -676,6 +647,7 @@ class ViTMAEPreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "8f25438ef9e46df1c703f8d82b4e8df5c3dcc78b",
            "filename": "src/transformers/models/vit_msn/modeling_vit_msn.py",
            "status": "modified",
            "additions": 59,
            "deletions": 84,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvit_msn%2Fmodeling_vit_msn.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -15,8 +15,7 @@\n \"\"\"PyTorch ViT MSN (masked siamese network) model.\"\"\"\n \n import collections.abc\n-import math\n-from typing import Dict, List, Optional, Set, Tuple, Union\n+from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -25,7 +24,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, ImageClassifierOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     add_start_docstrings,\n@@ -173,6 +172,37 @@ def forward(self, pixel_values: torch.Tensor, interpolate_pos_encoding: bool = F\n         return embeddings\n \n \n+# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->ViTMSN\n class ViTMSNSelfAttention(nn.Module):\n     def __init__(self, config: ViTMSNConfig) -> None:\n@@ -183,16 +213,18 @@ def __init__(self, config: ViTMSNConfig) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -201,85 +233,37 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaSelfAttention with ViT->ViTMSN\n-class ViTMSNSdpaSelfAttention(ViTMSNSelfAttention):\n-    def __init__(self, config: ViTMSNConfig) -> None:\n-        super().__init__(config)\n-        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        if output_attentions or head_mask is not None:\n-            logger.warning_once(\n-                \"`ViTMSNSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n-                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                head_mask=head_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+        context_layer, attention_probs = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n             head_mask,\n-            self.attention_probs_dropout_prob if self.training else 0.0,\n-            is_causal=False,\n-            scale=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n         )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n \n-        return context_layer, None\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->ViTMSN\n@@ -341,13 +325,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaAttention with ViT->ViTMSN\n-class ViTMSNSdpaAttention(ViTMSNAttention):\n-    def __init__(self, config: ViTMSNConfig) -> None:\n-        super().__init__(config)\n-        self.attention = ViTMSNSdpaSelfAttention(config)\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTIntermediate with ViT->ViTMSN\n class ViTMSNIntermediate(nn.Module):\n     def __init__(self, config: ViTMSNConfig) -> None:\n@@ -381,9 +358,6 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-VITMSN_ATTENTION_CLASSES = {\"eager\": ViTMSNAttention, \"sdpa\": ViTMSNSdpaAttention}\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->ViTMSN, VIT->VITMSN\n class ViTMSNLayer(nn.Module):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n@@ -392,7 +366,7 @@ def __init__(self, config: ViTMSNConfig) -> None:\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = VITMSN_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = ViTMSNAttention(config)\n         self.intermediate = ViTMSNIntermediate(config)\n         self.output = ViTMSNOutput(config)\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -491,6 +465,7 @@ class ViTMSNPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"ViTMSNAttention\", \"ViTMSNSdpaAttention\"]\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     # todo: Resort to https://github.com/facebookresearch/msn/blob/main/src/deit.py#L200-#L211\n     # when creating pre-training scripts."
        },
        {
            "sha": "c0d6d7f0227c9851c4b3cdfb671402688de7f74a",
            "filename": "src/transformers/models/vitpose_backbone/modeling_vitpose_backbone.py",
            "status": "modified",
            "additions": 61,
            "deletions": 27,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose_backbone%2Fmodeling_vitpose_backbone.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -20,16 +20,15 @@\n \"\"\"\n \n import collections.abc\n-import math\n-from typing import Optional, Set, Tuple, Union\n+from typing import Callable, Optional, Set, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BackboneOutput, BaseModelOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     add_start_docstrings,\n@@ -103,6 +102,37 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n+# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->VitPoseBackbone\n class VitPoseBackboneSelfAttention(nn.Module):\n     def __init__(self, config: VitPoseBackboneConfig) -> None:\n@@ -113,16 +143,18 @@ def __init__(self, config: VitPoseBackboneConfig) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -131,33 +163,33 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.matmul(attention_probs, value_layer)\n+        context_layer, attention_probs = attention_interface(\n+            self,\n+            query_layer,\n+            key_layer,\n+            value_layer,\n+            head_mask,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n+        )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n \n         outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n@@ -392,6 +424,8 @@ class VitPoseBackbonePreTrainedModel(PreTrainedModel):\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"VitPoseBackboneEmbeddings\", \"VitPoseBackboneLayer\"]\n+    _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm, VitPoseBackboneEmbeddings]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "bd6ce5234f098b7b784d1402298538df1e624f69",
            "filename": "src/transformers/models/vivit/modeling_vivit.py",
            "status": "modified",
            "additions": 59,
            "deletions": 84,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvivit%2Fmodeling_vivit.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -14,8 +14,7 @@\n # limitations under the License.\n \"\"\"PyTorch ViViT model.\"\"\"\n \n-import math\n-from typing import Optional, Set, Tuple, Union\n+from typing import Callable, Optional, Set, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n@@ -24,7 +23,7 @@\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     add_start_docstrings,\n@@ -166,6 +165,37 @@ def forward(self, pixel_values, interpolate_pos_encoding: bool = False):\n         return embeddings\n \n \n+# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->Vivit\n class VivitSelfAttention(nn.Module):\n     def __init__(self, config: VivitConfig) -> None:\n@@ -176,16 +206,18 @@ def __init__(self, config: VivitConfig) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -194,82 +226,37 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-# Adapted from transformers.models.vit.modeling_vit.ViTSdpaSelfAttention with ViT->Vivit\n-class VivitSdpaSelfAttention(VivitSelfAttention):\n-    def __init__(self, config: VivitConfig) -> None:\n-        super().__init__(config)\n-        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        if output_attentions or head_mask is not None:\n-            logger.warning_once(\n-                \"VivitSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support\"\n-                \" `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying\"\n-                \" the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be\"\n-                ' removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states,\n-                head_mask,\n-                output_attentions,\n-            )\n-\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+        context_layer, attention_probs = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n             head_mask,\n-            self.attention_probs_dropout_prob if self.training else 0.0,\n-            is_causal=False,\n-            scale=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n         )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n+\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n \n-        return context_layer, None\n+        return outputs\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->Vivit\n@@ -331,13 +318,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaAttention with ViT->Vivit\n-class VivitSdpaAttention(VivitAttention):\n-    def __init__(self, config: VivitConfig) -> None:\n-        super().__init__(config)\n-        self.attention = VivitSdpaSelfAttention(config)\n-\n-\n class VivitIntermediate(nn.Module):\n     def __init__(self, config):\n         super().__init__()\n@@ -372,20 +352,14 @@ def forward(self, hidden_states, input_tensor):\n         return hidden_states\n \n \n-VIVIT_ATTENTION_CLASSES = {\n-    \"eager\": VivitAttention,\n-    \"sdpa\": VivitSdpaAttention,\n-}\n-\n-\n class VivitLayer(nn.Module):\n     \"\"\"This corresponds to the EncoderBlock class in the scenic/vivit implementation.\"\"\"\n \n     def __init__(self, config):\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = VIVIT_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = VivitAttention(config)\n         self.intermediate = VivitIntermediate(config)\n         self.output = VivitOutput(config)\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -495,6 +469,7 @@ class VivitPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "06edd9b4e5e24502c62037f583dfa86d06a1f059",
            "filename": "src/transformers/models/yolos/modeling_yolos.py",
            "status": "modified",
            "additions": 59,
            "deletions": 84,
            "changes": 143,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fyolos%2Fmodeling_yolos.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -15,17 +15,16 @@\n \"\"\"PyTorch YOLOS model.\"\"\"\n \n import collections.abc\n-import math\n from dataclasses import dataclass\n-from typing import Dict, List, Optional, Set, Tuple, Union\n+from typing import Callable, Dict, List, Optional, Set, Tuple, Union\n \n import torch\n import torch.utils.checkpoint\n from torch import nn\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer\n from ...utils import (\n     ModelOutput,\n@@ -231,6 +230,37 @@ def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         return embeddings\n \n \n+# Copied from transformers.models.vit.modeling_vit.eager_attention_forward\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs,\n+):\n+    # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n+    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling\n+\n+    # Normalize the attention scores to probabilities.\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+\n+    # This is actually dropping out entire tokens to attend to, which might\n+    # seem a bit unusual, but is taken from the original Transformer paper.\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+\n+    # Mask heads if we want to\n+    if attention_mask is not None:\n+        attn_weights = attn_weights * attention_mask\n+\n+    attn_output = torch.matmul(attn_weights, value)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n # Copied from transformers.models.vit.modeling_vit.ViTSelfAttention with ViT->Yolos\n class YolosSelfAttention(nn.Module):\n     def __init__(self, config: YolosConfig) -> None:\n@@ -241,16 +271,18 @@ def __init__(self, config: YolosConfig) -> None:\n                 f\"heads {config.num_attention_heads}.\"\n             )\n \n+        self.config = config\n         self.num_attention_heads = config.num_attention_heads\n         self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n         self.all_head_size = self.num_attention_heads * self.attention_head_size\n+        self.dropout_prob = config.attention_probs_dropout_prob\n+        self.scaling = self.attention_head_size**-0.5\n+        self.is_causal = False\n \n         self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n         self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)\n \n-        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n-\n     def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n         new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n         x = x.view(new_x_shape)\n@@ -259,85 +291,37 @@ def transpose_for_scores(self, x: torch.Tensor) -> torch.Tensor:\n     def forward(\n         self, hidden_states, head_mask: Optional[torch.Tensor] = None, output_attentions: bool = False\n     ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        mixed_query_layer = self.query(hidden_states)\n-\n         key_layer = self.transpose_for_scores(self.key(hidden_states))\n         value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n-\n-        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n-        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n-\n-        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n-\n-        # Normalize the attention scores to probabilities.\n-        attention_probs = nn.functional.softmax(attention_scores, dim=-1)\n-\n-        # This is actually dropping out entire tokens to attend to, which might\n-        # seem a bit unusual, but is taken from the original Transformer paper.\n-        attention_probs = self.dropout(attention_probs)\n-\n-        # Mask heads if we want to\n-        if head_mask is not None:\n-            attention_probs = attention_probs * head_mask\n-\n-        context_layer = torch.matmul(attention_probs, value_layer)\n-\n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n-        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n-\n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        return outputs\n-\n-\n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaSelfAttention with ViT->Yolos\n-class YolosSdpaSelfAttention(YolosSelfAttention):\n-    def __init__(self, config: YolosConfig) -> None:\n-        super().__init__(config)\n-        self.attention_probs_dropout_prob = config.attention_probs_dropout_prob\n-\n-    def forward(\n-        self,\n-        hidden_states: torch.FloatTensor,\n-        head_mask: Optional[torch.Tensor] = None,\n-        output_attentions: bool = False,\n-    ) -> Union[Tuple[torch.Tensor, torch.Tensor], Tuple[torch.Tensor]]:\n-        if output_attentions or head_mask is not None:\n-            logger.warning_once(\n-                \"`YolosSdpaAttention` is used but `torch.nn.functional.scaled_dot_product_attention` does not support \"\n-                \"`output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but \"\n-                \"specifying the manual implementation will be required from Transformers version v5.0.0 onwards. \"\n-                'This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-            )\n-            return super().forward(\n-                hidden_states=hidden_states,\n-                head_mask=head_mask,\n-                output_attentions=output_attentions,\n-            )\n-\n-        mixed_query_layer = self.query(hidden_states)\n-\n-        key_layer = self.transpose_for_scores(self.key(hidden_states))\n-        value_layer = self.transpose_for_scores(self.value(hidden_states))\n-        query_layer = self.transpose_for_scores(mixed_query_layer)\n+        query_layer = self.transpose_for_scores(self.query(hidden_states))\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            if self.config._attn_implementation == \"sdpa\" and output_attentions:\n+                logger.warning_once(\n+                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n+                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+                )\n+            else:\n+                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n-        context_layer = torch.nn.functional.scaled_dot_product_attention(\n+        context_layer, attention_probs = attention_interface(\n+            self,\n             query_layer,\n             key_layer,\n             value_layer,\n             head_mask,\n-            self.attention_probs_dropout_prob if self.training else 0.0,\n-            is_causal=False,\n-            scale=None,\n+            is_causal=self.is_causal,\n+            scaling=self.scaling,\n+            dropout=0.0 if not self.training else self.dropout_prob,\n         )\n \n-        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n-        context_layer = context_layer.view(new_context_layer_shape)\n+        context_layer = context_layer.reshape(new_context_layer_shape)\n \n-        return context_layer, None\n+        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n+\n+        return outputs\n \n \n # Copied from transformers.models.vit.modeling_vit.ViTSelfOutput with ViT->Yolos\n@@ -399,13 +383,6 @@ def forward(\n         return outputs\n \n \n-# Copied from transformers.models.vit.modeling_vit.ViTSdpaAttention with ViT->Yolos\n-class YolosSdpaAttention(YolosAttention):\n-    def __init__(self, config: YolosConfig) -> None:\n-        super().__init__(config)\n-        self.attention = YolosSdpaSelfAttention(config)\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTIntermediate with ViT->Yolos\n class YolosIntermediate(nn.Module):\n     def __init__(self, config: YolosConfig) -> None:\n@@ -439,9 +416,6 @@ def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> to\n         return hidden_states\n \n \n-YOLOS_ATTENTION_CLASSES = {\"eager\": YolosAttention, \"sdpa\": YolosSdpaAttention}\n-\n-\n # Copied from transformers.models.vit.modeling_vit.ViTLayer with ViT->Yolos,VIT->YOLOS\n class YolosLayer(nn.Module):\n     \"\"\"This corresponds to the Block class in the timm implementation.\"\"\"\n@@ -450,7 +424,7 @@ def __init__(self, config: YolosConfig) -> None:\n         super().__init__()\n         self.chunk_size_feed_forward = config.chunk_size_feed_forward\n         self.seq_len_dim = 1\n-        self.attention = YOLOS_ATTENTION_CLASSES[config._attn_implementation](config)\n+        self.attention = YolosAttention(config)\n         self.intermediate = YolosIntermediate(config)\n         self.output = YolosOutput(config)\n         self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n@@ -575,6 +549,7 @@ class YolosPreTrainedModel(PreTrainedModel):\n     supports_gradient_checkpointing = True\n     _no_split_modules = []\n     _supports_sdpa = True\n+    _supports_flash_attn_2 = True\n \n     def _init_weights(self, module: Union[nn.Linear, nn.Conv2d, nn.LayerNorm]) -> None:\n         \"\"\"Initialize the weights\"\"\""
        },
        {
            "sha": "57cad7328f74a1fb715f6a13ead63313b92f9bf1",
            "filename": "src/transformers/models/zoedepth/modeling_zoedepth.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fmodeling_zoedepth.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -1219,7 +1219,8 @@ def forward(self, outconv_activation, bottleneck, feature_blocks, relative_depth\n         return out, None\n \n \n-# Copied from transformers.models.dpt.modeling_dpt.DPTPreTrainedModel with DPT->ZoeDepth,dpt->zoedepth\n+# Modified from transformers.models.dpt.modeling_dpt.DPTPreTrainedModel with DPT->ZoeDepth,dpt->zoedepth\n+# avoiding sdpa and flash_attn_2 support, it's done int the backend\n class ZoeDepthPreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained"
        },
        {
            "sha": "da40466d484be074909afd74f8fae0b9d330ba03",
            "filename": "tests/models/dpt/test_modeling_dpt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_modeling_dpt.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -255,6 +255,10 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n+    @unittest.skip(reason=\"Inductor error for dynamic shape\")\n+    def test_sdpa_can_compile_dynamic(self):\n+        pass\n+\n     def test_initialization(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "4b1abab20670ac531bf039c557882a45954ac6cd",
            "filename": "tests/models/videomae/test_modeling_videomae.py",
            "status": "modified",
            "additions": 64,
            "deletions": 1,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_modeling_videomae.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -15,14 +15,24 @@\n \"\"\"Testing suite for the PyTorch VideoMAE model.\"\"\"\n \n import copy\n+import tempfile\n import unittest\n \n import numpy as np\n from huggingface_hub import hf_hub_download\n+from pytest import mark\n \n from transformers import VideoMAEConfig\n from transformers.models.auto import get_values\n-from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.testing_utils import (\n+    is_flaky,\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_gpu,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import cached_property, is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n@@ -338,6 +348,59 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n \n             check_hidden_states_output(inputs_dict, config, model_class)\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    @is_flaky()\n+    def test_flash_attn_2_inference_equivalence(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n+            inputs_dict[\"pixel_values\"] = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n+\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.bfloat16)\n+                model.to(torch_device)\n+\n+                outputs = model(**inputs_dict, output_hidden_states=True)\n+                outputs_fa = model_fa(**inputs_dict, output_hidden_states=True)\n+\n+                logits = (\n+                    outputs.hidden_states[-1]\n+                    if not model.config.is_encoder_decoder\n+                    else outputs.decoder_hidden_states[-1]\n+                )\n+                logits_fa = (\n+                    outputs_fa.hidden_states[-1]\n+                    if not model.config.is_encoder_decoder\n+                    else outputs_fa.decoder_hidden_states[-1]\n+                )\n+\n+                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n+\n+                # check with inference + dropout\n+                model.train()\n+                _ = model_fa(**inputs_dict)\n+\n+    @unittest.skip(\"Not applicable for VideoMAE\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n \n # We will verify our results on a video of eating spaghetti\n # Frame indices used: [164 168 172 176 181 185 189 193 198 202 206 210 215 219 223 227]"
        },
        {
            "sha": "177ddc269d49b60a144eb9c8175fb542b22efd88",
            "filename": "tests/models/vit_mae/test_modeling_vit_mae.py",
            "status": "modified",
            "additions": 67,
            "deletions": 1,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit_mae%2Ftest_modeling_vit_mae.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -19,9 +19,18 @@\n import unittest\n \n import numpy as np\n+from pytest import mark\n \n from transformers import ViTMAEConfig\n-from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n+from transformers.testing_utils import (\n+    is_flaky,\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_gpu,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import cached_property, is_torch_available, is_vision_available\n \n from ...test_configuration_common import ConfigTester\n@@ -269,6 +278,63 @@ def test_model_from_pretrained(self):\n         model = ViTMAEModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    @is_flaky()\n+    def test_flash_attn_2_inference_equivalence(self):\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"Model architecture does not support attentions\")\n+\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            inputs_dict = self._prepare_for_class(inputs_dict, model_class)\n+            inputs_dict[\"pixel_values\"] = inputs_dict[\"pixel_values\"].to(torch.bfloat16)\n+\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+                model_fa = model_class.from_pretrained(\n+                    tmpdirname, torch_dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n+                )\n+                model_fa.to(torch_device)\n+\n+                model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.bfloat16)\n+                model.to(torch_device)\n+\n+                # ForPretraining model has random `noise` -> need to set seed\n+                # to make the test deterministic\n+                torch.manual_seed(12345)\n+                outputs = model(**inputs_dict, output_hidden_states=True)\n+                torch.manual_seed(12345)\n+                outputs_fa = model_fa(**inputs_dict, output_hidden_states=True)\n+\n+                logits = (\n+                    outputs.hidden_states[-1]\n+                    if not model.config.is_encoder_decoder\n+                    else outputs.decoder_hidden_states[-1]\n+                )\n+                logits_fa = (\n+                    outputs_fa.hidden_states[-1]\n+                    if not model.config.is_encoder_decoder\n+                    else outputs_fa.decoder_hidden_states[-1]\n+                )\n+\n+                assert torch.allclose(logits_fa, logits, atol=4e-2, rtol=4e-2)\n+\n+                # check with inference + dropout\n+                model.train()\n+                _ = model_fa(**inputs_dict)\n+\n+    @unittest.skip(\"Not applicable for VideoMAE\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "c16a888885681650cfe60f6344d9895598b819f6",
            "filename": "tests/test_configuration_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/tests%2Ftest_configuration_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/tests%2Ftest_configuration_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_configuration_common.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -130,7 +130,7 @@ def create_and_test_config_from_and_save_pretrained_composite(self):\n             general_config_dict = config.to_dict()\n \n             # Iterate over all sub_configs if there are any and load them with their own classes\n-            sub_configs = self.config_class.sub_configs\n+            sub_configs = general_config_loaded.sub_configs\n             for sub_config_key, sub_class in sub_configs.items():\n                 if sub_class.__name__ == \"AutoConfig\":\n                     sub_class = sub_class.for_model(**general_config_dict[sub_config_key]).__class__"
        },
        {
            "sha": "58c09d41788adbfa43d81e0b58313e73baf5aeae",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 13,
            "deletions": 14,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/66291778dd7cea6622219257bf890b20835a6de9/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/66291778dd7cea6622219257bf890b20835a6de9/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=66291778dd7cea6622219257bf890b20835a6de9",
            "patch": "@@ -315,8 +315,6 @@ def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         return inputs_dict\n \n     def test_save_load(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n         def check_save_load(out1, out2):\n             # make sure we don't have nans\n             out_2 = out2.cpu().numpy()\n@@ -330,6 +328,7 @@ def check_save_load(out1, out2):\n             self.assertLessEqual(max_diff, 1e-5)\n \n         for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n             model.to(torch_device)\n             model.eval()\n@@ -508,16 +507,16 @@ def test_peft_gradient_checkpointing_enable_disable(self):\n \n     @is_flaky(description=\"low likelihood of failure, reason not yet discovered\")\n     def test_save_load_fast_init_from_base(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        if config.__class__ not in MODEL_MAPPING:\n-            self.skipTest(reason=f\"{config.__class__.__name__} not in MODEL_MAPPING\")\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            if config.__class__ not in MODEL_MAPPING:\n+                self.skipTest(reason=f\"{config.__class__.__name__} not in MODEL_MAPPING\")\n \n-        base_class = MODEL_MAPPING[config.__class__]\n+            base_class = MODEL_MAPPING[config.__class__]\n \n-        if isinstance(base_class, tuple):\n-            base_class = base_class[0]\n+            if isinstance(base_class, tuple):\n+                base_class = base_class[0]\n \n-        for model_class in self.all_model_classes:\n             if model_class == base_class:\n                 continue\n \n@@ -2228,9 +2227,9 @@ def test_model_main_input_name(self):\n     def test_correct_missing_keys(self):\n         if not self.test_missing_keys:\n             self.skipTest(reason=\"test_missing_keys is set to `False`\")\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n         for model_class in self.all_model_classes:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n             base_model_prefix = model.base_model_prefix\n \n@@ -2287,8 +2286,8 @@ def check_same_values(layer_1, layer_2):\n \n     @require_safetensors\n     def test_can_use_safetensors(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n             model_tied = model_class(config)\n             with tempfile.TemporaryDirectory() as d:\n                 try:\n@@ -2323,9 +2322,9 @@ def test_can_use_safetensors(self):\n                     )\n \n     def test_load_save_without_tied_weights(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n-        config.tie_word_embeddings = False\n         for model_class in self.all_model_classes:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+            config.tie_word_embeddings = False\n             model = model_class(config)\n             with tempfile.TemporaryDirectory() as d:\n                 model.save_pretrained(d)\n@@ -2373,8 +2372,8 @@ def test_tied_weights_keys(self):\n             )\n \n     def test_model_weights_reload_no_missing_tied_weights(self):\n-        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:\n+            config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n             with tempfile.TemporaryDirectory() as tmp_dir:\n                 model.save_pretrained(tmp_dir)"
        }
    ],
    "stats": {
        "total": 1927,
        "additions": 942,
        "deletions": 985
    }
}