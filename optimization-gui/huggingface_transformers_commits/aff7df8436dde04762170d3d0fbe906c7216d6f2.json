{
    "author": "jiqing-feng",
    "message": "enable static cache on TP model (#39164)\n\n* enable static cache on TP model\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* check tp size before init kv cache\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix docstring\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* add tp tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix comment\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix other cache head size\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "aff7df8436dde04762170d3d0fbe906c7216d6f2",
    "files": [
        {
            "sha": "bfba7e2bbda35fcabcd472c5632fdc79f89a1e84",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/aff7df8436dde04762170d3d0fbe906c7216d6f2/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aff7df8436dde04762170d3d0fbe906c7216d6f2/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=aff7df8436dde04762170d3d0fbe906c7216d6f2",
            "patch": "@@ -1098,6 +1098,10 @@ class StaticCache(Cache):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n             and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n+        tp_size (`Optional[int]`, *optional*):\n+            The tensor parallel size of the model. This is used to adjust the number of key/value heads in the cache\n+            if the model is using tensor parallelism. If not provided, it defaults to `None`, which means that the\n+            number of key/value heads will not be adjusted.\n \n \n     Example:\n@@ -1130,6 +1134,7 @@ def __init__(\n         device: Union[torch.device, str, None] = None,\n         dtype: torch.dtype = torch.float32,\n         layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n+        tp_size: Optional[int] = None,\n     ) -> None:\n         super().__init__()\n         self.max_batch_size = max_batch_size\n@@ -1144,6 +1149,13 @@ def __init__(\n             if getattr(config, \"num_key_value_heads\", None) is None\n             else config.num_key_value_heads\n         )\n+        if tp_size is not None and tp_size > 1:\n+            if self.num_key_value_heads % tp_size != 0:\n+                raise ValueError(\n+                    f\"Number of key value heads {self.num_key_value_heads} must be divisible by tensor parallel size {tp_size}.\"\n+                )\n+            # If the model is using tensor parallelism, we need to adjust the number of heads accordingly.\n+            self.num_key_value_heads //= tp_size\n \n         self.key_cache: list[torch.Tensor] = []\n         self.value_cache: list[torch.Tensor] = []\n@@ -1573,6 +1585,10 @@ class HybridCache(Cache):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n             and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n+        tp_size (`Optional[int]`, *optional*):\n+            The tensor parallel size of the model. This is used to adjust the number of key/value heads in the cache\n+            if the model is using tensor parallelism. If not provided, it defaults to `None`, which means that the\n+            number of key/value heads will not be adjusted.\n \n     Example:\n \n@@ -1604,6 +1620,7 @@ def __init__(\n         device: Union[torch.device, str, None] = None,\n         dtype: torch.dtype = torch.float32,\n         layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n+        tp_size: Optional[int] = None,\n     ) -> None:\n         super().__init__()\n         if not hasattr(config, \"sliding_window\") or config.sliding_window is None:\n@@ -1627,6 +1644,13 @@ def __init__(\n             if getattr(config, \"num_key_value_heads\", None) is None\n             else config.num_key_value_heads\n         )\n+        if tp_size is not None and tp_size > 1:\n+            if self.num_key_value_heads % tp_size != 0:\n+                raise ValueError(\n+                    f\"Number of key value heads {self.num_key_value_heads} must be divisible by tensor parallel size {tp_size}.\"\n+                )\n+            # If the model is using tensor parallelism, we need to adjust the number of heads accordingly.\n+            self.num_key_value_heads //= tp_size\n \n         # If the attribute does not exist in the config, fallback to a simple StaticCache\n         if hasattr(config, \"layer_types\"):\n@@ -2197,6 +2221,10 @@ class OffloadedStaticCache(StaticCache):\n             Mapping between the layers and its device. This is required when you are manually initializing the cache\n             and the model is split between different gpus. You can know which layers mapped to which device by\n             checking the associated device_map: `model.hf_device_map`.\n+        tp_size (`Optional[int]`, *optional*):\n+            The tensor parallel size of the model. This is used to adjust the number of key/value heads in the cache\n+            if the model is using tensor parallelism. If not provided, it defaults to `None`, which means that the\n+            number of key/value heads will not be adjusted.\n \n     Example:\n \n@@ -2228,6 +2256,7 @@ def __init__(\n         dtype: Optional[torch.dtype] = None,\n         offload_device: Union[str, torch.device] = torch.device(\"cpu\"),\n         layer_device_map: Optional[dict[int, Union[str, torch.device, int]]] = None,\n+        tp_size: Optional[int] = None,\n     ) -> None:\n         super(Cache, self).__init__()\n \n@@ -2251,6 +2280,13 @@ def __init__(\n             if getattr(config, \"num_key_value_heads\", None) is None\n             else config.num_key_value_heads\n         )\n+        if tp_size is not None and tp_size > 1:\n+            if num_key_value_heads % tp_size != 0:\n+                raise ValueError(\n+                    f\"Number of key value heads {num_key_value_heads} must be divisible by tensor parallel size {tp_size}.\"\n+                )\n+            # If the model is using tensor parallelism, we need to adjust the number of heads accordingly.\n+            num_key_value_heads //= tp_size\n \n         cache_shape = (max_batch_size, num_key_value_heads, self.max_cache_len, head_dim)\n "
        },
        {
            "sha": "6208945434804a9c631cd0a912a3f965d2d44800",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/aff7df8436dde04762170d3d0fbe906c7216d6f2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aff7df8436dde04762170d3d0fbe906c7216d6f2/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=aff7df8436dde04762170d3d0fbe906c7216d6f2",
            "patch": "@@ -1963,6 +1963,9 @@ def _get_cache(\n                 \"device\": device,\n                 \"layer_device_map\": layer_device_map,\n             }\n+            if cache_implementation in [\"static\", \"hybrid\", \"offloaded_static\"]:\n+                cache_kwargs.update({\"tp_size\": self.tp_size})\n+\n             self._cache = cache_cls(**cache_kwargs)\n             if requires_cross_attention_cache:\n                 encoder_kwargs = cache_kwargs.copy()"
        },
        {
            "sha": "79a2c294c66109e31802506b6ff6d022a43a4a85",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/aff7df8436dde04762170d3d0fbe906c7216d6f2/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aff7df8436dde04762170d3d0fbe906c7216d6f2/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=aff7df8436dde04762170d3d0fbe906c7216d6f2",
            "patch": "@@ -4494,6 +4494,9 @@ def from_pretrained(\n                     raise ValueError(\"device_mesh must be 1 dimensional and will be used for TP\")\n                 device_map = torch.device(device_mesh.device_type, int(os.environ[\"LOCAL_RANK\"]))\n \n+            if tp_size is None:\n+                tp_size = torch.distributed.get_world_size()\n+\n         if use_auth_token is not None:\n             warnings.warn(\n                 \"The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\","
        },
        {
            "sha": "980d6fff8d474c43d4de3ecfe738e88b113f7453",
            "filename": "tests/tensor_parallel/test_tensor_parallel.py",
            "status": "modified",
            "additions": 42,
            "deletions": 1,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/aff7df8436dde04762170d3d0fbe906c7216d6f2/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aff7df8436dde04762170d3d0fbe906c7216d6f2/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftensor_parallel%2Ftest_tensor_parallel.py?ref=aff7df8436dde04762170d3d0fbe906c7216d6f2",
            "patch": "@@ -12,6 +12,8 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+# Run the test: CUDA_VISIBLE_DEVICES=0,1 RUN_SLOW=1 pytest -sv tests/tensor_parallel/test_tensor_parallel.py\n+\n import os\n import subprocess\n import tempfile\n@@ -62,7 +64,6 @@ def size(self):\n         assert torch.allclose(unpacked_weights, original_packed_weights)\n \n \n-# RUN_SLOW=1 pytest -sv tests/tensor_parallel/test_tensor_parallel.py\n class TestTensorParallel(TestCasePlus):\n     nproc_per_node = 2\n \n@@ -125,6 +126,46 @@ def test_model_forward(self):\n         )\n         self.torchrun(script_to_run)\n \n+    def test_model_generate(self):\n+        script_to_run = textwrap.dedent(\n+            \"\"\"\n+            import torch\n+            import os\n+            from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+            model_id = \"JackFram/llama-68m\"\n+\n+            rank = int(os.environ[\"RANK\"])\n+            world_size = int(os.environ[\"WORLD_SIZE\"])\n+\n+            model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=\"auto\", tp_plan=\"auto\")\n+            torch.distributed.barrier()\n+\n+            model.forward = torch.compile(model.forward)\n+\n+            has_dtensor = 0\n+            for name, parameter in model.named_parameters():\n+                if isinstance(parameter.data, torch.distributed.tensor.DTensor):\n+                    has_dtensor = 1\n+                    break\n+\n+            assert has_dtensor == 1, \"TP model must has DTensor\"\n+\n+            tokenizer = AutoTokenizer.from_pretrained(model_id)\n+            prompt = \"Can I help\"\n+\n+            inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n+            outputs = model.generate(inputs, max_new_tokens=10, cache_implementation=\"static\")\n+\n+            output_text = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+            assert output_text[0].startswith(prompt), f\"Expected output to start with '{prompt}', got '{output_text[0]}'\"\n+\n+            torch.distributed.barrier()\n+            torch.distributed.destroy_process_group()\n+            \"\"\"\n+        )\n+        self.torchrun(script_to_run)\n+\n     @require_huggingface_hub_greater_or_equal(\"0.31.4\")\n     def test_model_save(self):\n         from safetensors import safe_open"
        }
    ],
    "stats": {
        "total": 85,
        "additions": 84,
        "deletions": 1
    }
}