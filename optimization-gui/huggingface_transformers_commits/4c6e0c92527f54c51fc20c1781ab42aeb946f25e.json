{
    "author": "Cyrilvallez",
    "message": "Correct the new defaults (#34377)\n\n* Correct the new defaults\r\n\r\n* CIs\r\n\r\n* add check\r\n\r\n* Update utils.py\r\n\r\n* Update utils.py\r\n\r\n* Add the max_length in generate test checking shape without passing length\r\n\r\n* style\r\n\r\n* CIs\r\n\r\n* fix fx CI issue",
    "sha": "4c6e0c92527f54c51fc20c1781ab42aeb946f25e",
    "files": [
        {
            "sha": "efe953db051cb34b89d0966b247af62c74a45405",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c6e0c92527f54c51fc20c1781ab42aeb946f25e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c6e0c92527f54c51fc20c1781ab42aeb946f25e/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=4c6e0c92527f54c51fc20c1781ab42aeb946f25e",
            "patch": "@@ -1440,8 +1440,11 @@ def _prepare_generated_length(\n             and not self.config.is_encoder_decoder\n         ):\n             generation_config.max_length -= inputs_tensor.shape[1]\n-        else:  # by default let's always generate 10 new tokens\n+        elif has_default_max_length:  # by default let's always generate 20 new tokens\n             generation_config.max_length = generation_config.max_length + input_ids_length\n+            max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)\n+            if max_position_embeddings is not None:\n+                generation_config.max_length = min(generation_config.max_length, max_position_embeddings)\n \n         # same for min length\n         if generation_config.min_new_tokens is not None:"
        },
        {
            "sha": "64ebedcb45984b76d1300b496a48c161abde2ef1",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c6e0c92527f54c51fc20c1781ab42aeb946f25e/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c6e0c92527f54c51fc20c1781ab42aeb946f25e/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=4c6e0c92527f54c51fc20c1781ab42aeb946f25e",
            "patch": "@@ -488,7 +488,9 @@ def check_encoder_decoder_model_generate(self, input_ids, config, decoder_config\n \n         # Bert does not have a bos token id, so use pad_token_id instead\n         generated_output = enc_dec_model.generate(\n-            input_ids, decoder_start_token_id=enc_dec_model.config.decoder.pad_token_id\n+            input_ids,\n+            decoder_start_token_id=enc_dec_model.config.decoder.pad_token_id,\n+            max_length=decoder_config.max_length,\n         )\n         self.assertEqual(generated_output.shape, (input_ids.shape[0],) + (decoder_config.max_length,))\n "
        },
        {
            "sha": "7dcb7c406ae287e0f06769c816ac0d888d9eb8df",
            "filename": "tests/models/speech_encoder_decoder/test_modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c6e0c92527f54c51fc20c1781ab42aeb946f25e/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c6e0c92527f54c51fc20c1781ab42aeb946f25e/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py?ref=4c6e0c92527f54c51fc20c1781ab42aeb946f25e",
            "patch": "@@ -362,7 +362,9 @@ def check_encoder_decoder_model_generate(\n \n         # Bert does not have a bos token id, so use pad_token_id instead\n         generated_output = enc_dec_model.generate(\n-            inputs, decoder_start_token_id=enc_dec_model.config.decoder.pad_token_id\n+            inputs,\n+            decoder_start_token_id=enc_dec_model.config.decoder.pad_token_id,\n+            max_length=decoder_config.max_length,\n         )\n         self.assertEqual(generated_output.shape, (inputs.shape[0],) + (decoder_config.max_length,))\n "
        },
        {
            "sha": "77e2a19fea4861a1b705e93d8dc46af998ee49a5",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c6e0c92527f54c51fc20c1781ab42aeb946f25e/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c6e0c92527f54c51fc20c1781ab42aeb946f25e/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py?ref=4c6e0c92527f54c51fc20c1781ab42aeb946f25e",
            "patch": "@@ -306,7 +306,9 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n \n         # Bert does not have a bos token id, so use pad_token_id instead\n         generated_output = enc_dec_model.generate(\n-            inputs, decoder_start_token_id=enc_dec_model.config.decoder.pad_token_id\n+            inputs,\n+            decoder_start_token_id=enc_dec_model.config.decoder.pad_token_id,\n+            max_length=decoder_config.max_length,\n         )\n         self.assertEqual(generated_output.shape, (inputs.shape[0],) + (decoder_config.max_length,))\n \n@@ -873,6 +875,7 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n         generated_output = enc_dec_model.generate(\n             pixel_values=pixel_values,\n             decoder_start_token_id=enc_dec_model.config.decoder.bos_token_id,\n+            max_length=decoder_config.max_length,\n             **kwargs,\n         )\n         self.assertEqual(generated_output.shape, (pixel_values.shape[0],) + (decoder_config.max_length,))\n@@ -990,6 +993,7 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n         generated_output = enc_dec_model.generate(\n             pixel_values=pixel_values,\n             decoder_start_token_id=enc_dec_model.config.decoder.bos_token_id,\n+            max_length=decoder_config.max_length,\n             **kwargs,\n         )\n         self.assertEqual(generated_output.shape, (pixel_values.shape[0],) + (decoder_config.max_length,))\n@@ -1107,6 +1111,7 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n         generated_output = enc_dec_model.generate(\n             pixel_values=pixel_values,\n             decoder_start_token_id=enc_dec_model.config.decoder.bos_token_id,\n+            max_length=decoder_config.max_length,\n             **kwargs,\n         )\n         self.assertEqual(generated_output.shape, (pixel_values.shape[0],) + (decoder_config.max_length,))"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 16,
        "deletions": 4
    }
}