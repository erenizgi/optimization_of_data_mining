{
    "author": "manueldeprada",
    "message": "Fix CI: Tests failing on CPU due to `torch.device('cpu').index` being None (#39933)\n\nreplace routing_weights.device.index with a",
    "sha": "910b3193575049172c49f9d1de09603421084d29",
    "files": [
        {
            "sha": "afa66faa27023784d6caa9333c9f7f8a1d56ef20",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=910b3193575049172c49f9d1de09603421084d29",
            "patch": "@@ -119,7 +119,8 @@ def load_balancing_loss_func(\n             router_per_expert_attention_mask, dim=0\n         )\n \n-    rank = routing_weights.shape[1] * int(routing_weights.device.index)\n+    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n+    rank = routing_weights.shape[1] * int(device_index)\n     overall_loss = torch.sum(\n         tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n     )"
        },
        {
            "sha": "404b98fee392d151003b232d45aabbcdacdbee8d",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=910b3193575049172c49f9d1de09603421084d29",
            "patch": "@@ -1647,7 +1647,8 @@ def load_balancing_loss_func(\n             router_per_expert_attention_mask, dim=0\n         )\n \n-    rank = routing_weights.shape[1] * int(routing_weights.device.index)\n+    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n+    rank = routing_weights.shape[1] * int(device_index)\n     overall_loss = torch.sum(\n         tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n     )"
        },
        {
            "sha": "c17dfb03991a5e4d98c0b71c700ba9ce6e785ee3",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=910b3193575049172c49f9d1de09603421084d29",
            "patch": "@@ -918,7 +918,8 @@ def load_balancing_loss_func(\n             router_per_expert_attention_mask, dim=0\n         )\n \n-    rank = routing_weights.shape[1] * int(routing_weights.device.index)\n+    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n+    rank = routing_weights.shape[1] * int(device_index)\n     overall_loss = torch.sum(\n         tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n     )"
        },
        {
            "sha": "6db0a49dc6123dac289dcd06a6ccfa163fb206cc",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=910b3193575049172c49f9d1de09603421084d29",
            "patch": "@@ -148,7 +148,8 @@ def load_balancing_loss_func(\n             router_per_expert_attention_mask, dim=0\n         )\n \n-    rank = routing_weights.shape[1] * int(routing_weights.device.index)\n+    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n+    rank = routing_weights.shape[1] * int(device_index)\n     overall_loss = torch.sum(\n         tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n     )"
        },
        {
            "sha": "c0651df222c175dbf0365fa168649a3d8386672b",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=910b3193575049172c49f9d1de09603421084d29",
            "patch": "@@ -129,7 +129,8 @@ def load_balancing_loss_func(\n             router_per_expert_attention_mask, dim=0\n         )\n \n-    rank = routing_weights.shape[1] * int(routing_weights.device.index)\n+    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n+    rank = routing_weights.shape[1] * int(device_index)\n     overall_loss = torch.sum(\n         tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n     )"
        },
        {
            "sha": "842845b96e8519b48b766d7740c323b5e7beea22",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=910b3193575049172c49f9d1de09603421084d29",
            "patch": "@@ -118,7 +118,8 @@ def load_balancing_loss_func(\n             router_per_expert_attention_mask, dim=0\n         )\n \n-    rank = routing_weights.shape[1] * int(routing_weights.device.index)\n+    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n+    rank = routing_weights.shape[1] * int(device_index)\n     overall_loss = torch.sum(\n         tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n     )"
        },
        {
            "sha": "d46df0479577073dc2e00bffc365044c51aba55e",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=910b3193575049172c49f9d1de09603421084d29",
            "patch": "@@ -134,7 +134,8 @@ def load_balancing_loss_func(\n             router_per_expert_attention_mask, dim=0\n         )\n \n-    rank = routing_weights.shape[1] * int(routing_weights.device.index)\n+    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n+    rank = routing_weights.shape[1] * int(device_index)\n     overall_loss = torch.sum(\n         tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n     )"
        },
        {
            "sha": "9a6dc490c045eda453d79712fd46bc7ef9432e2c",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/910b3193575049172c49f9d1de09603421084d29/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=910b3193575049172c49f9d1de09603421084d29",
            "patch": "@@ -137,7 +137,8 @@ def load_balancing_loss_func(\n             router_per_expert_attention_mask, dim=0\n         )\n \n-    rank = routing_weights.shape[1] * int(routing_weights.device.index)\n+    device_index = routing_weights.device.index if routing_weights.device.index is not None else 0\n+    rank = routing_weights.shape[1] * int(device_index)\n     overall_loss = torch.sum(\n         tokens_per_expert[:, rank : rank + routing_weights.shape[1]] * router_prob_per_expert.unsqueeze(0)\n     )"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 16,
        "deletions": 8
    }
}