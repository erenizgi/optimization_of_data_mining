{
    "author": "cyyever",
    "message": "Add language specifiers to code blocks of markdown files (#41114)\n\n* Add language specifiers to code blocks of markdown files\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Update docs/source/en/model_doc/qwen3_omni_moe.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/chat_templating_writing.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/chat_templating_writing.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/chat_templating_writing.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* More fixes\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n* Update nemotron.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update phimoe.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update README.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Fix syntax error\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "449533af73874470e914a203391635e04ac2ffc8",
    "files": [
        {
            "sha": "d8cf3dfda3b7d05b42667d7d0ede36c00ddd3617",
            "filename": "docs/source/en/chat_templating_multimodal.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_multimodal.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -56,7 +56,7 @@ out = pipe(text=messages, max_new_tokens=128)\n print(out[0]['generated_text'][-1]['content'])\n ```\n \n-```\n+```text\n Ahoy, me hearty! These be two feline friends, likely some tabby cats, taking a siesta on a cozy pink blanket. They're resting near remote controls, perhaps after watching some TV or just enjoying some quiet time together. Cats sure know how to find comfort and relaxation, don't they?\n ```\n \n@@ -96,7 +96,7 @@ processed_chat = processor.apply_chat_template(messages, add_generation_prompt=T\n print(list(processed_chat.keys()))\n ```\n \n-```\n+```text\n ['input_ids', 'attention_mask', 'pixel_values', 'image_grid_thw']\n ```\n "
        },
        {
            "sha": "8df0c5e671f3bab6b02ad934b9487ba286ade452",
            "filename": "docs/source/en/chat_templating_writing.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fchat_templating_writing.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fchat_templating_writing.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fchat_templating_writing.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -188,7 +188,7 @@ The example below shows how a tool is defined in JSON schema format.\n \n An example of handling tool definitions in a chat template is shown below. The specific tokens and layouts should be changed to match the ones the model was trained with.\n \n-```\n+```jinja\n {%- if tools %}\n     {%- for tool in tools %}\n         {{- '<tool>' + tool['function']['name'] + '\\n' }}\n@@ -226,7 +226,7 @@ Tool calls are generally passed in the `tool_calls` key of an `\"assistant‚Äù` me\n \n A common pattern for handling tool calls is shown below. You can use this as a starting point, but make sure you template actually matches the format the model was trained with!\n \n-```\n+```jinja\n {%- if message['role'] == 'assistant' and 'tool_calls' in message %}\n     {%- for tool_call in message['tool_calls'] %}\n             {{- '<tool_call>' + tool_call['function']['name'] + '\\n' + tool_call['function']['arguments']|tojson + '\\n</tool_call>' }}\n@@ -249,7 +249,7 @@ Tool responses are message dicts with the `tool` role. They are much simpler tha\n \n Some templates may not even need the `name` key, in which case, you can write your template to only read the `content` key.\n \n-```\n+```jinja\n {%- if message['role'] == 'tool' %}\n     {{- \"<tool_result>\" + message['content'] + \"</tool_result>\" }}\n {%- endif %}"
        },
        {
            "sha": "bea40c282dee9f855259e902035b6fb890b09370",
            "filename": "docs/source/en/debugging.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fdebugging.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fdebugging.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdebugging.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -45,7 +45,7 @@ which nvcc\n \n You may also have more than one CUDA toolkit installed on your system.\n \n-```bash\n+```text\n /usr/local/cuda-10.2\n /usr/local/cuda-11.0\n ```"
        },
        {
            "sha": "642cc8a42d98aad574617435748ec9b1cc84281d",
            "filename": "docs/source/en/deepspeed.md",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdeepspeed.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -294,7 +294,7 @@ Consider running a [benchmark](https://github.com/microsoft/DeepSpeed/issues/998\n \n The example ZeRO-3 and ZeRO-Infinity config below sets most of the parameter values to `auto`, but you can also manually set configure these values.\n \n-```yaml\n+```json\n {\n     \"fp16\": {\n         \"enabled\": \"auto\",\n@@ -383,7 +383,7 @@ Gradient checkpointing saves memory by only storing *some* of the intermediate a\n \n The batch size can be automatically configured or manually set. When you choose the `\"auto\"` option, [`Trainer`] sets `train_micro_batch_size_per_gpu` and `train_batch_size` to the value of `world_size * per_device_train_batch_size * gradient_accumulation_steps`.\n \n-```yaml\n+```json\n {\n     \"train_micro_batch_size_per_gpu\": \"auto\",\n     \"train_batch_size\": \"auto\"\n@@ -400,7 +400,7 @@ Reduce operations are lossy, for example, when gradients are averaged across mul\n \n Choose the communication data type by setting the `communication_data_type` parameter in the config file. For example, choosing fp32 adds a small amount of overhead but ensures the reduction operation is accumulated in fp32 and when it is ready, it's downcasted to whichever half-precision data type you're training in.\n \n-```yaml\n+```json\n {\n     \"communication_data_type\": \"fp32\"\n }\n@@ -412,7 +412,7 @@ Gradient accumulation accumulates gradients over several mini-batches of data be\n \n Gradient accumulation can be automatically configured or manually set. When you choose the `\"auto\"` option, [`Trainer`] sets it to the value of `gradient_accumulation_steps`.\n \n-```yaml\n+```json\n {\n     \"gradient_accumulation_steps\": \"auto\"\n }\n@@ -424,7 +424,7 @@ Gradient clipping is useful for preventing exploding gradients which can lead to\n \n Gradient clipping can be automatically configured or manually set. When you choose the `\"auto\"` option, [`Trainer`] sets it to the value of `max_grad_norm`.\n \n-```yaml\n+```json\n {\n     \"gradient_clipping\": \"auto\"\n }\n@@ -439,7 +439,7 @@ Mixed precision accelerates training speed by performing some calculations in ha\n \n Train in fp32 if a model wasn't pretrained in mixed precision because it may cause underflow or overflow errors. Disable fp16, the default, in this case.\n \n-```yaml\n+```json\n {\n     \"fp16\": {\n         \"enabled\": false\n@@ -454,7 +454,7 @@ For Ampere GPUs and PyTorch 1.7+, the more efficient [tf32](https://pytorch.org/\n \n To configure AMP-like fp16 mixed precision, set up the config as shown below with `\"auto\"` or your own values. [`Trainer`] automatically enables or disables fp16 based on the value of `fp16_backend`, and the rest of the config can be set by you. fp16 is enabled from the command line when the following arguments are passed: `--fp16`, `--fp16_backend amp` or `--fp16_full_eval`.\n \n-```yaml\n+```json\n {\n     \"fp16\": {\n         \"enabled\": \"auto\",\n@@ -471,7 +471,7 @@ For additional DeepSpeed fp16 training options, take a look at the [FP16 Trainin\n \n To configure Apex-like fp16 mixed precision, set up the config as shown below with `\"auto\"` or your own values. [`Trainer`] automatically configures `amp` based on the values of `fp16_backend` and `fp16_opt_level`. It can also be enabled from the command line when the following arguments are passed: `--fp16`, `--fp16_backend apex` or `--fp16_opt_level 01`.\n \n-```yaml\n+```json\n {\n     \"amp\": {\n         \"enabled\": \"auto\",\n@@ -490,7 +490,7 @@ bf16 has the same dynamic range as fp32, and doesn't require loss scaling unlike\n \n bf16 can be set up in the config file or enabled from the command line when the following arguments are passed: `--bf16` or `--bf16_full_eval`.\n \n-```yaml\n+```json\n {\n     \"bf16\": {\n         \"enabled\": \"auto\"\n@@ -514,7 +514,7 @@ DeepSpeed offers several [optimizers](https://www.deepspeed.ai/docs/config-json/\n \n You can set the parameters to `\"auto\"` or manually input your own values.\n \n-```yaml\n+```json\n {\n    \"optimizer\": {\n        \"type\": \"AdamW\",\n@@ -530,15 +530,15 @@ You can set the parameters to `\"auto\"` or manually input your own values.\n \n Use an unsupported optimizer by adding the following to the top level configuration.\n \n-```yaml\n+```json\n {\n    \"zero_allow_untested_optimizer\": true\n }\n ```\n \n From DeepSpeed 0.8.3+, if you want to use offload, you'll also need to add the following to the top level configuration because offload works best with DeepSpeed's CPU Adam optimizer.\n \n-```yaml\n+```json\n {\n    \"zero_force_ds_cpu_optimizer\": false\n }\n@@ -558,7 +558,7 @@ If you don't configure the scheduler in the config file, [`Trainer`] automatical\n \n You can set the parameters to `\"auto\"` or manually input your own values.\n \n-```yaml\n+```json\n {\n    \"scheduler\": {\n          \"type\": \"WarmupDecayLR\",\n@@ -581,7 +581,7 @@ You can set the parameters to `\"auto\"` or manually input your own values.\n \n Resume training with a Universal checkpoint by setting `load_universal` to `true` in the config file.\n \n-```yaml\n+```json\n {\n     \"checkpoint\": {\n         \"load_universal\": true\n@@ -640,7 +640,7 @@ deepspeed --num_gpus=1 examples/pytorch/translation/run_translation.py \\\n \n A multi-node setup consists of multiple nodes, where each node has one of more GPUs running a workload. DeepSpeed expects a shared storage system, but if this is not the case, you need to adjust the config file to include a [checkpoint](https://www.deepspeed.ai/docs/config-json/#checkpoint-options) to allow loading without access to a shared filesystem.\n \n-```yaml\n+```json\n {\n   \"checkpoint\": {\n     \"use_node_local_storage\": true\n@@ -824,7 +824,7 @@ ZeRO-2 saves the model weights in fp16. To save the weights in fp16 for ZeRO-3,\n \n If you don't, [`Trainer`] won't save the weights in fp16 and won't create a `pytorch_model.bin` file. This is because DeepSpeed's state_dict contains a placeholder instead of the real weights, so you won't be able to load it.\n \n-```yaml\n+```json\n {\n     \"zero_optimization\": {\n         \"stage\": 3,\n@@ -986,7 +986,7 @@ NaN loss often occurs when a model is pretrained in bf16 and you try to use it w\n \n It is also possible that fp16 is causing overflow. For example, if your config file looks like the one below, you may see the following overflow errors in the logs.\n \n-```yaml\n+```json\n {\n     \"fp16\": {\n         \"enabled\": \"auto\","
        },
        {
            "sha": "7123896dd1a717cac56f8e567aeee2477313121e",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -289,7 +289,7 @@ print(tokenizer.batch_decode(gen_out)[0])\n \n If the custom method has pinned Python requirements that your environment doesn't meet, you'll get an exception about missing requirements. For instance, [transformers-community/custom_generate_bad_requirements](https://huggingface.co/transformers-community/custom_generate_bad_requirements) has an impossible set of requirements defined in its `custom_generate/requirements.txt` file, and you'll see the error message below if you try to run it.\n \n-```\n+```text\n ImportError: Missing requirements in your local environment for `transformers-community/custom_generate_bad_requirements`:\n foo (installed: None)\n bar==0.0.0 (installed: None)\n@@ -308,7 +308,7 @@ To create a new generation method, you need to create a new [**Model**](https://\n \n After you've added all required files, your repository should look like this\n \n-```\n+```text\n your_repo/\n ‚îú‚îÄ‚îÄ README.md          # include the 'custom_generate' tag\n ‚îú‚îÄ‚îÄ config.json\n@@ -399,7 +399,7 @@ The root level `README.md` in the model repository usually describes the model t\n \n For discoverability, we highly recommend you to add the `custom_generate` tag to your repository. To do so, the top of your `README.md` file should look like the example below. After you push the file, you should see the tag in your repository!\n \n-```\n+```text\n ---\n library_name: transformers\n tags:"
        },
        {
            "sha": "8f0d0b15b633b9f246538f30df3a93c6ba73b228",
            "filename": "docs/source/en/internal/model_debugging_utils.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -279,7 +279,7 @@ python utils/scan_skipped_tests.py --output_dir path/to/output\n \n **Example output:**\n \n-```\n+```text\n üî¨ Parsing 331 model test files once each...\n üìù Aggregating 224 tests...\n   (224/224) test_update_candidate_strategy_with_matches_1es_3d_is_nonecodet_schedule_fa_kwargs"
        },
        {
            "sha": "95309f46cd04f10944f8e3b38e1cf143edfb9092",
            "filename": "docs/source/en/jan.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fjan.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fjan.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fjan.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -25,7 +25,7 @@ You are now ready to chat!\n \n To conclude this example, let's look into a more advanced use-case. If you have a beefy machine to serve models with, but prefer using Jan on a different device, you need to add port forwarding. If you have `ssh` access from your Jan machine into your server, this can be accomplished by typing the following to your Jan machine's terminal\n \n-```\n+```bash\n ssh -N -f -L 8000:localhost:8000 your_server_account@your_server_IP -p port_to_ssh_into_your_server\n ```\n "
        },
        {
            "sha": "d3095055472c6ac8b2e7801c0ee38ffcd38661d9",
            "filename": "docs/source/en/llm_tutorial_optimization.md",
            "status": "modified",
            "additions": 15,
            "deletions": 15,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial_optimization.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -100,7 +100,7 @@ result\n \n **Output**:\n \n-```\n+```text\n Here is a Python function that transforms bytes to Giga bytes:\\n\\n```python\\ndef bytes_to_giga_bytes(bytes):\\n    return bytes / 1024 / 1024 / 1024\\n```\\n\\nThis function takes a single\n ```\n \n@@ -119,7 +119,7 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n \n **Output**:\n \n-```bash\n+```text\n 29.0260648727417\n ```\n \n@@ -208,7 +208,7 @@ result\n \n **Output**:\n \n-```\n+```text\n Here is a Python function that transforms bytes to Giga bytes:\\n\\n```python\\ndef bytes_to_giga_bytes(bytes):\\n    return bytes / 1024 / 1024 / 1024\\n```\\n\\nThis function takes a single\n ```\n \n@@ -220,7 +220,7 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n \n **Output**:\n \n-```\n+```text\n 15.219234466552734\n ```\n \n@@ -251,7 +251,7 @@ result\n \n **Output**:\n \n-```\n+```text\n Here is a Python function that transforms bytes to Giga bytes:\\n\\n```\\ndef bytes_to_gigabytes(bytes):\\n    return bytes / 1024 / 1024 / 1024\\n```\\n\\nThis function takes a single argument\n ```\n \n@@ -263,7 +263,7 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n \n **Output**:\n \n-```\n+```text\n 9.543574333190918\n ```\n \n@@ -423,7 +423,7 @@ result\n \n **Output**:\n \n-```\n+```text\n Generated in 10.96854019165039 seconds.\n Sure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n ````\n@@ -440,7 +440,7 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n \n **Output**:\n \n-```bash\n+```text\n 37.668193340301514\n ```\n \n@@ -472,7 +472,7 @@ result\n \n **Output**:\n \n-```\n+```text\n Generated in 3.0211617946624756 seconds.\n  Sure. Here is a function that does that.\\n\\ndef bytes_to_giga(bytes):\\n   return bytes / 1024 / 1024 / 1024\\n\\nAnswer: Sure. Here is a function that does that.\\n\\ndef\n ```\n@@ -487,7 +487,7 @@ bytes_to_giga_bytes(torch.cuda.max_memory_allocated())\n \n **Output**:\n \n-```\n+```text\n 32.617331981658936\n ```\n \n@@ -618,7 +618,7 @@ generated_text\n \n **Output**:\n \n-```\n+```text\n shape of input_ids torch.Size([1, 21])\n shape of input_ids torch.Size([1, 22])\n shape of input_ids torch.Size([1, 23])\n@@ -656,7 +656,7 @@ generated_text\n \n **Output**:\n \n-```\n+```text\n shape of input_ids torch.Size([1, 1])\n length of key-value cache 20\n shape of input_ids torch.Size([1, 1])\n@@ -690,7 +690,7 @@ Note that, despite our advice to use key-value caches, your LLM output may be sl\n \n The key-value cache is especially useful for applications such as chat where multiple passes of auto-regressive decoding are required. Let's look at an example.\n \n-```\n+```text\n User: How many people live in France?\n Assistant: Roughly 75 million people live in France\n User: And how many are in Germany?\n@@ -728,7 +728,7 @@ tokenizer.batch_decode(generation_output.sequences)[0][len(prompt):]\n \n **Output**:\n \n-```\n+```text\n  is a modified version of the function that returns Mega bytes instead.\n \n def bytes_to_megabytes(bytes):\n@@ -750,7 +750,7 @@ config = model.config\n \n **Output**:\n \n-```\n+```text\n 7864320000\n ```\n "
        },
        {
            "sha": "2a63deeba37853b3781d5c378f9c203dca2822c3",
            "filename": "docs/source/en/main_classes/pipelines.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmain_classes%2Fpipelines.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -159,7 +159,7 @@ for batch_size in [1, 8, 64, 256]:\n         pass\n ```\n \n-```\n+```text\n # On GTX 970\n ------------------------------\n Streaming no batching\n@@ -195,7 +195,7 @@ This is a occasional very long sentence compared to the other. In that case, the\n tokens long, so the whole batch will be [64, 400] instead of [64, 4], leading to the high slowdown. Even worse, on\n bigger batches, the program simply crashes.\n \n-```\n+```text\n ------------------------------\n Streaming no batching\n 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1000/1000 [00:05<00:00, 183.69it/s]"
        },
        {
            "sha": "bced0a4b2bcc1906e61866767cb2137175fcc78c",
            "filename": "docs/source/en/model_doc/audio-spectrogram-transformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faudio-spectrogram-transformer.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -61,7 +61,7 @@ page for more information.\n SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n-```\n+```py\n from transformers import ASTForAudioClassification\n model = ASTForAudioClassification.from_pretrained(\"MIT/ast-finetuned-audioset-10-10-0.4593\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ..."
        },
        {
            "sha": "5158bafa395ab49a96d00ef97d87f44e6e48c03f",
            "filename": "docs/source/en/model_doc/beit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbeit.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -87,7 +87,7 @@ page for more information.\n SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n-```\n+```py\n from transformers import BeitForImageClassification\n model = BeitForImageClassification.from_pretrained(\"microsoft/beit-base-patch16-224\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ..."
        },
        {
            "sha": "e3262f140f4d5edfd2fa1d91741ad48f3034c46c",
            "filename": "docs/source/en/model_doc/dab-detr.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdab-detr.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -80,7 +80,7 @@ for result in results:\n \n This should output\n \n-```\n+```text\n cat: 0.87 [14.7, 49.39, 320.52, 469.28]\n remote: 0.86 [41.08, 72.37, 173.39, 117.2]\n cat: 0.86 [344.45, 19.43, 639.85, 367.86]"
        },
        {
            "sha": "5c12b2f69dbbb85a0f3eaf7babfde90c10305ed1",
            "filename": "docs/source/en/model_doc/data2vec.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdata2vec.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -68,7 +68,7 @@ SDPA is used by default for `torch>=2.1.1` when an implementation is available,\n \n The SDPA implementation is currently available for the Data2VecAudio and Data2VecVision models.\n \n-```\n+```py\n from transformers import Data2VecVisionForImageClassification\n model = Data2VecVisionForImageClassification.from_pretrained(\"facebook/data2vec-vision-base\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ..."
        },
        {
            "sha": "2f61408a79cd55b4a999f37d934913660313750d",
            "filename": "docs/source/en/model_doc/deepseek_v3.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_v3.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -64,7 +64,7 @@ print(time.time()-start)\n \n This generated:\n \n-``````\n+``````text\n <ÔΩúAssistantÔΩú><think>\n Okay, the user wants to demonstrate how chat templating works. Let me break down what that means. Chat templating is about structuring the conversation data, especially for models that need specific input formats. Maybe they're referring to something like how messages are formatted with roles (user, assistant, system) in APIs like OpenAI.\n \n@@ -195,4 +195,4 @@ error, it means NCCL was probably not loaded.\n ## DeepseekV3ForTokenClassification\n \n [[autodoc]] DeepseekV3ForTokenClassification\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "185a741d5b44a8ea83e9542278f3990be5e22226",
            "filename": "docs/source/en/model_doc/deit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeit.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -86,7 +86,7 @@ page for more information.\n SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n-```\n+```py\n from transformers import DeiTForImageClassification\n model = DeiTForImageClassification.from_pretrained(\"facebook/deit-base-distilled-patch16-224\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ..."
        },
        {
            "sha": "f9189b5d3a207ea6152fbef2eb7fd0af528f4fd5",
            "filename": "docs/source/en/model_doc/gemma2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma2.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -80,7 +80,7 @@ print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n </hfoption>\n <hfoption id=\"transformers CLI\">\n \n-```\n+```bash\n echo -e \"Explain quantum computing simply.\" | transformers run --task text-generation --model google/gemma-2-2b --device 0\n ```\n "
        },
        {
            "sha": "2740bfb33393b34f625ab00a7185d79b2d5ef242",
            "filename": "docs/source/en/model_doc/gpt2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -75,7 +75,7 @@ echo -e \"Hello, I'm a language model\" | transformers run --task text-generation\n \n One can also serve the model using vLLM with the `transformers backend`.\n \n-```\n+```bash\n vllm serve openai-community/gpt2 --model-imp transformers\n ```\n "
        },
        {
            "sha": "f43aba5d0cba7a6e83245af4549d70939c706308",
            "filename": "docs/source/en/model_doc/modernbert-decoder.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -119,7 +119,7 @@ print(f\"Prediction probabilities: {predictions}\")\n \n <hfoption id=\"AutoModel (w/quantization)\">\n \n-```\n+```py\n import torch\n from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n "
        },
        {
            "sha": "9379cfe8bf0b1ca29553e1b0a476359f8d6ef1fb",
            "filename": "docs/source/en/model_doc/musicgen_melody.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -58,7 +58,7 @@ The model can generate an audio sample conditioned on a text and an audio prompt\n \n In the following examples, we load an audio file using the ü§ó Datasets library, which can be pip installed through the command below:\n \n-```\n+```bash\n pip install --upgrade pip\n pip install datasets[audio]\n ```\n@@ -289,4 +289,4 @@ Tips:\n ## MusicgenMelodyForConditionalGeneration\n \n [[autodoc]] MusicgenMelodyForConditionalGeneration\n-    - forward\n\\ No newline at end of file\n+    - forward"
        },
        {
            "sha": "50f6f99eae2f7ba9de140921a080118d9201bebf",
            "filename": "docs/source/en/model_doc/nemotron.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fnemotron.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -109,7 +109,7 @@ Please refer to our [paper](https://huggingface.co/papers/2407.14679) for the fu\n \n If you find our work helpful, please consider citing our paper:\n \n-```\n+```bibtex\n @article{minitron2024,\n       title={Compact Language Models via Pruning and Knowledge Distillation},\n       author={Saurav Muralidharan and Sharath Turuvekere Sreenivas and Raviraj Joshi and Marcin Chochowski and Mostofa Patwary and Mohammad Shoeybi and Bryan Catanzaro and Jan Kautz and Pavlo Molchanov},"
        },
        {
            "sha": "609f56c488ba9c3173d88ace8836574ac2e3bb02",
            "filename": "docs/source/en/model_doc/phimoe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphimoe.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -51,7 +51,7 @@ The current `transformers` version can be verified with: `pip list | grep transf\n \n Examples of required packages:\n \n-```\n+```bash\n flash_attn==2.5.8\n torch==2.3.1\n accelerate==0.31.0"
        },
        {
            "sha": "e2e0dc348a1c2f23dacaa93f841aa9ba2bb58df8",
            "filename": "docs/source/en/model_doc/qwen2_5_omni.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -273,7 +273,7 @@ processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", min_pixels=min\n #### Prompt for audio output\n If users need audio output, the system prompt must be set as \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\", otherwise the audio output may not work as expected.\n \n-```\n+```python\n {\n     \"role\": \"system\",\n     \"content\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\","
        },
        {
            "sha": "8e9539291199ee8322ee101c54522357d3855dc3",
            "filename": "docs/source/en/model_doc/qwen3_omni_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -273,7 +273,7 @@ processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\", min_pixels=min\n #### Prompt for audio output\n If users need audio output, the system prompt must be set as \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\", otherwise the audio output may not work as expected.\n \n-```\n+```json\n {\n     \"role\": \"system\",\n     \"content\": \"You are Qwen, a virtual human developed by the Qwen Team, Alibaba Group, capable of perceiving auditory and visual inputs, as well as generating text and speech.\","
        },
        {
            "sha": "80880cf6559dbe1a0be1113ba41b2167b1b86e8c",
            "filename": "docs/source/en/model_doc/t5gemma.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Ft5gemma.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -85,7 +85,7 @@ print(tokenizer.decode(outputs[0]))\n </hfoption>\n <hfoption id=\"transformers CLI\">\n \n-```\n+```bash\n echo -e \"Write me a poem about Machine Learning. Answer:\" | transformers run --task text2text-generation --model google/t5gemma-2b-2b-prefixlm --device 0\n ```\n "
        },
        {
            "sha": "a3ae5309411fa7a77f26745501be62f90918b197",
            "filename": "docs/source/en/model_doc/vaultgemma.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvaultgemma.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -81,7 +81,7 @@ print(tokenizer.decode(outputs[0]))\n </hfoption>\n <hfoption id=\"transformers CLI\">\n \n-```\n+```bash\n echo -e \"Write me a poem about Machine Learning. Answer:\" | transformers run --task text2text-generation --model google/vaultgemma-1b-pt --device 0\n ```\n "
        },
        {
            "sha": "590011c734562dfe0ae2e48d2a5b9a451b5b7ddb",
            "filename": "docs/source/en/model_doc/videomae.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideomae.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -51,7 +51,7 @@ page for more information.\n SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n-```\n+```py\n from transformers import VideoMAEForVideoClassification\n model = VideoMAEForVideoClassification.from_pretrained(\"MCG-NJU/videomae-base-finetuned-kinetics\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ..."
        },
        {
            "sha": "c10d1c489b76784b0352c52aa0504441b6f27d4a",
            "filename": "docs/source/en/model_doc/vit_hybrid.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_hybrid.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -64,7 +64,7 @@ page for more information.\n SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n-```\n+```py\n from transformers import ViTHybridForImageClassification\n model = ViTHybridForImageClassification.from_pretrained(\"google/vit-hybrid-base-bit-384\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ..."
        },
        {
            "sha": "d7a8172a18f36fbdfb92633024b246ed8f389d84",
            "filename": "docs/source/en/model_doc/vit_msn.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvit_msn.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -67,7 +67,7 @@ page for more information.\n SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n-```\n+```py\n from transformers import ViTMSNForImageClassification\n model = ViTMSNForImageClassification.from_pretrained(\"facebook/vit-msn-base\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ..."
        },
        {
            "sha": "fc127fa6f59567d86db18a64c34c636028545782",
            "filename": "docs/source/en/model_doc/vivit.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvivit.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -41,7 +41,7 @@ page for more information.\n SDPA is used by default for `torch>=2.1.1` when an implementation is available, but you may also set\n `attn_implementation=\"sdpa\"` in `from_pretrained()` to explicitly request SDPA to be used.\n \n-```\n+```py\n from transformers import VivitModel\n model = VivitModel.from_pretrained(\"google/vivit-b-16x2-kinetics400\", attn_implementation=\"sdpa\", dtype=torch.float16)\n ..."
        },
        {
            "sha": "f0a215b05c1be52378d8ebb7e7aa8eb1856133cf",
            "filename": "docs/source/en/model_memory_anatomy.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_memory_anatomy.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -112,7 +112,7 @@ as with `nvidia-smi` CLI:\n nvidia-smi\n ```\n \n-```bash\n+```text\n Tue Jan 11 08:58:05 2022\n +-----------------------------------------------------------------------------+\n | NVIDIA-SMI 460.91.03    Driver Version: 460.91.03    CUDA Version: 11.2     |\n@@ -172,7 +172,7 @@ Let's use the [`Trainer`] and train the model without using any GPU performance\n >>> print_summary(result)\n ```\n \n-```\n+```text\n Time: 57.82\n Samples/second: 8.86\n GPU memory occupied: 14949 MB."
        },
        {
            "sha": "17001cc81ee975cc97a31eaa7a81b94e6993e346",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -82,7 +82,7 @@ class RobertaForMaskedLM(BertForMaskedLM):\n \n If you don't use the defined dependency, you'll receive the following error.\n \n-```\n+```text\n ValueError: You defined `RobertaEmbeddings` in the modular_roberta.py, it should be used when you define `BertModel`, as it is one of it's direct dependencies. Make sure you use it in the `__init__` function.\n ```\n "
        },
        {
            "sha": "0e5140d731ecf785120c1041e0de3757e3dfb906",
            "filename": "docs/source/en/perf_train_gaudi.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fperf_train_gaudi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fperf_train_gaudi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_gaudi.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -21,14 +21,14 @@ The Intel Gaudi AI accelerator family includes [Intel Gaudi 1](https://habana.ai\n \n Some modeling code in Transformers is not optimized for HPU lazy mode. If you encounter any errors, set the environment variable below to use eager mode:\n \n-```\n-PT_HPU_LAZY_MODE=0\n+```bash\n+export PT_HPU_LAZY_MODE=0\n ```\n \n In some cases, you'll also need to enable int64 support to avoid casting issues with long integers:\n \n-```\n-PT_ENABLE_INT64_SUPPORT=1\n+```bash\n+export PT_ENABLE_INT64_SUPPORT=1\n ```\n \n Refer to the [Gaudi docs](https://docs.habana.ai/en/latest/index.html) for more details."
        },
        {
            "sha": "1fefe08d5ca9b109dc0db323f3c4c4743ca72880",
            "filename": "docs/source/en/serialization.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fserialization.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Fserialization.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fserialization.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -51,7 +51,7 @@ optimum-cli export onnx --model distilbert/distilbert-base-uncased-distilled-squ\n \n You should see logs indicating the progress and showing where the resulting `model.onnx` is saved.\n \n-```bash\n+```text\n Validating ONNX model distilbert_base_uncased_squad_onnx/model.onnx...\n \t-[‚úì] ONNX model output names match reference model (start_logits, end_logits)\n \t- Validating ONNX Model output \"start_logits\":"
        },
        {
            "sha": "645496d671bbe2a97669a1fe0b0a124ad3524209",
            "filename": "docs/source/en/tasks/image_to_image.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -102,7 +102,7 @@ with torch.no_grad():\n \n Output is an object of type `ImageSuperResolutionOutput` that looks like below üëá\n \n-```\n+```text\n (loss=None, reconstruction=tensor([[[[0.8270, 0.8269, 0.8275,  ..., 0.7463, 0.7446, 0.7453],\n           [0.8287, 0.8278, 0.8283,  ..., 0.7451, 0.7448, 0.7457],\n           [0.8280, 0.8273, 0.8269,  ..., 0.7447, 0.7446, 0.7452],"
        },
        {
            "sha": "7183c308c27a047e7856f9fd3d8d0795f61871ab",
            "filename": "docs/source/en/tasks/keypoint_matching.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Ftasks%2Fkeypoint_matching.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Ftasks%2Fkeypoint_matching.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fkeypoint_matching.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -69,7 +69,7 @@ print(outputs)\n \n Here's the outputs.\n \n-```\n+```text\n [{'keypoints0': tensor([[4514,  550],\n           [4813,  683],\n           [1972, 1547],"
        },
        {
            "sha": "7a88f61e5fc0fbffc4fdea7e6ff7b26fd124f6a9",
            "filename": "docs/source/en/tasks/visual_document_retrieval.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -38,10 +38,10 @@ from datasets import load_dataset\n dataset = load_dataset(\"davanstrien/ufo-ColPali\")\n dataset = dataset[\"train\"]\n dataset = dataset.filter(lambda example: example[\"specific_detail_query\"] is not None)\n-dataset\n+print(dataset)\n ```\n \n-```\n+```text\n Dataset({\n     features: ['image', 'raw_queries', 'broad_topical_query', 'broad_topical_explanation', 'specific_detail_query', 'specific_detail_explanation', 'visual_element_query', 'visual_element_explanation', 'parsed_into_json'],\n     num_rows: 2172\n@@ -120,7 +120,7 @@ indices, scores = find_top_k_indices_batched(ds_with_embeddings, text_embeds, pr\n print(indices, scores)\n ```\n \n-```\n+```text\n ([440, 442, 443],\n  [14.370786666870117,\n   13.675487518310547,"
        },
        {
            "sha": "34bc16628cad7bc30a29f80ce09f2311f499ed80",
            "filename": "docs/source/en/tokenizer_summary.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Ftokenizer_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Ftokenizer_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftokenizer_summary.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -42,7 +42,7 @@ For instance, let's look at the sentence `\"Don't you love ü§ó Transformers? We\n \n A simple way of tokenizing this text is to split it by spaces, which would give:\n \n-```\n+```text\n [\"Don't\", \"you\", \"love\", \"ü§ó\", \"Transformers?\", \"We\", \"sure\", \"do.\"]\n ```\n \n@@ -52,7 +52,7 @@ punctuation into account so that a model does not have to learn a different repr\n punctuation symbol that could follow it, which would explode the number of representations the model has to learn.\n Taking punctuation into account, tokenizing our exemplary text would give:\n \n-```\n+```text\n [\"Don\", \"'\", \"t\", \"you\", \"love\", \"ü§ó\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n ```\n \n@@ -65,7 +65,7 @@ input that was tokenized with the same rules that were used to tokenize its trai\n [spaCy](https://spacy.io/) and [Moses](http://www.statmt.org/moses/?n=Development.GetStarted) are two popular\n rule-based tokenizers. Applying them on our example, *spaCy* and *Moses* would output something like:\n \n-```\n+```text\n [\"Do\", \"n't\", \"you\", \"love\", \"ü§ó\", \"Transformers\", \"?\", \"We\", \"sure\", \"do\", \".\"]\n ```\n \n@@ -154,14 +154,14 @@ define before training the tokenizer.\n As an example, let's assume that after pre-tokenization, the following set of words including their frequency has been\n determined:\n \n-```\n+```text\n (\"hug\", 10), (\"pug\", 5), (\"pun\", 12), (\"bun\", 4), (\"hugs\", 5)\n ```\n \n Consequently, the base vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\"]`. Splitting all words into symbols of the\n base vocabulary, we obtain:\n \n-```\n+```text\n (\"h\" \"u\" \"g\", 10), (\"p\" \"u\" \"g\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"u\" \"g\" \"s\", 5)\n ```\n \n@@ -172,7 +172,7 @@ the example above `\"h\"` followed by `\"u\"` is present _10 + 5 = 15_ times (10 tim\n `\"u\"` symbols followed by a `\"g\"` symbol together. Next, `\"ug\"` is added to the vocabulary. The set of words then\n becomes\n \n-```\n+```text\n (\"h\" \"ug\", 10), (\"p\" \"ug\", 5), (\"p\" \"u\" \"n\", 12), (\"b\" \"u\" \"n\", 4), (\"h\" \"ug\" \"s\", 5)\n ```\n \n@@ -183,7 +183,7 @@ BPE then identifies the next most common symbol pair. It's `\"u\"` followed by `\"n\n At this stage, the vocabulary is `[\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"]` and our set of unique words\n is represented as\n \n-```\n+```text\n (\"hug\", 10), (\"p\" \"ug\", 5), (\"p\" \"un\", 12), (\"b\" \"un\", 4), (\"hug\" \"s\", 5)\n ```\n \n@@ -246,7 +246,7 @@ reached the desired size. The Unigram algorithm always keeps the base characters\n Because Unigram is not based on merge rules (in contrast to BPE and WordPiece), the algorithm has several ways of\n tokenizing new text after training. As an example, if a trained Unigram tokenizer exhibits the vocabulary:\n \n-```\n+```text\n [\"b\", \"g\", \"h\", \"n\", \"p\", \"s\", \"u\", \"ug\", \"un\", \"hug\"],\n ```\n "
        },
        {
            "sha": "0cc5829d2e8d83f8af9863e52d2e0a7412f16437",
            "filename": "docs/source/en/troubleshooting.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Ftroubleshooting.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/docs%2Fsource%2Fen%2Ftroubleshooting.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftroubleshooting.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -38,7 +38,7 @@ For more details about troubleshooting and getting help, take a look at [Chapter\n \n Some GPU instances on cloud and intranet setups are firewalled to external connections, resulting in a connection error. When your script attempts to download model weights or datasets, the download will hang and then timeout with the following message:\n \n-```\n+```text\n ValueError: Connection error, and we cannot find the requested files in the cached path.\n Please try again or make sure your Internet connection is on.\n ```\n@@ -49,7 +49,7 @@ In this case, you should try to run ü§ó Transformers on [offline mode](installa\n \n Training large models with millions of parameters can be challenging without the appropriate hardware. A common error you may encounter when the GPU runs out of memory is:\n \n-```\n+```text\n CUDA out of memory. Tried to allocate 256.00 MiB (GPU 0; 11.17 GiB total capacity; 9.70 GiB already allocated; 179.81 MiB free; 9.85 GiB reserved in total by PyTorch)\n ```\n \n@@ -68,7 +68,7 @@ Refer to the Performance [guide](performance) for more details about memory-savi\n \n Another common error you may encounter, especially if it is a newly released model, is `ImportError`:\n \n-```\n+```text\n ImportError: cannot import name 'ImageGPTImageProcessor' from 'transformers' (unknown location)\n ```\n \n@@ -82,7 +82,7 @@ pip install transformers --upgrade\n \n Sometimes you may run into a generic CUDA error about an error in the device code.\n \n-```\n+```text\n RuntimeError: CUDA error: device-side assert triggered\n ```\n "
        },
        {
            "sha": "d22a2a703b172260e8b6b917d33dbd56a0acc6c7",
            "filename": "tests/sagemaker/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/449533af73874470e914a203391635e04ac2ffc8/tests%2Fsagemaker%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/449533af73874470e914a203391635e04ac2ffc8/tests%2Fsagemaker%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fsagemaker%2FREADME.md?ref=449533af73874470e914a203391635e04ac2ffc8",
            "patch": "@@ -79,7 +79,7 @@ AWS is going to release new DLCs for PyTorch and/or TensorFlow. The Tests should\n \n Before we can run the tests we need to adjust the `requirements.txt` for Pytorch under `/tests/sagemaker/scripts/pytorch` and for Tensorflow under `/tests/sagemaker/scripts/pytorch`. We add the new framework version to it.\n \n-```\n+```bash\n torch==1.8.1 # for pytorch\n tensorflow-gpu==2.5.0 # for tensorflow\n ```"
        }
    ],
    "stats": {
        "total": 186,
        "additions": 93,
        "deletions": 93
    }
}