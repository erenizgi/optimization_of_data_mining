{
    "author": "yonigozlan",
    "message": "Add auto model for image-text-to-text (#32472)\n\n* Add Auto model for image-text-to-text\r\n\r\n* Remove donut from processing auto, add chameleon ti image text to text models\r\n\r\n* add qwen2_vl and llava_onevision\r\n\r\n* add pixtral to auto model for image-text-to-text\r\n\r\n* add mllama and idefics3\r\n\r\n* remove models in IGNORE_NON_AUTO_CONFIGURED\r\n\r\n* add AutoModelForImageTextToText to tests and doc",
    "sha": "e2001c341322c308ec663a37d91b725a1fdde7e8",
    "files": [
        {
            "sha": "059312850876d2b5b7a53c5f43f42a91c9eb3594",
            "filename": "docs/source/en/model_doc/auto.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2001c341322c308ec663a37d91b725a1fdde7e8/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2001c341322c308ec663a37d91b725a1fdde7e8/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fauto.md?ref=e2001c341322c308ec663a37d91b725a1fdde7e8",
            "patch": "@@ -381,3 +381,7 @@ The following auto classes are available for the following multimodal tasks.\n ### FlaxAutoModelForVision2Seq\n \n [[autodoc]] FlaxAutoModelForVision2Seq\n+\n+### AutoModelForImageTextToText\n+\n+[[autodoc]] AutoModelForImageTextToText"
        },
        {
            "sha": "b9146fbd33478a12f99b3e3e96db4ad56de98ec8",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2001c341322c308ec663a37d91b725a1fdde7e8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2001c341322c308ec663a37d91b725a1fdde7e8/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=e2001c341322c308ec663a37d91b725a1fdde7e8",
            "patch": "@@ -166,10 +166,10 @@ LLaVa-Next can perform inference with multiple images as input, where images eit\n import requests\n from PIL import Image\n import torch\n-from transformers import AutoProcessor, LlavaNextForConditionalGeneration\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n \n # Load the model in half-precision\n-model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+model = AutoModelForImageTextToText.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n processor = AutoProcessor.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\")\n \n # Get three different images\n@@ -246,7 +246,7 @@ We value your feedback to help identify bugs before the full release! Check out\n Simply change the snippet above with:\n \n ```python\n-from transformers import LlavaNextForConditionalGeneration, BitsAndBytesConfig\n+from transformers import AutoModelForImageTextToText, BitsAndBytesConfig\n \n # specify how to quantize the model\n quantization_config = BitsAndBytesConfig(\n@@ -255,17 +255,17 @@ quantization_config = BitsAndBytesConfig(\n     bnb_4bit_compute_dtype=torch.float16,\n )\n \n-model = LlavaNextForConditionalGeneration.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", quantization_config=quantization_config, device_map=\"auto\")\n+model = AutoModelForImageTextToText.from_pretrained(\"llava-hf/llava-v1.6-mistral-7b-hf\", quantization_config=quantization_config, device_map=\"auto\")\n ```\n \n ### Use Flash-Attention 2 to further speed-up generation\n \n First make sure to install flash-attn. Refer to the [original repository of Flash Attention](https://github.com/Dao-AILab/flash-attention) regarding that package installation. Simply change the snippet above with:\n \n ```python\n-from transformers import LlavaNextForConditionalGeneration\n+from transformers import AutoModelForImageTextToText\n \n-model = LlavaNextForConditionalGeneration.from_pretrained(\n+model = AutoModelForImageTextToText.from_pretrained(\n     model_id,\n     torch_dtype=torch.float16,\n     low_cpu_mem_usage=True,"
        },
        {
            "sha": "261abf947290d168f08787827d085b68d8af66b7",
            "filename": "docs/source/en/tasks/image_text_to_text.md",
            "status": "modified",
            "additions": 17,
            "deletions": 15,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2001c341322c308ec663a37d91b725a1fdde7e8/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2001c341322c308ec663a37d91b725a1fdde7e8/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_text_to_text.md?ref=e2001c341322c308ec663a37d91b725a1fdde7e8",
            "patch": "@@ -27,22 +27,22 @@ To begin with, there are multiple types of VLMs:\n - chat fine-tuned models for conversation\n - instruction fine-tuned models\n \n-This guide focuses on inference with an instruction-tuned model. \n+This guide focuses on inference with an instruction-tuned model.\n \n Let's begin installing the dependencies.\n \n ```bash\n-pip install -q transformers accelerate flash_attn \n+pip install -q transformers accelerate flash_attn\n ```\n \n-Let's initialize the model and the processor. \n+Let's initialize the model and the processor.\n \n ```python\n-from transformers import AutoProcessor, Idefics2ForConditionalGeneration\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n import torch\n \n device = torch.device(\"cuda\")\n-model = Idefics2ForConditionalGeneration.from_pretrained(\n+model = AutoModelForImageTextToText.from_pretrained(\n     \"HuggingFaceM4/idefics2-8b\",\n     torch_dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\",\n@@ -51,7 +51,7 @@ model = Idefics2ForConditionalGeneration.from_pretrained(\n processor = AutoProcessor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\n ```\n \n-This model has a [chat template](./chat_templating) that helps user parse chat outputs. Moreover, the model can also accept multiple images as input in a single conversation or message. We will now prepare the inputs. \n+This model has a [chat template](./chat_templating) that helps user parse chat outputs. Moreover, the model can also accept multiple images as input in a single conversation or message. We will now prepare the inputs.\n \n The image inputs look like the following.\n \n@@ -74,7 +74,7 @@ images = [Image.open(requests.get(img_urls[0], stream=True).raw),\n           Image.open(requests.get(img_urls[1], stream=True).raw)]\n ```\n \n-Below is an example of the chat template. We can feed conversation turns and the last message as an input by appending it at the end of the template. \n+Below is an example of the chat template. We can feed conversation turns and the last message as an input by appending it at the end of the template.\n \n \n ```python\n@@ -98,7 +98,7 @@ messages = [\n             {\"type\": \"image\"},\n             {\"type\": \"text\", \"text\": \"And how about this image?\"},\n         ]\n-    },       \n+    },\n ]\n ```\n \n@@ -180,11 +180,11 @@ def model_inference(\n         if acc_text.endswith(\"<end_of_utterance>\"):\n             acc_text = acc_text[:-18]\n         yield acc_text\n-    \n+\n     thread.join()\n ```\n \n-Now let's call the `model_inference` function we created and stream the values. \n+Now let's call the `model_inference` function we created and stream the values.\n \n ```python\n generator = model_inference(\n@@ -204,7 +204,7 @@ for value in generator:\n \n ## Fit models in smaller hardware\n \n-VLMs are often large and need to be optimized to fit on smaller hardware. Transformers supports many model quantization libraries, and here we will only show int8 quantization with [Quanto](./quantization/quanto#quanto). int8 quantization offers memory improvements up to 75 percent (if all weights are quantized). However it is no free lunch, since 8-bit is not a CUDA-native precision, the weights are quantized back and forth on the fly, which adds up to latency. \n+VLMs are often large and need to be optimized to fit on smaller hardware. Transformers supports many model quantization libraries, and here we will only show int8 quantization with [Quanto](./quantization/quanto#quanto). int8 quantization offers memory improvements up to 75 percent (if all weights are quantized). However it is no free lunch, since 8-bit is not a CUDA-native precision, the weights are quantized back and forth on the fly, which adds up to latency.\n \n First, install dependencies.\n \n@@ -215,18 +215,20 @@ pip install -U quanto bitsandbytes\n To quantize a model during loading, we need to first create [`QuantoConfig`]. Then load the model as usual, but pass `quantization_config` during model initialization.\n \n ```python\n-from transformers import Idefics2ForConditionalGeneration, AutoTokenizer, QuantoConfig\n+from transformers import AutoModelForImageTextToText, QuantoConfig\n \n model_id = \"HuggingFaceM4/idefics2-8b\"\n quantization_config = QuantoConfig(weights=\"int8\")\n-quantized_model = Idefics2ForConditionalGeneration.from_pretrained(model_id, device_map=\"cuda\", quantization_config=quantization_config)\n+quantized_model = AutoModelForImageTextToText.from_pretrained(\n+    model_id, device_map=\"cuda\", quantization_config=quantization_config\n+)\n ```\n \n-And that's it, we can use the model the same way with no changes. \n+And that's it, we can use the model the same way with no changes.\n \n ## Further Reading\n \n Here are some more resources for the image-text-to-text task.\n \n-- [Image-text-to-text task page](https://huggingface.co/tasks/image-text-to-text) covers model types, use cases, datasets, and more. \n+- [Image-text-to-text task page](https://huggingface.co/tasks/image-text-to-text) covers model types, use cases, datasets, and more.\n - [Vision Language Models Explained](https://huggingface.co/blog/vlms) is a blog post that covers everything about vision language models and supervised fine-tuning using [TRL](https://huggingface.co/docs/trl/en/index)."
        },
        {
            "sha": "492c46c79ea9054f01f4d71a19644adab40b14de",
            "filename": "docs/source/ja/model_doc/auto.md",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2001c341322c308ec663a37d91b725a1fdde7e8/docs%2Fsource%2Fja%2Fmodel_doc%2Fauto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2001c341322c308ec663a37d91b725a1fdde7e8/docs%2Fsource%2Fja%2Fmodel_doc%2Fauto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fmodel_doc%2Fauto.md?ref=e2001c341322c308ec663a37d91b725a1fdde7e8",
            "patch": "@@ -368,3 +368,7 @@ AutoModel.register(NewModelConfig, NewModel)\n ### FlaxAutoModelForVision2Seq\n \n [[autodoc]] FlaxAutoModelForVision2Seq\n+\n+### AutoModelForImageTextToText\n+\n+[[autodoc]] AutoModelForImageTextToText"
        },
        {
            "sha": "3d612f6c4c246ed0b97cb42b01e879d74607fbbe",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2001c341322c308ec663a37d91b725a1fdde7e8/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2001c341322c308ec663a37d91b725a1fdde7e8/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=e2001c341322c308ec663a37d91b725a1fdde7e8",
            "patch": "@@ -1407,6 +1407,7 @@\n             \"MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING\",\n             \"MODEL_FOR_IMAGE_MAPPING\",\n             \"MODEL_FOR_IMAGE_SEGMENTATION_MAPPING\",\n+            \"MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING\",\n             \"MODEL_FOR_IMAGE_TO_IMAGE_MAPPING\",\n             \"MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING\",\n             \"MODEL_FOR_KEYPOINT_DETECTION_MAPPING\",\n@@ -1448,6 +1449,7 @@\n             \"AutoModelForDocumentQuestionAnswering\",\n             \"AutoModelForImageClassification\",\n             \"AutoModelForImageSegmentation\",\n+            \"AutoModelForImageTextToText\",\n             \"AutoModelForImageToImage\",\n             \"AutoModelForInstanceSegmentation\",\n             \"AutoModelForKeypointDetection\",\n@@ -6272,6 +6274,7 @@\n             MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\n             MODEL_FOR_IMAGE_MAPPING,\n             MODEL_FOR_IMAGE_SEGMENTATION_MAPPING,\n+            MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING,\n             MODEL_FOR_IMAGE_TO_IMAGE_MAPPING,\n             MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING,\n             MODEL_FOR_KEYPOINT_DETECTION_MAPPING,\n@@ -6313,6 +6316,7 @@\n             AutoModelForDocumentQuestionAnswering,\n             AutoModelForImageClassification,\n             AutoModelForImageSegmentation,\n+            AutoModelForImageTextToText,\n             AutoModelForImageToImage,\n             AutoModelForInstanceSegmentation,\n             AutoModelForKeypointDetection,"
        },
        {
            "sha": "2ee0541a1a71b8752f363b42370f463df959efff",
            "filename": "src/transformers/models/auto/__init__.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2001c341322c308ec663a37d91b725a1fdde7e8/src%2Ftransformers%2Fmodels%2Fauto%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2001c341322c308ec663a37d91b725a1fdde7e8/src%2Ftransformers%2Fmodels%2Fauto%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2F__init__.py?ref=e2001c341322c308ec663a37d91b725a1fdde7e8",
            "patch": "@@ -74,6 +74,7 @@\n         \"MODEL_FOR_UNIVERSAL_SEGMENTATION_MAPPING\",\n         \"MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING\",\n         \"MODEL_FOR_VISION_2_SEQ_MAPPING\",\n+        \"MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING\",\n         \"MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING\",\n         \"MODEL_MAPPING\",\n         \"MODEL_WITH_LM_HEAD_MAPPING\",\n@@ -119,6 +120,7 @@\n         \"AutoModelWithLMHead\",\n         \"AutoModelForZeroShotImageClassification\",\n         \"AutoModelForZeroShotObjectDetection\",\n+        \"AutoModelForImageTextToText\",\n     ]\n \n try:\n@@ -238,6 +240,7 @@\n             MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING,\n             MODEL_FOR_IMAGE_MAPPING,\n             MODEL_FOR_IMAGE_SEGMENTATION_MAPPING,\n+            MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING,\n             MODEL_FOR_IMAGE_TO_IMAGE_MAPPING,\n             MODEL_FOR_INSTANCE_SEGMENTATION_MAPPING,\n             MODEL_FOR_KEYPOINT_DETECTION_MAPPING,\n@@ -279,6 +282,7 @@\n             AutoModelForDocumentQuestionAnswering,\n             AutoModelForImageClassification,\n             AutoModelForImageSegmentation,\n+            AutoModelForImageTextToText,\n             AutoModelForImageToImage,\n             AutoModelForInstanceSegmentation,\n             AutoModelForKeypointDetection,"
        },
        {
            "sha": "aa0d59de52ff4c2552acf99c9a4b325c534c7fb8",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2001c341322c308ec663a37d91b725a1fdde7e8/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2001c341322c308ec663a37d91b725a1fdde7e8/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=e2001c341322c308ec663a37d91b725a1fdde7e8",
            "patch": "@@ -757,6 +757,32 @@\n     ]\n )\n \n+MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES = OrderedDict(\n+    [\n+        (\"blip\", \"BlipForConditionalGeneration\"),\n+        (\"blip-2\", \"Blip2ForConditionalGeneration\"),\n+        (\"chameleon\", \"ChameleonForConditionalGeneration\"),\n+        (\"fuyu\", \"FuyuForCausalLM\"),\n+        (\"git\", \"GitForCausalLM\"),\n+        (\"idefics\", \"IdeficsForVisionText2Text\"),\n+        (\"idefics2\", \"Idefics2ForConditionalGeneration\"),\n+        (\"idefics3\", \"Idefics3ForConditionalGeneration\"),\n+        (\"instructblip\", \"InstructBlipForConditionalGeneration\"),\n+        (\"kosmos-2\", \"Kosmos2ForConditionalGeneration\"),\n+        (\"llava\", \"LlavaForConditionalGeneration\"),\n+        (\"llava_next\", \"LlavaNextForConditionalGeneration\"),\n+        (\"llava_onevision\", \"LlavaOnevisionForConditionalGeneration\"),\n+        (\"mllama\", \"MllamaForConditionalGeneration\"),\n+        (\"paligemma\", \"PaliGemmaForConditionalGeneration\"),\n+        (\"pix2struct\", \"Pix2StructForConditionalGeneration\"),\n+        (\"pixtral\", \"LlavaForConditionalGeneration\"),\n+        (\"qwen2_vl\", \"Qwen2VLForConditionalGeneration\"),\n+        (\"udop\", \"UdopForConditionalGeneration\"),\n+        (\"vipllava\", \"VipLlavaForConditionalGeneration\"),\n+        (\"vision-encoder-decoder\", \"VisionEncoderDecoderModel\"),\n+    ]\n+)\n+\n MODEL_FOR_MASKED_LM_MAPPING_NAMES = OrderedDict(\n     [\n         # Model for Masked LM mapping\n@@ -1419,6 +1445,9 @@\n     CONFIG_MAPPING_NAMES, MODEL_FOR_VIDEO_CLASSIFICATION_MAPPING_NAMES\n )\n MODEL_FOR_VISION_2_SEQ_MAPPING = _LazyAutoMapping(CONFIG_MAPPING_NAMES, MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES)\n+MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING = _LazyAutoMapping(\n+    CONFIG_MAPPING_NAMES, MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING_NAMES\n+)\n MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING = _LazyAutoMapping(\n     CONFIG_MAPPING_NAMES, MODEL_FOR_VISUAL_QUESTION_ANSWERING_MAPPING_NAMES\n )\n@@ -1713,6 +1742,13 @@ class AutoModelForVision2Seq(_BaseAutoModelClass):\n AutoModelForVision2Seq = auto_class_update(AutoModelForVision2Seq, head_doc=\"vision-to-text modeling\")\n \n \n+class AutoModelForImageTextToText(_BaseAutoModelClass):\n+    _model_mapping = MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING\n+\n+\n+AutoModelForImageTextToText = auto_class_update(AutoModelForImageTextToText, head_doc=\"image-text-to-text modeling\")\n+\n+\n class AutoModelForAudioClassification(_BaseAutoModelClass):\n     _model_mapping = MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\n "
        },
        {
            "sha": "c1f23bc1cb3f18134cf14b04210e52612b11cb5a",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2001c341322c308ec663a37d91b725a1fdde7e8/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2001c341322c308ec663a37d91b725a1fdde7e8/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=e2001c341322c308ec663a37d91b725a1fdde7e8",
            "patch": "@@ -99,6 +99,7 @@\n         (\"trocr\", \"TrOCRProcessor\"),\n         (\"tvlt\", \"TvltProcessor\"),\n         (\"tvp\", \"TvpProcessor\"),\n+        (\"udop\", \"UdopProcessor\"),\n         (\"unispeech\", \"Wav2Vec2Processor\"),\n         (\"unispeech-sat\", \"Wav2Vec2Processor\"),\n         (\"video_llava\", \"VideoLlavaProcessor\"),"
        },
        {
            "sha": "048de1cc8ae77a2a6e4625c030724dac9222b4f7",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2001c341322c308ec663a37d91b725a1fdde7e8/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2001c341322c308ec663a37d91b725a1fdde7e8/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=e2001c341322c308ec663a37d91b725a1fdde7e8",
            "patch": "@@ -707,6 +707,9 @@ def __init__(self, *args, **kwargs):\n MODEL_FOR_IMAGE_SEGMENTATION_MAPPING = None\n \n \n+MODEL_FOR_IMAGE_TEXT_TO_TEXT_MAPPING = None\n+\n+\n MODEL_FOR_IMAGE_TO_IMAGE_MAPPING = None\n \n \n@@ -874,6 +877,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class AutoModelForImageTextToText(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class AutoModelForImageToImage(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "8e211310fbcf75e53644d6abe490e96114d0af4b",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2001c341322c308ec663a37d91b725a1fdde7e8/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2001c341322c308ec663a37d91b725a1fdde7e8/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=e2001c341322c308ec663a37d91b725a1fdde7e8",
            "patch": "@@ -23,7 +23,7 @@\n import numpy as np\n import requests\n \n-from transformers import AutoModelForVision2Seq, AutoProcessor, Kosmos2Config\n+from transformers import AutoModelForImageTextToText, AutoProcessor, Kosmos2Config\n from transformers.models.kosmos2.configuration_kosmos2 import Kosmos2TextConfig, Kosmos2VisionConfig\n from transformers.testing_utils import IS_ROCM_SYSTEM, require_torch, require_vision, slow, torch_device\n from transformers.utils import is_torch_available, is_vision_available\n@@ -551,7 +551,7 @@ def test_snowman_image_captioning(self):\n         image.save(\"new_image.jpg\")\n         image = Image.open(\"new_image.jpg\")\n \n-        model = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\").to(torch_device)\n+        model = AutoModelForImageTextToText.from_pretrained(\"microsoft/kosmos-2-patch14-224\").to(torch_device)\n         processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n \n         prompt = \"<grounding>An image of\"\n@@ -697,7 +697,7 @@ def test_snowman_image_captioning_batch(self):\n         image.save(\"new_image.jpg\")\n         image = Image.open(\"new_image.jpg\")\n \n-        model = AutoModelForVision2Seq.from_pretrained(\"microsoft/kosmos-2-patch14-224\").to(torch_device)\n+        model = AutoModelForImageTextToText.from_pretrained(\"microsoft/kosmos-2-patch14-224\").to(torch_device)\n \n         prompt = [\"<grounding>Describe this image in detail:\", \"<grounding>An image of\"]\n "
        },
        {
            "sha": "3ecbd79eca48d93230f3316a969985cd6b6ea2e4",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e2001c341322c308ec663a37d91b725a1fdde7e8/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e2001c341322c308ec663a37d91b725a1fdde7e8/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=e2001c341322c308ec663a37d91b725a1fdde7e8",
            "patch": "@@ -170,7 +170,6 @@\n     \"ClapTextModelWithProjection\",\n     \"ClapAudioModel\",\n     \"ClapAudioModelWithProjection\",\n-    \"Blip2ForConditionalGeneration\",\n     \"Blip2TextModelWithProjection\",\n     \"Blip2VisionModelWithProjection\",\n     \"Blip2QFormerModel\",\n@@ -181,7 +180,6 @@\n     \"GitVisionModel\",\n     \"GraphormerModel\",\n     \"GraphormerForGraphClassification\",\n-    \"BlipForConditionalGeneration\",\n     \"BlipForImageTextRetrieval\",\n     \"BlipForQuestionAnswering\",\n     \"BlipVisionModel\",\n@@ -245,7 +243,6 @@\n     \"DetrForSegmentation\",\n     \"Pix2StructVisionModel\",\n     \"Pix2StructTextModel\",\n-    \"Pix2StructForConditionalGeneration\",\n     \"ConditionalDetrForSegmentation\",\n     \"DPRReader\",\n     \"FlaubertForQuestionAnswering\",\n@@ -322,7 +319,6 @@\n     \"SeamlessM4TCodeHifiGan\",\n     \"SeamlessM4TForSpeechToSpeech\",  # no auto class for speech-to-speech\n     \"TvpForVideoGrounding\",\n-    \"UdopForConditionalGeneration\",\n     \"SeamlessM4Tv2NARTextToUnitModel\",\n     \"SeamlessM4Tv2NARTextToUnitForConditionalGeneration\",\n     \"SeamlessM4Tv2CodeHifiGan\","
        }
    ],
    "stats": {
        "total": 117,
        "additions": 89,
        "deletions": 28
    }
}