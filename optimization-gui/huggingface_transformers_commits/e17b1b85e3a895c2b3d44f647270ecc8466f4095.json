{
    "author": "YangKai0616",
    "message": "[Fix] Fix FA2 kernels ut (#42803)\n\n* Fixed FA2 kernels UT\n\n* Update\n\n* Refactor FA2 kernel map\n\n* Update\n\n* Update",
    "sha": "e17b1b85e3a895c2b3d44f647270ecc8466f4095",
    "files": [
        {
            "sha": "c24770f440fd558aeea21e32fbd6cf7411f3ea1a",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/e17b1b85e3a895c2b3d44f647270ecc8466f4095/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e17b1b85e3a895c2b3d44f647270ecc8466f4095/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=e17b1b85e3a895c2b3d44f647270ecc8466f4095",
            "patch": "@@ -155,6 +155,12 @@\n _is_quantized = False\n _is_ds_init_called = False\n \n+# Mapping from flash attention implementations to their kernel fallback repositories\n+FLASH_ATTN_KERNEL_FALLBACK = {\n+    \"flash_attention_2\": \"kernels-community/flash-attn2\",\n+    \"flash_attention_3\": \"kernels-community/vllm-flash-attn3\",\n+}\n+\n \n def is_local_dist_rank_0():\n     return (\n@@ -1592,7 +1598,9 @@ def _flash_attn_2_can_dispatch(self, is_init_check: bool = False) -> bool:\n                 return True\n \n             if is_torch_xpu_available():\n-                logger.info(\"Detect using FlashAttention2 (via kernel `kernels-community/flash-attn2`) on XPU.\")\n+                logger.info(\n+                    f\"Detect using FlashAttention2 (via kernel `{FLASH_ATTN_KERNEL_FALLBACK['flash_attention_2']}`) on XPU.\"\n+                )\n                 return True\n \n             if importlib.util.find_spec(\"flash_attn\") is None:\n@@ -1824,14 +1832,12 @@ def _check_and_adjust_attn_implementation(\n             and is_kernels_available()\n             and not is_torch_npu_available()\n         ):\n-            if attn_implementation.endswith(\"2\"):\n-                applicable_attn_implementation = \"kernels-community/flash-attn2\"\n-                if is_torch_xpu_available():\n-                    # On XPU, kernels library is the native implementation\n-                    # Disabling this flag to avoid giving wrong fallbacks on errors and warnings\n-                    requested_original_flash_attn = False\n-            else:\n-                applicable_attn_implementation = \"kernels-community/vllm-flash-attn3\"\n+            applicable_attn_implementation = FLASH_ATTN_KERNEL_FALLBACK[attn_implementation.removeprefix(\"paged|\")]\n+\n+            if is_torch_xpu_available() and attn_implementation.removeprefix(\"paged|\") == \"flash_attention_2\":\n+                # On XPU, kernels library is the native implementation\n+                # Disabling this flag to avoid giving wrong fallbacks on errors and warnings\n+                requested_original_flash_attn = False\n \n             if is_paged:\n                 applicable_attn_implementation = f\"paged|{applicable_attn_implementation}\""
        },
        {
            "sha": "a6156f4fae7f2390239fe9ed57b1b7cd3ac2d9ff",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e17b1b85e3a895c2b3d44f647270ecc8466f4095/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e17b1b85e3a895c2b3d44f647270ecc8466f4095/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=e17b1b85e3a895c2b3d44f647270ecc8466f4095",
            "patch": "@@ -221,7 +221,7 @@\n     import torch\n     from safetensors.torch import load_file\n \n-    from .modeling_utils import PreTrainedModel\n+    from .modeling_utils import FLASH_ATTN_KERNEL_FALLBACK, PreTrainedModel\n \n     IS_ROCM_SYSTEM = torch.version.hip is not None\n     IS_CUDA_SYSTEM = torch.version.cuda is not None\n@@ -620,7 +620,7 @@ def require_flash_attn(test_case):\n     try:\n         from kernels import get_kernel\n \n-        get_kernel(\"kernels-community/flash-attn2\")\n+        get_kernel(FLASH_ATTN_KERNEL_FALLBACK[\"flash_attention_2\"])\n     except Exception as _:\n         kernels_available = False\n "
        },
        {
            "sha": "019151bbbf94279d366428b6cce711875b2fd0e5",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 17,
            "deletions": 3,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/e17b1b85e3a895c2b3d44f647270ecc8466f4095/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e17b1b85e3a895c2b3d44f647270ecc8466f4095/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=e17b1b85e3a895c2b3d44f647270ecc8466f4095",
            "patch": "@@ -52,7 +52,7 @@\n     unset_hf_deepspeed_config,\n )\n from transformers.modeling_layers import GradientCheckpointingLayer\n-from transformers.modeling_utils import _get_tied_weight_keys\n+from transformers.modeling_utils import FLASH_ATTN_KERNEL_FALLBACK, _get_tied_weight_keys\n from transformers.models.auto import get_values\n from transformers.models.auto.modeling_auto import (\n     MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES,\n@@ -3243,6 +3243,20 @@ def flash_attn_can_dispatch_composite_models(self, attn_implementation: str):\n             self.skipTest(f\"bfloat16 not supported on {torch_device} (on the specific device currently used)\")\n \n         dtype = torch.bfloat16\n+\n+        def _expected_attn_implementations(attention_implementation: str) -> set[str]:\n+            # Allow kernels fallbacks for flash attention tests.\n+            requested = attention_implementation\n+            base = requested.removeprefix(\"paged|\")\n+            prefix = \"paged|\" if requested.startswith(\"paged|\") else \"\"\n+\n+            expected = {requested}\n+            if base in FLASH_ATTN_KERNEL_FALLBACK:\n+                expected.add(f\"{prefix}{FLASH_ATTN_KERNEL_FALLBACK[base]}\")\n+            return expected\n+\n+        expected_attn_implementations = _expected_attn_implementations(attn_implementation)\n+\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n             model = model_class(config)\n@@ -3275,15 +3289,15 @@ def flash_attn_can_dispatch_composite_models(self, attn_implementation: str):\n                     for key in model_fa.config:\n                         if isinstance(getattr(model_fa.config, key), PreTrainedConfig):\n                             sub_config = getattr(model_fa.config, key)\n-                            self.assertTrue(sub_config._attn_implementation == attn_implementation)\n+                            self.assertIn(sub_config._attn_implementation, expected_attn_implementations)\n \n                     has_fa = False\n                     for name, submodule in model_fa.named_modules():\n                         class_name = submodule.__class__.__name__\n                         if (\n                             \"Attention\" in class_name\n                             and getattr(submodule, \"config\", None)\n-                            and submodule.config._attn_implementation == attn_implementation\n+                            and submodule.config._attn_implementation in expected_attn_implementations\n                         ):\n                             has_fa = True\n                             break"
        },
        {
            "sha": "4b0fffc8f04decef6e90fbc0e997c6040a006c1a",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/e17b1b85e3a895c2b3d44f647270ecc8466f4095/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e17b1b85e3a895c2b3d44f647270ecc8466f4095/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=e17b1b85e3a895c2b3d44f647270ecc8466f4095",
            "patch": "@@ -129,7 +129,11 @@\n         _prepare_4d_attention_mask,\n         _prepare_4d_causal_attention_mask,\n     )\n-    from transformers.modeling_utils import _find_disjoint, _find_identical\n+    from transformers.modeling_utils import (\n+        FLASH_ATTN_KERNEL_FALLBACK,\n+        _find_disjoint,\n+        _find_identical,\n+    )\n     from transformers.pytorch_utils import isin_mps_friendly\n \n     # Fake pretrained models for tests\n@@ -3028,7 +3032,7 @@ def test_kernels_fallback(self):\n                 )\n \n         self.assertTrue(\n-            \"You do not have `flash_attn` installed, using `kernels-community/flash-attn2` from the `kernels` library instead!\"\n+            f\"You do not have `flash_attn` installed, using `{FLASH_ATTN_KERNEL_FALLBACK['flash_attention_2']}` from the `kernels` library instead!\"\n             in cl.out\n         )\n \n@@ -3040,7 +3044,8 @@ def test_not_available_kernels(self):\n \n         with self.assertRaises(ImportError) as cm:\n             _ = AutoModel.from_pretrained(\n-                \"hf-tiny-model-private/tiny-random-MCTCTModel\", attn_implementation=\"kernels-community/flash-attn2\"\n+                \"hf-tiny-model-private/tiny-random-MCTCTModel\",\n+                attn_implementation=FLASH_ATTN_KERNEL_FALLBACK[\"flash_attention_2\"],\n             )\n \n         self.assertTrue(\"`kernels` is either not installed or uses an incompatible version.\" in str(cm.exception))"
        }
    ],
    "stats": {
        "total": 59,
        "additions": 42,
        "deletions": 17
    }
}