{
    "author": "seopp",
    "message": "ğŸŒ [i18n-KO] Translated `gemma3.md` to Korean (#39865)\n\n* docs: ko: gemma3.md\n\n* feat: nmt draft\n\n* fix: manual edits\n\n* fix: resolve suggestions\n\nCo-authored-by: Chaewon Song <chaewon1019@ewhain.net>\n\n* fix: resolve suggestions\n\n---------\n\nCo-authored-by: Chaewon Song <chaewon1019@ewhain.net>",
    "sha": "e651ae0a32d6603dee91c41985b4177fb0f562ae",
    "files": [
        {
            "sha": "1fffffabe1244fd302761802a21bae2334e012ba",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e651ae0a32d6603dee91c41985b4177fb0f562ae/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/e651ae0a32d6603dee91c41985b4177fb0f562ae/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=e651ae0a32d6603dee91c41985b4177fb0f562ae",
            "patch": "@@ -1053,7 +1053,7 @@\n         title: Emu3\n       - local: in_translation\n         title: FLAVA\n-      - local: in_translation\n+      - local: model_doc/gemma3\n         title: Gemma3\n       - local: in_translation\n         title: Gemma3n"
        },
        {
            "sha": "dc171cbece01a3d8db394143034d64d17ef56b94",
            "filename": "docs/source/ko/model_doc/gemma3.md",
            "status": "added",
            "additions": 274,
            "deletions": 0,
            "changes": 274,
            "blob_url": "https://github.com/huggingface/transformers/blob/e651ae0a32d6603dee91c41985b4177fb0f562ae/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e651ae0a32d6603dee91c41985b4177fb0f562ae/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma3.md?ref=e651ae0a32d6603dee91c41985b4177fb0f562ae",
            "patch": "@@ -0,0 +1,274 @@\n+\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# Gemma 3 [[gemma3]]\n+\n+[Gemma 3](https://goo.gle/Gemma3Report)ëŠ” ì‚¬ì „ í›ˆë ¨ëœ ë²„ì „ê³¼ ì§€ì‹œë¬¸ ì¡°ì • ë²„ì „ì„ ê°–ì¶˜ ë©€í‹°ëª¨ë‹¬ ëª¨ë¸ë¡œ, 1B, 13B, 27B ë§¤ê°œë³€ìˆ˜ë¡œ ì œê³µë©ë‹ˆë‹¤. ì•„í‚¤í…ì²˜ëŠ” ì´ì „ Gemma ë²„ì „ê³¼ ëŒ€ë¶€ë¶„ ë™ì¼í•©ë‹ˆë‹¤. ì£¼ìš” ì°¨ì´ì ì€ ëª¨ë“  ê¸€ë¡œë²Œ ì…€í”„ ì–´í…ì…˜ ë ˆì´ì–´ë§ˆë‹¤ 5ê°œì˜ ë¡œì»¬ ìŠ¬ë¼ì´ë”© ìœˆë„ìš° ì…€í”„ ì–´í…ì…˜ ë ˆì´ì–´ë¥¼ ë²ˆê°ˆì•„ ì‚¬ìš©í•˜ëŠ” ì , 128K í† í°ì˜ ë” ê¸´ ì»¨í…ìŠ¤íŠ¸ ê¸¸ì´ë¥¼ ì§€ì›í•˜ëŠ” ì , ê·¸ë¦¬ê³  ê³ í•´ìƒë„ ì´ë¯¸ì§€ë‚˜ ì •ì‚¬ê°í˜•ì´ ì•„ë‹Œ ì¢…íš¡ë¹„ì˜ ì´ë¯¸ì§€ì—ì„œ ì •ë³´ê°€ ì‚¬ë¼ì§€ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ê³ í•´ìƒë„ ì´ë¯¸ì§€ë¥¼ \"íŒ¨ë‹ ë° ìŠ¤ìºë‹\"í•  ìˆ˜ ìˆëŠ” [SigLip](./siglip) ì¸ì½”ë”ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì ì…ë‹ˆë‹¤.\n+\n+ì§€ì‹œë¬¸ ì¡°ì • ë²„ì „ì€ ì§€ì‹ ì¦ë¥˜ ë° ê°•í™” í•™ìŠµìœ¼ë¡œ í›„ì† í•™ìŠµë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+Gemma 3ì˜ ëª¨ë“  ì›ë³¸ ì²´í¬í¬ì¸íŠ¸ëŠ” [Gemma 3](https://huggingface.co/collections/google/gemma-3-release-67c6c6f89c4f76621268bb6d) ë¦´ë¦¬ìŠ¤ì—ì„œ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+> [!íŒ]\n+> Gemmaë¥¼ ë‹¤ì–‘í•œ ë¹„ì „ ë° ì–¸ì–´ ì‘ì—…ì— ì ìš©í•˜ëŠ” ì¶”ê°€ ì˜ˆì‹œë¥¼ ë³´ë ¤ë©´ ì˜¤ë¥¸ìª½ ì‚¬ì´ë“œë°”ì˜ Gemma 3 ëª¨ë¸ì„ í´ë¦­í•˜ì„¸ìš”.\n+\n+ì•„ë˜ ì˜ˆì‹œëŠ” [`Pipeline`] ë˜ëŠ” [`AutoModel`] í´ë˜ìŠ¤ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipeline = pipeline(\n+    task=\"image-text-to-text\",\n+    model=\"google/gemma-3-4b-pt\",\n+    device=0,\n+    torch_dtype=torch.bfloat16\n+)\n+pipeline(\n+    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\",\n+    text=\"<start_of_image> What is shown in this image?\"\n+)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoProcessor, Gemma3ForConditionalGeneration\n+\n+model = Gemma3ForConditionalGeneration.from_pretrained(\n+    \"google/gemma-3-4b-it\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+processor = AutoProcessor.from_pretrained(\n+    \"google/gemma-3-4b-it\",\n+    padding_side=\"left\"\n+)\n+\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n+        ]\n+    },\n+    {\n+        \"role\": \"user\", \"content\": [\n+            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ]\n+    },\n+]\n+inputs = processor.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+    add_generation_prompt=True,\n+).to(\"cuda\")\n+\n+output = model.generate(**inputs, max_new_tokens=50, cache_implementation=\"static\")\n+print(processor.decode(output[0], skip_special_tokens=True))\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e \"Plants create energy through a process known as\" | transformers run --task text-generation --model google/gemma-3-1b-pt --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+ì–‘ìí™”ëŠ” ê°€ì¤‘ì¹˜ë¥¼ ë” ë‚®ì€ ì •ë°€ë„ë¡œ í‘œí˜„í•˜ì—¬, í° ëª¨ë¸ì˜ ë©”ëª¨ë¦¬ ë¶€ë‹´ì„ ì¤„ì—¬ì¤ë‹ˆë‹¤. ì‚¬ìš© ê°€ëŠ¥í•œ ì–‘ìí™” ë°±ì—”ë“œì— ëŒ€í•œ ë” ìì„¸í•œ ë‚´ìš©ì€ [ì–‘ìí™”](../quantization/overview) ê°œìš”ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+ì•„ë˜ ì˜ˆì œì—ì„œëŠ” [torchao](../quantization/torchao)ë¥¼ ì‚¬ìš©í•˜ì—¬ ê°€ì¤‘ì¹˜ë¥¼ int4ë¡œë§Œ ì–‘ìí™”í•©ë‹ˆë‹¤.\n+\n+```py\n+# pip install torchao\n+import torch\n+from transformers import TorchAoConfig, Gemma3ForConditionalGeneration, AutoProcessor\n+\n+quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n+model = Gemma3ForConditionalGeneration.from_pretrained(\n+    \"google/gemma-3-27b-it\",\n+    torch_dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    quantization_config=quantization_config\n+)\n+processor = AutoProcessor.from_pretrained(\n+    \"google/gemma-3-27b-it\",\n+    padding_side=\"left\"\n+)\n+\n+messages = [\n+    {\n+        \"role\": \"system\",\n+        \"content\": [\n+            {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n+        ]\n+    },\n+    {\n+        \"role\": \"user\", \"content\": [\n+            {\"type\": \"image\", \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ]\n+    },\n+]\n+inputs = processor.apply_chat_template(\n+    messages,\n+    tokenize=True,\n+    return_dict=True,\n+    return_tensors=\"pt\",\n+    add_generation_prompt=True,\n+).to(\"cuda\")\n+\n+output = model.generate(**inputs, max_new_tokens=50, cache_implementation=\"static\")\n+print(processor.decode(output[0], skip_special_tokens=True))\n+```\n+\n+[AttentionMaskVisualizer](https://github.com/huggingface/transformers/blob/beb9b5b02246b9b7ee81ddf938f93f44cfeaad19/src/transformers/utils/attention_visualizer.py#L139)ë¥¼ ì‚¬ìš©í•˜ì—¬ ëª¨ë¸ì´ ì£¼ëª©í•  ìˆ˜ ìˆëŠ” í† í°ê³¼ ì£¼ëª©í•  ìˆ˜ ì—†ëŠ” í† í°ì„ ë” ì˜ ì´í•´í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```py\n+from transformers.utils.attention_visualizer import AttentionMaskVisualizer\n+\n+visualizer = AttentionMaskVisualizer(\"google/gemma-3-4b-it\")\n+visualizer(\"<img>What is shown in this image?\")\n+```\n+\n+<div class=\"flex justify-center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/gemma-3-attn-mask.png\"/>\n+</div>\n+\n+## ë…¸íŠ¸ [[notes]]\n+\n+- ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ ë° ì´ë¯¸ì§€ ì „ìš© ì…ë ¥ì—ëŠ” [`Gemma3ForConditionalGeneration`]ì„ ì‚¬ìš©í•˜ì„¸ìš”.\n+- Gemma 3ëŠ” ë‹¤ì¤‘ ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì§€ì›í•˜ì§€ë§Œ, í”„ë¡œì„¸ì„œì— ì „ë‹¬í•˜ê¸° ì „ì— ì´ë¯¸ì§€ê°€ ì˜¬ë°”ë¥´ê²Œ ë°°ì¹˜ë˜ì—ˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”. ê° ë°°ì¹˜ëŠ” í•˜ë‚˜ ì´ìƒì˜ ì´ë¯¸ì§€ë¥¼ í¬í•¨í•œ ë¦¬ìŠ¤íŠ¸ì—¬ì•¼ í•©ë‹ˆë‹¤.\n+\n+    ```py\n+    url_cow = \"https://media.istockphoto.com/id/1192867753/photo/cow-in-berchida-beach-siniscola.jpg?s=612x612&w=0&k=20&c=v0hjjniwsMNfJSuKWZuIn8pssmD5h5bSN1peBd1CmH4=\"\n+    url_cat = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+\n+    messages =[\n+        {\n+            \"role\": \"system\",\n+            \"content\": [\n+                {\"type\": \"text\", \"text\": \"You are a helpful assistant.\"}\n+            ]\n+        },\n+        {\n+            \"role\": \"user\",\n+            \"content\": [\n+                {\"type\": \"image\", \"url\": url_cow},\n+                {\"type\": \"image\", \"url\": url_cat},\n+                {\"type\": \"text\", \"text\": \"Which image is cuter?\"},\n+            ]\n+        },\n+    ]\n+    ```\n+- í”„ë¡œì„¸ì„œì— ì „ë‹¬ë˜ëŠ” í…ìŠ¤íŠ¸ì—ëŠ” ì´ë¯¸ì§€ê°€ ì‚½ì…ë˜ì–´ì•¼ í•˜ëŠ” ìœ„ì¹˜ë§ˆë‹¤ `<start_of_image>` í† í°ì´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤.\n+- í”„ë¡œì„¸ì„œì—ëŠ” ì±„íŒ… ë©”ì‹œì§€ë¥¼ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ë³€í™˜í•˜ëŠ” ìì²´ [`~ProcessorMixin.apply_chat_template`] ë©”ì†Œë“œê°€ ìˆìŠµë‹ˆë‹¤.\n+- ê¸°ë³¸ì ìœ¼ë¡œ ì´ë¯¸ì§€ëŠ” ì˜ë¦¬ì§€ ì•Šìœ¼ë©° ê¸°ë³¸ ì´ë¯¸ì§€ë§Œ ëª¨ë¸ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. ê³ í•´ìƒë„ ì´ë¯¸ì§€ë‚˜ ì •ì‚¬ê°í˜•ì´ ì•„ë‹Œ ì¢…íš¡ë¹„ì˜ ì´ë¯¸ì§€ì—ì„œëŠ” ë¹„ì „ ì¸ì½”ë”ê°€ 896x896ì˜ ê³ ì • í•´ìƒë„ë¥¼ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ì•„í‹°íŒ©íŠ¸ê°€ ë°œìƒí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ ì•„í‹°íŒ©íŠ¸ë¥¼ ë°©ì§€í•˜ê³  ì¶”ë¡  ì¤‘ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ë ¤ë©´, `do_pan_and_scan=True`ë¥¼ ì„¤ì •í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì—¬ëŸ¬ ê°œì˜ ì‘ì€ íŒ¨ì¹˜ë¡œ ìë¥´ê³  ê¸°ë³¸ ì´ë¯¸ì§€ ì„ë² ë”©ê³¼ ì´ì–´ ë¶™ì…ë‹ˆë‹¤. ë” ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•´ íŒ¬ê³¼ ìŠ¤ìº”ì„ ë¹„í™œì„±í™”í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+    ```diff\n+    inputs = processor.apply_chat_template(\n+        messages,\n+        tokenize=True,\n+        return_dict=True,\n+        return_tensors=\"pt\",\n+        add_generation_prompt=True,\n+    +   do_pan_and_scan=True,\n+        ).to(\"cuda\")\n+    ```\n+- í…ìŠ¤íŠ¸ ì „ìš© ëª¨ë“œë¡œ í›ˆë ¨ëœ Gemma-3 1B ì²´í¬í¬ì¸íŠ¸ì˜ ê²½ìš°, [`AutoModelForCausalLM`]ì„ ëŒ€ì‹  ì‚¬ìš©í•˜ì„¸ìš”.\n+\n+    ```py\n+    import torch\n+    from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+    tokenizer = AutoTokenizer.from_pretrained(\n+        \"google/gemma-3-1b-pt\",\n+    )\n+    model = AutoModelForCausalLM.from_pretrained(\n+        \"google/gemma-3-1b-pt\",\n+        torch_dtype=torch.bfloat16,\n+        device_map=\"auto\",\n+        attn_implementation=\"sdpa\"\n+    )\n+    input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(\"cuda\")\n+\n+    output = model.generate(**input_ids, cache_implementation=\"static\")\n+    print(tokenizer.decode(output[0], skip_special_tokens=True))\n+    ```\n+\n+## Gemma3ImageProcessor\n+\n+[[autodoc]] Gemma3ImageProcessor\n+\n+## Gemma3ImageProcessorFast\n+\n+[[autodoc]] Gemma3ImageProcessorFast\n+\n+## Gemma3Processor\n+\n+[[autodoc]] Gemma3Processor\n+\n+## Gemma3TextConfig\n+\n+[[autodoc]] Gemma3TextConfig\n+\n+## Gemma3Config\n+\n+[[autodoc]] Gemma3Config\n+\n+## Gemma3TextModel\n+\n+[[autodoc]] Gemma3TextModel\n+    - forward\n+\n+## Gemma3Model\n+\n+[[autodoc]] Gemma3Model\n+\n+## Gemma3ForCausalLM\n+\n+[[autodoc]] Gemma3ForCausalLM\n+    - forward\n+\n+## Gemma3ForConditionalGeneration\n+\n+[[autodoc]] Gemma3ForConditionalGeneration\n+    - forward\n+\n+## Gemma3ForSequenceClassification\n+\n+[[autodoc]] Gemma3ForSequenceClassification\n+    - forward"
        }
    ],
    "stats": {
        "total": 276,
        "additions": 275,
        "deletions": 1
    }
}