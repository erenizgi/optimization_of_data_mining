{
    "author": "faaany",
    "message": "[tests] further fix `Tester object has no attribute '_testMethodName'`  (#35781)\n\n* bug fix\r\n\r\n* update with more cases\r\n\r\n* more entries\r\n\r\n* Fix\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "f0ae65c198a818df52e7abdf9d02fe6011faf55c",
    "files": [
        {
            "sha": "ca8eeec59131a89af8da1cf4a6a7f53995554070",
            "filename": "tests/models/albert/test_modeling_flax_albert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Falbert%2Ftest_modeling_flax_albert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Falbert%2Ftest_modeling_flax_albert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falbert%2Ftest_modeling_flax_albert.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -36,7 +36,7 @@\n     )\n \n \n-class FlaxAlbertModelTester(unittest.TestCase):\n+class FlaxAlbertModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -80,7 +80,6 @@ def __init__(\n         self.type_sequence_label_size = type_sequence_label_size\n         self.initializer_range = initializer_range\n         self.num_choices = num_choices\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)"
        },
        {
            "sha": "255903e4077e4f75986c01d2845c0bbbfb16a054",
            "filename": "tests/models/aria/test_image_processing_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -35,7 +35,7 @@\n     import torch\n \n \n-class AriaImageProcessingTester(unittest.TestCase):\n+class AriaImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -55,7 +55,6 @@ def __init__(\n         do_convert_rgb=True,\n         resample=PILImageResampling.BICUBIC,\n     ):\n-        super().__init__()\n         self.size = size if size is not None else {\"longest_edge\": max_resolution}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "2ac3668d3b094ad690657317bfe7380887d8d554",
            "filename": "tests/models/beit/test_modeling_flax_beit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fbeit%2Ftest_modeling_flax_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fbeit%2Ftest_modeling_flax_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_flax_beit.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -36,7 +36,7 @@\n     from transformers import BeitImageProcessor\n \n \n-class FlaxBeitModelTester(unittest.TestCase):\n+class FlaxBeitModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -79,7 +79,6 @@ def __init__(\n         # in BeiT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n         num_patches = (image_size // patch_size) ** 2\n         self.seq_length = num_patches + 1\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])"
        },
        {
            "sha": "72d5c951e68b6c2af1fc1bff6e8b65573eb0e0da",
            "filename": "tests/models/bert/test_modeling_flax_bert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fbert%2Ftest_modeling_flax_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fbert%2Ftest_modeling_flax_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbert%2Ftest_modeling_flax_bert.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -35,7 +35,7 @@\n     )\n \n \n-class FlaxBertModelTester(unittest.TestCase):\n+class FlaxBertModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -79,7 +79,6 @@ def __init__(\n         self.type_sequence_label_size = type_sequence_label_size\n         self.initializer_range = initializer_range\n         self.num_choices = num_choices\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)"
        },
        {
            "sha": "8beb12b8c6c834c6445ec4f80ca8eaf5e3c5ca25",
            "filename": "tests/models/big_bird/test_modeling_flax_big_bird.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_flax_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_flax_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbig_bird%2Ftest_modeling_flax_big_bird.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -35,7 +35,7 @@\n     )\n \n \n-class FlaxBigBirdModelTester(unittest.TestCase):\n+class FlaxBigBirdModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -90,7 +90,6 @@ def __init__(\n         self.use_bias = use_bias\n         self.block_size = block_size\n         self.num_random_blocks = num_random_blocks\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)"
        },
        {
            "sha": "038a051a557c06a21dbe82f1c9362f0338fb4440",
            "filename": "tests/models/blip/test_image_processing_blip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fblip%2Ftest_image_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fblip%2Ftest_image_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_image_processing_blip.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -26,7 +26,7 @@\n     from transformers import BlipImageProcessor\n \n \n-class BlipImageProcessingTester(unittest.TestCase):\n+class BlipImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -43,7 +43,6 @@ def __init__(\n         image_std=[0.26862954, 0.26130258, 0.27577711],\n         do_convert_rgb=True,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 20, \"width\": 20}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "d70715c78a7ff8f965a7797716111739f94b9471",
            "filename": "tests/models/bridgetower/test_image_processing_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -31,7 +31,7 @@\n     from transformers import BridgeTowerImageProcessor\n \n \n-class BridgeTowerImageProcessingTester(unittest.TestCase):\n+class BridgeTowerImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -50,7 +50,6 @@ def __init__(\n         max_resolution=400,\n         num_channels=3,\n     ):\n-        super().__init__()\n         self.parent = parent\n         self.do_resize = do_resize\n         self.size = size if size is not None else {\"shortest_edge\": 288}"
        },
        {
            "sha": "1948e58f9ff15dabce81e91cf7d5b86e114e04dc",
            "filename": "tests/models/chameleon/test_image_processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fchameleon%2Ftest_image_processing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fchameleon%2Ftest_image_processing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_image_processing_chameleon.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -32,7 +32,7 @@\n     from transformers import ChameleonImageProcessor\n \n \n-class ChameleonImageProcessingTester(unittest.TestCase):\n+class ChameleonImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -50,7 +50,6 @@ def __init__(\n         image_std=[1.0, 1.0, 1.0],\n         do_convert_rgb=True,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"shortest_edge\": 18}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent"
        },
        {
            "sha": "01bea098ebd5a9833d93186ae1f74a231aca2ca6",
            "filename": "tests/models/chinese_clip/test_image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fchinese_clip%2Ftest_image_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fchinese_clip%2Ftest_image_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_image_processing_chinese_clip.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -26,7 +26,7 @@\n     from transformers import ChineseCLIPImageProcessor\n \n \n-class ChineseCLIPImageProcessingTester(unittest.TestCase):\n+class ChineseCLIPImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -44,7 +44,6 @@ def __init__(\n         image_std=[0.26862954, 0.26130258, 0.27577711],\n         do_convert_rgb=True,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 224, \"width\": 224}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent"
        },
        {
            "sha": "9ddfc0d41466dfa13479d9ae67e1d8f4a7ad268e",
            "filename": "tests/models/convnext/test_image_processing_convnext.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fconvnext%2Ftest_image_processing_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fconvnext%2Ftest_image_processing_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fconvnext%2Ftest_image_processing_convnext.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -26,7 +26,7 @@\n     from transformers import ConvNextImageProcessor\n \n \n-class ConvNextImageProcessingTester(unittest.TestCase):\n+class ConvNextImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -42,7 +42,6 @@ def __init__(\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"shortest_edge\": 20}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "25aa486b542591c507f7501dbb546ad3036dcf13",
            "filename": "tests/models/deformable_detr/test_image_processing_deformable_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeformable_detr%2Ftest_image_processing_deformable_detr.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -35,7 +35,7 @@\n     from transformers import DeformableDetrImageProcessor, DeformableDetrImageProcessorFast\n \n \n-class DeformableDetrImageProcessingTester(unittest.TestCase):\n+class DeformableDetrImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -52,7 +52,6 @@ def __init__(\n         rescale_factor=1 / 255,\n         do_pad=True,\n     ):\n-        super().__init__()\n         # by setting size[\"longest_edge\"] > max_resolution we're effectively not testing this :p\n         size = size if size is not None else {\"shortest_edge\": 18, \"longest_edge\": 1333}\n         self.parent = parent"
        },
        {
            "sha": "18487594771243769ca28b4770a6e6aef253e2fd",
            "filename": "tests/models/deit/test_image_processing_deit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdeit%2Ftest_image_processing_deit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdeit%2Ftest_image_processing_deit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeit%2Ftest_image_processing_deit.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -26,7 +26,7 @@\n     from transformers import DeiTImageProcessor\n \n \n-class DeiTImageProcessingTester(unittest.TestCase):\n+class DeiTImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -43,7 +43,6 @@ def __init__(\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 20, \"width\": 20}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n "
        },
        {
            "sha": "6f19de09dc476954f5fa1e67f07c79e2f23afa97",
            "filename": "tests/models/detr/test_image_processing_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_image_processing_detr.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -37,7 +37,7 @@\n         from transformers import DetrImageProcessorFast\n \n \n-class DetrImageProcessingTester(unittest.TestCase):\n+class DetrImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -54,7 +54,6 @@ def __init__(\n         image_std=[0.5, 0.5, 0.5],\n         do_pad=True,\n     ):\n-        super().__init__()\n         # by setting size[\"longest_edge\"] > max_resolution we're effectively not testing this :p\n         size = size if size is not None else {\"shortest_edge\": 18, \"longest_edge\": 1333}\n         self.parent = parent"
        },
        {
            "sha": "50655771ed14a575afd9388bab48d1440a291c67",
            "filename": "tests/models/distilbert/test_modeling_flax_distilbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_flax_distilbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_flax_distilbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdistilbert%2Ftest_modeling_flax_distilbert.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -35,7 +35,7 @@\n     )\n \n \n-class FlaxDistilBertModelTester(unittest.TestCase):\n+class FlaxDistilBertModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -79,7 +79,6 @@ def __init__(\n         self.type_sequence_label_size = type_sequence_label_size\n         self.initializer_range = initializer_range\n         self.num_choices = num_choices\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)"
        },
        {
            "sha": "052894c0cbd84a6056ff6d8676c7ed45de47959c",
            "filename": "tests/models/donut/test_image_processing_donut.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdonut%2Ftest_image_processing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdonut%2Ftest_image_processing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdonut%2Ftest_image_processing_donut.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -33,7 +33,7 @@\n     from transformers import DonutImageProcessor\n \n \n-class DonutImageProcessingTester(unittest.TestCase):\n+class DonutImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -51,7 +51,6 @@ def __init__(\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n     ):\n-        super().__init__()\n         self.parent = parent\n         self.batch_size = batch_size\n         self.num_channels = num_channels"
        },
        {
            "sha": "feb551d3bec8c7d0e373b49e5dd149d7de26986a",
            "filename": "tests/models/dpt/test_image_processing_dpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdpt%2Ftest_image_processing_dpt.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -34,7 +34,7 @@\n     from transformers import DPTImageProcessor\n \n \n-class DPTImageProcessingTester(unittest.TestCase):\n+class DPTImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -50,7 +50,6 @@ def __init__(\n         image_std=[0.5, 0.5, 0.5],\n         do_reduce_labels=False,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "618885dc97bed9afa009caa641e367912b219e61",
            "filename": "tests/models/efficientnet/test_image_processing_efficientnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fefficientnet%2Ftest_image_processing_efficientnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fefficientnet%2Ftest_image_processing_efficientnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientnet%2Ftest_image_processing_efficientnet.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -28,7 +28,7 @@\n     from transformers import EfficientNetImageProcessor\n \n \n-class EfficientNetImageProcessorTester(unittest.TestCase):\n+class EfficientNetImageProcessorTester:\n     def __init__(\n         self,\n         parent,\n@@ -43,7 +43,6 @@ def __init__(\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "698a492fc3c7972f7fdbdeca6ba53537e3819d0d",
            "filename": "tests/models/electra/test_modeling_flax_electra.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Felectra%2Ftest_modeling_flax_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Felectra%2Ftest_modeling_flax_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Felectra%2Ftest_modeling_flax_electra.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -21,7 +21,7 @@\n     )\n \n \n-class FlaxElectraModelTester(unittest.TestCase):\n+class FlaxElectraModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -67,7 +67,6 @@ def __init__(\n         self.type_sequence_label_size = type_sequence_label_size\n         self.initializer_range = initializer_range\n         self.num_choices = num_choices\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)"
        },
        {
            "sha": "e3c1f5b5b53663fd1409958a844470f2094f4650",
            "filename": "tests/models/flava/test_image_processing_flava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fflava%2Ftest_image_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fflava%2Ftest_image_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_image_processing_flava.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -42,7 +42,7 @@\n     FLAVA_IMAGE_MEAN = FLAVA_IMAGE_STD = FLAVA_CODEBOOK_MEAN = FLAVA_CODEBOOK_STD = None\n \n \n-class FlavaImageProcessingTester(unittest.TestCase):\n+class FlavaImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -76,7 +76,6 @@ def __init__(\n         codebook_image_mean=FLAVA_CODEBOOK_MEAN,\n         codebook_image_std=FLAVA_CODEBOOK_STD,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 224, \"width\": 224}\n         crop_size = crop_size if crop_size is not None else {\"height\": 224, \"width\": 224}\n         codebook_size = codebook_size if codebook_size is not None else {\"height\": 112, \"width\": 112}"
        },
        {
            "sha": "43d5da070720e2776b30da08bb6d51d1e6d89509",
            "filename": "tests/models/glpn/test_image_processing_glpn.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fglpn%2Ftest_image_processing_glpn.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fglpn%2Ftest_image_processing_glpn.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglpn%2Ftest_image_processing_glpn.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -33,7 +33,7 @@\n     from transformers import GLPNImageProcessor\n \n \n-class GLPNImageProcessingTester(unittest.TestCase):\n+class GLPNImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -46,7 +46,6 @@ def __init__(\n         size_divisor=32,\n         do_rescale=True,\n     ):\n-        super().__init__()\n         self.parent = parent\n         self.batch_size = batch_size\n         self.num_channels = num_channels"
        },
        {
            "sha": "d5e3870006ab56dbeb952e2e40fb836806f05e61",
            "filename": "tests/models/idefics3/test_image_processing_idefics3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -35,7 +35,7 @@\n     import torch\n \n \n-class Idefics3ImageProcessingTester(unittest.TestCase):\n+class Idefics3ImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -58,7 +58,6 @@ def __init__(\n         do_image_splitting=True,\n         resample=PILImageResampling.LANCZOS,\n     ):\n-        super().__init__()\n         self.size = size if size is not None else {\"longest_edge\": max_resolution}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "d32e954cca2575e2dc93ebcaba6a2a51bf1eab7e",
            "filename": "tests/models/imagegpt/test_image_processing_imagegpt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fimagegpt%2Ftest_image_processing_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fimagegpt%2Ftest_image_processing_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fimagegpt%2Ftest_image_processing_imagegpt.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -38,7 +38,7 @@\n     from transformers import ImageGPTImageProcessor\n \n \n-class ImageGPTImageProcessingTester(unittest.TestCase):\n+class ImageGPTImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -51,7 +51,6 @@ def __init__(\n         size=None,\n         do_normalize=True,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "dacb922e7216502d9c8adecf9b285fe6d8a71fcd",
            "filename": "tests/models/instructblipvideo/test_image_processing_instrictblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Finstructblipvideo%2Ftest_image_processing_instrictblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Finstructblipvideo%2Ftest_image_processing_instrictblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_image_processing_instrictblipvideo.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -33,7 +33,7 @@\n     from transformers import InstructBlipVideoImageProcessor\n \n \n-class InstructBlipVideoProcessingTester(unittest.TestCase):\n+class InstructBlipVideoProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -50,7 +50,6 @@ def __init__(\n         do_convert_rgb=True,\n         frames=4,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "41cc9943cf27045591af7d7bc58a139acb656fad",
            "filename": "tests/models/layoutlmv2/test_image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -28,7 +28,7 @@\n     from transformers import LayoutLMv2ImageProcessor\n \n \n-class LayoutLMv2ImageProcessingTester(unittest.TestCase):\n+class LayoutLMv2ImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -41,7 +41,6 @@ def __init__(\n         size=None,\n         apply_ocr=True,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "a6aebc88b27f411d9f0739a79a6ff4ae7d548f62",
            "filename": "tests/models/layoutlmv3/test_image_processing_layoutlmv3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Flayoutlmv3%2Ftest_image_processing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Flayoutlmv3%2Ftest_image_processing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_image_processing_layoutlmv3.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -28,7 +28,7 @@\n     from transformers import LayoutLMv3ImageProcessor\n \n \n-class LayoutLMv3ImageProcessingTester(unittest.TestCase):\n+class LayoutLMv3ImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -41,7 +41,6 @@ def __init__(\n         size=None,\n         apply_ocr=True,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "c04656ea8bdada6983583caf415bc38a5ae5312c",
            "filename": "tests/models/levit/test_image_processing_levit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Flevit%2Ftest_image_processing_levit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Flevit%2Ftest_image_processing_levit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flevit%2Ftest_image_processing_levit.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -26,7 +26,7 @@\n     from transformers import LevitImageProcessor\n \n \n-class LevitImageProcessingTester(unittest.TestCase):\n+class LevitImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -43,7 +43,6 @@ def __init__(\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"shortest_edge\": 18}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent"
        },
        {
            "sha": "99124f66b4a492ce8ab22f76b2e35842f1afd0df",
            "filename": "tests/models/llava/test_image_processing_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fllava%2Ftest_image_processing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fllava%2Ftest_image_processing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_image_processing_llava.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -31,7 +31,7 @@\n     from transformers import LlavaImageProcessor\n \n \n-class LlavaImageProcessingTester(unittest.TestCase):\n+class LlavaImageProcessingTester:\n     def __init__(\n         self,\n         parent,"
        },
        {
            "sha": "737728587352972585f8ab3225c41fa33a2481c8",
            "filename": "tests/models/mbart/test_modeling_flax_mbart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fmbart%2Ftest_modeling_flax_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fmbart%2Ftest_modeling_flax_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmbart%2Ftest_modeling_flax_mbart.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -74,7 +74,7 @@ def prepare_mbart_inputs_dict(\n     }\n \n \n-class FlaxMBartModelTester(unittest.TestCase):\n+class FlaxMBartModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -116,7 +116,6 @@ def __init__(\n         self.bos_token_id = bos_token_id\n         self.decoder_start_token_id = decoder_start_token_id\n         self.initializer_range = initializer_range\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         input_ids = np.clip(ids_tensor([self.batch_size, self.seq_length - 1], self.vocab_size), 3, self.vocab_size)"
        },
        {
            "sha": "351f1f16f2993ba94ca3849104cf6e84595ad921",
            "filename": "tests/models/mllama/test_image_processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fmllama%2Ftest_image_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fmllama%2Ftest_image_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_image_processing_mllama.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -34,7 +34,7 @@\n     import torch\n \n \n-class MllamaImageProcessingTester(unittest.TestCase):\n+class MllamaImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -55,7 +55,6 @@ def __init__(\n         do_pad=True,\n         max_image_tiles=4,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 224, \"width\": 224}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "c6553d445b15f2539e4449a73663691c529716f3",
            "filename": "tests/models/mobilenet_v1/test_image_processing_mobilenet_v1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fmobilenet_v1%2Ftest_image_processing_mobilenet_v1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fmobilenet_v1%2Ftest_image_processing_mobilenet_v1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilenet_v1%2Ftest_image_processing_mobilenet_v1.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -26,7 +26,7 @@\n     from transformers import MobileNetV1ImageProcessor\n \n \n-class MobileNetV1ImageProcessingTester(unittest.TestCase):\n+class MobileNetV1ImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -40,7 +40,6 @@ def __init__(\n         do_center_crop=True,\n         crop_size=None,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"shortest_edge\": 20}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent"
        },
        {
            "sha": "cf583e017572ed3ce1cf6897772dd598c1cef3fc",
            "filename": "tests/models/mobilenet_v2/test_image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -26,7 +26,7 @@\n     from transformers import MobileNetV2ImageProcessor\n \n \n-class MobileNetV2ImageProcessingTester(unittest.TestCase):\n+class MobileNetV2ImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -40,7 +40,6 @@ def __init__(\n         do_center_crop=True,\n         crop_size=None,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"shortest_edge\": 20}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent"
        },
        {
            "sha": "976c8c68e9b249f35490133d39a56d275a0156d5",
            "filename": "tests/models/mobilevit/test_image_processing_mobilevit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -33,7 +33,7 @@\n     from transformers import MobileViTImageProcessor\n \n \n-class MobileViTImageProcessingTester(unittest.TestCase):\n+class MobileViTImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -48,7 +48,6 @@ def __init__(\n         crop_size=None,\n         do_flip_channel_order=True,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"shortest_edge\": 20}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent"
        },
        {
            "sha": "69382a1c3fad448e2ca04c6760367d50d7e6f2ed",
            "filename": "tests/models/nougat/test_image_processing_nougat.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -34,7 +34,7 @@\n     from transformers import NougatImageProcessor\n \n \n-class NougatImageProcessingTester(unittest.TestCase):\n+class NougatImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -53,7 +53,6 @@ def __init__(\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 20, \"width\": 20}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "cc6cf4b8330a8276560a36a446602d75b966ddc4",
            "filename": "tests/models/owlv2/test_image_processing_owlv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fowlv2%2Ftest_image_processing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fowlv2%2Ftest_image_processing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_image_processing_owlv2.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -31,7 +31,7 @@\n     import torch\n \n \n-class Owlv2ImageProcessingTester(unittest.TestCase):\n+class Owlv2ImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -47,7 +47,6 @@ def __init__(\n         image_std=[0.26862954, 0.26130258, 0.27577711],\n         do_convert_rgb=True,\n     ):\n-        super().__init__()\n         self.parent = parent\n         self.batch_size = batch_size\n         self.num_channels = num_channels"
        },
        {
            "sha": "d6297de2290c1945ebe7db0a46e4a89536856799",
            "filename": "tests/models/owlvit/test_image_processing_owlvit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fowlvit%2Ftest_image_processing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fowlvit%2Ftest_image_processing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_image_processing_owlvit.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -26,7 +26,7 @@\n     from transformers import OwlViTImageProcessor\n \n \n-class OwlViTImageProcessingTester(unittest.TestCase):\n+class OwlViTImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -44,7 +44,6 @@ def __init__(\n         image_std=[0.26862954, 0.26130258, 0.27577711],\n         do_convert_rgb=True,\n     ):\n-        super().__init__()\n         self.parent = parent\n         self.batch_size = batch_size\n         self.num_channels = num_channels"
        },
        {
            "sha": "047cd23d36aa0778ebca53626d95eda74fe297fa",
            "filename": "tests/models/poolformer/test_image_processing_poolformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fpoolformer%2Ftest_image_processing_poolformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fpoolformer%2Ftest_image_processing_poolformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpoolformer%2Ftest_image_processing_poolformer.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -25,7 +25,7 @@\n     from transformers import PoolFormerImageProcessor\n \n \n-class PoolFormerImageProcessingTester(unittest.TestCase):\n+class PoolFormerImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -41,7 +41,6 @@ def __init__(\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"shortest_edge\": 30}\n         crop_size = crop_size if crop_size is not None else {\"height\": 30, \"width\": 30}\n         self.parent = parent"
        },
        {
            "sha": "de093307abeb69959ce99ceb0c3d5fe2cb606bca",
            "filename": "tests/models/pvt/test_image_processing_pvt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fpvt%2Ftest_image_processing_pvt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fpvt%2Ftest_image_processing_pvt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpvt%2Ftest_image_processing_pvt.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -26,7 +26,7 @@\n     from transformers import PvtImageProcessor\n \n \n-class PvtImageProcessingTester(unittest.TestCase):\n+class PvtImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -41,7 +41,6 @@ def __init__(\n         image_mean=[0.485, 0.456, 0.406],\n         image_std=[0.229, 0.224, 0.225],\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "30511c84c7949558d738b7c8f75384b34c22c5f1",
            "filename": "tests/models/regnet/test_modeling_flax_regnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fregnet%2Ftest_modeling_flax_regnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fregnet%2Ftest_modeling_flax_regnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fregnet%2Ftest_modeling_flax_regnet.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -36,7 +36,7 @@\n     from transformers import AutoImageProcessor\n \n \n-class FlaxRegNetModelTester(unittest.TestCase):\n+class FlaxRegNetModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -65,7 +65,6 @@ def __init__(\n         self.num_labels = num_labels\n         self.scope = scope\n         self.num_stages = len(hidden_sizes)\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])"
        },
        {
            "sha": "7399405f00c15d5dbe591bd5fe1bd0a8f284e1ca",
            "filename": "tests/models/resnet/test_modeling_flax_resnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fresnet%2Ftest_modeling_flax_resnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fresnet%2Ftest_modeling_flax_resnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fresnet%2Ftest_modeling_flax_resnet.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -35,7 +35,7 @@\n     from transformers import AutoImageProcessor\n \n \n-class FlaxResNetModelTester(unittest.TestCase):\n+class FlaxResNetModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -64,7 +64,6 @@ def __init__(\n         self.num_labels = num_labels\n         self.scope = scope\n         self.num_stages = len(hidden_sizes)\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])"
        },
        {
            "sha": "b9a877d2bdddad1dc3db57d4058ab003c888a911",
            "filename": "tests/models/roberta/test_modeling_flax_roberta.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Froberta%2Ftest_modeling_flax_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Froberta%2Ftest_modeling_flax_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta%2Ftest_modeling_flax_roberta.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -34,7 +34,7 @@\n     )\n \n \n-class FlaxRobertaModelTester(unittest.TestCase):\n+class FlaxRobertaModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -78,7 +78,6 @@ def __init__(\n         self.type_sequence_label_size = type_sequence_label_size\n         self.initializer_range = initializer_range\n         self.num_choices = num_choices\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)"
        },
        {
            "sha": "d464e28640aee80ecc20273b5a0cb97abd279b10",
            "filename": "tests/models/roberta_prelayernorm/test_modeling_flax_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_flax_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_flax_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froberta_prelayernorm%2Ftest_modeling_flax_roberta_prelayernorm.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -37,7 +37,7 @@\n \n \n # Copied from tests.models.roberta.test_modeling_flax_roberta.FlaxRobertaModelTester with Roberta->RobertaPreLayerNorm\n-class FlaxRobertaPreLayerNormModelTester(unittest.TestCase):\n+class FlaxRobertaPreLayerNormModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -81,7 +81,6 @@ def __init__(\n         self.type_sequence_label_size = type_sequence_label_size\n         self.initializer_range = initializer_range\n         self.num_choices = num_choices\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)"
        },
        {
            "sha": "856ed906060b950a72ca472acb6c02f7a01c9373",
            "filename": "tests/models/roformer/test_modeling_flax_roformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Froformer%2Ftest_modeling_flax_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Froformer%2Ftest_modeling_flax_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_flax_roformer.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -35,7 +35,7 @@\n     )\n \n \n-class FlaxRoFormerModelTester(unittest.TestCase):\n+class FlaxRoFormerModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -79,7 +79,6 @@ def __init__(\n         self.type_sequence_label_size = type_sequence_label_size\n         self.initializer_range = initializer_range\n         self.num_choices = num_choices\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)"
        },
        {
            "sha": "e27c1838f940860074017618ca8a49beabb0f241",
            "filename": "tests/models/rt_detr/test_image_processing_rt_detr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -31,7 +31,7 @@\n     import torch\n \n \n-class RTDetrImageProcessingTester(unittest.TestCase):\n+class RTDetrImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -45,7 +45,6 @@ def __init__(\n         do_pad=False,\n         return_tensors=\"pt\",\n     ):\n-        super().__init__()\n         self.parent = parent\n         self.batch_size = batch_size\n         self.num_channels = num_channels"
        },
        {
            "sha": "56653ae7aa5885ec483f2d2cd58df94cb4b5b03e",
            "filename": "tests/models/siglip/test_image_processing_siglip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fsiglip%2Ftest_image_processing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fsiglip%2Ftest_image_processing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_image_processing_siglip.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -26,7 +26,7 @@\n     from transformers import SiglipImageProcessor\n \n \n-class SiglipImageProcessingTester(unittest.TestCase):\n+class SiglipImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -43,7 +43,6 @@ def __init__(\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "1068a9a92bc03a8aabbd0bdb6d875f4b0b9f9a4e",
            "filename": "tests/models/superglue/test_image_processing_superglue.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsuperglue%2Ftest_image_processing_superglue.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -42,7 +42,7 @@ def random_tensor(size):\n     return torch.rand(size)\n \n \n-class SuperGlueImageProcessingTester(unittest.TestCase):\n+class SuperGlueImageProcessingTester:\n     def __init__(\n         self,\n         parent,"
        },
        {
            "sha": "4467a8344433d75a79dea4eb7fb9b62f7c9866ff",
            "filename": "tests/models/swin2sr/test_image_processing_swin2sr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -34,7 +34,7 @@\n     from transformers.image_transforms import get_image_size\n \n \n-class Swin2SRImageProcessingTester(unittest.TestCase):\n+class Swin2SRImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -48,7 +48,6 @@ def __init__(\n         do_pad=True,\n         pad_size=8,\n     ):\n-        super().__init__()\n         self.parent = parent\n         self.batch_size = batch_size\n         self.num_channels = num_channels"
        },
        {
            "sha": "7765397802db2bea2309ac2923bf332c908a8ff1",
            "filename": "tests/models/textnet/test_image_processing_textnet.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Ftextnet%2Ftest_image_processing_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Ftextnet%2Ftest_image_processing_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftextnet%2Ftest_image_processing_textnet.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -26,7 +26,7 @@\n     from transformers import TextNetImageProcessor\n \n \n-class TextNetImageProcessingTester(unittest.TestCase):\n+class TextNetImageProcessingTester:\n     def __init__(\n         self,\n         parent,"
        },
        {
            "sha": "99ddcc51b45763a92cd6018f7bd5326f7a13b806",
            "filename": "tests/models/tvp/test_image_processing_tvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftvp%2Ftest_image_processing_tvp.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -35,7 +35,7 @@\n     from transformers import TvpImageProcessor\n \n \n-class TvpImageProcessingTester(unittest.TestCase):\n+class TvpImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -58,7 +58,6 @@ def __init__(\n         num_channels=3,\n         num_frames=2,\n     ):\n-        super().__init__()\n         self.do_resize = do_resize\n         self.size = size\n         self.do_center_crop = do_center_crop"
        },
        {
            "sha": "4b877ab325cdcd7e2eea0c05175509ae33e9191c",
            "filename": "tests/models/video_llava/test_image_processing_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvideo_llava%2Ftest_image_processing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvideo_llava%2Ftest_image_processing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_image_processing_video_llava.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -34,7 +34,7 @@\n     from transformers import VideoLlavaImageProcessor\n \n \n-class VideoLlavaImageProcessingTester(unittest.TestCase):\n+class VideoLlavaImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -52,7 +52,6 @@ def __init__(\n         image_std=OPENAI_CLIP_STD,\n         do_convert_rgb=True,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"shortest_edge\": 20}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent"
        },
        {
            "sha": "f4e2b26fb809b381a8c051ffedf700c5389223f9",
            "filename": "tests/models/videomae/test_image_processing_videomae.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvideomae%2Ftest_image_processing_videomae.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvideomae%2Ftest_image_processing_videomae.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideomae%2Ftest_image_processing_videomae.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -33,7 +33,7 @@\n     from transformers import VideoMAEImageProcessor\n \n \n-class VideoMAEImageProcessingTester(unittest.TestCase):\n+class VideoMAEImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -50,7 +50,6 @@ def __init__(\n         image_std=[0.5, 0.5, 0.5],\n         crop_size=None,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"shortest_edge\": 18}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n "
        },
        {
            "sha": "bfd70694aa312eb0d9b697ee7a6c1576424c886c",
            "filename": "tests/models/vilt/test_image_processing_vilt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvilt%2Ftest_image_processing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvilt%2Ftest_image_processing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvilt%2Ftest_image_processing_vilt.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -30,7 +30,7 @@\n     from transformers import ViltImageProcessor\n \n \n-class ViltImageProcessingTester(unittest.TestCase):\n+class ViltImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -46,7 +46,6 @@ def __init__(\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"shortest_edge\": 30}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "e628f5d9df36d2bd42568bffa1b0d3262f7e9142",
            "filename": "tests/models/vit/test_image_processing_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvit%2Ftest_image_processing_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvit%2Ftest_image_processing_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit%2Ftest_image_processing_vit.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -29,7 +29,7 @@\n     from transformers import ViTImageProcessorFast\n \n \n-class ViTImageProcessingTester(unittest.TestCase):\n+class ViTImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -44,7 +44,6 @@ def __init__(\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n         self.batch_size = batch_size"
        },
        {
            "sha": "2e352056635d935474c0e42d85190bd3f4d18e5c",
            "filename": "tests/models/vit/test_modeling_flax_vit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvit%2Ftest_modeling_flax_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvit%2Ftest_modeling_flax_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit%2Ftest_modeling_flax_vit.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -30,7 +30,7 @@\n     from transformers.models.vit.modeling_flax_vit import FlaxViTForImageClassification, FlaxViTModel\n \n \n-class FlaxViTModelTester(unittest.TestCase):\n+class FlaxViTModelTester:\n     def __init__(\n         self,\n         parent,\n@@ -72,7 +72,6 @@ def __init__(\n         # in ViT, the seq length equals the number of patches + 1 (we add 1 for the [CLS] token)\n         num_patches = (image_size // patch_size) ** 2\n         self.seq_length = num_patches + 1\n-        super().__init__()\n \n     def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])"
        },
        {
            "sha": "340957fd3e557b942d44c500ac1e8ac756cacb22",
            "filename": "tests/models/vitmatte/test_image_processing_vitmatte.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -35,7 +35,7 @@\n     from transformers import VitMatteImageProcessor\n \n \n-class VitMatteImageProcessingTester(unittest.TestCase):\n+class VitMatteImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -52,7 +52,6 @@ def __init__(\n         image_mean=[0.5, 0.5, 0.5],\n         image_std=[0.5, 0.5, 0.5],\n     ):\n-        super().__init__()\n         self.parent = parent\n         self.batch_size = batch_size\n         self.num_channels = num_channels"
        },
        {
            "sha": "238fe20aa61661a442c7be7e04bf8a7c06f81247",
            "filename": "tests/models/vitpose/test_image_processing_vitpose.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitpose%2Ftest_image_processing_vitpose.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -34,7 +34,7 @@\n     from transformers import VitPoseImageProcessor\n \n \n-class VitPoseImageProcessingTester(unittest.TestCase):\n+class VitPoseImageProcessingTester:\n     def __init__(\n         self,\n         parent,"
        },
        {
            "sha": "3d05a3668985a65b244899ee5501f6bdd4277db5",
            "filename": "tests/models/vivit/test_image_processing_vivit.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvivit%2Ftest_image_processing_vivit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fvivit%2Ftest_image_processing_vivit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvivit%2Ftest_image_processing_vivit.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -33,7 +33,7 @@\n     from transformers import VivitImageProcessor\n \n \n-class VivitImageProcessingTester(unittest.TestCase):\n+class VivitImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -50,7 +50,6 @@ def __init__(\n         image_std=[0.5, 0.5, 0.5],\n         crop_size=None,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"shortest_edge\": 18}\n         crop_size = crop_size if crop_size is not None else {\"height\": 18, \"width\": 18}\n "
        },
        {
            "sha": "ba4986c46fd47aa2f35e5b5fcce1d80405370397",
            "filename": "tests/models/zoedepth/test_image_processing_zoedepth.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fzoedepth%2Ftest_image_processing_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0ae65c198a818df52e7abdf9d02fe6011faf55c/tests%2Fmodels%2Fzoedepth%2Ftest_image_processing_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzoedepth%2Ftest_image_processing_zoedepth.py?ref=f0ae65c198a818df52e7abdf9d02fe6011faf55c",
            "patch": "@@ -28,7 +28,7 @@\n     from transformers import ZoeDepthImageProcessor\n \n \n-class ZoeDepthImageProcessingTester(unittest.TestCase):\n+class ZoeDepthImageProcessingTester:\n     def __init__(\n         self,\n         parent,\n@@ -46,7 +46,6 @@ def __init__(\n         image_std=[0.5, 0.5, 0.5],\n         do_pad=False,\n     ):\n-        super().__init__()\n         size = size if size is not None else {\"height\": 18, \"width\": 18}\n         self.parent = parent\n         self.batch_size = batch_size"
        }
    ],
    "stats": {
        "total": 167,
        "additions": 57,
        "deletions": 110
    }
}