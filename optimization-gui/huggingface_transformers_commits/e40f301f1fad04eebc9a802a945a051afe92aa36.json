{
    "author": "zucchini-nlp",
    "message": "[smolvlm] skip the test (#38099)\n\nskip the test",
    "sha": "e40f301f1fad04eebc9a802a945a051afe92aa36",
    "files": [
        {
            "sha": "b858cc68a4e30708e5687452997d4ca965808909",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/e40f301f1fad04eebc9a802a945a051afe92aa36/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e40f301f1fad04eebc9a802a945a051afe92aa36/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=e40f301f1fad04eebc9a802a945a051afe92aa36",
            "patch": "@@ -180,13 +180,15 @@ def __init__(\n \n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template, **kwargs)\n \n-    def process_vision(self, text, images, output_kwargs, do_image_splitting=False, image_processor_size=None):\n+    def process_vision(\n+        self, text, images, output_kwargs, do_image_splitting=False, image_processor_size=None, processor=None\n+    ):\n         if text is not None:\n             n_images_in_text = [sample.count(self.image_token) for sample in text]\n \n         n_images_in_images = [len(sublist) for sublist in images]\n-        image_inputs = self.image_processor(\n-            images, do_image_splitting=do_image_splitting, size=image_processor_size, **output_kwargs[\"images_kwargs\"]\n+        image_inputs = processor(\n+            images, do_image_splitting=do_image_splitting, size=image_processor_size, **output_kwargs\n         )\n \n         if text is None:\n@@ -309,19 +311,21 @@ def __call__(\n             text, vision_inputs = self.process_vision(\n                 text,\n                 images,\n-                output_kwargs,\n+                output_kwargs[\"images_kwargs\"],\n                 do_image_splitting=self.do_image_splitting,\n                 image_processor_size=self.image_size,\n+                processor=self.image_processor,\n             )\n             inputs.update(vision_inputs)\n         elif videos is not None:\n             videos = make_batched_videos(videos)\n             text, vision_inputs = self.process_vision(\n                 text,\n                 videos,\n-                output_kwargs,\n+                output_kwargs[\"videos_kwargs\"],\n                 do_image_splitting=self.do_image_splitting,\n                 image_processor_size=self.video_size,\n+                processor=self.video_processor,\n             )\n             inputs.update(vision_inputs)\n "
        },
        {
            "sha": "252926ffb07fa2664b1e19b1213cbe1ec865e70a",
            "filename": "tests/models/smolvlm/test_processor_smolvlm.py",
            "status": "modified",
            "additions": 26,
            "deletions": 5,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/e40f301f1fad04eebc9a802a945a051afe92aa36/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e40f301f1fad04eebc9a802a945a051afe92aa36/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processor_smolvlm.py?ref=e40f301f1fad04eebc9a802a945a051afe92aa36",
            "patch": "@@ -22,7 +22,7 @@\n \n from transformers import SmolVLMProcessor\n from transformers.models.auto.processing_auto import AutoProcessor\n-from transformers.testing_utils import is_flaky, require_av, require_torch, require_vision\n+from transformers.testing_utils import require_av, require_torch, require_vision\n from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin\n@@ -118,10 +118,6 @@ def get_split_image_expected_tokens(self, processor, image_rows, image_cols):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n-    @is_flaky  # fails 15 out of 100, FIXME @raushan\n-    def test_structured_kwargs_nested_from_dict_video(self):\n-        super().test_structured_kwargs_nested_from_dict_video()\n-\n     def test_process_interleaved_images_prompts_no_image_splitting(self):\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding_side=\"left\")\n@@ -467,6 +463,31 @@ def test_unstructured_kwargs_batched(self):\n         self.assertEqual(inputs[\"pixel_values\"].shape[3], 300)\n         self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n \n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched_video(self):\n+        if \"video_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"video_processor attribute not present in {self.processor_class}\")\n+        processor_components = self.prepare_components()\n+        processor_kwargs = self.prepare_processor_dict()\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = self.prepare_text_inputs(batch_size=2, modality=\"video\")\n+        video_input = self.prepare_video_inputs(batch_size=2)\n+        inputs = processor(\n+            text=input_str,\n+            videos=video_input,\n+            return_tensors=\"pt\",\n+            do_rescale=True,\n+            rescale_factor=-1,\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertLessEqual(inputs[self.videos_input_name][0].mean(), 0)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n     @require_torch\n     @require_vision\n     def test_text_only_inference(self):"
        }
    ],
    "stats": {
        "total": 45,
        "additions": 35,
        "deletions": 10
    }
}