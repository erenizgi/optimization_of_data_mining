{
    "author": "TrickEye",
    "message": "Fix: typo  (#33880)\n\nUpdate llm_tutorial.md: typo",
    "sha": "2292be6c1b139a2f425dedc650e7cd59bfdbc5f3",
    "files": [
        {
            "sha": "097d7bf1e9ca38b0192e4b71b92cc60d1c53f291",
            "filename": "docs/source/en/llm_tutorial.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2292be6c1b139a2f425dedc650e7cd59bfdbc5f3/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2292be6c1b139a2f425dedc650e7cd59bfdbc5f3/docs%2Fsource%2Fen%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_tutorial.md?ref=2292be6c1b139a2f425dedc650e7cd59bfdbc5f3",
            "patch": "@@ -164,7 +164,7 @@ If not specified in the [`~generation.GenerationConfig`] file, `generate` return\n By default, and unless specified in the [`~generation.GenerationConfig`] file, `generate` selects the most likely token at each iteration (greedy decoding). Depending on your task, this may be undesirable; creative tasks like chatbots or writing an essay benefit from sampling. On the other hand, input-grounded tasks like audio transcription or translation benefit from greedy decoding. Enable sampling with `do_sample=True`, and you can learn more about this topic in this [blog post](https://huggingface.co/blog/how-to-generate).\n \n ```py\n->>> # Set seed or reproducibility -- you don't need this unless you want full reproducibility\n+>>> # Set seed for reproducibility -- you don't need this unless you want full reproducibility\n >>> from transformers import set_seed\n >>> set_seed(42)\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}