{
    "author": "pushkar-hue",
    "message": "gpt-oss is not working with flash-attention (#42736)\n\n* gpt-oss not working with flash attention\n\n* changed conditions to allow vllm kernel\n\n* changed conditions to allow vllm kernel and modeling file generated\n\n* test for flash attention initialization error\n\n* fixing modular\n\n* import error\n\n* tets fixed\n\n* removed unnecessary import\n\n* fixed styling and generated modeling file\n\n* updated tests to include runtime check for flash attn\n\n* fix dtype\n\n* add marker\n\n* other small fixups\n\n* moved attn_implementation check to config\n\n* modeling file generated after moving config logic\n\n* updated tests\n\n* reformatted\n\n* fixup to work with `set_attn_implementation`\n\n* style\n\n* fixup comment\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\nCo-authored-by: PRACHI PANDEY <avinashpandey@PRACHIs-MacBook-Air.local>\nCo-authored-by: vasqu <antonprogamer@gmail.com>",
    "sha": "e8c51d1848187b9e58d00bf7d638811686ab2a4b",
    "files": [
        {
            "sha": "0d38dd6f36631d63aff821d463a11ddeaaac5ca8",
            "filename": "src/transformers/models/gpt_oss/configuration_gpt_oss.py",
            "status": "modified",
            "additions": 17,
            "deletions": 0,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/e8c51d1848187b9e58d00bf7d638811686ab2a4b/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e8c51d1848187b9e58d00bf7d638811686ab2a4b/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fconfiguration_gpt_oss.py?ref=e8c51d1848187b9e58d00bf7d638811686ab2a4b",
            "patch": "@@ -117,5 +117,22 @@ def __init__(\n             **kwargs,\n         )\n \n+    def __setattr__(self, key, value):\n+        \"\"\"\n+        Overwritten to allow checking for the proper attention implementation to be used.\n+\n+        Due to `set_attn_implementation` which internally assigns `_attn_implementation_internal = \"...\"`, simply overwriting\n+        the specific attention setter is not enough. Using a property/setter for `_attn_implementation_internal` would result in\n+        a recursive dependency (as `_attn_implementation` acts as a wrapper around `_attn_implementation_internal`) - hence, this\n+        workaround.\n+        \"\"\"\n+        if key in (\"_attn_implementation\", \"_attn_implementation_internal\"):\n+            if value and \"flash\" in value and value.removeprefix(\"paged|\") != \"kernels-community/vllm-flash-attn3\":\n+                raise ValueError(\n+                    f\"GPT-OSS model does not support the specified flash attention implementation: {value}. \"\n+                    \"Only `kernels-community/vllm-flash-attn3` is supported.\"\n+                )\n+        super().__setattr__(key, value)\n+\n \n __all__ = [\"GptOssConfig\"]"
        },
        {
            "sha": "ee5da80985ac635bfbaad3af125350b60d8f89c1",
            "filename": "src/transformers/models/gpt_oss/modeling_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e8c51d1848187b9e58d00bf7d638811686ab2a4b/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e8c51d1848187b9e58d00bf7d638811686ab2a4b/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodeling_gpt_oss.py?ref=e8c51d1848187b9e58d00bf7d638811686ab2a4b",
            "patch": "@@ -434,7 +434,7 @@ class GptOssPreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = [\"past_key_values\"]\n     _supports_flash_attn = True\n     _supports_sdpa = False\n-    _supports_flex_attn = True\n+    _supports_flex_attn = False\n \n     _can_compile_fullgraph = True\n     _supports_attention_backend = True\n@@ -444,8 +444,6 @@ class GptOssPreTrainedModel(PreTrainedModel):\n         \"attentions\": GptOssAttention,\n     }\n     _keep_in_fp32_modules = [\"post_attention_layernorm\", \"input_layernorm\", \"norm\"]\n-    _supports_flash_attention = False\n-    _supports_flex_attention = False\n \n     @torch.no_grad()\n     def _init_weights(self, module):"
        },
        {
            "sha": "99fa31c0e7310cf7fd246dae45b1ef6f286a807e",
            "filename": "src/transformers/models/gpt_oss/modular_gpt_oss.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/e8c51d1848187b9e58d00bf7d638811686ab2a4b/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e8c51d1848187b9e58d00bf7d638811686ab2a4b/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_oss%2Fmodular_gpt_oss.py?ref=e8c51d1848187b9e58d00bf7d638811686ab2a4b",
            "patch": "@@ -354,8 +354,7 @@ def forward(\n class GptOssPreTrainedModel(LlamaPreTrainedModel):\n     _keep_in_fp32_modules = [\"post_attention_layernorm\", \"input_layernorm\", \"norm\"]\n     _supports_sdpa = False\n-    _supports_flash_attention = False\n-    _supports_flex_attention = False\n+    _supports_flex_attn = False\n     _can_record_outputs = {\n         \"router_logits\": OutputRecorder(GptOssTopKRouter, index=0),\n         \"hidden_states\": GptOssDecoderLayer,"
        },
        {
            "sha": "55fa9b98b74fa8f496bb19f8650f1e2ca8968a28",
            "filename": "tests/models/gpt_oss/test_modeling_gpt_oss.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/e8c51d1848187b9e58d00bf7d638811686ab2a4b/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e8c51d1848187b9e58d00bf7d638811686ab2a4b/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgpt_oss%2Ftest_modeling_gpt_oss.py?ref=e8c51d1848187b9e58d00bf7d638811686ab2a4b",
            "patch": "@@ -22,6 +22,7 @@\n import unittest\n from pathlib import Path\n \n+import pytest\n from parameterized import parameterized\n \n from transformers import (\n@@ -31,9 +32,12 @@\n )\n from transformers.testing_utils import (\n     cleanup,\n+    require_flash_attn,\n+    require_kernels,\n     require_read_token,\n     require_torch,\n     require_torch_accelerator,\n+    require_torch_gpu,\n     slow,\n     torch_device,\n )\n@@ -62,6 +66,37 @@ class GptOssModelTest(CausalLMModelTest, unittest.TestCase):\n     model_split_percents = [0.5, 0.6]\n     model_tester_class = GptOssModelTester\n \n+    @require_kernels\n+    @require_flash_attn\n+    @pytest.mark.flash_attn_test\n+    @require_torch_gpu\n+    def test_initialization_raises_error_for_flash_attn(self):\n+        \"\"\"\n+        Tests that initializing the model with unsupported Flash Attention implementations raises a ValueError,\n+        but allows the specific vllm kernel.\n+        \"\"\"\n+\n+        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+        kernel_attn = \"kernels-community/vllm-flash-attn3\"\n+\n+        # Checking each via `set_attn_implementation` and manually setting within the config\n+        model = GptOssModel(config).to(device=torch_device, dtype=torch.bfloat16)\n+        model.set_attn_implementation(\"kernels-community/vllm-flash-attn3\")\n+        self.assertTrue(model.config._attn_implementation == kernel_attn)\n+\n+        config._attn_implementation = kernel_attn\n+        self.assertTrue(model.config._attn_implementation == kernel_attn)\n+\n+        with torch.no_grad():\n+            output = model(**inputs_dict)\n+        self.assertIsNotNone(output)\n+\n+        with self.assertRaisesRegex(ValueError, \"GPT-OSS model does not support\"):\n+            model.set_attn_implementation(\"flash_attention_2\")\n+\n+        with self.assertRaisesRegex(ValueError, \"GPT-OSS model does not support\"):\n+            config._attn_implementation = \"flash_attention_2\"\n+\n     @unittest.skip(\"GptOss's forcefully disables sdpa due to Sink\")\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         pass"
        }
    ],
    "stats": {
        "total": 59,
        "additions": 54,
        "deletions": 5
    }
}