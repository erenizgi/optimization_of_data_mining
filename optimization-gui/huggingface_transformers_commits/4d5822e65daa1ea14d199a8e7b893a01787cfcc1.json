{
    "author": "zucchini-nlp",
    "message": "[smolvlm] fix video inference (#39147)\n\n* fix smolvlm\n\n* better do as before, set sampling params in overwritten `apply_chat_template`\n\n* style\n\n* update with `setdefault`",
    "sha": "4d5822e65daa1ea14d199a8e7b893a01787cfcc1",
    "files": [
        {
            "sha": "72f63c37ffdf83ceb5514ce4e63927f1b4ab66d6",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d5822e65daa1ea14d199a8e7b893a01787cfcc1/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d5822e65daa1ea14d199a8e7b893a01787cfcc1/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=4d5822e65daa1ea14d199a8e7b893a01787cfcc1",
            "patch": "@@ -434,6 +434,10 @@ def apply_chat_template(\n         if chat_template is None and has_video:\n             # re-assign to the correct default template for BC, if user is not requesting their own template\n             chat_template = DEFAULT_CHAT_TEMPLATE\n+\n+        kwargs.setdefault(\"num_frames\", self.video_processor.num_frames)\n+        kwargs.setdefault(\"fps\", self.video_processor.fps)\n+\n         return super().apply_chat_template(conversation, chat_template, **kwargs)\n \n "
        },
        {
            "sha": "135043e9860ae83b8f35cf70fa65ea36e912c0fc",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 36,
            "deletions": 12,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d5822e65daa1ea14d199a8e7b893a01787cfcc1/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d5822e65daa1ea14d199a8e7b893a01787cfcc1/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=4d5822e65daa1ea14d199a8e7b893a01787cfcc1",
            "patch": "@@ -536,23 +536,24 @@ def setUp(self):\n                 ).content\n             )\n         )\n-        self.image2 = Image.open(\n-            BytesIO(requests.get(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\").content)\n-        )\n-        self.image3 = Image.open(\n-            BytesIO(\n-                requests.get(\n-                    \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n-                ).content\n-            )\n-        )\n+\n+        self.video_messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"video\",\n+                        \"path\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/blog/assisted-generation/gif_1_1080p.mov\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Describe this video in detail\"},\n+                ],\n+            },\n+        ]\n \n     def tearDown(self):\n         cleanup(torch_device, gc_collect=True)\n \n     @slow\n-    # TODO (Orr?) this is a dummy test to check if the model generates things that make sense.\n-    # Needs to be expanded to a tiny video\n     def test_integration_test(self):\n         model = SmolVLMForConditionalGeneration.from_pretrained(\n             \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\",\n@@ -571,3 +572,26 @@ def test_integration_test(self):\n \n         expected_generated_text = \"\\n\\n\\n\\nIn this image, we see a view of the Statue of Liberty and the\"\n         self.assertEqual(generated_texts[0], expected_generated_text)\n+\n+    @slow\n+    def test_integration_test_video(self):\n+        model = SmolVLMForConditionalGeneration.from_pretrained(\n+            \"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\",\n+            torch_dtype=torch.bfloat16,\n+            device_map=\"auto\",\n+        )\n+\n+        # Create inputs\n+        inputs = self.processor.apply_chat_template(\n+            self.video_messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"pt\",\n+        ).to(device=torch_device, dtype=torch.bfloat16)\n+\n+        generated_ids = model.generate(**inputs, max_new_tokens=20)\n+        generated_texts = self.processor.batch_decode(generated_ids, skip_special_tokens=True)\n+\n+        expected_generated_text = 'User: You are provided the following series of nine frames from a 0:00:09 [H:MM:SS] video.\\n\\nFrame from 00:00:\\nFrame from 00:01:\\nFrame from 00:02:\\nFrame from 00:03:\\nFrame from 00:04:\\nFrame from 00:05:\\nFrame from 00:06:\\nFrame from 00:08:\\nFrame from 00:09:\\n\\nDescribe this video in detail\\nAssistant: The video depicts a large language model architecture, specifically a language model with a \"quick brown\" feature'  # fmt: skip\n+        self.assertEqual(generated_texts[0], expected_generated_text)"
        }
    ],
    "stats": {
        "total": 52,
        "additions": 40,
        "deletions": 12
    }
}