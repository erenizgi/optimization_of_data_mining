{
    "author": "ydshieh",
    "message": "[testing] Fix `qwen2_audio` (#41018)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "c05f9d2f0e406912396c4fa1eef7ac53ff033425",
    "files": [
        {
            "sha": "4d26443f63d67699b286afa40278402eef6fca17",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 31,
            "deletions": 33,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/c05f9d2f0e406912396c4fa1eef7ac53ff033425/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c05f9d2f0e406912396c4fa1eef7ac53ff033425/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=c05f9d2f0e406912396c4fa1eef7ac53ff033425",
            "patch": "@@ -198,6 +198,7 @@ def test_sdpa_can_dispatch_composite_models(self):\n @require_torch\n class Qwen2AudioForConditionalGenerationIntegrationTest(unittest.TestCase):\n     def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n         self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n \n     def tearDown(self):\n@@ -206,7 +207,9 @@ def tearDown(self):\n     @slow\n     def test_small_model_integration_test_single(self):\n         # Let' s make sure we test the preprocessing to replace what is used\n-        model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n+        model = Qwen2AudioForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen2-Audio-7B-Instruct\", device_map=torch_device, dtype=torch.float16\n+        )\n \n         url = \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\"\n         messages = [\n@@ -223,47 +226,35 @@ def test_small_model_integration_test_single(self):\n \n         formatted_prompt = self.processor.apply_chat_template(messages, add_generation_prompt=True)\n \n-        inputs = self.processor(text=formatted_prompt, audios=[raw_audio], return_tensors=\"pt\", padding=True)\n+        inputs = self.processor(text=formatted_prompt, audio=[raw_audio], return_tensors=\"pt\", padding=True).to(\n+            torch_device\n+        )\n \n+        torch.manual_seed(42)\n         output = model.generate(**inputs, max_new_tokens=32)\n \n         # fmt: off\n-        EXPECTED_INPUT_IDS = torch.tensor([[\n-            151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 14755, 220, 16, 25, 220, 151647,\n-            *[151646] * 101,\n-            151648, 198, 3838, 594, 429, 5112, 30, 151645, 198, 151644, 77091, 198,\n-        ]])\n-        # fmt: on\n-        self.assertTrue(torch.equal(inputs[\"input_ids\"], EXPECTED_INPUT_IDS))\n-\n-        EXPECTED_DECODED_TEXT = (\n-            \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nAudio 1: <|audio_bos|>\"\n-            + \"<|AUDIO|>\" * 101\n-            + \"<|audio_eos|>\\nWhat's that sound?<|im_end|>\\n<|im_start|>assistant\\nIt is the sound of glass breaking.<|im_end|>\"\n+        EXPECTED_INPUT_IDS = torch.tensor(\n+            [[151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 14755, 220, 16, 25, 220, 151647, *[151646] * 101 , 151648, 198, 3838, 594, 429, 5112, 30, 151645, 198, 151644, 77091, 198]],\n+            device=torch_device\n         )\n+        # fmt: on\n+        torch.testing.assert_close(inputs[\"input_ids\"], EXPECTED_INPUT_IDS)\n \n+        # fmt: off\n+        EXPECTED_DECODED_TEXT = \"<|im_start|>system\\nYou are a helpful assistant.<|im_end|>\\n<|im_start|>user\\nAudio 1: <|audio_bos|>\" + \"<|AUDIO|>\" * 101 + \"<|audio_eos|>\\nWhat's that sound?<|im_end|>\\n<|im_start|>assistant\\nIt is the sound of glass breaking.<|im_end|>\"\n+        # fmt: on\n         self.assertEqual(\n             self.processor.decode(output[0], skip_special_tokens=False),\n             EXPECTED_DECODED_TEXT,\n         )\n \n-        # test the error when incorrect number of audio tokens\n-        # fmt: off\n-        inputs[\"input_ids\"] = torch.tensor([[\n-            151644, 8948, 198, 2610, 525, 264, 10950, 17847, 13, 151645, 198, 151644, 872, 198, 14755, 220, 16, 25, 220, 151647,\n-            *[151646] * 200,\n-            151648, 198, 3838, 594, 429, 5112, 30, 151645, 198, 151644, 77091, 198,\n-        ]])\n-        # fmt: on\n-        with self.assertRaisesRegex(\n-            ValueError, \"Audio features and audio tokens do not match: tokens: 200, features 101\"\n-        ):\n-            model.generate(**inputs, max_new_tokens=32)\n-\n     @slow\n     def test_small_model_integration_test_batch(self):\n         # Let' s make sure we test the preprocessing to replace what is used\n-        model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n+        model = Qwen2AudioForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen2-Audio-7B-Instruct\", device_map=torch_device, dtype=torch.float16\n+        )\n \n         conversation1 = [\n             {\n@@ -322,23 +313,27 @@ def test_small_model_integration_test_batch(self):\n                                 )[0]\n                             )\n \n-        inputs = self.processor(text=text, audios=audios, return_tensors=\"pt\", padding=True)\n+        inputs = self.processor(text=text, audio=audios, return_tensors=\"pt\", padding=True).to(torch_device)\n \n+        torch.manual_seed(42)\n         output = model.generate(**inputs, max_new_tokens=32)\n \n         EXPECTED_DECODED_TEXT = [\n             \"system\\nYou are a helpful assistant.\\nuser\\nAudio 1: \\nWhat's that sound?\\nassistant\\nIt is the sound of glass shattering.\\nuser\\nAudio 2: \\nWhat can you hear?\\nassistant\\ncough and throat clearing.\",\n             \"system\\nYou are a helpful assistant.\\nuser\\nAudio 1: \\nWhat does the person say?\\nassistant\\nThe original content of this audio is: 'Mister Quiller is the apostle of the middle classes and we are glad to welcome his gospel.'\",\n         ]\n+\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),\n             EXPECTED_DECODED_TEXT,\n         )\n \n     @slow\n-    def test_small_model_integration_test_multiturn(self):\n+    def test_small_model_integration_test_multiurn(self):\n         # Let' s make sure we test the preprocessing to replace what is used\n-        model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n+        model = Qwen2AudioForConditionalGeneration.from_pretrained(\n+            \"Qwen/Qwen2-Audio-7B-Instruct\", device_map=torch_device, dtype=torch.float16\n+        )\n \n         messages = [\n             {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n@@ -379,12 +374,15 @@ def test_small_model_integration_test_multiturn(self):\n                             )[0]\n                         )\n \n-        inputs = self.processor(text=formatted_prompt, audios=audios, return_tensors=\"pt\", padding=True)\n+        inputs = self.processor(text=formatted_prompt, audio=audios, return_tensors=\"pt\", padding=True).to(\n+            torch_device\n+        )\n \n+        torch.manual_seed(42)\n         output = model.generate(**inputs, max_new_tokens=32, top_k=1)\n \n         EXPECTED_DECODED_TEXT = [\n-            \"system\\nYou are a helpful assistant.\\nuser\\nAudio 1: \\nWhat's that sound?\\nassistant\\nIt is the sound of glass shattering.\\nuser\\nAudio 2: \\nHow about this one?\\nassistant\\nThroat clearing.\",\n+            \"system\\nYou are a helpful assistant.\\nuser\\nAudio 1: \\nWhat's that sound?\\nassistant\\nIt is the sound of glass shattering.\\nuser\\nAudio 2: \\nHow about this one?\\nassistant\\nThroat clearing.\"\n         ]\n         self.assertEqual(\n             self.processor.batch_decode(output, skip_special_tokens=True),"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 31,
        "deletions": 33
    }
}