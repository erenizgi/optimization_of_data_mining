{
    "author": "Ayaa17",
    "message": "fix: multilingual midel convert to tflite get wrong token (#32079)\n\n* fix: multilingual midel convert to tflite get wrong token\r\n\r\n* fix: modify test_force_tokens_logits_processor the checking value as scores.dtype.min\r\n\r\n---------\r\n\r\nCo-authored-by: kent.sc.hung <kent.sc.hung@benq.com>\r\nCo-authored-by: Aya <[kent831217@gmail.com]>",
    "sha": "7562366d4be98152059e9f8e923bfb1bad600cb5",
    "files": [
        {
            "sha": "91e20fe02f7f4ff8320e29a9a1a9670c6ed86d12",
            "filename": "src/transformers/generation/tf_logits_process.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7562366d4be98152059e9f8e923bfb1bad600cb5/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7562366d4be98152059e9f8e923bfb1bad600cb5/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Ftf_logits_process.py?ref=7562366d4be98152059e9f8e923bfb1bad600cb5",
            "patch": "@@ -581,7 +581,7 @@ def _force_token(generation_idx):\n             batch_size = scores.shape[0]\n             current_token = self.force_token_array[generation_idx]\n \n-            new_scores = tf.ones_like(scores, dtype=scores.dtype) * -float(\"inf\")\n+            new_scores = tf.zeros_like(scores, dtype=scores.dtype) + tf.constant([scores.dtype.min])\n             indices = tf.stack((tf.range(batch_size), tf.tile([current_token], [batch_size])), axis=1)\n             updates = tf.zeros((batch_size,), dtype=scores.dtype)\n             new_scores = tf.tensor_scatter_nd_update(new_scores, indices, updates)"
        },
        {
            "sha": "f06f5695b1cef88a01ada684761aa06bd0f7919c",
            "filename": "tests/generation/test_tf_logits_process.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7562366d4be98152059e9f8e923bfb1bad600cb5/tests%2Fgeneration%2Ftest_tf_logits_process.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7562366d4be98152059e9f8e923bfb1bad600cb5/tests%2Fgeneration%2Ftest_tf_logits_process.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_tf_logits_process.py?ref=7562366d4be98152059e9f8e923bfb1bad600cb5",
            "patch": "@@ -406,7 +406,12 @@ def test_force_tokens_logits_processor(self, use_xla):\n \n         non_forced_inds = [i for i in range(vocab_size) if i != force_token_map[cur_len]]\n         self.assertTrue(\n-            tf.math.reduce_all(tf.math.is_inf(tf.gather(scores, [non_forced_inds], axis=1))),\n+            tf.math.reduce_all(\n+                tf.experimental.numpy.isclose(\n+                    tf.gather(scores, [non_forced_inds], axis=1),\n+                    tf.constant(scores.dtype.min),\n+                )\n+            )\n         )\n \n         # check that if the cur_len is not contained in the force_token_map, the logits are not modified"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 7,
        "deletions": 2
    }
}