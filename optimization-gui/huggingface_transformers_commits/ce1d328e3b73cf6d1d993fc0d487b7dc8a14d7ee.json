{
    "author": "SunMarc",
    "message": "Fix cache_utils for optimum.quanto kvcache quantization  (#34750)\n\n* add co-author\r\n\r\nCo-authored-by: w3rew <w3rew@users.noreply.github.com>\r\n\r\n* fix docs\r\n\r\n* fix cache\r\n\r\n* remove print\r\n\r\n---------\r\n\r\nCo-authored-by: w3rew <w3rew@users.noreply.github.com>",
    "sha": "ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee",
    "files": [
        {
            "sha": "0fb72d26058e55c18cc365d272774c737da9ccb5",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee",
            "patch": "@@ -55,7 +55,7 @@ Use the table below to help you decide which quantization method to use.\n | GGUF / GGML (llama.cpp)             | 游릭                       | 游릭   | 游릭        | 游댮              | 游릭                     | 游댮         | 游댮                       | 1 - 8          | 游댮                                   | [See GGUF section](../gguf)                | [See GGUF section](../gguf)                      | https://github.com/ggerganov/llama.cpp      |\n | [GPTQ](./gptq)                                | 游댮                       | 游댮   | 游릭        | 游릭              | 游댮                     | 游댮         | 游댮                       | 2 - 3 - 4 - 8          | 游릭                                   | 游릭            | 游릭                      | https://github.com/AutoGPTQ/AutoGPTQ        |\n | [HQQ](./hqq)                                 | 游릭                       | 游릭    | 游릭        | 游댮              | 游댮                     | 游댮         | 游릭                       | 1 - 8          | 游릭                                   | 游댮            | 游릭                      | https://github.com/mobiusml/hqq/            |\n-| [Quanto](./quanto)                              | 游릭                       | 游릭   | 游릭        | 游댮              | 游릭                     | 游댮         | 游릭                       | 2 / 4 / 8      | 游댮                                   | 游댮            | 游릭                      | https://github.com/huggingface/quanto       |\n+| [optimum-quanto](./quanto)                              | 游릭                       | 游릭   | 游릭        | 游댮              | 游릭                     | 游댮         | 游릭                       | 2 / 4 / 8      | 游댮                                   | 游댮            | 游릭                      | https://github.com/huggingface/optimum-quanto       |\n | [FBGEMM_FP8](./fbgemm_fp8.md)                              | 游릭                       | 游댮    | 游릭        | 游댮              | 游댮                      | 游댮         | 游댮                        | 8      | 游댮                                   | 游릭            | 游릭                      | https://github.com/pytorch/FBGEMM       |\n | [torchao](./torchao.md)                              | 游릭                       |     | 游릭        | 游댮              | partial support (int4 weight only)       | 游댮         |                       | 4 / 8      |                                   | 游릭游댮           | 游릭                      | https://github.com/pytorch/ao       |\n "
        },
        {
            "sha": "37df4ed589e8404c2197c48e1855c5bd4dc3b881",
            "filename": "docs/source/en/quantization/quanto.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fquanto.md?ref=ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee",
            "patch": "@@ -14,16 +14,16 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Quanto\n+# Optimum-quanto\n \n <Tip>\n \n-Try Quanto + transformers with this [notebook](https://colab.research.google.com/drive/16CXfVmtdQvciSh9BopZUDYcmXCDpvgrT?usp=sharing)!\n+Try optimum-quanto + transformers with this [notebook](https://colab.research.google.com/drive/16CXfVmtdQvciSh9BopZUDYcmXCDpvgrT?usp=sharing)!\n \n </Tip>\n \n \n-[游뱅 Quanto](https://github.com/huggingface/quanto) library is a versatile pytorch quantization toolkit. The quantization method used is the linear quantization. Quanto provides several unique features such as:\n+[游뱅 optimum-quanto](https://github.com/huggingface/optimum-quanto) library is a versatile pytorch quantization toolkit. The quantization method used is the linear quantization. Quanto provides several unique features such as:\n \n - weights quantization (`float8`,`int8`,`int4`,`int2`)\n - activation quantization (`float8`,`int8`)\n@@ -37,12 +37,12 @@ Try Quanto + transformers with this [notebook](https://colab.research.google.com\n Before you begin, make sure the following libraries are installed:\n \n ```bash\n-pip install quanto accelerate transformers\n+pip install optimum-quanto accelerate transformers\n ```\n \n Now you can quantize a model by passing [`QuantoConfig`] object in the [`~PreTrainedModel.from_pretrained`] method. This works for any model in any modality, as long as it contains `torch.nn.Linear` layers. \n \n-The integration with transformers only supports weights quantization. For the more complex use case such as activation quantization, calibration and quantization aware training, you should use [quanto](https://github.com/huggingface/quanto) library instead. \n+The integration with transformers only supports weights quantization. For the more complex use case such as activation quantization, calibration and quantization aware training, you should use [optimum-quanto](https://github.com/huggingface/optimum-quanto) library instead. \n \n ```py\n from transformers import AutoModelForCausalLM, AutoTokenizer, QuantoConfig\n@@ -55,7 +55,7 @@ quantized_model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"cud\n \n Note that serialization is not supported yet with transformers but it is coming soon! If you want to save the model, you can use quanto library instead.\n \n-Quanto library uses linear quantization algorithm for quantization. Even though this is a basic quantization technique, we get very good results! Have a look at the following benchmark (llama-2-7b on perplexity metric). You can find more benchmarks [here](https://github.com/huggingface/quanto/tree/main/bench/generation)\n+Optimum-quanto library uses linear quantization algorithm for quantization. Even though this is a basic quantization technique, we get very good results! Have a look at the following benchmark (llama-2-7b on perplexity metric). You can find more benchmarks [here](https://github.com/huggingface/optimum-quanto/tree/main/bench/generation)\n \n <div class=\"flex gap-4\">\n   <div>"
        },
        {
            "sha": "3b491b04607b5798efcc6aeec496fc342b67f4d3",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=ce1d328e3b73cf6d1d993fc0d487b7dc8a14d7ee",
            "patch": "@@ -784,6 +784,11 @@ def __init__(self, cache_config: CacheConfig) -> None:\n         super().__init__(cache_config)\n \n         if is_optimum_quanto_available():\n+            optimum_quanto_version = version.parse(importlib.metadata.version(\"optimum-quanto\"))\n+            if optimum_quanto_version <= version.parse(\"0.2.5\"):\n+                raise ImportError(\n+                    f\"You need optimum-quanto package version to be greater or equal than 0.2.5 to use `QuantoQuantizedCache`. Detected version {optimum_quanto_version}.\"\n+                )\n             from optimum.quanto import MaxOptimizer, qint2, qint4\n         elif is_quanto_available():\n             logger.warning_once(\n@@ -816,7 +821,8 @@ def _quantize(self, tensor, axis):\n         if is_optimum_quanto_available():\n             from optimum.quanto import quantize_weight\n \n-            qtensor = quantize_weight(tensor, self.qtype, axis, self.q_group_size)\n+            scale, zeropoint = self.optimizer(tensor, self.qtype, axis, self.q_group_size)\n+            qtensor = quantize_weight(tensor, self.qtype, axis, scale, zeropoint, self.q_group_size)\n             return qtensor\n         elif is_quanto_available():\n             logger.warning_once("
        }
    ],
    "stats": {
        "total": 22,
        "additions": 14,
        "deletions": 8
    }
}