{
    "author": "Taise228",
    "message": "Stop collecting all model parameters to save models when using DeepSpeed and LoRA (#41416)\n\nno need to gather all parameters to save model when using deepspeed and LoRA\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Ferdinand Mom <47445085+3outeille@users.noreply.github.com>",
    "sha": "89998bddcab4e3d5ee03ed9fa1bf1a9339584709",
    "files": [
        {
            "sha": "5b62e48606f76c8148f99090ad2536d2cfebb3a5",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/89998bddcab4e3d5ee03ed9fa1bf1a9339584709/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/89998bddcab4e3d5ee03ed9fa1bf1a9339584709/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=89998bddcab4e3d5ee03ed9fa1bf1a9339584709",
            "patch": "@@ -4021,7 +4021,16 @@ def save_model(self, output_dir: str | None = None, _internal_call: bool = False\n                     self._save(output_dir, state_dict=state_dict)\n         elif self.is_deepspeed_enabled:\n             try:\n-                state_dict = self.accelerator.get_state_dict(self.deepspeed)\n+                accept_exclude_frozen_parameters = \"exclude_frozen_parameters\" in set(\n+                    inspect.signature(self.model_wrapped.save_checkpoint).parameters.keys()\n+                )\n+                zero3_sharding = self.deepspeed.config.get(\"zero_optimization\", {}).get(\"stage\", None) == 3\n+                if accept_exclude_frozen_parameters and _is_peft_model(self.model) and zero3_sharding:\n+                    # When using PEFT with DeepSpeed ZeRO Stage 3,\n+                    # we do not need to load the frozen parameters\n+                    state_dict = self.deepspeed._zero3_consolidated_16bit_state_dict(exclude_frozen_parameters=True)\n+                else:\n+                    state_dict = self.accelerator.get_state_dict(self.deepspeed)\n                 if self.args.should_save:\n                     self._save(output_dir, state_dict=state_dict)\n             except ValueError:"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 10,
        "deletions": 1
    }
}