{
    "author": "louisbrulenaudet",
    "message": "Update llm_engine.py (#33332)\n\n* Update llm_engine.py\r\n- Added support for optional token and max_tokens parameters in the constructor.\r\n- Provided usage examples and detailed documentation for each method.",
    "sha": "134ba90da9693d0f595849364393f8c635a4f386",
    "files": [
        {
            "sha": "456c6172a77cb044538d985e9c8c1aeb76330bb9",
            "filename": "src/transformers/agents/llm_engine.py",
            "status": "modified",
            "additions": 66,
            "deletions": 7,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/134ba90da9693d0f595849364393f8c635a4f386/src%2Ftransformers%2Fagents%2Fllm_engine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/134ba90da9693d0f595849364393f8c635a4f386/src%2Ftransformers%2Fagents%2Fllm_engine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fagents%2Fllm_engine.py?ref=134ba90da9693d0f595849364393f8c635a4f386",
            "patch": "@@ -68,25 +68,84 @@ def get_clean_message_list(message_list: List[Dict[str, str]], role_conversions:\n \n \n class HfApiEngine:\n-    \"\"\"This engine leverages Hugging Face's Inference API service, either serverless or with a dedicated endpoint.\"\"\"\n+    \"\"\"A class to interact with Hugging Face's Inference API for language model interaction.\n+\n+    This engine allows you to communicate with Hugging Face's models using the Inference API. It can be used in both serverless mode or with a dedicated endpoint, supporting features like stop sequences and grammar customization.\n+\n+    Parameters:\n+        model (`str`, *optional*, defaults to `\"meta-llama/Meta-Llama-3.1-8B-Instruct\"`):\n+            The Hugging Face model ID to be used for inference. This can be a path or model identifier from the Hugging Face model hub.\n+        token (`str`, *optional*):\n+            The Hugging Face API token for authentication. If not provided, the class will use the token stored in the Hugging Face CLI configuration.\n+        max_tokens (`int`, *optional*, defaults to 1500):\n+            The maximum number of tokens allowed in the output.\n+        timeout (`int`, *optional*, defaults to 120):\n+            Timeout for the API request, in seconds.\n+\n+    Raises:\n+        ValueError:\n+            If the model name is not provided.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        model: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n+        token: Optional[str] = None,\n+        max_tokens: Optional[int] = 1500,\n+        timeout: Optional[int] = 120,\n+    ):\n+        \"\"\"Initialize the HfApiEngine.\"\"\"\n+        if not model:\n+            raise ValueError(\"Model name must be provided.\")\n \n-    def __init__(self, model: str = \"meta-llama/Meta-Llama-3.1-8B-Instruct\"):\n         self.model = model\n-        self.client = InferenceClient(self.model, timeout=120)\n+        self.client = InferenceClient(self.model, token=token, timeout=timeout)\n+        self.max_tokens = max_tokens\n \n     def __call__(\n-        self, messages: List[Dict[str, str]], stop_sequences: List[str] = [], grammar: Optional[str] = None\n+        self,\n+        messages: List[Dict[str, str]],\n+        stop_sequences: Optional[List[str]] = [],\n+        grammar: Optional[str] = None,\n     ) -> str:\n+        \"\"\"Process the input messages and return the model's response.\n+\n+        This method sends a list of messages to the Hugging Face Inference API, optionally with stop sequences and grammar customization.\n+\n+        Parameters:\n+            messages (`List[Dict[str, str]]`):\n+                A list of message dictionaries to be processed. Each dictionary should have the structure `{\"role\": \"user/system\", \"content\": \"message content\"}`.\n+            stop_sequences (`List[str]`, *optional*):\n+                A list of strings that will stop the generation if encountered in the model's output.\n+            grammar (`str`, *optional*):\n+                The grammar or formatting structure to use in the model's response.\n+\n+        Returns:\n+            `str`: The text content of the model's response.\n+\n+        Example:\n+            ```python\n+            >>> engine = HfApiEngine(\n+            ...     model=\"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n+            ...     token=\"your_hf_token_here\",\n+            ...     max_tokens=2000\n+            ... )\n+            >>> messages = [{\"role\": \"user\", \"content\": \"Explain quantum mechanics in simple terms.\"}]\n+            >>> response = engine(messages, stop_sequences=[\"END\"])\n+            >>> print(response)\n+            \"Quantum mechanics is the branch of physics that studies...\"\n+            ```\n+        \"\"\"\n         # Get clean message list\n         messages = get_clean_message_list(messages, role_conversions=llama_role_conversions)\n \n-        # Get LLM output\n+        # Send messages to the Hugging Face Inference API\n         if grammar is not None:\n             response = self.client.chat_completion(\n-                messages, stop=stop_sequences, max_tokens=1500, response_format=grammar\n+                messages, stop=stop_sequences, max_tokens=self.max_tokens, response_format=grammar\n             )\n         else:\n-            response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=1500)\n+            response = self.client.chat_completion(messages, stop=stop_sequences, max_tokens=self.max_tokens)\n \n         response = response.choices[0].message.content\n "
        }
    ],
    "stats": {
        "total": 73,
        "additions": 66,
        "deletions": 7
    }
}