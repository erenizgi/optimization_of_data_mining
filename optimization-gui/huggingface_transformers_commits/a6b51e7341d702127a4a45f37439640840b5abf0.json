{
    "author": "gante",
    "message": "[Whisper + beam search] fix usage of `beam_indices` (#38259)\n\n* tmp\n\n* fix test_tiny_token_timestamp_batch_generation\n\n* better comments\n\n* test\n\n* comments\n\n* Apply suggestions from code review\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "a6b51e7341d702127a4a45f37439640840b5abf0",
    "files": [
        {
            "sha": "db362355b8751bd2492a7b2be8d7fa829eebedfb",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 28,
            "deletions": 18,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/a6b51e7341d702127a4a45f37439640840b5abf0/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a6b51e7341d702127a4a45f37439640840b5abf0/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=a6b51e7341d702127a4a45f37439640840b5abf0",
            "patch": "@@ -231,42 +231,52 @@ def _extract_token_timestamps(\n             tensor containing the timestamps in seconds for each predicted token\n         \"\"\"\n         # Create a list with `decoder_layers` elements, each a tensor of shape\n-        # (batch size, attention_heads, output length, input length).\n+        # (batch size * num beams, attention_heads, output length, input length).\n         cross_attentions = []\n         for i in range(self.config.decoder_layers):\n             cross_attentions.append(torch.cat([x[i] for x in generate_outputs.cross_attentions], dim=2))\n \n         # Select specific cross-attention layers and heads. This is a tensor\n-        # of shape (batch size, num selected, output length, input length).\n+        # of shape (batch size * num beams, num selected heads, output length, input length).\n         weights = torch.stack([cross_attentions[l][:, h] for l, h in alignment_heads])\n         weights = weights.permute([1, 0, 2, 3])\n \n         weight_length = None\n \n         if \"beam_indices\" in generate_outputs:\n-            # If beam search has been used, the output sequences may have been generated for more timesteps than their sequence_lengths\n-            # since the beam search strategy chooses the most probable sequences at the end of the search.\n-            # In that case, the cross_attentions weights are too long and we have to make sure that they have the right output_length\n-            weight_length = (generate_outputs.beam_indices != -1).sum(-1).max()\n-            weight_length = weight_length if num_input_ids is None else weight_length + num_input_ids\n-\n-            # beam search takes `decoder_input_ids` into account in the `beam_indices` length\n-            # but forgot to shift the beam_indices by the number of `decoder_input_ids`\n-            beam_indices = torch.zeros_like(generate_outputs.beam_indices[:, :weight_length])\n-            # we actually shift the beam indices here\n-            beam_indices[:, num_input_ids:] = generate_outputs.beam_indices[:, : weight_length - num_input_ids]\n+            # If beam search was used, the sequence length of the outputs may not be the real sequence length:\n+            # beam search may end up returning a sequence that finished a few steps earlier while decoding.\n+            # In that case, the `cross_attentions` weights are too long and we have to make sure that they have\n+            # the right `output_length`\n \n-            weights = weights[:, :, :weight_length]\n+            # get the real sequence length of the longest sequence, crop the beam_indices to the real length\n+            weight_length = (generate_outputs.beam_indices != -1).sum(-1).max()\n+            beam_indices = generate_outputs.beam_indices[:, :weight_length]\n+\n+            # The first forward pass (prefill) may have processed more than one token and, therefore, contain\n+            # cross-attention weights for several tokens.\n+            # Let's unroll the first `beam_indices` accordingly, so we can use it to gather the weights.\n+            if num_input_ids is not None and num_input_ids > 1:\n+                # `-1`: `beam_indices` can be used as-is to gather the weights when `num_input_ids` is 1\n+                weight_length += num_input_ids - 1\n+                beam_indices_first_step_unrolled = (\n+                    torch.ones(beam_indices.shape[0], num_input_ids - 1, device=beam_indices.device, dtype=torch.long)\n+                    * (beam_indices[:, 0:1])\n+                )\n+                unrolled_beam_indices = torch.cat([beam_indices_first_step_unrolled, beam_indices], dim=-1)\n+            else:\n+                unrolled_beam_indices = beam_indices\n \n             # If beam index is still -1, it means that the associated token id is EOS\n             # We need to replace the index with 0 since index_select gives an error if any of the indexes is -1.\n-            beam_indices = beam_indices.masked_fill(beam_indices == -1, 0)\n+            unrolled_beam_indices = unrolled_beam_indices.masked_fill(unrolled_beam_indices == -1, 0)\n \n-            # Select the cross attention from the right beam for each output sequences\n+            # Select the cross attention from the right beam for each output sequence, up to the real sequence\n+            # length (`weight_length`)\n             weights = torch.stack(\n                 [\n-                    torch.index_select(weights[:, :, i, :], dim=0, index=beam_indices[:, i])\n-                    for i in range(beam_indices.shape[1])\n+                    torch.index_select(weights[:, :, i, :], dim=0, index=unrolled_beam_indices[:, i])\n+                    for i in range(unrolled_beam_indices.shape[1])\n                 ],\n                 dim=2,\n             )"
        },
        {
            "sha": "37e459db983ef22174750e4811560ce36d477dd7",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a6b51e7341d702127a4a45f37439640840b5abf0/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a6b51e7341d702127a4a45f37439640840b5abf0/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=a6b51e7341d702127a4a45f37439640840b5abf0",
            "patch": "@@ -2155,7 +2155,6 @@ def test_tiny_token_timestamp_batch_generation(self):\n \n         # task id and lang id prompts should not have timestamp tokens\n         self.assertEqual(generate_outputs[\"sequences\"].shape[-1] - 2, generate_outputs[\"token_timestamps\"].shape[-1])\n-\n         self.assertEqual(len(generate_outputs[\"sequences\"]), num_return_sequences * num_samples)\n \n     @slow"
        }
    ],
    "stats": {
        "total": 47,
        "additions": 28,
        "deletions": 19
    }
}