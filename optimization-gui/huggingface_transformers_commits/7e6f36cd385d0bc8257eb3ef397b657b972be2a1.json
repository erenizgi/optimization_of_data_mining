{
    "author": "ydshieh",
    "message": "Skip all `AriaForConditionalGenerationIntegrationTest` on `T4` (#37746)\n\n* skip\n\n* ruff\n\n* trigger CI\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "7e6f36cd385d0bc8257eb3ef397b657b972be2a1",
    "files": [
        {
            "sha": "2f802d9b70e1715ee428b75a5f5a82be7c5bb516",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7e6f36cd385d0bc8257eb3ef397b657b972be2a1/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7e6f36cd385d0bc8257eb3ef397b657b972be2a1/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=7e6f36cd385d0bc8257eb3ef397b657b972be2a1",
            "patch": "@@ -31,6 +31,7 @@\n from transformers.testing_utils import (\n     require_bitsandbytes,\n     require_torch,\n+    require_torch_large_accelerator,\n     require_vision,\n     slow,\n     torch_device,\n@@ -291,6 +292,7 @@ def tearDown(self):\n         torch.cuda.empty_cache()\n \n     @slow\n+    @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_small_model_integration_test(self):\n         # Let's make sure we test the preprocessing to replace what is used\n@@ -313,6 +315,7 @@ def test_small_model_integration_test(self):\n         )\n \n     @slow\n+    @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_single(self):\n         # Let's make sure we test the preprocessing to replace what is used\n@@ -335,6 +338,7 @@ def test_small_model_integration_test_llama_single(self):\n         )\n \n     @slow\n+    @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched(self):\n         # Let's make sure we test the preprocessing to replace what is used\n@@ -362,6 +366,7 @@ def test_small_model_integration_test_llama_batched(self):\n         )\n \n     @slow\n+    @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_small_model_integration_test_batch(self):\n         # Let's make sure we test the preprocessing to replace what is used\n@@ -388,6 +393,7 @@ def test_small_model_integration_test_batch(self):\n         )\n \n     @slow\n+    @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_small_model_integration_test_llama_batched_regression(self):\n         # Let's make sure we test the preprocessing to replace what is used\n@@ -416,7 +422,7 @@ def test_small_model_integration_test_llama_batched_regression(self):\n         )\n \n     @slow\n-    @require_torch\n+    @require_torch_large_accelerator\n     @require_vision\n     @require_bitsandbytes\n     def test_batched_generation(self):\n@@ -501,6 +507,7 @@ def test_tokenizer_integration(self):\n         self.assertEqual(fast_tokenizer.tokenize(prompt), EXPECTED_OUTPUT)\n \n     @slow\n+    @require_torch_large_accelerator\n     @require_bitsandbytes\n     def test_generation_no_images(self):\n         model_id = \"rhymes-ai/Aria\""
        }
    ],
    "stats": {
        "total": 9,
        "additions": 8,
        "deletions": 1
    }
}