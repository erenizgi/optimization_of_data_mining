{
    "author": "Soum-Soum",
    "message": "Refactor ColPali model documentation (#37309)\n\n* Refactor ColPali model documentation\n\n* Apply suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Include quantisation exemple + real images\n\n* simpler image loading\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "3165eb7c2808832d0de86c8f508d9da6b2124044",
    "files": [
        {
            "sha": "bb6efc42056c961c179879f2daee81efe327eb2e",
            "filename": "docs/source/en/model_doc/colpali.md",
            "status": "modified",
            "additions": 89,
            "deletions": 32,
            "changes": 121,
            "blob_url": "https://github.com/huggingface/transformers/blob/3165eb7c2808832d0de86c8f508d9da6b2124044/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3165eb7c2808832d0de86c8f508d9da6b2124044/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md?ref=3165eb7c2808832d0de86c8f508d9da6b2124044",
            "patch": "@@ -1,5 +1,4 @@\n <!--Copyright 2024 The HuggingFace Team. All rights reserved.\n-\n Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n the License. You may obtain a copy of the License at\n \n@@ -9,77 +8,135 @@ Unless required by applicable law or agreed to in writing, software distributed\n an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n specific language governing permissions and limitations under the License.\n \n-‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+‚ö†Ô∏è Note that this file is in Markdown but contains specific syntax for our doc-builder (similar to MDX) that may not be\n rendered properly in your Markdown viewer.\n-\n -->\n \n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n # ColPali\n \n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-</div>\n+[ColPali](https://huggingface.co/papers/2407.01449) is a model designed to retrieve documents by analyzing their visual features. Unlike traditional systems that rely heavily on text extraction and OCR, ColPali treats each page as an image. It uses [Paligemma-3B](./paligemma) to capture not only text, but also the layout, tables, charts, and other visual elements to create detailed embeddings. This offers a more comprehensive understanding of documents and enables more efficient and accurate retrieval.\n \n-## Overview\n+You can find all the original ColPali checkpoints under the [ColPali](https://huggingface.co/collections/vidore/hf-native-colvision-models-6755d68fc60a8553acaa96f7) collection.\n \n-The *ColPali* model was proposed in [ColPali: Efficient Document Retrieval with Vision Language Models](https://doi.org/10.48550/arXiv.2407.01449) by **Manuel Faysse***, **Hugues Sibille***, **Tony Wu***, Bilel Omrani, Gautier Viaud, C√©line Hudelot, Pierre Colombo (* denotes equal contribution). Work lead by ILLUIN Technology.\n+> [!TIP]\n+> Click on the ColPali models in the right sidebar for more examples of how to use ColPali for image retrieval.\n \n-In our proposed *ColPali* approach, we leverage VLMs to construct efficient multi-vector embeddings directly from document images (‚Äúscreenshots‚Äù) for document retrieval. We train the model to maximize the similarity between these document embeddings and the corresponding query embeddings, using the late interaction method introduced in ColBERT.\n+<hfoptions id=\"usage\">\n+<hfoption id=\"image retrieval\">\n \n-Using *ColPali* removes the need for potentially complex and brittle layout recognition and OCR pipelines with a single model that can take into account both the textual and visual content (layout, charts, etc.) of a document.\n+```py\n+import requests\n+import torch\n+from PIL import Image\n+from transformers import ColPaliForRetrieval, ColPaliProcessor\n \n-## Resources\n+# Load model (bfloat16 support is limited; fallback to float32 if needed)\n+model = ColPaliForRetrieval.from_pretrained(\n+    \"vidore/colpali-v1.2-hf\",\n+    torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32,\n+    device_map=\"auto\",  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon\n+).eval()\n+\n+processor = ColPaliProcessor.from_pretrained(model_name)\n \n-- The *ColPali* arXiv paper can be found [here](https://doi.org/10.48550/arXiv.2407.01449). üìÑ\n-- The official blog post detailing ColPali can be found [here](https://huggingface.co/blog/manu/colpali). üìù\n-- The original model implementation code for the ColPali model and for the `colpali-engine` package can be found [here](https://github.com/illuin-tech/colpali). üåé\n-- Cookbooks for learning to use the transformers-native version of *ColPali*, fine-tuning, and similarity maps generation can be found [here](https://github.com/tonywu71/colpali-cookbooks). üìö\n+url1 = \"https://upload.wikimedia.org/wikipedia/commons/8/89/US-original-Declaration-1776.jpg\"\n+url2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Romeoandjuliet1597.jpg/500px-Romeoandjuliet1597.jpg\"\n \n-This model was contributed by [@tonywu71](https://huggingface.co/tonywu71) and [@yonigozlan](https://huggingface.co/yonigozlan).\n+images = [\n+    Image.open(requests.get(url1, stream=True).raw),\n+    Image.open(requests.get(url2, stream=True).raw),\n+]\n \n-## Usage\n+queries = [\n+    \"Who printed the edition of Romeo and Juliet?\",\n+    \"When was the United States Declaration of Independence proclaimed?\",\n+]\n \n-This example demonstrates how to use *ColPali* to embed both queries and images, calculate their similarity scores, and identify the most relevant matches. For a specific query, you can retrieve the top-k most similar images by selecting the ones with the highest similarity scores.\n+# Process the inputs\n+inputs_images = processor(images=images, return_tensors=\"pt\").to(model.device)\n+inputs_text = processor(text=queries, return_tensors=\"pt\").to(model.device)\n \n-```python\n+# Forward pass\n+with torch.no_grad():\n+    image_embeddings = model(**inputs_images).embeddings\n+    query_embeddings = model(**inputs_text).embeddings\n+\n+scores = processor.score_retrieval(query_embeddings, image_embeddings)\n+\n+print(\"Retrieval scores (query x image):\")\n+print(scores)\n+```\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [bitsandbytes](../quantization/bitsandbytes.md) to quantize the weights to int4.\n+\n+```py\n+import requests\n import torch\n from PIL import Image\n-\n from transformers import ColPaliForRetrieval, ColPaliProcessor\n+from transformers import BitsAndBytesConfig\n+\n+# 4-bit quantization configuration\n+bnb_config = BitsAndBytesConfig(\n+    load_in_4bit=True,\n+    bnb_4bit_use_double_quant=True,\n+    bnb_4bit_quant_type=\"nf4\",\n+    bnb_4bit_compute_dtype=torch.float16,\n+)\n \n model_name = \"vidore/colpali-v1.2-hf\"\n \n+# Load model \n model = ColPaliForRetrieval.from_pretrained(\n     model_name,\n-    torch_dtype=torch.bfloat16,\n-    device_map=\"cuda:0\",  # or \"mps\" if on Apple Silicon\n+    quantization_config=bnb_config,\n+    device_map=\"cuda\"\n ).eval()\n \n processor = ColPaliProcessor.from_pretrained(model_name)\n \n-# Your inputs (replace dummy images with screenshots of your documents)\n+url1 = \"https://upload.wikimedia.org/wikipedia/commons/8/89/US-original-Declaration-1776.jpg\"\n+url2 = \"https://upload.wikimedia.org/wikipedia/commons/thumb/4/4c/Romeoandjuliet1597.jpg/500px-Romeoandjuliet1597.jpg\"\n+\n images = [\n-    Image.new(\"RGB\", (32, 32), color=\"white\"),\n-    Image.new(\"RGB\", (16, 16), color=\"black\"),\n+    Image.open(requests.get(url1, stream=True).raw),\n+    Image.open(requests.get(url2, stream=True).raw),\n ]\n+\n queries = [\n-    \"What is the organizational structure for our R&D department?\",\n-    \"Can you provide a breakdown of last year‚Äôs financial performance?\",\n+    \"Who printed the edition of Romeo and Juliet?\",\n+    \"When was the United States Declaration of Independence proclaimed?\",\n ]\n \n # Process the inputs\n-batch_images = processor(images=images).to(model.device)\n-batch_queries = processor(text=queries).to(model.device)\n+inputs_images = processor(images=images, return_tensors=\"pt\").to(model.device)\n+inputs_text = processor(text=queries, return_tensors=\"pt\").to(model.device)\n \n # Forward pass\n with torch.no_grad():\n-    image_embeddings = model(**batch_images).embeddings\n-    query_embeddings = model(**batch_queries).embeddings\n+    image_embeddings = model(**inputs_images).embeddings\n+    query_embeddings = model(**inputs_text).embeddings\n \n-# Score the queries against the images\n scores = processor.score_retrieval(query_embeddings, image_embeddings)\n+\n+print(\"Retrieval scores (query x image):\")\n+print(scores)\n ```\n \n+## Notes\n+\n+- [`~ColPaliProcessor.score_retrieval`] returns a 2D tensor where the first dimension is the number of queries and the second dimension is the number of images. A higher score indicates more similarity between the query and image.\n+\n ## ColPaliConfig\n \n [[autodoc]] ColPaliConfig"
        }
    ],
    "stats": {
        "total": 121,
        "additions": 89,
        "deletions": 32
    }
}