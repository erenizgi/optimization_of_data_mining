{
    "author": "huyiwen",
    "message": "[doc] deepspeed universal checkpoint (#35015)\n\n* universal checkpoint\n\n* Update docs/source/en/deepspeed.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/deepspeed.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/deepspeed.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "c9c682d19c9e92a261c3879322016e61095e37cb",
    "files": [
        {
            "sha": "ad3d4240f856194400152ee2fb384e7807be2997",
            "filename": "docs/source/en/deepspeed.md",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c9c682d19c9e92a261c3879322016e61095e37cb/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c9c682d19c9e92a261c3879322016e61095e37cb/docs%2Fsource%2Fen%2Fdeepspeed.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fdeepspeed.md?ref=c9c682d19c9e92a261c3879322016e61095e37cb",
            "patch": "@@ -586,6 +586,20 @@ You can choose the communication data type by setting the `communication_data_ty\n }\n ```\n \n+### Universal Checkpointing\n+\n+[Universal Checkpointing](https://www.deepspeed.ai/tutorials/universal-checkpointing) is an efficient and flexible feature for saving and loading model checkpoints. It enables seamless model training continuation and fine-tuning across different model architectures, parallelism techniques, and training configurations.\n+\n+Resume training with a universal checkpoint by setting [load_universal](https://www.deepspeed.ai/docs/config-json/#checkpoint-options) to `true` in the config file.\n+\n+```yaml\n+{\n+    \"checkpoint\": {\n+        \"load_universal\": true\n+    }\n+}\n+```\n+\n ## Deployment\n \n DeepSpeed can be deployed by different launchers such as [torchrun](https://pytorch.org/docs/stable/elastic/run.html), the `deepspeed` launcher, or [Accelerate](https://huggingface.co/docs/accelerate/basic_tutorials/launch#using-accelerate-launch). To deploy, add `--deepspeed ds_config.json` to the [`Trainer`] command line. It’s recommended to use DeepSpeed’s [`add_config_arguments`](https://deepspeed.readthedocs.io/en/latest/initialize.html#argument-parsing) utility to add any necessary command line arguments to your code."
        }
    ],
    "stats": {
        "total": 14,
        "additions": 14,
        "deletions": 0
    }
}