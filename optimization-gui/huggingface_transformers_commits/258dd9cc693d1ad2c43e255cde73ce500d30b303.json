{
    "author": "MekkCyber",
    "message": "Add Space to Bitsandbytes doc (#36834)\n\n* add space\n\n* address review",
    "sha": "258dd9cc693d1ad2c43e255cde73ce500d30b303",
    "files": [
        {
            "sha": "e9b581b89f2b54a6f3e18f7147e61265b2cc2909",
            "filename": "docs/source/en/quantization/bitsandbytes.md",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/258dd9cc693d1ad2c43e255cde73ce500d30b303/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/258dd9cc693d1ad2c43e255cde73ce500d30b303/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fbitsandbytes.md?ref=258dd9cc693d1ad2c43e255cde73ce500d30b303",
            "patch": "@@ -20,7 +20,10 @@ rendered properly in your Markdown viewer.\n \n [LLM.int8()](https://hf.co/papers/2208.07339) is a quantization method that aims to make large language model inference more accessible without significant degradation. Unlike naive 8-bit quantization, which can result in loss of critical information and accuracy, LLM.int8() dynamically adapts to ensure sensitive components of the computation retain higher precision when needed.\n \n-QLoRA, or 4-bit quantization, compresses a model even further to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allowing training.\n+QLoRA, or 4-bit quantization, compresses a model even further to 4-bits and inserts a small set of trainable low-rank adaptation (LoRA) weights to allowing training. \n+\n+> **Note:** For a user-friendly quantization experience, you can use the `bitsandbytes` [community space](https://huggingface.co/spaces/bnb-community/bnb-my-repo).\n+\n \n Run the command below to install bitsandbytes.\n "
        },
        {
            "sha": "d69675e909aa0e754d523c656e78132a71ae701c",
            "filename": "docs/source/en/quantization/overview.md",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/258dd9cc693d1ad2c43e255cde73ce500d30b303/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/258dd9cc693d1ad2c43e255cde73ce500d30b303/docs%2Fsource%2Fen%2Fquantization%2Foverview.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Foverview.md?ref=258dd9cc693d1ad2c43e255cde73ce500d30b303",
            "patch": "@@ -46,4 +46,13 @@ Use the Space below to help you pick a quantization method depending on your har\n If you are new to quantization, we recommend checking out these beginner-friendly quantization courses in collaboration with DeepLearning.AI.\n \n * [Quantization Fundamentals with Hugging Face](https://www.deeplearning.ai/short-courses/quantization-fundamentals-with-hugging-face/)\n-* [Quantization in Depth](https://www.deeplearning.ai/short-courses/quantization-in-depth\n\\ No newline at end of file\n+* [Quantization in Depth](https://www.deeplearning.ai/short-courses/quantization-in-depth)\n+\n+## User-Friendly Quantization Tools\n+\n+If you are looking for a user-friendly quantization experience, you can use the following community spaces and notebooks: \n+\n+* [Bitsandbytes Space](https://huggingface.co/spaces/bnb-community/bnb-my-repo)\n+* [GGUF Space](https://huggingface.co/spaces/ggml-org/gguf-my-repo)\n+* [MLX Space](https://huggingface.co/spaces/mlx-community/mlx-my-repo)\n+* [AuoQuant Notebook](https://colab.research.google.com/drive/1b6nqC7UZVt8bx4MksX7s656GXPM-eWw4?usp=sharing#scrollTo=ZC9Nsr9u5WhN)\n\\ No newline at end of file"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 14,
        "deletions": 2
    }
}