{
    "author": "zucchini-nlp",
    "message": "[processor] move commonalities to mixin (#40339)\n\n* move commonalities to mixin\n\n* revert - unrelated\n\n* fix copies\n\n* fix style\n\n* comments",
    "sha": "19ffe0219dae122203f9726669f88ef1c6ea3bb4",
    "files": [
        {
            "sha": "3b73e391d6b50be9dc3748d73b8a32e867ebe537",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -135,25 +135,5 @@ def __call__(\n         else:\n             return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"AlignProcessor\"]"
        },
        {
            "sha": "ef58cad3d11e3732346bb5ce39effbb6a78deab1",
            "filename": "src/transformers/models/altclip/processing_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -124,25 +124,5 @@ def __call__(\n         else:\n             return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to XLMRobertaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`].\n-        Please refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to XLMRobertaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"AltCLIPProcessor\"]"
        },
        {
            "sha": "d4c36734846b401df2dd47f2d490a7304e6777dc",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -1054,26 +1054,12 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n \n-        # Remove `num_crops`, it is popped and used only when processing. Make a copy of list when remocing\n+        # Remove `num_crops`, it is popped and used only when processing. Make a copy of list when removing\n         # otherwise `self.image_processor.model_input_names` is also modified\n         image_processor_input_names = [name for name in image_processor_input_names if name != \"num_crops\"]\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))"
        },
        {
            "sha": "9264776e80fdab173276ce38ccd8f89965b161f2",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -175,26 +175,12 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n \n-        # Remove `num_crops`, it is popped and used only when processing. Make a copy of list when remocing\n+        # Remove `num_crops`, it is popped and used only when processing. Make a copy of list when removing\n         # otherwise `self.image_processor.model_input_names` is also modified\n         image_processor_input_names = [name for name in image_processor_input_names if name != \"num_crops\"]\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))"
        },
        {
            "sha": "1878d2c0b5d60ef600b15cb24328c9f1861ef1f2",
            "filename": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -252,25 +252,5 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(tokenizer_input_names) + list(image_processor_input_names)\n-\n \n __all__ = [\"AyaVisionProcessor\"]"
        },
        {
            "sha": "86be17edefc0c6ffd3b27f653c9aed26a7725b66",
            "filename": "src/transformers/models/blip/processing_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -114,25 +114,12 @@ def __call__(\n \n         return text_encoding\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        tokenizer_input_names = [name for name in tokenizer_input_names if name != \"token_type_ids\"]\n+        return tokenizer_input_names + image_processor_input_names\n \n \n __all__ = [\"BlipProcessor\"]"
        },
        {
            "sha": "880d325a65220dcd6759ef701765ae852cc6850f",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -150,28 +150,5 @@ def __call__(\n         encoding = BatchFeature(encoding, tensor_type=return_tensors)\n         return encoding\n \n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.decode with BertTokenizerFast->PreTrainedTokenizer\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.model_input_names\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"Blip2Processor\"]"
        },
        {
            "sha": "c3151f9c1386ae0d1fd2bdf94b0c12c2da2b310c",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -168,7 +168,7 @@ class BridgeTowerImageProcessor(BaseImageProcessor):\n             the `do_pad` parameter in the `preprocess` method.\n     \"\"\"\n \n-    model_input_names = [\"pixel_values\"]\n+    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n \n     def __init__(\n         self,"
        },
        {
            "sha": "355315a296dbcaa99cbf52ee4e273acf071b4f2d",
            "filename": "src/transformers/models/bridgetower/image_processing_bridgetower_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fimage_processing_bridgetower_fast.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -123,6 +123,7 @@ class BridgeTowerImageProcessorFast(BaseImageProcessorFast):\n     do_pad = True\n     size_divisor = 32\n     valid_kwargs = BridgeTowerFastImageProcessorKwargs\n+    model_input_names = [\"pixel_values\", \"pixel_mask\"]\n \n     def __init__(self, **kwargs: Unpack[BridgeTowerFastImageProcessorKwargs]):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "4aa463c055b13891871d199feeeba1cb66dc4fd3",
            "filename": "src/transformers/models/bridgetower/processing_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -90,25 +90,5 @@ def __call__(\n \n         return encoding\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to RobertaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to RobertaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"BridgeTowerProcessor\"]"
        },
        {
            "sha": "46248c725d6edc7fc4ff58e29d58e10d23b99ac1",
            "filename": "src/transformers/models/bros/processing_bros.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -89,24 +89,5 @@ def __call__(\n \n         return encoding\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names))\n-\n \n __all__ = [\"BrosProcessor\"]"
        },
        {
            "sha": "2c73cb78aeb577953bc3f355735da2cb817fd241",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -192,28 +192,5 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Llama\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"ChameleonProcessor\"]"
        },
        {
            "sha": "f4b950c9e373a6a116acdacab7bdb3aef1f9fc7d",
            "filename": "src/transformers/models/chinese_clip/processing_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -131,26 +131,6 @@ def __call__(\n         else:\n             return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "cda35867d5091ca0c89c35ab62ca6c42e26b2d5a",
            "filename": "src/transformers/models/clap/processing_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -96,25 +96,5 @@ def __call__(self, text=None, audios=None, return_tensors=None, **kwargs):\n         else:\n             return BatchEncoding(data=dict(**audio_features), tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to RobertaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to RobertaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        feature_extractor_input_names = self.feature_extractor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + feature_extractor_input_names))\n-\n \n __all__ = [\"ClapProcessor\"]"
        },
        {
            "sha": "b7b076a4c80b55c756c05732344a16e9cf7b7977",
            "filename": "src/transformers/models/clip/processing_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -116,26 +116,6 @@ def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n         else:\n             return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "ad26148a1486e435dac6e7841795e4d11844744e",
            "filename": "src/transformers/models/clipseg/processing_clipseg.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -130,20 +130,6 @@ def __call__(self, text=None, images=None, visual_prompt=None, return_tensors=No\n         else:\n             return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "125beb1e21e11ced89473e19a9b36e90b49c2caf",
            "filename": "src/transformers/models/clvp/processing_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -73,21 +73,5 @@ def __call__(self, *args, **kwargs):\n             inputs[\"attention_mask\"] = encodings[\"attention_mask\"]\n             return inputs\n \n-    # Copied from transformers.models.whisper.processing_whisper.WhisperProcessor.batch_decode with Whisper->Clvp\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to ClvpTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.whisper.processing_whisper.WhisperProcessor.decode with Whisper->Clvp\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to ClvpTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n \n __all__ = [\"ClvpProcessor\"]"
        },
        {
            "sha": "ec6055489f70b9607dd486b1dd29857915dcdaf6",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -274,26 +274,6 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n             vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n         return MultiModalData(**vision_data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n     @property\n     def query_augmentation_token(self) -> str:\n         \"\"\""
        },
        {
            "sha": "d847c80aa88be3c184eeeeb05deaff0fbe0cf34a",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -250,6 +250,18 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+\n+        # ColQwen doesn't process videos. Make a copy of list when removing\n+        # otherwise `self.feature_extractor.model_input_names` is also modified\n+        image_processor_input_names = [\n+            name for name in image_processor_input_names if name not in [\"pixel_values_videos\", \"video_grid_thw\"]\n+        ]\n+        return tokenizer_input_names + image_processor_input_names\n+\n \n class ColQwen2PreTrainedModel(ColPaliPreTrainedModel):\n     pass"
        },
        {
            "sha": "68c67a976d2513f467115b012a81659c9d23b4ab",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 20,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -249,26 +249,6 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n     @property\n     def query_augmentation_token(self) -> str:\n         \"\"\"\n@@ -411,5 +391,17 @@ def score_retrieval(\n \n         return torch.cat(scores, dim=0)\n \n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+\n+        # ColQwen doesn't process videos. Make a copy of list when removing\n+        # otherwise `self.feature_extractor.model_input_names` is also modified\n+        image_processor_input_names = [\n+            name for name in image_processor_input_names if name not in [\"pixel_values_videos\", \"video_grid_thw\"]\n+        ]\n+        return tokenizer_input_names + image_processor_input_names\n+\n \n __all__ = [\"ColQwen2Processor\"]"
        },
        {
            "sha": "0f929f6a2a0c702e812ff8c57a3bf155b72f3931",
            "filename": "src/transformers/models/csm/processing_csm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -360,5 +360,15 @@ def __call__(\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        feature_extractor_input_names = self.feature_extractor.model_input_names\n+\n+        # Remove `padding_mask`, it is popped and not used when processing. Make a copy of list when removing\n+        # otherwise `self.feature_extractor.model_input_names` is also modified\n+        feature_extractor_input_names = [name for name in feature_extractor_input_names if name != \"padding_mask\"]\n+        return list(tokenizer_input_names + feature_extractor_input_names + [\"input_values_cutoffs\"])\n+\n \n __all__ = [\"CsmProcessor\"]"
        },
        {
            "sha": "9661a5082d4838b4e60042be2ceb747c03e045ce",
            "filename": "src/transformers/models/deprecated/mctct/processing_mctct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmctct%2Fprocessing_mctct.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -82,13 +82,6 @@ def __call__(self, *args, **kwargs):\n             inputs[\"labels\"] = encodings[\"input_ids\"]\n             return inputs\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to AutoTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n     def pad(self, *args, **kwargs):\n         \"\"\"\n         When used in normal mode, this method forwards all its arguments to MCTCTFeatureExtractor's"
        },
        {
            "sha": "b69420bf540cbe737d6509b79d943af9d41e6c24",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/processing_speech_to_text_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fprocessing_speech_to_text_2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -84,20 +84,6 @@ def __call__(self, *args, **kwargs):\n             inputs[\"labels\"] = encodings[\"input_ids\"]\n             return inputs\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Speech2Text2Tokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Speech2Text2Tokenizer's [`~PreTrainedTokenizer.decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @contextmanager\n     def as_target_processor(self):\n         \"\"\""
        },
        {
            "sha": "1ecbac0dfa16f7c2cf70d07775bba2beeb456973",
            "filename": "src/transformers/models/deprecated/tvlt/processing_tvlt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fprocessing_tvlt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fprocessing_tvlt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Ftvlt%2Fprocessing_tvlt.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -82,11 +82,5 @@ def __call__(\n             output_dict.update(images_mixed_dict)\n         return output_dict\n \n-    @property\n-    def model_input_names(self):\n-        image_processor_input_names = self.image_processor.model_input_names\n-        feature_extractor_input_names = self.feature_extractor.model_input_names\n-        return list(dict.fromkeys(image_processor_input_names + feature_extractor_input_names))\n-\n \n __all__ = [\"TvltProcessor\"]"
        },
        {
            "sha": "402f5152a64bda378ccdf5edd512c86fe643145c",
            "filename": "src/transformers/models/dia/processing_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -82,16 +82,6 @@ class DiaProcessor(ProcessorMixin):\n     def __init__(self, feature_extractor, tokenizer, audio_tokenizer):\n         super().__init__(feature_extractor, tokenizer, audio_tokenizer=audio_tokenizer)\n \n-    @property\n-    def model_input_names(self):\n-        \"\"\"\n-        We no longer pass the raw audio values but the codebooks encoded by the `audio_tokenizer`.\n-        Conventions may differ between audio models due to architectural choices.\n-        \"\"\"\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        audio_tokenizer_input_names = [\"decoder_input_ids\", \"decoder_attention_mask\"]\n-        return list(dict.fromkeys(tokenizer_input_names + audio_tokenizer_input_names))\n-\n     def __call__(\n         self,\n         text: Union[str, list[str]],"
        },
        {
            "sha": "288ba1107dd8f4ff2fdbfe1b2d56a6d1cb0b165a",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 4,
            "deletions": 12,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -116,19 +116,11 @@ def __call__(\n             inputs[\"input_ids\"] = encodings[\"input_ids\"]\n             return inputs\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to DonutTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n+    @property\n+    def model_input_names(self):\n+        image_processor_input_names = self.image_processor.model_input_names\n \n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to DonutTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to the\n-        docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n+        return list(image_processor_input_names + [\"input_ids\", \"labels\"])\n \n     @contextmanager\n     def as_target_processor(self):"
        },
        {
            "sha": "19ad898737691dfa8ea6c9a8fca21a3d5c9cf88d",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -244,25 +244,5 @@ def calculate_generate_size(self, ratio, image_area, spatial_factor):\n     def postprocess(self, images: ImageInput, **kwargs):\n         return self.image_processor.postprocess(images, **kwargs)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Emu3TokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Emu3TokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"Emu3Processor\"]"
        },
        {
            "sha": "6b5e670300f6446c045c1eae5ebfecb4afdc91b5",
            "filename": "src/transformers/models/flava/processing_flava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -128,26 +128,6 @@ def __call__(\n         else:\n             return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "8c75879e8b6910bb7db331e7705578c730d41941",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 14,
            "deletions": 13,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -774,19 +774,20 @@ def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens\n \n         return self.batch_decode(padded_output_sequences, skip_special_tokens=skip_special_tokens, **kwargs)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+\n+        # Make a copy of list when removing otherwise `self.image_processor.model_input_names` is also modified\n+        extra_image_inputs = [\n+            \"image_input_ids\",\n+            \"image_patch_indices_per_subsequence\",\n+            \"images\",\n+            \"image_patch_indices_per_batch\",\n+        ]\n+        image_processor_input_names = [name for name in image_processor_input_names if name not in extra_image_inputs]\n+        return list(tokenizer_input_names + image_processor_input_names + [\"image_patches_indices\"])\n \n \n __all__ = [\"FuyuProcessor\"]"
        },
        {
            "sha": "683a8ffdfe8b00c8931aaffae7f990a782f33d13",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 17,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -174,27 +174,13 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Gemma\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Gemma\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names + [\"token_type_ids\"]\n         image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+        image_processor_input_names = [name for name in image_processor_input_names if name != \"num_crops\"]\n+        return list(tokenizer_input_names + image_processor_input_names)\n \n \n __all__ = [\"Gemma3Processor\"]"
        },
        {
            "sha": "62c60395d0512abc8feaf6b342c7e3b6ac12a99d",
            "filename": "src/transformers/models/gemma3n/processing_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -164,28 +164,5 @@ def __call__(\n         text_inputs[\"token_type_ids\"] = token_type_ids.tolist()\n         return BatchFeature(data={**text_inputs, **image_inputs, **audio_inputs}, tensor_type=return_tensors)\n \n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Gemma\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Gemma\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names + [\"token_type_ids\"]\n-        image_processor_input_names = self.image_processor.model_input_names\n-        feature_extactor_input_names = self.feature_extractor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names + feature_extactor_input_names))\n-\n \n __all__ = [\"Gemma3nProcessor\"]"
        },
        {
            "sha": "dff60bce1b8460496130dd1c83cc771ba1e231b7",
            "filename": "src/transformers/models/git/processing_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -114,23 +114,5 @@ def __call__(\n \n         return BatchFeature(data=data, tensor_type=output_kwargs[\"common_kwargs\"].get(\"return_tensors\"))\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        return [\"input_ids\", \"attention_mask\", \"pixel_values\"]\n-\n \n __all__ = [\"GitProcessor\"]"
        },
        {
            "sha": "04a58954df03793e53efdcfa1d7b33b7fefc71b1",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -53,10 +53,10 @@\n     Qwen2_5_VLVisionAttention,\n     Qwen2_5_VLVisionBlock,\n )\n-from ..qwen2_5_vl.processing_qwen2_5_vl import (\n-    Qwen2_5_VLProcessor,\n-    Qwen2_5_VLProcessorKwargs,\n-    Qwen2_5_VLVideosProcessorKwargs,\n+from ..qwen2_5_vl.processing_qwen2_5_vl import Qwen2_5_VLVideosProcessorKwargs\n+from ..qwen2_vl.processing_qwen2_vl import (\n+    Qwen2_VLProcessor,\n+    Qwen2_VLProcessorKwargs,\n )\n \n \n@@ -1502,7 +1502,7 @@ class Glm4vImagesKwargs(ImagesKwargs):\n     merge_size: Optional[int]\n \n \n-class Glm4vProcessorKwargs(Qwen2_5_VLProcessorKwargs):\n+class Glm4vProcessorKwargs(Qwen2_VLProcessorKwargs):\n     images_kwargs: Glm4vImagesKwargs\n     videos_kwargs: Glm4vVideosProcessorKwargs\n     _defaults = {\n@@ -1513,7 +1513,7 @@ class Glm4vProcessorKwargs(Qwen2_5_VLProcessorKwargs):\n     }\n \n \n-class Glm4vProcessor(Qwen2_5_VLProcessor):\n+class Glm4vProcessor(Qwen2_VLProcessor):\n     r\"\"\"\n     Constructs a GLM-4V processor which wraps a GLM-4V image processor and a GLM-4 tokenizer into a single processor.\n     [`~Glm4vProcessor.__call__`] and [`~Glm4vProcessor.decode`] for more information."
        },
        {
            "sha": "e4c5f926cfff5b11dd730709db5176d2af875490",
            "filename": "src/transformers/models/glm4v/processing_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 23,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -41,13 +41,13 @@ class Glm4vImagesKwargs(ImagesKwargs):\n \n class Glm4vProcessorKwargs(ProcessingKwargs, total=False):\n     images_kwargs: Glm4vImagesKwargs\n-    videos_kwargs: Glm4vVideosProcessorKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": False,\n             \"return_mm_token_type_ids\": False,\n         },\n     }\n+    videos_kwargs: Glm4vVideosProcessorKwargs\n \n \n class Glm4vProcessor(ProcessorMixin):\n@@ -66,7 +66,6 @@ class Glm4vProcessor(ProcessorMixin):\n     \"\"\"\n \n     attributes = [\"image_processor\", \"tokenizer\", \"video_processor\"]\n-\n     image_processor_class = \"AutoImageProcessor\"\n     video_processor_class = \"AutoVideoProcessor\"\n \n@@ -251,20 +250,6 @@ def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwarg\n \n         return MultiModalData(**vision_data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     def post_process_image_text_to_text(\n         self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs\n     ):\n@@ -292,12 +277,5 @@ def post_process_image_text_to_text(\n             **kwargs,\n         )\n \n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        names_from_processor = list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-        return names_from_processor + [\"second_per_grid_ts\"]\n-\n \n __all__ = [\"Glm4vProcessor\"]"
        },
        {
            "sha": "22a08ed8c78f8192ef26faaaa5724d1f02882daa",
            "filename": "src/transformers/models/got_ocr2/processing_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -257,25 +257,5 @@ def __call__(\n \n         return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(tokenizer_input_names) + list(image_processor_input_names)\n-\n \n __all__ = [\"GotOcr2Processor\"]"
        },
        {
            "sha": "24f13589f7950feb5ff5dd46cf85c7e857dff465",
            "filename": "src/transformers/models/grounding_dino/processing_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -215,29 +215,6 @@ def _preprocess_input_text(self, text):\n \n         return text\n \n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.decode with BertTokenizerFast->PreTrainedTokenizer\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.model_input_names\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n     def post_process_grounded_object_detection(\n         self,\n         outputs: \"GroundingDinoObjectDetectionOutput\","
        },
        {
            "sha": "259f35feb7ec17d65f52037ed59ce364adf2ad3e",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -518,25 +518,11 @@ def image_tokens(last_was_image):\n             }\n         )\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        return list(tokenizer_input_names + image_processor_input_names + [\"image_attention_mask\"])\n \n \n __all__ = [\"IdeficsProcessor\"]"
        },
        {
            "sha": "751c4693b95ec6404b9ed3b2836cc788bb17ad39",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -262,25 +262,5 @@ def __call__(\n \n         return BatchFeature(inputs, tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"Idefics2Processor\"]"
        },
        {
            "sha": "23e32c5f92443d95f680815883105c584a9a53b7",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -408,27 +408,5 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Idefics3TokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        batched_decode_output = self.tokenizer.batch_decode(*args, **kwargs)\n-        return [self._regex_to_remove_extra_special_tokens.sub(\"<image>\", s) for s in batched_decode_output]\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Idefics3TokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        decode_output = self.tokenizer.decode(*args, **kwargs)\n-        return self._regex_to_remove_extra_special_tokens.sub(\"<image>\", decode_output)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(image_processor_input_names + tokenizer_input_names))\n-\n \n __all__ = [\"Idefics3Processor\"]"
        },
        {
            "sha": "eee860e45c836a293baa69d0f8acf10d6a94fd56",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 18,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -148,28 +148,12 @@ def __call__(\n         encoding = BatchFeature(encoding, tensor_type=return_tensors)\n         return encoding\n \n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.decode with BertTokenizerFast->PreTrainedTokenizer\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.model_input_names\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        qformer_input_names = [\"qformer_input_ids\", \"qformer_attention_mask\"]\n+        return tokenizer_input_names + image_processor_input_names + qformer_input_names\n \n     # overwrite to save the Q-Former tokenizer in a separate folder\n     def save_pretrained(self, save_directory, **kwargs):"
        },
        {
            "sha": "a518c3a1a19cfb2982144fd56c4fd3e31dd121c5",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 3,
            "deletions": 19,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -173,28 +173,12 @@ def __call__(\n         encoding = BatchFeature(encoding, tensor_type=return_tensors)\n         return encoding\n \n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.decode with BertTokenizerFast->PreTrainedTokenizer\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.model_input_names\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        video_processor_input_names = self.video_processor.model_input_names\n+        qformer_input_names = [\"qformer_input_ids\", \"qformer_attention_mask\"]\n+        return tokenizer_input_names + video_processor_input_names + qformer_input_names\n \n     # overwrite to save the Q-Former tokenizer in a separate folder\n     def save_pretrained(self, save_directory, **kwargs):"
        },
        {
            "sha": "095e1594694844bd72b08bd939ab71099706fe9a",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -293,25 +293,5 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(tokenizer_input_names) + list(image_processor_input_names)\n-\n \n __all__ = [\"InternVLProcessor\"]"
        },
        {
            "sha": "2c106002e42e441c55226736039e0a8e92411a1f",
            "filename": "src/transformers/models/janus/processing_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -153,32 +153,12 @@ def __call__(\n \n         return BatchFeature(data=data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     def postprocess(self, images: ImageInput, **kwargs):\n         \"\"\"\n         Forwards all arguments to the image processor's `postprocess` method.\n         Refer to the original method's docstring for more details.\n         \"\"\"\n         return self.image_processor.postprocess(images, **kwargs)\n \n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"JanusProcessor\"]"
        },
        {
            "sha": "cb6f63e2e6181242f624d69f5de7a4e55d321d50",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -405,22 +405,6 @@ def preprocess_examples(\n \n         return result\n \n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.decode with BertTokenizerFast->PreTrainedTokenizer\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     def post_process_generation(self, text, cleanup_and_extract=True):\n         caption = text.split(self.eoi_token)[-1]\n         if cleanup_and_extract:\n@@ -447,11 +431,10 @@ def post_process_image_text_to_text(self, generated_outputs, skip_special_tokens\n         return [self.post_process_generation(text, cleanup_and_extract=False) for text in generated_texts]\n \n     @property\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.model_input_names\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        return tokenizer_input_names + image_processor_input_names + [\"image_embeds_position_mask\"]\n \n     def _insert_patch_index_tokens(self, text: str, bboxes: Union[list[tuple[int]], list[tuple[float]]]) -> str:\n         if bboxes is None or len(bboxes) == 0:"
        },
        {
            "sha": "2641c749483e03c379edef368fde953eac121447",
            "filename": "src/transformers/models/kyutai_speech_to_text/processing_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -86,19 +86,5 @@ def __call__(\n \n         return inputs\n \n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to KyutaiSpeechToTextTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to KyutaiSpeechToTextTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n \n __all__ = [\"KyutaiSpeechToTextProcessor\"]"
        },
        {
            "sha": "27fc874f7293a872b63d100f604a26c06caa41eb",
            "filename": "src/transformers/models/layoutlmv2/processing_layoutlmv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -166,20 +166,6 @@ def get_overflowing_images(self, images, overflow_to_sample_mapping):\n \n         return images_with_overflow\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def model_input_names(self):\n         return [\"input_ids\", \"bbox\", \"token_type_ids\", \"attention_mask\", \"image\"]"
        },
        {
            "sha": "2e34e3a4655a8147c538b0b5738e8770737e9750",
            "filename": "src/transformers/models/layoutlmv3/processing_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -164,20 +164,6 @@ def get_overflowing_images(self, images, overflow_to_sample_mapping):\n \n         return images_with_overflow\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def model_input_names(self):\n         return [\"input_ids\", \"bbox\", \"attention_mask\", \"pixel_values\"]"
        },
        {
            "sha": "c3d9189397eb986a999dbb65b52a910d34bcc8c8",
            "filename": "src/transformers/models/layoutxlm/processing_layoutxlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -165,20 +165,6 @@ def get_overflowing_images(self, images, overflow_to_sample_mapping):\n \n         return images_with_overflow\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def model_input_names(self):\n         return [\"input_ids\", \"bbox\", \"attention_mask\", \"image\"]"
        },
        {
            "sha": "2307ceeffb5cc00fde9b432d64c265998aa5bebd",
            "filename": "src/transformers/models/llama4/processing_llama4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -234,25 +234,5 @@ def __call__(\n \n         return BatchFeature(data={**text_inputs, **image_inputs}, tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(tokenizer_input_names) + list(image_processor_input_names)\n-\n \n __all__ = [\"Llama4Processor\"]"
        },
        {
            "sha": "2dead73d6968de35ece1fe90b3b8dbe3b9197ed5",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -208,28 +208,5 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Llama\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"LlavaProcessor\"]"
        },
        {
            "sha": "e2821289537197657af14b7a8909fc6231a4f151",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -262,28 +262,5 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Llama\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"LlavaNextProcessor\"]"
        },
        {
            "sha": "a90ea1752143da622479a72e633cf9ce47d9e59f",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -302,28 +302,5 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Llama\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"LlavaNextVideoProcessor\"]"
        },
        {
            "sha": "2c924142cbe87779271ba8087c0f3100a4d5414c",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -145,7 +145,7 @@ class LlavaOnevisionImageProcessor(BaseImageProcessor):\n             Whether to convert the image to RGB.\n     \"\"\"\n \n-    model_input_names = [\"pixel_values_videos\"]\n+    model_input_names = [\"pixel_values\", \"image_sizes\", \"batch_num_images\"]\n \n     def __init__(\n         self,"
        },
        {
            "sha": "cd023d8fb3392fc82d076282185ae75fcc14a58e",
            "filename": "src/transformers/models/llava_onevision/image_processing_llava_onevision_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fimage_processing_llava_onevision_fast.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -81,7 +81,7 @@ class LlavaOnevisionImageProcessorFast(BaseImageProcessorFast):\n     do_pad = True\n     image_grid_pinpoints = [[384, 384], [384, 768], [384, 1152], [384, 1536], [384, 1920], [384, 2304], [768, 384], [768, 768], [768, 1152], [768, 1536], [768, 1920], [768, 2304], [1152, 384], [1152, 768], [1152, 1152], [1152, 1536], [1152, 1920], [1152, 2304], [1536, 384], [1536, 768], [1536, 1152], [1536, 1536], [1536, 1920], [1536, 2304], [1920, 384], [1920, 768], [1920, 1152], [1920, 1536], [1920, 1920], [1920, 2304], [2304, 384], [2304, 768], [2304, 1152], [2304, 1536], [2304, 1920], [2304, 2304]]  # fmt: skip\n     valid_kwargs = LlavaOnevisionFastImageProcessorKwargs\n-    model_input_names = [\"pixel_values_videos\"]\n+    model_input_names = [\"pixel_values\", \"image_sizes\", \"batch_num_images\"]\n \n     def __init__(self, **kwargs: Unpack[LlavaOnevisionFastImageProcessorKwargs]):\n         super().__init__(**kwargs)"
        },
        {
            "sha": "784435e6ca0f763e8dc7ed13a6314c7d7a983d14",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -95,7 +95,7 @@ class LlavaOnevisionImageProcessorFast(LlavaNextImageProcessorFast):\n     do_convert_rgb = True\n     do_pad = True\n     image_grid_pinpoints = [[384, 384], [384, 768], [384, 1152], [384, 1536], [384, 1920], [384, 2304], [768, 384], [768, 768], [768, 1152], [768, 1536], [768, 1920], [768, 2304], [1152, 384], [1152, 768], [1152, 1152], [1152, 1536], [1152, 1920], [1152, 2304], [1536, 384], [1536, 768], [1536, 1152], [1536, 1536], [1536, 1920], [1536, 2304], [1920, 384], [1920, 768], [1920, 1152], [1920, 1536], [1920, 1920], [1920, 2304], [2304, 384], [2304, 768], [2304, 1152], [2304, 1536], [2304, 1920], [2304, 2304]]  # fmt: skip\n-    model_input_names = [\"pixel_values_videos\"]\n+    model_input_names = [\"pixel_values\", \"image_sizes\", \"batch_num_images\"]\n \n     # Copied from transformers.models.llava.image_processing_llava_fast.LlavaImageProcessorFast.pad_to_square\n     def pad_to_square("
        },
        {
            "sha": "6b0fbc6dd8e31ba35cd25a4da2a634cbdcbdc11e",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -329,28 +329,5 @@ def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwarg\n \n         return MultiModalData(**vision_data)\n \n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Llama\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"LlavaOnevisionProcessor\"]"
        },
        {
            "sha": "c6208b07bbea4a48fee3577a509315bda5db5db9",
            "filename": "src/transformers/models/markuplm/processing_markuplm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -127,24 +127,5 @@ def __call__(\n \n         return encoded_inputs\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to TrOCRTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to TrOCRTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to the\n-        docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        return tokenizer_input_names\n-\n \n __all__ = [\"MarkupLMProcessor\"]"
        },
        {
            "sha": "0e989b2da7a2164a23059844cc6b09244a5f137c",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 15,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -340,20 +340,6 @@ def __call__(\n \n         return batch_feature\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     def post_process_image_text_to_text(\n         self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs\n     ):\n@@ -386,7 +372,7 @@ def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n \n-        # Remove `num_tiles`, it is popped and used only when processing. Make a copy of list when remocing\n+        # Remove `num_tiles`, it is popped and used only when processing. Make a copy of list when removing\n         # otherwise `self.image_processor.model_input_names` is also modified\n         image_processor_input_names = [name for name in image_processor_input_names if name != \"num_tiles\"]\n         return list(tokenizer_input_names + image_processor_input_names + [\"cross_attention_mask\"])"
        },
        {
            "sha": "dec23738095dbb75f4bbe7e31070579b6448bb11",
            "filename": "src/transformers/models/musicgen/processing_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -106,13 +106,6 @@ def batch_decode(self, *args, **kwargs):\n         else:\n             return self.tokenizer.batch_decode(*args, **kwargs)\n \n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to T5Tokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to the\n-        docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     def _decode_audio(self, audio_values, padding_mask: Optional = None) -> list[np.ndarray]:\n         \"\"\"\n         This method strips any padding from the audio values to return a list of numpy audio arrays."
        },
        {
            "sha": "a35da013622fac61f9b0c36ad153e2ac9cb3dca8",
            "filename": "src/transformers/models/musicgen_melody/processing_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -114,14 +114,6 @@ def batch_decode(self, *args, **kwargs):\n         else:\n             return self.tokenizer.batch_decode(*args, **kwargs)\n \n-    # Copied from transformers.models.musicgen.processing_musicgen.MusicgenProcessor.decode\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to T5Tokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to the\n-        docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     # Copied from transformers.models.musicgen.processing_musicgen.MusicgenProcessor._decode_audio with padding_mask->attention_mask\n     def _decode_audio(self, audio_values, attention_mask: Optional = None) -> list[np.ndarray]:\n         \"\"\""
        },
        {
            "sha": "b50c23e4c4d4c40d4be6d66d2904ad1863623f82",
            "filename": "src/transformers/models/nougat/processing_nougat.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -138,20 +138,6 @@ def __call__(\n             inputs[\"labels\"] = encodings[\"input_ids\"]\n             return inputs\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to NougatTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to NougatTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     def post_process_generation(self, *args, **kwargs):\n         \"\"\"\n         This method forwards all its arguments to NougatTokenizer's [`~PreTrainedTokenizer.post_process_generation`]."
        },
        {
            "sha": "6a4786729b5332e37c631533402b189098156f7d",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "modified",
            "additions": 11,
            "deletions": 15,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -289,21 +289,17 @@ def __call__(\n \n         return encoding\n \n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.decode with BertTokenizerFast->PreTrainedTokenizer\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n+    @property\n+    def model_input_names(self):\n+        image_processor_input_names = self.image_processor.model_input_names\n+        tokenizer_input_names = [\n+            \"classes_attention_mask\",\n+            \"tasks_attention_mask\",\n+            \"tasks_input_ids\",\n+            \"classes_input_ids\",\n+            \"classes_structure\",\n+        ]\n+        return tokenizer_input_names + image_processor_input_names\n \n     def _get_default_image_size(self) -> tuple[int, int]:\n         height = ("
        },
        {
            "sha": "b66670cf56f88616e311f351962a45e857651c6a",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 16,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -288,21 +288,5 @@ def post_process_image_guided_detection(\n             outputs=outputs, threshold=threshold, nms_threshold=nms_threshold, target_sizes=target_sizes\n         )\n \n-    # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.batch_decode\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.decode\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n \n __all__ = [\"Owlv2Processor\"]"
        },
        {
            "sha": "cc606ad416787938dcd7b9457cfe301a0e585f17",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -306,20 +306,6 @@ def post_process_image_guided_detection(\n             outputs=outputs, threshold=threshold, nms_threshold=nms_threshold, target_sizes=target_sizes\n         )\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "831112f1de9e9021a9748a8ffd2a4a00a3544a49",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -337,28 +337,5 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n             vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n         return MultiModalData(**vision_data)\n \n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Gemma\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Gemma\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names with CLIP->PaliGemma\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"PaliGemmaProcessor\"]"
        },
        {
            "sha": "5af18aa155e219c87b151b0585e66ee2908d61be",
            "filename": "src/transformers/models/perception_lm/processing_perception_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -240,25 +240,5 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n             vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n         return MultiModalData(**vision_data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PerceptionLMTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PerceptionLMTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"PerceptionLMProcessor\"]"
        },
        {
            "sha": "177d54aa75a939821f7254dfccfd913cb688f34a",
            "filename": "src/transformers/models/phi4_multimodal/processing_phi4_multimodal.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -169,26 +169,5 @@ def __call__(\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GPT2Tokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GPT2Tokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        audio_processor_input_names = self.audio_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names + audio_processor_input_names))\n-\n \n __all__ = [\"Phi4MultimodalProcessor\"]"
        },
        {
            "sha": "aba1b888b03be105f636aca29928984b0de96f2f",
            "filename": "src/transformers/models/pix2struct/processing_pix2struct.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -133,25 +133,12 @@ def __call__(\n \n         return encoding_image_processor\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Pix2StructTokenizerFast's [`~PreTrainedTokenizer.batch_decode`].\n-        Please refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Pix2StructTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        decoder_ids = [\"decoder_attention_mask\", \"decoder_input_ids\"]\n+        return tokenizer_input_names + image_processor_input_names + decoder_ids\n \n \n __all__ = [\"Pix2StructProcessor\"]"
        },
        {
            "sha": "b59aa840ff87e986fec80d3b8ebdb781a338ac60",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 18,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -267,28 +267,11 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n \n         return MultiModalData(**vision_data)\n \n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Llama\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n         image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+        return tokenizer_input_names + image_processor_input_names + [\"image_sizes\"]\n \n \n __all__ = [\"PixtralProcessor\"]"
        },
        {
            "sha": "bb914b3f87106197bccdd8c7e82828a889098e05",
            "filename": "src/transformers/models/pop2piano/processing_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -123,12 +123,6 @@ def batch_decode(\n             token_ids=token_ids, feature_extractor_output=feature_extractor_output, return_midi=return_midi\n         )\n \n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        feature_extractor_input_names = self.feature_extractor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + feature_extractor_input_names))\n-\n     def save_pretrained(self, save_directory, **kwargs):\n         if os.path.isfile(save_directory):\n             raise ValueError(f\"Provided path ({save_directory}) should be a directory, not a file\")"
        },
        {
            "sha": "3b3f764a84dff14d7abe2a938566234758ff64cf",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -318,20 +318,6 @@ def _iter():\n \n         return list(_iter())\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     def apply_chat_template(self, conversations, chat_template=None, **kwargs):\n         is_batched = False\n         if isinstance(conversations[0], dict):"
        },
        {
            "sha": "25dead191c7bb62dbd576e4f64a6153b0172b68b",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -240,20 +240,6 @@ def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwarg\n \n         return MultiModalData(**vision_data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     def post_process_image_text_to_text(\n         self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs\n     ):"
        },
        {
            "sha": "da96b54fe694194171a306025fef11e878de079b",
            "filename": "src/transformers/models/qwen2_audio/processing_qwen2_audio.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -174,20 +174,6 @@ def __call__(\n \n         return BatchFeature(data={**inputs}, tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names"
        },
        {
            "sha": "1c6fe1d22f9b15d4eb08eab32b50328fa154a77d",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -224,20 +224,6 @@ def _get_num_multimodal_tokens(self, image_sizes=None, video_sizes=None, **kwarg\n \n         return MultiModalData(**vision_data)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Qwen2TokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     def post_process_image_text_to_text(\n         self, generated_outputs, skip_special_tokens=True, clean_up_tokenization_spaces=False, **kwargs\n     ):\n@@ -265,11 +251,5 @@ def post_process_image_text_to_text(\n             **kwargs,\n         )\n \n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"Qwen2VLProcessor\"]"
        },
        {
            "sha": "603adde95040037ff33dfce024a56d40246a7d6b",
            "filename": "src/transformers/models/sam/processing_sam.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -288,7 +288,7 @@ def _check_and_preprocess_points(\n     @property\n     def model_input_names(self):\n         image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(image_processor_input_names))\n+        return list(image_processor_input_names + [\"original_sizes\", \"reshaped_input_sizes\"])\n \n     def post_process_masks(self, *args, **kwargs):\n         return self.image_processor.post_process_masks(*args, **kwargs)"
        },
        {
            "sha": "49681c7c6a266415fa92be977bf019d878671a4d",
            "filename": "src/transformers/models/sam_hq/processing_samhq.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -269,7 +269,7 @@ def _check_and_preprocess_points(\n     @property\n     def model_input_names(self):\n         image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(image_processor_input_names))\n+        return list(image_processor_input_names + [\"original_sizes\", \"reshaped_input_sizes\"])\n \n     def post_process_masks(self, *args, **kwargs):\n         return self.image_processor.post_process_masks(*args, **kwargs)"
        },
        {
            "sha": "ffee704e92c982075cc9d376a059ce6e52901f3c",
            "filename": "src/transformers/models/seamless_m4t/processing_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -96,25 +96,5 @@ def __call__(self, text=None, audios=None, src_lang=None, tgt_lang=None, **kwarg\n             encoding = self.feature_extractor(audios, sampling_rate=sampling_rate, **kwargs)\n             return encoding\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to SeamlessM4TTokenizerFast's [`~PreTrainedTokenizer.batch_decode`].\n-        Please refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to SeamlessM4TTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        feature_extractor_input_names = self.feature_extractor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + feature_extractor_input_names))\n-\n \n __all__ = [\"SeamlessM4TProcessor\"]"
        },
        {
            "sha": "69d8e6a89e588cc714f4557ace7be067e07074a0",
            "filename": "src/transformers/models/shieldgemma2/processing_shieldgemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fshieldgemma2%2Fprocessing_shieldgemma2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -171,25 +171,5 @@ def __call__(\n         text = self.apply_chat_template(messages, tokenize=False)\n         return super().__call__(images=expanded_images, text=text, **kwargs)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to GemmaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names + [\"token_type_ids\"]\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"ShieldGemma2Processor\"]"
        },
        {
            "sha": "8326838e20d35af2656715a007ba0bbad2d28e9b",
            "filename": "src/transformers/models/siglip/processing_siglip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -120,26 +120,5 @@ def __call__(\n         else:\n             return BatchFeature(data=dict(**image_features), tensor_type=return_tensors)\n \n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to SiglipTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to SiglipTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    @property\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names with CLIP->Siglip, T5->Siglip\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"SiglipProcessor\"]"
        },
        {
            "sha": "2b638027474a8e5492a76f04873197315ca19aaa",
            "filename": "src/transformers/models/siglip2/processing_siglip2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -147,25 +147,5 @@ def __call__(\n             return_tensors = output_kwargs[\"common_kwargs\"][\"return_tensors\"]\n             return BatchFeature(data=dict(**image_features), tensor_type=return_tensors)\n \n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Siglip2Tokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Siglip2Tokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"Siglip2Processor\"]"
        },
        {
            "sha": "c8dd26631bf0b2faee5401eb605a51c0cd67c54a",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -365,28 +365,6 @@ def __call__(\n \n         return BatchFeature(inputs, tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to SmolVLMTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        batched_decode_output = self.tokenizer.batch_decode(*args, **kwargs)\n-        return batched_decode_output\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to SmolVLMTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        decode_output = self.tokenizer.decode(*args, **kwargs)\n-        return decode_output\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(image_processor_input_names + tokenizer_input_names))\n-\n     def apply_chat_template(\n         self,\n         conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],"
        },
        {
            "sha": "2df5808b9f66c0421471fd17b14eded19d36767b",
            "filename": "src/transformers/models/speech_to_text/processing_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -85,20 +85,6 @@ def __call__(self, *args, **kwargs):\n             inputs[\"labels\"] = encodings[\"input_ids\"]\n             return inputs\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Speech2TextTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to Speech2TextTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @contextmanager\n     def as_target_processor(self):\n         \"\"\""
        },
        {
            "sha": "6be19f8ae9ed1c1cde5c4579b39344221b970758",
            "filename": "src/transformers/models/speecht5/processing_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fprocessing_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fprocessing_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fprocessing_speecht5.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -168,19 +168,5 @@ def pad(self, *args, **kwargs):\n \n         return inputs\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to SpeechT5Tokenizer's [`~SpeechT5Tokenizer.batch_decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to SpeechT5Tokenizer's [`~SpeechT5Tokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n \n __all__ = [\"SpeechT5Processor\"]"
        },
        {
            "sha": "52cd1dc041db7f45346db8668266d92d9dfbd614",
            "filename": "src/transformers/models/trocr/processing_trocr.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -109,19 +109,10 @@ def __call__(\n             inputs[\"labels\"] = encodings[\"input_ids\"]\n             return inputs\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to TrOCRTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to TrOCRTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to the\n-        docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n+    @property\n+    def model_input_names(self):\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return image_processor_input_names + [\"labels\"]\n \n     @contextmanager\n     def as_target_processor(self):"
        },
        {
            "sha": "a4fbe2adeae32ae1821844778fb52fbf2111af3b",
            "filename": "src/transformers/models/tvp/processing_tvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -108,20 +108,6 @@ def __call__(self, text=None, videos=None, return_tensors=None, **kwargs):\n \n         return BatchEncoding(data=encoding, tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     def post_process_video_grounding(self, logits, video_durations):\n         \"\"\"\n         Compute the time of the video.\n@@ -145,12 +131,5 @@ def post_process_video_grounding(self, logits, video_durations):\n \n         return start, end\n \n-    @property\n-    # Copied from transformers.models.blip.processing_blip.BlipProcessor.model_input_names\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"TvpProcessor\"]"
        },
        {
            "sha": "94b1565c9a22fdf87505cebbe9ae500c8660372c",
            "filename": "src/transformers/models/udop/processing_udop.py",
            "status": "modified",
            "additions": 4,
            "deletions": 17,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -183,25 +183,12 @@ def get_overflowing_images(self, images, overflow_to_sample_mapping):\n \n         return images_with_overflow\n \n-    # Copied from transformers.models.layoutlmv3.processing_layoutlmv3.LayoutLMv3Processor.batch_decode\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.layoutlmv3.processing_layoutlmv3.LayoutLMv3Processor.decode\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     @property\n     def model_input_names(self):\n-        return [\"pixel_values\", \"input_ids\", \"bbox\", \"attention_mask\"]\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+\n+        return list(tokenizer_input_names + image_processor_input_names + [\"bbox\"])\n \n \n __all__ = [\"UdopProcessor\"]"
        },
        {
            "sha": "2c7119fdc2a63662b7cf82337e4752fa2038f823",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -196,28 +196,5 @@ def __call__(\n \n         return BatchFeature(data=data, tensor_type=return_tensors)\n \n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.decode with CLIP->Llama\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to LlamaTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    # Copied from transformers.models.clip.processing_clip.CLIPProcessor.model_input_names\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n \n __all__ = [\"VideoLlavaProcessor\"]"
        },
        {
            "sha": "91cc5c17561dff5dbdc83a23f170e57960fc2808",
            "filename": "src/transformers/models/vilt/processing_vilt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -111,26 +111,6 @@ def __call__(\n \n         return encoding\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to BertTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "816d229ceac72c9de7b91e3f4b877ea299780d17",
            "filename": "src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -135,26 +135,6 @@ def __call__(\n                 tensor_type=output_kwargs[\"common_kwargs\"].get(\"return_tensors\"),\n             )\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to VisionTextDualEncoderTokenizer's\n-        [`~PreTrainedTokenizer.batch_decode`]. Please refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to VisionTextDualEncoderTokenizer's [`~PreTrainedTokenizer.decode`].\n-        Please refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        tokenizer_input_names = self.tokenizer.model_input_names\n-        image_processor_input_names = self.image_processor.model_input_names\n-        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "0cf2d121f9daad7962ba6bd632cfb347d6f2c5c1",
            "filename": "src/transformers/models/voxtral/processing_voxtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 26,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \n import io\n-import warnings\n from typing import Optional, Union\n \n from ...utils import is_mistral_common_available, is_soundfile_available, is_torch_available, logging\n@@ -432,30 +431,5 @@ def apply_transcription_request(\n \n         return texts\n \n-    # Deprecated typo'd method for backward compatibility\n-    def apply_transcrition_request(self, *args, **kwargs):\n-        \"\"\"\n-        Deprecated typo'd method. Use `apply_transcription_request` instead.\n-        \"\"\"\n-        warnings.warn(\n-            \"`apply_transcrition_request` is deprecated due to a typo and will be removed in a future release. Please use `apply_transcription_request` instead.\",\n-            FutureWarning,\n-        )\n-        return self.apply_transcription_request(*args, **kwargs)\n-\n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to MistralCommonTokenizer's [`~MistralCommonTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to MistralCommonTokenizer's [`~MistralCommonTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n \n __all__ = [\"VoxtralProcessor\"]"
        },
        {
            "sha": "c2da5ac9e398d4fa968ded541d7e19ce5083acb3",
            "filename": "src/transformers/models/wav2vec2/processing_wav2vec2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -165,19 +165,11 @@ def pad(self, *args, **kwargs):\n             input_features[\"labels\"] = labels[\"input_ids\"]\n             return input_features\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n+    @property\n+    def model_input_names(self):\n+        # The processor doesn't return text ids and the model seems to not need them\n+        feature_extractor_input_names = self.feature_extractor.model_input_names\n+        return feature_extractor_input_names + [\"labels\"]\n \n     @contextmanager\n     def as_target_processor(self):"
        },
        {
            "sha": "253ddaac6c19996e70e26a4f59351d72caa5d6b5",
            "filename": "src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 13,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -145,19 +145,11 @@ def pad(self, input_features=None, labels=None, **kwargs):\n             input_features[\"labels\"] = labels[\"input_ids\"]\n             return input_features\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer\n-        to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n+    @property\n+    def model_input_names(self):\n+        # The processor doesn't return text ids and the model seems to not need them\n+        feature_extractor_input_names = self.feature_extractor.model_input_names\n+        return feature_extractor_input_names + [\"labels\"]\n \n \n __all__ = [\"Wav2Vec2BertProcessor\"]"
        },
        {
            "sha": "26dd02d3c40a9351c0fb7fc5f6e8f5137df9e449",
            "filename": "src/transformers/models/whisper/processing_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -79,20 +79,6 @@ def __call__(self, *args, **kwargs):\n             inputs[\"labels\"] = encodings[\"input_ids\"]\n             return inputs\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to WhisperTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to WhisperTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n     def get_prompt_ids(self, text: str, return_tensors=\"np\"):\n         return self.tokenizer.get_prompt_ids(text, return_tensors=return_tensors)\n "
        },
        {
            "sha": "d8c947f3abbee39077470045ccd122ba77cfa1e7",
            "filename": "src/transformers/models/x_clip/processing_x_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -113,24 +113,6 @@ def __call__(self, text=None, videos=None, return_tensors=None, **kwargs):\n         else:\n             return BatchEncoding(data=dict(**image_features), tensor_type=return_tensors)\n \n-    def batch_decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.batch_decode`]. Please\n-        refer to the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.batch_decode(*args, **kwargs)\n-\n-    def decode(self, *args, **kwargs):\n-        \"\"\"\n-        This method forwards all its arguments to CLIPTokenizerFast's [`~PreTrainedTokenizer.decode`]. Please refer to\n-        the docstring of this method for more information.\n-        \"\"\"\n-        return self.tokenizer.decode(*args, **kwargs)\n-\n-    @property\n-    def model_input_names(self):\n-        return [\"input_ids\", \"attention_mask\", \"position_ids\", \"pixel_values\"]\n-\n     @property\n     def feature_extractor_class(self):\n         warnings.warn("
        },
        {
            "sha": "eb836db1ff27b36cd57611138778fff1c4ba0930",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 24,
            "deletions": 2,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -1397,10 +1397,32 @@ def get_possibly_dynamic_module(module_name):\n             f\"other functions can find it!\"\n         )\n \n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        if not hasattr(self, \"tokenizer\"):\n+            raise ValueError(f\"Cannot batch decode text: {self.__class__.__name__} has no tokenizer.\")\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to PreTrainedTokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        if not hasattr(self, \"tokenizer\"):\n+            raise ValueError(f\"Cannot decode text: {self.__class__.__name__} has no tokenizer.\")\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n     @property\n     def model_input_names(self):\n-        first_attribute = getattr(self, self.attributes[0])\n-        return getattr(first_attribute, \"model_input_names\", None)\n+        model_input_names = []\n+        for attribute_name in self.attributes:\n+            attribute = getattr(self, attribute_name, None)\n+            attr_input_names = getattr(attribute, \"model_input_names\")\n+            model_input_names.extend(attr_input_names)\n+        return model_input_names\n \n     @staticmethod\n     def validate_init_kwargs(processor_config, valid_kwargs):"
        },
        {
            "sha": "0adfc5a822052d9edf2eb14876d8e3b67ebffb5d",
            "filename": "tests/models/align/test_processing_align.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Falign%2Ftest_processing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Falign%2Ftest_processing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_processing_align.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -163,7 +163,7 @@ def test_processor(self):\n \n         inputs = processor(text=input_str, images=image_input)\n \n-        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"token_type_ids\", \"attention_mask\", \"pixel_values\"])\n+        self.assertSetEqual(set(inputs.keys()), {\"input_ids\", \"token_type_ids\", \"attention_mask\", \"pixel_values\"})\n \n         # test if it raises when no input is passed\n         with pytest.raises(ValueError):\n@@ -181,16 +181,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = AlignProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)"
        },
        {
            "sha": "d9f045332ed34632073118fa71f8a7c4b744ec87",
            "filename": "tests/models/blip/test_processing_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fblip%2Ftest_processing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fblip%2Ftest_processing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_processing_blip.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -129,20 +129,6 @@ def test_tokenizer_decode(self):\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n \n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = BlipProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        # For now the processor supports only ['pixel_values', 'input_ids', 'attention_mask']\n-        self.assertListEqual(list(inputs.keys()), [\"pixel_values\", \"input_ids\", \"attention_mask\"])\n-\n     @require_torch\n     @require_vision\n     def test_unstructured_kwargs_batched(self):"
        },
        {
            "sha": "e5c17a11ce028f4c20395994987d3add0ab4fb8b",
            "filename": "tests/models/blip_2/test_processing_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fblip_2%2Ftest_processing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fblip_2%2Ftest_processing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_processing_blip_2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -118,18 +118,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-        processor_kwargs = self.prepare_processor_dict()\n-\n-        processor = Blip2Processor(tokenizer=tokenizer, image_processor=image_processor, **processor_kwargs)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        # For now the processor supports only ['pixel_values', 'input_ids', 'attention_mask']\n-        self.assertCountEqual(list(inputs.keys()), [\"input_ids\", \"pixel_values\", \"attention_mask\"])"
        },
        {
            "sha": "27eabc6071c1f4c1336a8440d923c34a8456346b",
            "filename": "tests/models/chinese_clip/test_processing_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_processing_chinese_clip.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -202,16 +202,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = ChineseCLIPProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"AlexandraT-shirt15\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)"
        },
        {
            "sha": "cce1705a51a71f31af0d855869e71aa28ca22f74",
            "filename": "tests/models/clap/test_processing_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fclap%2Ftest_processing_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fclap%2Ftest_processing_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_processing_clap.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -111,15 +111,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = ClapProcessor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n-\n-        self.assertListEqual(\n-            processor.model_input_names[2:],\n-            feature_extractor.model_input_names,\n-            msg=\"`processor` and `feature_extractor` model input names do not match\",\n-        )"
        },
        {
            "sha": "6ca9a47b29c7cf18bea2c719477fc73713fc0c17",
            "filename": "tests/models/clip/test_processing_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 14,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fclip%2Ftest_processing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fclip%2Ftest_processing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_processing_clip.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -148,7 +148,7 @@ def test_processor(self):\n \n         inputs = processor(text=input_str, images=image_input)\n \n-        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values\"])\n+        self.assertSetEqual(set(inputs.keys()), {\"input_ids\", \"attention_mask\", \"pixel_values\"})\n \n         # test if it raises when no input is passed\n         with pytest.raises(ValueError):\n@@ -166,16 +166,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = CLIPProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)"
        },
        {
            "sha": "34caf6df8a5ac2d2a2eb124cbccbeaaf3c41f3e9",
            "filename": "tests/models/clvp/test_processing_clvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fclvp%2Ftest_processing_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fclvp%2Ftest_processing_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclvp%2Ftest_processing_clvp.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -122,15 +122,3 @@ def test_save_load_pretrained_additional_features(self):\n \n         self.assertEqual(processor.feature_extractor.to_json_string(), feature_extractor_add_kwargs.to_json_string())\n         self.assertIsInstance(processor.feature_extractor, ClvpFeatureExtractor)\n-\n-    def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = ClvpProcessor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n-\n-        self.assertListEqual(\n-            sorted(processor.model_input_names),\n-            sorted(set(feature_extractor.model_input_names + tokenizer.model_input_names)),\n-            msg=\"`processor` and `feature_extractor` model input names do not match\",\n-        )"
        },
        {
            "sha": "35a74a11a00bc1cc957a0344954d1eff8c1d65cb",
            "filename": "tests/models/colpali/test_processing_colpali.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -274,3 +274,11 @@ def test_structured_kwargs_nested_from_dict(self):\n \n         inputs = processor(images=image_input, **all_kwargs)\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n+\n+    # Can process only text or images at a time\n+    def test_model_input_names(self):\n+        processor = self.get_processor()\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(images=image_input)\n+\n+        self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))"
        },
        {
            "sha": "560f87a56d9142c12b6def93218d67e3425fee1c",
            "filename": "tests/models/colqwen2/test_processing_colqwen2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -273,3 +273,11 @@ def test_structured_kwargs_nested_from_dict(self):\n \n         inputs = processor(images=image_input, **all_kwargs)\n         self.assertEqual(inputs[self.text_input_name].shape[-1], 76)\n+\n+    # Can process only text or images at a time\n+    def test_model_input_names(self):\n+        processor = self.get_processor()\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(images=image_input)\n+\n+        self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))"
        },
        {
            "sha": "b5d2b196813ee7abe4d890921ea6773f097d8a4f",
            "filename": "tests/models/dia/test_processing_dia.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fdia%2Ftest_processing_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fdia%2Ftest_processing_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdia%2Ftest_processing_dia.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -119,15 +119,6 @@ def test_save_load_pretrained_additional_features(self):\n         self.assertTrue(check_models_equal(processor.audio_tokenizer, audio_tokenizer_add_kwargs))\n         self.assertIsInstance(processor.audio_tokenizer, DacModel)\n \n-    def test_model_input_names(self):\n-        tokenizer = self.get_tokenizer()\n-\n-        self.assertListEqual(\n-            self.processor.model_input_names,\n-            list(dict.fromkeys(tokenizer.model_input_names + [\"decoder_input_ids\", \"decoder_attention_mask\"])),\n-            msg=\"`processor` model input names do not match the expected names.\",\n-        )\n-\n     def test_tokenize(self):\n         tokenizer = self.get_tokenizer()\n         random_text = [\"This is a processing test for tokenization\", \"[S1] Dia template style [S2] Nice\"]"
        },
        {
            "sha": "8a3f7252b70089ed1049496d066efb3becec1d09",
            "filename": "tests/models/flava/test_processing_flava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fflava%2Ftest_processing_flava.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -219,16 +219,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = FlavaProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)"
        },
        {
            "sha": "320b821d6f79cf75d4204c8995f164021dc1eb36",
            "filename": "tests/models/gemma3n/test_processing_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 20,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_processing_gemma3n.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -29,6 +29,7 @@\n     from transformers.models.gemma3n import Gemma3nAudioFeatureExtractor, Gemma3nProcessor\n \n \n+# TODO: omni-modal processor can't run tests from `ProcessorTesterMixin`\n @require_torch\n @require_torchaudio\n @require_vision\n@@ -169,23 +170,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-        image_processor = self.get_image_processor()\n-        processor = Gemma3nProcessor(\n-            tokenizer=tokenizer, feature_extractor=feature_extractor, image_processor=image_processor\n-        )\n-\n-        for key in feature_extractor.model_input_names:\n-            self.assertIn(\n-                key,\n-                processor.model_input_names,\n-            )\n-\n-        for key in image_processor.model_input_names:\n-            self.assertIn(\n-                key,\n-                processor.model_input_names,\n-            )"
        },
        {
            "sha": "95d4ee6e7f5149e33d9b1a109230dcc0d37cc733",
            "filename": "tests/models/git/test_processing_git.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fgit%2Ftest_processing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fgit%2Ftest_processing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_processing_git.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -130,17 +130,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = GitProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        # For now the processor supports only ['input_ids', 'attention_mask', 'pixel_values']\n-        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"attention_mask\", \"pixel_values\"])"
        },
        {
            "sha": "42d9743c1f381c137b8d7afcf9d6672c03858827",
            "filename": "tests/models/grounding_dino/test_processing_grounding_dino.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_processing_grounding_dino.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -258,20 +258,6 @@ def test_tokenizer_decode(self):\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n \n-    # Copied from tests.models.clip.test_processing_clip.CLIPProcessorTest.test_model_input_names with CLIP->GroundingDino\n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = GroundingDinoProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n-\n     def test_text_preprocessing_equivalence(self):\n         processor = GroundingDinoProcessor.from_pretrained(self.tmpdirname)\n "
        },
        {
            "sha": "eeb043e045400ad3be2e20de4a999f2a7728bbbd",
            "filename": "tests/models/idefics/test_processing_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fidefics%2Ftest_processing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fidefics%2Ftest_processing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_processing_idefics.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -207,15 +207,3 @@ def test_tokenizer_left_padding(self):\n \n         self.assertListEqual(max_length[\"attention_mask\"][-1].tolist(), predicted_attention_masks[1])\n         self.assertListEqual(longest[\"attention_mask\"][-1].tolist(), predicted_attention_masks[0])\n-\n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = IdeficsProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-        prompts = self.prepare_prompts()\n-\n-        inputs = processor(text=prompts, padding=\"longest\", return_tensors=\"pt\")\n-\n-        # For now the processor supports only ['pixel_values', 'input_ids', 'attention_mask']\n-        self.assertSetEqual(set(inputs.keys()), set(self.input_keys))"
        },
        {
            "sha": "019fe85f72e12fdc1a26118068b381b535f79401",
            "filename": "tests/models/instructblip/test_processing_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Finstructblip%2Ftest_processing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Finstructblip%2Ftest_processing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_processing_instructblip.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -156,26 +156,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-        qformer_tokenizer = self.get_qformer_tokenizer()\n-        processor_kwargs = self.prepare_processor_dict()\n-\n-        processor = InstructBlipProcessor(\n-            tokenizer=tokenizer,\n-            image_processor=image_processor,\n-            qformer_tokenizer=qformer_tokenizer,\n-            **processor_kwargs,\n-        )\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        self.assertListEqual(\n-            list(inputs.keys()),\n-            [\"qformer_input_ids\", \"qformer_attention_mask\", \"input_ids\", \"attention_mask\", \"pixel_values\"],\n-        )"
        },
        {
            "sha": "dc476ff2436fc59f4836a16be8fb5f1bdb921742",
            "filename": "tests/models/instructblipvideo/test_processing_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Finstructblipvideo%2Ftest_processing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Finstructblipvideo%2Ftest_processing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_processing_instructblipvideo.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -183,26 +183,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        video_processor = self.get_video_processor()\n-        tokenizer = self.get_tokenizer()\n-        qformer_tokenizer = self.get_qformer_tokenizer()\n-        processor_kwargs = self.prepare_processor_dict()\n-\n-        processor = InstructBlipVideoProcessor(\n-            tokenizer=tokenizer,\n-            video_processor=video_processor,\n-            qformer_tokenizer=qformer_tokenizer,\n-            **processor_kwargs,\n-        )\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        self.assertListEqual(\n-            list(inputs.keys()),\n-            [\"qformer_input_ids\", \"qformer_attention_mask\", \"input_ids\", \"attention_mask\", \"pixel_values\"],\n-        )"
        },
        {
            "sha": "478e9f201007ffae6bf702fe8448f2fddd55cbfe",
            "filename": "tests/models/kosmos2/test_processing_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 23,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -185,29 +185,6 @@ def test_tokenizer_decode(self):\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n \n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = Kosmos2Processor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"This is a test\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # both image and text\n-        inputs = processor(text=input_str, images=image_input)\n-        self.assertListEqual(\n-            list(inputs.keys()), [\"pixel_values\", \"input_ids\", \"attention_mask\", \"image_embeds_position_mask\"]\n-        )\n-\n-        # only text\n-        inputs = processor(text=input_str)\n-        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"attention_mask\"])\n-\n-        # only image\n-        inputs = processor(images=image_input)\n-        self.assertListEqual(list(inputs.keys()), [\"pixel_values\"])\n-\n     @require_torch\n     def test_full_processor(self):\n         url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\""
        },
        {
            "sha": "cdc0bfe7208e5daa4e059f7c66e855c7881e25d2",
            "filename": "tests/models/layoutlmv2/test_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Flayoutlmv2%2Ftest_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Flayoutlmv2%2Ftest_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_processing_layoutlmv2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -133,20 +133,6 @@ def test_save_load_pretrained_additional_features(self):\n         self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n         self.assertIsInstance(processor.image_processor, LayoutLMv2ImageProcessor)\n \n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = LayoutLMv2Processor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # add extra args\n-        inputs = processor(text=input_str, images=image_input, return_codebook_pixels=False, return_image_mask=False)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n-\n     @slow\n     def test_overflowing_tokens(self):\n         # In the case of overflowing tokens, test that we still have 1-to-1 mapping between the images and input_ids (sequences that are too long are broken down into multiple sequences)."
        },
        {
            "sha": "eaba5fcfa5571478c9a7149f50cb90d08a93c54c",
            "filename": "tests/models/layoutlmv3/test_processing_layoutlmv3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Flayoutlmv3%2Ftest_processing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Flayoutlmv3%2Ftest_processing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_processing_layoutlmv3.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -146,20 +146,6 @@ def test_save_load_pretrained_additional_features(self):\n         self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n         self.assertIsInstance(processor.image_processor, LayoutLMv3ImageProcessor)\n \n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = LayoutLMv3Processor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # add extra args\n-        inputs = processor(text=input_str, images=image_input, return_codebook_pixels=False, return_image_mask=False)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n-\n \n # different use cases tests\n @require_torch"
        },
        {
            "sha": "9638bef1b2d4669b0fea63d9b68261e36a38184e",
            "filename": "tests/models/layoutxlm/test_processing_layoutxlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Flayoutxlm%2Ftest_processing_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Flayoutxlm%2Ftest_processing_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_processing_layoutxlm.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -139,20 +139,6 @@ def test_save_load_pretrained_additional_features(self):\n         self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n         self.assertIsInstance(processor.image_processor, LayoutLMv2ImageProcessor)\n \n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = LayoutXLMProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        # add extra args\n-        inputs = processor(text=input_str, images=image_input, return_codebook_pixels=False, return_image_mask=False)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n-\n     @slow\n     def test_overflowing_tokens(self):\n         # In the case of overflowing tokens, test that we still have 1-to-1 mapping between the images and input_ids (sequences that are too long are broken down into multiple sequences)."
        },
        {
            "sha": "fa2e913d99526aef739dcd83dd795610cca452f2",
            "filename": "tests/models/markuplm/test_processing_markuplm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fmarkuplm%2Ftest_processing_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fmarkuplm%2Ftest_processing_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_processing_markuplm.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -130,18 +130,6 @@ def test_save_load_pretrained_additional_features(self):\n         self.assertEqual(processor.feature_extractor.to_json_string(), feature_extractor_add_kwargs.to_json_string())\n         self.assertIsInstance(processor.feature_extractor, MarkupLMFeatureExtractor)\n \n-    def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = MarkupLMProcessor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n-\n-        self.assertListEqual(\n-            processor.model_input_names,\n-            tokenizer.model_input_names,\n-            msg=\"`processor` and `tokenizer` model input names do not match\",\n-        )\n-\n \n # different use cases tests\n @require_bs4"
        },
        {
            "sha": "a28e956bc6ecea4f65382d53d50d0f1c6346ee81",
            "filename": "tests/models/mgp_str/test_processing_mgp_str.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fmgp_str%2Ftest_processing_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fmgp_str%2Ftest_processing_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmgp_str%2Ftest_processing_mgp_str.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -182,19 +182,6 @@ def test_tokenizer_decode(self):\n \n         self.assertListEqual(decode_strs, decoded_processor)\n \n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = MgpstrProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = None\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n-\n     def test_processor_batch_decode(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()"
        },
        {
            "sha": "f2d556e364f3644bc828b6f0b65ae5cd4c8de315",
            "filename": "tests/models/musicgen/test_processing_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fmusicgen%2Ftest_processing_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fmusicgen%2Ftest_processing_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_processing_musicgen.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -137,18 +137,6 @@ def test_tokenizer_decode(self):\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n \n-    def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = MusicgenProcessor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n-\n-        self.assertListEqual(\n-            processor.model_input_names,\n-            feature_extractor.model_input_names,\n-            msg=\"`processor` and `feature_extractor` model input names do not match\",\n-        )\n-\n     def test_decode_audio(self):\n         feature_extractor = self.get_feature_extractor(padding_side=\"left\")\n         tokenizer = self.get_tokenizer()"
        },
        {
            "sha": "1a52661fe20faec0836230d401b511e438704d5d",
            "filename": "tests/models/musicgen_melody/test_processing_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fmusicgen_melody%2Ftest_processing_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fmusicgen_melody%2Ftest_processing_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_processing_musicgen_melody.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -142,18 +142,6 @@ def test_tokenizer_decode(self):\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n \n-    def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = MusicgenMelodyProcessor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n-\n-        self.assertListEqual(\n-            processor.model_input_names,\n-            feature_extractor.model_input_names,\n-            msg=\"`processor` and `feature_extractor` model input names do not match\",\n-        )\n-\n     # Ignore copy\n     def test_decode_audio(self):\n         feature_extractor = self.get_feature_extractor(padding_side=\"left\")"
        },
        {
            "sha": "4206543ca4db1948179ebfd0043cd195a79abf0b",
            "filename": "tests/models/omdet_turbo/test_processing_omdet_turbo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fomdet_turbo%2Ftest_processing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fomdet_turbo%2Ftest_processing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fomdet_turbo%2Ftest_processing_omdet_turbo.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -190,16 +190,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = OmDetTurboProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_tasks = \"task\"\n-        input_classes = [\"class1\", \"class2\"]\n-        image_input = self.prepare_image_inputs()\n-        inputs = processor(images=image_input, text=input_classes, task=input_tasks, return_tensors=\"pt\")\n-\n-        self.assertListEqual(list(inputs.keys()), self.input_keys)"
        },
        {
            "sha": "a2b6041c0c7f496ef9d24d73deff89cffae18e33",
            "filename": "tests/models/pix2struct/test_processing_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_processing_pix2struct.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -165,27 +165,6 @@ def test_tokenizer_decode(self):\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n \n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = Pix2StructProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = self.prepare_text_inputs()\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        # For now the processor supports only [\"flattened_patches\", \"input_ids\", \"attention_mask\", \"decoder_attention_mask\"]\n-        self.assertListEqual(\n-            list(inputs.keys()), [\"flattened_patches\", \"attention_mask\", \"decoder_attention_mask\", \"decoder_input_ids\"]\n-        )\n-\n-        inputs = processor(text=input_str)\n-\n-        # For now the processor supports only [\"flattened_patches\", \"input_ids\", \"attention_mask\", \"decoder_attention_mask\"]\n-        self.assertListEqual(list(inputs.keys()), [\"input_ids\", \"attention_mask\"])\n-\n     @require_torch\n     @require_vision\n     def test_image_processor_defaults_preserved_by_image_kwargs(self):"
        },
        {
            "sha": "69ba11135b8064487cff313831c42e500655e2fe",
            "filename": "tests/models/pop2piano/test_processing_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 22,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fpop2piano%2Ftest_processing_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fpop2piano%2Ftest_processing_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpop2piano%2Ftest_processing_pop2piano.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -243,25 +243,3 @@ def test_processor(self):\n         # test if it raises when no input is passed\n         with pytest.raises(ValueError):\n             processor()\n-\n-    def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = Pop2PianoProcessor(\n-            tokenizer=tokenizer,\n-            feature_extractor=feature_extractor,\n-        )\n-\n-        audio, sampling_rate, _, notes = self.get_inputs()\n-        feature_extractor(audio, sampling_rate, return_tensors=\"pt\")\n-\n-        inputs = processor(\n-            audio=audio,\n-            sampling_rate=sampling_rate,\n-            notes=notes,\n-        )\n-        self.assertListEqual(\n-            list(inputs.keys()),\n-            [\"input_features\", \"beatsteps\", \"extrapolated_beatstep\", \"token_ids\"],\n-        )"
        },
        {
            "sha": "2a584efe8099387a30195c94205238e310c5be9f",
            "filename": "tests/models/qwen2_5_omni/test_processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -318,26 +318,6 @@ def test_processor(self):\n         with pytest.raises(ValueError):\n             processor(images=image_input)\n \n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-        feature_extractor = self.get_feature_extractor()\n-        video_processor = self.get_video_processor()\n-        processor = self.processor_class(\n-            tokenizer=tokenizer,\n-            video_processor=video_processor,\n-            feature_extractor=feature_extractor,\n-            image_processor=image_processor,\n-        )\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        video_inputs = self.prepare_video_inputs()\n-        audio_input = self.prepare_audio_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, videos=video_inputs, audio=audio_input)\n-        self.assertListEqual(sorted(inputs.keys()), sorted(processor.model_input_names))\n-\n     @require_torch\n     def _test_apply_chat_template(\n         self,"
        },
        {
            "sha": "879f07526fd7991b68dadc8834e7d30065f88d5a",
            "filename": "tests/models/qwen2_5_vl/test_processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -137,23 +137,6 @@ def test_processor(self):\n         with pytest.raises(TypeError):\n             processor(images=image_input)\n \n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-        video_processor = self.get_video_processor()\n-\n-        processor = Qwen2_5_VLProcessor(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        video_inputs = self.prepare_video_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, videos=video_inputs)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n-\n     @require_torch\n     @require_av\n     def _test_apply_chat_template("
        },
        {
            "sha": "8e7a5bc8975700d867fa6d40f798908cd1d22f26",
            "filename": "tests/models/qwen2_vl/test_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -138,23 +138,6 @@ def test_processor(self):\n         with pytest.raises(TypeError):\n             processor(images=image_input)\n \n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-        video_processor = self.get_video_processor()\n-\n-        processor = Qwen2VLProcessor(\n-            tokenizer=tokenizer, image_processor=image_processor, video_processor=video_processor\n-        )\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-        video_inputs = self.prepare_video_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input, videos=video_inputs)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n-\n     @require_torch\n     @require_av\n     def _test_apply_chat_template("
        },
        {
            "sha": "9ed008b834da90d17cb655a06db9827a09a8f7ed",
            "filename": "tests/models/speech_to_text/test_processing_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fspeech_to_text%2Ftest_processing_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fspeech_to_text%2Ftest_processing_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_to_text%2Ftest_processing_speech_to_text.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -145,15 +145,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = Speech2TextProcessor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n-\n-        self.assertListEqual(\n-            processor.model_input_names,\n-            feature_extractor.model_input_names,\n-            msg=\"`processor` and `feature_extractor` model input names do not match\",\n-        )"
        },
        {
            "sha": "a850c5fd938323adf536b459d0cf07ebc48dc488",
            "filename": "tests/models/speecht5/test_processing_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fspeecht5%2Ftest_processing_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fspeecht5%2Ftest_processing_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_processing_speecht5.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -179,15 +179,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = SpeechT5Processor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n-\n-        self.assertListEqual(\n-            processor.model_input_names,\n-            feature_extractor.model_input_names,\n-            msg=\"`processor` and `feature_extractor` model input names do not match\",\n-        )"
        },
        {
            "sha": "de1fa89a7ebf7ee7b7d577078fbf7a1246e40293",
            "filename": "tests/models/udop/test_processing_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fudop%2Ftest_processing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fudop%2Ftest_processing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_processing_udop.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -145,19 +145,6 @@ def test_save_load_pretrained_additional_features(self):\n         self.assertEqual(processor.image_processor.to_json_string(), image_processor_add_kwargs.to_json_string())\n         self.assertIsInstance(processor.image_processor, LayoutLMv3ImageProcessor)\n \n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = UdopProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(images=image_input, text=input_str)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n-\n     def test_text_target(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()"
        },
        {
            "sha": "0d6eadefc5dc5bf56f0a74cdb7331e0fb8e6e2d8",
            "filename": "tests/models/vision_text_dual_encoder/test_processing_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processing_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processing_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_text_dual_encoder%2Ftest_processing_vision_text_dual_encoder.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -167,16 +167,3 @@ def test_tokenizer_decode(self):\n         decoded_tok = tokenizer.batch_decode(predicted_ids)\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n-\n-    def test_model_input_names(self):\n-        image_processor = self.get_image_processor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = VisionTextDualEncoderProcessor(tokenizer=tokenizer, image_processor=image_processor)\n-\n-        input_str = \"lower newer\"\n-        image_input = self.prepare_image_inputs()\n-\n-        inputs = processor(text=input_str, images=image_input)\n-\n-        self.assertListEqual(list(inputs.keys()), processor.model_input_names)"
        },
        {
            "sha": "dc9ae4136315e763191e7e27cae946849638379f",
            "filename": "tests/models/wav2vec2/test_processing_wav2vec2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fwav2vec2%2Ftest_processing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fwav2vec2%2Ftest_processing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2%2Ftest_processing_wav2vec2.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -157,13 +157,11 @@ def test_tokenizer_decode(self):\n         self.assertListEqual(decoded_tok, decoded_processor)\n \n     def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n+        processor = self.get_processor()\n \n-        processor = Wav2Vec2Processor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        text = \"lower newer\"\n+        audio_inputs = self.prepare_audio_inputs()\n+\n+        inputs = processor(text=text, audio=audio_inputs, return_attention_mask=True, return_tensors=\"pt\")\n \n-        self.assertListEqual(\n-            processor.model_input_names,\n-            feature_extractor.model_input_names,\n-            msg=\"`processor` and `feature_extractor` model input names do not match\",\n-        )\n+        self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))"
        },
        {
            "sha": "d05d3e2d44e0b69ea6c0932ae6cbf9245a4c56f8",
            "filename": "tests/models/wav2vec2_bert/test_processing_wav2vec2_bert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_bert%2Ftest_processing_wav2vec2_bert.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -158,13 +158,11 @@ def test_tokenizer_decode(self):\n         self.assertListEqual(decoded_tok, decoded_processor)\n \n     def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n+        processor = self.get_processor()\n \n-        processor = Wav2Vec2BertProcessor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n+        text = \"lower newer\"\n+        audio_inputs = self.prepare_audio_inputs()\n+\n+        inputs = processor(text=text, audio=audio_inputs, return_attention_mask=True, return_tensors=\"pt\")\n \n-        self.assertListEqual(\n-            processor.model_input_names,\n-            feature_extractor.model_input_names,\n-            msg=\"`processor` and `feature_extractor` model input names do not match\",\n-        )\n+        self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))"
        },
        {
            "sha": "705fe30bba38f8185101ed4c502c47ebd504f3c6",
            "filename": "tests/models/wav2vec2_with_lm/test_processing_wav2vec2_with_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processing_wav2vec2_with_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processing_wav2vec2_with_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwav2vec2_with_lm%2Ftest_processing_wav2vec2_with_lm.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -404,19 +404,6 @@ def test_processor_from_auto_processor(self):\n \n         self.assertListEqual(decoded_wav2vec2.text, decoded_auto.text)\n \n-    def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-        decoder = self.get_decoder()\n-\n-        processor = Wav2Vec2ProcessorWithLM(tokenizer=tokenizer, feature_extractor=feature_extractor, decoder=decoder)\n-\n-        self.assertListEqual(\n-            processor.model_input_names,\n-            feature_extractor.model_input_names,\n-            msg=\"`processor` and `feature_extractor` model input names do not match\",\n-        )\n-\n     @staticmethod\n     def get_from_offsets(offsets, key):\n         retrieved_list = [d[key] for d in offsets]"
        },
        {
            "sha": "3373712da848530bfcd3e34814c2965678242f63",
            "filename": "tests/models/whisper/test_processing_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fwhisper%2Ftest_processing_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Fmodels%2Fwhisper%2Ftest_processing_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_processing_whisper.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -124,18 +124,6 @@ def test_tokenizer_decode(self):\n \n         self.assertListEqual(decoded_tok, decoded_processor)\n \n-    def test_model_input_names(self):\n-        feature_extractor = self.get_feature_extractor()\n-        tokenizer = self.get_tokenizer()\n-\n-        processor = WhisperProcessor(tokenizer=tokenizer, feature_extractor=feature_extractor)\n-\n-        self.assertListEqual(\n-            processor.model_input_names,\n-            feature_extractor.model_input_names,\n-            msg=\"`processor` and `feature_extractor` model input names do not match\",\n-        )\n-\n     def test_get_decoder_prompt_ids(self):\n         feature_extractor = self.get_feature_extractor()\n         tokenizer = self.get_tokenizer()"
        },
        {
            "sha": "b4d9d74985525593fbbf8a413f8b9e501a910780",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 30,
            "deletions": 10,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/19ffe0219dae122203f9726669f88ef1c6ea3bb4/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=19ffe0219dae122203f9726669f88ef1c6ea3bb4",
            "patch": "@@ -171,6 +171,14 @@ def prepare_video_inputs(self, batch_size: Optional[int] = None):\n             return video_input\n         return [video_input] * batch_size\n \n+    def prepare_audio_inputs(self, batch_size: Optional[int] = None):\n+        \"\"\"This function prepares a list of numpy audio.\"\"\"\n+        raw_speech = floats_list((1, 1000))\n+        raw_speech = [np.asarray(audio) for audio in raw_speech]\n+        if batch_size is None:\n+            return raw_speech\n+        return raw_speech * batch_size\n+\n     def test_processor_to_json_string(self):\n         processor = self.get_processor()\n         obj = json.loads(processor.to_json_string())\n@@ -206,6 +214,23 @@ def test_processor_from_and_save_pretrained(self):\n                     if \"tokenizer\" not in attribute:\n                         self.assertEqual(repr(attribute_first), repr(attribute_second))\n \n+    def test_model_input_names(self):\n+        processor = self.get_processor()\n+\n+        text = self.prepare_text_inputs(modality=\"image\")\n+        image_input = self.prepare_image_inputs()\n+        video_inputs = self.prepare_video_inputs()\n+        audio_inputs = self.prepare_audio_inputs()\n+        inputs_dict = {\"text\": text, \"images\": image_input, \"videos\": video_inputs, \"audio\": audio_inputs}\n+\n+        call_signature = inspect.signature(processor.__call__)\n+        input_args = [param.name for param in call_signature.parameters.values()]\n+        inputs_dict = {k: v for k, v in inputs_dict.items() if k in input_args}\n+\n+        inputs = processor(**inputs_dict, return_tensors=\"pt\")\n+\n+        self.assertSetEqual(set(inputs.keys()), set(processor.model_input_names))\n+\n     # These kwargs-related tests ensure that processors are correctly instantiated.\n     # they need to be applied only if an image_processor exists.\n \n@@ -434,8 +459,7 @@ def test_tokenizer_defaults_preserved_by_kwargs_audio(self):\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n-        raw_speech = floats_list((3, 1000))\n-        raw_speech = [np.asarray(audio) for audio in raw_speech]\n+        raw_speech = self.prepare_audio_inputs(batch_size=3)\n         inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\")\n         self.assertEqual(len(inputs[self.text_input_name][0]), 300)\n \n@@ -452,8 +476,7 @@ def test_kwargs_overrides_default_tokenizer_kwargs_audio(self):\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n-        raw_speech = floats_list((3, 1000))\n-        raw_speech = [np.asarray(audio) for audio in raw_speech]\n+        raw_speech = self.prepare_audio_inputs(batch_size=3)\n         inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\", max_length=300, padding=\"max_length\")\n \n         self.assertEqual(len(inputs[self.text_input_name][0]), 300)\n@@ -471,8 +494,7 @@ def test_unstructured_kwargs_audio(self):\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n-        raw_speech = floats_list((3, 1000))\n-        raw_speech = [np.asarray(audio) for audio in raw_speech]\n+        raw_speech = self.prepare_audio_inputs(batch_size=3)\n         inputs = processor(text=input_str, audio=raw_speech, return_tensors=\"pt\", max_length=300, padding=\"max_length\")\n \n         self.assertEqual(len(inputs[self.text_input_name][0]), 300)\n@@ -490,8 +512,7 @@ def test_doubly_passed_kwargs_audio(self):\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n-        raw_speech = floats_list((3, 1000))\n-        raw_speech = [np.asarray(audio) for audio in raw_speech]\n+        raw_speech = self.prepare_audio_inputs(batch_size=3)\n         with self.assertRaises(ValueError):\n             _ = processor(\n                 text=input_str,\n@@ -514,8 +535,7 @@ def test_structured_kwargs_audio_nested(self):\n         self.skip_processor_without_typed_kwargs(processor)\n \n         input_str = self.prepare_text_inputs(batch_size=3, modality=\"audio\")\n-        raw_speech = floats_list((3, 1000))\n-        raw_speech = [np.asarray(audio) for audio in raw_speech]\n+        raw_speech = self.prepare_audio_inputs(batch_size=3)\n \n         # Define the kwargs for each modality\n         all_kwargs = {"
        }
    ],
    "stats": {
        "total": 2227,
        "additions": 198,
        "deletions": 2029
    }
}