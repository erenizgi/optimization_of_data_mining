{
    "author": "molbap",
    "message": "Small fix rope kwargs (#35589)\n\n* don't know why this keeps popping up?\r\n\r\n* remove unused rope_kwargs",
    "sha": "395b114bd1a5686fa5670915321af20569d2a9f9",
    "files": [
        {
            "sha": "7b67713c42b7430f717333fc58ee35127aea9404",
            "filename": "docs/source/en/model_doc/musicgen_melody.md",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -266,7 +266,6 @@ Tips:\n ## MusicgenMelodyFeatureExtractor\n \n [[autodoc]] MusicgenMelodyFeatureExtractor\n-    - _extract_stem_indices\n \n ## MusicgenMelodyConfig\n "
        },
        {
            "sha": "739aa0af8dba3e49d6d0ff2187fd18afe486bf1c",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -725,7 +725,6 @@ def _init_weights(self, module):\n class AriaTextRotaryEmbedding(nn.Module):\n     def __init__(self, config: AriaTextConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -737,7 +736,7 @@ def __init__(self, config: AriaTextConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -749,9 +748,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "ee27ce28634fddaf474c71c826abf3b222dd7390",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -122,7 +122,6 @@ def __init__(self, config: BambaConfig, batch_size, dtype=torch.float16, device=\n class BambaRotaryEmbedding(nn.Module):\n     def __init__(self, config: BambaConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -134,7 +133,7 @@ def __init__(self, config: BambaConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -146,9 +145,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "356d3300075f9119f6dfdeb564f61b624f7e4061",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -75,7 +75,6 @@ def forward(self, hidden_states):\n class CohereRotaryEmbedding(nn.Module):\n     def __init__(self, config: CohereConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -87,7 +86,7 @@ def __init__(self, config: CohereConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -99,9 +98,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "9c8a8891e19d8f402dc11fa6466b5c406d8c428b",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -55,7 +55,6 @@\n class Cohere2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Cohere2Config, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -67,7 +66,7 @@ def __init__(self, config: Cohere2Config, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -79,9 +78,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "ac2be71e5fd46d72f200129d4db7d5301c5b997c",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -618,7 +618,6 @@ def __init__(\n         device=None,\n     ):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -630,7 +629,7 @@ def __init__(\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -642,9 +641,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "f14fe15604ba89ecb7ab0e3c5aa30222ea033fc5",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -112,7 +112,6 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n class FalconRotaryEmbedding(nn.Module):\n     def __init__(self, config: FalconConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -124,7 +123,7 @@ def __init__(self, config: FalconConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -136,9 +135,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "810fcd63e0d4b5ce1a9ef7ade699d87ca4f10686",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -94,7 +94,6 @@ def forward(self, x):\n class GemmaRotaryEmbedding(nn.Module):\n     def __init__(self, config: GemmaConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -106,7 +105,7 @@ def __init__(self, config: GemmaConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -118,9 +117,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "a1f68976613c244242fbbc3c3245dcbba5739794",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -326,7 +326,6 @@ def forward(\n class Gemma2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Gemma2Config, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -338,7 +337,7 @@ def __init__(self, config: Gemma2Config, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -350,9 +349,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "c6ea3a1d5f40c899291036a7a3d50faf995f7d87",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -257,7 +257,6 @@ def extra_repr(self):\n class GlmRotaryEmbedding(nn.Module):\n     def __init__(self, config: GlmConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -269,7 +268,7 @@ def __init__(self, config: GlmConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -281,9 +280,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "df78d645b54a55692472ffc63a65cdeafc1c5e46",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -493,7 +493,6 @@ def __init__(self, config, layer_idx=None):\n class GPTNeoXRotaryEmbedding(nn.Module):\n     def __init__(self, config: GPTNeoXConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -505,7 +504,7 @@ def __init__(self, config: GPTNeoXConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -517,9 +516,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "2db8f03c634e78069f78b6141788f14c14b2eb04",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -227,7 +227,6 @@ def _attn(self, query, key, value, attention_mask=None, head_mask=None):\n class GPTNeoXJapaneseRotaryEmbedding(nn.Module):\n     def __init__(self, config: GPTNeoXJapaneseConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -239,7 +238,7 @@ def __init__(self, config: GPTNeoXJapaneseConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -251,9 +250,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "ef73b8015f8758d461876ccb9d161162841fbf7b",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -311,7 +311,6 @@ def forward(\n class GraniteRotaryEmbedding(nn.Module):\n     def __init__(self, config: GraniteConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -323,7 +322,7 @@ def __init__(self, config: GraniteConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -335,9 +334,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "5263eafefb5f814035bc4add8943c90e57270f62",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -160,7 +160,6 @@ def extra_repr(self):\n class GraniteMoeRotaryEmbedding(nn.Module):\n     def __init__(self, config: GraniteMoeConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -172,7 +171,7 @@ def __init__(self, config: GraniteMoeConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -184,9 +183,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "a0682baf699122d72fc205c9d4dbb340a06c94a7",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -388,7 +388,6 @@ def extra_repr(self):\n class JetMoeRotaryEmbedding(nn.Module):\n     def __init__(self, config: JetMoeConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -400,7 +399,7 @@ def __init__(self, config: JetMoeConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -412,9 +411,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "00568a973748a33dd94e8c47093bf55d10ec4701",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -82,7 +82,6 @@ def extra_repr(self):\n class LlamaRotaryEmbedding(nn.Module):\n     def __init__(self, config: LlamaConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -94,7 +93,7 @@ def __init__(self, config: LlamaConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -106,9 +105,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "cb495ec57ea07027d90931b01e62c6aeb5aeebf1",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -367,7 +367,6 @@ def forward(self, x: torch.Tensor):\n class MimiRotaryEmbedding(nn.Module):\n     def __init__(self, config: MimiConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -379,7 +378,7 @@ def __init__(self, config: MimiConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -391,9 +390,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "ec31cc41d2674048813594f31a534c3188a2ed8c",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -272,7 +272,6 @@ def forward(\n class MistralRotaryEmbedding(nn.Module):\n     def __init__(self, config: MistralConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -284,7 +283,7 @@ def __init__(self, config: MistralConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -296,9 +295,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "1183fd4dbd43d61c5334e97dab952e2add008e83",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -394,7 +394,6 @@ def forward(\n class MixtralRotaryEmbedding(nn.Module):\n     def __init__(self, config: MixtralConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -406,7 +405,7 @@ def __init__(self, config: MixtralConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -418,9 +417,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "3ff8bb925c4d1b6d441af6a20f9eb25dc7729aee",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -310,7 +310,6 @@ def forward(self, x, layer_idx=None):\n class MoshiRotaryEmbedding(nn.Module):\n     def __init__(self, config: MoshiConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -322,7 +321,7 @@ def __init__(self, config: MoshiConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -334,9 +333,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "69a8b2ae0ce3490590e7b84199d92c0f3e607156",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -98,7 +98,6 @@ def __init__(\n         self.original_max_seq_len = config.max_position_embeddings\n \n         self.config = config\n-        self.rope_kwargs = None\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n         inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n@@ -113,9 +112,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "b6742702c5bee5969ae3ac0ff6910edd38ae19e6",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -276,7 +276,6 @@ def forward(\n class OlmoRotaryEmbedding(nn.Module):\n     def __init__(self, config: OlmoConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -288,7 +287,7 @@ def __init__(self, config: OlmoConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -300,9 +299,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "1a256cf098e95eb18760077a14127e3fa0e6bbcb",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -277,7 +277,6 @@ def forward(\n class Olmo2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Olmo2Config, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -289,7 +288,7 @@ def __init__(self, config: Olmo2Config, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -301,9 +300,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "b8bc6f5de9deacff2fab5c72a7886cfa24829ebe",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -160,7 +160,6 @@ def extra_repr(self):\n class OlmoeRotaryEmbedding(nn.Module):\n     def __init__(self, config: OlmoeConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -172,7 +171,7 @@ def __init__(self, config: OlmoeConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -184,9 +183,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "e80435a8e7603f8deb9c50e883661b17de124dff",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -59,7 +59,6 @@\n class PersimmonRotaryEmbedding(nn.Module):\n     def __init__(self, config: PersimmonConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -71,7 +70,7 @@ def __init__(self, config: PersimmonConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -83,9 +82,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "8f1867e4f4990c8d06435403c7f4119084bf05f8",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -272,7 +272,6 @@ def forward(\n class PhiRotaryEmbedding(nn.Module):\n     def __init__(self, config: PhiConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -284,7 +283,7 @@ def __init__(self, config: PhiConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -296,9 +295,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "66b01c6a0abac1f331bd88997289abd6dfe6e43b",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -320,7 +320,6 @@ def forward(\n class Phi3RotaryEmbedding(nn.Module):\n     def __init__(self, config: Phi3Config, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -332,7 +331,7 @@ def __init__(self, config: Phi3Config, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -344,9 +343,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "03f1748d0157008eef32a23051d6179fa4a77ffd",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -285,7 +285,6 @@ def forward(\n class Qwen2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Qwen2Config, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -297,7 +296,7 @@ def __init__(self, config: Qwen2Config, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -309,9 +308,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "b1e290d70b6a7b5561b75b851a5cf8efb2c65035",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -169,7 +169,6 @@ def extra_repr(self):\n class Qwen2MoeRotaryEmbedding(nn.Module):\n     def __init__(self, config: Qwen2MoeConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -181,7 +180,7 @@ def __init__(self, config: Qwen2MoeConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -193,9 +192,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "309e33d008be038e38f8fd806804e859d3b0e09c",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -65,7 +65,6 @@\n class StableLmRotaryEmbedding(nn.Module):\n     def __init__(self, config: StableLmConfig, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -77,7 +76,7 @@ def __init__(self, config: StableLmConfig, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -89,9 +88,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        },
        {
            "sha": "605f63b3014f5a0be2e05cb860fab207f7c78a81",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/395b114bd1a5686fa5670915321af20569d2a9f9/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=395b114bd1a5686fa5670915321af20569d2a9f9",
            "patch": "@@ -276,7 +276,6 @@ def forward(\n class Starcoder2RotaryEmbedding(nn.Module):\n     def __init__(self, config: Starcoder2Config, device=None):\n         super().__init__()\n-        self.rope_kwargs = {}\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n             self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n@@ -288,7 +287,7 @@ def __init__(self, config: Starcoder2Config, device=None):\n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n \n-        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, **self.rope_kwargs)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -300,9 +299,7 @@ def _dynamic_frequency_update(self, position_ids, device):\n         \"\"\"\n         seq_len = torch.max(position_ids) + 1\n         if seq_len > self.max_seq_len_cached:  # growth\n-            inv_freq, self.attention_scaling = self.rope_init_fn(\n-                self.config, device, seq_len=seq_len, **self.rope_kwargs\n-            )\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n             self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n             self.max_seq_len_cached = seq_len\n "
        }
    ],
    "stats": {
        "total": 209,
        "additions": 59,
        "deletions": 150
    }
}