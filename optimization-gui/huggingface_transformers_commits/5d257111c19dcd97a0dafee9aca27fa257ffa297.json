{
    "author": "muellerzr",
    "message": "Trainer Refactor: Part 1 (#35567)\n\n* start\r\n\r\n* So far: 30%\r\n\r\n* Small fix\r\n\r\n* Continuing update\r\n\r\n* Continuing\r\n\r\n* Forgot to check if not None\r\n\r\n* Continuing refactor\r\n\r\n* Fix if else\r\n\r\n* Fix ref\r\n\r\n* Should make tests pass\r\n\r\n* Keep grad norm same\r\n\r\n* Document\r\n\r\n* Apply suggestions from code review\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\r\n\r\n* Err instead of info for logging RNG state error\r\n\r\n* Seperate out to func\r\n\r\n---------\r\n\r\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "5d257111c19dcd97a0dafee9aca27fa257ffa297",
    "files": [
        {
            "sha": "927adcb395544d827cc1225f788afc7f53b45840",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 94,
            "deletions": 131,
            "changes": 225,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d257111c19dcd97a0dafee9aca27fa257ffa297/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d257111c19dcd97a0dafee9aca27fa257ffa297/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=5d257111c19dcd97a0dafee9aca27fa257ffa297",
            "patch": "@@ -41,7 +41,6 @@\n # isort: off\n from .integrations import (\n     get_reporting_integration_callbacks,\n-    hp_params,\n )\n \n # isort: on\n@@ -108,6 +107,7 @@\n     nested_xla_mesh_reduce,\n     reissue_pt_warnings,\n     remove_dummy_checkpoint,\n+    set_rng_state_for_device,\n )\n from .trainer_utils import (\n     PREFIX_CHECKPOINT_DIR,\n@@ -2219,46 +2219,25 @@ def _inner_training_loop(\n         # number of training steps per epoch: num_update_steps_per_epoch\n         # total number of training steps to execute: max_steps\n         total_train_batch_size = self._train_batch_size * args.gradient_accumulation_steps * args.world_size\n+        (\n+            num_train_epochs,\n+            num_update_steps_per_epoch,\n+            num_examples,\n+            num_train_samples,\n+            epoch_based,\n+            len_dataloader,\n+            max_steps,\n+        ) = self.set_initial_training_values(args, train_dataloader, total_train_batch_size)\n \n-        len_dataloader = None\n         num_train_tokens = None\n-        if has_length(train_dataloader):\n-            len_dataloader = len(train_dataloader)\n-            num_update_steps_per_epoch = len_dataloader // args.gradient_accumulation_steps\n-            num_update_steps_per_epoch = max(num_update_steps_per_epoch, 1)\n-            num_examples = self.num_examples(train_dataloader)\n-            if args.max_steps > 0:\n-                max_steps = args.max_steps\n-                num_train_epochs = args.max_steps // num_update_steps_per_epoch + int(\n-                    args.max_steps % num_update_steps_per_epoch > 0\n-                )\n-                # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's\n-                # the best we can do.\n-                num_train_samples = args.max_steps * total_train_batch_size\n-                if args.include_tokens_per_second:\n-                    num_train_tokens = (\n-                        self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n-                    )\n+        if self.args.include_tokens_per_second:\n+            num_train_tokens = self.num_tokens(train_dataloader, None if epoch_based else max_steps)\n+            # If going by epochs, multiply tokens linearly\n+            if len_dataloader is not None and epoch_based:\n+                num_train_tokens *= args.num_train_epochs\n+            # Otherwise since its steps, we just multiply by grad accum\n             else:\n-                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n-                num_train_epochs = math.ceil(args.num_train_epochs)\n-                num_train_samples = self.num_examples(train_dataloader) * args.num_train_epochs\n-                if args.include_tokens_per_second:\n-                    num_train_tokens = self.num_tokens(train_dataloader) * args.num_train_epochs\n-        elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size\n-            max_steps = args.max_steps\n-            # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n-            num_train_epochs = sys.maxsize\n-            num_update_steps_per_epoch = max_steps\n-            num_examples = total_train_batch_size * args.max_steps\n-            num_train_samples = args.max_steps * total_train_batch_size\n-            if args.include_tokens_per_second:\n-                num_train_tokens = self.num_tokens(train_dataloader, args.max_steps) * args.gradient_accumulation_steps\n-        else:\n-            raise ValueError(\n-                \"args.max_steps must be set to a positive value if dataloader does not have a length, was\"\n-                f\" {args.max_steps}\"\n-            )\n+                num_train_tokens *= args.gradient_accumulation_steps\n \n         if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n             if self.args.n_gpu > 1:\n@@ -2293,21 +2272,7 @@ def _inner_training_loop(\n         self.state.train_batch_size = self._train_batch_size\n \n         # Compute absolute values for logging, eval, and save if given as ratio\n-        if args.logging_steps is not None:\n-            if args.logging_steps < 1:\n-                self.state.logging_steps = math.ceil(max_steps * args.logging_steps)\n-            else:\n-                self.state.logging_steps = args.logging_steps\n-        if args.eval_steps is not None:\n-            if args.eval_steps < 1:\n-                self.state.eval_steps = math.ceil(max_steps * args.eval_steps)\n-            else:\n-                self.state.eval_steps = args.eval_steps\n-        if args.save_steps is not None:\n-            if args.save_steps < 1:\n-                self.state.save_steps = math.ceil(max_steps * args.save_steps)\n-            else:\n-                self.state.save_steps = args.save_steps\n+        self.state.compute_steps(args, max_steps)\n \n         # Activate gradient checkpointing if needed\n         if args.gradient_checkpointing:\n@@ -2420,25 +2385,7 @@ def _inner_training_loop(\n                 )\n \n         # Update the references\n-        self.callback_handler.model = self.model\n-        self.callback_handler.optimizer = self.optimizer\n-        self.callback_handler.lr_scheduler = self.lr_scheduler\n-        self.callback_handler.train_dataloader = train_dataloader\n-        if self.hp_name is not None and self._trial is not None:\n-            # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n-            # parameter to Train when using DDP.\n-            self.state.trial_name = self.hp_name(self._trial)\n-        if trial is not None:\n-            assignments = trial.assignments if self.hp_search_backend == HPSearchBackend.SIGOPT else trial\n-            self.state.trial_params = hp_params(assignments)\n-        else:\n-            self.state.trial_params = None\n-        # This should be the same if the state has been saved but in case the training arguments changed, it's safer\n-        # to set this after the load.\n-        self.state.max_steps = max_steps\n-        self.state.num_train_epochs = num_train_epochs\n-        self.state.is_local_process_zero = self.is_local_process_zero()\n-        self.state.is_world_process_zero = self.is_world_process_zero()\n+        self.state.init_training_references(self, train_dataloader, max_steps, num_train_epochs, trial)\n \n         # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n         tr_loss = torch.tensor(0.0).to(args.device)\n@@ -2495,10 +2442,7 @@ def _inner_training_loop(\n                     step += 1\n                     do_sync_step = (step + 1) % args.gradient_accumulation_steps == 0 or (step + 1) == steps_in_epoch\n                     # Since we perform prefetching, we need to manually set sync_gradients\n-                    if not do_sync_step:\n-                        self.accelerator.gradient_state._set_sync_gradients(False)\n-                    else:\n-                        self.accelerator.gradient_state._set_sync_gradients(True)\n+                    self.accelerator.gradient_state._set_sync_gradients(do_sync_step)\n \n                     if self.args.include_num_input_tokens_seen:\n                         main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n@@ -2565,8 +2509,6 @@ def _inner_training_loop(\n \n                         # Gradient clipping\n                         if args.max_grad_norm is not None and args.max_grad_norm > 0:\n-                            # deepspeed does its own clipping\n-\n                             if is_sagemaker_mp_enabled() and args.fp16:\n                                 _grad_norm = self.optimizer.clip_master_grads(args.max_grad_norm)\n                             elif self.use_apex:\n@@ -2598,8 +2540,7 @@ def _inner_training_loop(\n \n                         self.control = self.callback_handler.on_optimizer_step(args, self.state, self.control)\n \n-                        optimizer_was_run = not self.accelerator.optimizer_step_was_skipped\n-                        if optimizer_was_run:\n+                        if not self.accelerator.optimizer_step_was_skipped:\n                             # Delay optimizer scheduling until metrics are generated\n                             if not isinstance(self.lr_scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n                                 self.lr_scheduler.step()\n@@ -3119,52 +3060,19 @@ def _load_rng_state(self, checkpoint):\n         random.setstate(checkpoint_rng_state[\"python\"])\n         np.random.set_state(checkpoint_rng_state[\"numpy\"])\n         torch.random.set_rng_state(checkpoint_rng_state[\"cpu\"])\n-        if torch.cuda.is_available():\n-            if self.args.parallel_mode == ParallelMode.DISTRIBUTED:\n-                torch.cuda.random.set_rng_state_all(checkpoint_rng_state[\"cuda\"])\n-            else:\n-                try:\n-                    torch.cuda.random.set_rng_state(checkpoint_rng_state[\"cuda\"])\n-                except Exception as e:\n-                    logger.info(\n-                        f\"Didn't manage to set back the RNG states of the GPU because of the following error:\\n {e}\"\n-                        \"\\nThis won't yield the same results as if the training had not been interrupted.\"\n-                    )\n         if is_torch_xla_available():\n             xm.set_rng_state(checkpoint_rng_state[\"xla\"])\n+\n+        is_distributed = self.args.parallel_mode == ParallelMode.DISTRIBUTED\n+        if torch.cuda.is_available():\n+            set_rng_state_for_device(\"GPU\", torch.cuda, checkpoint_rng_state, is_distributed)\n         if is_torch_npu_available():\n-            if self.args.parallel_mode == ParallelMode.DISTRIBUTED:\n-                torch.npu.random.set_rng_state_all(checkpoint_rng_state[\"npu\"])\n-            else:\n-                try:\n-                    torch.npu.random.set_rng_state(checkpoint_rng_state[\"npu\"])\n-                except Exception as e:\n-                    logger.info(\n-                        f\"Didn't manage to set back the RNG states of the NPU because of the following error:\\n {e}\"\n-                        \"\\nThis won't yield the same results as if the training had not been interrupted.\"\n-                    )\n+            set_rng_state_for_device(\"NPU\", torch.npu, checkpoint_rng_state, is_distributed)\n         if is_torch_mlu_available():\n-            if self.args.parallel_mode == ParallelMode.DISTRIBUTED:\n-                torch.mlu.random.set_rng_state_all(checkpoint_rng_state[\"mlu\"])\n-            else:\n-                try:\n-                    torch.mlu.random.set_rng_state(checkpoint_rng_state[\"mlu\"])\n-                except Exception as e:\n-                    logger.info(\n-                        f\"Didn't manage to set back the RNG states of the MLU because of the following error:\\n {e}\"\n-                        \"\\nThis won't yield the same results as if the training had not been interrupted.\"\n-                    )\n+            set_rng_state_for_device(\"MLU\", torch.mlu, checkpoint_rng_state, is_distributed)\n+\n         if is_torch_musa_available():\n-            if self.args.parallel_mode == ParallelMode.DISTRIBUTED:\n-                torch.musa.set_rng_state_all(checkpoint_rng_state[\"musa\"])\n-            else:\n-                try:\n-                    torch.musa.set_rng_state(checkpoint_rng_state[\"musa\"])\n-                except Exception as e:\n-                    logger.info(\n-                        f\"Didn't manage to set back the RNG states of the MUSA because of the following error:\\n {e}\"\n-                        \"\\nThis won't yield the same results as if the training had not been interrupted.\"\n-                    )\n+            set_rng_state_for_device(\"MUSA\", torch.musa, checkpoint_rng_state, is_distributed)\n \n     def _determine_best_metric(self, metrics, trial):\n         \"\"\"\n@@ -5050,11 +4958,10 @@ def create_accelerator_and_postprocess(self):\n         accelerator_config = self.args.accelerator_config.to_dict()\n \n         if is_accelerate_available(\"0.28.0\"):\n+            # Extract dataloader config params from accelerator config\n+            dataloader_params = [\"split_batches\", \"dispatch_batches\", \"even_batches\", \"use_seedable_sampler\"]\n             dataloader_config = DataLoaderConfiguration(\n-                split_batches=accelerator_config.pop(\"split_batches\"),\n-                dispatch_batches=accelerator_config.pop(\"dispatch_batches\"),\n-                even_batches=accelerator_config.pop(\"even_batches\"),\n-                use_seedable_sampler=accelerator_config.pop(\"use_seedable_sampler\"),\n+                **{param: accelerator_config.pop(param) for param in dataloader_params}\n             )\n             if is_accelerate_available(\"1.1.0\"):\n                 dataloader_config.data_seed = self.args.data_seed\n@@ -5099,12 +5006,8 @@ def create_accelerator_and_postprocess(self):\n         # post accelerator creation setup\n         if self.is_fsdp_enabled:\n             fsdp_plugin = self.accelerator.state.fsdp_plugin\n-            fsdp_plugin.limit_all_gathers = self.args.fsdp_config.get(\n-                \"limit_all_gathers\", fsdp_plugin.limit_all_gathers\n-            )\n-            fsdp_plugin.activation_checkpointing = self.args.fsdp_config.get(\n-                \"activation_checkpointing\", fsdp_plugin.activation_checkpointing\n-            )\n+            for param in [\"limit_all_gathers\", \"activation_checkpointing\"]:\n+                setattr(fsdp_plugin, param, self.args.fsdp_config.get(param, getattr(fsdp_plugin, param)))\n             if fsdp_plugin.activation_checkpointing and self.args.gradient_checkpointing:\n                 raise ValueError(\n                     \"The activation_checkpointing in FSDP config and the gradient_checkpointing in training arg \"\n@@ -5186,3 +5089,63 @@ def get_batch_samples(self, epoch_iterator, num_batches):\n             num_items_in_batch = num_items_in_batch.item()\n \n         return batch_samples, num_items_in_batch\n+\n+    def set_initial_training_values(\n+        self, args: TrainingArguments, dataloader: DataLoader, total_train_batch_size: int\n+    ):\n+        \"\"\"\n+        Calculates and returns the following values:\n+        - `num_train_epochs`\n+        - `num_update_steps_per_epoch`\n+        - `num_examples`\n+        - `num_train_samples`\n+        - `epoch_based`\n+        - `len_dataloader`\n+        - `max_steps`\n+        \"\"\"\n+        # Case 1: we rely on `args.max_steps` first\n+        max_steps = args.max_steps\n+        # If max_steps is negative, we use the number of epochs to determine the number of total steps later\n+        epoch_based = max_steps < 0\n+        len_dataloader = len(dataloader) if has_length(dataloader) else None\n+\n+        # Case 2: We have a dataloader length and can extrapolate\n+        if len_dataloader is not None:\n+            num_update_steps_per_epoch = max(len_dataloader // args.gradient_accumulation_steps, 1)\n+            # Case 3: We have a length but are using epochs, we can extrapolate the number of steps\n+            if epoch_based:\n+                max_steps = math.ceil(args.num_train_epochs * num_update_steps_per_epoch)\n+\n+        # Now we figure out `num_examples`, `num_train_epochs`, and `train_samples`\n+        if len_dataloader:\n+            num_examples = self.num_examples(dataloader)\n+            if args.max_steps > 0:\n+                num_train_epochs = max_steps // num_update_steps_per_epoch + int(\n+                    max_steps % num_update_steps_per_epoch > 0\n+                )\n+                # May be slightly incorrect if the last batch in the training dataloader has a smaller size but it's\n+                # the best we can do.\n+                num_train_samples = max_steps * total_train_batch_size\n+            else:\n+                num_train_epochs = math.ceil(args.num_train_epochs)\n+                num_train_samples = self.num_examples(dataloader) * args.num_train_epochs\n+        elif args.max_steps > 0:  # Rely on max_steps when dataloader does not have a working size\n+            # Setting a very large number of epochs so we go as many times as necessary over the iterator.\n+            num_train_epochs = sys.maxsize\n+            num_update_steps_per_epoch = max_steps\n+            num_examples = total_train_batch_size * args.max_steps\n+            num_train_samples = args.max_steps * total_train_batch_size\n+        else:\n+            raise ValueError(\n+                \"args.max_steps must be set to a positive value if dataloader does not have a length, was\"\n+                f\" {args.max_steps}\"\n+            )\n+        return (\n+            num_train_epochs,\n+            num_update_steps_per_epoch,\n+            num_examples,\n+            num_train_samples,\n+            epoch_based,\n+            len_dataloader,\n+            max_steps,\n+        )"
        },
        {
            "sha": "a2f938e733d2b070d1c8f2b44d9b40bb85deedbd",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 39,
            "deletions": 1,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d257111c19dcd97a0dafee9aca27fa257ffa297/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d257111c19dcd97a0dafee9aca27fa257ffa297/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=5d257111c19dcd97a0dafee9aca27fa257ffa297",
            "patch": "@@ -18,13 +18,14 @@\n \n import dataclasses\n import json\n+import math\n from dataclasses import dataclass\n from typing import Dict, List, Optional, Union\n \n import numpy as np\n from tqdm.auto import tqdm\n \n-from .trainer_utils import IntervalStrategy, SaveStrategy, has_length\n+from .trainer_utils import HPSearchBackend, IntervalStrategy, SaveStrategy, has_length\n from .training_args import TrainingArguments\n from .utils import logging\n \n@@ -150,6 +151,43 @@ def load_from_json(cls, json_path: str):\n             text = f.read()\n         return cls(**json.loads(text))\n \n+    def compute_steps(self, args, max_steps):\n+        \"\"\"\n+        Calculates and stores the absolute value for logging,\n+        eval, and save steps based on if it was a proportion\n+        or not.\n+        \"\"\"\n+        for step_kind in (\"logging\", \"eval\", \"save\"):\n+            num_steps = getattr(args, f\"{step_kind}_steps\")\n+            if num_steps is not None:\n+                if num_steps < 1:\n+                    num_steps = math.ceil(max_steps * num_steps)\n+                setattr(self, f\"{step_kind}_steps\", num_steps)\n+\n+    def init_training_references(self, trainer, train_dataloader, max_steps, num_train_epochs, trial):\n+        \"\"\"\n+        Stores the initial training references needed in `self`\n+        \"\"\"\n+        for attr in (\"model\", \"optimizer\", \"lr_scheduler\"):\n+            setattr(self, attr, getattr(trainer, attr))\n+\n+        self.train_dataloader = train_dataloader\n+        if trainer.hp_name is not None and trainer._trial is not None:\n+            # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n+            # parameter to Train when using DDP.\n+            self.trial_name = trainer.hp_name(trainer._trial)\n+        self.trial_params = None\n+        if trial is not None:\n+            from transformers.integrations import hp_params\n+\n+            assignments = trial.assignments if trainer.hp_search_backend == HPSearchBackend.SIGOPT else trial\n+            self.trial_params = hp_params(assignments)\n+\n+        self.max_steps = max_steps\n+        self.num_train_epochs = num_train_epochs\n+        self.is_local_process_zero = trainer.is_local_process_zero()\n+        self.is_world_process_zero = trainer.is_world_process_zero()\n+\n \n class ExportableState:\n     \"\"\""
        },
        {
            "sha": "3e7cf6c49fdfb43cba771dd7d162edd3b53b9812",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d257111c19dcd97a0dafee9aca27fa257ffa297/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d257111c19dcd97a0dafee9aca27fa257ffa297/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=5d257111c19dcd97a0dafee9aca27fa257ffa297",
            "patch": "@@ -1390,3 +1390,17 @@ def get_lr(self):\n \n     def _get_closed_form_lr(self):\n         return self.base_lrs\n+\n+\n+def set_rng_state_for_device(device_name, device_module, checkpoint_rng_state, is_distributed):\n+    \"\"\"Helper to set RNG state for a specific device type (CUDA, NPU, MLU, MUSA)\"\"\"\n+    device_state_key = device_name.lower()\n+    err_template = \"Didn't manage to set back the RNG states of the {backend} because of the following error:\\n {exception}\\nThis won't yield the same results as if the training had not been interrupted.\"\n+    try:\n+        if is_distributed:\n+            device_module.random.set_rng_state_all(checkpoint_rng_state[device_state_key])\n+        else:\n+            device_module.random.set_rng_state(checkpoint_rng_state[device_state_key])\n+    except Exception as e:\n+        # Log error if setting RNG state fails\n+        logger.error(err_template.format(backend=device_name, exception=e))"
        }
    ],
    "stats": {
        "total": 279,
        "additions": 147,
        "deletions": 132
    }
}