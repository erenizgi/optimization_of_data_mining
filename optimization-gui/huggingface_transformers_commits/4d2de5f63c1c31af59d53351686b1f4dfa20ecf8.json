{
    "author": "damianoamatruda",
    "message": "Fix XGLM loss computation (PyTorch and TensorFlow) (#35878)\n\n* Fix XGLM loss computation (PyTorch and TensorFlow)\r\n\r\n* Update expected output string in XGLM sample test\r\n\r\nThis updates the expected output string of test_xglm_sample for torch\r\n2.0 to the correct one and removes the one for torch 1.13.1 + cu116\r\n(transformers moved to torch 2.0 with PR #35358).\r\n\r\n* Update expected output IDs in XGLM generation test",
    "sha": "4d2de5f63c1c31af59d53351686b1f4dfa20ecf8",
    "files": [
        {
            "sha": "dc9f82900a30e70d6f70aac41af779cc29eaf389",
            "filename": "src/transformers/models/xglm/modeling_tf_xglm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d2de5f63c1c31af59d53351686b1f4dfa20ecf8/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d2de5f63c1c31af59d53351686b1f4dfa20ecf8/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py?ref=4d2de5f63c1c31af59d53351686b1f4dfa20ecf8",
            "patch": "@@ -969,7 +969,7 @@ def call(\n         if labels is not None:\n             # shift labels to the left and cut last logit token\n             labels = tf.concat(\n-                [labels[:, 1:], tf.fill((labels.shape[0], 1), tf.cast(self.config.pad_token_id, labels.dtype))],\n+                [labels[:, 1:], tf.fill((labels.shape[0], 1), tf.cast(-100, labels.dtype))],\n                 axis=-1,\n             )\n             loss = self.hf_compute_loss(labels, lm_logits)"
        },
        {
            "sha": "0ca0fa77fc5f0a10744e51dfc37f531a30c41b00",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d2de5f63c1c31af59d53351686b1f4dfa20ecf8/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d2de5f63c1c31af59d53351686b1f4dfa20ecf8/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=4d2de5f63c1c31af59d53351686b1f4dfa20ecf8",
            "patch": "@@ -691,33 +691,6 @@ def forward(\n         )\n \n \n-def xglm_cross_entropy_loss(\n-    logits,\n-    labels,\n-    num_items_in_batch: int = None,\n-    ignore_index: int = -100,\n-    pad_token_id: int = -100,\n-    vocab_size: int = None,\n-):\n-    \"\"\"\n-    Loss function for XGLM that takes into account `num_items_in_batch`\n-    \"\"\"\n-    shift_labels = labels.new_zeros(labels.shape)\n-    shift_labels[:, :-1] = labels[:, 1:].clone()\n-    shift_labels[:, -1] = pad_token_id\n-    # move labels to correct device to enable model parallelism\n-    labels = labels.float().to(logits.device)\n-\n-    logits = logits.view(-1, vocab_size).float()\n-    shift_labels = shift_labels.view(-1)\n-\n-    reduction = \"sum\" if num_items_in_batch is not None else \"mean\"\n-    loss = nn.functional.cross_entropy(logits, shift_labels, ignore_index=ignore_index, reduction=reduction)\n-    if reduction == \"sum\":\n-        loss = loss / num_items_in_batch\n-    return loss\n-\n-\n @add_start_docstrings(\n     \"\"\"\n     The XGLM Model transformer with a language modeling head on top (linear layer with weights tied to the input\n@@ -737,8 +710,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-        self._loss_function = xglm_cross_entropy_loss\n-\n     def get_input_embeddings(self):\n         return self.model.embed_tokens\n "
        },
        {
            "sha": "88e961fca7f86849d626b35b49b8b3a58b44b68e",
            "filename": "tests/models/xglm/test_modeling_tf_xglm.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d2de5f63c1c31af59d53351686b1f4dfa20ecf8/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d2de5f63c1c31af59d53351686b1f4dfa20ecf8/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_tf_xglm.py?ref=4d2de5f63c1c31af59d53351686b1f4dfa20ecf8",
            "patch": "@@ -238,3 +238,22 @@ def test_batch_generation(self):\n         ]\n         self.assertListEqual(expected_output_sentence, batch_out_sentence)\n         self.assertListEqual(expected_output_sentence, [non_padded_sentence, padded_sentence])\n+\n+    def test_loss_with_padding(self):\n+        tokenizer = XGLMTokenizer.from_pretrained(\"facebook/xglm-564M\")\n+        model = TFXGLMForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n+\n+        tokenizer.padding_side = \"right\"\n+\n+        sequence = \"Sequence\"\n+\n+        tokenized_non_padded = tokenizer(sequence, return_tensors=\"tf\")\n+        labels_non_padded = tokenized_non_padded.input_ids\n+        loss_non_padded = model(tokenized_non_padded, labels=labels_non_padded).loss\n+\n+        tokenized_padded = tokenizer(sequence, padding=\"max_length\", max_length=16, return_tensors=\"tf\")\n+        labels_padded = tokenized_padded.input_ids\n+        labels_padded = tf.where(labels_padded == tokenizer.pad_token_id, -100, labels_padded)\n+        loss_padded = model(tokenized_padded, labels=labels_padded).loss\n+\n+        tf.debugging.assert_near(loss_non_padded, loss_padded, atol=1e-3)"
        },
        {
            "sha": "92210d0461536014688c176791ca7c0764ee2c24",
            "filename": "tests/models/xglm/test_modeling_xglm.py",
            "status": "modified",
            "additions": 28,
            "deletions": 9,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/4d2de5f63c1c31af59d53351686b1f4dfa20ecf8/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4d2de5f63c1c31af59d53351686b1f4dfa20ecf8/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fxglm%2Ftest_modeling_xglm.py?ref=4d2de5f63c1c31af59d53351686b1f4dfa20ecf8",
            "patch": "@@ -356,7 +356,7 @@ def _test_lm_generate_xglm_helper(\n         model.to(torch_device)\n         input_ids = torch.tensor([[2, 268, 9865]], dtype=torch.long, device=torch_device)  # The dog\n         # </s> The dog is a very friendly dog. He is very affectionate and loves to play with other\n-        expected_output_ids = [2, 268, 9865, 67, 11, 1988, 57252, 9865, 5, 984, 67, 1988, 213838, 1658, 53, 70446, 33, 6657, 278, 1581]  # fmt: skip\n+        expected_output_ids = [2, 268, 9865, 67, 11, 1988, 57252, 9865, 5, 984, 67, 1988, 213838, 1658, 53, 70446, 33, 6657, 278, 1581, 72616, 5, 984]  # fmt: skip\n         output_ids = model.generate(input_ids, do_sample=False, num_beams=1)\n         if verify_outputs:\n             self.assertListEqual(output_ids[0].tolist(), expected_output_ids)\n@@ -423,14 +423,11 @@ def test_xglm_sample(self):\n         output_ids = model.generate(input_ids, do_sample=True, num_beams=1)\n         output_str = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n \n-        EXPECTED_OUTPUT_STRS = [\n-            # TODO: remove this once we move to torch 2.0\n-            # torch 1.13.1 + cu116\n-            \"Today is a nice day and the sun is shining. A nice day with warm rainy\",\n-            # torch 2.0 + cu117\n-            \"Today is a nice day and the water is still cold. We just stopped off for some fresh\",\n-        ]\n-        self.assertIn(output_str, EXPECTED_OUTPUT_STRS)\n+        EXPECTED_OUTPUT_STR = (\n+            \"Today is a nice day and the water is still cold. We just stopped off for some fresh coffee. This place\"\n+            \" looks like a\"\n+        )\n+        self.assertEqual(output_str, EXPECTED_OUTPUT_STR)\n \n     @require_torch_accelerator\n     @require_torch_fp16\n@@ -451,3 +448,25 @@ def test_batched_nan_fp16(self):\n             self.assertFalse(\n                 torch.isnan(outputs.logits[0]).any().item()\n             )  # the first logits could contain NaNs if it fails\n+\n+    def test_loss_with_padding(self):\n+        tokenizer = XGLMTokenizer.from_pretrained(\"facebook/xglm-564M\")\n+        model = XGLMForCausalLM.from_pretrained(\"facebook/xglm-564M\")\n+        model.to(torch_device)\n+\n+        tokenizer.padding_side = \"right\"\n+\n+        sequence = \"Sequence\"\n+\n+        tokenized_non_padded = tokenizer(sequence, return_tensors=\"pt\")\n+        tokenized_non_padded.to(torch_device)\n+        labels_non_padded = tokenized_non_padded.input_ids.clone()\n+        loss_non_padded = model(**tokenized_non_padded, labels=labels_non_padded).loss\n+\n+        tokenized_padded = tokenizer(sequence, padding=\"max_length\", max_length=16, return_tensors=\"pt\")\n+        tokenized_padded.to(torch_device)\n+        labels_padded = tokenized_padded.input_ids.clone()\n+        labels_padded[labels_padded == tokenizer.pad_token_id] = -100\n+        loss_padded = model(**tokenized_padded, labels=labels_padded).loss\n+\n+        torch.testing.assert_close(loss_non_padded, loss_padded, rtol=1e-3, atol=1e-3)"
        }
    ],
    "stats": {
        "total": 87,
        "additions": 48,
        "deletions": 39
    }
}