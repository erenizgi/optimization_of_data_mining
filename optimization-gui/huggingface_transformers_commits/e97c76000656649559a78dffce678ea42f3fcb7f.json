{
    "author": "zucchini-nlp",
    "message": "[chat templates} support loading audio from video (#36955)\n\n* add audio from video\n\n* typos\n\n* delete print\n\n* comments",
    "sha": "e97c76000656649559a78dffce678ea42f3fcb7f",
    "files": [
        {
            "sha": "2b2d158179868176cf68f74414286bc543c3893c",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 64,
            "deletions": 54,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97c76000656649559a78dffce678ea42f3fcb7f/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97c76000656649559a78dffce678ea42f3fcb7f/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=e97c76000656649559a78dffce678ea42f3fcb7f",
            "patch": "@@ -23,7 +23,7 @@\n import typing\n import warnings\n from pathlib import Path\n-from typing import Any, Callable, Optional, TypedDict, Union\n+from typing import Any, Callable, Dict, List, Optional, TypedDict, Union\n \n import numpy as np\n import typing_extensions\n@@ -386,14 +386,10 @@ class TokenizerChatTemplateKwargs(TypedDict, total=False):\n     return_assistant_tokens_mask: Optional[bool] = False\n \n \n-class ProcessorChatTemplateKwargs(TokenizerChatTemplateKwargs, total=False):\n+class ChatTemplateLoadKwargs(TypedDict, total=False):\n     \"\"\"\n-    Keyword arguments for processor chat templates.\n+    Keyword arguments used to load multimodal data in processor chat templates.\n \n-    tokenize (`bool`, *optional*, defaults to `False`):\n-        Whether to tokenize the output or not.\n-    return_dict (`bool`, defaults to `False`):\n-        Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n     num_frames (`int`, *optional*):\n         Number of frames to sample uniformly. If not passed, the whole video is loaded.\n     video_load_backend (`str`, *optional*, defaults to `\"pyav\"`):\n@@ -415,13 +411,26 @@ def sample_indices_fn(num_frames, fps, metadata, **kwargs):\n                 return np.linspace(start_idx, end_idx, num_frames, dtype=int)\n     \"\"\"\n \n-    tokenize: Optional[bool] = False\n-    return_dict: Optional[bool] = False\n     num_frames: Optional[int] = None\n     video_load_backend: Optional[str] = \"pyav\"\n     video_fps: Optional[int] = None\n     sampling_rate: Optional[int] = 16_000\n     sample_indices_fn: Optional[Callable] = None\n+    load_audio_from_video: Optional[bool] = False\n+\n+\n+class ProcessorChatTemplateKwargs(ChatTemplateLoadKwargs, TokenizerChatTemplateKwargs, total=False):\n+    \"\"\"\n+    Keyword arguments for processor's `apply_chat_template`.\n+\n+    tokenize (`bool`, *optional*, defaults to `False`):\n+        Whether to tokenize the output or not.\n+    return_dict (`bool`, defaults to `False`):\n+        Whether to return a dictionary with named outputs. Has no effect if tokenize is `False`.\n+    \"\"\"\n+\n+    tokenize: Optional[bool] = False\n+    return_dict: Optional[bool] = False\n \n \n class AllKwargsForChatTemplate(\n@@ -1236,11 +1245,11 @@ def __call__(\n \n     def _process_messages_for_chat_template(\n         self,\n-        conversation: list[list[dict[str, str]]],\n-        batch_images: list[ImageInput],\n-        batch_videos: list[VideoInput],\n-        batch_video_metadata: list[list[dict[str, any]]],\n-        **chat_template_kwargs: Unpack[AllKwargsForChatTemplate],\n+        conversation: List[List[Dict[str, str]]],\n+        batch_images: List[ImageInput],\n+        batch_videos: List[VideoInput],\n+        batch_video_metadata: List[List[Dict[str, any]]],\n+        **mm_load_kwargs: Unpack[ChatTemplateLoadKwargs],\n     ):\n         \"\"\"\n         Used within `apply_chat_template` when a model has a special way to process conversation history. For example,\n@@ -1311,18 +1320,18 @@ def apply_chat_template(\n                 )\n \n         # Fill two sets of kwargs that should be used by tokenizer's `apply_chat_template`\n-        # and for multimodal chat template\n+        # and for multimodal data loading. Everything else will be used in `__call__`\n         tokenizer_template_kwargs = {}\n         for tokenizer_key in TokenizerChatTemplateKwargs.__annotations__.keys():\n-            tokenizer_value = getattr(TokenizerChatTemplateKwargs, tokenizer_key, None)\n-            value = kwargs.pop(tokenizer_key, tokenizer_value)\n+            default_value = getattr(TokenizerChatTemplateKwargs, tokenizer_key, None)\n+            value = kwargs.pop(tokenizer_key, default_value)\n             tokenizer_template_kwargs[tokenizer_key] = value\n \n-        chat_template_kwargs = {}\n-        for key in ProcessorChatTemplateKwargs.__annotations__.keys():\n-            processor_value = getattr(ProcessorChatTemplateKwargs, key, None)\n-            value = kwargs.pop(key, processor_value)\n-            chat_template_kwargs[key] = value\n+        mm_load_kwargs = {}\n+        for mm_load_key in ChatTemplateLoadKwargs.__annotations__.keys():\n+            default_value = getattr(ChatTemplateLoadKwargs, mm_load_key, None)\n+            value = kwargs.pop(mm_load_key, default_value)\n+            mm_load_kwargs[mm_load_key] = value\n \n         if isinstance(conversation, (list, tuple)) and (\n             isinstance(conversation[0], (list, tuple)) or hasattr(conversation[0], \"content\")\n@@ -1333,13 +1342,8 @@ def apply_chat_template(\n             is_batched = False\n             conversations = [conversation]\n \n-        num_frames = chat_template_kwargs.get(\"num_frames\")\n-        video_fps = chat_template_kwargs.get(\"video_fps\")\n-        video_load_backend = chat_template_kwargs.get(\"video_load_backend\")\n-        tokenize = chat_template_kwargs.get(\"tokenize\")\n-        return_dict = chat_template_kwargs.get(\"return_dict\")\n-        sample_indices_fn = chat_template_kwargs.get(\"sample_indices_fn\")\n-        sampling_rate = chat_template_kwargs.pop(\"sampling_rate\")\n+        tokenize = kwargs.pop(\"tokenize\", False)\n+        return_dict = kwargs.pop(\"return_dict\", False)\n \n         if tokenize:\n             batch_images, batch_videos = [], []\n@@ -1369,31 +1373,37 @@ def apply_chat_template(\n                         if key in vision_info and vision_info[\"type\"] == \"video\"\n                     ]\n \n-                    # Audio models do not accept nested list of audios (yet!)\n-                    for fname in audio_fnames:\n-                        batch_audios.append(load_audio(fname, sampling_rate=sampling_rate))\n                     for fname in image_fnames:\n                         images.append(load_image(fname))\n-                    for fname in video_fnames:\n-                        if isinstance(fname, (list, tuple)) and isinstance(fname[0], str):\n-                            video = [np.array(load_image(image_fname)).T for image_fname in fname]\n-                            # create a 4D video because `load_video` always returns a 4D array\n-                            video = np.stack(video)\n-                            metadata = None\n-                            logger.warning(\n-                                \"When loading the video from list of images, we cannot infer metadata such as `fps` or `duration`. \"\n-                                \"If you model applies special processing based on metadata, please load the whole video and let the model sample frames.\"\n-                            )\n-                        else:\n-                            video, metadata = load_video(\n-                                fname,\n-                                num_frames=num_frames,\n-                                fps=video_fps,\n-                                backend=video_load_backend,\n-                                sample_indices_fn=sample_indices_fn,\n-                            )\n-                        videos.append(video)\n-                        video_metadata.append(metadata)\n+\n+                    # Audio models do not accept nested list of audios (yet!) so we construct a flat input audio list\n+                    if not mm_load_kwargs[\"load_audio_from_video\"]:\n+                        for fname in audio_fnames:\n+                            batch_audios.append(load_audio(fname, sampling_rate=mm_load_kwargs[\"sampling_rate\"]))\n+                    else:\n+                        for fname in video_fnames:\n+                            if isinstance(fname, (list, tuple)) and isinstance(fname[0], str):\n+                                video = [np.array(load_image(image_fname)).T for image_fname in fname]\n+                                # create a 4D video because `load_video` always returns a 4D array\n+                                video = np.stack(video)\n+                                metadata = None\n+                                audios = None\n+                                logger.warning(\n+                                    \"When loading the video from list of images, we cannot infer metadata such as `fps` or `duration`. \"\n+                                    \"If your model uses this metadata during processing, please load the whole video and let the model sample frames instead.\"\n+                                )\n+                            else:\n+                                video, metadata = load_video(\n+                                    fname,\n+                                    num_frames=mm_load_kwargs[\"num_frames\"],\n+                                    fps=mm_load_kwargs[\"video_fps\"],\n+                                    backend=mm_load_kwargs[\"video_load_backend\"],\n+                                    sample_indices_fn=mm_load_kwargs[\"sample_indices_fn\"],\n+                                )\n+                                audios = load_audio(fname, sampling_rate=mm_load_kwargs[\"sampling_rate\"])\n+                            batch_audios.append(audios)\n+                            videos.append(video)\n+                            video_metadata.append(metadata)\n \n                 # Currently all processors can accept nested list of batches, but not flat list of visuals\n                 # So we'll make a batched list of images and let the processor handle it\n@@ -1409,7 +1419,7 @@ def apply_chat_template(\n                 batch_images=batch_images,\n                 batch_videos=batch_videos,\n                 batch_video_metadata=batch_video_metadata,\n-                **chat_template_kwargs,\n+                **mm_load_kwargs,\n             )\n \n         prompt = self.tokenizer.apply_chat_template(\n@@ -1438,7 +1448,7 @@ def apply_chat_template(\n                 text=prompt,\n                 images=batch_images if batch_images else None,\n                 videos=batch_videos if batch_videos else None,\n-                audios=batch_audios if batch_audios else None,\n+                audio=batch_audios if batch_audios else None,\n                 **kwargs,\n             )\n             if return_dict:"
        },
        {
            "sha": "f4e1e1b543fa25cb767e37f34fb41bf2dc3e8460",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 65,
            "deletions": 4,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/e97c76000656649559a78dffce678ea42f3fcb7f/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e97c76000656649559a78dffce678ea42f3fcb7f/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=e97c76000656649559a78dffce678ea42f3fcb7f",
            "patch": "@@ -1097,10 +1097,7 @@ def test_chat_template_video_custom_sampling(self):\n                 {\n                     \"role\": \"user\",\n                     \"content\": [\n-                        {\n-                            \"type\": \"video\",\n-                            \"path\": video_file_path,\n-                        },\n+                        {\"type\": \"video\", \"path\": video_file_path},\n                         {\"type\": \"text\", \"text\": \"What is shown in this video?\"},\n                     ],\n                 },\n@@ -1189,6 +1186,70 @@ def _process_messages_for_chat_template(\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name]), 1)\n         self.assertEqual(len(out_dict_with_video[self.videos_input_name][0]), 243)\n \n+    @require_librosa\n+    @require_av\n+    def test_audio_chat_template_from_video(self):\n+        processor = self.get_processor()\n+        if processor.chat_template is None:\n+            self.skipTest(\"Processor has no chat template\")\n+\n+        signature = inspect.signature(processor.__call__)\n+        if \"videos\" not in {*signature.parameters.keys()} or (\n+            signature.parameters.get(\"videos\") is not None\n+            and signature.parameters[\"videos\"].annotation == inspect._empty\n+        ):\n+            self.skipTest(f\"{self.processor_class} does not suport video inputs\")\n+\n+        if \"feature_extractor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"feature_extractor attribute not present in {self.processor_class}\")\n+\n+        video_file_path = hf_hub_download(\n+            repo_id=\"raushan-testing-hf/videos-test\", filename=\"sample_demo_1.mp4\", repo_type=\"dataset\"\n+        )\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"video\", \"path\": video_file_path},\n+                    {\"type\": \"text\", \"text\": \"Which of these animals is making the sound?\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": [{\"type\": \"text\", \"text\": \"It is a cow.\"}],\n+            },\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\n+                        \"type\": \"audio\",\n+                        \"url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\",\n+                    },\n+                    {\"type\": \"text\", \"text\": \"Is it the same sound?\"},\n+                ],\n+            },\n+        ]\n+\n+        formatted_prompt = processor.apply_chat_template([messages], add_generation_prompt=True, tokenize=False)\n+        self.assertEqual(len(formatted_prompt), 1)  # batch size=1\n+\n+        out_dict = processor.apply_chat_template(\n+            messages,\n+            add_generation_prompt=True,\n+            tokenize=True,\n+            return_dict=True,\n+            return_tensors=\"np\",\n+            load_audio_from_video=True,\n+        )\n+        self.assertTrue(self.audio_input_name in out_dict)\n+        self.assertTrue(self.video_input_name in out_dict)\n+\n+        # should always have input_ids and attention_mask\n+        self.assertEqual(len(out_dict[\"input_ids\"]), 1)  # batch-size=1\n+        self.assertEqual(len(out_dict[\"attention_mask\"]), 1)  # batch-size=1\n+        self.assertEqual(len(out_dict[self.audio_input_name]), 2)  # 2 audios in the conversation\n+        self.assertEqual(len(out_dict[self.video_input_name]), 1)  # 1 video in the conversation\n+\n     @require_librosa\n     def test_audio_chat_template_single(self):\n         processor = self.get_processor()"
        }
    ],
    "stats": {
        "total": 187,
        "additions": 129,
        "deletions": 58
    }
}