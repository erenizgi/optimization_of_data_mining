{
    "author": "yonigozlan",
    "message": "add Qwen2-VL image processor fast (#35733)\n\n* add qwen2_vl image processor fast\r\n\r\n* add device to ImagesKwargs\r\n\r\n* remove automatic fix copies\r\n\r\n* fix fast_is_faster_than_slow\r\n\r\n* remove unnecessary import",
    "sha": "107f9f51271937589f25f59dcfab8a5aebb6dbe3",
    "files": [
        {
            "sha": "c39728ef71eceece37bb904dfb643423a3fcc1e1",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/107f9f51271937589f25f59dcfab8a5aebb6dbe3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/107f9f51271937589f25f59dcfab8a5aebb6dbe3/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=107f9f51271937589f25f59dcfab8a5aebb6dbe3",
            "patch": "@@ -315,6 +315,11 @@ model = Qwen2VLForConditionalGeneration.from_pretrained(\n [[autodoc]] Qwen2VLImageProcessor\n     - preprocess\n \n+## Qwen2VLImageProcessorFast\n+\n+[[autodoc]] Qwen2VLImageProcessorFast\n+    - preprocess\n+\n ## Qwen2VLProcessor\n \n [[autodoc]] Qwen2VLProcessor"
        },
        {
            "sha": "c8c7092384272565d5ba7a45df01753369341663",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=107f9f51271937589f25f59dcfab8a5aebb6dbe3",
            "patch": "@@ -1299,6 +1299,7 @@\n     _import_structure[\"models.deformable_detr\"].append(\"DeformableDetrImageProcessorFast\")\n     _import_structure[\"models.detr\"].append(\"DetrImageProcessorFast\")\n     _import_structure[\"models.pixtral\"].append(\"PixtralImageProcessorFast\")\n+    _import_structure[\"models.qwen2_vl\"].append(\"Qwen2VLImageProcessorFast\")\n     _import_structure[\"models.rt_detr\"].append(\"RTDetrImageProcessorFast\")\n     _import_structure[\"models.vit\"].append(\"ViTImageProcessorFast\")\n \n@@ -6397,6 +6398,7 @@\n         from .models.deformable_detr import DeformableDetrImageProcessorFast\n         from .models.detr import DetrImageProcessorFast\n         from .models.pixtral import PixtralImageProcessorFast\n+        from .models.qwen2_vl import Qwen2VLImageProcessorFast\n         from .models.rt_detr import RTDetrImageProcessorFast\n         from .models.vit import ViTImageProcessorFast\n "
        },
        {
            "sha": "1cb067f386f156663a2c87ed98d2984350c89b44",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=107f9f51271937589f25f59dcfab8a5aebb6dbe3",
            "patch": "@@ -125,7 +125,7 @@\n             (\"poolformer\", (\"PoolFormerImageProcessor\",)),\n             (\"pvt\", (\"PvtImageProcessor\",)),\n             (\"pvt_v2\", (\"PvtImageProcessor\",)),\n-            (\"qwen2_vl\", (\"Qwen2VLImageProcessor\",)),\n+            (\"qwen2_vl\", (\"Qwen2VLImageProcessor\", \"Qwen2VLImageProcessorFast\")),\n             (\"regnet\", (\"ConvNextImageProcessor\",)),\n             (\"resnet\", (\"ConvNextImageProcessor\",)),\n             (\"rt_detr\", (\"RTDetrImageProcessor\", \"RTDetrImageProcessorFast\")),"
        },
        {
            "sha": "70a719cc3a2f73f2f677ab999195f05e04f35f4b",
            "filename": "src/transformers/models/qwen2_vl/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2F__init__.py?ref=107f9f51271937589f25f59dcfab8a5aebb6dbe3",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_qwen2_vl import *\n     from .image_processing_qwen2_vl import *\n+    from .image_processing_qwen2_vl_fast import *\n     from .modeling_qwen2_vl import *\n     from .processing_qwen2_vl import *\n else:"
        },
        {
            "sha": "a08b838facc6013b00bb8b7d80abe9b853671c9f",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "added",
            "additions": 422,
            "deletions": 0,
            "changes": 422,
            "blob_url": "https://github.com/huggingface/transformers/blob/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=107f9f51271937589f25f59dcfab8a5aebb6dbe3",
            "patch": "@@ -0,0 +1,422 @@\n+# coding=utf-8\n+# Copyright 2025 The Qwen team, Alibaba Group and the HuggingFace Inc. team. All rights reserved.\n+#\n+# This code is based on EleutherAI's GPT-NeoX library and the GPT-NeoX\n+# and OPT implementations in this library. It has been modified from its\n+# original forms to accommodate minor architectural differences compared\n+# to GPT-NeoX and OPT used by the Meta AI team that trained the model.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Qwen2-VL.\"\"\"\n+\n+from typing import Dict, List, Optional, Union\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n+)\n+from ...image_transforms import (\n+    convert_to_rgb,\n+)\n+from ...image_utils import (\n+    OPENAI_CLIP_MEAN,\n+    OPENAI_CLIP_STD,\n+    ChannelDimension,\n+    ImageInput,\n+    ImageType,\n+    PILImageResampling,\n+    VideoInput,\n+    get_image_size,\n+    get_image_type,\n+    infer_channel_dimension_format,\n+    make_list_of_images,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import (\n+    TensorType,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    is_vision_available,\n+    logging,\n+)\n+from .image_processing_qwen2_vl import make_batched_images, make_batched_videos, smart_resize\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from ...image_utils import pil_torch_interpolation_mapping\n+\n+\n+if is_torchvision_v2_available():\n+    from torchvision.transforms.v2 import functional as F\n+elif is_torchvision_available():\n+    from torchvision.transforms import functional as F\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Qwen2VLImageProcessorFast(BaseImageProcessorFast):\n+    r\"\"\"\n+    Constructs a fast Qwen2-VL image processor that dynamically resizes images based on the original images.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's (height, width) dimensions.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BICUBIC`):\n+            Resampling filter to use when resizing the image.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.48145466, 0.4578275, 0.40821073]`):\n+            Mean to use if normalizing the image. This is a float or list of floats for each channel in the image.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `[0.26862954, 0.26130258, 0.27577711]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats for each channel in the image.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+        min_pixels (`int`, *optional*, defaults to `56 * 56`):\n+            The min pixels of the image to resize the image.\n+        max_pixels (`int`, *optional*, defaults to `28 * 28 * 1280`):\n+            The max pixels of the image to resize the image.\n+        patch_size (`int`, *optional*, defaults to 14):\n+            The spacial patch size of the vision encoder.\n+        temporal_patch_size (`int`, *optional*, defaults to 2):\n+            The temporal patch size of the vision encoder.\n+        merge_size (`int`, *optional*, defaults to 2):\n+            The merge size of the vision encoder to llm encoder.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\", \"image_grid_thw\", \"pixel_values_videos\", \"video_grid_thw\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        resample: PILImageResampling = PILImageResampling.BICUBIC,\n+        do_rescale: bool = True,\n+        rescale_factor: Union[int, float] = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = True,\n+        min_pixels: int = 56 * 56,\n+        max_pixels: int = 28 * 28 * 1280,\n+        patch_size: int = 14,\n+        temporal_patch_size: int = 2,\n+        merge_size: int = 2,\n+        **kwargs,\n+    ) -> None:\n+        super().__init__(**kwargs)\n+        self.do_resize = do_resize\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean if image_mean is not None else OPENAI_CLIP_MEAN\n+        self.image_std = image_std if image_std is not None else OPENAI_CLIP_STD\n+        self.min_pixels = min_pixels\n+        self.max_pixels = max_pixels\n+        self.patch_size = patch_size\n+        self.temporal_patch_size = temporal_patch_size\n+        self.merge_size = merge_size\n+        self.size = {\"min_pixels\": min_pixels, \"max_pixels\": max_pixels}\n+        self.do_convert_rgb = do_convert_rgb\n+\n+    def _preprocess(\n+        self,\n+        images: Union[ImageInput, VideoInput],\n+        do_resize: bool = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        device: Optional[Union[str, torch.device]] = None,\n+    ):\n+        \"\"\"\n+        Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image or batch of images to preprocess. Expects pixel values ranging from 0 to 255. If pixel values range from 0 to 1, set `do_rescale=False`.\n+            vision_info (`List[Dict]`, *optional*):\n+                Optional list of dictionaries containing additional information about vision inputs.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            resample (`PILImageResampling`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the `PILImageResampling` enums.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Scale factor to use if rescaling the image.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            data_format (`ChannelDimension`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+        \"\"\"\n+        images = make_list_of_images(images)\n+\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+        image_type = get_image_type(images[0])\n+        if image_type == ImageType.PIL:\n+            images = [F.pil_to_tensor(image) for image in images]\n+        elif image_type == ImageType.NUMPY:\n+            # not using F.to_tensor as it doesn't handle (C, H, W) numpy arrays\n+            images = [torch.from_numpy(image).contiguous() for image in images]\n+\n+        if device is not None:\n+            images = [image.to(device) for image in images]\n+\n+        # We assume that all images have the same channel dimension format.\n+        if input_data_format is None:\n+            input_data_format = infer_channel_dimension_format(images[0])\n+        if input_data_format == ChannelDimension.LAST:\n+            images = [image.permute(2, 0, 1).contiguous() for image in images]\n+            input_data_format = ChannelDimension.FIRST\n+\n+        if do_rescale and do_normalize:\n+            # fused rescale and normalize\n+            image_mean = torch.tensor(image_mean, device=images[0].device) * (1.0 / rescale_factor)\n+            image_std = torch.tensor(image_std, device=images[0].device) * (1.0 / rescale_factor)\n+\n+        height, width = get_image_size(images[0], channel_dim=input_data_format)\n+        interpolation = (\n+            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n+        )\n+        resized_height, resized_width = height, width\n+        processed_images = []\n+        for image in images:\n+            if do_resize:\n+                resized_height, resized_width = smart_resize(\n+                    height,\n+                    width,\n+                    factor=self.patch_size * self.merge_size,\n+                    min_pixels=self.min_pixels,\n+                    max_pixels=self.max_pixels,\n+                )\n+                image = F.resize(image, size=(resized_height, resized_width), interpolation=interpolation)\n+\n+            if do_rescale and do_normalize:\n+                # fused rescale and normalize\n+                image = F.normalize(image.to(dtype=torch.float32), image_mean, image_std)\n+            elif do_rescale:\n+                image = image * rescale_factor\n+            elif do_normalize:\n+                image = F.normalize(image, image_mean, image_std)\n+\n+            processed_images.append(image)\n+\n+        patches = torch.stack(processed_images)\n+        if patches.shape[0] % self.temporal_patch_size != 0:\n+            repeats = patches[-1].unsqueeze(0).repeat(self.temporal_patch_size - 1, 1, 1, 1)\n+            patches = torch.cat([patches, repeats], dim=0)\n+\n+        channel = patches.shape[1]\n+        grid_t = patches.shape[0] // self.temporal_patch_size\n+        grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size\n+\n+        patches = patches.view(\n+            grid_t,\n+            self.temporal_patch_size,\n+            channel,\n+            grid_h // self.merge_size,\n+            self.merge_size,\n+            self.patch_size,\n+            grid_w // self.merge_size,\n+            self.merge_size,\n+            self.patch_size,\n+        )\n+        patches = patches.permute(0, 3, 6, 4, 7, 2, 1, 5, 8)\n+        flatten_patches = patches.reshape(\n+            grid_t * grid_h * grid_w, channel * self.temporal_patch_size * self.patch_size * self.patch_size\n+        )\n+\n+        return flatten_patches, (grid_t, grid_h, grid_w)\n+\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        videos: VideoInput = None,\n+        do_resize: bool = None,\n+        size: Dict[str, int] = None,\n+        resample: PILImageResampling = None,\n+        do_rescale: bool = None,\n+        rescale_factor: float = None,\n+        do_normalize: bool = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: bool = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        **kwargs,\n+    ):\n+        \"\"\"\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            videos (`VideoInput`):\n+                Video to preprocess. Expects a single or batch of videos with pixel values ranging from 0 to 255. If\n+                passing in videos with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing. Shortest edge of the image is resized to size[\"shortest_edge\"], with\n+                the longest edge resized to keep the input aspect ratio.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            data_format (`ChannelDimension` or `str`, *optional*, defaults to `ChannelDimension.FIRST`):\n+                The channel dimension format for the output image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - Unset: Use the channel dimension format of the input image.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        size = size if size is not None else self.size\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        device = kwargs.pop(\"device\", None)\n+\n+        # Make hashable for cache\n+        image_mean = tuple(image_mean) if isinstance(image_mean, list) else image_mean\n+        image_std = tuple(image_std) if isinstance(image_std, list) else image_std\n+\n+        if images is not None:\n+            images = make_batched_images(images)\n+        if videos is not None:\n+            videos = make_batched_videos(videos)\n+\n+        if images is not None and not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+\n+        validate_preprocess_arguments(\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            do_resize=do_resize,\n+            size=size,\n+            resample=resample,\n+        )\n+\n+        if images is not None:\n+            pixel_values, vision_grid_thws = [], []\n+            for image in images:\n+                patches, image_grid_thw = self._preprocess(\n+                    image,\n+                    do_resize=do_resize,\n+                    resample=resample,\n+                    do_rescale=do_rescale,\n+                    rescale_factor=rescale_factor,\n+                    do_normalize=do_normalize,\n+                    image_mean=image_mean,\n+                    image_std=image_std,\n+                    data_format=data_format,\n+                    do_convert_rgb=do_convert_rgb,\n+                    input_data_format=input_data_format,\n+                    device=device,\n+                )\n+                pixel_values.extend(patches)\n+                vision_grid_thws.append(image_grid_thw)\n+            pixel_values = torch.stack(pixel_values)\n+            vision_grid_thws = torch.tensor(vision_grid_thws)\n+            data = {\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws}\n+\n+        if videos is not None:\n+            pixel_values, vision_grid_thws = [], []\n+            for images in videos:\n+                patches, video_grid_thw = self._preprocess(\n+                    images,\n+                    do_resize=do_resize,\n+                    resample=resample,\n+                    do_rescale=do_rescale,\n+                    rescale_factor=rescale_factor,\n+                    do_normalize=do_normalize,\n+                    image_mean=image_mean,\n+                    image_std=image_std,\n+                    data_format=data_format,\n+                    do_convert_rgb=do_convert_rgb,\n+                    input_data_format=input_data_format,\n+                    device=device,\n+                )\n+                pixel_values.extend(patches)\n+                vision_grid_thws.append(video_grid_thw)\n+            pixel_values = torch.stack(pixel_values)\n+            vision_grid_thws = torch.tensor(vision_grid_thws)\n+            data = {\"pixel_values_videos\": pixel_values, \"video_grid_thw\": vision_grid_thws}\n+\n+        return BatchFeature(data=data, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"Qwen2VLImageProcessorFast\"]"
        },
        {
            "sha": "b94230c7d4a1dac99fcd79563a247268ba6ea8e4",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=107f9f51271937589f25f59dcfab8a5aebb6dbe3",
            "patch": "@@ -171,6 +171,8 @@ class methods and docstrings.\n             The channel dimension format for the output image.\n         input_data_format (`ChannelDimension` or `str`, *optional*):\n             The channel dimension format for the input image.\n+        device (`str`, *optional*):\n+            The device to use for processing (e.g. \"cpu\", \"cuda\"), only relevant for fast image processing.\n     \"\"\"\n \n     do_resize: Optional[bool]\n@@ -188,6 +190,7 @@ class methods and docstrings.\n     do_center_crop: Optional[bool]\n     data_format: Optional[ChannelDimension]\n     input_data_format: Optional[Union[str, ChannelDimension]]\n+    device: Optional[str]\n \n \n class VideosKwargs(TypedDict, total=False):"
        },
        {
            "sha": "86c997ea7a138fd53b2e15778c169d531e2b9dc5",
            "filename": "src/transformers/utils/dummy_torchvision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/107f9f51271937589f25f59dcfab8a5aebb6dbe3/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py?ref=107f9f51271937589f25f59dcfab8a5aebb6dbe3",
            "patch": "@@ -30,6 +30,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torchvision\"])\n \n \n+class Qwen2VLImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"torchvision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torchvision\"])\n+\n+\n class RTDetrImageProcessorFast(metaclass=DummyObject):\n     _backends = [\"torchvision\"]\n "
        },
        {
            "sha": "317e0e28ad1479695c84039665a78442754c9001",
            "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 149,
            "deletions": 137,
            "changes": 286,
            "blob_url": "https://github.com/huggingface/transformers/blob/107f9f51271937589f25f59dcfab8a5aebb6dbe3/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/107f9f51271937589f25f59dcfab8a5aebb6dbe3/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py?ref=107f9f51271937589f25f59dcfab8a5aebb6dbe3",
            "patch": "@@ -20,7 +20,7 @@\n from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs, prepare_video_inputs\n \n@@ -33,6 +33,9 @@\n \n     from transformers import Qwen2VLImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import Qwen2VLImageProcessorFast\n+\n \n class Qwen2VLImageProcessingTester:\n     def __init__(\n@@ -114,6 +117,7 @@ def prepare_video_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class Qwen2VLImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = Qwen2VLImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = Qwen2VLImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -124,163 +128,171 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n-        self.assertTrue(hasattr(image_processing, \"image_mean\"))\n-        self.assertTrue(hasattr(image_processing, \"image_std\"))\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"min_pixels\"))\n-        self.assertTrue(hasattr(image_processing, \"max_pixels\"))\n-        self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n-        self.assertTrue(hasattr(image_processing, \"patch_size\"))\n-        self.assertTrue(hasattr(image_processing, \"temporal_patch_size\"))\n-        self.assertTrue(hasattr(image_processing, \"merge_size\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"min_pixels\"))\n+            self.assertTrue(hasattr(image_processing, \"max_pixels\"))\n+            self.assertTrue(hasattr(image_processing, \"do_convert_rgb\"))\n+            self.assertTrue(hasattr(image_processing, \"patch_size\"))\n+            self.assertTrue(hasattr(image_processing, \"temporal_patch_size\"))\n+            self.assertTrue(hasattr(image_processing, \"merge_size\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.min_pixels, 56 * 56)\n-        self.assertEqual(image_processor.max_pixels, 28 * 28 * 1280)\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.min_pixels, 56 * 56)\n+            self.assertEqual(image_processor.max_pixels, 28 * 28 * 1280)\n \n-        image_processor = self.image_processing_class.from_dict(\n-            self.image_processor_dict, min_pixels=256 * 256, max_pixels=640 * 640\n-        )\n-        self.assertEqual(image_processor.min_pixels, 256 * 256)\n-        self.assertEqual(image_processor.max_pixels, 640 * 640)\n+            image_processor = image_processing_class.from_dict(\n+                self.image_processor_dict, min_pixels=256 * 256, max_pixels=640 * 640\n+            )\n+            self.assertEqual(image_processor.min_pixels, 256 * 256)\n+            self.assertEqual(image_processor.max_pixels, 640 * 640)\n \n     def test_select_best_resolution(self):\n         # Test with a final resize resolution\n         best_resolution = smart_resize(561, 278, factor=28)\n         self.assertEqual(best_resolution, (560, 280))\n \n     def test_call_pil(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PIL images\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image[0], Image.Image)\n-\n-        # Test not batched input\n-        prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (4900, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-        # Test batched\n-        prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (34300, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PIL images\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image[0], Image.Image)\n+\n+            # Test not batched input\n+            prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            encoded_images = prcocess_out.pixel_values\n+            image_grid_thws = prcocess_out.image_grid_thw\n+            expected_output_image_shape = (4900, 1176)\n+            expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+            # Test batched\n+            prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = prcocess_out.pixel_values\n+            image_grid_thws = prcocess_out.image_grid_thw\n+            expected_output_image_shape = (34300, 1176)\n+            expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n \n     def test_call_numpy(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random numpy tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n-        for image in image_inputs:\n-            self.assertIsInstance(image[0], np.ndarray)\n-\n-        # Test not batched input\n-        prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (4900, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-        # Test batched\n-        prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (34300, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random numpy tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, numpify=True)\n+            for image in image_inputs:\n+                self.assertIsInstance(image[0], np.ndarray)\n+\n+            # Test not batched input\n+            prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            encoded_images = prcocess_out.pixel_values\n+            image_grid_thws = prcocess_out.image_grid_thw\n+            expected_output_image_shape = (4900, 1176)\n+            expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+            # Test batched\n+            prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = prcocess_out.pixel_values\n+            image_grid_thws = prcocess_out.image_grid_thw\n+            expected_output_image_shape = (34300, 1176)\n+            expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n \n     def test_call_pytorch(self):\n-        # Initialize image_processing\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        # create random PyTorch tensors\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n-\n-        for image in image_inputs:\n-            self.assertIsInstance(image[0], torch.Tensor)\n-\n-        # Test not batched input\n-        prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (4900, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-        # Test batched\n-        prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (34300, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+        for image_processing_class in self.image_processor_list:\n+            # Initialize image_processing\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            # create random PyTorch tensors\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+\n+            for image in image_inputs:\n+                self.assertIsInstance(image[0], torch.Tensor)\n+\n+            # Test not batched input\n+            prcocess_out = image_processing(image_inputs[0], return_tensors=\"pt\")\n+            encoded_images = prcocess_out.pixel_values\n+            image_grid_thws = prcocess_out.image_grid_thw\n+            expected_output_image_shape = (4900, 1176)\n+            expected_image_grid_thws = torch.Tensor([[1, 70, 70]])\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+            # Test batched\n+            prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = prcocess_out.pixel_values\n+            image_grid_thws = prcocess_out.image_grid_thw\n+            expected_output_image_shape = (34300, 1176)\n+            expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n \n     @unittest.skip(reason=\"Qwen2VLImageProcessor doesn't treat 4 channel PIL and numpy consistently yet\")\n     def test_call_numpy_4_channels(self):\n         pass\n \n     def test_nested_input(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n-\n-        # Test batched as a list of images\n-        prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n-        encoded_images = prcocess_out.pixel_values\n-        image_grid_thws = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (34300, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n-        self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-        # Test batched as a nested list of images, where each sublist is one batch\n-        image_inputs_nested = image_inputs[:3] + image_inputs[3:]\n-        prcocess_out = image_processing(image_inputs_nested, return_tensors=\"pt\")\n-        encoded_images_nested = prcocess_out.pixel_values\n-        image_grid_thws_nested = prcocess_out.image_grid_thw\n-        expected_output_image_shape = (34300, 1176)\n-        expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n-        self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n-        self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n-\n-        # Image processor should return same pixel values, independently of ipnut format\n-        self.assertTrue((encoded_images_nested == encoded_images).all())\n-        self.assertTrue((image_grid_thws_nested == expected_image_grid_thws).all())\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True)\n+\n+            # Test batched as a list of images\n+            prcocess_out = image_processing(image_inputs, return_tensors=\"pt\")\n+            encoded_images = prcocess_out.pixel_values\n+            image_grid_thws = prcocess_out.image_grid_thw\n+            expected_output_image_shape = (34300, 1176)\n+            expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n+            self.assertEqual(tuple(encoded_images.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+            # Test batched as a nested list of images, where each sublist is one batch\n+            image_inputs_nested = image_inputs[:3] + image_inputs[3:]\n+            prcocess_out = image_processing(image_inputs_nested, return_tensors=\"pt\")\n+            encoded_images_nested = prcocess_out.pixel_values\n+            image_grid_thws_nested = prcocess_out.image_grid_thw\n+            expected_output_image_shape = (34300, 1176)\n+            expected_image_grid_thws = torch.Tensor([[1, 70, 70]] * 7)\n+            self.assertEqual(tuple(encoded_images_nested.shape), expected_output_image_shape)\n+            self.assertTrue((image_grid_thws == expected_image_grid_thws).all())\n+\n+            # Image processor should return same pixel values, independently of ipnut format\n+            self.assertTrue((encoded_images_nested == encoded_images).all())\n+            self.assertTrue((image_grid_thws_nested == expected_image_grid_thws).all())\n \n     def test_video_inputs(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        expected_dims_by_frames = {1: 34300, 2: 34300, 3: 68600, 4: 68600, 5: 102900, 6: 102900}\n-\n-        for num_frames, expected_dims in expected_dims_by_frames.items():\n-            image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=num_frames)\n-            video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-            prcocess_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-            encoded_video = prcocess_out.pixel_values_videos\n-            expected_output_video_shape = (expected_dims, 1176)\n-            self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            expected_dims_by_frames = {1: 34300, 2: 34300, 3: 68600, 4: 68600, 5: 102900, 6: 102900}\n+\n+            for num_frames, expected_dims in expected_dims_by_frames.items():\n+                image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=num_frames)\n+                video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n+                prcocess_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n+                encoded_video = prcocess_out.pixel_values_videos\n+                expected_output_video_shape = (expected_dims, 1176)\n+                self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n \n     def test_custom_patch_size(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-\n-        for patch_size in (1, 3, 5, 7):\n-            image_processor_tester = Qwen2VLImageProcessingTester(self, patch_size=patch_size)\n-            video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n-            prcocess_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n-            encoded_video = prcocess_out.pixel_values_videos\n-            expected_output_video_shape = (171500, 1176)\n-            self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+\n+            for patch_size in (1, 3, 5, 7):\n+                image_processor_tester = Qwen2VLImageProcessingTester(self, patch_size=patch_size)\n+                video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n+                prcocess_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n+                encoded_video = prcocess_out.pixel_values_videos\n+                expected_output_video_shape = (171500, 1176)\n+                self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)"
        },
        {
            "sha": "1f2d1d0fe7e170fdde821458f82d0be9f66de4b8",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/107f9f51271937589f25f59dcfab8a5aebb6dbe3/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/107f9f51271937589f25f59dcfab8a5aebb6dbe3/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=107f9f51271937589f25f59dcfab8a5aebb6dbe3",
            "patch": "@@ -181,7 +181,10 @@ def test_slow_fast_equivalence(self):\n         encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n         encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n \n-        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-2))\n+        self.assertTrue(torch.allclose(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1))\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 1e-3\n+        )\n \n     @require_vision\n     @require_torch\n@@ -193,6 +196,8 @@ def test_fast_is_faster_than_slow(self):\n             self.skipTest(reason=\"Skipping speed test as one of the image processors is not defined\")\n \n         def measure_time(image_processor, image):\n+            # Warmup\n+            _ = image_processor(image, return_tensors=\"pt\")\n             start = time.time()\n             _ = image_processor(image, return_tensors=\"pt\")\n             return time.time() - start"
        }
    ],
    "stats": {
        "total": 735,
        "additions": 596,
        "deletions": 139
    }
}