{
    "author": "jla524",
    "message": "Fix Qwen2VL processor to handle odd number of frames (#35431)\n\n* fix: processing odd number of frames\n\n* feat: add test case\n\n* update: test one frame\n\n* feat: support custom patch size\n\n* fix: test with videos\n\n* revert: change on patch repeat\n\n* fix: much wow\n\n* update: fixups\n\n* fixup pls\n\n* ruff fixup\n\n* fix typo at least",
    "sha": "3c1895aa65a8ee998cb5784b06603d84d807f0f7",
    "files": [
        {
            "sha": "b8656a910318206efe5941078e706f2ed3f36721",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c1895aa65a8ee998cb5784b06603d84d807f0f7/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c1895aa65a8ee998cb5784b06603d84d807f0f7/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=3c1895aa65a8ee998cb5784b06603d84d807f0f7",
            "patch": "@@ -291,8 +291,9 @@ def _preprocess(\n         patches = np.array(processed_images)\n         if data_format == ChannelDimension.LAST:\n             patches = patches.transpose(0, 3, 1, 2)\n-        if patches.shape[0] == 1:\n-            patches = np.tile(patches, (self.temporal_patch_size, 1, 1, 1))\n+        if patches.shape[0] % self.temporal_patch_size != 0:\n+            repeats = np.repeat(patches[-1][np.newaxis], self.temporal_patch_size - 1, axis=0)\n+            patches = np.concatenate([patches, repeats], axis=0)\n         channel = patches.shape[1]\n         grid_t = patches.shape[0] // self.temporal_patch_size\n         grid_h, grid_w = resized_height // self.patch_size, resized_width // self.patch_size"
        },
        {
            "sha": "76220dc66e960a3532501df2dd5eb015fff68ec2",
            "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 38,
            "deletions": 1,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c1895aa65a8ee998cb5784b06603d84d807f0f7/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c1895aa65a8ee998cb5784b06603d84d807f0f7/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py?ref=3c1895aa65a8ee998cb5784b06603d84d807f0f7",
            "patch": "@@ -22,7 +22,7 @@\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n-from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs, prepare_video_inputs\n \n \n if is_torch_available():\n@@ -40,6 +40,7 @@ def __init__(\n         parent,\n         batch_size=7,\n         num_channels=3,\n+        num_frames=10,\n         min_resolution=56,\n         max_resolution=1024,\n         min_pixels=56 * 56,\n@@ -58,6 +59,7 @@ def __init__(\n         self.min_resolution = min_resolution\n         self.max_resolution = max_resolution\n         self.num_channels = num_channels\n+        self.num_frames = num_frames\n         self.image_mean = OPENAI_CLIP_MEAN\n         self.image_std = OPENAI_CLIP_STD\n         self.min_pixels = min_pixels\n@@ -95,6 +97,18 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n         )\n         return [[image] for image in images]\n \n+    def prepare_video_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_video_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            num_frames=self.num_frames,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n \n @require_torch\n @require_vision\n@@ -247,3 +261,26 @@ def test_nested_input(self):\n         # Image processor should return same pixel values, independently of ipnut format\n         self.assertTrue((encoded_images_nested == encoded_images).all())\n         self.assertTrue((image_grid_thws_nested == expected_image_grid_thws).all())\n+\n+    def test_video_inputs(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+        expected_dims_by_frames = {1: 34300, 2: 34300, 3: 68600, 4: 68600, 5: 102900, 6: 102900}\n+\n+        for num_frames, expected_dims in expected_dims_by_frames.items():\n+            image_processor_tester = Qwen2VLImageProcessingTester(self, num_frames=num_frames)\n+            video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n+            prcocess_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n+            encoded_video = prcocess_out.pixel_values_videos\n+            expected_output_video_shape = (expected_dims, 1176)\n+            self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)\n+\n+    def test_custom_patch_size(self):\n+        image_processing = self.image_processing_class(**self.image_processor_dict)\n+\n+        for patch_size in (1, 3, 5, 7):\n+            image_processor_tester = Qwen2VLImageProcessingTester(self, patch_size=patch_size)\n+            video_inputs = image_processor_tester.prepare_video_inputs(equal_resolution=True)\n+            prcocess_out = image_processing(None, videos=video_inputs, return_tensors=\"pt\")\n+            encoded_video = prcocess_out.pixel_values_videos\n+            expected_output_video_shape = (171500, 1176)\n+            self.assertEqual(tuple(encoded_video.shape), expected_output_video_shape)"
        },
        {
            "sha": "aedd37992632a5c3659df3158b9a2debef2d11f7",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c1895aa65a8ee998cb5784b06603d84d807f0f7/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c1895aa65a8ee998cb5784b06603d84d807f0f7/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=3c1895aa65a8ee998cb5784b06603d84d807f0f7",
            "patch": "@@ -253,7 +253,7 @@ def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong\n         when number of images don't match number of image tokens in the text.\n-        Also we need to test multi-image cases when one prompr has multiple image tokens.\n+        Also we need to test multi-image cases when one prompt has multiple image tokens.\n         \"\"\"\n         config, input_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         for model_class in self.all_model_classes:"
        },
        {
            "sha": "971462f9e3528e64a95a71d9d505bc3153eed140",
            "filename": "tests/test_image_processing_common.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/3c1895aa65a8ee998cb5784b06603d84d807f0f7/tests%2Ftest_image_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3c1895aa65a8ee998cb5784b06603d84d807f0f7/tests%2Ftest_image_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_image_processing_common.py?ref=3c1895aa65a8ee998cb5784b06603d84d807f0f7",
            "patch": "@@ -125,19 +125,19 @@ def prepare_video_inputs(\n     assert not (numpify and torchify), \"You cannot specify both numpy and PyTorch tensors at the same time\"\n \n     video_inputs = []\n-    for i in range(batch_size):\n+    for _ in range(batch_size):\n         if equal_resolution:\n             width = height = max_resolution\n         else:\n             width, height = np.random.choice(np.arange(min_resolution, max_resolution), 2)\n-            video = prepare_video(\n-                num_frames=num_frames,\n-                num_channels=num_channels,\n-                width=width,\n-                height=height,\n-                numpify=numpify,\n-                torchify=torchify,\n-            )\n+        video = prepare_video(\n+            num_frames=num_frames,\n+            num_channels=num_channels,\n+            width=width,\n+            height=height,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n         video_inputs.append(video)\n \n     return video_inputs"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 51,
        "deletions": 13
    }
}