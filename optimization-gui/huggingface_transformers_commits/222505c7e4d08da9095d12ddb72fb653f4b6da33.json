{
    "author": "EduardoPach",
    "message": "[GroundingDino] Fix grounding dino loss ðŸš¨ (#31828)\n\n* Starting to fix GroundingDinoLoss and GroundingDinoHungarianMatcher\n\n* More updates\n\n* More updates\n\n* fixed: GroundingDinoLoss\n\n* fixed: failing tests\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update tests/models/grounding_dino/test_modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\n\n* Addressed comments\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\nCo-authored-by: Sangbum Daniel Choi <34004152+SangbumChoi@users.noreply.github.com>\n\n* add: cardinality loss and make box loss as copy from\n\n* change: default for reduction loss is sum\n\n* fix: vectorized generate fake box\n\n* fix copies\n\n* Addressed comments\n\n* addressed comments\n\n* addressed one-hot\n\n* Update tests/models/grounding_dino/test_modeling_grounding_dino.py\n\nCo-authored-by: Sangbum Daniel Choi <34004152+SangbumChoi@users.noreply.github.com>\n\n* Addressed comments\n\n* fixed test\n\n* Update src/transformers/models/grounding_dino/modeling_grounding_dino.py\n\n* Update tests/models/grounding_dino/test_modeling_grounding_dino.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* Starting to fix GroundingDinoLoss and GroundingDinoHungarianMatcher\n\n* More updates\n\n* More updates\n\n* fixed: GroundingDinoLoss\n\n* add: cardinality loss and make box loss as copy from\n\n* fix copies\n\n* Revert \"Update tests/models/grounding_dino/test_modeling_grounding_dino.py\"\n\nThis reverts commit aa74c4c57c430e54cc74c414d6269edb65c73e83.\n\n* [run-slow] groundigdino\n\n* remove nestedtensor\n\n* [run-slow] groundig_dino\n\n* [run-slow] grounding_dino\n\n* [run-slow] grounding_dino\n\n* [run-slow] grounding_dino\n\n* check\n\n* check\n\n* add: enconder intermediate outputs to ImageLoss forward\n\n* add: GroundingDinoForObjectDetectionLoss in the loss directory\n\n* make style\n\n* fix the loss function\n\n* remove class_reduction since it sum is default\n\n* remove class_reduction\n\n* Update src/transformers/loss/loss_grounding_dino.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* simple fix\n\n* Update src/transformers/loss/loss_grounding_dino.py\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\n\n* minor fix\n\n* Update src/transformers/loss/loss_for_object_detection.py\n\n---------\n\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\nCo-authored-by: Sangbum Daniel Choi <34004152+SangbumChoi@users.noreply.github.com>\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>\nCo-authored-by: sangbumchoi <danielsejong55@gmail.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "222505c7e4d08da9095d12ddb72fb653f4b6da33",
    "files": [
        {
            "sha": "0b5e4f6054953ac79161fb86453e0479a54332f1",
            "filename": "src/transformers/loss/loss_grounding_dino.py",
            "status": "added",
            "additions": 271,
            "deletions": 0,
            "changes": 271,
            "blob_url": "https://github.com/huggingface/transformers/blob/222505c7e4d08da9095d12ddb72fb653f4b6da33/src%2Ftransformers%2Floss%2Floss_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/222505c7e4d08da9095d12ddb72fb653f4b6da33/src%2Ftransformers%2Floss%2Floss_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_grounding_dino.py?ref=222505c7e4d08da9095d12ddb72fb653f4b6da33",
            "patch": "@@ -0,0 +1,271 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import torch\n+import torch.nn as nn\n+\n+from ..image_transforms import center_to_corners_format\n+from ..utils import is_scipy_available\n+from .loss_for_object_detection import HungarianMatcher, ImageLoss, _set_aux_loss, generalized_box_iou\n+\n+\n+if is_scipy_available():\n+    from scipy.optimize import linear_sum_assignment\n+\n+\n+# Similar to the one used in `DeformableDetr` but we reduce with sum and normalize by num_boxes\n+# instead of mean.\n+def sigmoid_focal_loss(\n+    inputs: torch.Tensor,\n+    targets: torch.Tensor,\n+    num_boxes: int,\n+    alpha: float = 0.25,\n+    gamma: float = 2,\n+):\n+    \"\"\"\n+    Loss used in RetinaNet for dense detection: https://arxiv.org/abs/1708.02002.\n+\n+    Args:\n+        inputs (`torch.FloatTensor` of arbitrary shape):\n+            The predictions for each example.\n+        targets (`torch.FloatTensor` with the same shape as `inputs`)\n+            A tensor storing the binary classification label for each element in the `inputs` (0 for the negative class\n+            and 1 for the positive class).\n+        num_boxes (`int`):\n+            The total number of boxes in the batch.\n+        alpha (`float`, *optional*, defaults to 0.25):\n+            Optional weighting factor in the range (0,1) to balance positive vs. negative examples.\n+        gamma (`int`, *optional*, defaults to 2):\n+            Exponent of the modulating factor (1 - p_t) to balance easy vs hard examples.\n+\n+    Returns:\n+        Loss tensor\n+    \"\"\"\n+    prob = inputs.sigmoid()\n+    ce_loss = nn.functional.binary_cross_entropy_with_logits(inputs, targets, reduction=\"none\")\n+    # add modulating factor\n+    p_t = prob * targets + (1 - prob) * (1 - targets)\n+    loss = ce_loss * ((1 - p_t) ** gamma)\n+\n+    if alpha >= 0:\n+        alpha_t = alpha * targets + (1 - alpha) * (1 - targets)\n+        loss = alpha_t * loss\n+\n+    return loss.sum() / num_boxes\n+\n+\n+class GroundingDinoHungarianMatcher(HungarianMatcher):\n+    @torch.no_grad()\n+    def forward(self, outputs, targets):\n+        \"\"\"\n+        Args:\n+            outputs (`dict`):\n+                A dictionary that contains at least these entries:\n+                * \"logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n+                * \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates.\n+                * \"label_maps\": Tuple of tensors of dim [num_classes, hidden_dim].\n+            targets (`List[dict]`):\n+                A list of targets (len(targets) = batch_size), where each target is a dict containing:\n+                * \"class_labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of\n+                  ground-truth\n+                 objects in the target) containing the class labels\n+                * \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates.\n+\n+        Returns:\n+            `List[Tuple]`: A list of size `batch_size`, containing tuples of (index_i, index_j) where:\n+            - index_i is the indices of the selected predictions (in order)\n+            - index_j is the indices of the corresponding selected targets (in order)\n+            For each batch element, it holds: len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n+        \"\"\"\n+        batch_size, num_queries = outputs[\"logits\"].shape[:2]\n+\n+        # We flatten to compute the cost matrices in a batch\n+        out_prob = outputs[\"logits\"].flatten(0, 1).sigmoid()  # [batch_size * num_queries, hidden_dim]\n+        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n+        label_maps = outputs[\"label_maps\"]\n+\n+        # First take the label map for each class in each batch and then concatenate them\n+        label_maps = torch.cat([label_map[target[\"class_labels\"]] for label_map, target in zip(label_maps, targets)])\n+        # Normalize label maps based on number of tokens per class\n+        label_maps = label_maps / label_maps.sum(dim=-1, keepdim=True)\n+\n+        # Also concat the target labels and boxes\n+        target_bbox = torch.cat([v[\"boxes\"] for v in targets])\n+\n+        # Compute the classification cost.\n+        alpha = 0.25\n+        gamma = 2.0\n+        neg_cost_class = (1 - alpha) * (out_prob**gamma) * (-(1 - out_prob + 1e-8).log())\n+        pos_cost_class = alpha * ((1 - out_prob) ** gamma) * (-(out_prob + 1e-8).log())\n+        # Compute the classification cost by taking pos and neg cost in the appropriate index\n+        class_cost = (pos_cost_class - neg_cost_class) @ label_maps.t()\n+\n+        # Compute the L1 cost between boxes\n+        bbox_cost = torch.cdist(out_bbox, target_bbox, p=1)\n+\n+        # Compute the giou cost between boxes\n+        giou_cost = -generalized_box_iou(center_to_corners_format(out_bbox), center_to_corners_format(target_bbox))\n+\n+        # Final cost matrix\n+        cost_matrix = self.bbox_cost * bbox_cost + self.class_cost * class_cost + self.giou_cost * giou_cost\n+        cost_matrix = cost_matrix.view(batch_size, num_queries, -1).cpu()\n+\n+        sizes = [len(v[\"boxes\"]) for v in targets]\n+        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(cost_matrix.split(sizes, -1))]\n+        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n+\n+\n+class GroundingDinoImageLoss(ImageLoss):\n+    \"\"\"\n+    This class computes the losses for `GroundingDinoForObjectDetection`. The process happens in two steps: 1) we\n+    compute hungarian assignment between ground truth boxes and the outputs of the model 2) we supervise each pair of\n+    matched ground-truth / prediction (supervise class and box).\n+\n+    Args:\n+        matcher (`GroundingDinoHungarianMatcher`):\n+            Module able to compute a matching between targets and proposals.\n+        focal_alpha (`float`):\n+            Alpha parameter in focal loss.\n+        losses (`List[str]`):\n+            List of all the losses to be applied. See `get_loss` for a list of all available losses.\n+    \"\"\"\n+\n+    def __init__(self, matcher, focal_alpha, losses):\n+        nn.Module.__init__(self)\n+        self.matcher = matcher\n+        self.focal_alpha = focal_alpha\n+        self.losses = losses\n+\n+    def _get_target_classes_one_hot(self, outputs, targets, indices):\n+        \"\"\"\n+        Create one_hot based on the matching indices\n+        \"\"\"\n+        logits = outputs[\"logits\"]\n+        # Add offsets to class_labels to select the correct label map\n+        class_labels = torch.cat(\n+            [\n+                target[\"class_labels\"][J] + len(outputs[\"label_maps\"][i]) if i > 0 else target[\"class_labels\"][J]\n+                for i, (target, (_, J)) in enumerate(zip(targets, indices))\n+            ]\n+        )\n+        label_maps = torch.cat(outputs[\"label_maps\"], dim=0)\n+\n+        idx = self._get_source_permutation_idx(indices)\n+        target_classes_onehot = torch.zeros_like(logits, device=logits.device, dtype=torch.long)\n+        target_classes_onehot[idx] = label_maps[class_labels].to(torch.long)\n+\n+        return target_classes_onehot\n+\n+    def loss_labels(self, outputs, targets, indices, num_boxes):\n+        \"\"\"\n+        Classification loss (Binary focal loss) targets dicts must contain the key \"class_labels\" containing a tensor\n+        of dim [nb_target_boxes]\n+        \"\"\"\n+        if \"logits\" not in outputs:\n+            raise KeyError(\"No logits were found in the outputs\")\n+        if \"text_mask\" not in outputs:\n+            raise KeyError(\"No text_mask were found in the outputs\")\n+\n+        target_classes_onehot = self._get_target_classes_one_hot(outputs, targets, indices)\n+        source_logits = outputs[\"logits\"]\n+        text_mask = outputs[\"text_mask\"]\n+\n+        # Select only valid logits\n+        source_logits = torch.masked_select(source_logits, text_mask)\n+        target_classes_onehot = torch.masked_select(target_classes_onehot, text_mask)\n+\n+        target_classes_onehot = target_classes_onehot.float()\n+        loss_ce = sigmoid_focal_loss(\n+            inputs=source_logits,\n+            targets=target_classes_onehot,\n+            num_boxes=num_boxes,\n+            alpha=self.focal_alpha,\n+            gamma=2,\n+        )\n+\n+        losses = {\"loss_ce\": loss_ce}\n+\n+        return losses\n+\n+\n+def GroundingDinoForObjectDetectionLoss(\n+    logits,\n+    labels,\n+    device,\n+    pred_boxes,\n+    config,\n+    label_maps,\n+    text_mask,\n+    outputs_class=None,\n+    outputs_coord=None,\n+    encoder_logits=None,\n+    encoder_pred_boxes=None,\n+):\n+    # First: create the matcher\n+    matcher = GroundingDinoHungarianMatcher(\n+        class_cost=config.class_cost, bbox_cost=config.bbox_cost, giou_cost=config.giou_cost\n+    )\n+    # Second: create the criterion\n+    losses = [\"labels\", \"boxes\", \"cardinality\"]\n+    criterion = GroundingDinoImageLoss(\n+        matcher=matcher,\n+        focal_alpha=config.focal_alpha,\n+        losses=losses,\n+    )\n+    criterion.to(device)\n+    # Third: compute the losses, based on outputs and labels\n+    outputs_loss = {}\n+    outputs_loss[\"logits\"] = logits\n+    outputs_loss[\"pred_boxes\"] = pred_boxes\n+    outputs_loss[\"label_maps\"] = label_maps\n+    outputs_loss[\"text_mask\"] = text_mask\n+\n+    auxiliary_outputs = None\n+    if config.auxiliary_loss:\n+        auxiliary_outputs = _set_aux_loss(outputs_class, outputs_coord)\n+        for aux_output in auxiliary_outputs:\n+            aux_output[\"label_maps\"] = label_maps\n+            aux_output[\"text_mask\"] = text_mask\n+        outputs_loss[\"auxiliary_outputs\"] = auxiliary_outputs\n+\n+    loss_dict = criterion(outputs_loss, labels)\n+\n+    if config.two_stage:\n+        encoder_outputs_loss = {\n+            \"logits\": encoder_logits,\n+            \"pred_boxes\": encoder_pred_boxes,\n+            \"label_maps\": label_maps,\n+            \"text_mask\": text_mask,\n+        }\n+        encoder_loss_dict = criterion(encoder_outputs_loss, labels)\n+        encoder_loss_dict = {k + \"_enc\": v for k, v in encoder_loss_dict.items()}\n+        loss_dict.update(encoder_loss_dict)\n+    # Fourth: compute total loss, as a weighted sum of the various losses\n+    weight_dict = {\n+        \"loss_ce\": 2.0,\n+        \"loss_bbox\": config.bbox_loss_coefficient,\n+        \"loss_giou\": config.giou_loss_coefficient,\n+    }\n+\n+    if config.two_stage:\n+        enc_weight_dict = {k + \"_enc\": v for k, v in weight_dict.items()}\n+        weight_dict.update(enc_weight_dict)\n+\n+    if config.auxiliary_loss:\n+        aux_weight_dict = {}\n+        for i in range(config.decoder_layers - 1):\n+            aux_weight_dict.update({k + f\"_{i}\": v for k, v in weight_dict.items()})\n+        weight_dict.update(aux_weight_dict)\n+\n+    loss = sum(loss_dict[k] * weight_dict[k] for k in loss_dict.keys() if k in weight_dict)\n+    return loss, loss_dict, auxiliary_outputs"
        },
        {
            "sha": "61a0eda47d1cb3bfd4aedd0e911763ca4f762301",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/222505c7e4d08da9095d12ddb72fb653f4b6da33/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/222505c7e4d08da9095d12ddb72fb653f4b6da33/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=222505c7e4d08da9095d12ddb72fb653f4b6da33",
            "patch": "@@ -18,6 +18,7 @@\n \n from .loss_deformable_detr import DeformableDetrForObjectDetectionLoss, DeformableDetrForSegmentationLoss\n from .loss_for_object_detection import ForObjectDetectionLoss, ForSegmentationLoss\n+from .loss_grounding_dino import GroundingDinoForObjectDetectionLoss\n from .loss_rt_detr import RTDetrForObjectDetectionLoss\n \n \n@@ -129,7 +130,7 @@ def ForTokenClassification(logits, labels, config, **kwargs):\n     \"DeformableDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n     \"ConditionalDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n     \"DabDetrForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n-    \"GroundingDinoForObjectDetection\": DeformableDetrForObjectDetectionLoss,\n+    \"GroundingDinoForObjectDetection\": GroundingDinoForObjectDetectionLoss,\n     \"ConditionalDetrForSegmentation\": DeformableDetrForSegmentationLoss,\n     \"RTDetrForObjectDetection\": RTDetrForObjectDetectionLoss,\n     \"RTDetrV2ForObjectDetection\": RTDetrForObjectDetectionLoss,"
        },
        {
            "sha": "d8a4610cfd33141ebccb9a89322d77e638d1888e",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 113,
            "deletions": 10,
            "changes": 123,
            "blob_url": "https://github.com/huggingface/transformers/blob/222505c7e4d08da9095d12ddb72fb653f4b6da33/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/222505c7e4d08da9095d12ddb72fb653f4b6da33/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=222505c7e4d08da9095d12ddb72fb653f4b6da33",
            "patch": "@@ -252,6 +252,10 @@ class GroundingDinoModelOutput(ModelOutput):\n             background).\n         enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n             Logits of predicted bounding boxes coordinates in the first stage.\n+        encoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.two_stage=True`):\n+            Logits of top `config.num_queries` scoring bounding boxes in the first stage.\n+        encoder_pred_boxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n+            Coordinates of top `config.num_queries` scoring bounding boxes in the first stage.\n     \"\"\"\n \n     last_hidden_state: torch.FloatTensor = None\n@@ -267,6 +271,8 @@ class GroundingDinoModelOutput(ModelOutput):\n     encoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     enc_outputs_class: Optional[torch.FloatTensor] = None\n     enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n+    encoder_logits: Optional[torch.FloatTensor] = None\n+    encoder_pred_boxes: Optional[torch.FloatTensor] = None\n \n \n @dataclass\n@@ -331,6 +337,10 @@ class GroundingDinoObjectDetectionOutput(ModelOutput):\n             background).\n         enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n             Logits of predicted bounding boxes coordinates in the first stage.\n+        encoder_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.num_labels)`, *optional*, returned when `config.two_stage=True`):\n+            Logits of top `config.num_queries` scoring bounding boxes in the first stage.\n+        encoder_pred_boxes (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n+            Coordinates of top `config.num_queries` scoring bounding boxes in the first stage.\n         input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n             Encoded candidate labels sequence. Used in processor to post process object detection result.\n     \"\"\"\n@@ -353,6 +363,8 @@ class GroundingDinoObjectDetectionOutput(ModelOutput):\n     encoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     enc_outputs_class: Optional[torch.FloatTensor] = None\n     enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n+    encoder_logits: Optional[torch.FloatTensor] = None\n+    encoder_pred_boxes: Optional[torch.FloatTensor] = None\n     input_ids: Optional[torch.LongTensor] = None\n \n \n@@ -2374,8 +2386,11 @@ def forward(\n             )\n \n         # Fifth, prepare decoder inputs\n+        topk_proposals = None\n         enc_outputs_class = None\n         enc_outputs_coord_logits = None\n+        encoder_logits = None\n+        encoder_pred_boxes = None\n         if self.config.two_stage:\n             object_query_embedding, output_proposals = self.generate_encoder_output_proposals(\n                 encoder_outputs[0], ~mask_flatten, spatial_shapes\n@@ -2408,6 +2423,10 @@ def forward(\n                 target = torch.gather(\n                     object_query_embedding, 1, topk_proposals.unsqueeze(-1).repeat(1, 1, self.d_model)\n                 ).detach()\n+\n+            # Set intermediate topk proposals (coords and class) for loss computation\n+            encoder_pred_boxes = reference_points\n+            encoder_logits = self.encoder_output_class_embed(target, text_features, text_token_mask)\n         else:\n             target = query_embeds.unsqueeze(0).repeat(batch_size, 1, 1)\n             reference_points = self.reference_points.weight.unsqueeze(0).repeat(batch_size, 1, 1).sigmoid()\n@@ -2430,7 +2449,16 @@ def forward(\n         )\n \n         if not return_dict:\n-            enc_outputs = tuple(value for value in [enc_outputs_class, enc_outputs_coord_logits] if value is not None)\n+            enc_outputs = tuple(\n+                value\n+                for value in [\n+                    enc_outputs_class,\n+                    enc_outputs_coord_logits,\n+                    encoder_logits,\n+                    encoder_pred_boxes,\n+                ]\n+                if value is not None\n+            )\n             tuple_outputs = (\n                 (decoder_outputs[0], init_reference_points) + decoder_outputs[1:] + encoder_outputs + enc_outputs\n             )\n@@ -2451,6 +2479,8 @@ def forward(\n             encoder_attentions=encoder_outputs.attentions,\n             enc_outputs_class=enc_outputs_class,\n             enc_outputs_coord_logits=enc_outputs_coord_logits,\n+            encoder_logits=encoder_logits,\n+            encoder_pred_boxes=encoder_pred_boxes,\n         )\n \n \n@@ -2476,6 +2506,73 @@ def forward(self, x):\n         return x\n \n \n+def build_label_maps(logits: torch.FloatTensor, input_ids: torch.LongTensor) -> Tuple[torch.FloatTensor]:\n+    \"\"\"\n+    Computes a mapping between tokens and their corresponding labels, where `num_labels` is determined by the number of classes in the input prompt.\n+    The function identifies segments of tokens between specific delimiter tokens and generates label maps for those segments.\n+    Args:\n+        logits (`torch.Tensor` of shape `(batch_size, seq_length, hidden_size)`):\n+            The output logits from the model, where `hidden_size` corresponds to the dimension of the model's output features.\n+\n+        input_ids (`torch.Tensor` of shape `(batch_size, seq_length)`):\n+            The input token IDs corresponding to the input prompt. For example, given the prompt \"fish. shark.\",\n+            `input_ids` might look like `[101, 3869, 1012, 11420, 1012, 102]` where each number corresponds to a token including special tokens.\n+    Returns:\n+        tuple: A tuple containing label maps for each instance in the batch.\n+        - label_maps (tuple of `torch.Tensor`):\n+            A tuple of tensors, where each tensor in the tuple corresponds to an instance in the batch. Each tensor\n+            has shape `(num_labels, hidden_size)` and contains binary values (0 or 1), where `1` indicates the tokens\n+            that are associated with a specific label (class) between delimiter tokens, and `0` elsewhere.\n+    Example:\n+        Given an input prompt \"fish. shark.\" and corresponding `input_ids` as `[101, 3869, 1012, 11420, 1012, 102]`:\n+        - The function identifies the tokens for \"fish\" (IDs `[3869]`) and \"shark\" (IDs `[11420]`).\n+        - The function then constructs label maps for these tokens, where each label map indicates which tokens\n+          correspond to which label between the delimiter tokens (e.g., between the period `.`).\n+        - The output is a tuple of label maps, one for each instance in the batch.\n+    Note:\n+        - `SPECIAL_TOKENS` should be a predefined list of tokens that are considered special (e.g., `[CLS]`, `[SEP]`, etc.).\n+    \"\"\"\n+    max_seq_len = logits.shape[-1]\n+    # Add [PAD] token to the list of special tokens\n+    delimiter_tokens = torch.tensor(SPECIAL_TOKENS + [0], device=input_ids.device)\n+\n+    delimiter_token_masks = torch.isin(input_ids, delimiter_tokens)\n+    label_groups = torch.cumsum(delimiter_token_masks, dim=1) * (~delimiter_token_masks).to(torch.int32)\n+\n+    label_maps = ()\n+\n+    # Iterate over batch dimension as we can have different number of labels\n+    for label_group in label_groups:\n+        # `label_group` is a tensor of shape `(seq_len,)` with zeros for non-label tokens and integers for label tokens\n+        # label tokens with same integer value are part of the same label group\n+\n+        # Get unique labels and exclude 0 (i.e. non-label tokens)\n+        unique_labels = torch.unique(label_group)[1:, None]\n+        num_labels = unique_labels.shape[0]\n+\n+        # Create one-hot encoding for each label group\n+        label_map = label_group.unsqueeze(0).repeat(num_labels, 1)\n+        label_map = torch.where(label_map == unique_labels, 1, 0)\n+\n+        # Pad label_map to match `max_seq_len`\n+        label_map = F.pad(label_map, (0, max_seq_len - label_map.shape[1]), value=0)\n+\n+        label_maps += (label_map,)\n+\n+    return label_maps\n+\n+\n+def build_text_mask(logits, attention_mask):\n+    \"\"\"\n+    Create text_mask based on the matching indices\n+    \"\"\"\n+    seq_len = attention_mask.shape[1]\n+    text_mask = torch.zeros_like(logits, device=logits.device, dtype=attention_mask.dtype)\n+    text_mask[:, :, :seq_len] = attention_mask[:, None, :]\n+\n+    return text_mask.bool()\n+\n+\n @add_start_docstrings(\n     \"\"\"\n     Grounding DINO Model (consisting of a backbone and encoder-decoder Transformer) with object detection heads on top,\n@@ -2514,14 +2611,6 @@ def __init__(self, config: GroundingDinoConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # taken from https://github.com/facebookresearch/detr/blob/master/models/detr.py\n-    @torch.jit.unused\n-    def _set_aux_loss(self, outputs_class, outputs_coord):\n-        # this is a workaround to make torchscript happy, as torchscript\n-        # doesn't support dictionary with non-homogeneous values, such\n-        # as a dict having both a Tensor and a list.\n-        return [{\"logits\": a, \"pred_boxes\": b} for a, b in zip(outputs_class[:-1], outputs_coord[:-1])]\n-\n     @add_start_docstrings_to_model_forward(GROUNDING_DINO_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=GroundingDinoObjectDetectionOutput, config_class=_CONFIG_FOR_DOC)\n     def forward(\n@@ -2648,8 +2737,20 @@ def forward(\n \n         loss, loss_dict, auxiliary_outputs = None, None, None\n         if labels is not None:\n+            label_maps = build_label_maps(logits, input_ids)\n+            text_mask = build_text_mask(logits, attention_mask)\n             loss, loss_dict, auxiliary_outputs = self.loss_function(\n-                logits, labels, self.device, pred_boxes, self.config, outputs_class, outputs_coord\n+                logits,\n+                labels,\n+                self.device,\n+                pred_boxes,\n+                self.config,\n+                label_maps,\n+                text_mask,\n+                outputs_class=outputs_class,\n+                outputs_coord=outputs_coord,\n+                encoder_logits=outputs[-2],\n+                encoder_pred_boxes=outputs[-1],\n             )\n \n         if not return_dict:\n@@ -2677,6 +2778,8 @@ def forward(\n             init_reference_points=outputs.init_reference_points,\n             enc_outputs_class=outputs.enc_outputs_class,\n             enc_outputs_coord_logits=outputs.enc_outputs_coord_logits,\n+            encoder_logits=outputs.encoder_logits,\n+            encoder_pred_boxes=outputs.encoder_pred_boxes,\n             input_ids=input_ids,\n         )\n "
        },
        {
            "sha": "88ba1caa2bb6a157d6d84d1c0544bc2e135112d3",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 101,
            "deletions": 7,
            "changes": 108,
            "blob_url": "https://github.com/huggingface/transformers/blob/222505c7e4d08da9095d12ddb72fb653f4b6da33/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/222505c7e4d08da9095d12ddb72fb653f4b6da33/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=222505c7e4d08da9095d12ddb72fb653f4b6da33",
            "patch": "@@ -20,6 +20,8 @@\n import re\n import unittest\n \n+from datasets import load_dataset\n+\n from transformers import (\n     GroundingDinoConfig,\n     SwinConfig,\n@@ -28,6 +30,7 @@\n )\n from transformers.file_utils import cached_property\n from transformers.testing_utils import (\n+    is_flaky,\n     require_timm,\n     require_torch,\n     require_torch_accelerator,\n@@ -37,14 +40,14 @@\n )\n \n from ...test_configuration_common import ConfigTester\n-from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor, ids_tensor\n+from ...test_modeling_common import ModelTesterMixin, _config_zero_init, floats_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n \n \n if is_torch_available():\n     import torch\n \n-    from transformers import GroundingDinoForObjectDetection, GroundingDinoModel\n+    from transformers import GroundingDinoConfig, GroundingDinoForObjectDetection, GroundingDinoModel\n     from transformers.pytorch_utils import id_tensor_storage\n \n \n@@ -54,6 +57,39 @@\n     from transformers import AutoProcessor\n \n \n+def generate_fake_bounding_boxes(n_boxes):\n+    \"\"\"Generate bounding boxes in the format (center_x, center_y, width, height)\"\"\"\n+    # Validate the input\n+    if not isinstance(n_boxes, int):\n+        raise ValueError(\"n_boxes must be an integer\")\n+    if n_boxes <= 0:\n+        raise ValueError(\"n_boxes must be a positive integer\")\n+\n+    # Generate random bounding boxes in the format (center_x, center_y, width, height)\n+    bounding_boxes = torch.rand((n_boxes, 4))\n+\n+    # Extract the components\n+    center_x = bounding_boxes[:, 0]\n+    center_y = bounding_boxes[:, 1]\n+    width = bounding_boxes[:, 2]\n+    height = bounding_boxes[:, 3]\n+\n+    # Ensure width and height do not exceed bounds\n+    width = torch.min(width, torch.tensor(1.0))\n+    height = torch.min(height, torch.tensor(1.0))\n+\n+    # Ensure the bounding box stays within the normalized space\n+    center_x = torch.where(center_x - width / 2 < 0, width / 2, center_x)\n+    center_x = torch.where(center_x + width / 2 > 1, 1 - width / 2, center_x)\n+    center_y = torch.where(center_y - height / 2 < 0, height / 2, center_y)\n+    center_y = torch.where(center_y + height / 2 > 1, 1 - height / 2, center_y)\n+\n+    # Combine back into bounding boxes\n+    bounding_boxes = torch.stack([center_x, center_y, width, height], dim=1)\n+\n+    return bounding_boxes\n+\n+\n class GroundingDinoModelTester:\n     def __init__(\n         self,\n@@ -72,7 +108,7 @@ def __init__(\n         num_channels=3,\n         image_size=98,\n         n_targets=8,\n-        num_labels=3,\n+        num_labels=2,\n         num_feature_levels=4,\n         encoder_n_points=2,\n         decoder_n_points=6,\n@@ -115,7 +151,11 @@ def prepare_config_and_inputs(self):\n         pixel_values = floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n         pixel_mask = torch.ones([self.batch_size, self.image_size, self.image_size], device=torch_device)\n \n-        input_ids = ids_tensor([self.batch_size, self.max_text_len], self.num_labels)\n+        # When using `GroundingDino` the text input template is '{label1}. {label2}. {label3. ... {labelN}.'\n+        # Therefore to avoid errors when running tests with `labels` `input_ids` have to follow this structure.\n+        # Otherwise when running `build_label_maps` it will throw an error when trying to split the input_ids into segments.\n+        input_ids = torch.tensor([101, 3869, 1012, 11420, 3869, 1012, 102], device=torch_device)\n+        input_ids = input_ids.unsqueeze(0).expand(self.batch_size, -1)\n \n         labels = None\n         if self.use_labels:\n@@ -126,7 +166,7 @@ def prepare_config_and_inputs(self):\n                 target[\"class_labels\"] = torch.randint(\n                     high=self.num_labels, size=(self.n_targets,), device=torch_device\n                 )\n-                target[\"boxes\"] = torch.rand(self.n_targets, 4, device=torch_device)\n+                target[\"boxes\"] = generate_fake_bounding_boxes(self.n_targets).to(torch_device)\n                 target[\"masks\"] = torch.rand(self.n_targets, self.image_size, self.image_size, device=torch_device)\n                 labels.append(target)\n \n@@ -317,7 +357,7 @@ def test_attention_outputs(self):\n             )\n             out_len = len(outputs)\n \n-            correct_outlen = 10\n+            correct_outlen = 12\n \n             # loss is at first position\n             if \"labels\" in inputs_dict:\n@@ -677,6 +717,7 @@ def test_inference_object_detection_head(self):\n         self.assertListEqual(results[\"text_labels\"], expected_labels)\n \n     @require_torch_accelerator\n+    @is_flaky()\n     def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n         processor = self.default_processor\n         image = prepare_img()\n@@ -716,6 +757,7 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n         torch.testing.assert_close(results_cpu[\"scores\"], result_gpu[\"scores\"].cpu(), rtol=1e-3, atol=1e-3)\n         torch.testing.assert_close(results_cpu[\"boxes\"], result_gpu[\"boxes\"].cpu(), rtol=1e-3, atol=1e-3)\n \n+    @is_flaky()\n     def test_cross_attention_mask(self):\n         model = GroundingDinoForObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-tiny\").to(torch_device)\n \n@@ -740,4 +782,56 @@ def test_cross_attention_mask(self):\n \n         torch.testing.assert_close(outputs1.logits, outputs_batched.logits[:1], rtol=1e-3, atol=1e-3)\n         # For some reason 12 elements are > 1e-3, but the rest are fine\n-        torch.testing.assert_close(outputs2.logits, outputs_batched.logits[1:], rtol=1.8e-3, atol=1.8e-3)\n+        self.assertTrue(torch.allclose(outputs2.logits, outputs_batched.logits[1:], atol=1.8e-3))\n+\n+    def test_grounding_dino_loss(self):\n+        ds = load_dataset(\"EduardoPacheco/aquarium-sample\", split=\"train\")\n+        image_processor = self.default_processor.image_processor\n+        tokenizer = self.default_processor.tokenizer\n+        id2label = {0: \"fish\", 1: \"jellyfish\", 2: \"penguins\", 3: \"sharks\", 4: \"puffins\", 5: \"stingrays\", 6: \"starfish\"}\n+        prompt = \". \".join(id2label.values()) + \".\"\n+\n+        text_inputs = tokenizer([prompt, prompt], return_tensors=\"pt\")\n+        image_inputs = image_processor(images=ds[\"image\"], annotations=ds[\"annotations\"], return_tensors=\"pt\")\n+\n+        # Passing auxiliary_loss=True to compare with the expected loss\n+        model = GroundingDinoForObjectDetection.from_pretrained(\n+            \"IDEA-Research/grounding-dino-tiny\",\n+            auxiliary_loss=True,\n+        )\n+        # Interested in the loss only\n+        model.eval()\n+        with torch.no_grad():\n+            outputs = model(**text_inputs, **image_inputs)\n+\n+        # Loss differs by CPU and GPU, also this can be changed in future.\n+        expected_loss_dict = {\n+            \"loss_ce\": torch.tensor(1.1147),\n+            \"loss_bbox\": torch.tensor(0.2031),\n+            \"loss_giou\": torch.tensor(0.5819),\n+            \"loss_ce_0\": torch.tensor(1.1941),\n+            \"loss_bbox_0\": torch.tensor(0.1978),\n+            \"loss_giou_0\": torch.tensor(0.5524),\n+            \"loss_ce_1\": torch.tensor(1.1621),\n+            \"loss_bbox_1\": torch.tensor(0.1909),\n+            \"loss_giou_1\": torch.tensor(0.5892),\n+            \"loss_ce_2\": torch.tensor(1.1641),\n+            \"loss_bbox_2\": torch.tensor(0.1892),\n+            \"loss_giou_2\": torch.tensor(0.5626),\n+            \"loss_ce_3\": torch.tensor(1.1943),\n+            \"loss_bbox_3\": torch.tensor(0.1941),\n+            \"loss_giou_3\": torch.tensor(0.5607),\n+            \"loss_ce_4\": torch.tensor(1.0956),\n+            \"loss_bbox_4\": torch.tensor(0.2008),\n+            \"loss_giou_4\": torch.tensor(0.5836),\n+            \"loss_ce_enc\": torch.tensor(16226.3164),\n+            \"loss_bbox_enc\": torch.tensor(0.3063),\n+            \"loss_giou_enc\": torch.tensor(0.7380),\n+        }\n+\n+        expected_loss = torch.tensor(32482.2305)\n+\n+        for key in expected_loss_dict:\n+            self.assertTrue(torch.allclose(outputs.loss_dict[key], expected_loss_dict[key], atol=1e-3))\n+\n+        self.assertTrue(torch.allclose(outputs.loss, expected_loss, atol=1e-3))"
        }
    ],
    "stats": {
        "total": 505,
        "additions": 487,
        "deletions": 18
    }
}