{
    "author": "zucchini-nlp",
    "message": "Fix GPT2 with cross attention (#39754)\n\n* fix\n\n* use new mask API\n\n* style\n\n* fix copies and attention tests\n\n* fix head pruning tests",
    "sha": "ccb2e0e03b41429eeede933f38c80e36fcee772f",
    "files": [
        {
            "sha": "563615d217b92e7d35ca06cf969a299a34ee7826",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 35,
            "deletions": 26,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccb2e0e03b41429eeede933f38c80e36fcee772f/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccb2e0e03b41429eeede933f38c80e36fcee772f/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=ccb2e0e03b41429eeede933f38c80e36fcee772f",
            "patch": "@@ -268,53 +268,62 @@ def forward(\n         **kwargs,\n     ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]:\n         is_cross_attention = encoder_hidden_states is not None\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n         if is_cross_attention:\n             if not hasattr(self, \"q_attn\"):\n                 raise ValueError(\n                     \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                     \"Please make sure to instantiate class with `DecisionTransformerGPT2Attention(..., is_cross_attention=True)`.\"\n                 )\n-\n             query_states = self.q_attn(hidden_states)\n-            key_states, value_states = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n             attention_mask = encoder_attention_mask\n+\n+            # Try to get key/value states from cache if possible\n+            if past_key_value is not None and is_updated:\n+                key_states = curr_past_key_value.layers[self.layer_idx].keys\n+                value_states = curr_past_key_value.layers[self.layer_idx].values\n+            else:\n+                key_states, value_states = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n+                shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n+                key_states = key_states.view(shape_kv).transpose(1, 2)\n+                value_states = value_states.view(shape_kv).transpose(1, 2)\n         else:\n             query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2)\n+            shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n+            key_states = key_states.view(shape_kv).transpose(1, 2)\n+            value_states = value_states.view(shape_kv).transpose(1, 2)\n \n         shape_q = (*query_states.shape[:-1], -1, self.head_dim)\n-        shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n-\n         query_states = query_states.view(shape_q).transpose(1, 2)\n-        key_states = key_states.view(shape_kv).transpose(1, 2)\n-        value_states = value_states.view(shape_kv).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                if is_cross_attention:\n-                    past_key_value = past_key_value.cross_attention_cache\n-                else:\n-                    past_key_value = past_key_value.self_attention_cache\n-            cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(\n-                key_states, value_states, self.layer_idx, cache_kwargs=cache_kwargs\n+        if (past_key_value is not None and not is_cross_attention) or (\n+            past_key_value is not None and is_cross_attention and not is_updated\n+        ):\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            cache_position = cache_position if not is_cross_attention else None\n+            key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n             )\n+            # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+            if is_cross_attention:\n+                past_key_value.is_updated[self.layer_idx] = True\n \n         is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n \n         using_eager = self.config._attn_implementation == \"eager\"\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and (output_attentions or head_mask is not None):\n-                using_eager = True\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                # Attention functions are consistent with previous equivalent attention classes, however they do not support some options\n-                # (e.g. layer scaling, head mask) that eager supports. These implementations are thus equivalent to previous code, but\n-                # not necessarily to eager (if mentioned options are provided).\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         if using_eager and self.reorder_and_upcast_attn:\n             attn_output, attn_weights = self._upcast_and_reordered_attn("
        },
        {
            "sha": "36d1bdb8b8b33ba5205100b92964980d1399c50a",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 46,
            "deletions": 150,
            "changes": 196,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccb2e0e03b41429eeede933f38c80e36fcee772f/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccb2e0e03b41429eeede933f38c80e36fcee772f/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=ccb2e0e03b41429eeede933f38c80e36fcee772f",
            "patch": "@@ -27,9 +27,10 @@\n from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN, get_activation\n-from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache, StaticCache\n+from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import AttentionMaskConverter, _prepare_4d_attention_mask_for_sdpa\n+from ...masking_utils import create_causal_mask\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask_for_sdpa\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutputWithPastAndCrossAttentions,\n@@ -278,53 +279,62 @@ def forward(\n         **kwargs,\n     ) -> tuple[Union[torch.Tensor, tuple[torch.Tensor]], ...]:\n         is_cross_attention = encoder_hidden_states is not None\n+        if past_key_value is not None:\n+            if isinstance(past_key_value, EncoderDecoderCache):\n+                is_updated = past_key_value.is_updated.get(self.layer_idx)\n+                if is_cross_attention:\n+                    # after the first generated id, we can subsequently re-use all key/value_layer from cache\n+                    curr_past_key_value = past_key_value.cross_attention_cache\n+                else:\n+                    curr_past_key_value = past_key_value.self_attention_cache\n+            else:\n+                curr_past_key_value = past_key_value\n+\n         if is_cross_attention:\n             if not hasattr(self, \"q_attn\"):\n                 raise ValueError(\n                     \"If class is used as cross attention, the weights `q_attn` have to be defined. \"\n                     \"Please make sure to instantiate class with `GPT2Attention(..., is_cross_attention=True)`.\"\n                 )\n-\n             query_states = self.q_attn(hidden_states)\n-            key_states, value_states = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n             attention_mask = encoder_attention_mask\n+\n+            # Try to get key/value states from cache if possible\n+            if past_key_value is not None and is_updated:\n+                key_states = curr_past_key_value.layers[self.layer_idx].keys\n+                value_states = curr_past_key_value.layers[self.layer_idx].values\n+            else:\n+                key_states, value_states = self.c_attn(encoder_hidden_states).split(self.split_size, dim=2)\n+                shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n+                key_states = key_states.view(shape_kv).transpose(1, 2)\n+                value_states = value_states.view(shape_kv).transpose(1, 2)\n         else:\n             query_states, key_states, value_states = self.c_attn(hidden_states).split(self.split_size, dim=2)\n+            shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n+            key_states = key_states.view(shape_kv).transpose(1, 2)\n+            value_states = value_states.view(shape_kv).transpose(1, 2)\n \n         shape_q = (*query_states.shape[:-1], -1, self.head_dim)\n-        shape_kv = (*key_states.shape[:-1], -1, self.head_dim)\n-\n         query_states = query_states.view(shape_q).transpose(1, 2)\n-        key_states = key_states.view(shape_kv).transpose(1, 2)\n-        value_states = value_states.view(shape_kv).transpose(1, 2)\n \n-        if past_key_value is not None:\n-            if isinstance(past_key_value, EncoderDecoderCache):\n-                if is_cross_attention:\n-                    past_key_value = past_key_value.cross_attention_cache\n-                else:\n-                    past_key_value = past_key_value.self_attention_cache\n-            cache_kwargs = {\"cache_position\": cache_position}\n-            key_states, value_states = past_key_value.update(\n-                key_states, value_states, self.layer_idx, cache_kwargs=cache_kwargs\n+        if (past_key_value is not None and not is_cross_attention) or (\n+            past_key_value is not None and is_cross_attention and not is_updated\n+        ):\n+            # save all key/value_layer to cache to be re-used for fast auto-regressive generation\n+            cache_position = cache_position if not is_cross_attention else None\n+            key_states, value_states = curr_past_key_value.update(\n+                key_states, value_states, self.layer_idx, {\"cache_position\": cache_position}\n             )\n+            # set flag that curr layer for cross-attn is already updated so we can re-use in subsequent calls\n+            if is_cross_attention:\n+                past_key_value.is_updated[self.layer_idx] = True\n \n         is_causal = attention_mask is None and query_states.shape[-2] > 1 and not is_cross_attention\n \n         using_eager = self.config._attn_implementation == \"eager\"\n         attention_interface: Callable = eager_attention_forward\n         if self.config._attn_implementation != \"eager\":\n-            if self.config._attn_implementation == \"sdpa\" and (output_attentions or head_mask is not None):\n-                using_eager = True\n-                logger.warning_once(\n-                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n-                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n-                )\n-            else:\n-                # Attention functions are consistent with previous equivalent attention classes, however they do not support some options\n-                # (e.g. layer scaling, head mask) that eager supports. These implementations are thus equivalent to previous code, but\n-                # not necessarily to eager (if mentioned options are provided).\n-                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n \n         if using_eager and self.reorder_and_upcast_attn:\n             attn_output, attn_weights = self._upcast_and_reordered_attn(\n@@ -861,8 +871,14 @@ def forward(\n         # ._update_causal_mask() and ._prepare_4d_causal_attention_mask_with_cache_position() copied from LlamaModel\n         if attention_mask is not None and attention_mask.ndim < 4:\n             attention_mask = attention_mask.view(batch_size, -1)\n-        causal_mask = self._update_causal_mask(\n-            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n+\n+        causal_mask = create_causal_mask(\n+            config=self.config,\n+            input_embeds=inputs_embeds,\n+            attention_mask=attention_mask,\n+            cache_position=cache_position,\n+            past_key_values=past_key_values,\n+            position_ids=position_ids,\n         )\n \n         # If a 2D or 3D attention mask is provided for the cross-attention\n@@ -903,9 +919,6 @@ def forward(\n             # Model parallel\n             if self.model_parallel:\n                 torch.cuda.set_device(hidden_states.device)\n-                # Ensure that attention_mask is always on the same device as hidden_states\n-                if attention_mask is not None:\n-                    attention_mask = attention_mask.to(hidden_states.device)\n                 if isinstance(head_mask, torch.Tensor):\n                     head_mask = head_mask.to(hidden_states.device)\n             if output_hidden_states:\n@@ -966,123 +979,6 @@ def forward(\n             cross_attentions=all_cross_attentions,\n         )\n \n-    def _update_causal_mask(\n-        self,\n-        attention_mask: torch.Tensor,\n-        input_tensor: torch.Tensor,\n-        cache_position: torch.Tensor,\n-        past_key_values: Cache,\n-        output_attentions: bool,\n-    ):\n-        if self.config._attn_implementation == \"flash_attention_2\":\n-            if attention_mask is not None and 0.0 in attention_mask:\n-                return attention_mask\n-            return None\n-\n-        # For SDPA, when possible, we will rely on its `is_causal` argument instead of its `attn_mask` argument, in\n-        # order to dispatch on Flash Attention 2. This feature is not compatible with static cache, as SDPA will fail\n-        # to infer the attention mask.\n-        past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n-        using_static_cache = isinstance(past_key_values, StaticCache)\n-\n-        # When output attentions is True, sdpa implementation's forward method calls the eager implementation's forward\n-        if self.config._attn_implementation == \"sdpa\" and not using_static_cache and not output_attentions:\n-            if AttentionMaskConverter._ignore_causal_mask_sdpa(\n-                attention_mask,\n-                inputs_embeds=input_tensor,\n-                past_key_values_length=past_seen_tokens,\n-                is_training=self.training,\n-            ):\n-                return None\n-\n-        dtype = input_tensor.dtype\n-        sequence_length = input_tensor.shape[1]\n-        if using_static_cache:\n-            target_length = past_key_values.get_max_cache_shape()\n-        else:\n-            target_length = (\n-                attention_mask.shape[-1]\n-                if isinstance(attention_mask, torch.Tensor)\n-                else past_seen_tokens + sequence_length + 1\n-            )\n-\n-        # In case the provided `attention` mask is 2D, we generate a causal mask here (4D).\n-        causal_mask = self._prepare_4d_causal_attention_mask_with_cache_position(\n-            attention_mask,\n-            sequence_length=sequence_length,\n-            target_length=target_length,\n-            dtype=dtype,\n-            cache_position=cache_position,\n-            batch_size=input_tensor.shape[0],\n-        )\n-\n-        if (\n-            self.config._attn_implementation == \"sdpa\"\n-            and attention_mask is not None\n-            and attention_mask.device.type == \"cuda\"\n-            and not output_attentions\n-        ):\n-            # Attend to all tokens in fully masked rows in the causal_mask, for example the relevant first rows when\n-            # using left padding. This is required by F.scaled_dot_product_attention memory-efficient attention path.\n-            # Details: https://github.com/pytorch/pytorch/issues/110213\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = AttentionMaskConverter._unmask_unattended(causal_mask, min_dtype)\n-\n-        return causal_mask\n-\n-    @staticmethod\n-    def _prepare_4d_causal_attention_mask_with_cache_position(\n-        attention_mask: torch.Tensor,\n-        sequence_length: int,\n-        target_length: int,\n-        dtype: torch.dtype,\n-        cache_position: torch.Tensor,\n-        batch_size: int,\n-        **kwargs,\n-    ):\n-        \"\"\"\n-        Creates a causal 4D mask of shape `(batch_size, 1, query_length, key_value_length)` from a 2D mask of shape\n-        `(batch_size, key_value_length)`, or if the input `attention_mask` is already 4D, do nothing.\n-\n-        Args:\n-            attention_mask (`torch.Tensor`):\n-                A 2D attention mask of shape `(batch_size, key_value_length)` or a 4D attention mask of shape\n-                `(batch_size, 1, query_length, key_value_length)`.\n-            sequence_length (`int`):\n-                The sequence length being processed.\n-            target_length (`int`):\n-                The target length: when generating with static cache, the mask should be as long as the static cache,\n-                to account for the 0 padding, the part of the cache that is not filled yet.\n-            dtype (`torch.dtype`):\n-                The dtype to use for the 4D attention mask.\n-            cache_position (`torch.Tensor`):\n-                Indices depicting the position of the input sequence tokens in the sequence.\n-            batch_size (`torch.Tensor`):\n-                Batch size.\n-        \"\"\"\n-        if attention_mask is not None and attention_mask.dim() == 4:\n-            # In this case we assume that the mask comes already in inverted form and requires no inversion or slicing.\n-            causal_mask = attention_mask\n-        else:\n-            min_dtype = torch.finfo(dtype).min\n-            causal_mask = torch.full(\n-                (sequence_length, target_length), fill_value=min_dtype, dtype=dtype, device=cache_position.device\n-            )\n-            if sequence_length != 1:\n-                causal_mask = torch.triu(causal_mask, diagonal=1)\n-            causal_mask *= torch.arange(target_length, device=cache_position.device) > cache_position.reshape(-1, 1)\n-            causal_mask = causal_mask[None, None, :, :].expand(batch_size, 1, -1, -1)\n-            if attention_mask is not None:\n-                causal_mask = causal_mask.clone()  # copy to contiguous memory for in-place edit\n-                mask_length = attention_mask.shape[-1]\n-                padding_mask = causal_mask[:, :, :, :mask_length] + attention_mask[:, None, None, :]\n-                padding_mask = padding_mask == 0\n-                causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n-                    padding_mask, min_dtype\n-                )\n-\n-        return causal_mask\n-\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "527699035332266627f6cb8732979fdb0e73eede",
            "filename": "src/transformers/models/vision_encoder_decoder/modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccb2e0e03b41429eeede933f38c80e36fcee772f/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccb2e0e03b41429eeede933f38c80e36fcee772f/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_encoder_decoder%2Fmodeling_vision_encoder_decoder.py?ref=ccb2e0e03b41429eeede933f38c80e36fcee772f",
            "patch": "@@ -449,6 +449,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n         **kwargs,\n     ) -> Union[tuple[torch.FloatTensor], Seq2SeqLMOutput]:\n         r\"\"\"\n@@ -561,6 +562,7 @@ def forward(\n             use_cache=use_cache,\n             past_key_values=past_key_values,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n             **kwargs_decoder,\n         )\n "
        },
        {
            "sha": "fcc47466a3976399f209b2689cfb159c8c6108c9",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccb2e0e03b41429eeede933f38c80e36fcee772f/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccb2e0e03b41429eeede933f38c80e36fcee772f/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=ccb2e0e03b41429eeede933f38c80e36fcee772f",
            "patch": "@@ -1775,6 +1775,7 @@ def test_head_pruning(self):\n             model = model_class(config=config)\n             model.to(torch_device)\n             model.eval()\n+            model.set_attn_implementation(\"eager\")\n             heads_to_prune = {\n                 0: list(range(1, self.model_tester.num_attention_heads)),\n                 -1: [0],\n@@ -1808,6 +1809,7 @@ def test_head_pruning_save_load_from_pretrained(self):\n             model = model_class(config=config)\n             model.to(torch_device)\n             model.eval()\n+            model.set_attn_implementation(\"eager\")\n             heads_to_prune = {\n                 0: list(range(1, self.model_tester.num_attention_heads)),\n                 -1: [0],\n@@ -1816,7 +1818,7 @@ def test_head_pruning_save_load_from_pretrained(self):\n \n             with tempfile.TemporaryDirectory() as temp_dir_name:\n                 model.save_pretrained(temp_dir_name)\n-                model = model_class.from_pretrained(temp_dir_name)\n+                model = model_class.from_pretrained(temp_dir_name, attn_implementation=\"eager\")\n                 model.to(torch_device)\n \n             with torch.no_grad():\n@@ -1852,6 +1854,7 @@ def test_head_pruning_save_load_from_config_init(self):\n             model = model_class(config=config)\n             model.to(torch_device)\n             model.eval()\n+            model.set_attn_implementation(\"eager\")\n \n             with torch.no_grad():\n                 outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n@@ -1884,6 +1887,7 @@ def test_head_pruning_integration(self):\n             model = model_class(config=config)\n             model.to(torch_device)\n             model.eval()\n+            model.set_attn_implementation(\"eager\")\n \n             with torch.no_grad():\n                 outputs = model(**self._prepare_for_class(inputs_dict, model_class))\n@@ -1894,7 +1898,7 @@ def test_head_pruning_integration(self):\n \n             with tempfile.TemporaryDirectory() as temp_dir_name:\n                 model.save_pretrained(temp_dir_name)\n-                model = model_class.from_pretrained(temp_dir_name)\n+                model = model_class.from_pretrained(temp_dir_name, attn_implementation=\"eager\")\n                 model.to(torch_device)\n \n             with torch.no_grad():"
        }
    ],
    "stats": {
        "total": 267,
        "additions": 89,
        "deletions": 178
    }
}