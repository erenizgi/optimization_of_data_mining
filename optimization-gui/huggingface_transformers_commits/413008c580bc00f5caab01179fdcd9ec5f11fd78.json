{
    "author": "molbap",
    "message": "add uniform processors for altclip + chinese_clip (#31198)\n\n* add initial design for uniform processors + align model\r\n\r\n* add uniform processors for altclip + chinese_clip\r\n\r\n* fix mutable default :eyes:\r\n\r\n* add configuration test\r\n\r\n* handle structured kwargs w defaults + add test\r\n\r\n* protect torch-specific test\r\n\r\n* fix style\r\n\r\n* fix\r\n\r\n* rebase\r\n\r\n* update processor to generic kwargs + test\r\n\r\n* fix style\r\n\r\n* add sensible kwargs merge\r\n\r\n* update test\r\n\r\n* fix assertEqual\r\n\r\n* move kwargs merging to processing common\r\n\r\n* rework kwargs for type hinting\r\n\r\n* just get Unpack from extensions\r\n\r\n* run-slow[align]\r\n\r\n* handle kwargs passed as nested dict\r\n\r\n* add from_pretrained test for nested kwargs handling\r\n\r\n* [run-slow]align\r\n\r\n* update documentation + imports\r\n\r\n* update audio inputs\r\n\r\n* protect audio types, silly\r\n\r\n* try removing imports\r\n\r\n* make things simpler\r\n\r\n* simplerer\r\n\r\n* move out kwargs test to common mixin\r\n\r\n* [run-slow]align\r\n\r\n* skip tests for old processors\r\n\r\n* [run-slow]align, clip\r\n\r\n* !$#@!! protect imports, darn it\r\n\r\n* [run-slow]align, clip\r\n\r\n* [run-slow]align, clip\r\n\r\n* update common processor testing\r\n\r\n* add altclip\r\n\r\n* add chinese_clip\r\n\r\n* add pad_size\r\n\r\n* [run-slow]align, clip, chinese_clip, altclip\r\n\r\n* remove duplicated tests\r\n\r\n* fix\r\n\r\n* update doc\r\n\r\n* improve documentation for default values\r\n\r\n* add model_max_length testing\r\n\r\nThis parameter depends on tokenizers received.\r\n\r\n* Raise if kwargs are specified in two places\r\n\r\n* fix\r\n\r\n* match defaults\r\n\r\n* force padding\r\n\r\n* fix tokenizer test\r\n\r\n* clean defaults\r\n\r\n* move tests to common\r\n\r\n* remove try/catch block\r\n\r\n* deprecate kwarg\r\n\r\n* format\r\n\r\n* add copyright + remove unused method\r\n\r\n* [run-slow]altclip, chinese_clip\r\n\r\n* clean imports\r\n\r\n* fix version\r\n\r\n* clean up deprecation\r\n\r\n* fix style\r\n\r\n* add corner case test on kwarg overlap\r\n\r\n* resume processing - add Unpack as importable\r\n\r\n* add tmpdirname\r\n\r\n* fix altclip\r\n\r\n* fix up\r\n\r\n* add back crop_size to specific tests\r\n\r\n* generalize tests to possible video_processor\r\n\r\n* add back crop_size arg\r\n\r\n* fixup overlapping kwargs test for qformer_tokenizer\r\n\r\n* remove copied from\r\n\r\n* fixup chinese_clip tests values\r\n\r\n* fixup tests - qformer tokenizers\r\n\r\n* [run-slow] altclip, chinese_clip\r\n\r\n* remove prepare_image_inputs",
    "sha": "413008c580bc00f5caab01179fdcd9ec5f11fd78",
    "files": [
        {
            "sha": "7cfe14e52b44f914417037b49327cb09717de649",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 1,
            "deletions": 6,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/413008c580bc00f5caab01179fdcd9ec5f11fd78/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413008c580bc00f5caab01179fdcd9ec5f11fd78/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=413008c580bc00f5caab01179fdcd9ec5f11fd78",
            "patch": "@@ -18,16 +18,11 @@\n \n from typing import List, Union\n \n-\n-try:\n-    from typing import Unpack\n-except ImportError:\n-    from typing_extensions import Unpack\n-\n from ...image_utils import ImageInput\n from ...processing_utils import (\n     ProcessingKwargs,\n     ProcessorMixin,\n+    Unpack,\n )\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n "
        },
        {
            "sha": "153ecc2e2bfc87d55302c9b71bb86b800ff0bcae",
            "filename": "src/transformers/models/altclip/processing_altclip.py",
            "status": "modified",
            "additions": 43,
            "deletions": 30,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/413008c580bc00f5caab01179fdcd9ec5f11fd78/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413008c580bc00f5caab01179fdcd9ec5f11fd78/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py?ref=413008c580bc00f5caab01179fdcd9ec5f11fd78",
            "patch": "@@ -16,10 +16,16 @@\n Image/Text processor class for AltCLIP\n \"\"\"\n \n-import warnings\n+from typing import List, Union\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+from ...utils.deprecation import deprecate_kwarg\n+\n+\n+class AltClipProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {}\n \n \n class AltCLIPProcessor(ProcessorMixin):\n@@ -41,25 +47,23 @@ class AltCLIPProcessor(ProcessorMixin):\n     image_processor_class = \"CLIPImageProcessor\"\n     tokenizer_class = (\"XLMRobertaTokenizer\", \"XLMRobertaTokenizerFast\")\n \n-    def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n-        feature_extractor = None\n-        if \"feature_extractor\" in kwargs:\n-            warnings.warn(\n-                \"The `feature_extractor` argument is deprecated and will be removed in v5, use `image_processor`\"\n-                \" instead.\",\n-                FutureWarning,\n-            )\n-            feature_extractor = kwargs.pop(\"feature_extractor\")\n-\n-        image_processor = image_processor if image_processor is not None else feature_extractor\n+    @deprecate_kwarg(old_name=\"feature_extractor\", version=\"5.0.0\", new_name=\"image_processor\")\n+    def __init__(self, image_processor=None, tokenizer=None):\n         if image_processor is None:\n             raise ValueError(\"You need to specify an `image_processor`.\")\n         if tokenizer is None:\n             raise ValueError(\"You need to specify a `tokenizer`.\")\n \n         super().__init__(image_processor, tokenizer)\n \n-    def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n+    def __call__(\n+        self,\n+        images: ImageInput = None,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[AltClipProcessorKwargs],\n+    ) -> BatchEncoding:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to XLMRobertaTokenizerFast's [`~XLMRobertaTokenizerFast.__call__`] if `text` is not\n@@ -68,22 +72,20 @@ def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n         of the above two methods for more information.\n \n         Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n+\n+            images (`ImageInput`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`):\n                 The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n                 (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n                 `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-\n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n-\n+                    - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                    - `'np'`: Return NumPy `np.ndarray` objects.\n+                    - `'jax'`: Return JAX `jnp.ndarray` objects.\n         Returns:\n             [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n \n@@ -95,13 +97,24 @@ def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n         \"\"\"\n \n         if text is None and images is None:\n-            raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n+            raise ValueError(\"You must specify either text or images.\")\n \n-        if text is not None:\n-            encoding = self.tokenizer(text, return_tensors=return_tensors, **kwargs)\n+        if text is None and images is None:\n+            raise ValueError(\"You must specify either text or images.\")\n+        output_kwargs = self._merge_kwargs(\n+            AltClipProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n \n+        if text is not None:\n+            encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         if images is not None:\n-            image_features = self.image_processor(images, return_tensors=return_tensors, **kwargs)\n+            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+\n+        # BC for explicit return_tensors\n+        if \"return_tensors\" in output_kwargs[\"common_kwargs\"]:\n+            return_tensors = output_kwargs[\"common_kwargs\"].pop(\"return_tensors\", None)\n \n         if text is not None and images is not None:\n             encoding[\"pixel_values\"] = image_features.pixel_values"
        },
        {
            "sha": "52349f84bffe0b6a3c058e3390b2f889cc58b0ef",
            "filename": "src/transformers/models/chinese_clip/image_processing_chinese_clip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/413008c580bc00f5caab01179fdcd9ec5f11fd78/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413008c580bc00f5caab01179fdcd9ec5f11fd78/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fimage_processing_chinese_clip.py?ref=413008c580bc00f5caab01179fdcd9ec5f11fd78",
            "patch": "@@ -231,6 +231,7 @@ def preprocess(\n                 - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n                 - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n         \"\"\"\n+\n         do_resize = do_resize if do_resize is not None else self.do_resize\n         size = size if size is not None else self.size\n         size = get_size_dict(size, default_to_square=False)"
        },
        {
            "sha": "2cfd314c64986609faec3320ce04011740d377ee",
            "filename": "src/transformers/models/chinese_clip/processing_chinese_clip.py",
            "status": "modified",
            "additions": 31,
            "deletions": 12,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/413008c580bc00f5caab01179fdcd9ec5f11fd78/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413008c580bc00f5caab01179fdcd9ec5f11fd78/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py?ref=413008c580bc00f5caab01179fdcd9ec5f11fd78",
            "patch": "@@ -17,9 +17,15 @@\n \"\"\"\n \n import warnings\n+from typing import List, Union\n \n-from ...processing_utils import ProcessorMixin\n-from ...tokenization_utils_base import BatchEncoding\n+from ...image_utils import ImageInput\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+\n+\n+class ChineseClipProcessorKwargs(ProcessingKwargs, total=False):\n+    _defaults = {}\n \n \n class ChineseCLIPProcessor(ProcessorMixin):\n@@ -60,7 +66,14 @@ def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n         self.current_processor = self.image_processor\n \n-    def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n+    def __call__(\n+        self,\n+        text: Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]] = None,\n+        images: ImageInput = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[ChineseClipProcessorKwargs],\n+    ) -> BatchEncoding:\n         \"\"\"\n         Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n         and `kwargs` arguments to BertTokenizerFast's [`~BertTokenizerFast.__call__`] if `text` is not `None` to encode\n@@ -79,12 +92,10 @@ def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n \n             return_tensors (`str` or [`~utils.TensorType`], *optional*):\n                 If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'tf'`: Return TensorFlow `tf.constant` objects.\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-                - `'jax'`: Return JAX `jnp.ndarray` objects.\n-\n+                    - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                    - `'np'`: Return NumPy `np.ndarray` objects.\n+                    - `'jax'`: Return JAX `jnp.ndarray` objects.\n         Returns:\n             [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n \n@@ -97,12 +108,20 @@ def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n \n         if text is None and images is None:\n             raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n+        output_kwargs = self._merge_kwargs(\n+            ChineseClipProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n \n         if text is not None:\n-            encoding = self.tokenizer(text, return_tensors=return_tensors, **kwargs)\n-\n+            encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n         if images is not None:\n-            image_features = self.image_processor(images, return_tensors=return_tensors, **kwargs)\n+            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+\n+        # BC for explicit return_tensors\n+        if \"return_tensors\" in output_kwargs[\"common_kwargs\"]:\n+            return_tensors = output_kwargs[\"common_kwargs\"].pop(\"return_tensors\", None)\n \n         if text is not None and images is not None:\n             encoding[\"pixel_values\"] = image_features.pixel_values"
        },
        {
            "sha": "53e83613a07c8f51905a80fe7bf8d232b2dec38d",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/413008c580bc00f5caab01179fdcd9ec5f11fd78/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413008c580bc00f5caab01179fdcd9ec5f11fd78/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=413008c580bc00f5caab01179fdcd9ec5f11fd78",
            "patch": "@@ -20,11 +20,14 @@\n import inspect\n import json\n import os\n+import sys\n+import typing\n import warnings\n from pathlib import Path\n from typing import Any, Dict, List, Optional, Tuple, TypedDict, Union\n \n import numpy as np\n+import typing_extensions\n \n from .dynamic_module_utils import custom_object_save\n from .image_utils import ChannelDimension, is_valid_image, is_vision_available\n@@ -67,6 +70,11 @@\n     \"AutoImageProcessor\": \"ImageProcessingMixin\",\n }\n \n+if sys.version_info >= (3, 11):\n+    Unpack = typing.Unpack\n+else:\n+    Unpack = typing_extensions.Unpack\n+\n \n class TextKwargs(TypedDict, total=False):\n     \"\"\"\n@@ -151,6 +159,8 @@ class methods and docstrings.\n             Standard deviation to use if normalizing the image.\n         do_pad (`bool`, *optional*):\n             Whether to pad the image to the `(max_height, max_width)` of the images in the batch.\n+        pad_size (`Dict[str, int]`, *optional*):\n+            The size `{\"height\": int, \"width\" int}` to pad the images to.\n         do_center_crop (`bool`, *optional*):\n             Whether to center crop the image.\n         data_format (`ChannelDimension` or `str`, *optional*):\n@@ -170,6 +180,7 @@ class methods and docstrings.\n     image_mean: Optional[Union[float, List[float]]]\n     image_std: Optional[Union[float, List[float]]]\n     do_pad: Optional[bool]\n+    pad_size: Optional[Dict[str, int]]\n     do_center_crop: Optional[bool]\n     data_format: Optional[ChannelDimension]\n     input_data_format: Optional[Union[str, ChannelDimension]]\n@@ -814,7 +825,8 @@ class MyProcessingKwargs(ProcessingKwargs, CommonKwargs, TextKwargs, ImagesKwarg\n                     # check if this key was passed as a flat kwarg.\n                     if kwarg_value != \"__empty__\" and modality_key in non_modality_kwargs:\n                         raise ValueError(\n-                            f\"Keyword argument {modality_key} was passed two times: in a dictionary for {modality} and as a **kwarg.\"\n+                            f\"Keyword argument {modality_key} was passed two times:\\n\"\n+                            f\"in a dictionary for {modality} and as a **kwarg.\"\n                         )\n                 elif modality_key in kwargs:\n                     kwarg_value = kwargs.pop(modality_key, \"__empty__\")"
        },
        {
            "sha": "1aca22809694048e51d4cd784f14ef1885d3bce6",
            "filename": "tests/models/altclip/test_processor_altclip.py",
            "status": "added",
            "additions": 165,
            "deletions": 0,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/413008c580bc00f5caab01179fdcd9ec5f11fd78/tests%2Fmodels%2Faltclip%2Ftest_processor_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413008c580bc00f5caab01179fdcd9ec5f11fd78/tests%2Fmodels%2Faltclip%2Ftest_processor_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_processor_altclip.py?ref=413008c580bc00f5caab01179fdcd9ec5f11fd78",
            "patch": "@@ -0,0 +1,165 @@\n+# coding=utf-8\n+# Copyright 2024 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import tempfile\n+import unittest\n+\n+from transformers import XLMRobertaTokenizer, XLMRobertaTokenizerFast\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_vision_available\n+\n+from ...test_processing_common import ProcessorTesterMixin\n+\n+\n+if is_vision_available():\n+    from transformers import AltCLIPProcessor, CLIPImageProcessor\n+\n+\n+@require_vision\n+class AltClipProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = AltCLIPProcessor\n+\n+    def setUp(self):\n+        self.model_id = \"BAAI/AltCLIP\"\n+        self.tmpdirname = tempfile.mkdtemp()\n+        image_processor = CLIPImageProcessor()\n+        tokenizer = XLMRobertaTokenizer.from_pretrained(self.model_id)\n+\n+        processor = self.processor_class(image_processor, tokenizer)\n+\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def get_tokenizer(self, **kwargs):\n+        return XLMRobertaTokenizer.from_pretrained(self.model_id, **kwargs)\n+\n+    def get_rust_tokenizer(self, **kwargs):\n+        return XLMRobertaTokenizerFast.from_pretrained(self.model_id, **kwargs)\n+\n+    def get_image_processor(self, **kwargs):\n+        return CLIPImageProcessor.from_pretrained(self.model_id, **kwargs)\n+\n+    @require_torch\n+    @require_vision\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            crop_size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 7)\n+\n+    def test_structured_kwargs_nested(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"crop_size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    def test_structured_kwargs_nested_from_dict(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"crop_size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    def test_unstructured_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            crop_size={\"height\": 214, \"width\": 214},\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", crop_size=(234, 234))\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 234)"
        },
        {
            "sha": "5b191ce2df0894376958d1e36416933dae079411",
            "filename": "tests/models/chinese_clip/test_processor_chinese_clip.py",
            "status": "modified",
            "additions": 126,
            "deletions": 0,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/413008c580bc00f5caab01179fdcd9ec5f11fd78/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413008c580bc00f5caab01179fdcd9ec5f11fd78/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_processor_chinese_clip.py?ref=413008c580bc00f5caab01179fdcd9ec5f11fd78",
            "patch": "@@ -206,3 +206,129 @@ def test_model_input_names(self):\n         inputs = processor(text=input_str, images=image_input)\n \n         self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n+\n+    def test_unstructured_kwargs_batched(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = [\"lower newer\", \"upper older longer string\"]\n+        image_input = self.prepare_image_inputs() * 2\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            crop_size={\"height\": 214, \"width\": 214},\n+            padding=\"longest\",\n+            max_length=76,\n+        )\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n+\n+    def test_structured_kwargs_nested(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"crop_size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    def test_structured_kwargs_nested_from_dict(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        # Define the kwargs for each modality\n+        all_kwargs = {\n+            \"common_kwargs\": {\"return_tensors\": \"pt\"},\n+            \"images_kwargs\": {\"crop_size\": {\"height\": 214, \"width\": 214}},\n+            \"text_kwargs\": {\"padding\": \"max_length\", \"max_length\": 76},\n+        }\n+\n+        inputs = processor(text=input_str, images=image_input, **all_kwargs)\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    def test_unstructured_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\")\n+        tokenizer = self.get_component(\"tokenizer\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+        inputs = processor(\n+            text=input_str,\n+            images=image_input,\n+            return_tensors=\"pt\",\n+            crop_size={\"height\": 214, \"width\": 214},\n+            padding=\"max_length\",\n+            max_length=76,\n+        )\n+\n+        self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n+        self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    def test_image_processor_defaults_preserved_by_image_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", crop_size=(234, 234))\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input)\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 234)\n+\n+    def test_kwargs_overrides_default_image_processor_kwargs(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        image_processor = self.get_component(\"image_processor\", crop_size=(234, 234))\n+        tokenizer = self.get_component(\"tokenizer\", max_length=117, padding=\"max_length\")\n+\n+        processor = self.processor_class(tokenizer=tokenizer, image_processor=image_processor)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        inputs = processor(text=input_str, images=image_input, crop_size=[224, 224])\n+        self.assertEqual(len(inputs[\"pixel_values\"][0][0]), 224)"
        },
        {
            "sha": "e03e555fed085769b24b0893d10fda4d531509cc",
            "filename": "tests/models/instructblip/test_processor_instructblip.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/413008c580bc00f5caab01179fdcd9ec5f11fd78/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413008c580bc00f5caab01179fdcd9ec5f11fd78/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_processor_instructblip.py?ref=413008c580bc00f5caab01179fdcd9ec5f11fd78",
            "patch": "@@ -409,3 +409,31 @@ def test_structured_kwargs_nested_from_dict(self):\n         self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n \n         self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    def test_overlapping_text_kwargs_handling(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_kwargs = {}\n+        processor_kwargs[\"image_processor\"] = self.get_component(\"image_processor\")\n+        processor_kwargs[\"tokenizer\"] = tokenizer = self.get_component(\"tokenizer\")\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        if \"video_processor\" in self.processor_class.attributes:\n+            processor_kwargs[\"video_processor\"] = self.get_component(\"video_processor\")\n+\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(**processor_kwargs, qformer_tokenizer=qformer_tokenizer)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                padding=\"max_length\",\n+                text_kwargs={\"padding\": \"do_not_pad\"},\n+            )"
        },
        {
            "sha": "8b29c771759217d4fcb3126917266aec71473ed2",
            "filename": "tests/models/instructblipvideo/test_processor_instructblipvideo.py",
            "status": "modified",
            "additions": 28,
            "deletions": 0,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/413008c580bc00f5caab01179fdcd9ec5f11fd78/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413008c580bc00f5caab01179fdcd9ec5f11fd78/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_processor_instructblipvideo.py?ref=413008c580bc00f5caab01179fdcd9ec5f11fd78",
            "patch": "@@ -423,3 +423,31 @@ def test_structured_kwargs_nested_from_dict(self):\n         self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n \n         self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    def test_overlapping_text_kwargs_handling(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_kwargs = {}\n+        processor_kwargs[\"image_processor\"] = self.get_component(\"image_processor\")\n+        processor_kwargs[\"tokenizer\"] = tokenizer = self.get_component(\"tokenizer\")\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        if \"video_processor\" in self.processor_class.attributes:\n+            processor_kwargs[\"video_processor\"] = self.get_component(\"video_processor\")\n+\n+        qformer_tokenizer = self.get_component(\"qformer_tokenizer\")\n+\n+        processor = self.processor_class(**processor_kwargs, qformer_tokenizer=qformer_tokenizer)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                padding=\"max_length\",\n+                text_kwargs={\"padding\": \"do_not_pad\"},\n+            )"
        },
        {
            "sha": "fe17de4eeb5cdfd51c3852692d4edbc84d155d6d",
            "filename": "tests/test_processing_common.py",
            "status": "modified",
            "additions": 27,
            "deletions": 3,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/413008c580bc00f5caab01179fdcd9ec5f11fd78/tests%2Ftest_processing_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/413008c580bc00f5caab01179fdcd9ec5f11fd78/tests%2Ftest_processing_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_processing_common.py?ref=413008c580bc00f5caab01179fdcd9ec5f11fd78",
            "patch": "@@ -146,7 +146,6 @@ def test_tokenizer_defaults_preserved_by_kwargs(self):\n         self.skip_processor_without_typed_kwargs(processor)\n         input_str = \"lower newer\"\n         image_input = self.prepare_image_inputs()\n-\n         inputs = processor(text=input_str, images=image_input, return_tensors=\"pt\")\n         self.assertEqual(len(inputs[\"input_ids\"][0]), 117)\n \n@@ -175,7 +174,6 @@ def test_kwargs_overrides_default_tokenizer_kwargs(self):\n         self.skip_processor_without_typed_kwargs(processor)\n         input_str = \"lower newer\"\n         image_input = self.prepare_image_inputs()\n-\n         inputs = processor(\n             text=input_str, images=image_input, return_tensors=\"pt\", max_length=112, padding=\"max_length\"\n         )\n@@ -238,7 +236,6 @@ def test_unstructured_kwargs_batched(self):\n             padding=\"longest\",\n             max_length=76,\n         )\n-\n         self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n \n         self.assertEqual(len(inputs[\"input_ids\"][0]), 6)\n@@ -311,3 +308,30 @@ def test_structured_kwargs_nested_from_dict(self):\n         self.assertEqual(inputs[\"pixel_values\"].shape[2], 214)\n \n         self.assertEqual(len(inputs[\"input_ids\"][0]), 76)\n+\n+    # TODO: the same test, but for audio + text processors that have strong overlap in kwargs\n+    # TODO (molbap) use the same structure of attribute kwargs for other tests to avoid duplication\n+    def test_overlapping_text_kwargs_handling(self):\n+        if \"image_processor\" not in self.processor_class.attributes:\n+            self.skipTest(f\"image_processor attribute not present in {self.processor_class}\")\n+        processor_kwargs = {}\n+        processor_kwargs[\"image_processor\"] = self.get_component(\"image_processor\")\n+        processor_kwargs[\"tokenizer\"] = tokenizer = self.get_component(\"tokenizer\")\n+        if not tokenizer.pad_token:\n+            tokenizer.pad_token = \"[TEST_PAD]\"\n+        if \"video_processor\" in self.processor_class.attributes:\n+            processor_kwargs[\"video_processor\"] = self.get_component(\"video_processor\")\n+        processor = self.processor_class(**processor_kwargs)\n+        self.skip_processor_without_typed_kwargs(processor)\n+\n+        input_str = \"lower newer\"\n+        image_input = self.prepare_image_inputs()\n+\n+        with self.assertRaises(ValueError):\n+            _ = processor(\n+                text=input_str,\n+                images=image_input,\n+                return_tensors=\"pt\",\n+                padding=\"max_length\",\n+                text_kwargs={\"padding\": \"do_not_pad\"},\n+            )"
        }
    ],
    "stats": {
        "total": 515,
        "additions": 463,
        "deletions": 52
    }
}