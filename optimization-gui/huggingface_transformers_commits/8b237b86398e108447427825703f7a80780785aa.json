{
    "author": "zucchini-nlp",
    "message": "[processors] add tests for helper fn (#39629)\n\n* add tests for helpers\n\n* duplicate test for each model\n\n* why llava next video has no helper\n\n* oops must have been in the commit\n\n* fix test after rebase\n\n* add copy from",
    "sha": "8b237b86398e108447427825703f7a80780785aa",
    "files": [
        {
            "sha": "4d0ae92dd0a7ee6d73db7e9f98dde2c759c3d2f7",
            "filename": "src/transformers/models/aria/image_processing_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fimage_processing_aria.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -515,8 +515,8 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        split_image = images_kwargs.get(\"split_image\", None) or self.split_image\n-        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n+        split_image = images_kwargs[\"split_image\"] if \"split_image\" in images_kwargs else self.split_image\n+        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n \n         resized_height, resized_width = select_best_resolution((height, width), self.split_resolutions)\n         num_patches = 1 if not split_image else resized_height // max_image_size * resized_width // max_image_size"
        },
        {
            "sha": "a531bc43b39b5db8a67a7614fc73a70abe180234",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -901,8 +901,8 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        split_image = images_kwargs.get(\"split_image\", None) or self.split_image\n-        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n+        split_image = images_kwargs[\"split_image\"] if \"split_image\" in images_kwargs else self.split_image\n+        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n \n         resized_height, resized_width = select_best_resolution((height, width), self.split_resolutions)\n         num_patches = 1 if not split_image else resized_height // max_image_size * resized_width // max_image_size"
        },
        {
            "sha": "e0d8118d440427c496a22b18c7f7c27784d55389",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -264,9 +264,8 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n             image_sizes (list[list[str]], *optional*):\n                 The input sizes formatted as (height, width) per each image.\n         Returns:\n-            dict[str, list[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n-            to a list containing the number of placeholder tokens required. If the model doesn't accept\n-            a certain modality or no input sizes are provided, the dict value is set to an empty list.\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n         \"\"\"\n         vision_data = {}\n         if image_sizes is not None:"
        },
        {
            "sha": "5c7bfb2dc068c2b936a773a6dc299605b0bfa3d3",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 27,
            "deletions": 1,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -22,7 +22,7 @@\n from ...cache_utils import Cache\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image\n-from ...processing_utils import ProcessingKwargs, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_torch_available, logging\n from .configuration_colqwen2 import ColQwen2Config\n@@ -224,6 +224,32 @@ def __call__(\n \n             return batch_query\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (`list[list[int]]`, *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = ColQwen2ProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+            merge_size = images_kwargs.get(\"merge_size\", None) or self.image_processor.merge_size\n+\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+            num_image_tokens = [(num_patches // merge_size**2) for num_patches in num_image_patches]\n+            vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n \n class ColQwen2PreTrainedModel(ColPaliPreTrainedModel):\n     pass"
        },
        {
            "sha": "a8b99380ac2adbcf78a5d63261200b2d4d7625f6",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 14,
            "deletions": 7,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -226,20 +226,27 @@ def __call__(\n     def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n         \"\"\"\n         Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n-\n         Args:\n-            image_sizes (list[list[str]], *optional*):\n+            image_sizes (`list[list[int]]`, *optional*):\n                 The input sizes formatted as (height, width) per each image.\n         Returns:\n-            dict[str, list[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n-            to a list containing the number of placeholder tokens required. If the model doesn't accept\n-            a certain modality or no input sizes are provided, the dict value is set to an empty list.\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n         \"\"\"\n+\n         vision_data = {}\n         if image_sizes is not None:\n-            num_image_tokens = [self.image_seq_length] * len(image_sizes)\n-            num_image_patches = [1] * len(image_sizes)\n+            images_kwargs = ColQwen2ProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+            merge_size = images_kwargs.get(\"merge_size\", None) or self.image_processor.merge_size\n+\n+            num_image_patches = [\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n+                for image_size in image_sizes\n+            ]\n+            num_image_tokens = [(num_patches // merge_size**2) for num_patches in num_image_patches]\n             vision_data.update({\"num_image_tokens\": num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n         return MultiModalData(**vision_data)\n \n     def batch_decode(self, *args, **kwargs):"
        },
        {
            "sha": "2b4f9aa24bc52a982ef10e41a662f2ecafefbf20",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -449,8 +449,8 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of image patches per image.\n         \"\"\"\n-        patch_size = images_kwargs.get(\"patch_size\", None) or self.patch_size\n-        merge_size = images_kwargs.get(\"merge_size\", None) or self.merge_size\n+        patch_size = images_kwargs[\"patch_size\"] if \"patch_size\" in images_kwargs else self.patch_size\n+        merge_size = images_kwargs[\"merge_size\"] if \"merge_size\" in images_kwargs else self.merge_size\n \n         factor = patch_size * merge_size\n         resized_height, resized_width = smart_resize("
        },
        {
            "sha": "a1f0eca4cc0af997eb3c2dd5ec4de84bd8e1d837",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -505,10 +505,12 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        min_patches = images_kwargs.get(\"min_patches\", None) or self.min_patches\n-        max_patches = images_kwargs.get(\"max_patches\", None) or self.max_patches\n-        patch_size = images_kwargs.get(\"size\", None) or self.size\n-        crop_to_patches = images_kwargs.get(\"crop_to_patches\", None) or self.crop_to_patches\n+        min_patches = images_kwargs[\"min_patches\"] if \"min_patches\" in images_kwargs else self.min_patches\n+        max_patches = images_kwargs[\"max_patches\"] if \"max_patches\" in images_kwargs else self.max_patches\n+        patch_size = images_kwargs[\"patch_size\"] if \"patch_size\" in images_kwargs else self.size\n+        crop_to_patches = (\n+            images_kwargs[\"crop_to_patches\"] if \"crop_to_patches\" in images_kwargs else self.crop_to_patches\n+        )\n \n         num_patches = 1\n         if crop_to_patches and max_patches > 1:"
        },
        {
            "sha": "04cf09fe39e568831aaf841906f427976d9d4a85",
            "filename": "src/transformers/models/got_ocr2/image_processing_got_ocr2_fast.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fimage_processing_got_ocr2_fast.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -223,7 +223,7 @@ def _preprocess(\n             data={\"pixel_values\": processed_images, \"num_patches\": num_patches}, tensor_type=return_tensors\n         )\n \n-    def get_number_of_image_tokens(self, height: int, width: int, images_kwargs=None):\n+    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n         \"\"\"\n         A utility that returns number patches for a given image size.\n \n@@ -237,10 +237,12 @@ def get_number_of_image_tokens(self, height: int, width: int, images_kwargs=None\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        min_patches = images_kwargs.get(\"min_patches\", None) or self.min_patches\n-        max_patches = images_kwargs.get(\"max_patches\", None) or self.max_patches\n-        patch_size = images_kwargs.get(\"size\", None) or self.size\n-        crop_to_patches = images_kwargs.get(\"crop_to_patches\", None) or self.crop_to_patches\n+        min_patches = images_kwargs[\"min_patches\"] if \"min_patches\" in images_kwargs else self.min_patches\n+        max_patches = images_kwargs[\"max_patches\"] if \"max_patches\" in images_kwargs else self.max_patches\n+        patch_size = images_kwargs[\"patch_size\"] if \"patch_size\" in images_kwargs else self.size\n+        crop_to_patches = (\n+            images_kwargs[\"crop_to_patches\"] if \"crop_to_patches\" in images_kwargs else self.crop_to_patches\n+        )\n \n         num_patches = 1\n         if crop_to_patches and max_patches > 1:"
        },
        {
            "sha": "194dd092bb3b6c3006fec6f3872e4f66441b1351",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -866,9 +866,11 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        do_image_splitting = images_kwargs.get(\"do_image_splitting\", None) or self.do_image_splitting\n-        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n-        size = images_kwargs.get(\"size\", None) or self.size\n+        do_image_splitting = (\n+            images_kwargs[\"do_image_splitting\"] if \"do_image_splitting\" in images_kwargs else self.do_image_splitting\n+        )\n+        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n+        size = images_kwargs[\"size\"] if \"size\" in images_kwargs else self.size\n \n         num_patches = num_rows = num_cols = 1\n         if do_image_splitting:"
        },
        {
            "sha": "a2251e785372064f7bffa2cf08622710d8734223",
            "filename": "src/transformers/models/idefics3/image_processing_idefics3_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fimage_processing_idefics3_fast.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -514,9 +514,11 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        do_image_splitting = images_kwargs.get(\"do_image_splitting\", None) or self.do_image_splitting\n-        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n-        size = images_kwargs.get(\"size\", None) or self.size\n+        do_image_splitting = (\n+            images_kwargs[\"do_image_splitting\"] if \"do_image_splitting\" in images_kwargs else self.do_image_splitting\n+        )\n+        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n+        size = images_kwargs[\"size\"] if \"size\" in images_kwargs else self.size\n \n         num_patches = num_rows = num_cols = 1\n         if do_image_splitting:"
        },
        {
            "sha": "61f6f044821b3f6f6377b39870d9b6a4896385ff",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -284,7 +284,7 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n             images_kwargs.update(kwargs)\n \n             num_image_patches = [\n-                self.image_processor.get_number_of_image_tokens(*image_size, images_kwargs)\n+                self.image_processor.get_number_of_image_patches(*image_size, images_kwargs)\n                 for image_size in image_sizes\n             ]\n             # Add 2 for BOI and EOI tokens"
        },
        {
            "sha": "92d8d56618ed5587c44d53c877f262953ff328af",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -231,14 +231,9 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n         Args:\n             image_sizes (list[list[str]], *optional*):\n                 The input sizes formatted as (height, width) per each image.\n-            video_sizes (list[list[str]], *optional*):\n-                The input sizes formatted as (num_frames, height, width) per each video.\n-            audio_lengths (list[int], *optional*):\n-                The input length formatted as per each audio.\n         Returns:\n-            dict[str, list[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n-            to a list containing the number of placeholder tokens required. If the model doesn't accept\n-            a certain modality or no input sizes are provided, the dict value is set to an empty list.\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n         \"\"\"\n         vision_data = {}\n         if image_sizes is not None:"
        },
        {
            "sha": "e04c968c193be57af2e08e096fdf481a1d0750bb",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 38,
            "deletions": 1,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -23,7 +23,7 @@\n from ...feature_extraction_utils import BatchFeature\n from ...image_processing_utils import select_best_resolution\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n from ...video_utils import VideoInput\n@@ -265,6 +265,43 @@ def _get_unpadded_features(self, height, width, patches_height, patches_width, s\n         newline_features = current_height\n         return (unpadded_features, newline_features)\n \n+    def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n+        \"\"\"\n+        Computes the number of placeholder tokens needed for multimodal inputs with the given sizes.\n+        Args:\n+            image_sizes (list[list[str]], *optional*):\n+                The input sizes formatted as (height, width) per each image.\n+        Returns:\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n+        \"\"\"\n+        vision_data = {}\n+        if image_sizes is not None:\n+            images_kwargs = LlavaNextVideoProcessorKwargs._defaults.get(\"images_kwargs\", {})\n+            images_kwargs.update(kwargs)\n+\n+            size = images_kwargs.get(\"size\", None) or self.image_processor.size\n+            size = (\n+                (size[\"shortest_edge\"], size[\"shortest_edge\"])\n+                if \"shortest_edge\" in size\n+                else (min(size[\"height\"], size[\"width\"]), min(size[\"height\"], size[\"width\"]))\n+            )\n+            processed_height, processed_width = size\n+\n+            batch_num_image_tokens = []\n+            num_image_patches = [1] * len(image_sizes)  # llava-next doesn't batch pixels as Idefics, thus `1` patch`\n+            for image_size in image_sizes:\n+                orig_height, orig_width = image_size\n+                num_image_tokens = self._get_number_of_features(\n+                    orig_height, orig_width, processed_height, processed_width\n+                )\n+                if self.vision_feature_select_strategy == \"default\":\n+                    num_image_tokens -= 1\n+                batch_num_image_tokens.append(num_image_tokens)\n+            vision_data.update({\"num_image_tokens\": batch_num_image_tokens, \"num_image_patches\": num_image_patches})\n+\n+        return MultiModalData(**vision_data)\n+\n     # Copied from transformers.models.clip.processing_clip.CLIPProcessor.batch_decode with CLIP->Llama\n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "e9629b2d2f56275353da2ce6c3c8d4f5821cd4bd",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -327,9 +327,8 @@ def _get_num_multimodal_tokens(self, image_sizes=None, **kwargs):\n             image_sizes (list[list[str]], *optional*):\n                 The input sizes formatted as (height, width) per each image.\n         Returns:\n-            dict[str, list[int]]: A dictionary mapping each modality (\"image\", \"video\", \"audio\")\n-            to a list containing the number of placeholder tokens required. If the model doesn't accept\n-            a certain modality or no input sizes are provided, the dict value is set to an empty list.\n+            `MultiModalData`: A `MultiModalData` object holding number of tokens per each of the provided\n+            input modalities, along with other useful data.\n         \"\"\"\n         vision_data = {}\n         if image_sizes is not None:"
        },
        {
            "sha": "e42eeeef2092507ed8342c77649cbb095a50de96",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -502,10 +502,10 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of image patches per image.\n         \"\"\"\n-        min_pixels = images_kwargs.get(\"min_pixels\", None) or self.size[\"shortest_edge\"]\n-        max_pixels = images_kwargs.get(\"max_pixels\", None) or self.size[\"longest_edge\"]\n-        patch_size = images_kwargs.get(\"patch_size\", None) or self.patch_size\n-        merge_size = images_kwargs.get(\"merge_size\", None) or self.merge_size\n+        min_pixels = images_kwargs[\"min_pixels\"] if \"min_pixels\" in images_kwargs else self.size[\"shortest_edge\"]\n+        max_pixels = images_kwargs[\"max_pixels\"] if \"max_pixels\" in images_kwargs else self.size[\"longest_edge\"]\n+        patch_size = images_kwargs[\"patch_size\"] if \"patch_size\" in images_kwargs else self.patch_size\n+        merge_size = images_kwargs[\"merge_size\"] if \"merge_size\" in images_kwargs else self.merge_size\n \n         factor = patch_size * merge_size\n         resized_height, resized_width = smart_resize("
        },
        {
            "sha": "cadecfbf3ffb0c988abb598c2565b47ca920db2b",
            "filename": "src/transformers/models/qwen2_vl/image_processing_qwen2_vl_fast.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fimage_processing_qwen2_vl_fast.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -299,10 +299,10 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of image patches per image.\n         \"\"\"\n-        min_pixels = images_kwargs.get(\"min_pixels\", None) or self.size[\"shortest_edge\"]\n-        max_pixels = images_kwargs.get(\"max_pixels\", None) or self.size[\"longest_edge\"]\n-        patch_size = images_kwargs.get(\"patch_size\", None) or self.patch_size\n-        merge_size = images_kwargs.get(\"merge_size\", None) or self.merge_size\n+        min_pixels = images_kwargs[\"min_pixels\"] if \"min_pixels\" in images_kwargs else self.size[\"shortest_edge\"]\n+        max_pixels = images_kwargs[\"max_pixels\"] if \"max_pixels\" in images_kwargs else self.size[\"longest_edge\"]\n+        patch_size = images_kwargs[\"patch_size\"] if \"patch_size\" in images_kwargs else self.patch_size\n+        merge_size = images_kwargs[\"merge_size\"] if \"merge_size\" in images_kwargs else self.merge_size\n \n         factor = patch_size * merge_size\n         resized_height, resized_width = smart_resize("
        },
        {
            "sha": "440f263d0aa7ce7b66bb41068111c4bcf065bb7e",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -863,9 +863,11 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        do_image_splitting = images_kwargs.get(\"do_image_splitting\", None) or self.do_image_splitting\n-        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n-        size = images_kwargs.get(\"size\", None) or self.size\n+        do_image_splitting = (\n+            images_kwargs[\"do_image_splitting\"] if \"do_image_splitting\" in images_kwargs else self.do_image_splitting\n+        )\n+        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n+        size = images_kwargs[\"size\"] if \"size\" in images_kwargs else self.size\n \n         num_patches = num_rows = num_cols = 1\n         if do_image_splitting:"
        },
        {
            "sha": "ecbd3a7e07e38b59c6e00fce10a3b1683a75072d",
            "filename": "src/transformers/models/smolvlm/image_processing_smolvlm_fast.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fimage_processing_smolvlm_fast.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -504,9 +504,11 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n         Returns:\n             `int`: Number of patches per image.\n         \"\"\"\n-        do_image_splitting = images_kwargs.get(\"do_image_splitting\", None) or self.do_image_splitting\n-        max_image_size = images_kwargs.get(\"max_image_size\", None) or self.max_image_size\n-        size = images_kwargs.get(\"size\", None) or self.size\n+        do_image_splitting = (\n+            images_kwargs[\"do_image_splitting\"] if \"do_image_splitting\" in images_kwargs else self.do_image_splitting\n+        )\n+        max_image_size = images_kwargs[\"max_image_size\"] if \"max_image_size\" in images_kwargs else self.max_image_size\n+        size = images_kwargs[\"size\"] if \"size\" in images_kwargs else self.size\n \n         num_patches = num_rows = num_cols = 1\n         if do_image_splitting:"
        },
        {
            "sha": "7974a27129acd471744ed0f5c439ec9bce17cadf",
            "filename": "tests/models/aria/test_image_processing_aria.py",
            "status": "modified",
            "additions": 16,
            "deletions": 0,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_image_processing_aria.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -302,3 +302,19 @@ def test_pad_for_patching(self):\n                 encoded_images.shape[:-1] if input_data_format == ChannelDimension.LAST else encoded_images.shape[1:]\n             )\n             self.assertEqual(encoded_image_shape, image_shape)\n+\n+    def test_get_num_patches_without_images(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            num_patches = image_processing.get_number_of_image_patches(height=100, width=100, images_kwargs={})\n+            self.assertEqual(num_patches, 1)\n+\n+            num_patches = image_processing.get_number_of_image_patches(\n+                height=300, width=500, images_kwargs={\"split_image\": True}\n+            )\n+            self.assertEqual(num_patches, 1)\n+\n+            num_patches = image_processing.get_number_of_image_patches(\n+                height=100, width=100, images_kwargs={\"split_image\": True, \"max_image_size\": 200}\n+            )\n+            self.assertEqual(num_patches, 19)"
        },
        {
            "sha": "4c228d3c16fef5bca800a4f88f5eb22a720e6324",
            "filename": "tests/models/aria/test_processor_aria.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Faria%2Ftest_processor_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_processor_aria.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -95,6 +95,19 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     def test_process_interleaved_images_prompts_image_splitting(self):\n         processor = self.get_processor()\n         processor.image_processor.split_image = True"
        },
        {
            "sha": "b768f08a03794a41e3cc1d5563f41c0bb85dbb07",
            "filename": "tests/models/aya_vision/test_processor_aya_vision.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_processor_aya_vision.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -80,6 +80,19 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     @require_torch\n     def test_process_interleaved_images_videos(self):\n         processor = self.get_processor()"
        },
        {
            "sha": "57f3b810af0d58cc4f9f089755bdaafe454860fb",
            "filename": "tests/models/chameleon/test_processor_chameleon.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_processor_chameleon.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -74,3 +74,16 @@ def test_special_mm_token_truncation(self):\n     @staticmethod\n     def prepare_processor_dict():\n         return {\"image_seq_length\": 2}  # fmt: skip\n+\n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)"
        },
        {
            "sha": "539b604a3518a52aad40fd12da4b33e85748b331",
            "filename": "tests/models/colpali/test_processing_colpali.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_processing_colpali.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -54,6 +54,19 @@ def setUpClass(cls):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     @require_torch\n     @require_vision\n     def test_process_images(self):"
        },
        {
            "sha": "25e6b523c88a6118d80729be8ebc8c0db7764299",
            "filename": "tests/models/colqwen2/test_processing_colqwen2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolqwen2%2Ftest_processing_colqwen2.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -57,6 +57,19 @@ def get_image_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname)\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     def test_process_images(self):\n         # Processor configuration\n         image_input = self.prepare_image_inputs()"
        },
        {
            "sha": "bb7c8187e5a7b8d79cc1afd5d5540ec61333f663",
            "filename": "tests/models/emu3/test_processor_emu3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_processor_emu3.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -90,3 +90,16 @@ def test_processor_postprocess(self):\n \n         # For an image where pixels go from 0 to 255 the diff can be 1 due to some numerical precision errors when scaling and unscaling\n         self.assertTrue(np.abs(orig_image - unnormalized_images).max() >= 1)\n+\n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)"
        },
        {
            "sha": "6fb935cbec538507bd1a3241760c5c434d629aed",
            "filename": "tests/models/fuyu/test_processor_fuyu.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Ffuyu%2Ftest_processor_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Ffuyu%2Ftest_processor_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_processor_fuyu.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -64,6 +64,19 @@ def get_tokenizer(self, **kwargs):\n     def get_image_processor(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).image_processor\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     def test_fuyu_processing(self):\n         \"\"\"\n         Test to ensure that the standard processing on a gold example matches adept's code."
        },
        {
            "sha": "98984a3c08dbe05fef32c6368a1a82c8c64d1682",
            "filename": "tests/models/gemma3/test_processing_gemma3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_processing_gemma3.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -58,6 +58,19 @@ def setUpClass(cls):\n         processor.save_pretrained(cls.tmpdirname)\n         cls.image_token = processor.boi_token\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     @classmethod\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)"
        },
        {
            "sha": "ccfcf2f0628ab593c718a4f4c8a06072bac10af7",
            "filename": "tests/models/got_ocr2/test_image_processing_got_ocr2.py",
            "status": "modified",
            "additions": 21,
            "deletions": 0,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_image_processing_got_ocr2.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -169,3 +169,24 @@ def test_crop_to_patches(self):\n         )\n         self.assertEqual(len(processed_images[0]), 5)\n         self.assertEqual(processed_images.shape[-2:], (20, 20))\n+\n+    def test_get_num_patches_without_images(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            num_patches = image_processing.get_number_of_image_patches(height=100, width=100, images_kwargs={})\n+            self.assertEqual(num_patches, 1)\n+\n+            num_patches = image_processing.get_number_of_image_patches(\n+                height=300, width=500, images_kwargs={\"crop_to_patches\": False}\n+            )\n+            self.assertEqual(num_patches, 1)\n+\n+            num_patches = image_processing.get_number_of_image_patches(\n+                height=100, width=100, images_kwargs={\"crop_to_patches\": True}\n+            )\n+            self.assertEqual(num_patches, 10)\n+\n+            num_patches = image_processing.get_number_of_image_patches(\n+                height=100, width=100, images_kwargs={\"crop_to_patches\": True, \"max_patches\": 200}\n+            )\n+            self.assertEqual(num_patches, 50)"
        },
        {
            "sha": "f855de282485dc7c5e2d5f31bd514f6bd8e9ad73",
            "filename": "tests/models/idefics3/test_image_processing_idefics3.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -358,3 +358,28 @@ def test_slow_fast_equivalence_batched(self):\n         )\n         self.assertEqual(encoding_slow.rows, encoding_fast.rows)\n         self.assertEqual(encoding_slow.cols, encoding_fast.cols)\n+\n+    def test_get_num_patches_without_images(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            num_patches_and_row_cols = image_processing.get_number_of_image_patches(\n+                height=100, width=100, images_kwargs={}\n+            )\n+            self.assertEqual(num_patches_and_row_cols, (5, 2, 2))\n+\n+            num_patches_and_row_cols = image_processing.get_number_of_image_patches(\n+                height=300, width=500, images_kwargs={\"do_image_splitting\": False}\n+            )\n+            self.assertEqual(num_patches_and_row_cols, (1, 1, 1))\n+\n+            num_patches_and_row_cols = image_processing.get_number_of_image_patches(\n+                height=300, width=500, images_kwargs={\"do_image_splitting\": True}\n+            )\n+            self.assertEqual(num_patches_and_row_cols, (5, 2, 2))\n+\n+            num_patches_and_row_cols = image_processing.get_number_of_image_patches(\n+                height=300,\n+                width=600,\n+                images_kwargs={\"do_image_splitting\": True, \"max_image_size\": {\"longest_edge\": 30}},\n+            )\n+            self.assertEqual(num_patches_and_row_cols, (3, 1, 2))"
        },
        {
            "sha": "7020a2439838fb2c985e085e96a35b38d54a879b",
            "filename": "tests/models/idefics3/test_processor_idefics3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_processor_idefics3.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -84,6 +84,19 @@ def get_processor(self, **kwargs):\n     def prepare_processor_dict():\n         return {\"image_seq_len\": 2}\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     def get_split_image_expected_tokens(self, processor, image_rows, image_cols):\n         text_split_images = []\n         for n_h in range(image_rows):"
        },
        {
            "sha": "f3340e8af093599701d71d6b30688009e65cc3ee",
            "filename": "tests/models/internvl/test_processor_internvl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_processor_internvl.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -97,6 +97,19 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     @require_av\n     @require_torch\n     def test_process_interleaved_images_videos(self):"
        },
        {
            "sha": "41b9d8a09e499f82cce99084118e9fa331a3e3b7",
            "filename": "tests/models/llava/test_processor_llava.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_processor_llava.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -61,6 +61,18 @@ def prepare_processor_dict():\n             \"vision_feature_select_strategy\": \"default\"\n         }  # fmt: skip\n \n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)\n         processor_dict_loaded = json.loads(processor_loaded.to_json_string())"
        },
        {
            "sha": "d6156adb75192690a02e47319be04df7e74e375f",
            "filename": "tests/models/llava_next/test_processor_llava_next.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_processor_llava_next.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -66,6 +66,19 @@ def prepare_processor_dict():\n             \"vision_feature_select_strategy\": \"default\"\n         }  # fmt: skip\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)"
        },
        {
            "sha": "17bcb3657dd04889fe93018d47adfba0c4ffabee",
            "filename": "tests/models/llava_next_video/test_processor_llava_next_video.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_processor_llava_next_video.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -75,6 +75,19 @@ def prepare_processor_dict(cls):\n             \"vision_feature_select_strategy\": \"default\",\n         }\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)"
        },
        {
            "sha": "1eb3b0d0d470901783cd71f9406b2bb7adcf5ffc",
            "filename": "tests/models/llava_onevision/test_processor_llava_onevision.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_processor_llava_onevision.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -79,6 +79,19 @@ def prepare_processor_dict():\n             \"vision_feature_select_strategy\": \"default\"\n         }  # fmt: skip\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_chat_template_is_saved\n     def test_chat_template_is_saved(self):\n         processor_loaded = self.processor_class.from_pretrained(self.tmpdirname)"
        },
        {
            "sha": "821e18d550d95811cbe003d728ca39da274dcfaf",
            "filename": "tests/models/paligemma/test_processor_paligemma.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_processor_paligemma.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -48,6 +48,19 @@ def setUpClass(cls):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     @require_torch\n     @require_vision\n     def test_image_seq_length(self):"
        },
        {
            "sha": "c3f478950f4f3805783d6fbfed7df446ae2ea6c2",
            "filename": "tests/models/qwen2_5_vl/test_processor_qwen2_5_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processor_qwen2_5_vl.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -65,6 +65,19 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     def test_save_load_pretrained_default(self):\n         tokenizer = self.get_tokenizer()\n         image_processor = self.get_image_processor()"
        },
        {
            "sha": "d17cd690cffc472569228292acf48dd24b8115e6",
            "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 14,
            "deletions": 0,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -394,3 +394,17 @@ def test_slow_fast_equivalence_batched(self):\n         self._assert_slow_fast_tensors_equivalence(\n             encoding_slow.image_grid_thw.float(), encoding_fast.image_grid_thw.float()\n         )\n+\n+    def test_get_num_patches_without_images(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            num_patches = image_processing.get_number_of_image_patches(height=100, width=100, images_kwargs={})\n+            self.assertEqual(num_patches, 64)\n+\n+            num_patches = image_processing.get_number_of_image_patches(height=200, width=50, images_kwargs={})\n+            self.assertEqual(num_patches, 56)\n+\n+            num_patches = image_processing.get_number_of_image_patches(\n+                height=100, width=100, images_kwargs={\"patch_size\": 28}\n+            )\n+            self.assertEqual(num_patches, 16)"
        },
        {
            "sha": "69fae59595592e186edea88a3578a6e61c789e43",
            "filename": "tests/models/qwen2_vl/test_processor_qwen2_vl.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processor_qwen2_vl.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -68,6 +68,19 @@ def get_processor(self, **kwargs):\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n+    # Copied from tests.models.llava.test_processor_llava.LlavaProcessorTest.test_get_num_vision_tokens\n+    def test_get_num_vision_tokens(self):\n+        \"Tests general functionality of the helper used internally in vLLM\"\n+\n+        processor = self.get_processor()\n+\n+        output = processor._get_num_multimodal_tokens(image_sizes=[(100, 100), (300, 100), (500, 30)])\n+        self.assertTrue(\"num_image_tokens\" in output)\n+        self.assertEqual(len(output[\"num_image_tokens\"]), 3)\n+\n+        self.assertTrue(\"num_image_patches\" in output)\n+        self.assertEqual(len(output[\"num_image_patches\"]), 3)\n+\n     def test_save_load_pretrained_default(self):\n         tokenizer = self.get_tokenizer()\n         image_processor = self.get_image_processor()"
        },
        {
            "sha": "be687e79527a4ff08ad35b52ae8585634012a1ce",
            "filename": "tests/models/smolvlm/test_image_processing_smolvlm.py",
            "status": "modified",
            "additions": 25,
            "deletions": 0,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fsmolvlm%2Ftest_image_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b237b86398e108447427825703f7a80780785aa/tests%2Fmodels%2Fsmolvlm%2Ftest_image_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_image_processing_smolvlm.py?ref=8b237b86398e108447427825703f7a80780785aa",
            "patch": "@@ -358,3 +358,28 @@ def test_slow_fast_equivalence_batched(self):\n         )\n         self.assertEqual(encoding_slow.rows, encoding_fast.rows)\n         self.assertEqual(encoding_slow.cols, encoding_fast.cols)\n+\n+    def test_get_num_patches_without_images(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            num_patches_and_row_cols = image_processing.get_number_of_image_patches(\n+                height=100, width=100, images_kwargs={}\n+            )\n+            self.assertEqual(num_patches_and_row_cols, (5, 2, 2))\n+\n+            num_patches_and_row_cols = image_processing.get_number_of_image_patches(\n+                height=300, width=500, images_kwargs={\"do_image_splitting\": False}\n+            )\n+            self.assertEqual(num_patches_and_row_cols, (1, 1, 1))\n+\n+            num_patches_and_row_cols = image_processing.get_number_of_image_patches(\n+                height=300, width=500, images_kwargs={\"do_image_splitting\": True}\n+            )\n+            self.assertEqual(num_patches_and_row_cols, (5, 2, 2))\n+\n+            num_patches_and_row_cols = image_processing.get_number_of_image_patches(\n+                height=300,\n+                width=600,\n+                images_kwargs={\"do_image_splitting\": True, \"max_image_size\": {\"longest_edge\": 30}},\n+            )\n+            self.assertEqual(num_patches_and_row_cols, (3, 1, 2))"
        }
    ],
    "stats": {
        "total": 512,
        "additions": 454,
        "deletions": 58
    }
}