{
    "author": "Cyrilvallez",
    "message": "Delete bad rebasing functions (#39672)\n\n* remove outdated stuff\n\n* remove comment\n\n* use register\n\n* remove finally clause (to allow further check if fallback to sdpa)\n\n* general exception\n\n* add wrapper\n\n* revert check\n\n* typo",
    "sha": "ddb0546d145c2f944d94444ec8327571908c280b",
    "files": [
        {
            "sha": "2c47965099be66542166f5426fc70b8a09aeb5ff",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 107,
            "changes": 113,
            "blob_url": "https://github.com/huggingface/transformers/blob/ddb0546d145c2f944d94444ec8327571908c280b/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ddb0546d145c2f944d94444ec8327571908c280b/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ddb0546d145c2f944d94444ec8327571908c280b",
            "patch": "@@ -2094,8 +2094,6 @@ class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToH\n     _supports_attention_backend = False\n     _can_record_outputs = None\n \n-    # This attribute sets the default parameter to be\n-\n     @property\n     @torch._dynamo.allow_in_graph\n     def can_record_outputs(self) -> dict[str, OutputRecorder]:\n@@ -2359,101 +2357,6 @@ def _from_config(cls, config, **kwargs):\n \n         return model\n \n-    @classmethod\n-    def _check_attn_implementation(cls, attn_implementation: Union[str, dict]) -> Union[str, dict]:\n-        \"\"\"\n-        Checks that the requested attention implementation exists and tries to get the kernel from hub\n-        if `attn_implementation` matches hf kernels pattern.\n-        \"\"\"\n-        if isinstance(attn_implementation, str) and re.match(r\"^[^/:]+/[^/:]+:[^/:]+$\", attn_implementation):\n-            if not is_kernels_available():\n-                raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n-\n-            # Extract repo_id and kernel_name from the string\n-            repo_id, kernel_name = attn_implementation.split(\":\")\n-            kernel_name = kernel_name.strip()\n-            repo_id = repo_id.strip()\n-\n-            try:\n-                kernel = get_kernel(repo_id)\n-                ALL_ATTENTION_FUNCTIONS.register(f\"kernel_{repo_id.replace('/', '_')}\", getattr(kernel, kernel_name))\n-                attn_implementation = f\"kernel_{repo_id.replace('/', '_')}\"\n-            except FileNotFoundError as e:\n-                logger.warning(\n-                    f\"Could not find a kernel repository '{repo_id}' compatible with your devicein the hub: {e}. Using eager attention implementation instead.\"\n-                )\n-                attn_implementation = None  # try to dispatch SDPA and fallback eager if not available\n-            except AttributeError:\n-                raise ValueError(\n-                    \"the kernel function name or class specified in the attn_implementation argument is not valid. \\\n-                                 Please check the documentation for the correct format, \\\n-                                 and check that the kernel exports the class and the function correctly.\"\n-                )\n-        if (\n-            not isinstance(attn_implementation, dict)\n-            and attn_implementation not in [\"eager\", None] + ALL_ATTENTION_FUNCTIONS.valid_keys()\n-        ):\n-            message = f'Specified `attn_implementation=\"{attn_implementation}\"` is not supported. The only possible arguments are `attn_implementation=\"eager\"` (manual attention implementation)'\n-            # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n-            if cls._supports_flash_attn or getattr(cls, \"_supports_flash_attn_2\", False):\n-                message += (\n-                    ', `\"attn_implementation=flash_attention_3\"` (implementation using flash attention 3)'\n-                    ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n-                )\n-            if cls._supports_sdpa:\n-                message += ', `\"attn_implementation=sdpa\"` (implementation using torch.nn.functional.scaled_dot_product_attention)'\n-            if cls._supports_flex_attn:\n-                message += ', `\"attn_implementation=flex_attention\"` (implementation using torch\\'s flex_attention)'\n-            raise ValueError(message + \".\")\n-\n-        return attn_implementation\n-\n-    def set_attention_implementation(self, attn_implementation: Union[str, dict]):\n-        \"\"\"\n-        Checks and dispatches to the requested attention implementation.\n-        \"\"\"\n-        requested_attn_implementation = self._check_attn_implementation(attn_implementation)\n-\n-        # Composite models consisting of several PretrainedModels can specify attention implementation as a dict where\n-        # keys are sub-config names. But most people will specify one `str` which means that should dispatch it for all sub-models.\n-        # See https://github.com/huggingface/transformers/pull/32238\n-        for key in self.config.sub_configs.keys():\n-            sub_config = getattr(self.config, key)\n-            curr_attn_implementation = (\n-                requested_attn_implementation\n-                if not isinstance(requested_attn_implementation, dict)\n-                else requested_attn_implementation.get(key, None)\n-            )\n-            # For models with backbone sub-config might be not initialized. Set the requested att\n-            # if the config hasn't got any attn pre-set and the requested attn in not `None` (i.e not the default attn)\n-            if (\n-                sub_config is not None\n-                and sub_config._attn_implementation_internal is None\n-                and curr_attn_implementation is not None\n-            ):\n-                sub_config._attn_implementation_internal = curr_attn_implementation\n-\n-        if requested_attn_implementation == \"flash_attention_3\" and self._flash_attn_3_can_dispatch():\n-            self.config._attn_implementation = \"flash_attention_3\"\n-        if requested_attn_implementation == \"flash_attention_2\" and self._flash_attn_2_can_dispatch():\n-            self.config._attn_implementation = \"flash_attention_2\"\n-        elif requested_attn_implementation == \"flex_attention\" and self._flex_attn_can_dispatch():\n-            self.config._attn_implementation = \"flex_attention\"\n-        elif (\n-            requested_attn_implementation in [None, \"sdpa\"]\n-            and not is_torch_xla_available()\n-            and self._sdpa_can_dispatch(hard_check_only=requested_attn_implementation is not None)\n-        ):\n-            self.config._attn_implementation = \"sdpa\"\n-        elif requested_attn_implementation in ALL_ATTENTION_FUNCTIONS.valid_keys():\n-            self.config._attn_implementation = requested_attn_implementation\n-        elif isinstance(requested_attn_implementation, dict):\n-            self.config._attn_implementation = requested_attn_implementation.get(\"\", None)\n-        else:\n-            self.config._attn_implementation = \"eager\"\n-\n-        self.config._attn_implementation_autoset = True\n-\n     @classmethod\n     def _set_default_torch_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n         \"\"\"\n@@ -2800,23 +2703,19 @@ def _check_and_adjust_attn_implementation(\n             try:\n                 kernel = get_kernel(repo_id)\n                 if hasattr(kernel, \"flash_attn_varlen_func\"):\n-                    ALL_ATTENTION_FUNCTIONS._global_mapping[repo_id] = partial(\n-                        flash_attention_forward, implementation=kernel\n-                    )\n+                    kernel_function = partial(flash_attention_forward, implementation=kernel)\n                 elif kernel_name is not None:\n-                    ALL_ATTENTION_FUNCTIONS[repo_id] = getattr(kernel, kernel_name)\n-                ALL_MASK_ATTENTION_FUNCTIONS._global_mapping[repo_id] = ALL_MASK_ATTENTION_FUNCTIONS[\n-                    \"flash_attention_2\"\n-                ]\n+                    kernel_function = getattr(kernel, kernel_name)\n+                # Register it\n+                ALL_ATTENTION_FUNCTIONS.register(repo_id, kernel_function)\n+                ALL_MASK_ATTENTION_FUNCTIONS.register(repo_id, ALL_MASK_ATTENTION_FUNCTIONS[\"flash_attention_2\"])\n                 applicable_attn_implementation = repo_id\n-            except FileNotFoundError as e:\n+            except Exception as e:\n                 logger.warning_once(\n                     f\"Could not find a kernel repository '{repo_id}' compatible with your device in the hub: {e}. Using \"\n                     \"default attention implementation instead (sdpa if available, eager otherwise).\"\n                 )\n                 applicable_attn_implementation = \"sdpa\"  # Try to fallback to sdpa in this case\n-            finally:\n-                return applicable_attn_implementation\n         if applicable_attn_implementation not in [\"eager\"] + ALL_ATTENTION_FUNCTIONS.valid_keys():\n             message = (\n                 f'Specified `attn_implementation=\"{attn_implementation}\"` is not supported. The only possible arguments are '"
        }
    ],
    "stats": {
        "total": 113,
        "additions": 6,
        "deletions": 107
    }
}