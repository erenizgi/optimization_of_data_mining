{
    "author": "tiger-of-shawn",
    "message": "Qwen2.5-Omni: Update modeling_qwen2_5_omni.py to fix error when loading quantized weights with AutoAWQ.  (#38013)\n\n* Update modular_qwen2_5_omni.py\n\nfix the error when loading quantized model by AuotAWQ.\n\n* Update modeling_qwen2_5_omni.py\n\nsync code to modular_qwen2_5_omni.py",
    "sha": "dbc4b91db4eff5d660ae212d87b7657674b0b484",
    "files": [
        {
            "sha": "92be4356e81da45d6fb31139d0963894c78d99e9",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbc4b91db4eff5d660ae212d87b7657674b0b484/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbc4b91db4eff5d660ae212d87b7657674b0b484/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=dbc4b91db4eff5d660ae212d87b7657674b0b484",
            "patch": "@@ -127,8 +127,10 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            if module.weight is not None:\n+                module.weight.data.fill_(1.0)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n         elif isinstance(module, Qwen2RMSNorm):\n             module.weight.data.fill_(1.0)\n "
        },
        {
            "sha": "ce228ef73a131a50886fdbfb0e4254ae3fc8e326",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbc4b91db4eff5d660ae212d87b7657674b0b484/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbc4b91db4eff5d660ae212d87b7657674b0b484/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=dbc4b91db4eff5d660ae212d87b7657674b0b484",
            "patch": "@@ -1062,8 +1062,10 @@ def _init_weights(self, module):\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()\n         elif isinstance(module, nn.LayerNorm):\n-            module.weight.data.fill_(1.0)\n-            module.bias.data.zero_()\n+            if module.weight is not None:\n+                module.weight.data.fill_(1.0)\n+            if module.bias is not None:\n+                module.bias.data.zero_()\n         elif isinstance(module, Qwen2RMSNorm):\n             module.weight.data.fill_(1.0)\n "
        }
    ],
    "stats": {
        "total": 12,
        "additions": 8,
        "deletions": 4
    }
}