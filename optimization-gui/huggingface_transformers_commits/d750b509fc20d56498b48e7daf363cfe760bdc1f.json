{
    "author": "gante",
    "message": "Config: unified logic to retrieve text config (#33219)",
    "sha": "d750b509fc20d56498b48e7daf363cfe760bdc1f",
    "files": [
        {
            "sha": "a69da1a3eafb27f37ee7115472aad11b8736690b",
            "filename": ".circleci/parse_test_outputs.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d750b509fc20d56498b48e7daf363cfe760bdc1f/.circleci%2Fparse_test_outputs.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d750b509fc20d56498b48e7daf363cfe760bdc1f/.circleci%2Fparse_test_outputs.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.circleci%2Fparse_test_outputs.py?ref=d750b509fc20d56498b48e7daf363cfe760bdc1f",
            "patch": "@@ -67,4 +67,4 @@ def main():\n \n \n if __name__ == \"__main__\":\n-    main()\n\\ No newline at end of file\n+    main()"
        },
        {
            "sha": "b07d2224af99ceef6e3a46e17967779e90ae42f4",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 35,
            "deletions": 5,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=d750b509fc20d56498b48e7daf363cfe760bdc1f",
            "patch": "@@ -1019,17 +1019,17 @@ def _get_non_default_generation_parameters(self) -> Dict[str, Any]:\n         \"\"\"\n         non_default_generation_parameters = {}\n         decoder_attribute_name = None\n-        default_config = None\n \n         # Composite models don't have a default config, use their decoder config as a fallback for default values\n         # If no known pattern is matched, then `default_config = None` -> check against the global generation defaults\n         try:\n             default_config = self.__class__()\n         except ValueError:\n-            for decoder_attribute_name in (\"decoder\", \"generator\", \"text_config\"):\n-                if hasattr(self, decoder_attribute_name):\n-                    default_config = getattr(self, decoder_attribute_name).__class__()\n-                    break\n+            decoder_config = self.get_text_config(decoder=True)\n+            if decoder_config is not self:\n+                default_config = decoder_config.__class__()\n+            else:\n+                decoder_config = None\n \n         # If it is a composite model, we want to check the subconfig that will be used for generation\n         self_decoder_config = self if decoder_attribute_name is None else getattr(self, decoder_attribute_name)\n@@ -1057,6 +1057,36 @@ def _get_non_default_generation_parameters(self) -> Dict[str, Any]:\n \n         return non_default_generation_parameters\n \n+    def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n+        \"\"\"\n+        Returns the config that is meant to be used with text IO. On most models, it is the original config instance\n+        itself. On specific composite models, it is under a set of valid names.\n+\n+        If `decoder` is set to `True`, then only search for decoder config names.\n+        \"\"\"\n+        decoder_possible_text_config_names = (\"decoder\", \"generator\", \"text_config\")\n+        encoder_possible_text_config_names = (\"text_encoder\",)\n+        if decoder:\n+            possible_text_config_names = decoder_possible_text_config_names\n+        else:\n+            possible_text_config_names = encoder_possible_text_config_names + decoder_possible_text_config_names\n+\n+        valid_text_config_names = []\n+        for text_config_name in possible_text_config_names:\n+            if hasattr(self, text_config_name):\n+                text_config = getattr(self, text_config_name, None)\n+                if text_config is not None:\n+                    valid_text_config_names += [text_config_name]\n+\n+        if len(valid_text_config_names) > 1:\n+            raise ValueError(\n+                f\"Multiple valid text configs were found in the model config: {valid_text_config_names}. In this \"\n+                \"case, using `get_text_config()` would be ambiguous. Please specify the desied text config directly.\"\n+            )\n+        elif len(valid_text_config_names) == 1:\n+            return getattr(self, valid_text_config_names[0])\n+        return self\n+\n \n def get_configuration_file(configuration_files: List[str]) -> str:\n     \"\"\""
        },
        {
            "sha": "df4d028bede5415deece56e67e1460f90ac6cc75",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 20,
            "deletions": 15,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=d750b509fc20d56498b48e7daf363cfe760bdc1f",
            "patch": "@@ -1192,25 +1192,30 @@ def from_model_config(cls, model_config: PretrainedConfig) -> \"GenerationConfig\"\n         \"\"\"\n         config_dict = model_config.to_dict()\n         config_dict.pop(\"_from_model_config\", None)\n-        config = cls.from_dict(config_dict, return_unused_kwargs=False, _from_model_config=True)\n+        generation_config = cls.from_dict(config_dict, return_unused_kwargs=False, _from_model_config=True)\n \n         # Special case: some models have generation attributes set in the decoder. Use them if still unset in the\n-        # generation config.\n-        for decoder_name in (\"decoder\", \"generator\", \"text_config\"):\n-            if decoder_name in config_dict:\n-                default_generation_config = GenerationConfig()\n-                decoder_config = config_dict[decoder_name]\n-                for attr in config.to_dict().keys():\n-                    if attr in decoder_config and getattr(config, attr) == getattr(default_generation_config, attr):\n-                        setattr(config, attr, decoder_config[attr])\n+        # generation config (which in turn is defined from the outer attributes of model config).\n+        decoder_config = model_config.get_text_config(decoder=True)\n+        if decoder_config is not model_config:\n+            default_generation_config = GenerationConfig()\n+            decoder_config_dict = decoder_config.to_dict()\n+            for attr in generation_config.to_dict().keys():\n+                is_unset = getattr(generation_config, attr) == getattr(default_generation_config, attr)\n+                if attr in decoder_config_dict and is_unset:\n+                    setattr(generation_config, attr, decoder_config_dict[attr])\n \n         # If any `output_...` flag is set to `True`, we ensure `return_dict_in_generate` is set to `True`.\n-        if config.return_dict_in_generate is False:\n-            if any(getattr(config, extra_output_flag, False) for extra_output_flag in config.extra_output_flags):\n-                config.return_dict_in_generate = True\n-\n-        config._original_object_hash = hash(config)  # Hash to detect whether the instance was modified\n-        return config\n+        if generation_config.return_dict_in_generate is False:\n+            if any(\n+                getattr(generation_config, extra_output_flag, False)\n+                for extra_output_flag in generation_config.extra_output_flags\n+            ):\n+                generation_config.return_dict_in_generate = True\n+\n+        # Hash to detect whether the instance was modified\n+        generation_config._original_object_hash = hash(generation_config)\n+        return generation_config\n \n     def update(self, **kwargs):\n         \"\"\""
        },
        {
            "sha": "18e1931d070d6a90f835c5a3c4ee0aa03eb0a203",
            "filename": "src/transformers/integrations/awq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fintegrations%2Fawq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fawq.py?ref=d750b509fc20d56498b48e7daf363cfe760bdc1f",
            "patch": "@@ -209,10 +209,7 @@ def get_modules_to_fuse(model, quantization_config):\n         current_fused_mapping = AWQ_FUSED_MAPPINGS[model.config.model_type]\n \n         # Properly deal with the case where we have a multi-modal model as well (e.g. Llava)\n-        if not hasattr(model.config, \"text_config\"):\n-            config = model.config\n-        else:\n-            config = model.config.text_config\n+        config = model.config.get_text_config(decoder=True)\n \n         # Handle hidden_size, num_attention_heads, num_key_value_heads on our own.\n         hidden_size = config.hidden_size\n@@ -345,11 +342,8 @@ def _fuse_awq_mlp(model, current_module_name, fuse_module_names, module, target_\n         previous_device = gate_proj.qweight.device\n \n         # Deal also with the case model has `text_config` attribute\n-        hidden_act = (\n-            model.config.hidden_act\n-            if not hasattr(model.config, \"text_config\")\n-            else model.config.text_config.hidden_act\n-        )\n+        config = model.config.get_text_config(decoder=True)\n+        hidden_act = config.hidden_act\n         activation_fn = ACT2FN[hidden_act]\n         new_module = target_cls(gate_proj, down_proj, up_proj, activation_fn)\n "
        },
        {
            "sha": "eb0e61e26fac5208bc4b661f24d9582f7a787952",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 5,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=d750b509fc20d56498b48e7daf363cfe760bdc1f",
            "patch": "@@ -2025,11 +2025,8 @@ def resize_token_embeddings(\n         else:\n             vocab_size = model_embeds.weight.shape[0]\n \n-        # Update base model and current model config\n-        if hasattr(self.config, \"text_config\"):\n-            self.config.text_config.vocab_size = vocab_size\n-        else:\n-            self.config.vocab_size = vocab_size\n+        # Update base model and current model config.\n+        self.config.get_text_config().vocab_size = vocab_size\n         self.vocab_size = vocab_size\n \n         # Tie weights again if needed"
        },
        {
            "sha": "479b0fac2b049064c6e90ebd5e988e6e8b5d472f",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=d750b509fc20d56498b48e7daf363cfe760bdc1f",
            "patch": "@@ -735,7 +735,7 @@ def _init_weights(self, module):\n             nn.init.normal_(module.fc1.proj.weight if getattr(module.fc1, \"proj\") else module.fc1.weight, std=fc_std)\n             nn.init.normal_(module.fc2.weight, std=in_proj_std)\n         elif isinstance(module, ClvpEncoder):\n-            config = self.config.text_config if hasattr(self.config, \"text_config\") else self.config\n+            config = self.config.get_text_config()\n             factor = config.initializer_factor\n             module.projection.weight.data.normal_(mean=0.0, std=factor * (config.hidden_size**-0.5))\n         elif isinstance(module, ClvpConditioningEncoder):"
        },
        {
            "sha": "666b26984a4999186ed5c8c72d80ffff0056162b",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d750b509fc20d56498b48e7daf363cfe760bdc1f/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=d750b509fc20d56498b48e7daf363cfe760bdc1f",
            "patch": "@@ -1330,7 +1330,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n-        num_logits_to_keep=0,\n+        num_logits_to_keep=None,\n         **kwargs,\n     ):\n         # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n@@ -1381,14 +1381,16 @@ def prepare_inputs_for_generation(\n                 batch_size=batch_size,\n             )\n \n+        if num_logits_to_keep is not None:\n+            model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n+\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,\n                 \"cache_position\": cache_position,\n                 \"past_key_values\": past_key_values,\n                 \"use_cache\": use_cache,\n                 \"attention_mask\": attention_mask,\n-                \"num_logits_to_keep\": num_logits_to_keep,\n             }\n         )\n         return model_inputs"
        },
        {
            "sha": "a3bc7ca705e4740825558cdd959cdcdd9af34b17",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d750b509fc20d56498b48e7daf363cfe760bdc1f/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d750b509fc20d56498b48e7daf363cfe760bdc1f/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=d750b509fc20d56498b48e7daf363cfe760bdc1f",
            "patch": "@@ -831,7 +831,7 @@ def test_constrained_beam_search_generate(self):\n \n             # Sample constraints\n             min_id = 3\n-            max_id = config.vocab_size\n+            max_id = config.get_text_config(decoder=True).vocab_size\n \n             force_tokens = torch.randint(min_id, max_id, (1, 2)).tolist()[0]\n             constraints = [\n@@ -889,7 +889,7 @@ def test_constrained_beam_search_generate_dict_output(self):\n \n             # Sample constraints\n             min_id = 3\n-            max_id = model.config.vocab_size\n+            max_id = model.config.get_text_config(decoder=True).vocab_size\n             force_tokens = torch.randint(min_id, max_id, (1, 2)).tolist()[0]\n             constraints = [\n                 PhrasalConstraint(force_tokens),\n@@ -2012,18 +2012,20 @@ def _check_outputs(self, output, input_ids, config, use_cache=False, num_return_\n                 self.assertTrue(output.past_key_values is None)\n \n     def _check_scores(self, batch_size, scores, length, config):\n-        expected_shape = (batch_size, config.vocab_size)\n+        vocab_size = config.get_text_config(decoder=True).vocab_size\n+        expected_shape = (batch_size, vocab_size)\n         self.assertIsInstance(scores, tuple)\n         self.assertEqual(len(scores), length)\n         self.assertListEqual([iter_scores.shape for iter_scores in scores], [expected_shape] * len(scores))\n \n     def _check_logits(self, batch_size, scores, config):\n+        vocab_size = config.get_text_config(decoder=True).vocab_size\n         self.assertIsInstance(scores, tuple)\n         self.assertListEqual([iter_scores.shape[0] for iter_scores in scores], [batch_size] * len(scores))\n         # vocabulary difference equal to one (imagegptmodel?) or zero (all other models)\n-        vocab_diff = config.vocab_size - scores[0].shape[-1]\n+        vocab_diff = vocab_size - scores[0].shape[-1]\n         self.assertTrue(vocab_diff in [0, 1])\n-        self.assertListEqual([config.vocab_size - score.shape[-1] for score in scores], [vocab_diff] * len(scores))\n+        self.assertListEqual([vocab_size - score.shape[-1] for score in scores], [vocab_diff] * len(scores))\n \n     def _check_attentions_for_generate(\n         self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1"
        },
        {
            "sha": "6a0afc60f8556758e4763db1e4e28064fbce1791",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 16,
            "deletions": 41,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/d750b509fc20d56498b48e7daf363cfe760bdc1f/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d750b509fc20d56498b48e7daf363cfe760bdc1f/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=d750b509fc20d56498b48e7daf363cfe760bdc1f",
            "patch": "@@ -1747,12 +1747,13 @@ def test_resize_position_vector_embeddings(self):\n             self.assertTrue(models_equal)\n \n     def test_resize_tokens_embeddings(self):\n+        if not self.test_resize_embeddings:\n+            self.skipTest(reason=\"test_resize_embeddings is set to `False`\")\n+\n         (\n             original_config,\n             inputs_dict,\n         ) = self.model_tester.prepare_config_and_inputs_for_common()\n-        if not self.test_resize_embeddings:\n-            self.skipTest(reason=\"test_resize_embeddings is set to `False`\")\n \n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n@@ -1764,18 +1765,15 @@ def test_resize_tokens_embeddings(self):\n             if self.model_tester.is_training is False:\n                 model.eval()\n \n-            model_vocab_size = config.text_config.vocab_size if hasattr(config, \"text_config\") else config.vocab_size\n+            model_vocab_size = config.get_text_config().vocab_size\n             # Retrieve the embeddings and clone theme\n             model_embed = model.resize_token_embeddings(model_vocab_size)\n             cloned_embeddings = model_embed.weight.clone()\n \n             # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n             model_embed = model.resize_token_embeddings(model_vocab_size + 10)\n-            new_model_vocab_size = (\n-                model.config.text_config.vocab_size\n-                if hasattr(model.config, \"text_config\")\n-                else model.config.vocab_size\n-            )\n+            new_model_vocab_size = model.config.get_text_config().vocab_size\n+\n             self.assertEqual(new_model_vocab_size, model_vocab_size + 10)\n             # Check that it actually resizes the embeddings matrix\n             self.assertEqual(model_embed.weight.shape[0], cloned_embeddings.shape[0] + 10)\n@@ -1787,11 +1785,7 @@ def test_resize_tokens_embeddings(self):\n \n             # Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size\n             model_embed = model.resize_token_embeddings(model_vocab_size - 15)\n-            new_model_vocab_size = (\n-                model.config.text_config.vocab_size\n-                if hasattr(model.config, \"text_config\")\n-                else model.config.vocab_size\n-            )\n+            new_model_vocab_size = model.config.get_text_config().vocab_size\n             self.assertEqual(new_model_vocab_size, model_vocab_size - 15)\n             # Check that it actually resizes the embeddings matrix\n             self.assertEqual(model_embed.weight.shape[0], cloned_embeddings.shape[0] - 15)\n@@ -1817,21 +1811,13 @@ def test_resize_tokens_embeddings(self):\n             model = model_class(config)\n             model.to(torch_device)\n \n-            model_vocab_size = config.text_config.vocab_size if hasattr(config, \"text_config\") else config.vocab_size\n+            model_vocab_size = config.get_text_config().vocab_size\n             model.resize_token_embeddings(model_vocab_size + 10, pad_to_multiple_of=1)\n-            new_model_vocab_size = (\n-                model.config.text_config.vocab_size\n-                if hasattr(model.config, \"text_config\")\n-                else model.config.vocab_size\n-            )\n+            new_model_vocab_size = model.config.get_text_config().vocab_size\n             self.assertTrue(new_model_vocab_size + 10, model_vocab_size)\n \n             model_embed = model.resize_token_embeddings(model_vocab_size, pad_to_multiple_of=64)\n-            new_model_vocab_size = (\n-                model.config.text_config.vocab_size\n-                if hasattr(model.config, \"text_config\")\n-                else model.config.vocab_size\n-            )\n+            new_model_vocab_size = model.config.get_text_config().vocab_size\n             self.assertTrue(model_embed.weight.shape[0] // 64, 0)\n \n             self.assertTrue(model_embed.weight.shape[0], new_model_vocab_size)\n@@ -1852,13 +1838,10 @@ def test_resize_tokens_embeddings(self):\n                 model.resize_token_embeddings(model_vocab_size, pad_to_multiple_of=1.3)\n \n     def test_resize_embeddings_untied(self):\n-        (\n-            original_config,\n-            inputs_dict,\n-        ) = self.model_tester.prepare_config_and_inputs_for_common()\n         if not self.test_resize_embeddings:\n             self.skipTest(reason=\"test_resize_embeddings is set to `False`\")\n \n+        original_config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         original_config.tie_word_embeddings = False\n \n         # if model cannot untied embeddings -> leave test\n@@ -1874,13 +1857,9 @@ def test_resize_embeddings_untied(self):\n                 continue\n \n             # Check that resizing the token embeddings with a larger vocab size increases the model's vocab size\n-            model_vocab_size = config.text_config.vocab_size if hasattr(config, \"text_config\") else config.vocab_size\n+            model_vocab_size = config.get_text_config().vocab_size\n             model.resize_token_embeddings(model_vocab_size + 10)\n-            new_model_vocab_size = (\n-                model.config.text_config.vocab_size\n-                if hasattr(model.config, \"text_config\")\n-                else model.config.vocab_size\n-            )\n+            new_model_vocab_size = model.config.get_text_config().vocab_size\n             self.assertEqual(new_model_vocab_size, model_vocab_size + 10)\n             output_embeds = model.get_output_embeddings()\n             self.assertEqual(output_embeds.weight.shape[0], model_vocab_size + 10)\n@@ -1892,11 +1871,7 @@ def test_resize_embeddings_untied(self):\n \n             # Check that resizing the token embeddings with a smaller vocab size decreases the model's vocab size\n             model.resize_token_embeddings(model_vocab_size - 15)\n-            new_model_vocab_size = (\n-                model.config.text_config.vocab_size\n-                if hasattr(model.config, \"text_config\")\n-                else model.config.vocab_size\n-            )\n+            new_model_vocab_size = model.config.get_text_config().vocab_size\n             self.assertEqual(new_model_vocab_size, model_vocab_size - 15)\n             # Check that it actually resizes the embeddings matrix\n             output_embeds = model.get_output_embeddings()\n@@ -1988,7 +1963,7 @@ def check_same_values(layer_1, layer_2):\n             # self.assertTrue(check_same_values(embeddings, decoding))\n \n             # Check that after resize they remain tied.\n-            vocab_size = config.text_config.vocab_size if hasattr(config, \"text_config\") else config.vocab_size\n+            vocab_size = config.get_text_config().vocab_size\n             model_tied.resize_token_embeddings(vocab_size + 10)\n             params_tied_2 = list(model_tied.parameters())\n             self.assertEqual(len(params_tied_2), len(params_tied))\n@@ -4831,7 +4806,7 @@ def test_forward_with_num_logits_to_keep(self):\n \n             config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n             batch_size, sequence_length = inputs[\"input_ids\"].shape\n-            vocab_size = config.vocab_size\n+            vocab_size = config.get_text_config().vocab_size\n             model = model_class(config).to(device=torch_device).eval()\n \n             # num_logits_to_keep=0 is a special case meaning \"keep all logits\""
        },
        {
            "sha": "64ce4381b2712b51d450beb6b5f3181f2332278b",
            "filename": "tests/test_pipeline_mixin.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/d750b509fc20d56498b48e7daf363cfe760bdc1f/tests%2Ftest_pipeline_mixin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d750b509fc20d56498b48e7daf363cfe760bdc1f/tests%2Ftest_pipeline_mixin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_pipeline_mixin.py?ref=d750b509fc20d56498b48e7daf363cfe760bdc1f",
            "patch": "@@ -675,14 +675,12 @@ def validate_test_components(test_case, task, model, tokenizer, processor):\n     # Avoid `IndexError` in embedding layers\n     CONFIG_WITHOUT_VOCAB_SIZE = [\"CanineConfig\"]\n     if tokenizer is not None:\n-        config_vocab_size = getattr(model.config, \"vocab_size\", None)\n+        # Removing `decoder=True` in `get_text_config` can lead to conflicting values e.g. in MusicGen\n+        config_vocab_size = getattr(model.config.get_text_config(decoder=True), \"vocab_size\", None)\n         # For CLIP-like models\n         if config_vocab_size is None:\n-            if hasattr(model.config, \"text_config\"):\n+            if hasattr(model.config, \"text_encoder\"):\n                 config_vocab_size = getattr(model.config.text_config, \"vocab_size\", None)\n-            elif hasattr(model.config, \"text_encoder\"):\n-                config_vocab_size = getattr(model.config.text_encoder, \"vocab_size\", None)\n-\n         if config_vocab_size is None and model.config.__class__.__name__ not in CONFIG_WITHOUT_VOCAB_SIZE:\n             raise ValueError(\n                 \"Could not determine `vocab_size` from model configuration while `tokenizer` is not `None`.\""
        }
    ],
    "stats": {
        "total": 181,
        "additions": 92,
        "deletions": 89
    }
}