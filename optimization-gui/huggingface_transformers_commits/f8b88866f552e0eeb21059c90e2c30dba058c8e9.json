{
    "author": "zucchini-nlp",
    "message": "[VLMs] support passing embeds along with pixels (#38467)\n\n* VLMs can work with embeds now\n\n* update more models\n\n* fix tests\n\n* fix copies\n\n* fixup\n\n* fix\n\n* style\n\n* unskip tests\n\n* fix copies\n\n* fix tests\n\n* style\n\n* omni modality models\n\n* qwen models had extra indentation\n\n* fix some other tests\n\n* fix copies\n\n* fix test last time\n\n* unrelated changes revert\n\n* we can't rely only on embeds\n\n* delete file\n\n* de-flake mistral3\n\n* fix qwen models\n\n* fix style\n\n* fix tests\n\n* fix copies\n\n* deflake the test\n\n* modular reverted by fixes, fix again\n\n* flaky test, overwritten\n\n* fix copies\n\n* style",
    "sha": "f8b88866f552e0eeb21059c90e2c30dba058c8e9",
    "files": [
        {
            "sha": "de6ae44bb5a16ee693baad1f191c31c701044ec8",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -733,7 +733,9 @@ def _prepare_model_inputs(\n         # - encoder-decoder models should complain if the user attempts to pass `inputs_embeds` and `input_ids`, and\n         # pull the former to inputs. It will be used in place of `input_ids` to get the encoder hidden states.\n         if input_name == \"input_ids\" and \"inputs_embeds\" in model_kwargs:\n-            if not self.config.is_encoder_decoder:\n+            if model_kwargs[\"inputs_embeds\"] is None:\n+                model_kwargs.pop(\"inputs_embeds\")\n+            elif not self.config.is_encoder_decoder:\n                 has_inputs_embeds_forwarding = \"inputs_embeds\" in set(\n                     inspect.signature(self.prepare_inputs_for_generation).parameters.keys()\n                 )\n@@ -748,10 +750,11 @@ def _prepare_model_inputs(\n                 model_kwargs[\"input_ids\"] = self._maybe_initialize_input_ids_for_generation(\n                     inputs, bos_token_id, model_kwargs=model_kwargs\n                 )\n+                inputs, input_name = model_kwargs[\"inputs_embeds\"], \"inputs_embeds\"\n             else:\n                 if inputs is not None:\n                     raise ValueError(\"You passed `inputs_embeds` and `input_ids` to `.generate()`. Please pick one.\")\n-            inputs, input_name = model_kwargs[\"inputs_embeds\"], \"inputs_embeds\"\n+                inputs, input_name = model_kwargs[\"inputs_embeds\"], \"inputs_embeds\"\n \n         # 4. if `inputs` is still None, try to create `input_ids` from BOS token\n         inputs = self._maybe_initialize_input_ids_for_generation(inputs, bos_token_id, model_kwargs)"
        },
        {
            "sha": "00b912c5b274f0df6fe04b4553cb9d6939f32c43",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1113,11 +1113,12 @@ def forward(\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n                     torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+                special_image_mask = special_image_mask.all(-1)\n             else:\n-                image_embeds = input_ids == self.config.image_token_id\n-                special_image_mask = image_embeds.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-                n_image_tokens = (image_embeds).sum(dim=1).sum(dim=0)\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = self.get_image_features(\n                 pixel_values=pixel_values,\n                 pixel_mask=pixel_mask,"
        },
        {
            "sha": "a40041a82bb7edbaf85cd959a86cd62d8cb9851e",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1446,11 +1446,12 @@ def forward(\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n                     torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+                special_image_mask = special_image_mask.all(-1)\n             else:\n-                image_embeds = input_ids == self.config.image_token_id\n-                special_image_mask = image_embeds.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n-                n_image_tokens = (image_embeds).sum(dim=1).sum(dim=0)\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = self.get_image_features(\n                 pixel_values=pixel_values,\n                 pixel_mask=pixel_mask,"
        },
        {
            "sha": "4983a0dcca7d3e47210d788dc7edc4d2acd1d146",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -302,14 +302,14 @@ def forward(\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n                     torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+                special_image_mask = special_image_mask.all(-1)\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "93e7e3184a10688677d4ffe48312bede5b96dd7c",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -223,14 +223,14 @@ def forward(\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n                     torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+                special_image_mask = special_image_mask.all(-1)\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "c81b990b7dee95a59a1d20224f2a17e2ea6d0808",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 41,
            "deletions": 13,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1855,6 +1855,7 @@ class Blip2ForConditionalGeneration(Blip2PreTrainedModel, GenerationMixin):\n     _supports_cache_class = True\n     _supports_static_cache = True\n     _supports_quantized_cache = False  # not all LM bacbones support (e.g. T5)\n+\n     _keep_in_fp32_modules = [\"query_tokens\", \"qformer\"]\n \n     def __init__(self, config: Blip2Config):\n@@ -1971,10 +1972,11 @@ def get_image_features(\n     def forward(\n         self,\n         pixel_values: torch.FloatTensor,\n-        input_ids: torch.FloatTensor,\n+        input_ids: torch.LongTensor,\n         attention_mask: Optional[torch.LongTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2066,14 +2068,25 @@ def forward(\n         language_model_attention_mask = torch.ones(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n-        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n         # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n-        # otherwise we expand manually by concating\n+        # otherwise we expand manually by concatenating\n         if getattr(self.config, \"image_token_id\", None) is not None:\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n         else:\n@@ -2146,6 +2159,7 @@ def generate(\n         pixel_values: torch.FloatTensor,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         interpolate_pos_encoding: bool = False,\n         **generate_kwargs,\n     ) -> torch.LongTensor:\n@@ -2159,6 +2173,10 @@ def generate(\n                 The sequence used as a prompt for the generation.\n             attention_mask (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n                 Mask to avoid performing attention on padding token indices\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Embedded representation of the inputs. Should be float, not int tokens.\n+            interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+                Whether to interpolate the positional encoding of the image embeddings.\n \n         Returns:\n             captions (list): A list of strings of length batch_size * num_captions.\n@@ -2193,22 +2211,32 @@ def generate(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n \n-        if input_ids is None:\n-            start_tokens = [self.config.text_config.bos_token_id]\n-            if getattr(self.config, \"image_token_id\", None) is not None:\n-                start_tokens = [self.config.image_token_id] * self.config.num_query_tokens + start_tokens\n-            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n-            input_ids = input_ids.repeat(batch_size, 1)\n+        if inputs_embeds is None:\n+            if input_ids is None:\n+                start_tokens = [self.config.text_config.bos_token_id]\n+                if getattr(self.config, \"image_token_id\", None) is not None:\n+                    start_tokens = [self.config.image_token_id] * self.config.num_query_tokens + start_tokens\n+                input_ids = torch.tensor([start_tokens], dtype=torch.long, device=image_embeds.device)\n+                input_ids = input_ids.repeat(batch_size, 1)\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-        inputs_embeds = self.get_input_embeddings()(input_ids)\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n         # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"image_token_id\", None) is not None:\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in BLIP-2 should be done in processing. \""
        },
        {
            "sha": "010fe244de937d44e5fcdd3a8d278a0a9f623b9a",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 17,
            "deletions": 14,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -963,25 +963,28 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n+        if inputs_embeds is None:\n+            inputs_embeds = self.embed_tokens(input_ids)\n \n         if pixel_values is not None:\n-            image_tokens = self.get_image_tokens(pixel_values)\n-            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n-            if not is_torchdynamo_compiling() and input_ids[special_image_mask].numel() != image_tokens.numel():\n-                n_image_tokens_in_text = (input_ids == self.vocabulary_mapping.image_token_id).sum()\n-                n_image_features = image_tokens.shape[0] * image_tokens.shape[1]\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.vocabulary_mapping.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n+\n+            n_image_tokens_in_text = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n+            image_embeds = self.get_image_features(pixel_values)\n+            if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_embeds.numel():\n+                n_image_features = image_embeds.shape[0] * image_embeds.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens_in_text}, features {n_image_features}\"\n                 )\n-            image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n-            input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n-\n-        if inputs_embeds is None:\n-            inputs_embeds = self.embed_tokens(input_ids)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_embeds)\n \n         # torch.jit.trace() doesn't support cache objects in the output\n         if use_cache and past_key_values is None and not torch.jit.is_tracing():"
        },
        {
            "sha": "422838603c0c76463c7d15ac7b1122b6ccd498e0",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1537,20 +1537,26 @@ def forward(\n                 \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n             )\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n-            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n-            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n-            image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n-            input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n+            image_embeds = self.get_image_features(pixel_values, image_sizes)\n+            image_embeds = torch.cat(image_embeds, dim=0)\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.vocabulary_mapping.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_embeds)\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs = self.text_model(\n-            input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,"
        },
        {
            "sha": "bfb22a969078dcd46c83f627c5e7340be3e27dd8",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 15,
            "deletions": 9,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1033,20 +1033,26 @@ def forward(\n                 \"You cannot specify both input_ids and inputs_embeds at the same time, and must specify either one\"\n             )\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n-            image_tokens = self.get_image_tokens(pixel_values, image_sizes)\n-            special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n-            image_tokens = image_tokens.to(input_ids.device, input_ids.dtype)\n-            input_ids = input_ids.masked_scatter(special_image_mask, image_tokens)\n+            image_embeds = self.get_image_features(pixel_values, image_sizes)\n+            image_embeds = torch.cat(image_embeds, dim=0)\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.vocabulary_mapping.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.vocabulary_mapping.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_embeds)\n \n         # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n         outputs = self.text_model(\n-            input_ids=input_ids,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,"
        },
        {
            "sha": "56ba62133f4e8332035b411fa1a2f15c7c7f96b8",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -206,14 +206,22 @@ def forward(\n \n         if inputs_embeds is None:\n             inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n-            if image_patches is not None and past_key_values is None:\n-                patch_embeddings = self.get_image_features(image_patches)\n-                patch_embeddings = torch.cat(patch_embeddings, dim=0)\n-\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-                patch_embeddings = patch_embeddings.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, patch_embeddings)\n+\n+        if image_patches is not None:\n+            patch_embeddings = self.get_image_features(image_patches)\n+            patch_embeddings = torch.cat(patch_embeddings, dim=0)\n+\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            patch_embeddings = patch_embeddings.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, patch_embeddings)\n \n         outputs = self.language_model(\n             inputs_embeds=inputs_embeds,"
        },
        {
            "sha": "2d86e9f04c02a8fdc6b9d9cbd76744caa10f642b",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -898,9 +898,11 @@ def forward(\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n                     torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n+                special_image_mask = special_image_mask.all(-1)\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n                 image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]"
        },
        {
            "sha": "0b0960a6a9893001fe19f15448f29ededf3bfe76",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -800,9 +800,11 @@ def forward(\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n                     torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n+                special_image_mask = special_image_mask.all(-1)\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n                 image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]"
        },
        {
            "sha": "4148dfd10ac75885a839954f295a8a2b1907e230",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 53,
            "deletions": 24,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1237,50 +1237,59 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n             image_embeds = torch.cat(image_embeds, dim=0)\n-            n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+\n+            if input_ids is None:\n+                image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                image_mask = image_mask.all(-1)\n+            else:\n+                image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = image_mask.sum()\n+            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             n_image_features = image_embeds.shape[0]\n             if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n \n-            mask = input_ids == self.config.image_token_id\n-            mask_unsqueezed = mask.unsqueeze(-1)\n-            mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)\n-            image_mask = mask_expanded.to(inputs_embeds.device)\n             image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n         if pixel_values_videos is not None:\n             video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n             video_embeds = torch.cat(video_embeds, dim=0)\n-            n_video_tokens = (input_ids == self.config.image_token_id).sum()\n+\n+            if input_ids is None:\n+                video_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                video_mask = video_mask.all(-1)\n+            else:\n+                video_mask = input_ids == self.config.video_token_id\n+\n+            n_video_tokens = (video_mask).sum()\n             n_video_features = video_embeds.shape[0]\n+            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                 )\n \n-            mask = input_ids == self.config.image_token_id  # GLM-4.1V use image_token_id for video\n-            mask_unsqueezed = mask.unsqueeze(-1)\n-            mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)\n-            video_mask = mask_expanded.to(inputs_embeds.device)\n             video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:\n-            attention_mask_tensor = attention_mask\n+            attention_mask_tensor = (\n+                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n+            )\n             if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n                 attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n                 attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n@@ -1571,6 +1580,7 @@ def prepare_inputs_for_generation(\n     def _get_image_nums_and_video_nums(\n         self,\n         input_ids: Optional[torch.LongTensor],\n+        inputs_embeds: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Get the number of images and videos for each sample to calculate the separation length of the sample tensor.\n@@ -1585,9 +1595,29 @@ def _get_image_nums_and_video_nums(\n             video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)\n         \"\"\"\n \n-        is_image = input_ids == self.config.image_start_token_id\n-        is_video_start = input_ids == self.config.video_start_token_id\n-        is_video_end = input_ids == self.config.video_end_token_id\n+        if inputs_embeds is not None:\n+            is_image = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_start_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            is_video_start = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_start_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            is_video_end = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_end_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+        else:\n+            is_image = input_ids == self.config.image_start_token_id\n+            is_video_start = input_ids == self.config.video_start_token_id\n+            is_video_end = input_ids == self.config.video_end_token_id\n \n         # Cumulative sum to track if we're inside a video span\n         # We'll assume well-formed video tags (i.e. matching starts and ends)\n@@ -1623,7 +1653,9 @@ def _expand_inputs_for_generation(\n         def _expand_dict_for_generation_visual(dict_to_expand):\n             image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n             video_grid_thw = model_kwargs.get(\"video_grid_thw\", None)\n-            image_nums, video_nums = self._get_image_nums_and_video_nums(input_ids)\n+            image_nums, video_nums = self._get_image_nums_and_video_nums(\n+                input_ids, inputs_embeds=model_kwargs.get(\"inputs_embeds\", None)\n+            )\n \n             def _repeat_interleave_samples(x, lengths, repeat_times):\n                 samples = torch.split(x, lengths)\n@@ -1679,10 +1711,7 @@ def _expand_dict_for_generation(dict_to_expand):\n                     dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n             return dict_to_expand\n \n-        # input_ids is required for expanding visual inputs\n-        # If input_ids is unavailable, visual inputs will not be used; therefore, there is no need to expand visual inputs.\n-        if input_ids is not None and input_ids.numel() != 0:\n-            model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n+        model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n \n         if input_ids is not None:\n             input_ids = input_ids.repeat_interleave(expand_size, dim=0)"
        },
        {
            "sha": "cf4a6b9233f94cc249b036e4fb0d3cc38613dd43",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 49,
            "deletions": 19,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1237,50 +1237,59 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n             image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n             image_embeds = torch.cat(image_embeds, dim=0)\n-            n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+\n+            if input_ids is None:\n+                image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                image_mask = image_mask.all(-1)\n+            else:\n+                image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = image_mask.sum()\n+            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             n_image_features = image_embeds.shape[0]\n             if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n \n-            mask = input_ids == self.config.image_token_id\n-            mask_unsqueezed = mask.unsqueeze(-1)\n-            mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)\n-            image_mask = mask_expanded.to(inputs_embeds.device)\n             image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n         if pixel_values_videos is not None:\n             video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n             video_embeds = torch.cat(video_embeds, dim=0)\n-            n_video_tokens = (input_ids == self.config.image_token_id).sum()\n+\n+            if input_ids is None:\n+                video_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                video_mask = video_mask.all(-1)\n+            else:\n+                video_mask = input_ids == self.config.video_token_id\n+\n+            n_video_tokens = (video_mask).sum()\n             n_video_features = video_embeds.shape[0]\n+            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n             if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n                 )\n \n-            mask = input_ids == self.config.image_token_id  # GLM-4.1V use image_token_id for video\n-            mask_unsqueezed = mask.unsqueeze(-1)\n-            mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)\n-            video_mask = mask_expanded.to(inputs_embeds.device)\n             video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:\n-            attention_mask_tensor = attention_mask\n+            attention_mask_tensor = (\n+                attention_mask if not isinstance(attention_mask, dict) else attention_mask[\"full_attention\"]\n+            )\n             if attention_mask_tensor is not None and attention_mask_tensor.ndim == 4:\n                 attention_mask_tensor = torch.diagonal(attention_mask_tensor[:, 0], dim1=1, dim2=2)\n                 attention_mask_tensor = attention_mask_tensor / torch.finfo(attention_mask_tensor.dtype).min\n@@ -1500,6 +1509,7 @@ def prepare_inputs_for_generation(\n     def _get_image_nums_and_video_nums(\n         self,\n         input_ids: Optional[torch.LongTensor],\n+        inputs_embeds: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Get the number of images and videos for each sample to calculate the separation length of the sample tensor.\n@@ -1514,9 +1524,29 @@ def _get_image_nums_and_video_nums(\n             video_nums (`torch.LongTensor` of shape `(batch_size, num_videos_sample)`)\n         \"\"\"\n \n-        is_image = input_ids == self.config.image_start_token_id\n-        is_video_start = input_ids == self.config.video_start_token_id\n-        is_video_end = input_ids == self.config.video_end_token_id\n+        if inputs_embeds is not None:\n+            is_image = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_start_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            is_video_start = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_start_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            is_video_end = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_end_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+        else:\n+            is_image = input_ids == self.config.image_start_token_id\n+            is_video_start = input_ids == self.config.video_start_token_id\n+            is_video_end = input_ids == self.config.video_end_token_id\n \n         # Cumulative sum to track if we're inside a video span\n         # We'll assume well-formed video tags (i.e. matching starts and ends)"
        },
        {
            "sha": "2163dfcea7507d53389dcc541f9826bdbda9c070",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -648,24 +648,27 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n-            n_image_tokens = (input_ids == self.config.image_token_id).sum()\n             n_image_features = image_features.shape[0] * image_features.shape[1]\n             if n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n "
        },
        {
            "sha": "e2b001f2425c3bd6483cd58b15dbe8d28f437fcc",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -339,24 +339,27 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             image_features = self.get_image_features(pixel_values=pixel_values.to(inputs_embeds.dtype))\n-            n_image_tokens = (input_ids == self.config.image_token_id).sum()\n             n_image_features = image_features.shape[0] * image_features.shape[1]\n             if n_image_tokens != n_image_features:\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n                 )\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_features)\n "
        },
        {
            "sha": "e18e4ee1379dba0a69c82fb6e846ca2b3328a5be",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 16,
            "deletions": 54,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -933,10 +933,18 @@ def inputs_merger(\n         - The merging happens so that we obtain the following sequence: `vector_tok_1 vector_tok_2 vector_tok_3 vector_fake_tok_around_image {sequence of image_seq_len image hidden states} vector_fake_toke_around_image vector_tok_4`. That sequence is fed to the LM.\n         - To fit the format of that sequence, `input_ids`, `input_embeds`, `attention_mask` are all 3 adapted to insert the image hidden states.\n         \"\"\"\n-        special_image_token_mask = input_ids == self.image_token_id\n-        new_inputs_embeds = inputs_embeds.clone()\n-        new_inputs_embeds[special_image_token_mask] = image_hidden_states.to(new_inputs_embeds.device)\n-        return new_inputs_embeds\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        image_hidden_states = image_hidden_states.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_hidden_states)\n+        return inputs_embeds\n \n     def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n         \"\"\"\n@@ -1041,25 +1049,8 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n-        past_seen_tokens = 0\n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache:\n-            if not isinstance(past_key_values, Cache):\n-                return_legacy_cache = True\n-                if past_key_values is None:\n-                    past_key_values = DynamicCache()\n-                else:\n-                    past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                    logger.warning_once(\n-                        \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                        \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                        \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                    )\n-            past_seen_tokens = past_key_values.get_seq_length()\n-\n-        if inputs_embeds is not None and input_ids is None and past_seen_tokens == 0:\n-            raise ValueError(\"When first calling the model, if input_embeds are passed, input_ids should not be None.\")\n+        if use_cache and not isinstance(past_key_values, Cache):\n+            past_key_values = DynamicCache()\n \n         if inputs_embeds is None:\n             inputs_embeds = self.text_model.get_input_embeddings()(input_ids)\n@@ -1072,7 +1063,7 @@ def forward(\n         elif image_hidden_states is not None:\n             image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=input_ids.device)\n \n-        if past_seen_tokens == 0 and inputs_embeds is not None and image_hidden_states is not None:\n+        if image_hidden_states is not None:\n             # When we generate, we don't want to replace the potential image_token_id that we generated by images\n             # that simply don't exist\n             inputs_embeds = self.inputs_merger(\n@@ -1094,9 +1085,6 @@ def forward(\n             **kwargs,\n         )\n \n-        if return_legacy_cache and use_cache:\n-            outputs.past_key_values = outputs.past_key_values.to_legacy_cache()\n-\n         return Idefics2BaseModelOutputWithPast(\n             last_hidden_state=outputs.last_hidden_state,\n             past_key_values=outputs.past_key_values,\n@@ -1304,37 +1292,11 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        # but IDEFICS requires both ids and embeds to be present\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs[\"input_ids\"] = input_ids\n-\n-        if image_hidden_states is not None:\n+        if image_hidden_states is not None or cache_position[0] != 0:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_attention_mask\"] = None\n \n         return model_inputs\n \n-    def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder, **kwargs):\n-        model_kwargs = super()._update_model_kwargs_for_generation(\n-            outputs=outputs,\n-            model_kwargs=model_kwargs,\n-            is_encoder_decoder=is_encoder_decoder,\n-            **kwargs,\n-        )\n-        # Get the precomputed image_hidden_states\n-        model_kwargs[\"image_hidden_states\"] = outputs.image_hidden_states\n-        return model_kwargs\n-\n-    @staticmethod\n-    # Copied from transformers.models.opt.modeling_opt.OPTForCausalLM._reorder_cache\n-    def _reorder_cache(past_key_values, beam_idx):\n-        reordered_past = ()\n-        for layer_past in past_key_values:\n-            reordered_past += (\n-                tuple(past_state.index_select(0, beam_idx.to(past_state.device)) for past_state in layer_past),\n-            )\n-        return reordered_past\n-\n \n __all__ = [\"Idefics2ForConditionalGeneration\", \"Idefics2PreTrainedModel\", \"Idefics2Model\"]"
        },
        {
            "sha": "a0494cb7411947a7c619c7f090a616844f6054da",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 15,
            "deletions": 32,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -663,15 +663,18 @@ def inputs_merger(\n         - The merging happens so that we obtain the following sequence: `vector_tok_1 vector_tok_2 vector_tok_3 vector_fake_tok_around_image {sequence of image_seq_len image hidden states} vector_fake_toke_around_image vector_tok_4`. That sequence is fed to the LM.\n         - To fit the format of that sequence, `input_ids`, `input_embeds`, `attention_mask` are all 3 adapted to insert the image hidden states.\n         \"\"\"\n-        special_image_token_mask = input_ids == self.image_token_id\n-        # Fixes RuntimeError: a leaf Variable that requires grad is being used in an in-place operation.\n-        new_inputs_embeds = inputs_embeds.clone()\n-        # Flatten `image_hidden_states` if not flat yet\n-        image_hidden_states = image_hidden_states.view(-1, image_hidden_states.shape[-1])\n-        # cast to the dtype of the input_embeds to support quantized models\n+        if input_ids is None:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n+        else:\n+            special_image_mask = input_ids == self.config.image_token_id\n+\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n         image_hidden_states = image_hidden_states.to(inputs_embeds.device, inputs_embeds.dtype)\n-        new_inputs_embeds[special_image_token_mask] = image_hidden_states\n-        return new_inputs_embeds\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, image_hidden_states)\n+        return inputs_embeds\n \n     def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n         \"\"\"\n@@ -773,11 +776,8 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n-        past_seen_tokens = 0\n-        if use_cache:\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            past_seen_tokens = past_key_values.get_seq_length()\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if inputs_embeds is None:\n             inputs_embeds = self.text_model.get_input_embeddings()(input_ids).to(self.device)\n@@ -790,7 +790,7 @@ def forward(\n         elif image_hidden_states is not None:\n             image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=input_ids.device)\n \n-        if past_seen_tokens == 0 and input_ids is not None and image_hidden_states is not None:\n+        if image_hidden_states is not None:\n             # When we generate, we don't want to replace the potential image_token_id that we generated by images\n             # that simply don't exist\n             inputs_embeds = self.inputs_merger(\n@@ -1042,28 +1042,11 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        # but IDEFICS requires both ids and embeds to be present\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs[\"input_ids\"] = input_ids\n-\n-        if image_hidden_states is not None:\n+        if image_hidden_states is not None or cache_position[0] != 0:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_attention_mask\"] = None\n \n         return model_inputs\n \n-    # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration._update_model_kwargs_for_generation\n-    def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder, **kwargs):\n-        model_kwargs = super()._update_model_kwargs_for_generation(\n-            outputs=outputs,\n-            model_kwargs=model_kwargs,\n-            is_encoder_decoder=is_encoder_decoder,\n-            **kwargs,\n-        )\n-        # Get the precomputed image_hidden_states\n-        model_kwargs[\"image_hidden_states\"] = outputs.image_hidden_states\n-        return model_kwargs\n-\n \n __all__ = [\"Idefics3ForConditionalGeneration\", \"Idefics3PreTrainedModel\", \"Idefics3Model\", \"Idefics3VisionTransformer\"]"
        },
        {
            "sha": "8c75db38d226a5397434304a8eea07b87370ed92",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 51,
            "deletions": 18,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1255,6 +1255,7 @@ def forward(\n         attention_mask: Optional[torch.LongTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -1328,12 +1329,20 @@ def forward(\n \n         # step 3: use the language model, conditioned on the query outputs and the prompt\n         language_model_inputs = self.language_projection(query_output)\n-        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n-        if attention_mask is None:\n-            attention_mask = torch.ones_like(input_ids)\n+        if inputs_embeds is None:\n+            inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n+            special_image_mask = input_ids == self.config.image_token_id\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(input_ids)\n+        else:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n \n-        special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n-        inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         if self.config.use_decoder_only_language_model:\n             outputs = self.language_model(\n@@ -1513,6 +1522,7 @@ def forward(\n         attention_mask: Optional[torch.LongTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1604,15 +1614,26 @@ def forward(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n \n-        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n         # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"image_token_id\", None) is not None:\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in InstructBLIP should be done in processing. \"\n@@ -1673,6 +1694,7 @@ def generate(\n         qformer_attention_mask: Optional[torch.LongTensor] = None,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         interpolate_pos_encoding: bool = False,\n         **generate_kwargs,\n     ) -> torch.LongTensor:\n@@ -1690,6 +1712,8 @@ def generate(\n                 The sequence used as a prompt for the generation.\n             attention_mask (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n                 Mask to avoid performing attention on padding token indices.\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Embedded representation of the inputs. Should be float, not int tokens.\n             interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n                 Whether to interpolate the positional encoding of the image embeddings.\n \n@@ -1712,23 +1736,32 @@ def generate(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n \n-        if input_ids is None:\n-            start_tokens = [self.config.text_config.bos_token_id]\n-            if getattr(self.config, \"image_token_id\", None) is not None:\n-                start_tokens = [self.config.image_token_id] * self.config.num_query_tokens + start_tokens\n-            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=pixel_values.device)\n-            input_ids = input_ids.repeat(batch_size, 1)\n+        if inputs_embeds is None:\n+            if input_ids is None:\n+                start_tokens = [self.config.text_config.bos_token_id]\n+                if getattr(self.config, \"image_token_id\", None) is not None:\n+                    start_tokens = [self.config.image_token_id] * self.config.num_query_tokens + start_tokens\n+                input_ids = torch.tensor([start_tokens], dtype=torch.long, device=language_model_inputs.device)\n+                input_ids = input_ids.repeat(batch_size, 1)\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n         # if the model already has \"image_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"image_token_id\", None) is not None:\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for image tokens in InstructBLIP should be done in processing. \""
        },
        {
            "sha": "2989e08d09112fad6d110f1ad156d4e926506819",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 51,
            "deletions": 18,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1251,6 +1251,7 @@ def forward(\n         attention_mask: Optional[torch.LongTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -1334,12 +1335,20 @@ def forward(\n \n         # unbatch inputs back, each video-frame gets `num_query_tokens` seq length\n         language_model_inputs = language_model_inputs.reshape(batch_size, self.config.num_query_tokens * frames, -1)\n-        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n-        if attention_mask is None:\n-            attention_mask = torch.ones_like(input_ids)\n+        if inputs_embeds is None:\n+            inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n+            special_image_mask = input_ids == self.config.video_token_id\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(input_ids)\n+        else:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n \n-        special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n-        inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         if self.config.use_decoder_only_language_model:\n             outputs = self.language_model(\n@@ -1485,6 +1494,7 @@ def forward(\n         attention_mask: Optional[torch.LongTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1599,15 +1609,26 @@ def forward(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n \n-        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n         # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"video_token_id\", None) is not None:\n-            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.video_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n@@ -1668,6 +1689,7 @@ def generate(\n         qformer_attention_mask: Optional[torch.LongTensor] = None,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         interpolate_pos_encoding: bool = False,\n         **generate_kwargs,\n     ) -> torch.LongTensor:\n@@ -1685,6 +1707,8 @@ def generate(\n                 The sequence used as a prompt for the generation.\n             attention_mask (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n                 Mask to avoid performing attention on padding token indices.\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Embedded representation of the inputs. Should be float, not int tokens.\n             interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n                 Whether to interpolate the positional encoding of the image embeddings.\n \n@@ -1708,23 +1732,32 @@ def generate(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n \n-        if input_ids is None:\n-            start_tokens = [self.config.text_config.bos_token_id]\n-            if getattr(self.config, \"video_token_id\", None) is not None:\n-                start_tokens = [self.config.video_token_id] * self.config.num_query_tokens * 4 + start_tokens\n-            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=pixel_values.device)\n-            input_ids = input_ids.repeat(batch_size, 1)\n+        if inputs_embeds is None:\n+            if input_ids is None:\n+                start_tokens = [self.config.text_config.bos_token_id]\n+                if getattr(self.config, \"video_token_id\", None) is not None:\n+                    start_tokens = [self.config.video_token_id] * self.config.num_query_tokens * 4 + start_tokens\n+                input_ids = torch.tensor([start_tokens], dtype=torch.long, device=language_model_inputs.device)\n+                input_ids = input_ids.repeat(batch_size, 1)\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n         # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"video_token_id\", None) is not None:\n-            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.video_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \""
        },
        {
            "sha": "4ec768e4a8753fd1d4ed9b24bc8abdf77b0ce7ae",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 51,
            "deletions": 18,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -202,6 +202,7 @@ def forward(\n         attention_mask: Optional[torch.LongTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n@@ -255,12 +256,20 @@ def forward(\n \n         # unbatch inputs back, each video-frame gets `num_query_tokens` seq length\n         language_model_inputs = language_model_inputs.reshape(batch_size, self.config.num_query_tokens * frames, -1)\n-        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n-        if attention_mask is None:\n-            attention_mask = torch.ones_like(input_ids)\n+        if inputs_embeds is None:\n+            inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n+            special_image_mask = input_ids == self.config.video_token_id\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(input_ids)\n+        else:\n+            special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            special_image_mask = special_image_mask.all(-1)\n \n-        special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n-        inputs_embeds[special_image_mask] = language_model_inputs.flatten()\n+        special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+        language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+        inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n \n         if self.config.use_decoder_only_language_model:\n             outputs = self.language_model(\n@@ -372,6 +381,7 @@ def forward(\n         attention_mask: Optional[torch.LongTensor] = None,\n         decoder_input_ids: Optional[torch.LongTensor] = None,\n         decoder_attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -451,15 +461,26 @@ def forward(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n \n-        inputs_embeds = self.language_model.get_input_embeddings()(input_ids)\n+        if inputs_embeds is None:\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n+\n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n         # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"video_token_id\", None) is not None:\n-            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.video_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n@@ -520,6 +541,7 @@ def generate(\n         qformer_attention_mask: Optional[torch.LongTensor] = None,\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n         interpolate_pos_encoding: bool = False,\n         **generate_kwargs,\n     ) -> torch.LongTensor:\n@@ -537,6 +559,8 @@ def generate(\n                 The sequence used as a prompt for the generation.\n             attention_mask (`torch.LongTensor` of shape (batch_size, sequence_length), *optional*):\n                 Mask to avoid performing attention on padding token indices.\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Embedded representation of the inputs. Should be float, not int tokens.\n             interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n                 Whether to interpolate the positional encoding of the image embeddings.\n \n@@ -560,23 +584,32 @@ def generate(\n             language_model_inputs.size()[:-1], dtype=torch.long, device=language_model_inputs.device\n         )\n \n-        if input_ids is None:\n-            start_tokens = [self.config.text_config.bos_token_id]\n-            if getattr(self.config, \"video_token_id\", None) is not None:\n-                start_tokens = [self.config.video_token_id] * self.config.num_query_tokens * 4 + start_tokens\n-            input_ids = torch.tensor([start_tokens], dtype=torch.long, device=pixel_values.device)\n-            input_ids = input_ids.repeat(batch_size, 1)\n+        if inputs_embeds is None:\n+            if input_ids is None:\n+                start_tokens = [self.config.text_config.bos_token_id]\n+                if getattr(self.config, \"video_token_id\", None) is not None:\n+                    start_tokens = [self.config.video_token_id] * self.config.num_query_tokens * 4 + start_tokens\n+                input_ids = torch.tensor([start_tokens], dtype=torch.long, device=language_model_inputs.device)\n+                input_ids = input_ids.repeat(batch_size, 1)\n+            inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if attention_mask is None:\n             attention_mask = torch.ones_like(input_ids)\n \n-        inputs_embeds = self.get_input_embeddings()(input_ids)\n-\n         # if the model already has \"video_token_id\" then the input is expanded to account for image embeds\n         # otherwise we expand manually by concatenating\n         if getattr(self.config, \"video_token_id\", None) is not None:\n-            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1).expand_as(inputs_embeds)\n-            inputs_embeds[special_image_mask] = language_model_inputs.flatten().to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.video_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            language_model_inputs = language_model_inputs.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, language_model_inputs)\n         else:\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \""
        },
        {
            "sha": "26f26fae83825f91ea0e71e66266aad0cee55134",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -710,14 +710,14 @@ def forward(\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n                     torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+                special_image_mask = special_image_mask.all(-1)\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "7ddc11eb7be6e8870c34bd6dd551fdf487592077",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -641,14 +641,14 @@ def forward(\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n                     torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+                special_image_mask = special_image_mask.all(-1)\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "87db9b8a6ecb6cdb0139a22ca2b31da7ef716f71",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1102,23 +1102,21 @@ def forward(\n                 )\n                 use_cache = False\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n-            image_embeds = self.get_image_features(pixel_values)\n-            image_attention_mask = input_ids == self.config.image_token_id\n-\n-            embed_dim = inputs_embeds.shape[-1]\n-            image_features = image_embeds.reshape(-1, embed_dim)\n-            image_attention_mask = image_attention_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n+            if input_ids is None:\n+                image_attention_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                image_attention_mask = image_attention_mask.all(-1)\n+            else:\n+                image_attention_mask = input_ids == self.config.image_token_id\n \n-            image_attention_mask = image_attention_mask.to(inputs_embeds.device)\n+            image_attention_mask = image_attention_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_embeds = self.get_image_features(pixel_values)\n+            image_features = image_embeds.reshape(-1, inputs_embeds.shape[-1])\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(image_attention_mask, image_features)\n "
        },
        {
            "sha": "5fb18d83d72732920fc77e71ea3967e23da6eca2",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 10,
            "deletions": 12,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -955,23 +955,21 @@ def forward(\n                 )\n                 use_cache = False\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         if pixel_values is not None:\n-            image_embeds = self.get_image_features(pixel_values)\n-            image_attention_mask = input_ids == self.config.image_token_id\n-\n-            embed_dim = inputs_embeds.shape[-1]\n-            image_features = image_embeds.reshape(-1, embed_dim)\n-            image_attention_mask = image_attention_mask.unsqueeze(-1).expand(-1, -1, embed_dim)\n+            if input_ids is None:\n+                image_attention_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                image_attention_mask = image_attention_mask.all(-1)\n+            else:\n+                image_attention_mask = input_ids == self.config.image_token_id\n \n-            image_attention_mask = image_attention_mask.to(inputs_embeds.device)\n+            image_attention_mask = image_attention_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_embeds = self.get_image_features(pixel_values)\n+            image_features = image_embeds.reshape(-1, inputs_embeds.shape[-1])\n             image_features = image_features.to(inputs_embeds.device, inputs_embeds.dtype)\n             inputs_embeds = inputs_embeds.masked_scatter(image_attention_mask, image_features)\n "
        },
        {
            "sha": "0ca82e4b771c393b4e51568450a0c0c7e877228e",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 9,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1467,25 +1467,19 @@ def prepare_inputs_for_generation(\n         image_embeds_position_mask=None,\n         past_key_values=None,\n         attention_mask=None,\n+        inputs_embeds=None,\n         use_cache=None,\n         cache_position=None,\n         **model_kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        # Kosmos2 has offset for position ids, so we need to create them correctly\n-        position_ids = create_position_ids_from_input_ids(\n-            input_ids,\n-            padding_idx=self.config.pad_token_id,\n-            past_key_values_length=0,\n-        )\n-\n         if past_key_values is not None:\n             image_embeds = None\n             image_embeds_position_mask = None\n         # appending `False` to `image_embeds_position_mask` (because `input_ids` grows during generation)\n         elif image_embeds_position_mask is not None:\n-            batch_size, seq_len = input_ids.size()\n+            batch_size, seq_len = inputs_embeds.size()[:-1] if inputs_embeds is not None else input_ids.size()\n             mask_len = image_embeds_position_mask.size()[-1]\n             image_embeds_position_mask = torch.cat(\n                 (\n@@ -1501,11 +1495,13 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             image_embeds=image_embeds,\n             image_embeds_position_mask=image_embeds_position_mask,\n+            inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n-            position_ids=position_ids,\n             cache_position=cache_position,\n             **model_kwargs,\n         )\n+        # Kosmos2 has offset for position ids, so we need to create them correctly in PositionEmbedding layer\n+        model_inputs.pop(\"position_ids\", None)\n \n         return model_inputs\n \n@@ -1875,6 +1871,7 @@ def generate(\n         input_ids: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         image_embeds: Optional[torch.Tensor] = None,\n+        inputs_embeds: Optional[torch.Tensor] = None,\n         **kwargs,\n     ):\n         # in order to allow `inputs` argument (as in `GenerationMixin`)\n@@ -1900,6 +1897,7 @@ def generate(\n             attention_mask=attention_mask,\n             image_embeds=image_embeds,\n             image_embeds_position_mask=image_embeds_position_mask,\n+            inputs_embeds=inputs_embeds,\n             **kwargs,\n         )\n "
        },
        {
            "sha": "259482934f3a81e25599ee09ccf4364178b958d2",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 13,
            "deletions": 12,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1358,27 +1358,28 @@ def forward(\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n                 image_sizes=image_sizes,\n             )\n-            original_inputs_embeds_shape = inputs_embeds.shape\n \n             vision_flat = image_features.view(-1, image_features.size(-1))\n             projected_vision_flat = self.multi_modal_projector(vision_flat)\n \n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            final_mask = special_image_mask.to(inputs_embeds.device)\n-            inputs_embeds = inputs_embeds.view(-1, inputs_embeds.size(-1))\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n \n-            final_mask_1d = final_mask[..., 0].reshape(-1)\n-            num_tokens_to_fill = final_mask_1d.sum()\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n \n-            if num_tokens_to_fill != projected_vision_flat.size(0):\n+            if n_image_tokens != projected_vision_flat.size(0):\n                 raise ValueError(\n-                    f\"Mismatch: final_mask wants {num_tokens_to_fill} embeddings, \"\n+                    f\"Mismatch: final_mask wants {n_image_tokens} embeddings, \"\n                     f\"but multi_modal_projector returned {projected_vision_flat.size(0)}\"\n                 )\n-\n-            expanded_mask = final_mask_1d.unsqueeze(-1).expand(-1, inputs_embeds.size(-1))\n-            inputs_embeds = inputs_embeds.masked_scatter(expanded_mask, projected_vision_flat)\n-            inputs_embeds = inputs_embeds.view(original_inputs_embeds_shape)\n+            projected_vision_flat = projected_vision_flat.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(special_image_mask, projected_vision_flat)\n \n         outputs = self.language_model(\n             attention_mask=attention_mask,"
        },
        {
            "sha": "346dc98f2bf7dc1db243ba214988b6b7828817ab",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -284,14 +284,14 @@ def forward(\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n                     torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                n_image_tokens = (special_image_mask).sum(dim=1).sum(dim=0)[0]\n+                special_image_mask = special_image_mask.all(-1)\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "14ad299a73fe8ddd786dc9e4ab2ae72ffcd7513f",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -468,11 +468,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -485,10 +480,18 @@ def forward(\n             )\n             image_features = torch.cat(image_features, dim=0)\n \n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "dbd6ceaed1f1064607cc03bab1be3f592be76bfb",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 22,
            "deletions": 12,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -519,12 +519,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n-                \"and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -537,10 +531,18 @@ def forward(\n             )\n             image_features = torch.cat(image_features, dim=0)\n \n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n@@ -559,10 +561,18 @@ def forward(\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n \n-            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.video_token_id\n+\n+            n_video_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n                 n_video_features = video_features.shape[0]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\""
        },
        {
            "sha": "0a3285934994327a1e93f451137cbd8f7d0d4c5c",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 22,
            "deletions": 12,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -440,12 +440,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n-                \"and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -458,10 +452,18 @@ def forward(\n             )\n             image_features = torch.cat(image_features, dim=0)\n \n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n@@ -480,10 +482,18 @@ def forward(\n             video_features = torch.cat(video_features, dim=0)\n             video_feature_lens = torch.tensor(video_feature_lens, dtype=torch.long, device=video_features.device)\n \n-            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.video_token_id\n+\n+            n_video_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n                 n_video_features = video_features.shape[0]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\""
        },
        {
            "sha": "616a314e3be33e95a49d3da363e6ba724bba9a6d",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 22,
            "deletions": 12,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -551,12 +551,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n-                \"and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -571,10 +565,18 @@ def forward(\n             )\n             image_features = torch.cat(image_features, dim=0)\n \n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n@@ -595,10 +597,18 @@ def forward(\n             video_features = torch.cat((video_features, image_newline), dim=1)\n             video_features = video_features.flatten(0, 1)\n \n-            special_video_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n-            special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_video_mask = special_video_mask.all(-1)\n+            else:\n+                special_video_mask = input_ids == self.config.video_token_id\n+\n+            n_video_tokens = (special_video_mask).sum()\n+            special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_video_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n                 n_video_features = video_features.shape[0]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\""
        },
        {
            "sha": "2461c89b72a2cd959bf9865426ade6ef1ef9ba0b",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 22,
            "deletions": 12,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -535,12 +535,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if (pixel_values is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both `pixel_values`/`pixel_values_videos` and `inputs_embeds` at the same time, \"\n-                \"and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -555,10 +549,18 @@ def forward(\n             )\n             image_features = torch.cat(image_features, dim=0)\n \n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n@@ -579,10 +581,18 @@ def forward(\n             video_features = torch.cat((video_features, image_newline), dim=1)\n             video_features = video_features.flatten(0, 1)\n \n-            special_video_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n-            special_video_mask = special_video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_video_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_video_mask = special_video_mask.all(-1)\n+            else:\n+                special_video_mask = input_ids == self.config.video_token_id\n+\n+            n_video_tokens = (special_video_mask).sum()\n+            special_video_mask = special_video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_video_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n                 n_video_features = video_features.shape[0]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\""
        },
        {
            "sha": "63b8a0b0b252138152e2a3f2301a93d8a4e964ab",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -308,11 +308,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -324,10 +319,18 @@ def forward(\n             )\n             image_features = torch.cat(image_features, dim=0)\n \n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "2027d323a51f6e1544e2111f7a7952d742d872cc",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -204,11 +204,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -220,10 +215,18 @@ def forward(\n             )\n             image_features = torch.cat(image_features, dim=0)\n \n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "36183c66b6605776a7f94486d620a0cea4ad62aa",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -331,9 +331,11 @@ def forward(\n                 special_image_mask = inputs_embeds == self.get_input_embeddings()(\n                     torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n+                special_image_mask = special_image_mask.all(-1)\n             else:\n-                special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-                special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n \n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n                 image_tokens_in_text = (special_image_mask).sum(dim=1).sum(dim=0)[0]"
        },
        {
            "sha": "37a290e80012b87909a0ec5fd3a1384af27f62fc",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 42,
            "deletions": 34,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1903,43 +1903,51 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         # 2. Merge text , audios , image and video\n-        if input_ids is not None and input_ids.shape[1] != 1:  # Prefill stage\n-            if input_features is not None:\n-                audio_features = self.get_audio_features(\n-                    input_features,\n-                    feature_attention_mask=feature_attention_mask,\n-                    audio_feature_lengths=audio_feature_lengths,\n-                )\n-                audio_mask = (\n-                    (input_ids == self.config.audio_token_id)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+        if input_features is not None:\n+            audio_features = self.get_audio_features(\n+                input_features,\n+                feature_attention_mask=feature_attention_mask,\n+                audio_feature_lengths=audio_feature_lengths,\n+            )\n+            if input_ids is None:\n+                audio_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.audio_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                audio_features = audio_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(audio_mask, audio_features)\n-\n-            if pixel_values is not None:\n-                image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-                image_mask = (\n-                    (input_ids == self.config.image_token_id)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+                audio_mask = audio_mask.all(-1)\n+            else:\n+                audio_mask = input_ids == self.config.audio_token_id\n+\n+            audio_mask = audio_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            audio_features = audio_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(audio_mask, audio_features)\n+\n+        if pixel_values is not None:\n+            image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+            if input_ids is None:\n+                image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n-\n-            if pixel_values_videos is not None:\n-                video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-                video_mask = (\n-                    (input_ids == self.config.video_token_id)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+                image_mask = image_mask.all(-1)\n+            else:\n+                image_mask = input_ids == self.config.image_token_id\n+\n+            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n+\n+        if pixel_values_videos is not None:\n+            video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+            if input_ids is None:\n+                video_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n+                video_mask = video_mask.all(-1)\n+            else:\n+                video_mask = input_ids == self.config.video_token_id\n+\n+            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if feature_attention_mask is not None:\n             audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)"
        },
        {
            "sha": "645a5fb837f410596a5255d1dab79e4a18af2e6e",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 42,
            "deletions": 34,
            "changes": 76,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -2350,43 +2350,51 @@ def forward(\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n         # 2. Merge text , audios , image and video\n-        if input_ids is not None and input_ids.shape[1] != 1:  # Prefill stage\n-            if input_features is not None:\n-                audio_features = self.get_audio_features(\n-                    input_features,\n-                    feature_attention_mask=feature_attention_mask,\n-                    audio_feature_lengths=audio_feature_lengths,\n-                )\n-                audio_mask = (\n-                    (input_ids == self.config.audio_token_id)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+        if input_features is not None:\n+            audio_features = self.get_audio_features(\n+                input_features,\n+                feature_attention_mask=feature_attention_mask,\n+                audio_feature_lengths=audio_feature_lengths,\n+            )\n+            if input_ids is None:\n+                audio_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.audio_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                audio_features = audio_features.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(audio_mask, audio_features)\n-\n-            if pixel_values is not None:\n-                image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-                image_mask = (\n-                    (input_ids == self.config.image_token_id)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+                audio_mask = audio_mask.all(-1)\n+            else:\n+                audio_mask = input_ids == self.config.audio_token_id\n+\n+            audio_mask = audio_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            audio_features = audio_features.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(audio_mask, audio_features)\n+\n+        if pixel_values is not None:\n+            image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+            if input_ids is None:\n+                image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n-\n-            if pixel_values_videos is not None:\n-                video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-                video_mask = (\n-                    (input_ids == self.config.video_token_id)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+                image_mask = image_mask.all(-1)\n+            else:\n+                image_mask = input_ids == self.config.image_token_id\n+\n+            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n+\n+        if pixel_values_videos is not None:\n+            video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+            if input_ids is None:\n+                video_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n+                video_mask = video_mask.all(-1)\n+            else:\n+                video_mask = input_ids == self.config.video_token_id\n+\n+            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if feature_attention_mask is not None:\n             audio_feature_lengths = torch.sum(feature_attention_mask, dim=1)"
        },
        {
            "sha": "1b69973cc3b3a692e6ced3b017499dc658596501",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 71,
            "deletions": 40,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1245,41 +1245,51 @@ def forward(\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n-            if pixel_values is not None:\n-                image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-                image_embeds = torch.cat(image_embeds, dim=0)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_embeds.shape[0]\n-                if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n-                    raise ValueError(\n-                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                    )\n \n-                mask = input_ids == self.config.image_token_id\n-                mask_unsqueezed = mask.unsqueeze(-1)\n-                mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)\n-                image_mask = mask_expanded.to(inputs_embeds.device)\n-\n-                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n-\n-            if pixel_values_videos is not None:\n-                video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-                video_embeds = torch.cat(video_embeds, dim=0)\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n-                n_video_features = video_embeds.shape[0]\n-                if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n-                    raise ValueError(\n-                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                    )\n+        if pixel_values is not None:\n+            image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+            image_embeds = torch.cat(image_embeds, dim=0)\n+\n+            if input_ids is None:\n+                image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                image_mask = image_mask.all(-1)\n+            else:\n+                image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (image_mask).sum()\n+            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            n_image_features = image_embeds.shape[0]\n+            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n-                mask = input_ids == self.config.video_token_id\n-                mask_unsqueezed = mask.unsqueeze(-1)\n-                mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)\n-                video_mask = mask_expanded.to(inputs_embeds.device)\n+        if pixel_values_videos is not None:\n+            video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+            video_embeds = torch.cat(video_embeds, dim=0)\n+\n+            if input_ids is None:\n+                video_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                video_mask = video_mask.all(-1)\n+            else:\n+                video_mask = input_ids == self.config.video_token_id\n+\n+            n_video_tokens = (video_mask).sum()\n+            n_video_features = video_embeds.shape[0]\n+            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                )\n \n-                video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n+            video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:\n             attention_mask_tensor = (\n@@ -1586,6 +1596,7 @@ def prepare_inputs_for_generation(\n     def _get_image_nums_and_video_nums(\n         self,\n         input_ids: Optional[torch.LongTensor],\n+        inputs_embeds: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Get the number of images and videos for each sample to calculate the separation length of the sample tensor.\n@@ -1603,10 +1614,31 @@ def _get_image_nums_and_video_nums(\n         video_token_id = self.config.video_token_id\n         vision_start_token_id = self.config.vision_start_token_id\n \n-        vision_start_mask = input_ids == vision_start_token_id\n+        if inputs_embeds is not None:\n+            vision_start_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(vision_start_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            image_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            video_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+        else:\n+            vision_start_mask = input_ids == vision_start_token_id\n+            image_mask = input_ids == image_token_id\n+            video_mask = input_ids == video_token_id\n+\n         vision_first_mask = torch.roll(vision_start_mask, shifts=1, dims=1)\n-        image_mask = input_ids == image_token_id\n-        video_mask = input_ids == video_token_id\n         image_nums = torch.sum(vision_first_mask & image_mask, dim=1)\n         video_nums = torch.sum(vision_first_mask & video_mask, dim=1)\n \n@@ -1632,7 +1664,9 @@ def _expand_inputs_for_generation(\n         def _expand_dict_for_generation_visual(dict_to_expand):\n             image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n             video_grid_thw = model_kwargs.get(\"video_grid_thw\", None)\n-            image_nums, video_nums = self._get_image_nums_and_video_nums(input_ids)\n+            image_nums, video_nums = self._get_image_nums_and_video_nums(\n+                input_ids, inputs_embeds=model_kwargs.get(\"inputs_embeds\", None)\n+            )\n \n             def _repeat_interleave_samples(x, lengths, repeat_times):\n                 samples = torch.split(x, lengths)\n@@ -1688,10 +1722,7 @@ def _expand_dict_for_generation(dict_to_expand):\n                     dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n             return dict_to_expand\n \n-        # input_ids is required for expanding visual inputs\n-        # If input_ids is unavailable, visual inputs will not be used; therefore, there is no need to expand visual inputs.\n-        if input_ids is not None and input_ids.numel() != 0:\n-            model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n+        model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n \n         if input_ids is not None:\n             input_ids = input_ids.repeat_interleave(expand_size, dim=0)"
        },
        {
            "sha": "2686194e09efde6e4ba0fbd78d298511048815d9",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 42,
            "deletions": 32,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -609,41 +609,51 @@ def forward(\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n-            if pixel_values is not None:\n-                image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-                image_embeds = torch.cat(image_embeds, dim=0)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n-                n_image_features = image_embeds.shape[0]\n-                if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n-                    raise ValueError(\n-                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                    )\n \n-                mask = input_ids == self.config.image_token_id\n-                mask_unsqueezed = mask.unsqueeze(-1)\n-                mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)\n-                image_mask = mask_expanded.to(inputs_embeds.device)\n-\n-                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n-\n-            if pixel_values_videos is not None:\n-                video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-                video_embeds = torch.cat(video_embeds, dim=0)\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n-                n_video_features = video_embeds.shape[0]\n-                if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n-                    raise ValueError(\n-                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                    )\n+        if pixel_values is not None:\n+            image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+            image_embeds = torch.cat(image_embeds, dim=0)\n+\n+            if input_ids is None:\n+                image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                image_mask = image_mask.all(-1)\n+            else:\n+                image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (image_mask).sum()\n+            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            n_image_features = image_embeds.shape[0]\n+            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+            image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n+\n+        if pixel_values_videos is not None:\n+            video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+            video_embeds = torch.cat(video_embeds, dim=0)\n \n-                mask = input_ids == self.config.video_token_id\n-                mask_unsqueezed = mask.unsqueeze(-1)\n-                mask_expanded = mask_unsqueezed.expand_as(inputs_embeds)\n-                video_mask = mask_expanded.to(inputs_embeds.device)\n+            if input_ids is None:\n+                video_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                video_mask = video_mask.all(-1)\n+            else:\n+                video_mask = input_ids == self.config.video_token_id\n+\n+            n_video_tokens = (video_mask).sum()\n+            n_video_features = video_embeds.shape[0]\n+            video_mask = video_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                )\n \n-                video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n+            video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:\n             attention_mask_tensor = ("
        },
        {
            "sha": "311622b22221f2790243258e5aff57f511f7de31",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 73,
            "deletions": 41,
            "changes": 114,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -1182,41 +1182,52 @@ def forward(\n \n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n-            if pixel_values is not None:\n-                image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n-                image_embeds = torch.cat(image_embeds, dim=0)\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum().item()\n-                n_image_features = image_embeds.shape[0]\n-                if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n-                    raise ValueError(\n-                        f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n-                    )\n-                image_mask = (\n-                    (input_ids == self.config.image_token_id)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+\n+        if pixel_values is not None:\n+            image_embeds = self.get_image_features(pixel_values, image_grid_thw)\n+            image_embeds = torch.cat(image_embeds, dim=0)\n+\n+            if input_ids is None:\n+                image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n-\n-            if pixel_values_videos is not None:\n-                video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n-                video_embeds = torch.cat(video_embeds, dim=0)\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum().item()\n-                n_video_features = video_embeds.shape[0]\n-                if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n-                    raise ValueError(\n-                        f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n-                    )\n-                video_mask = (\n-                    (input_ids == self.config.video_token_id)\n-                    .unsqueeze(-1)\n-                    .expand_as(inputs_embeds)\n-                    .to(inputs_embeds.device)\n+                image_mask = image_mask.all(-1)\n+            else:\n+                image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = image_mask.sum()\n+            image_mask = image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+            n_image_features = image_embeds.shape[0]\n+            if not is_torchdynamo_compiling() and n_image_tokens != n_image_features:\n+                raise ValueError(\n+                    f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n+                )\n+\n+            image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n+\n+        if pixel_values_videos is not None:\n+            video_embeds = self.get_video_features(pixel_values_videos, video_grid_thw)\n+            video_embeds = torch.cat(video_embeds, dim=0)\n+\n+            if input_ids is None:\n+                video_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n                 )\n-                video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n-                inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n+                n_video_tokens = (video_mask).sum(dim=1).sum(dim=0)[0]\n+            else:\n+                video_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n+                video_mask = video_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+                n_video_tokens = (input_ids == self.config.image_token_id).sum()\n+\n+            n_video_features = video_embeds.shape[0]\n+            if not is_torchdynamo_compiling() and n_video_tokens != n_video_features:\n+                raise ValueError(\n+                    f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\"\n+                )\n+\n+            video_embeds = video_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n+            inputs_embeds = inputs_embeds.masked_scatter(video_mask, video_embeds)\n \n         if position_ids is None:\n             attention_mask_tensor = (\n@@ -1480,6 +1491,7 @@ def prepare_inputs_for_generation(\n     def _get_image_nums_and_video_nums(\n         self,\n         input_ids: Optional[torch.LongTensor],\n+        inputs_embeds: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Get the number of images and videos for each sample to calculate the separation length of the sample tensor.\n@@ -1497,10 +1509,31 @@ def _get_image_nums_and_video_nums(\n         video_token_id = self.config.video_token_id\n         vision_start_token_id = self.config.vision_start_token_id\n \n-        vision_start_mask = input_ids == vision_start_token_id\n+        if inputs_embeds is not None:\n+            vision_start_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(vision_start_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            image_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+            video_mask = (\n+                inputs_embeds\n+                == self.get_input_embeddings()(\n+                    torch.tensor(video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+            )[..., 0]\n+        else:\n+            vision_start_mask = input_ids == vision_start_token_id\n+            image_mask = input_ids == image_token_id\n+            video_mask = input_ids == video_token_id\n+\n         vision_first_mask = torch.roll(vision_start_mask, shifts=1, dims=1)\n-        image_mask = input_ids == image_token_id\n-        video_mask = input_ids == video_token_id\n         image_nums = torch.sum(vision_first_mask & image_mask, dim=1)\n         video_nums = torch.sum(vision_first_mask & video_mask, dim=1)\n \n@@ -1526,7 +1559,9 @@ def _expand_inputs_for_generation(\n         def _expand_dict_for_generation_visual(dict_to_expand):\n             image_grid_thw = model_kwargs.get(\"image_grid_thw\", None)\n             video_grid_thw = model_kwargs.get(\"video_grid_thw\", None)\n-            image_nums, video_nums = self._get_image_nums_and_video_nums(input_ids)\n+            image_nums, video_nums = self._get_image_nums_and_video_nums(\n+                input_ids, inputs_embeds=model_kwargs.get(\"inputs_embeds\", None)\n+            )\n \n             def _repeat_interleave_samples(x, lengths, repeat_times):\n                 samples = torch.split(x, lengths)\n@@ -1582,10 +1617,7 @@ def _expand_dict_for_generation(dict_to_expand):\n                     dict_to_expand[key] = dict_to_expand[key].repeat_interleave(expand_size, dim=0)\n             return dict_to_expand\n \n-        # input_ids is required for expanding visual inputs\n-        # If input_ids is unavailable, visual inputs will not be used; therefore, there is no need to expand visual inputs.\n-        if input_ids is not None and input_ids.numel() != 0:\n-            model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n+        model_kwargs = _expand_dict_for_generation_visual(model_kwargs)\n \n         if input_ids is not None:\n             input_ids = input_ids.repeat_interleave(expand_size, dim=0)"
        },
        {
            "sha": "155f8110a54e02cfd0b5e7d08c5f03e708e3e6dd",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 16,
            "deletions": 30,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -595,7 +595,14 @@ def inputs_merger(\n         \"\"\"\n         _, patch_size, _ = image_hidden_states.shape\n \n-        image_mask = input_ids == self.image_token_id\n+        if input_ids is None:\n+            image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            image_mask = image_mask[..., 0]  # slice off the hidden dim\n+        else:\n+            image_mask = input_ids == self.config.image_token_id\n+\n         num_image_tokens = image_mask.sum(dim=1)\n         if not torch.all(num_image_tokens % patch_size == 0):\n             raise ValueError(\"At least one sample has <image> tokens not divisible by patch_size.\")\n@@ -717,27 +724,22 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n-        past_seen_tokens = 0\n-        if use_cache:\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            past_seen_tokens = past_key_values.get_seq_length()\n-\n-        if inputs_embeds is not None and input_ids is None and past_seen_tokens == 0:\n-            raise ValueError(\"When first calling the model, if input_embeds are passed, input_ids should not be None.\")\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if inputs_embeds is None:\n             inputs_embeds = self.text_model.get_input_embeddings()(input_ids).to(input_ids.device)\n \n         # START VISUAL INPUTS INTEGRATION\n         if pixel_values is not None and image_hidden_states is not None:\n             raise ValueError(\"You cannot specify both pixel_values and image_hidden_states at the same time\")\n-        elif pixel_values is not None:\n-            image_hidden_states = self.get_image_features(pixel_values, pixel_attention_mask).to(input_ids.device)\n+\n+        if pixel_values is not None:\n+            image_hidden_states = self.get_image_features(pixel_values, pixel_attention_mask).to(inputs_embeds.device)\n         elif image_hidden_states is not None:\n-            image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=input_ids.device)\n+            image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=inputs_embeds.device)\n \n-        if inputs_embeds is not None and image_hidden_states is not None:\n+        if image_hidden_states is not None:\n             # When we generate, we don't want to replace the potential image_token_id that we generated by images\n             # that simply don't exist\n             inputs_embeds = self.inputs_merger(\n@@ -996,27 +998,11 @@ def prepare_inputs_for_generation(\n             **kwargs,\n         )\n \n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        # but IDEFICS requires both ids and embeds to be present\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs[\"input_ids\"] = input_ids\n-\n-        if image_hidden_states is not None:\n+        if image_hidden_states is not None or cache_position[0] != 0:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_attention_mask\"] = None\n \n         return model_inputs\n \n-    def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_decoder, **kwargs):\n-        model_kwargs = super()._update_model_kwargs_for_generation(\n-            outputs=outputs,\n-            model_kwargs=model_kwargs,\n-            is_encoder_decoder=is_encoder_decoder,\n-            **kwargs,\n-        )\n-        # Get the precomputed image_hidden_states\n-        model_kwargs[\"image_hidden_states\"] = outputs.image_hidden_states\n-        return model_kwargs\n-\n \n __all__ = [\"SmolVLMForConditionalGeneration\", \"SmolVLMPreTrainedModel\", \"SmolVLMModel\", \"SmolVLMVisionTransformer\"]"
        },
        {
            "sha": "e20f3d7b6bf02141b386381e1bef9d9c62b203cb",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 15,
            "deletions": 13,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -180,7 +180,14 @@ def inputs_merger(\n     ):\n         _, patch_size, _ = image_hidden_states.shape\n \n-        image_mask = input_ids == self.image_token_id\n+        if input_ids is None:\n+            image_mask = inputs_embeds == self.get_input_embeddings()(\n+                torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+            )\n+            image_mask = image_mask[..., 0]  # slice off the hidden dim\n+        else:\n+            image_mask = input_ids == self.config.image_token_id\n+\n         num_image_tokens = image_mask.sum(dim=1)\n         if not torch.all(num_image_tokens % patch_size == 0):\n             raise ValueError(\"At least one sample has <image> tokens not divisible by patch_size.\")\n@@ -296,27 +303,22 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n-        past_seen_tokens = 0\n-        if use_cache:\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            past_seen_tokens = past_key_values.get_seq_length()\n-\n-        if inputs_embeds is not None and input_ids is None and past_seen_tokens == 0:\n-            raise ValueError(\"When first calling the model, if input_embeds are passed, input_ids should not be None.\")\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if inputs_embeds is None:\n             inputs_embeds = self.text_model.get_input_embeddings()(input_ids).to(input_ids.device)\n \n         # START VISUAL INPUTS INTEGRATION\n         if pixel_values is not None and image_hidden_states is not None:\n             raise ValueError(\"You cannot specify both pixel_values and image_hidden_states at the same time\")\n-        elif pixel_values is not None:\n-            image_hidden_states = self.get_image_features(pixel_values, pixel_attention_mask).to(input_ids.device)\n+\n+        if pixel_values is not None:\n+            image_hidden_states = self.get_image_features(pixel_values, pixel_attention_mask).to(inputs_embeds.device)\n         elif image_hidden_states is not None:\n-            image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=input_ids.device)\n+            image_hidden_states = image_hidden_states.to(dtype=self.dtype, device=inputs_embeds.device)\n \n-        if inputs_embeds is not None and image_hidden_states is not None:\n+        if image_hidden_states is not None:\n             # When we generate, we don't want to replace the potential image_token_id that we generated by images\n             # that simply don't exist\n             inputs_embeds = self.inputs_merger("
        },
        {
            "sha": "d5171e8831e8c53d6892851f23849b4aef0d6acc",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 22,
            "deletions": 12,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -328,12 +328,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if (pixel_values_images is not None or pixel_values_videos is not None) and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both `pixel_values_images`/`pixel_values_videos` and `inputs_embeds` at the same \"\n-                \"time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -343,10 +337,18 @@ def forward(\n                 vision_feature_layer=vision_feature_layer,\n                 vision_feature_select_strategy=vision_feature_select_strategy,\n             )\n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\"\n@@ -359,10 +361,18 @@ def forward(\n                 pixel_values_videos=pixel_values_videos, vision_feature_layer=vision_feature_layer\n             )\n \n-            special_image_mask = (input_ids == self.config.video_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.video_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.video_token_id\n+\n+            n_video_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != video_features.numel():\n-                n_video_tokens = (input_ids == self.config.video_token_id).sum()\n                 n_video_features = video_features.shape[0] * video_features.shape[1]\n                 raise ValueError(\n                     f\"Video features and video tokens do not match: tokens: {n_video_tokens}, features {n_video_features}\""
        },
        {
            "sha": "7e668bf978de42b5f1bc3f96a0dec8afeaa39c4a",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -233,11 +233,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -246,10 +241,18 @@ def forward(\n                 pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n             )\n \n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "97458112ad5d37e9bab584f4746782e8e7df276e",
            "filename": "src/transformers/models/vipllava/modular_vipllava.py",
            "status": "modified",
            "additions": 11,
            "deletions": 8,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -136,11 +136,6 @@ def forward(\n         if (input_ids is None) ^ (inputs_embeds is not None):\n             raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n \n-        if pixel_values is not None and inputs_embeds is not None:\n-            raise ValueError(\n-                \"You cannot specify both pixel_values and inputs_embeds at the same time, and must specify either one\"\n-            )\n-\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n@@ -149,10 +144,18 @@ def forward(\n                 pixel_values=pixel_values, vision_feature_layers=vision_feature_layers\n             )\n \n-            special_image_mask = (input_ids == self.config.image_token_id).unsqueeze(-1)\n-            special_image_mask = special_image_mask.expand_as(inputs_embeds).to(inputs_embeds.device)\n+            if input_ids is None:\n+                special_image_mask = inputs_embeds == self.get_input_embeddings()(\n+                    torch.tensor(self.config.image_token_id, dtype=torch.long, device=inputs_embeds.device)\n+                )\n+                special_image_mask = special_image_mask.all(-1)\n+            else:\n+                special_image_mask = input_ids == self.config.image_token_id\n+\n+            n_image_tokens = (special_image_mask).sum()\n+            special_image_mask = special_image_mask.unsqueeze(-1).expand_as(inputs_embeds).to(inputs_embeds.device)\n+\n             if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n-                n_image_tokens = (input_ids == self.config.image_token_id).sum()\n                 n_image_features = image_features.shape[0] * image_features.shape[1]\n                 raise ValueError(\n                     f\"Image features and image tokens do not match: tokens: {n_image_tokens}, features {n_image_features}\""
        },
        {
            "sha": "c9e7f9f4f13a3dd7937696d614937811f2d0278a",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 77,
            "deletions": 69,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -118,27 +118,6 @@\n from transformers.utils import is_sklearn_available\n \n \n-# TODO: raushan remove this when VLMs start accepting input embeds\n-VLM_CLASS_NAMES = [\n-    \"llava\",\n-    \"idefics2\",\n-    \"idefics3\",\n-    \"mllama\",\n-    \"paligemma\",\n-    \"emu3\",\n-    \"gotocr2\",\n-    \"qwen2vl\",\n-    \"qwen2_5_vl\",\n-    \"ayavision\",\n-    \"janus\",\n-    \"gemma3\",\n-    \"mistral3\",\n-    \"chameleon\",\n-    \"internvl\",\n-    \"qwen2_5omni\",  # the file is named `qwen2_5_omni`, but the model class is `Qwen2_5Omni`\n-]\n-\n-\n class GenerationTesterMixin:\n     input_name = \"input_ids\"\n     model_tester = None\n@@ -1228,7 +1207,23 @@ def test_prompt_lookup_decoding_matches_greedy_search(self):\n                     \"blip2\",  # overridden `generate()`\n                     \"instructblip\",\n                     \"instructblipvideo\",\n-                    *VLM_CLASS_NAMES,  # shouldn't suggest image tokens\n+                    # All models below: shouldn't suggest image tokens. Can be fixed by passing `suppress_ids` to candidate generator: @joaa @raushan\n+                    \"llava\",\n+                    \"idefics2\",\n+                    \"idefics3\",\n+                    \"mllama\",\n+                    \"paligemma\",\n+                    \"emu3\",\n+                    \"gotocr2\",\n+                    \"qwen2vl\",\n+                    \"qwen2_5_vl\",\n+                    \"ayavision\",\n+                    \"janus\",\n+                    \"gemma3\",\n+                    \"mistral3\",\n+                    \"chameleon\",\n+                    \"internvl\",\n+                    \"qwen2_5omni\",  # the file is named `qwen2_5_omni`, but the model class is `Qwen2_5Omni`,\n                 ]\n             ):\n                 self.skipTest(reason=\"May fix in the future: need model-specific fixes\")\n@@ -1641,6 +1636,58 @@ def test_past_key_values_format(self, custom_all_cache_shapes=None):\n                     self.assertEqual(self_attention_layer_key_cache.shape, all_cache_shapes[i][0])\n                     self.assertEqual(self_attention_layer_value_cache.shape, all_cache_shapes[i][1])\n \n+    @pytest.mark.generate\n+    def test_generate_from_random_inputs_embeds(self):\n+        \"\"\"\n+        Text-only: Tests that different `inputs_embeds` generate different outputs in models with `main_input==\"input_ids\"`.\n+        Some models have 'images' as main input and thus can't generate with random text embeddings.\n+        See `test_generate_from_inputs_embeds` for more general checks.\n+        \"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+\n+            if config.is_encoder_decoder:\n+                continue\n+            config.is_decoder = True\n+\n+            model = model_class(config).to(torch_device).eval()\n+            if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n+                continue\n+\n+            #  No easy fix, let's skip the test for now\n+            has_complex_embeds_computation = any(\n+                model_name in model_class.__name__.lower() for model_name in [\"moshi\"]\n+            )\n+\n+            if model_class.main_input_name != \"input_ids\" or has_complex_embeds_computation:\n+                self.skipTest(\n+                    \"The model's main input name in not `input_ids` and we need kwargs from input dict as well.\"\n+                )\n+\n+            if hasattr(config, \"scale_embedding\"):\n+                config.scale_embedding = False\n+\n+            generation_kwargs = {\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n+                \"do_sample\": False,\n+                \"max_new_tokens\": 5,\n+                \"min_new_tokens\": 5,  # generate exactly 5 tokens\n+            }\n+\n+            input_ids = inputs_dict.pop(\"input_ids\")\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+            outputs_from_embeds = model.generate(input_ids, inputs_embeds=inputs_embeds, **generation_kwargs)\n+\n+            # If we pass different inputs_embeds, we should get different outputs (the output text may be the\n+            # same, but the logits will almost surely be different)\n+            random_embeds = torch.rand_like(inputs_embeds)\n+            outputs_from_rand_embeds = model.generate(\n+                input_ids=input_ids, inputs_embeds=random_embeds, **generation_kwargs\n+            )\n+            for i in range(len(outputs_from_rand_embeds.scores)):\n+                self.assertFalse(torch.allclose(outputs_from_embeds.scores[i], outputs_from_rand_embeds.scores[i]))\n+\n     @pytest.mark.generate\n     @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n     def test_generate_from_inputs_embeds(self, _, num_beams):\n@@ -1662,34 +1709,22 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n                 continue\n \n             # There are a few exception patterns in this test:\n-            # 1 - Some models can't generate without `input_ids`, when `inputs_embeds` are passed\n-            requires_inputs_ids = any(model_name in model_class.__name__.lower() for model_name in [\"idefics\"])\n-            # 2 - Complex `inputs_embeds` computation, i.e. the correct computation of inputs embeds is more complex\n+            # 1 - Complex `inputs_embeds` computation, i.e. the correct computation of inputs embeds is more complex\n             # than calling the embedding layer with `input_ids`. Subcases of this exception:\n-            #   2.A - Ignore `scale_embedding`, if the model supports it (it is controlled by a model-dependent flag)\n+            #   1.A - Ignore `scale_embedding`, if the model supports it (it is controlled by a model-dependent flag)\n             if hasattr(config, \"scale_embedding\"):\n                 config.scale_embedding = False\n-            #   2.B - Some VLMs assume `inputs_embeds` and `pixel_values` are mutually exclusive AND fall in the\n-            #   exception above (complex `inputs_embeds` computation). Popping `pixel_values` allow us to run the\n-            #   checks without adding test complexity. Ditto for `pixel_values_videos` and `pixel_values_images`\n-            pixel_values_is_mutually_exclusive = any(\n-                model_name in model_class.__name__.lower() for model_name in VLM_CLASS_NAMES\n-            )\n-            if pixel_values_is_mutually_exclusive:\n-                inputs_dict.pop(\"pixel_values\", None)\n-                inputs_dict.pop(\"pixel_values_videos\", None)\n-                inputs_dict.pop(\"pixel_values_images\", None)\n             # HACK - in the case of granite speech, input_features and inputs_embeds are mutually exclusive;\n             # this is similar to VLMs and should likely be standardized for similar audio models in the future,\n             # then made generic here.\n             if \"granitespeech\" in model_class.__name__.lower():\n                 inputs_dict.pop(\"input_features\", None)\n \n-            #   2.C - No easy fix, let's skip the check that compares the outputs from `input_ids` and `inputs_embeds`\n+            #   1.B - No easy fix, let's skip the check that compares the outputs from `input_ids` and `inputs_embeds`\n             has_complex_embeds_computation = any(\n                 model_name in model_class.__name__.lower() for model_name in [\"moshi\"]\n             )\n-            # 3 - `inputs_dict` doesn't contain `attention_mask`. When `attention_mask` is not passed to generate,\n+            # 2 - `inputs_dict` doesn't contain `attention_mask`. When `attention_mask` is not passed to generate,\n             # we infer it from `input_ids`. The last test case will fail if there is a pad token in the original input.\n             missing_attention_mask = \"attention_mask\" not in inputs_dict\n \n@@ -1702,31 +1737,23 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n                 \"do_sample\": False,\n                 \"max_new_tokens\": 5,\n                 \"min_new_tokens\": 5,  # generate exactly 5 tokens\n+                \"use_cache\": True,\n             }\n-            outputs_from_ids = model.generate(input_ids, **generation_kwargs, **inputs_dict)\n+            outputs_from_ids = model.generate(input_ids=input_ids, **generation_kwargs, **inputs_dict)\n             self.assertEqual(outputs_from_ids.sequences.shape[:2], (input_ids.shape[0], input_ids.shape[1] + 5))\n \n             # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output).\n             # The output of the two calls should be the same.\n             inputs_embeds = model.get_input_embeddings()(input_ids)\n             outputs_from_embeds = model.generate(\n-                input_ids, inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n+                input_ids=input_ids, inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n             )\n             if not has_complex_embeds_computation:\n                 self._check_similar_generate_outputs(outputs_from_ids, outputs_from_embeds)\n \n-            # If we pass different inputs_embeds, we should get different outputs (the output text may be the\n-            # same, but the logits will almost surely be different)\n-            random_embeds = torch.rand_like(inputs_embeds)\n-            outputs_from_rand_embeds = model.generate(\n-                input_ids, inputs_embeds=random_embeds, **generation_kwargs, **inputs_dict\n-            )\n-            for i in range(len(outputs_from_rand_embeds.scores)):\n-                self.assertFalse(torch.allclose(outputs_from_embeds.scores[i], outputs_from_rand_embeds.scores[i]))\n-\n             # input_ids is not a required input on most models -- if we don't pass it, the newly generated tokens will\n             # be the same\n-            if not (requires_inputs_ids or missing_attention_mask):\n+            if not missing_attention_mask:\n                 outputs_from_embeds_wo_ids = model.generate(\n                     inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n                 )\n@@ -1753,17 +1780,6 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n             if \"inputs_embeds\" not in inspect.signature(model.prepare_inputs_for_generation).parameters.keys():\n                 self.skipTest(reason=\"This model does not support `inputs_embeds` in generation\")\n \n-            #   Some VLMs assume `inputs_embeds` and `pixel_values` are mutually exclusive AND fall in the\n-            #   exception above (complex `inputs_embeds` computation). Popping `pixel_values` allow us to run the\n-            #   checks without adding test complexity. Ditto for `pixel_values_videos` and `pixel_values_images`\n-            pixel_values_is_mutually_exclusive = any(\n-                model_name in model_class.__name__.lower() for model_name in VLM_CLASS_NAMES\n-            )\n-            if pixel_values_is_mutually_exclusive:\n-                inputs_dict.pop(\"pixel_values\", None)\n-                inputs_dict.pop(\"pixel_values_videos\", None)\n-                inputs_dict.pop(\"pixel_values_images\", None)\n-\n             input_ids = inputs_dict.pop(\"input_ids\")\n \n             model.config.use_cache = True\n@@ -1925,14 +1941,6 @@ def test_generate_continue_from_inputs_embeds(self):\n             if \"past_key_values\" not in outputs:\n                 self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n \n-            pixel_values_is_mutually_exclusive = any(\n-                model_name in model_class.__name__.lower() for model_name in VLM_CLASS_NAMES\n-            )\n-            if pixel_values_is_mutually_exclusive:\n-                inputs_dict.pop(\"pixel_values\", None)\n-                inputs_dict.pop(\"pixel_values_videos\", None)\n-                inputs_dict.pop(\"pixel_values_images\", None)\n-\n             input_ids = inputs_dict.pop(\"input_ids\")\n \n             model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1"
        },
        {
            "sha": "ece423338a69eb3bd0db5d8a05390da5abd99ebf",
            "filename": "tests/models/aria/test_modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 51,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_modeling_aria.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -189,49 +189,6 @@ def setUp(self):\n         self.model_tester = AriaVisionText2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=AriaConfig, has_text_modality=False)\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     @unittest.skip(\n         reason=\"This architecture seems to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )\n@@ -270,14 +227,6 @@ def test_initialization(self):\n     def test_dola_decoding_sample(self):\n         pass\n \n-    @unittest.skip(reason=\"Unsupported\")\n-    def test_generate_from_inputs_embeds_0_greedy(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Unsupported\")\n-    def test_generate_from_inputs_embeds_1_beam_search(self):\n-        pass\n-\n     @unittest.skip(reason=\"Dynamic control flow due to MoE\")\n     def test_generate_with_static_cache(self):\n         pass"
        },
        {
            "sha": "d472e0eb90f134223c02184f453f6aec9330076e",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 1,
            "deletions": 48,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -62,7 +62,7 @@ def __init__(\n         bos_token_id=0,\n         eos_token_id=0,\n         pad_token_id=0,\n-        image_token_index=1,\n+        image_token_index=2,\n         num_channels=3,\n         image_size=64,\n         model_type=\"aya_vision\",\n@@ -183,49 +183,6 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n     def test_model_outputs_equivalence(self, **kwargs):\n         pass\n@@ -285,10 +242,6 @@ def test_generate_with_static_cache(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    @unittest.skip(\"Cohere2 has HybridCache and doesn't support progressive generation using input embeds.\")\n-    def test_generate_continue_from_inputs_embeds(self):\n-        pass\n-\n     @unittest.skip(\"Failing because of unique cache (HybridCache)\")\n     def test_multi_gpu_data_parallel_forward(self):\n         pass"
        },
        {
            "sha": "af95bbb2c32815837bdabe93d9a81fb546703d0f",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -20,7 +20,6 @@\n import numpy as np\n import pytest\n import requests\n-from parameterized import parameterized\n \n from transformers import CONFIG_MAPPING, Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n from transformers.testing_utils import (\n@@ -674,15 +673,6 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             # They should result in very similar logits\n             torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n-    @unittest.skip(\"BLIP2 cannot generate only from input ids, and requires pixel values in all cases to be present\")\n-    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n-    def test_generate_from_inputs_embeds(self, _, num_beams):\n-        pass\n-\n-    @unittest.skip(\"BLIP2 cannot generate only from input ids, and requires pixel values in all cases to be present\")\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n \n # this class is based on `T5ModelTester` found in tests/models/t5/test_modeling_t5.py\n class Blip2TextModelTester:"
        },
        {
            "sha": "fb5847fd6024be9074d7aab7dfbd9010dc364f46",
            "filename": "tests/models/chameleon/test_modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchameleon%2Ftest_modeling_chameleon.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -355,49 +355,6 @@ def test_mismatching_num_image_tokens(self):\n             pixel_values = torch.cat([pixel_values, pixel_values], dim=0)\n             _ = model(input_ids=input_ids, pixel_values=pixel_values)\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n \n @require_torch\n class ChameleonIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "3cdfc0622738e8af35da41daf516758ea5985750",
            "filename": "tests/models/colpali/test_modeling_colpali.py",
            "status": "modified",
            "additions": 0,
            "deletions": 44,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcolpali%2Ftest_modeling_colpali.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -189,50 +189,6 @@ def setUp(self):\n         self.model_tester = ColPaliForRetrievalModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=ColPaliConfig, has_text_modality=False)\n \n-        # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     @slow\n     @require_vision\n     def test_colpali_forward_inputs(self):"
        },
        {
            "sha": "6d9780509b50b649d74526bdd620d60bcf9029fc",
            "filename": "tests/models/emu3/test_modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Femu3%2Ftest_modeling_emu3.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -331,49 +331,6 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     @unittest.skip(\n         \"Emu3 has a VQ module that uses `weight.data` directly in forward which prevent offloding on that module\"\n     )"
        },
        {
            "sha": "1f32df39749069413e63f62da29e56bd4ff41473",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -131,10 +131,6 @@ def test_generate_with_static_cache(self):\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    @unittest.skip(\"Gemma3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\")\n-    def test_generate_continue_from_inputs_embeds(self):\n-        pass\n-\n     @unittest.skip(\"Gemma3 has HybridCache which auto-compiles. Compile and FA2 don't work together.\")\n     def test_eager_matches_fa2_generate(self):\n         pass"
        },
        {
            "sha": "48d4a9b858ea8da321fce03c4f25d0150939082e",
            "filename": "tests/models/glm4v/test_modeling_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 32,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_modeling_glm4v.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -13,12 +13,10 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch GLM-4.1V model.\"\"\"\n \n-import copy\n import gc\n import unittest\n \n import requests\n-from parameterized import parameterized\n \n from transformers import (\n     AutoProcessor,\n@@ -237,11 +235,6 @@ def prepare_config_and_inputs_for_generate(self, batch_size=2):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n-    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n-    @unittest.skip(\"Cannot generate from inputs embeds with pixel values\")\n-    def test_generate_from_inputs_embeds(self):\n-        pass\n-\n     @unittest.skip(reason=\"Size mismatch\")\n     def test_multi_gpu_data_parallel_forward(self):\n         pass\n@@ -250,34 +243,11 @@ def test_multi_gpu_data_parallel_forward(self):\n     def test_model_is_small(self):\n         pass\n \n-    @unittest.skip(\"Cannot generate from inputs embeds with pixel values\")\n+    @unittest.skip(\"Error with compilation\")\n     def test_generate_from_inputs_embeds_with_static_cache(self):\n         pass\n \n-    # The multimodal base model embeds will not match ids, due to pixel values. We can't change base test\n-    # because in some models `pixel_values` are required. Will be fixed when we add support for merging `embeds+pixels`\n-    # TODO: @raushan\n-\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-            del inputs[\"image_grid_thw\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-            with torch.no_grad():\n-                model(**inputs)[0]\n-\n+    # RoPE index doesn't match when using embeddings\n     def test_inputs_embeds_matches_input_ids(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n "
        },
        {
            "sha": "87f182ac9cdb7e51e4f46b06be0de37ea2f0f832",
            "filename": "tests/models/got_ocr2/test_modeling_got_ocr2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 59,
            "changes": 65,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgot_ocr2%2Ftest_modeling_got_ocr2.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -51,9 +51,6 @@ def __init__(\n         num_channels=3,\n         ignore_index=-100,\n         image_size=64,\n-        bos_token_id=0,\n-        eos_token_id=0,\n-        pad_token_id=0,\n         image_token_index=1,\n         model_type=\"got_ocr2\",\n         is_training=True,\n@@ -71,6 +68,9 @@ def __init__(\n             \"rope_theta\": 10000,\n             \"mlp_ratio\": 4,\n             \"tie_word_embeddings\": True,\n+            \"bos_token_id\": 2,\n+            \"eos_token_id\": 3,\n+            \"pad_token_id\": 4,\n         },\n         vision_config={\n             \"num_hidden_layers\": 2,\n@@ -85,9 +85,9 @@ def __init__(\n     ):\n         self.parent = parent\n         self.ignore_index = ignore_index\n-        self.bos_token_id = bos_token_id\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n+        self.bos_token_id = text_config[\"bos_token_id\"]\n+        self.eos_token_id = text_config[\"eos_token_id\"]\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n         self.image_token_index = image_token_index\n         self.model_type = model_type\n         self.text_config = text_config\n@@ -109,9 +109,6 @@ def get_config(self):\n             text_config=self.text_config,\n             vision_config=self.vision_config,\n             model_type=self.model_type,\n-            bos_token_id=self.bos_token_id,\n-            eos_token_id=self.eos_token_id,\n-            pad_token_id=self.pad_token_id,\n             image_token_index=self.image_token_index,\n         )\n \n@@ -127,7 +124,6 @@ def prepare_config_and_inputs_for_common(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n \n-        # input_ids[:, -1] = self.pad_token_id\n         input_ids[input_ids == self.image_token_index] = self.pad_token_id\n         input_ids[:, : self.num_image_tokens] = self.image_token_index\n \n@@ -181,55 +177,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n-    @unittest.skip(\n-        reason=\"VLMs can't generate from inputs embeds and pixels. This can be tested as part of bacbone LM, no need to run the test for VLMs\"\n-    )\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n     @unittest.skip(\n         reason=\"GotOcr2's language backbone is Qwen2 which uses GQA so the KV cache is a non standard format\"\n     )"
        },
        {
            "sha": "94ab9491dfba281127349743f078f480927133dc",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 12,
            "deletions": 7,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -315,13 +315,6 @@ def prepare_config_and_inputs_for_common(self):\n     def prepare_pixel_values(self):\n         return floats_tensor([self.batch_size, self.num_channels, self.image_size, self.image_size])\n \n-    @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n-    @unittest.skip(reason=\"Idefics has a hard requirement on SDPA, skipping this test\")\n-    def test_eager_matches_sdpa_inference(\n-        self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n-    ):\n-        pass\n-\n \n @require_torch\n class IdeficsModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixin, unittest.TestCase):\n@@ -611,6 +604,12 @@ def test_model_from_pretrained(self):\n     def test_sdpa_can_dispatch_non_composite_models(self):\n         pass\n \n+    @unittest.skip(reason=\"Idefics can't do text-only inference\")\n+    def test_generate_from_random_inputs_embeds(\n+        self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+    ):\n+        pass\n+\n \n @require_torch\n class IdeficsForVisionText2TextTest(IdeficsModelTest, GenerationTesterMixin, unittest.TestCase):\n@@ -899,6 +898,12 @@ def test_sdpa_can_dispatch_non_composite_models(self):\n     def test_generation_tester_mixin_inheritance(self):\n         pass\n \n+    @unittest.skip(reason=\"Idefics can't do text-only inference\")\n+    def test_generate_from_random_inputs_embeds(\n+        self, name, torch_dtype, padding_side, use_attention_mask, output_attentions, enable_kernels\n+    ):\n+        pass\n+\n \n @require_torch\n @require_vision"
        },
        {
            "sha": "7bd0656fff2deed8a36a2ab35fdc3b05ca7164d5",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -108,6 +108,7 @@ def __init__(\n         image_token_id=99,\n     ):\n         self.parent = parent\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n         self.is_training = is_training\n         self.batch_size = batch_size\n         self.num_images = num_images\n@@ -158,6 +159,7 @@ def prepare_config_and_inputs_for_common(self):\n \n         # For simplicity just set the last n tokens to the image token\n         n_image_tokens_per_batch = self.num_images * self.perceiver_config[\"resampler_n_latents\"]\n+        input_ids[input_ids == self.image_token_id] = self.pad_token_id\n         input_ids[:, -n_image_tokens_per_batch:] = self.image_token_id\n         attention_mask = input_ids.ne(1).to(torch_device)\n         inputs_dict = {"
        },
        {
            "sha": "5cf06a50be10e32735f4617ab2bbad07b5db98b1",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -96,6 +96,7 @@ def __init__(\n         image_token_id=57,\n     ):\n         self.parent = parent\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n         self.is_training = is_training\n         self.batch_size = batch_size\n         self.num_images = num_images\n@@ -148,6 +149,7 @@ def prepare_config_and_inputs_for_common(self):\n \n         # For simplicity just set the last n tokens to the image token\n         n_image_tokens_per_batch = self.seq_length\n+        input_ids[input_ids == self.image_token_id] = self.pad_token_id\n         input_ids[:, -n_image_tokens_per_batch:] = self.image_token_id\n         attention_mask = input_ids.ne(1).to(torch_device)\n         inputs_dict = {"
        },
        {
            "sha": "f4419cba5be3c6263d68e94c939d3899c19561ba",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -20,7 +20,6 @@\n import numpy as np\n import pytest\n import requests\n-from parameterized import parameterized\n \n from transformers import (\n     CONFIG_MAPPING,\n@@ -522,12 +521,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_get_set_embeddings(self):\n         pass\n \n-    @unittest.skip(\n-        \"InstructBLIP cannot generate only from input ids, and requires pixel values in all cases to be present\"\n-    )\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -656,13 +649,6 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             # They should result in very similar logits\n             torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n-    @unittest.skip(\n-        \"InstructBLIP cannot generate only from input ids, and requires pixel values in all cases to be present\"\n-    )\n-    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n-    def test_generate_from_inputs_embeds(self, _, num_beams):\n-        pass\n-\n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\""
        },
        {
            "sha": "d996ab778a5e553b10b80989c17c95a54b0acee9",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -20,7 +20,6 @@\n import numpy as np\n import pytest\n from huggingface_hub import hf_hub_download\n-from parameterized import parameterized\n \n from transformers import (\n     CONFIG_MAPPING,\n@@ -535,12 +534,6 @@ def test_retain_grad_hidden_states_attentions(self):\n     def test_model_common_attributes(self):\n         pass\n \n-    @unittest.skip(\n-        \"InstructBLIPVideo cannot generate only from input ids, and requires pixel values in all cases to be present\"\n-    )\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n     def test_forward_signature(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n \n@@ -669,13 +662,6 @@ def _prepare_model_kwargs(input_ids, attention_mask, signature):\n             # They should result in very similar logits\n             torch.testing.assert_close(next_logits_wo_padding, next_logits_with_padding, rtol=1e-5, atol=1e-5)\n \n-    @unittest.skip(\n-        \"InstructBLIPVideo cannot generate only from input ids, and requires pixel values in all cases to be present\"\n-    )\n-    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n-    def test_generate_from_inputs_embeds(self, _, num_beams):\n-        pass\n-\n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\""
        },
        {
            "sha": "7b5d6a29050386082b2c0e48af09223fb81f24b4",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 56,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -63,9 +63,6 @@ def __init__(\n         image_seq_length=64,\n         vision_feature_layer=-1,\n         ignore_index=-100,\n-        bos_token_id=0,\n-        eos_token_id=0,\n-        pad_token_id=0,\n         image_token_id=1,\n         num_channels=3,\n         image_size=64,\n@@ -85,9 +82,9 @@ def __init__(\n             \"rope_theta\": 10000,\n             \"mlp_ratio\": 4,\n             \"tie_word_embeddings\": True,\n-            \"bos_token_id\": 0,\n-            \"eos_token_id\": 0,\n-            \"pad_token_id\": 0,\n+            \"bos_token_id\": 3,\n+            \"eos_token_id\": 4,\n+            \"pad_token_id\": 5,\n         },\n         vision_config={\n             \"hidden_size\": 32,\n@@ -103,9 +100,9 @@ def __init__(\n     ):\n         self.parent = parent\n         self.ignore_index = ignore_index\n-        self.bos_token_id = bos_token_id\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n+        self.bos_token_id = text_config[\"bos_token_id\"]\n+        self.eos_token_id = text_config[\"eos_token_id\"]\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n         self.image_token_id = image_token_id\n         self.model_type = model_type\n         self.text_config = text_config\n@@ -128,9 +125,6 @@ def get_config(self):\n             text_config=self.text_config,\n             vision_config=self.vision_config,\n             model_type=self.model_type,\n-            bos_token_id=self.bos_token_id,\n-            eos_token_id=self.eos_token_id,\n-            pad_token_id=self.pad_token_id,\n             image_token_id=self.image_token_id,\n             image_seq_length=self.image_seq_length,\n             vision_feature_layer=self.vision_feature_layer,\n@@ -148,7 +142,6 @@ def prepare_config_and_inputs_for_common(self):\n         input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n         attention_mask = torch.ones(input_ids.shape, dtype=torch.long, device=torch_device)\n \n-        # input_ids[:, -1] = self.pad_token_id\n         input_ids[input_ids == self.image_token_id] = self.pad_token_id\n         input_ids[:, : self.image_seq_length] = self.image_token_id\n \n@@ -222,49 +215,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n     def test_sdpa_can_compile_dynamic(self):\n         pass"
        },
        {
            "sha": "254da46ac8827c372a14ebd95638c349f91e2237",
            "filename": "tests/models/janus/test_modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 44,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -153,6 +153,7 @@ def get_config(self):\n             text_config=self.text_config,\n             vision_config=self.vision_config,\n             vq_config=self.get_vq_config(),\n+            image_token_id=self.image_token_index,\n         )\n \n     def prepare_config_and_inputs(self):\n@@ -200,50 +201,6 @@ def setUp(self):\n         self.model_tester = JanusVisionText2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=JanusConfig, has_text_modality=False)\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-            del inputs[\"generation_mode\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # Overwrite inputs_embeds tests because we need to delete \"pixel values\" for VLMs.\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-            del inputs[\"generation_mode\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     def test_sdpa_can_dispatch_composite_models(self):\n         for model_class in self.all_model_classes:\n             config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "e23b8672d26d491abf79d3a1c38f4ded6a3896a8",
            "filename": "tests/models/kosmos2/test_modeling_kosmos2.py",
            "status": "modified",
            "additions": 47,
            "deletions": 8,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_modeling_kosmos2.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -457,14 +457,6 @@ def check_same_values(layer_1, layer_2):\n             # self.assertTrue(model.transformer.wte.weight.shape, model.lm_head.weight.shape)\n             # self.assertTrue(check_same_values(model.transformer.wte, model.lm_head))\n \n-    @pytest.mark.generate\n-    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n-    @unittest.skip(\n-        \"KOSMOS-2 doesn't support inputs embeds. The test isn't skipped by checking input args because KOSMOS-2 has `generate()` overwritten\"\n-    )\n-    def test_generate_from_inputs_embeds(self):\n-        pass\n-\n     @parameterized.expand(TEST_EAGER_MATCHES_SDPA_INFERENCE_PARAMETERIZATION)\n     @require_torch_sdpa\n     @unittest.skip(\"KOSMOS-2 doesn't support padding\")\n@@ -613,6 +605,53 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n             # (Even with this call, there are still memory leak by ~0.04MB)\n             self.clear_torch_jit_class_registry()\n \n+    @pytest.mark.generate\n+    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n+    def test_generate_from_inputs_embeds(self, _, num_beams):\n+        \"\"\"Tests that we can generate from `inputs_embeds` instead of `input_ids` in LLMs, VLMs, etc\"\"\"\n+        # NOTE: overwritten because Kosmos with ids prepares position ids differently from embeds\n+        # If the model get ids, all pad tokens are masked from position ids. That is not possible with embeds\n+\n+        # When supported, tests that the decoder model can generate from `inputs_embeds` instead of `input_ids`\n+        # if fails, you should probably update the `prepare_inputs_for_generation` function\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            config.is_decoder = True\n+\n+            # Skip models without explicit support\n+            model = model_class(config).to(torch_device).eval()\n+\n+            # Traditional way of generating text\n+            input_ids = inputs_dict.pop(\"input_ids\")\n+            input_ids[input_ids == config.get_text_config().pad_token_id] = 0\n+            generation_kwargs = {\n+                \"return_dict_in_generate\": True,\n+                \"output_scores\": True,\n+                \"num_beams\": num_beams,\n+                \"do_sample\": False,\n+                \"max_new_tokens\": 5,\n+                \"min_new_tokens\": 5,  # generate exactly 5 tokens\n+                \"use_cache\": True,\n+            }\n+            outputs_from_ids = model.generate(input_ids=input_ids, **generation_kwargs, **inputs_dict)\n+            self.assertEqual(outputs_from_ids.sequences.shape[:2], (input_ids.shape[0], input_ids.shape[1] + 5))\n+\n+            # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output).\n+            # The output of the two calls should be the same.\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+            outputs_from_embeds = model.generate(\n+                input_ids=input_ids, inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n+            )\n+            self._check_similar_generate_outputs(outputs_from_ids, outputs_from_embeds)\n+\n+            # input_ids is not a required input on most models -- if we don't pass it, the newly generated tokens will\n+            # be the same\n+            outputs_from_embeds_wo_ids = model.generate(\n+                inputs_embeds=inputs_embeds, **generation_kwargs, **inputs_dict\n+            )\n+            outputs_from_embeds.sequences = outputs_from_embeds.sequences[:, inputs_embeds.shape[1] :]\n+            self._check_similar_generate_outputs(outputs_from_embeds_wo_ids, outputs_from_embeds)\n+\n \n # We will verify our results on an image of cute cats\n def prepare_img():"
        },
        {
            "sha": "6900ce27977932b955d3b1da3a6e6a12f5309f25",
            "filename": "tests/models/llava/test_modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava%2Ftest_modeling_llava.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -196,49 +196,6 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
        },
        {
            "sha": "8c91176225f7edd5bb49dd012b4e1cef6491b89f",
            "filename": "tests/models/llava_next/test_modeling_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next%2Ftest_modeling_llava_next.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -222,49 +222,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
        },
        {
            "sha": "352a3ef191506e77b459c7c7a00fd801a2f76d0c",
            "filename": "tests/models/llava_next_video/test_modeling_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 46,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_next_video%2Ftest_modeling_llava_next_video.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -86,7 +86,7 @@ def __init__(\n             \"initializer_range\": 0.02,\n             \"num_labels\": 3,\n             \"num_choices\": 4,\n-            \"pad_token_id\": 2,\n+            \"pad_token_id\": 3,\n         },\n         is_training=True,\n         vision_config={\n@@ -234,51 +234,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-            del inputs[\"pixel_values_videos\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-            del inputs[\"pixel_values_videos\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
        },
        {
            "sha": "2b134bf5a0e32640bfcfd254ea8bb5cf0d6137f5",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -230,49 +230,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     def test_odd_sized_image(self):\n         # prepare model configuration\n         config = self.model_tester.get_config()"
        },
        {
            "sha": "9f0e4ef6c5331a383fb9dcfbaf0ddf22981b6568",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "modified",
            "additions": 6,
            "deletions": 52,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -57,9 +57,6 @@ def __init__(\n         image_seq_length=4,\n         vision_feature_layer=-1,\n         ignore_index=-100,\n-        bos_token_id=0,\n-        eos_token_id=0,\n-        pad_token_id=0,\n         image_token_index=1,\n         num_channels=3,\n         image_size=30,\n@@ -80,9 +77,9 @@ def __init__(\n             \"rms_norm_eps\": 1e-05,\n             \"rope_theta\": 1000000000.0,\n             \"sliding_window\": None,\n-            \"bos_token_id\": 0,\n-            \"eos_token_id\": 0,\n-            \"pad_token_id\": 0,\n+            \"bos_token_id\": 2,\n+            \"eos_token_id\": 3,\n+            \"pad_token_id\": 4,\n         },\n         vision_config={\n             \"model_type\": \"pixtral\",\n@@ -98,9 +95,9 @@ def __init__(\n     ):\n         self.parent = parent\n         self.ignore_index = ignore_index\n-        self.bos_token_id = bos_token_id\n-        self.eos_token_id = eos_token_id\n-        self.pad_token_id = pad_token_id\n+        self.bos_token_id = text_config[\"bos_token_id\"]\n+        self.eos_token_id = text_config[\"eos_token_id\"]\n+        self.pad_token_id = text_config[\"pad_token_id\"]\n         self.image_token_index = image_token_index\n         self.model_type = model_type\n         self.text_config = text_config\n@@ -209,49 +206,6 @@ def test_initialization(self):\n                         msg=f\"Parameter {name} of model {model_class} seems not properly initialized\",\n                     )\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     @unittest.skip(reason=\"Compile not yet supported because in LLava models\")\n     def test_sdpa_can_compile_dynamic(self):\n         pass"
        },
        {
            "sha": "f75270283f1f2c669ba9f3fd628896cc1d89bd3d",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -199,49 +199,6 @@ def setUp(self):\n         self.model_tester = PaliGemmaVisionText2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=PaliGemmaConfig, has_text_modality=False)\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     # Copied from tests.models.llava.test_modeling_llava.LlavaForConditionalGenerationModelTest.test_mismatching_num_image_tokens\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n@@ -327,12 +284,6 @@ def test_determinism(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"VLMs doesn't accept inputs embeds and pixel values at the same time. So if the test passed for backbone LM, it passes for VLM also\"\n-    )\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n     @unittest.skip(\n         \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n     )"
        },
        {
            "sha": "ba8d7a6cac67303336d3f07d9ad429648e20d21a",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 49,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -183,49 +183,6 @@ def setUp(self):\n         self.model_tester = PaliGemma2VisionText2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=PaliGemmaConfig, has_text_modality=False)\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     # Copied from tests.models.llava.test_modeling_llava.LlavaForConditionalGenerationModelTest.test_mismatching_num_image_tokens\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n@@ -311,12 +268,6 @@ def test_determinism(self):\n     def test_feed_forward_chunking(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"VLMs doesn't accept inputs embeds and pixel values at the same time. So if the test passed for backbone LM, it passes for VLM also\"\n-    )\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n     @unittest.skip(\n         \"VLMs need lots of steps to prepare images/mask correctly to get pad-free inputs. Can be tested as part of LLM test\"\n     )"
        },
        {
            "sha": "a8505605c48348740a31f9789c84806ef2d75254",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -22,7 +22,6 @@\n \n import librosa\n import requests\n-from parameterized import parameterized\n \n from transformers import (\n     AutoProcessor,\n@@ -289,10 +288,6 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_sdpa_can_dispatch_on_flash(self):\n         pass\n \n-    @unittest.skip(reason=\"QwenOmniThinker does not use inputs_embeds\")\n-    def test_inputs_embeds(self):\n-        pass\n-\n     @unittest.skip(reason=\"QwenOmniThinker does not support output_hidden_states test\")\n     def test_model_outputs_equivalence(self):\n         pass\n@@ -337,11 +332,6 @@ def test_sdpa_can_dispatch_composite_models(self):\n                     if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n                         raise ValueError(\"The eager model should not have SDPA attention layers\")\n \n-    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n-    @unittest.skip(\"Cannot generate from inputs embeds\")\n-    def test_generate_from_inputs_embeds(self):\n-        pass\n-\n     @unittest.skip(\"Cannot do contrastive generation, has custom `generate()`\")\n     def test_contrastive_generate(self):\n         pass"
        },
        {
            "sha": "019d3793333ccd58452739e8627463868a520e05",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -357,39 +357,10 @@ def test_multi_gpu_data_parallel_forward(self):\n     def test_model_is_small(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"VLMs can't generate from inputs embeds and pixels. This can be tested as part of bacbone LM, no need to run the tes for VLMs\"\n-    )\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n     @is_flaky()  # TODO (joao/raushan): Investigate why this test is flaky on this model\n     def test_prompt_lookup_decoding_matches_greedy_search(self):\n         super().test_prompt_lookup_decoding_matches_greedy_search()\n \n-    # The multimodal base model embeds will not match ids, due to pixel values. We can't change base test\n-    # because in some models `pixel_values` are required. Will be fixed when we add support for merging `embeds+pixels`\n-    # TODO: @raushan\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n \n @require_torch\n class Qwen2_5_VLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "451f940ee00d464eda9154b3dbdaa6f6608d3a60",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 29,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -313,35 +313,6 @@ def test_multi_gpu_data_parallel_forward(self):\n     def test_model_is_small(self):\n         pass\n \n-    @unittest.skip(\n-        reason=\"VLMs can't generate from inputs embeds and pixels. This can be tested as part of bacbone LM, no need to run the test for VLMs\"\n-    )\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n-    # The multimodal base model embeds will not match ids, due to pixel values. We can't change base test\n-    # because in some models `pixel_values` are required. Will be fixed when we add support for merging `embeds+pixels`\n-    # TODO: @raushan\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n \n @require_torch\n class Qwen2VLIntegrationTest(unittest.TestCase):"
        },
        {
            "sha": "280399eb6b8241fccd906b7490d7ad63e2695fd2",
            "filename": "tests/models/smolvlm/test_modeling_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_modeling_smolvlm.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -181,14 +181,6 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n-    @unittest.skip(reason=\"input_embeds cannot be passed in without input_ids\")\n-    def test_inputs_embeds():\n-        pass\n-\n-    @unittest.skip(reason=\"input_embeds cannot be passed in without input_ids\")\n-    def test_inputs_embeds_matches_input_ids(self):\n-        pass\n-\n     @unittest.skip(reason=\"Model does not support padding right\")\n     def test_flash_attn_2_inference_padding_right(self):\n         pass\n@@ -347,10 +339,6 @@ def setUp(self):\n         self.model_tester = SmolVLMVisionText2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=SmolVLMConfig, has_text_modality=False)\n \n-    @unittest.skip(reason=\"input_embeds cannot be passed in without input_ids\")\n-    def test_inputs_embeds():\n-        pass\n-\n     @unittest.skip(reason=\"Model does not support padding right\")\n     def test_flash_attn_2_inference_padding_right(self):\n         pass\n@@ -394,14 +382,6 @@ def test_training_gradient_checkpointing_use_reentrant(self):\n     def test_training_gradient_checkpointing_use_reentrant_false(self):\n         pass\n \n-    @unittest.skip(reason=\"Unsupported\")\n-    def test_generate_from_inputs_embeds_0_greedy(self):\n-        pass\n-\n-    @unittest.skip(reason=\"Unsupported\")\n-    def test_generate_from_inputs_embeds_1_beam_search(self):\n-        pass\n-\n     @unittest.skip(reason=\"Unsupported\")\n     def test_generate_with_static_cache(self):\n         pass"
        },
        {
            "sha": "4c9e4ff3ceb5b6e696d4117a055aa7f7c88bf521",
            "filename": "tests/models/video_llava/test_modeling_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 45,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llava%2Ftest_modeling_video_llava.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -344,51 +344,6 @@ def recursive_check(batched_object, single_row_object, model_name, key):\n                     continue\n                 recursive_check(model_batched_output[key], model_row_output[key], model_name, key)\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values_images\"]\n-            del inputs[\"pixel_values_videos\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values_images\"]\n-            del inputs[\"pixel_values_videos\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     def test_mismatching_num_image_tokens(self):\n         \"\"\"\n         Tests that VLMs through an error with explicit message saying what is wrong"
        },
        {
            "sha": "655809775564ba16e140a3ddf971ef0e53f25089",
            "filename": "tests/models/vipllava/test_modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 43,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvipllava%2Ftest_modeling_vipllava.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -192,49 +192,6 @@ def setUp(self):\n     def test_config(self):\n         self.config_tester.run_common_tests()\n \n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    def test_inputs_embeds(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            wte = model.get_input_embeddings()\n-            inputs[\"inputs_embeds\"] = wte(input_ids)\n-\n-            with torch.no_grad():\n-                model(**inputs)\n-\n-    # overwrite inputs_embeds tests because we need to delete \"pixel values\" for LVLMs\n-    # while some other models require pixel_values to be present\n-    def test_inputs_embeds_matches_input_ids(self):\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = self._prepare_for_class(inputs_dict, model_class)\n-            input_ids = inputs[\"input_ids\"]\n-            del inputs[\"input_ids\"]\n-            del inputs[\"pixel_values\"]\n-\n-            inputs_embeds = model.get_input_embeddings()(input_ids)\n-\n-            with torch.no_grad():\n-                out_ids = model(input_ids=input_ids, **inputs)[0]\n-                out_embeds = model(inputs_embeds=inputs_embeds, **inputs)[0]\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n     # Copied from tests.models.llava.test_modeling_llava.LlavaForConditionalGenerationModelTest.test_mismatching_num_image_tokens\n     def test_mismatching_num_image_tokens(self):\n         \"\"\""
        },
        {
            "sha": "0587c73bd9b876195228ca476d1ef3b852587bb7",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f8b88866f552e0eeb21059c90e2c30dba058c8e9/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=f8b88866f552e0eeb21059c90e2c30dba058c8e9",
            "patch": "@@ -2829,7 +2829,9 @@ def test_inputs_embeds_matches_input_ids(self):\n                 self.skipTest(reason=\"This model doesn't use `inputs_embeds`\")\n \n             inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n-            pad_token_id = config.pad_token_id if config.pad_token_id is not None else 1\n+            pad_token_id = (\n+                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 1\n+            )\n \n             wte = model.get_input_embeddings()\n             if not self.is_encoder_decoder:"
        }
    ],
    "stats": {
        "total": 2846,
        "additions": 1136,
        "deletions": 1710
    }
}