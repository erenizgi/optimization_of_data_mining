{
    "author": "zucchini-nlp",
    "message": "[generation] bring back tests on vision models (#38603)\n\n* bring back geenration tests on VLMs\n\n* remove head mask tests overwritten",
    "sha": "dbfc79c17c618ab664090f4850510fceb28d0bc7",
    "files": [
        {
            "sha": "f8fce7b5c6ecd984bb9ab6398fb806590c1d625f",
            "filename": "src/transformers/models/fsmt/configuration_fsmt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -28,10 +28,11 @@ class DecoderConfig(PretrainedConfig):\n \n     model_type = \"fsmt_decoder\"\n \n-    def __init__(self, vocab_size=0, bos_token_id=0):\n+    def __init__(self, vocab_size=0, bos_token_id=0, is_encoder_decoder=True):\n         super().__init__()\n         self.vocab_size = vocab_size\n         self.bos_token_id = bos_token_id\n+        self.is_encoder_decoder = is_encoder_decoder\n \n \n class FSMTConfig(PretrainedConfig):\n@@ -187,7 +188,9 @@ def __init__(\n         self.init_std = init_std  # Normal(0, this parameter)\n         self.activation_function = activation_function\n \n-        self.decoder = DecoderConfig(vocab_size=tgt_vocab_size, bos_token_id=eos_token_id)\n+        self.decoder = DecoderConfig(\n+            vocab_size=tgt_vocab_size, bos_token_id=eos_token_id, is_encoder_decoder=is_encoder_decoder\n+        )\n         if \"decoder\" in common_kwargs:\n             del common_kwargs[\"decoder\"]\n "
        },
        {
            "sha": "695eef9708de097f81f984a3240ef74740766943",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 27,
            "deletions": 74,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -499,7 +499,7 @@ def test_greedy_generate(self):\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._greedy_generate(model=model, inputs_dict=inputs_dict)\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -523,7 +523,7 @@ def test_greedy_generate_dict_outputs(self):\n                 use_cache=False,\n             )\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n                 # Retrocompatibility check\n@@ -563,7 +563,7 @@ def test_greedy_generate_dict_outputs_use_cache(self):\n                 use_cache=True,  # Enable cache\n             )\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(\n@@ -580,7 +580,7 @@ def test_sample_generate(self):\n             model = model_class(config).to(torch_device).eval()\n             output_generate = self._sample_generate(model=model, inputs_dict=inputs_dict, num_return_sequences=1)\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -605,7 +605,7 @@ def test_sample_generate_dict_output(self):\n                 use_cache=False,\n             )\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n                 # Retrocompatibility check\n@@ -630,7 +630,7 @@ def test_beam_search_generate(self):\n             beam_kwargs = self._get_beam_kwargs()\n             output_generate = self._beam_search_generate(model=model, inputs_dict=inputs_dict, beam_kwargs=beam_kwargs)\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -655,7 +655,7 @@ def test_beam_search_generate_dict_output(self):\n                 return_dict_in_generate=True,\n                 use_cache=False,\n             )\n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                 # Retrocompatibility check\n@@ -704,7 +704,7 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n                 use_cache=True,  # Enable cache\n             )\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(\n@@ -757,7 +757,7 @@ def test_beam_sample_generate(self):\n                 beam_kwargs=beam_kwargs,\n             )\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -784,7 +784,7 @@ def test_beam_sample_generate_dict_output(self):\n                 use_cache=False,\n             )\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                 # Retrocompatibility check\n@@ -838,7 +838,7 @@ def test_group_beam_search_generate(self):\n                 inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n             )\n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -851,7 +851,7 @@ def test_group_beam_search_generate(self):\n                 inputs_dict=inputs_dict,\n                 beam_kwargs=beam_kwargs,\n             )\n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -876,7 +876,7 @@ def test_group_beam_search_generate_dict_output(self):\n                 return_dict_in_generate=True,\n                 use_cache=False,\n             )\n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                 # Retrocompatibility check\n@@ -921,7 +921,7 @@ def test_constrained_beam_search_generate(self):\n                 beam_kwargs=beam_kwargs,\n             )\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -945,7 +945,7 @@ def test_constrained_beam_search_generate(self):\n                 beam_kwargs=beam_kwargs,\n             )\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -985,7 +985,7 @@ def test_constrained_beam_search_generate_dict_output(self):\n                 use_cache=False,\n             )\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateBeamEncoderDecoderOutput)\n                 # Retrocompatibility check\n@@ -1029,7 +1029,7 @@ def test_contrastive_generate(self):\n                 inputs_dict=inputs_dict,\n                 use_cache=True,  # Enable cache\n             )\n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(output_generate.shape[1] == self.max_new_tokens + inputs_dict[\"input_ids\"].shape[1])\n@@ -1065,7 +1065,7 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n                 use_cache=True,  # Enable cache\n             )\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n             else:\n                 self.assertTrue(\n@@ -1297,7 +1297,7 @@ def test_dola_decoding_sample(self):\n                 config._attn_implementation = \"eager\"\n \n             # Encoder-decoder models are not supported\n-            if config.is_encoder_decoder:\n+            if config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.skipTest(\"DoLa is not supported for encoder-decoder models\")\n             config.is_decoder = True\n             model = model_class(config).to(torch_device).eval()\n@@ -1427,52 +1427,6 @@ def test_prompt_lookup_decoding_stops_at_eos(self):\n         # PLD shouldn't propose any new tokens based on eos-match\n         self.assertTrue(output_prompt_lookup.shape[-1] == 10)\n \n-    @pytest.mark.generate\n-    def test_generate_with_head_masking(self):\n-        \"\"\"Test designed for encoder-decoder models to ensure the attention head masking is used.\"\"\"\n-        attention_names = [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n-        for model_class in self.all_generative_model_classes:\n-            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n-            config._attn_implementation = \"eager\"  # head mask works only in eager mode and will be removed soon\n-            text_config = config.get_text_config()\n-            if self.has_attentions:\n-                config._attn_implementation = \"eager\"  # can't output attentions otherwise\n-\n-            # We want to test only encoder-decoder models\n-            if not text_config.is_encoder_decoder:\n-                continue\n-            model = model_class(config).to(torch_device)\n-\n-            head_masking = {\n-                \"head_mask\": torch.zeros(\n-                    text_config.encoder_layers, text_config.encoder_attention_heads, device=torch_device\n-                ),\n-                \"decoder_head_mask\": torch.zeros(\n-                    text_config.decoder_layers, text_config.decoder_attention_heads, device=torch_device\n-                ),\n-                \"cross_attn_head_mask\": torch.zeros(\n-                    text_config.decoder_layers, text_config.decoder_attention_heads, device=torch_device\n-                ),\n-            }\n-\n-            signature = inspect.signature(model.forward)\n-            # We want to test only models where encoder/decoder head masking is implemented\n-            if not set(head_masking.keys()) < {*signature.parameters.keys()}:\n-                continue\n-\n-            for attn_name, (name, mask) in zip(attention_names, head_masking.items()):\n-                out = model.generate(\n-                    num_beams=1,\n-                    output_attentions=self.has_attentions,\n-                    return_dict_in_generate=True,\n-                    remove_invalid_values=True,\n-                    **{name: mask},\n-                    **inputs_dict,\n-                )\n-                # We check the state of decoder_attentions and cross_attentions just from the last step\n-                attn_weights = out[attn_name] if attn_name == attention_names[0] else out[attn_name][-1]\n-                self.assertEqual(sum([w.sum().item() for w in attn_weights]), 0.0)\n-\n     @pytest.mark.generate\n     def test_left_padding_compatibility(self):\n         # NOTE: left-padding results in small numerical differences. This is expected.\n@@ -1491,7 +1445,7 @@ def test_left_padding_compatibility(self):\n         decoder_only_classes = []\n         for model_class in self.all_generative_model_classes:\n             config, _ = self.prepare_config_and_inputs_for_generate()\n-            if config.is_encoder_decoder:\n+            if config.get_text_config(decoder=True).is_encoder_decoder:\n                 continue\n             else:\n                 decoder_only_classes.append(model_class)\n@@ -1696,7 +1650,7 @@ def test_generate_from_inputs_embeds(self, _, num_beams):\n \n             # This test is for decoder-only models (encoder-decoder models have native input embeddings support in the\n             # decoder)\n-            if config.is_encoder_decoder:\n+            if config.get_text_config(decoder=True).is_encoder_decoder:\n                 continue\n             config.is_decoder = True\n \n@@ -1790,7 +1744,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n \n-            if config.is_encoder_decoder:\n+            if config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n             model = model_class(config).to(torch_device).eval()\n@@ -1952,7 +1906,7 @@ def test_generate_continue_from_inputs_embeds(self):\n             if \"token_type_ids\" in inputs_dict:\n                 del inputs_dict[\"token_type_ids\"]\n \n-            if config.is_encoder_decoder:\n+            if config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder\")\n             # TODO (joao, raushan): the correct line below is `if not hasattr(config.get_text_config(), \"use_cache\")`,\n             # but it breaks a few models. Fix and then apply `_check_similar_generate_outputs` pattern\n@@ -2031,7 +1985,7 @@ def test_generate_with_static_cache(self):\n             set_config_for_less_flaky_test(config)\n             main_input = inputs_dict[model_class.main_input_name]\n \n-            if config.is_encoder_decoder:\n+            if config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.skipTest(reason=\"This model is encoder-decoder and has Encoder-Decoder Cache\")\n \n             config.is_decoder = True\n@@ -2183,7 +2137,7 @@ def test_generate_compile_model_forward(self):\n                     if not has_defined_cache_implementation:\n                         decoder_cache = (\n                             gen_out.past_key_values.self_attention_cache\n-                            if config.is_encoder_decoder\n+                            if config.get_text_config(decoder=True).is_encoder_decoder\n                             else gen_out.past_key_values\n                         )\n                         self.assertTrue(isinstance(decoder_cache, DynamicCache))\n@@ -2209,7 +2163,7 @@ def test_generate_compile_model_forward(self):\n                         # sanity checks\n                         decoder_cache = (\n                             gen_out.past_key_values.self_attention_cache\n-                            if config.is_encoder_decoder\n+                            if config.get_text_config(decoder=True).is_encoder_decoder\n                             else gen_out.past_key_values\n                         )\n                         self.assertFalse(isinstance(decoder_cache, DynamicCache))\n@@ -2283,7 +2237,7 @@ def test_generate_compilation_all_outputs(self):\n             else:\n                 self.assertTrue(hasattr(model, \"_compiled_call\"))  # our auto compile should have been called\n \n-            if model.config.is_encoder_decoder:\n+            if model.config.get_text_config(decoder=True).is_encoder_decoder:\n                 self.assertTrue(output_generate.sequences.shape[1] == self.max_new_tokens + 1)\n                 self.assertIsInstance(output_generate, GenerateEncoderDecoderOutput)\n             else:\n@@ -5154,7 +5108,6 @@ def assert_no_sklearn(self):\n \n     @parameterized.expand([(is_sklearn_available(),), (False,)])\n     def test_update_candidate_strategy_no_matches_short(self, sklearn_available):\n-        print(\"test_update_candidate_strategy_no_matches_short\")\n         self.original_matches = []\n         self.candidate_generator.matches = self.original_matches\n         self.num_matches = 0"
        },
        {
            "sha": "22dfe8be07b48d1f887cbccbd5afc0ab8a5fc107",
            "filename": "tests/models/bigbird_pegasus/test_modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbigbird_pegasus%2Ftest_modeling_bigbird_pegasus.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -468,13 +468,6 @@ def test_for_change_to_full_attn(self):\n     def test_load_save_without_tied_weights(self):\n         pass\n \n-    def test_generate_with_head_masking(self):\n-        # overwritten to temporarily switch the attention type to `original_full`\n-        original_self_attention_type = self.model_tester.attention_type\n-        self.model_tester.attention_type = \"original_full\"\n-        super().test_generate_with_head_masking()\n-        self.model_tester.attention_type = original_self_attention_type\n-\n \n @require_torch\n @require_sentencepiece"
        },
        {
            "sha": "7f5edfeb2cff86540c5873a4dcc4f8a89d3607ef",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -782,7 +782,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_vision\n class BlipVQAModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipForQuestionAnswering,) if is_torch_available() else ()\n-    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    # Doesn't run generation tests due to custom generation logic -- won't fix\n     all_generative_model_classes = ()\n     fx_compatible = False\n     test_head_masking = False\n@@ -1091,7 +1091,7 @@ def test_model_from_pretrained(self):\n @require_torch\n class BlipTextImageModelTest(ModelTesterMixin, unittest.TestCase):\n     all_model_classes = (BlipForConditionalGeneration,) if is_torch_available() else ()\n-    # Doesn't run generation tests. There are interface mismatches when using `generate` -- TODO @gante\n+    # Doesn't run generation tests due to custom generation logic -- wont fix\n     all_generative_model_classes = ()\n     fx_compatible = False\n     test_head_masking = False"
        },
        {
            "sha": "f11583d72939e756fd4c8d69ee075852fcd637b6",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -774,6 +774,7 @@ def get_config(self):\n             bos_token_id=self.pad_token_id,\n             pad_token_id=self.pad_token_id,\n             decoder_start_token_id=self.decoder_start_token_id,\n+            is_encoder_decoder=True,\n         )\n \n \n@@ -795,6 +796,9 @@ def __init__(\n         self.text_model_tester = Blip2TextModelTester(parent, **text_kwargs)\n         self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n         self.seq_length = self.text_model_tester.seq_length  # need seq_length for common tests\n+        self.encoder_seq_length = (\n+            self.text_model_tester.encoder_seq_length + num_query_tokens\n+        )  # need enc seq_length for gen tests\n         self.is_training = is_training\n         self.num_query_tokens = num_query_tokens\n \n@@ -859,11 +863,9 @@ def prepare_config_and_inputs_for_common(self):\n \n \n @require_torch\n-class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2ForConditionalGeneration, Blip2Model) if is_torch_available() else ()\n     additional_model_inputs = [\"input_ids\", \"decoder_input_ids\"]\n-    # Doesn't run generation tests. TODO: fix generation tests for Blip2ForConditionalGeneration\n-    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\n             \"feature-extraction\": Blip2Model,"
        },
        {
            "sha": "1fbf788e3771d1af45ae411205b8008f232738fd",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 27,
            "deletions": 3,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -324,10 +324,8 @@ def test_eager_matches_sdpa_inference(\n \n \n @require_torch\n-class IdeficsModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+class IdeficsModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (IdeficsModel, IdeficsForVisionText2Text) if is_torch_available() else ()\n-    # Doesn't run generation tests here -- idefics has a dedicated tester for generation tests below\n-    all_generative_model_classes = ()\n     pipeline_model_mapping = (\n         {\"feature-extraction\": IdeficsModel, \"image-text-to-text\": IdeficsForVisionText2Text}\n         if is_torch_available()\n@@ -336,6 +334,7 @@ class IdeficsModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase)\n     test_pruning = False\n     test_headmasking = False\n     test_torchscript = False\n+    has_attentions = False  # only supports SDOA and thus no attention probs returned\n \n     def _prepare_for_class(self, inputs_dict, model_class, return_labels=False):\n         inputs_dict = super()._prepare_for_class(inputs_dict, model_class, return_labels=return_labels)\n@@ -494,6 +493,31 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n     def test_retain_grad_hidden_states_attentions(self):\n         return\n \n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"\"\"IDEFICS cannot generate with no images provided!\"\"\")\n+    def test_generate_without_input_ids(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"\"\"IDEFICS cannot generate with no images provided!\"\"\")\n+    def test_generate_continue_from_inputs_embeds(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"\"\"IDEFICS cannot do contrastive generation yet and it is not worth fixing\"\"\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"\"\"IDEFICS cannot do contrastive generation yet and it is not worth fixing\"\"\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"\"\"IDEFICS cannot do contrastive generation yet and it is not worth fixing\"\"\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n     def test_attention_outputs(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True"
        },
        {
            "sha": "c43050c462f48160786c3bae9f656840d998288f",
            "filename": "tests/models/longt5/test_modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flongt5%2Ftest_modeling_longt5.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -626,40 +626,6 @@ def test_model_from_pretrained(self):\n         model = LongT5Model.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    def test_generate_with_head_masking(self):\n-        attention_names = [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        config = config_and_inputs[0]\n-        max_length = config_and_inputs[1].shape[-1] + 3\n-        model = LongT5ForConditionalGeneration(config).eval()\n-        model.to(torch_device)\n-\n-        head_masking = {\n-            \"head_mask\": torch.zeros(config.num_layers, config.num_heads, device=torch_device),\n-            \"decoder_head_mask\": torch.zeros(config.num_decoder_layers, config.num_heads, device=torch_device),\n-            \"cross_attn_head_mask\": torch.zeros(config.num_decoder_layers, config.num_heads, device=torch_device),\n-        }\n-\n-        for attn_name, (name, mask) in zip(attention_names, head_masking.items()):\n-            head_masks = {name: mask}\n-            # Explicitly pass decoder_head_mask as it is required from LONGT5 model when head_mask specified\n-            if name == \"head_mask\":\n-                head_masks[\"decoder_head_mask\"] = torch.ones(\n-                    config.num_decoder_layers, config.num_heads, device=torch_device\n-                )\n-\n-            out = model.generate(\n-                config_and_inputs[1],\n-                num_beams=1,\n-                max_length=max_length,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-                **head_masks,\n-            )\n-            # We check the state of decoder_attentions and cross_attentions just from the last step\n-            attn_weights = out[attn_name] if attn_name == attention_names[0] else out[attn_name][-1]\n-            self.assertEqual(sum([w.sum().item() for w in attn_weights]), 0.0)\n-\n     def test_attention_outputs(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"has_attentions is set to False\")"
        },
        {
            "sha": "2c49d92574161c6013a5f41dbeb6f1ca67a7f7e2",
            "filename": "tests/models/mt5/test_modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmt5%2Ftest_modeling_mt5.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -868,40 +868,6 @@ def test_model_from_pretrained(self):\n         model = MT5Model.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    def test_generate_with_head_masking(self):\n-        attention_names = [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        config = config_and_inputs[0]\n-        max_length = config_and_inputs[1].shape[-1] + 3\n-        model = MT5ForConditionalGeneration(config).eval()\n-        model.to(torch_device)\n-\n-        head_masking = {\n-            \"head_mask\": torch.zeros(config.num_layers, config.num_heads, device=torch_device),\n-            \"decoder_head_mask\": torch.zeros(config.num_decoder_layers, config.num_heads, device=torch_device),\n-            \"cross_attn_head_mask\": torch.zeros(config.num_decoder_layers, config.num_heads, device=torch_device),\n-        }\n-\n-        for attn_name, (name, mask) in zip(attention_names, head_masking.items()):\n-            head_masks = {name: mask}\n-            # Explicitly pass decoder_head_mask as it is required from MT5 model when head_mask specified\n-            if name == \"head_mask\":\n-                head_masks[\"decoder_head_mask\"] = torch.ones(\n-                    config.num_decoder_layers, config.num_heads, device=torch_device\n-                )\n-\n-            out = model.generate(\n-                config_and_inputs[1],\n-                num_beams=1,\n-                max_length=max_length,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-                **head_masks,\n-            )\n-            # We check the state of decoder_attentions and cross_attentions just from the last step\n-            attn_weights = out[attn_name] if attn_name == attention_names[0] else out[attn_name][-1]\n-            self.assertEqual(sum([w.sum().item() for w in attn_weights]), 0.0)\n-\n \n # Copied from tests.models.t5.test_modeling_t5.T5EncoderOnlyModelTester with T5->MT5\n class MT5EncoderOnlyModelTester:"
        },
        {
            "sha": "58b535fd0ae711be624db43c7ca42ec51cd144e7",
            "filename": "tests/models/prophetnet/test_modeling_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fprophetnet%2Ftest_modeling_prophetnet.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -1117,10 +1117,6 @@ def test_retain_grad_hidden_states_attentions(self):\n         self.assertIsNotNone(encoder_hidden_states.grad)\n         self.assertIsNotNone(encoder_attentions.grad)\n \n-    @unittest.skip(reason=\"Generating with head_masking has not been implemented for ProphetNet models yet.\")\n-    def test_generate_with_head_masking(self):\n-        pass\n-\n \n @require_torch\n class ProphetNetStandaloneDecoderModelTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):"
        },
        {
            "sha": "2255e895ce797108511295f63f2c1bfe87db42a2",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -741,10 +741,6 @@ def _mock_init_weights(self, module):\n         if hasattr(module, \"masked_spec_embed\") and module.masked_spec_embed is not None:\n             module.masked_spec_embed.data.fill_(3)\n \n-    @unittest.skip(reason=\"Temporarily broken\")  # TODO (joao, eustache): have a look at this test\n-    def test_generate_with_head_masking(self):\n-        pass\n-\n     @unittest.skip(reason=\"Temporarily broken\")  # TODO (joao, eustache): have a look at this test\n     def test_generate_without_input_ids(self):\n         pass"
        },
        {
            "sha": "2b5eb30dcf4a9fc869674188712a01117a23612d",
            "filename": "tests/models/switch_transformers/test_modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswitch_transformers%2Ftest_modeling_switch_transformers.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -709,40 +709,6 @@ def test_model_from_pretrained(self):\n         model = SwitchTransformersModel.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    def test_generate_with_head_masking(self):\n-        attention_names = [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        config = config_and_inputs[0]\n-        max_length = config_and_inputs[1].shape[-1] + 3\n-        model = SwitchTransformersForConditionalGeneration(config).eval()\n-        model.to(torch_device)\n-\n-        head_masking = {\n-            \"head_mask\": torch.zeros(config.num_layers, config.num_heads, device=torch_device),\n-            \"decoder_head_mask\": torch.zeros(config.num_decoder_layers, config.num_heads, device=torch_device),\n-            \"cross_attn_head_mask\": torch.zeros(config.num_decoder_layers, config.num_heads, device=torch_device),\n-        }\n-\n-        for attn_name, (name, mask) in zip(attention_names, head_masking.items()):\n-            head_masks = {name: mask}\n-            # Explicitly pass decoder_head_mask as it is required from SWITCH_TRANSFORMERS model when head_mask specified\n-            if name == \"head_mask\":\n-                head_masks[\"decoder_head_mask\"] = torch.ones(\n-                    config.num_decoder_layers, config.num_heads, device=torch_device\n-                )\n-\n-            out = model.generate(\n-                config_and_inputs[1],\n-                num_beams=1,\n-                max_length=max_length,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-                **head_masks,\n-            )\n-            # We check the state of decoder_attentions and cross_attentions just from the last step\n-            attn_weights = out[attn_name] if attn_name == attention_names[0] else out[attn_name][-1]\n-            self.assertEqual(sum([w.sum().item() for w in attn_weights]), 0.0)\n-\n     @unittest.skip(\n         reason=\"This architecture has tied weights by default and there is no way to remove it, check: https://github.com/huggingface/transformers/pull/31771#issuecomment-2210915245\"\n     )"
        },
        {
            "sha": "16fde95468adae38eb654b5095ff0d3037a953fe",
            "filename": "tests/models/t5/test_modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ft5%2Ftest_modeling_t5.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -873,40 +873,6 @@ def test_model_from_pretrained(self):\n         model = T5Model.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    def test_generate_with_head_masking(self):\n-        attention_names = [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        config = config_and_inputs[0]\n-        max_length = config_and_inputs[1].shape[-1] + 3\n-        model = T5ForConditionalGeneration(config).eval()\n-        model.to(torch_device)\n-\n-        head_masking = {\n-            \"head_mask\": torch.zeros(config.num_layers, config.num_heads, device=torch_device),\n-            \"decoder_head_mask\": torch.zeros(config.num_decoder_layers, config.num_heads, device=torch_device),\n-            \"cross_attn_head_mask\": torch.zeros(config.num_decoder_layers, config.num_heads, device=torch_device),\n-        }\n-\n-        for attn_name, (name, mask) in zip(attention_names, head_masking.items()):\n-            head_masks = {name: mask}\n-            # Explicitly pass decoder_head_mask as it is required from T5 model when head_mask specified\n-            if name == \"head_mask\":\n-                head_masks[\"decoder_head_mask\"] = torch.ones(\n-                    config.num_decoder_layers, config.num_heads, device=torch_device\n-                )\n-\n-            out = model.generate(\n-                config_and_inputs[1],\n-                num_beams=1,\n-                max_length=max_length,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-                **head_masks,\n-            )\n-            # We check the state of decoder_attentions and cross_attentions just from the last step\n-            attn_weights = out[attn_name] if attn_name == attention_names[0] else out[attn_name][-1]\n-            self.assertEqual(sum([w.sum().item() for w in attn_weights]), 0.0)\n-\n \n class T5EncoderOnlyModelTester:\n     def __init__("
        },
        {
            "sha": "1a9c50c7ca693ebaf5756005c89436793d58cde2",
            "filename": "tests/models/udop/test_modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_modeling_udop.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -419,10 +419,6 @@ def test_model_from_pretrained(self):\n         model = UdopForConditionalGeneration.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n-    @unittest.skip(reason=\"TODO: Fix me @joao\")\n-    def test_generate_with_head_masking(self):\n-        pass\n-\n     @unittest.skip(reason=\"TODO: Fix me @joao\")\n     def test_generate_without_input_ids(self):\n         pass"
        },
        {
            "sha": "6887f4c7e5fa027404a005c145b7ce1fee4fbb77",
            "filename": "tests/models/umt5/test_modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 33,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dbfc79c17c618ab664090f4850510fceb28d0bc7/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fumt5%2Ftest_modeling_umt5.py?ref=dbfc79c17c618ab664090f4850510fceb28d0bc7",
            "patch": "@@ -489,39 +489,6 @@ def test_model_fp16_forward(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model_fp16_forward(*config_and_inputs)\n \n-    def test_generate_with_head_masking(self):\n-        attention_names = [\"encoder_attentions\", \"decoder_attentions\", \"cross_attentions\"]\n-        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n-        config = config_and_inputs[0]\n-        model = UMT5ForConditionalGeneration(config).eval()\n-        model.to(torch_device)\n-\n-        head_masking = {\n-            \"head_mask\": torch.zeros(config.num_layers, config.num_heads, device=torch_device),\n-            \"decoder_head_mask\": torch.zeros(config.num_decoder_layers, config.num_heads, device=torch_device),\n-            \"cross_attn_head_mask\": torch.zeros(config.num_decoder_layers, config.num_heads, device=torch_device),\n-        }\n-\n-        for attn_name, (name, mask) in zip(attention_names, head_masking.items()):\n-            head_masks = {name: mask}\n-            # Explicitly pass decoder_head_mask as it is required from T5 model when head_mask specified\n-            if name == \"head_mask\":\n-                head_masks[\"decoder_head_mask\"] = torch.ones(\n-                    config.num_decoder_layers, config.num_heads, device=torch_device\n-                )\n-\n-            out = model.generate(\n-                config_and_inputs[1][\"input_ids\"],\n-                num_beams=1,\n-                max_length=3,\n-                output_attentions=True,\n-                return_dict_in_generate=True,\n-                **head_masks,\n-            )\n-            # We check the state of decoder_attentions and cross_attentions just from the last step\n-            attn_weights = out[attn_name] if attn_name == attention_names[0] else out[attn_name][-1]\n-            self.assertEqual(sum([w.sum().item() for w in attn_weights]), 0.0)\n-\n     @unittest.skip(\n         reason=\"This architecture seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n     )"
        }
    ],
    "stats": {
        "total": 338,
        "additions": 66,
        "deletions": 272
    }
}