{
    "author": "Cyrilvallez",
    "message": "Move missing weights and non-persistent buffers to correct device earlier (#43021)\n\n* move to correct device earlier\n\n* fix typo\n\n* simplify",
    "sha": "d6a6c82680cba9c51decdacac6dd6315ea4a766a",
    "files": [
        {
            "sha": "9e2e16e0cddb4a0aff4f678af8b14b377958c072",
            "filename": "src/transformers/core_model_loading.py",
            "status": "modified",
            "additions": 3,
            "deletions": 11,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d6a6c82680cba9c51decdacac6dd6315ea4a766a/src%2Ftransformers%2Fcore_model_loading.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d6a6c82680cba9c51decdacac6dd6315ea4a766a/src%2Ftransformers%2Fcore_model_loading.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcore_model_loading.py?ref=d6a6c82680cba9c51decdacac6dd6315ea4a766a",
            "patch": "@@ -31,7 +31,7 @@\n \n import torch\n \n-from .integrations.accelerate import offload_weight\n+from .integrations.accelerate import get_device, offload_weight\n from .integrations.tensor_parallel import ALL_PARALLEL_STYLES\n from .utils import is_env_variable_true, is_torch_greater_or_equal, logging\n \n@@ -986,10 +986,6 @@ def convert_and_load_state_dict_in_model(\n     prefix = model.base_model_prefix\n     tp_plan = tp_plan or {}\n     device_map = device_map or {\"\": \"cpu\"}\n-    # Here, we first sort by number of submodules, then length of the full string, to make sure to match correctly\n-    device_map_regex = re.compile(\n-        \"|\".join(rf\"({k})\" for k in sorted(device_map.keys(), key=lambda x: (x.count(\".\"), len(x)), reverse=True))\n-    )\n     dtype_plan = dtype_plan or {}\n     weight_mapping = weight_mapping or []\n     meta_model_state_dict = model.state_dict()\n@@ -1081,10 +1077,7 @@ def convert_and_load_state_dict_in_model(\n                     )\n \n             if future_or_tensor is None:\n-                device_match = device_map_regex.match(renamed_key)\n-                param_device = device_map[device_match.group()] if device_match else device_map.get(\"\", \"cpu\")\n-                # If disk, we need to materialize on cpu first\n-                param_device = \"cpu\" if param_device == \"disk\" else param_device\n+                param_device = get_device(device_map, renamed_key, valid_torch_device=True)\n                 future_or_tensor = spawn_materialize(thread_pool, tensor, param_device, _dtype)\n \n             mapping.add_tensor(renamed_key, original_key, source_pattern, future_or_tensor)\n@@ -1113,8 +1106,7 @@ def convert_and_load_state_dict_in_model(\n                     )\n                     for target_name, param in realized_value.items():\n                         param = param[0] if isinstance(param, list) else param\n-                        device_match = device_map_regex.match(target_name)\n-                        param_device = device_map[device_match.group()] if device_match else device_map.get(\"\", \"cpu\")\n+                        param_device = get_device(device_map, target_name)\n                         # Offloading support\n                         if param_device == \"disk\":\n                             disk_offload_index = offload_and_maybe_resave_param("
        },
        {
            "sha": "8a35af258d0f581ad3ba305fd53a7eb1072c335d",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 13,
            "deletions": 1,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/d6a6c82680cba9c51decdacac6dd6315ea4a766a/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d6a6c82680cba9c51decdacac6dd6315ea4a766a/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=d6a6c82680cba9c51decdacac6dd6315ea4a766a",
            "patch": "@@ -353,10 +353,13 @@ def accelerate_dispatch(model, hf_quantizer, device_map, offload_folder, offload\n         dispatch_model(model, **device_map_kwargs)\n \n \n-def expand_device_map(device_map, param_names):\n+def expand_device_map(device_map: dict | None, param_names: list[str]):\n     \"\"\"\n     Expand a device map to return the correspondence parameter name to device.\n     \"\"\"\n+    if device_map is None:\n+        return dict.fromkeys(param_names, \"cpu\")\n+\n     # Here, we first sort by number of submodules, then length of the full string, to make sure to match correctly\n     device_map_regex = re.compile(\n         \"|\".join(rf\"({k})\" for k in sorted(device_map.keys(), key=lambda x: (x.count(\".\"), len(x)), reverse=True))\n@@ -369,6 +372,15 @@ def expand_device_map(device_map, param_names):\n     return new_device_map\n \n \n+def get_device(device_map: dict | None, param_name: str, valid_torch_device: bool = False) -> torch.device | str | int:\n+    \"\"\"Return the device on which `param_name` should be according to the `device_map`. If `valid_torch_device` is `True`,\n+    then if the device is `\"disk\"`, `\"cpu\"` will be returned instead.\"\"\"\n+    device = expand_device_map(device_map, [param_name])[param_name]\n+    if valid_torch_device and device == \"disk\":\n+        return \"cpu\"\n+    return device\n+\n+\n def accelerate_disk_offload(\n     model: \"PreTrainedModel\",\n     disk_offload_folder: str | None,"
        },
        {
            "sha": "35099e8d994bd904ee4f01e749dacb29e8229135",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 30,
            "deletions": 47,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/d6a6c82680cba9c51decdacac6dd6315ea4a766a/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d6a6c82680cba9c51decdacac6dd6315ea4a766a/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=d6a6c82680cba9c51decdacac6dd6315ea4a766a",
            "patch": "@@ -62,6 +62,7 @@\n     accelerate_dispatch,\n     check_and_set_device_map,\n     expand_device_map,\n+    get_device,\n     load_offloaded_parameter,\n )\n from .integrations.deepspeed import _load_state_dict_into_zero3_model\n@@ -4111,12 +4112,12 @@ def _load_pretrained_model(\n         # Marks tied weights as `_is_hf_initialized` to avoid initializing them (it's very important for efficiency)\n         model.mark_tied_weights_as_initialized()\n \n-        # Move missing (and potentially mismatched) keys back to cpu from meta device (because they were not moved when\n-        # loading the weights as they were not in the loaded state dict)\n+        # Move missing (and potentially mismatched) keys and non-persistent buffers back to their expected device from\n+        # meta device (because they were not moved when loading the weights as they were not in the loaded state dict)\n         missing_and_mismatched = missing_keys | {k[0] for k in mismatched_keys}\n-        model._move_missing_keys_from_meta_to_cpu(missing_and_mismatched, hf_quantizer)\n+        model._move_missing_keys_from_meta_to_device(missing_and_mismatched, device_map, device_mesh, hf_quantizer)\n \n-        # Correctly initialize the missing (and potentially mismatched) keys (all parameters without the `_is_hf_initialzed` flag)\n+        # Correctly initialize the missing (and potentially mismatched) keys (all parameters without the `_is_hf_initialized` flag)\n         model._initialize_missing_keys(is_quantized)\n \n         # Tie the weights\n@@ -4125,29 +4126,6 @@ def _load_pretrained_model(\n         # Adjust missing and unexpected keys\n         missing_keys, unexpected_keys = model._adjust_missing_and_unexpected_keys(missing_keys, unexpected_keys)\n \n-        unique_devices = set(device_map.values()) if device_map is not None else set()\n-        # Post-processing for only 1-value device_map (this includes TP) as we won't use hooks in this case\n-        if len(unique_devices) == 1:\n-            device = unique_devices.pop()\n-            # This is needed for all non-persistent buffers (such as RotaryEmbedding modules), which were not initialized\n-            # on the correct device as it is not part of the state_dict\n-            for _, buffer in model.named_non_persistent_buffers():\n-                buffer.data = buffer.data.to(device)\n-\n-            # The missing/mismatch weights were not moved to device (and parallelized for TP) as they were not part of the\n-            # loaded weights: do it now if we have any\n-            missing_and_mismatched = missing_keys | {k[0] for k in mismatched_keys}\n-            for name in missing_and_mismatched:\n-                param = model.get_parameter_or_buffer(name)\n-                # For TP, shard the param\n-                if device_mesh is not None:\n-                    shard_and_distribute_module(\n-                        model, param.to(device), param, name, None, False, device_mesh.get_local_rank(), device_mesh\n-                    )\n-                # Otherwise, just move it to device\n-                else:\n-                    param.data = param.data.to(device)\n-\n         log_state_dict_report(\n             model=model,\n             pretrained_model_name_or_path=pretrained_model_name_or_path,\n@@ -4345,13 +4323,23 @@ def get_compiled_call(self, compile_config: Optional[CompileConfig]) -> Callable\n     def is_backend_compatible(cls):\n         return cls._supports_attention_backend\n \n-    def _move_missing_keys_from_meta_to_cpu(\n-        self, missing_keys: list[str], hf_quantizer: Optional[HfQuantizer]\n+    def _move_missing_keys_from_meta_to_device(\n+        self,\n+        missing_keys: list[str],\n+        device_map: dict | None,\n+        device_mesh: \"torch.distributed.device_mesh.DeviceMesh | None\",\n+        hf_quantizer: HfQuantizer | None,\n     ) -> None:\n-        \"\"\"Move the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts) back\n-        from meta device to cpu.\n+        \"\"\"Move the missing keys (keys that are part of the model parameters, but were NOT found in the loaded state dicts)\n+        back from meta device to their device according to the `device_map` if any, else cpu. Takes care of sharding those\n+        missing parameters if `device_mesh` is provided, i.e. we are using TP.\n+        All non-persistent buffers are also moved back to the correct device (they are not part of the state_dict, but are\n+        not missing either).\n         \"\"\"\n         is_quantized = hf_quantizer is not None\n+        # This is the only case where we do not initialize the model on meta device, so we don't have to do anything here\n+        if is_deepspeed_zero3_enabled() and not is_quantized:\n+            return\n \n         # In this case we need to move everything back\n         if is_fsdp_enabled() and not is_local_dist_rank_0() and not is_quantized:\n@@ -4368,26 +4356,21 @@ def _move_missing_keys_from_meta_to_cpu(\n         # will be re-initialized for nothing (which can be quite long)\n         for key in missing_keys - self.all_tied_weights_keys.keys():\n             param = self.get_parameter_or_buffer(key)\n-            if is_deepspeed_zero3_enabled() and not is_quantized:\n-                import deepspeed\n-\n-                with deepspeed.zero.GatheredParameters([param], modifier_rank=0):\n-                    # needed for the sharding\n-                    param.data.copy_(torch.empty_like(param, device=param.device))\n+            param_device = get_device(device_map, key, valid_torch_device=True)\n+            value = torch.empty_like(param, device=param_device)\n+            # For TP, we may need to shard the param\n+            if device_mesh is not None:\n+                shard_and_distribute_module(\n+                    self, value, param, key, None, False, device_mesh.get_local_rank(), device_mesh\n+                )\n+            # Otherwise, just move it to device\n             else:\n-                value = torch.empty_like(param, device=\"cpu\")\n                 _load_parameter_into_model(self, key, value)\n         # We need to move back non-persistent buffers as well, as they are not part of loaded weights anyway\n         for key, buffer in self.named_non_persistent_buffers():\n-            if is_deepspeed_zero3_enabled() and not is_quantized:\n-                import deepspeed\n-\n-                with deepspeed.zero.GatheredParameters([buffer], modifier_rank=0):\n-                    # needed for the sharding\n-                    param.data.copy_(torch.empty_like(param, device=param.device))\n-            else:\n-                value = torch.empty_like(buffer, device=\"cpu\")\n-                _load_parameter_into_model(self, key, value)\n+            buffer_device = get_device(device_map, key, valid_torch_device=True)\n+            value = torch.empty_like(buffer, device=buffer_device)\n+            _load_parameter_into_model(self, key, value)\n \n     def _initialize_missing_keys(self, is_quantized: bool) -> None:\n         \"\"\""
        }
    ],
    "stats": {
        "total": 105,
        "additions": 46,
        "deletions": 59
    }
}