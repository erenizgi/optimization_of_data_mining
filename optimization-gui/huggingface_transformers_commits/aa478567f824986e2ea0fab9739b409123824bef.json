{
    "author": "ivarflakstad",
    "message": "Allow rocm systems to run these tests (#37278)\n\n* Allow rocm systems to run these tests\n\n* Fix skipTest logic\n\n* Use get_device_properties to check system capabilities",
    "sha": "aa478567f824986e2ea0fab9739b409123824bef",
    "files": [
        {
            "sha": "28801cd1e285cce4658445b49fb8f5c13867bc9a",
            "filename": "tests/models/musicgen/test_modeling_musicgen.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa478567f824986e2ea0fab9739b409123824bef/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa478567f824986e2ea0fab9739b409123824bef/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen%2Ftest_modeling_musicgen.py?ref=aa478567f824986e2ea0fab9739b409123824bef",
            "patch": "@@ -31,6 +31,7 @@\n     T5Config,\n )\n from transformers.testing_utils import (\n+    get_device_properties,\n     is_torch_available,\n     require_flash_attn,\n     require_torch,\n@@ -1093,12 +1094,15 @@ def test_sdpa_can_dispatch_on_flash(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n-        torch.compiler.reset()\n-        compute_capability = torch.cuda.get_device_capability()\n-        major, _ = compute_capability\n-\n-        if not torch.version.cuda or major < 8:\n+        (device_type, major) = get_device_properties()\n+        if device_type == \"cuda\" and major < 8:\n             self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n+        elif device_type == \"rocm\" and major < 9:\n+            self.skipTest(reason=\"This test requires an AMD GPU with compute capability >= 9.0\")\n+        else:\n+            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU\")\n+\n+        torch.compiler.reset()\n \n         for model_class in self.all_model_classes:\n             if not model_class._supports_sdpa:"
        },
        {
            "sha": "bf441bb19e6fbbd1a094d1c826b4845cbf9212cc",
            "filename": "tests/models/musicgen_melody/test_modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 9,
            "deletions": 5,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa478567f824986e2ea0fab9739b409123824bef/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa478567f824986e2ea0fab9739b409123824bef/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmusicgen_melody%2Ftest_modeling_musicgen_melody.py?ref=aa478567f824986e2ea0fab9739b409123824bef",
            "patch": "@@ -30,6 +30,7 @@\n     T5Config,\n )\n from transformers.testing_utils import (\n+    get_device_properties,\n     is_torch_available,\n     is_torchaudio_available,\n     require_flash_attn,\n@@ -1083,12 +1084,15 @@ def test_sdpa_can_dispatch_on_flash(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n-        torch.compiler.reset()\n-        compute_capability = torch.cuda.get_device_capability()\n-        major, _ = compute_capability\n-\n-        if not torch.version.cuda or major < 8:\n+        (device_type, major) = get_device_properties()\n+        if device_type == \"cuda\" and major < 8:\n             self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n+        elif device_type == \"rocm\" and major < 9:\n+            self.skipTest(reason=\"This test requires an AMD GPU with compute capability >= 9.0\")\n+        else:\n+            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU\")\n+\n+        torch.compiler.reset()\n \n         for model_class in self.all_model_classes:\n             if not model_class._supports_sdpa:"
        },
        {
            "sha": "be65971c95fd78a8de8b424585403242525c6e75",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 18,
            "deletions": 11,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/aa478567f824986e2ea0fab9739b409123824bef/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/aa478567f824986e2ea0fab9739b409123824bef/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=aa478567f824986e2ea0fab9739b409123824bef",
            "patch": "@@ -72,6 +72,7 @@\n )\n from transformers.testing_utils import (\n     CaptureLogger,\n+    get_device_properties,\n     hub_retry,\n     is_flaky,\n     require_accelerate,\n@@ -3763,12 +3764,15 @@ def test_sdpa_can_dispatch_on_flash(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n \n-        torch.compiler.reset()\n-        compute_capability = torch.cuda.get_device_capability()\n-        major, _ = compute_capability\n-\n-        if not torch.version.cuda or major < 8:\n+        (device_type, major) = get_device_properties()\n+        if device_type == \"cuda\" and major < 8:\n             self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n+        elif device_type == \"rocm\" and major < 9:\n+            self.skipTest(reason=\"This test requires an AMD GPU with compute capability >= 9.0\")\n+        else:\n+            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU\")\n+\n+        torch.compiler.reset()\n \n         for model_class in self.all_model_classes:\n             if not model_class._supports_sdpa:\n@@ -3808,13 +3812,16 @@ def test_sdpa_can_dispatch_on_flash(self):\n     def test_sdpa_can_compile_dynamic(self):\n         if not self.has_attentions:\n             self.skipTest(reason=\"Model architecture does not support attentions\")\n-        torch.compiler.reset()\n-        if \"cuda\" in torch_device:\n-            compute_capability = torch.cuda.get_device_capability()\n-            major, _ = compute_capability\n \n-            if not torch.version.cuda or major < 8:\n-                self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n+        (device_type, major) = get_device_properties()\n+        if device_type == \"cuda\" and major < 8:\n+            self.skipTest(reason=\"This test requires an NVIDIA GPU with compute capability >= 8.0\")\n+        elif device_type == \"rocm\" and major < 9:\n+            self.skipTest(reason=\"This test requires an AMD GPU with compute capability >= 9.0\")\n+        else:\n+            self.skipTest(reason=\"This test requires a Nvidia or AMD GPU\")\n+\n+        torch.compiler.reset()\n \n         for model_class in self.all_model_classes:\n             if not model_class._supports_sdpa:"
        }
    ],
    "stats": {
        "total": 57,
        "additions": 36,
        "deletions": 21
    }
}