{
    "author": "qubvel",
    "message": "Add SigLIP 2 (#36323)\n\n* Docs\n\n* Inits\n\n* Auto classes\n\n* Add siglip base\n\n* Add base tests\n\n* Fix Siglip V1 for fix res version\n\n* Add image processor\n\n* Update conversion\n\n* Experimenting with vectorized embeddings\n\n* Fixup\n\n* Add modular Siglip2Processor\n\n* Add modular configuration\n\n* Rename num patches\n\n* Correct image and text features merging\n\n* Working conversion script\n\n* Refactoring conversion script\n\n* Remove unused code in conversion script\n\n* Shorten dict a bit\n\n* Refactoring conversion\n\n* Done conversion refactoring\n\n* Fixup\n\n* Modular siglip2\n\n* Make model exportable and compilable without graph breaks\n\n* Remove position_ids from image_processor\n\n* REmove position ids from modeling file\n\n* Update modular\n\n* Type hint\n\n* Fixup\n\n* Set defaults to processor\n\n* Add integration test\n\n* Revert spatial shapes back to tensor\n\n* Change order\n\n* Fix most of the tests\n\n* Fix docstring\n\n* Remove interpolate_pos_encoding arg (not needed)\n\n* Update docs\n\n* Standardize processing\n\n* Fix attention_mask in vision head\n\n* Siglip v1: remove double transpose in FA2\n\n* Update modular file\n\n* Update FA2 test\n\n* Update expected logits\n\n* Fix interpolation for siglip2 image processor\n\n* Skip init test\n\n* Skip dispatch on flash test\n\n* Fix modeling tests\n\n* Fixup\n\n* Add dummy objects\n\n* Fix some docstrings\n\n* Add siglip2 in index.md\n\n* Fix consistency\n\n* Add docs\n\n* Remove size and data format\n\n* Add image processor tests\n\n* Fix\n\n* Add fast image processor\n\n* Fix style\n\n* Fix\n\n* Docs\n\n* Set lowercase for tokenizer\n\n* Adjust head size for Siglip v1\n\n* Update siglip2 for consistency with siglip1\n\n* Update siglip2 conversion\n\n* Update pipeline\n\n* Update checkpoints in tests\n\n* Update checkpoint name\n\n* Fix pooling for image classification model\n\n* Fix FA2 test\n\n* Update processor\n\n* Fix check repo\n\n* Update docs\n\n* Fix typos\n\n* Fix docstring for fast image processor\n\n* Add siglip2 to FA2 docs\n\n* Fix fast ip tests\n\n* Fix constitency\n\n* Fix tokenizer class for siglip v1\n\n* Fix missing header\n\n* Refactor scaling for clip, siglip, siglip2\n\n* Remove unused imports\n\n* Make fast IP default for siglip2\n\n* Update docs\n\n* Update checkpoints\n\n* Update modular\n\n* Update paper link\n\n* Fixup\n\n* Fix name in toctree\n\n* Fix test",
    "sha": "a957b7911a758d54597914b4479fe6e81424d64f",
    "files": [
        {
            "sha": "7d7201da5027c7523e8a07e1b785676e3ffc0bc6",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -965,6 +965,8 @@\n         title: Segment Anything\n       - local: model_doc/siglip\n         title: SigLIP\n+      - local: model_doc/siglip2\n+        title: SigLIP2\n       - local: model_doc/smolvlm\n         title: SmolVLM\n       - local: model_doc/speech-encoder-decoder"
        },
        {
            "sha": "a6961b06a47b3bcfc75e365166e1f8ccd90c07fc",
            "filename": "docs/source/en/index.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/docs%2Fsource%2Fen%2Findex.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/docs%2Fsource%2Fen%2Findex.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Findex.md?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -317,6 +317,7 @@ Flax), PyTorch, and/or TensorFlow.\n |                           [SEW](model_doc/sew)                           |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                         [SEW-D](model_doc/sew-d)                         |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                        [SigLIP](model_doc/siglip)                        |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n+|                       [SigLIP2](model_doc/siglip2)                       |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |                       [SmolVLM](model_doc/smolvlm)                       |       ‚úÖ        |         ‚ùå         |      ‚ùå      |\n |        [Speech Encoder decoder](model_doc/speech-encoder-decoder)        |       ‚úÖ        |         ‚ùå         |      ‚úÖ      |\n |                 [Speech2Text](model_doc/speech_to_text)                  |       ‚úÖ        |         ‚úÖ         |      ‚ùå      |"
        },
        {
            "sha": "054e09189ad15109e1609522a221a19a34ab3a4b",
            "filename": "docs/source/en/model_doc/siglip2.md",
            "status": "added",
            "additions": 276,
            "deletions": 0,
            "changes": 276,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -0,0 +1,276 @@\n+<!--Copyright 2025 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+\n+‚ö†Ô∏è Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# SigLIP2\n+\n+## Overview\n+\n+The SigLIP2 model was proposed in [SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features](https://huggingface.co/papers/2502.14786) by Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin,\n+Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier H√©naff, Jeremiah Harmsen,\n+Andreas Steiner and Xiaohua Zhai.\n+\n+The model comes in two variants\n+\n+ 1) FixRes - model works with fixed resolution images (backward compatible with SigLIP v1)\n+ 2) NaFlex - model works with variable image aspect ratios and resolutions (SigLIP2 in `transformers`)\n+\n+The abstract from the paper is the following:\n+\n+*We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success\n+of the original SigLIP. In this second iteration, we extend the original image-text training objective with\n+several prior, independently developed techniques into a unified recipe‚Äîthis includes decoder-based\n+pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With\n+these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, \n+including zero-shot classification (best SigLIP 2 ViT-g/16 achieves 85.0% ImageNet zero-shot\n+accuracy), image-text retrieval, and transfer performance when extracting visual representations for\n+Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements \n+on localization and dense prediction tasks. We also train variants which support multiple resolutions \n+and preserve the input‚Äôs native aspect ratio. Finally, we train on a more diverse data-mixture that\n+includes de-biasing techniques, leading to much better multilingual understanding and improved fair-\n+ness. To provide users with the ability to trade-off inference cost with performance, we release model\n+checkpoints at four sizes (ViT-B/86M, L/303M, So400m/400M, and g/1B).*\n+\n+## Usage tips\n+\n+- Usage of SigLIP2 is similar to [SigLIP](siglip) and [CLIP](clip). The main difference from CLIP is the training loss, which does not require a global view of all the pairwise similarities of images and texts within a batch. One needs to apply the sigmoid activation function to the logits, rather than the softmax.\n+- Training is supported but does not use `torch.distributed` utilities which may limit the scalability of batch size. However, DDP and FDSP works on single-node multi-gpu setup.\n+- When using the standalone [`GemmaTokenizerFast`] make sure to pass `padding=\"max_length\"` and `max_length=64` as that's how the model was trained.\n+- Model was trained with *lowercased* text, make sure you make the same preprocessing for your text labels.\n+- To get the same results as the pipeline, a prompt template of \"this is a photo of {label}\" should be used.\n+- The NaFlex variant supports processing images at higher resolutions by adjusting the `max_num_patches` parameter in the `Processor`. The default value is `max_num_patches=256`. Increasing `max_num_patches` to 1024 (4x) will approximately double processed image height and width, while preserving the aspect ratio.\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/siglip2_metrics_table.png\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+This model was contributed by [qubvel](https://huggingface.co/qubvel-hf).\n+The original code can be found [here](https://github.com/google-research/big_vision/tree/main).\n+\n+## Usage example\n+\n+There are 2 main ways to use SigLIP2: either using the pipeline API, which abstracts away all the complexity for you, or by using the `Siglip2Model` class yourself.\n+\n+### FixRes variant\n+\n+**Pipeline API**\n+\n+The pipeline allows to use the model in a few lines of code:\n+\n+```python\n+>>> from transformers import pipeline\n+>>> from PIL import Image\n+>>> import requests\n+\n+>>> # load pipe\n+>>> image_classifier = pipeline(\n+...     task=\"zero-shot-image-classification\",\n+...     model=\"google/siglip2-base-patch16-224\",\n+... )\n+\n+>>> # load image\n+>>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> # inference\n+>>> candidate_labels = [\"2 cats\", \"a plane\", \"a remote\"]\n+>>> outputs = image_classifier(image, candidate_labels=candidate_labels)\n+>>> outputs = [{\"score\": round(output[\"score\"], 4), \"label\": output[\"label\"] } for output in outputs]\n+>>> print(outputs)\n+[{'score': 0.1499, 'label': '2 cats'}, {'score': 0.0008, 'label': 'a remote'}, {'score': 0.0, 'label': 'a plane'}]\n+```\n+\n+**Using the model yourself**\n+\n+If you want to do the pre- and postprocessing yourself, here's how to do that:\n+\n+```python\n+>>> from PIL import Image\n+>>> import requests\n+>>> from transformers import AutoProcessor, AutoModel\n+>>> import torch\n+\n+>>> model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-224\")\n+>>> processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n+\n+>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n+# follows the pipeline prompt template to get same results\n+>>> texts = [f\"This is a photo of {label}.\" for label in candidate_labels]\n+\n+# IMPORTANT: we pass `padding=max_length` and `max_length=64` since the model was trained with this\n+>>> inputs = processor(text=texts, images=image, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> logits_per_image = outputs.logits_per_image\n+>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n+>>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n+15.0% that image 0 is '2 cats'\n+```\n+\n+### NaFlex variant\n+\n+NaFlex combines ideas from FlexiViT, i.e. supporting multiple, predefined sequence lengths \n+with a single ViT model, and NaViT, namely processing images at their native aspect ratio.\n+This enables processing different types of images at appropriate resolution, e.g. using a\n+larger resolution to process document images, while at the same time minimizing the impact \n+of aspect ratio distortion on certain inference tasks, e.g. on OCR.\n+\n+Given a patch size and target sequence length, NaFlex preprocesses the data by first resizing \n+the input image such that the height and width after resizing are multiples of the patch size,\n+while \n+    \n+    1. keeping the aspect ratio distortion as small as possible\n+    2. producing a sequence length of at most the desired target sequence length (`max_num_patches`)\n+    \n+The resulting distortion in width and height is at most `(patch_size - 1) / width` and\n+`(patch_size - 1) / height`, respectively, which tends to be small for common resolutions and aspect ratios. \n+After resizing, the image is split into a sequence of patches, and a mask with padding information is added.\n+\n+```python\n+>>> from PIL import Image\n+>>> import requests\n+>>> from transformers import AutoProcessor, AutoModel\n+>>> import torch\n+\n+>>> model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-naflex\")\n+>>> processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-naflex\")\n+\n+>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n+# follows the pipeline prompt template to get same results\n+>>> texts = [f\"This is a photo of {label}.\" for label in candidate_labels]\n+\n+# default value for `max_num_patches` is 256, but you can increase resulted image resolution providing\n+# higher values e.g. `max_num_patches=512`\n+>>> inputs = processor(text=texts, images=image, max_num_patches=256, return_tensors=\"pt\")\n+\n+>>> with torch.no_grad():\n+...     outputs = model(**inputs)\n+\n+>>> logits_per_image = outputs.logits_per_image\n+>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n+>>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n+21.1% that image 0 is '2 cats'\n+```\n+\n+## Resources\n+\n+A list of official Hugging Face and community (indicated by üåé) resources to help you get started with SigLIP2.\n+\n+- [Zero-shot image classification task guide](../tasks/zero_shot_image_classification)\n+- Demo notebook for SigLIP2 can be found [here](https://github.com/qubvel/transformers-notebooks/tree/master/notebooks/SigLIP2_inference.ipynb). üåé\n+\n+If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n+\n+\n+## Combining SigLIP2 and Flash Attention 2\n+\n+First, make sure to install the latest version of Flash Attention 2.\n+\n+```bash\n+pip install -U flash-attn --no-build-isolation\n+```\n+\n+Make also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16``)\n+\n+To load and run a model using Flash Attention 2, refer to the snippet below:\n+\n+```python\n+>>> import torch\n+>>> import requests\n+>>> from PIL import Image\n+>>> from transformers import AutoProcessor, AutoModel\n+>>> device = \"cuda\" # the device to load the model onto\n+\n+>>> model = AutoModel.from_pretrained(\n+...     \"google/siglip2-so400m-patch14-384\",\n+...     attn_implementation=\"flash_attention_2\",\n+...     torch_dtype=torch.float16,\n+...     device_map=device,\n+... )\n+>>> processor = AutoProcessor.from_pretrained(\"google/siglip2-so400m-patch14-384\")\n+\n+>>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+>>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+>>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n+# follows the pipeline prompt template to get same results\n+>>> texts = [f'This is a photo of {label}.' for label in candidate_labels]\n+# important: we pass `padding=max_length` since the model was trained with this\n+>>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(device)\n+\n+>>> with torch.no_grad():\n+...     with torch.autocast(device):\n+...         outputs = model(**inputs)\n+\n+>>> logits_per_image = outputs.logits_per_image\n+>>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n+>>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n+19.8% that image 0 is '2 cats'\n+```\n+\n+## Siglip2Config\n+\n+[[autodoc]] Siglip2Config\n+\n+## Siglip2TextConfig\n+\n+[[autodoc]] Siglip2TextConfig\n+\n+## Siglip2VisionConfig\n+\n+[[autodoc]] Siglip2VisionConfig\n+\n+## Siglip2ImageProcessor\n+\n+[[autodoc]] Siglip2ImageProcessor\n+    - preprocess\n+\n+## Siglip2ImageProcessorFast\n+\n+[[autodoc]] Siglip2ImageProcessorFast\n+    - preprocess\n+\n+## Siglip2Processor\n+\n+[[autodoc]] Siglip2Processor\n+\n+## Siglip2Model\n+\n+[[autodoc]] Siglip2Model\n+    - forward\n+    - get_text_features\n+    - get_image_features\n+\n+## Siglip2TextModel\n+\n+[[autodoc]] Siglip2TextModel\n+    - forward\n+\n+## Siglip2VisionModel\n+\n+[[autodoc]] Siglip2VisionModel\n+    - forward\n+\n+## Siglip2ForImageClassification\n+\n+[[autodoc]] Siglip2ForImageClassification\n+    - forward"
        },
        {
            "sha": "7f57a99c7d35105696d79c76686ce8958de31cd5",
            "filename": "docs/source/en/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_infer_gpu_one.md?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -111,6 +111,7 @@ FlashAttention-2 is currently supported for the following architectures:\n * [data2vec_audio](https://huggingface.co/docs/transformers/main/en/model_doc/data2vec#transformers.Data2VecAudioModel)\n * [Sew](https://huggingface.co/docs/transformers/main/en/model_doc/sew#transformers.SEWModel)\n * [SigLIP](https://huggingface.co/docs/transformers/model_doc/siglip)\n+* [SigLIP2](https://huggingface.co/docs/transformers/model_doc/siglip2)\n * [UniSpeech](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech#transformers.UniSpeechModel)\n * [unispeech_sat](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech-sat#transformers.UniSpeechSatModel)\n * [helium](https://huggingface.co/docs/transformers/main/en/model_doc/heliumtransformers.HeliumModel)\n@@ -310,6 +311,7 @@ For now, Transformers supports SDPA inference and training for the following arc\n * [RoBERTa](https://huggingface.co/docs/transformers/model_doc/roberta#transformers.RobertaModel)\n * [Sew](https://huggingface.co/docs/transformers/main/en/model_doc/sew#transformers.SEWModel)\n * [SigLIP](https://huggingface.co/docs/transformers/model_doc/siglip)\n+* [SigLIP2](https://huggingface.co/docs/transformers/model_doc/siglip2)\n * [StableLm](https://huggingface.co/docs/transformers/model_doc/stablelm#transformers.StableLmModel)\n * [Starcoder2](https://huggingface.co/docs/transformers/model_doc/starcoder2#transformers.Starcoder2Model)\n * [UniSpeech](https://huggingface.co/docs/transformers/v4.39.3/en/model_doc/unispeech#transformers.UniSpeechModel)"
        },
        {
            "sha": "ed268290100828492717bbbf65f632aebc769e07",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 32,
            "deletions": 0,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -776,6 +776,12 @@\n         \"SiglipTextConfig\",\n         \"SiglipVisionConfig\",\n     ],\n+    \"models.siglip2\": [\n+        \"Siglip2Config\",\n+        \"Siglip2Processor\",\n+        \"Siglip2TextConfig\",\n+        \"Siglip2VisionConfig\",\n+    ],\n     \"models.smolvlm\": [\"SmolVLMConfig\"],\n     \"models.speech_encoder_decoder\": [\"SpeechEncoderDecoderConfig\"],\n     \"models.speech_to_text\": [\n@@ -1289,6 +1295,7 @@\n     _import_structure[\"models.segformer\"].extend([\"SegformerFeatureExtractor\", \"SegformerImageProcessor\"])\n     _import_structure[\"models.seggpt\"].extend([\"SegGptImageProcessor\"])\n     _import_structure[\"models.siglip\"].append(\"SiglipImageProcessor\")\n+    _import_structure[\"models.siglip2\"].append(\"Siglip2ImageProcessor\")\n     _import_structure[\"models.smolvlm\"].extend([\"SmolVLMImageProcessor\"])\n     _import_structure[\"models.superglue\"].extend([\"SuperGlueImageProcessor\"])\n     _import_structure[\"models.superpoint\"].extend([\"SuperPointImageProcessor\"])\n@@ -1330,6 +1337,7 @@\n     _import_structure[\"models.qwen2_vl\"].append(\"Qwen2VLImageProcessorFast\")\n     _import_structure[\"models.rt_detr\"].append(\"RTDetrImageProcessorFast\")\n     _import_structure[\"models.siglip\"].append(\"SiglipImageProcessorFast\")\n+    _import_structure[\"models.siglip2\"].append(\"Siglip2ImageProcessorFast\")\n     _import_structure[\"models.vit\"].append(\"ViTImageProcessorFast\")\n \n try:\n@@ -3559,6 +3567,15 @@\n             \"SiglipVisionModel\",\n         ]\n     )\n+    _import_structure[\"models.siglip2\"].extend(\n+        [\n+            \"Siglip2ForImageClassification\",\n+            \"Siglip2Model\",\n+            \"Siglip2PreTrainedModel\",\n+            \"Siglip2TextModel\",\n+            \"Siglip2VisionModel\",\n+        ]\n+    )\n     _import_structure[\"models.smolvlm\"].extend(\n         [\n             \"SmolVLMForConditionalGeneration\",\n@@ -5942,6 +5959,12 @@\n         SiglipTextConfig,\n         SiglipVisionConfig,\n     )\n+    from .models.siglip2 import (\n+        Siglip2Config,\n+        Siglip2Processor,\n+        Siglip2TextConfig,\n+        Siglip2VisionConfig,\n+    )\n     from .models.smolvlm import SmolVLMConfig\n     from .models.speech_encoder_decoder import SpeechEncoderDecoderConfig\n     from .models.speech_to_text import (\n@@ -6472,6 +6495,7 @@\n         from .models.segformer import SegformerFeatureExtractor, SegformerImageProcessor\n         from .models.seggpt import SegGptImageProcessor\n         from .models.siglip import SiglipImageProcessor\n+        from .models.siglip2 import Siglip2ImageProcessor\n         from .models.smolvlm import SmolVLMImageProcessor\n         from .models.superglue import SuperGlueImageProcessor\n         from .models.superpoint import SuperPointImageProcessor\n@@ -6509,6 +6533,7 @@\n         from .models.qwen2_vl import Qwen2VLImageProcessorFast\n         from .models.rt_detr import RTDetrImageProcessorFast\n         from .models.siglip import SiglipImageProcessorFast\n+        from .models.siglip2 import Siglip2ImageProcessorFast\n         from .models.vit import ViTImageProcessorFast\n \n     try:\n@@ -8288,6 +8313,13 @@\n             SiglipTextModel,\n             SiglipVisionModel,\n         )\n+        from .models.siglip2 import (\n+            Siglip2ForImageClassification,\n+            Siglip2Model,\n+            Siglip2PreTrainedModel,\n+            Siglip2TextModel,\n+            Siglip2VisionModel,\n+        )\n         from .models.smolvlm import (\n             SmolVLMForConditionalGeneration,\n             SmolVLMModel,"
        },
        {
            "sha": "74dad4a2418ba1aa9400d4852f5c5d19e902c936",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -245,6 +245,7 @@\n     sew,\n     sew_d,\n     siglip,\n+    siglip2,\n     smolvlm,\n     speech_encoder_decoder,\n     speech_to_text,"
        },
        {
            "sha": "8b2b514496d88ca28281b2f3c70c5d54fff20b67",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -271,6 +271,7 @@\n         (\"sew\", \"SEWConfig\"),\n         (\"sew-d\", \"SEWDConfig\"),\n         (\"siglip\", \"SiglipConfig\"),\n+        (\"siglip2\", \"Siglip2Config\"),\n         (\"siglip_vision_model\", \"SiglipVisionConfig\"),\n         (\"smolvlm\", \"SmolVLMConfig\"),\n         (\"smolvlm_vision\", \"SmolVLMVisionConfig\"),\n@@ -617,6 +618,8 @@\n         (\"sew\", \"SEW\"),\n         (\"sew-d\", \"SEW-D\"),\n         (\"siglip\", \"SigLIP\"),\n+        (\"siglip2\", \"SigLIP2\"),\n+        (\"siglip2_vision_model\", \"Siglip2VisionModel\"),\n         (\"siglip_vision_model\", \"SiglipVisionModel\"),\n         (\"smolvlm\", \"SmolVLM\"),\n         (\"smolvlm_vision\", \"SmolVLMVisionTransformer\"),"
        },
        {
            "sha": "4942b8f39b7e72395a4fe2194c98855ccabe7c64",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -136,6 +136,7 @@\n             (\"segformer\", (\"SegformerImageProcessor\",)),\n             (\"seggpt\", (\"SegGptImageProcessor\",)),\n             (\"siglip\", (\"SiglipImageProcessor\", \"SiglipImageProcessorFast\")),\n+            (\"siglip2\", (\"Siglip2ImageProcessor\", \"Siglip2ImageProcessorFast\")),\n             (\"superglue\", \"SuperGlueImageProcessor\"),\n             (\"swiftformer\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),"
        },
        {
            "sha": "cf6518c417603a06c92fa935604c969c986106f3",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -250,6 +250,7 @@\n         (\"sew\", \"SEWModel\"),\n         (\"sew-d\", \"SEWDModel\"),\n         (\"siglip\", \"SiglipModel\"),\n+        (\"siglip2\", \"Siglip2Model\"),\n         (\"siglip_vision_model\", \"SiglipVisionModel\"),\n         (\"smolvlm\", \"SmolVLMModel\"),\n         (\"smolvlm_vision\", \"SmolVLMVisionTransformer\"),\n@@ -721,6 +722,7 @@\n         (\"resnet\", \"ResNetForImageClassification\"),\n         (\"segformer\", \"SegformerForImageClassification\"),\n         (\"siglip\", \"SiglipForImageClassification\"),\n+        (\"siglip2\", \"Siglip2ForImageClassification\"),\n         (\"swiftformer\", \"SwiftFormerForImageClassification\"),\n         (\"swin\", \"SwinForImageClassification\"),\n         (\"swinv2\", \"Swinv2ForImageClassification\"),\n@@ -1403,6 +1405,7 @@\n         (\"clip\", \"CLIPModel\"),\n         (\"clipseg\", \"CLIPSegModel\"),\n         (\"siglip\", \"SiglipModel\"),\n+        (\"siglip2\", \"Siglip2Model\"),\n     ]\n )\n "
        },
        {
            "sha": "03b8c860f60b6a1145b0c20449740fc8aaa8fdfb",
            "filename": "src/transformers/models/auto/processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fprocessing_auto.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -99,6 +99,7 @@\n         (\"sew\", \"Wav2Vec2Processor\"),\n         (\"sew-d\", \"Wav2Vec2Processor\"),\n         (\"siglip\", \"SiglipProcessor\"),\n+        (\"siglip2\", \"Siglip2Processor\"),\n         (\"speech_to_text\", \"Speech2TextProcessor\"),\n         (\"speech_to_text_2\", \"Speech2Text2Processor\"),\n         (\"speecht5\", \"SpeechT5Processor\"),"
        },
        {
            "sha": "61c2c2e23d2ffec1289306fc4dc71c386bdae1b5",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -479,6 +479,13 @@\n                 ),\n             ),\n             (\"siglip\", (\"SiglipTokenizer\" if is_sentencepiece_available() else None, None)),\n+            (\n+                \"siglip2\",\n+                (\n+                    \"GemmaTokenizer\" if is_sentencepiece_available() else None,\n+                    \"GemmaTokenizerFast\" if is_tokenizers_available() else None,\n+                ),\n+            ),\n             (\"speech_to_text\", (\"Speech2TextTokenizer\" if is_sentencepiece_available() else None, None)),\n             (\"speech_to_text_2\", (\"Speech2Text2Tokenizer\", None)),\n             (\"speecht5\", (\"SpeechT5Tokenizer\" if is_sentencepiece_available() else None, None)),"
        },
        {
            "sha": "5e4ebd24690ac79a299b14ccbcdc4449124f7acc",
            "filename": "src/transformers/models/clip/modeling_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fmodeling_clip.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -1394,10 +1394,9 @@ def forward(\n         text_embeds = text_embeds / _get_vector_norm(text_embeds)\n \n         # cosine similarity as logits\n-        logit_scale = self.logit_scale.exp()\n-        logits_per_text = torch.matmul(text_embeds, image_embeds.t().to(text_embeds.device)) * logit_scale.to(\n-            text_embeds.device\n-        )\n+        logits_per_text = torch.matmul(text_embeds, image_embeds.t().to(text_embeds.device))\n+        logits_per_text = logits_per_text * self.logit_scale.exp().to(text_embeds.device)\n+\n         logits_per_image = logits_per_text.t()\n \n         loss = None"
        },
        {
            "sha": "f4a140cecc2c1082c0fa973059df4690a63b52b6",
            "filename": "src/transformers/models/siglip/configuration_siglip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -59,6 +59,8 @@ class SiglipTextConfig(PretrainedConfig):\n             The id of the beginning-of-sequence token in the vocabulary.\n         eos_token_id (`int`, *optional*, defaults to 49407):\n             The id of the end-of-sequence token in the vocabulary.\n+        projection_size (`int`, *optional*, defaults to `hidden_size`):\n+            The size of the projection head.\n \n     Example:\n \n@@ -94,6 +96,7 @@ def __init__(\n         pad_token_id=1,\n         bos_token_id=49406,\n         eos_token_id=49407,\n+        projection_size=None,\n         **kwargs,\n     ):\n         super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n@@ -107,6 +110,7 @@ def __init__(\n         self.layer_norm_eps = layer_norm_eps\n         self.hidden_act = hidden_act\n         self.attention_dropout = attention_dropout\n+        self.projection_size = projection_size if projection_size is not None else hidden_size\n \n \n class SiglipVisionConfig(PretrainedConfig):"
        },
        {
            "sha": "8b0a8a250dd81e7920dcab0298d012e3e0fa86ff",
            "filename": "src/transformers/models/siglip/convert_siglip_to_hf.py",
            "status": "modified",
            "additions": 220,
            "deletions": 99,
            "changes": 319,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconvert_siglip_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconvert_siglip_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconvert_siglip_to_hf.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -19,7 +19,8 @@\n \n import argparse\n import collections\n-from pathlib import Path\n+import os\n+from typing import Tuple\n \n import numpy as np\n import requests\n@@ -28,14 +29,48 @@\n from numpy import load\n from PIL import Image\n \n-from transformers import SiglipConfig, SiglipImageProcessor, SiglipModel, SiglipProcessor, SiglipTokenizer\n+from transformers import (\n+    GemmaTokenizerFast,\n+    SiglipConfig,\n+    SiglipImageProcessor,\n+    SiglipModel,\n+    SiglipProcessor,\n+    SiglipTokenizer,\n+)\n from transformers.utils import logging\n \n \n logging.set_verbosity_info()\n logger = logging.get_logger(__name__)\n \n \n+MODEL_CONFIGS = {\n+    \"base\": {\n+        \"hidden_size\": 768,\n+        \"intermediate_size\": 3072,\n+        \"num_hidden_layers\": 12,\n+        \"num_attention_heads\": 12,\n+    },\n+    \"large\": {\n+        \"hidden_size\": 1024,\n+        \"intermediate_size\": 4096,\n+        \"num_hidden_layers\": 24,\n+        \"num_attention_heads\": 16,\n+    },\n+    \"giant-opt\": {\n+        \"hidden_size\": 1536,\n+        \"intermediate_size\": 6144,\n+        \"num_hidden_layers\": 40,\n+        \"num_attention_heads\": 16,\n+    },\n+    \"so400m\": {\n+        \"hidden_size\": 1152,\n+        \"intermediate_size\": 4304,\n+        \"num_hidden_layers\": 27,\n+        \"num_attention_heads\": 16,\n+    },\n+}\n+\n model_name_to_checkpoint = {\n     # base checkpoints\n     \"siglip-base-patch16-224\": \"/Users/nielsrogge/Documents/SigLIP/webli_en_b16_224_63724782.npz\",\n@@ -49,56 +84,146 @@\n     \"siglip-base-patch16-256-i18n\": \"/Users/nielsrogge/Documents/SigLIP/webli_i18n_b16_256_66117334.npz\",\n     # so400m checkpoints\n     \"siglip-so400m-patch14-384\": \"/Users/nielsrogge/Documents/SigLIP/webli_en_so400m_384_58765454.npz\",\n+    # ----------------- v2 -----------------\n+    # base checkpoints\n+    \"siglip2-base-patch32-256\": \"gv-hf/siglip2/siglip2_b32_256.npz\",\n+    \"siglip2-base-patch16-224\": \"gv-hf/siglip2/siglip2_b16_224.npz\",\n+    \"siglip2-base-patch16-256\": \"gv-hf/siglip2/siglip2_b16_256.npz\",\n+    \"siglip2-base-patch16-384\": \"gv-hf/siglip2/siglip2_b16_384.npz\",\n+    \"siglip2-base-patch16-512\": \"gv-hf/siglip2/siglip2_b16_512.npz\",\n+    # large checkpoints\n+    \"siglip2-large-patch16-256\": \"gv-hf/siglip2/siglip2_l16_256.npz\",\n+    \"siglip2-large-patch16-384\": \"gv-hf/siglip2/siglip2_l16_384.npz\",\n+    \"siglip2-large-patch16-512\": \"gv-hf/siglip2/siglip2_l16_512.npz\",\n+    # giant opt checkpoints\n+    \"siglip2-giant-opt-patch16-256\": \"gv-hf/siglip2/siglip2_g-opt16_256.npz\",\n+    \"siglip2-giant-opt-patch16-384\": \"gv-hf/siglip2/siglip2_g-opt16_384.npz\",\n+    # so400m checkpoints\n+    \"siglip2-so400m-patch14-224\": \"gv-hf/siglip2/siglip2_so400m14_224.npz\",\n+    \"siglip2-so400m-patch14-384\": \"gv-hf/siglip2/siglip2_so400m14_384.npz\",\n+    \"siglip2-so400m-patch16-256\": \"gv-hf/siglip2/siglip2_so400m16_256.npz\",\n+    \"siglip2-so400m-patch16-384\": \"gv-hf/siglip2/siglip2_so400m16_384.npz\",\n+    \"siglip2-so400m-patch16-512\": \"gv-hf/siglip2/siglip2_so400m16_512.npz\",\n }\n \n-model_name_to_image_size = {\n-    \"siglip-base-patch16-224\": 224,\n-    \"siglip-base-patch16-256\": 256,\n-    \"siglip-base-patch16-384\": 384,\n-    \"siglip-base-patch16-512\": 512,\n-    \"siglip-large-patch16-256\": 256,\n-    \"siglip-large-patch16-384\": 384,\n-    \"siglip-base-patch16-256-i18n\": 256,\n-    \"siglip-so400m-patch14-384\": 384,\n-}\n+# ------------------------------------------------------------------------------------------------------\n+#  CONFIG\n+# ------------------------------------------------------------------------------------------------------\n+\n+\n+def get_image_size_from_model_name(model_name: str) -> int:\n+    if \"-i18n\" not in model_name:\n+        size = model_name.split(\"-\")[-1]\n+    else:\n+        size = model_name.split(\"-\")[-2]\n+    return int(size)\n+\n+\n+def get_patch_size_from_model_name(model_name: str) -> int:\n+    patch_str = [x for x in model_name.split(\"-\") if \"patch\" in x][0]\n+    return int(patch_str[-2:])\n+\n+\n+def get_vocab_size_from_model_name(model_name: str) -> int:\n+    if \"siglip2\" in model_name:\n+        vocab_size = 256000\n+    elif \"-i18n\" in model_name:\n+        vocab_size = 250000\n+    else:\n+        vocab_size = 32000\n+    return vocab_size\n+\n+\n+def get_vocab_file_from_model_name(model_name: str) -> str:\n+    # get vocab file\n+    if \"i18n\" in model_name:\n+        vocab_file = \"/Users/nielsrogge/Documents/SigLIP/multilingual_vocab/sentencepiece.model\"\n+    else:\n+        vocab_file = \"/Users/nielsrogge/Documents/SigLIP/english_vocab/sentencepiece.model\"\n+    return vocab_file\n+\n+\n+def get_text_and_vision_vit_variants(model_name: str) -> Tuple[str, str]:\n+    variant = model_name.split(\"-\")[1] if \"giant-opt\" not in model_name else \"giant-opt\"\n+    return {\n+        \"base\": (\"base\", \"base\"),\n+        \"large\": (\"large\", \"large\"),\n+        \"so400m\": (\"so400m\", \"so400m\"),\n+        # g-opt siglip2 is not symmetric\n+        \"giant-opt\": (\"so400m\", \"giant-opt\"),\n+    }[variant]\n \n \n def get_siglip_config(model_name):\n-    config = SiglipConfig()\n-\n-    vocab_size = 250000 if \"i18n\" in model_name else 32000\n-    image_size = model_name_to_image_size[model_name]\n-    patch_size = 16 if \"patch16\" in model_name else 14\n-\n-    # size of the architecture\n-    config.vision_config.image_size = image_size\n-    config.vision_config.patch_size = patch_size\n-    config.text_config.vocab_size = vocab_size\n-\n-    if \"base\" in model_name:\n-        pass\n-    elif \"large\" in model_name:\n-        config.text_config.hidden_size = 1024\n-        config.text_config.intermediate_size = 4096\n-        config.text_config.num_hidden_layers = 24\n-        config.text_config.num_attention_heads = 16\n-        config.vision_config.hidden_size = 1024\n-        config.vision_config.intermediate_size = 4096\n-        config.vision_config.num_hidden_layers = 24\n-        config.vision_config.num_attention_heads = 16\n-    elif \"so400m\" in model_name:\n-        config.text_config.hidden_size = 1152\n-        config.text_config.intermediate_size = 4304\n-        config.text_config.num_hidden_layers = 27\n-        config.text_config.num_attention_heads = 16\n-        config.vision_config.hidden_size = 1152\n-        config.vision_config.intermediate_size = 4304\n-        config.vision_config.num_hidden_layers = 27\n-        config.vision_config.num_attention_heads = 16\n+    text_variant, vision_variant = get_text_and_vision_vit_variants(model_name)\n+    text_config = MODEL_CONFIGS[text_variant].copy()\n+    vision_config = MODEL_CONFIGS[vision_variant].copy()\n+\n+    text_config[\"vocab_size\"] = get_vocab_size_from_model_name(model_name)\n+    vision_config[\"image_size\"] = get_image_size_from_model_name(model_name)\n+    vision_config[\"patch_size\"] = get_patch_size_from_model_name(model_name)\n+\n+    if text_config[\"hidden_size\"] != vision_config[\"hidden_size\"]:\n+        text_config[\"projection_size\"] = vision_config[\"hidden_size\"]\n+\n+    return SiglipConfig(text_config=text_config, vision_config=vision_config)\n+\n+\n+# ------------------------------------------------------------------------------------------------------\n+#  PROCESSING\n+# ------------------------------------------------------------------------------------------------------\n+\n+\n+def get_tokenizer(model_name: str) -> GemmaTokenizerFast:\n+    if \"siglip2\" in model_name:\n+        tokenizer = GemmaTokenizerFast.from_pretrained(\n+            \"google/gemma-2-9b-it\",\n+            add_bos_token=False,\n+            add_eos_token=True,\n+            padding_side=\"right\",\n+            do_lower_case=True,\n+            # important: make tokenizer NOT return attention_mask since original one doesn't require it\n+            model_input_names=[\"input_ids\"],\n+        )\n+    else:\n+        # for siglip v1\n+        vocab_file = get_vocab_file_from_model_name(model_name)\n+        # important: make tokenizer not return attention_mask since original one doesn't require it\n+        tokenizer = SiglipTokenizer(vocab_file=vocab_file, model_input_names=[\"input_ids\"])\n+    return tokenizer\n+\n+\n+def get_image_processor(model_name: str) -> SiglipImageProcessor:\n+    image_size = get_image_size_from_model_name(model_name)\n+    size = {\"height\": image_size, \"width\": image_size}\n+    if \"siglip2\" in model_name:\n+        image_processor = SiglipImageProcessor(size=size, resample=2)  # bilinear resampling\n     else:\n-        raise ValueError(\"Model not supported\")\n+        image_processor = SiglipImageProcessor(size=size)\n+    return image_processor\n \n-    return config\n+\n+# ------------------------------------------------------------------------------------------------------\n+#  CONVERT FUNCTIONS\n+# ------------------------------------------------------------------------------------------------------\n+\n+\n+def split_encoderblock_layers(state_dict: dict) -> dict:\n+    \"\"\"\n+    Split the encoderblock weight into layers. In some cases they are concatenated in\n+    the original checkpoints.\n+    \"\"\"\n+    # Make shallow copy\n+    state_dict = state_dict.copy()\n+    # Split encoderblock weight into layers\n+    keys = list(state_dict.keys())\n+    for key in keys:\n+        if \"/encoderblock/\" in key:\n+            weight = state_dict.pop(key)\n+            for i, weight_i in enumerate(weight):\n+                new_name = key.replace(\"encoderblock\", f\"encoderblock_{i}\")\n+                state_dict[new_name] = weight_i\n+    return state_dict\n \n \n def create_rename_keys(config):\n@@ -258,88 +383,83 @@ def convert_siglip_checkpoint(model_name, pytorch_dump_folder_path, verify_logit\n     Copy/paste/tweak model's weights to our SigLIP structure.\n     \"\"\"\n \n-    # define default SigLIP configuration\n+    # Define default SigLIP configuration\n     config = get_siglip_config(model_name)\n \n-    # get checkpoint\n+    # Get checkpoint\n     checkpoint = model_name_to_checkpoint[model_name]\n+    if not os.path.exists(checkpoint):\n+        org, repo_id, *filepath = checkpoint.split(\"/\")\n+        checkpoint = hf_hub_download(repo_id=f\"{org}/{repo_id}\", filename=\"/\".join(filepath))\n \n-    # get vocab file\n-    if \"i18n\" in model_name:\n-        vocab_file = \"/Users/nielsrogge/Documents/SigLIP/multilingual_vocab/sentencepiece.model\"\n-    else:\n-        vocab_file = \"/Users/nielsrogge/Documents/SigLIP/english_vocab/sentencepiece.model\"\n-\n-    # load original state dict\n+    # Load original state dict\n     data = load(checkpoint)\n     state_dict = flatten_nested_dict(data)\n+    state_dict = split_encoderblock_layers(state_dict)\n \n-    # remove and rename some keys\n+    # Remove and rename some keys\n     rename_keys = create_rename_keys(config)\n     for src, dest in rename_keys:\n         rename_key(state_dict, src, dest, config)\n \n     # qkv matrices of attention pooling head need special treatment\n     read_in_q_k_v_head(state_dict, config)\n \n-    # load HuggingFace model\n+    # Load HuggingFace model\n     model = SiglipModel(config).eval()\n     model.load_state_dict(state_dict)\n \n-    # create processor\n-    # important: make tokenizer not return attention_mask since original one doesn't require it\n-    image_size = config.vision_config.image_size\n-    size = {\"height\": image_size, \"width\": image_size}\n-    image_processor = SiglipImageProcessor(size=size)\n-    tokenizer = SiglipTokenizer(vocab_file=vocab_file, model_input_names=[\"input_ids\"])\n+    # Create processor\n+    image_processor = get_image_processor(model_name)\n+    tokenizer = get_tokenizer(model_name)\n     processor = SiglipProcessor(image_processor=image_processor, tokenizer=tokenizer)\n \n-    # verify on dummy images and texts\n+    # Verify forward pass on dummy images and texts\n     url_1 = \"https://cdn.openai.com/multimodal-neurons/assets/apple/apple-ipod.jpg\"\n     image_1 = Image.open(requests.get(url_1, stream=True).raw).convert(\"RGB\")\n     url_2 = \"https://cdn.openai.com/multimodal-neurons/assets/apple/apple-blank.jpg\"\n     image_2 = Image.open(requests.get(url_2, stream=True).raw).convert(\"RGB\")\n     texts = [\"an apple\", \"a picture of an apple\"]\n \n-    inputs = processor(images=[image_1, image_2], text=texts, return_tensors=\"pt\", padding=\"max_length\")\n-\n-    # verify input_ids against original ones\n-    if image_size == 224:\n-        filename = \"siglip_pixel_values.pt\"\n-    elif image_size == 256:\n-        filename = \"siglip_pixel_values_256.pt\"\n-    elif image_size == 384:\n-        filename = \"siglip_pixel_values_384.pt\"\n-    elif image_size == 512:\n-        filename = \"siglip_pixel_values_512.pt\"\n-    else:\n-        raise ValueError(\"Image size not supported\")\n-\n-    filepath = hf_hub_download(repo_id=\"nielsr/test-image\", filename=filename, repo_type=\"dataset\")\n-    original_pixel_values = torch.load(filepath)\n-    filepath = hf_hub_download(repo_id=\"nielsr/test-image\", filename=\"siglip_input_ids.pt\", repo_type=\"dataset\")\n-    original_input_ids = torch.load(filepath)\n+    inputs = processor(images=[image_1, image_2], text=texts, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n+    with torch.no_grad():\n+        outputs = model(**inputs)\n \n-    if \"i18n\" not in model_name:\n-        assert inputs.input_ids.tolist() == original_input_ids.tolist()\n+    if verify_logits:\n+        image_size = config.vision_config.image_size\n+\n+        # verify input_ids against original ones\n+        if image_size == 224:\n+            filename = \"siglip_pixel_values.pt\"\n+        elif image_size == 256:\n+            filename = \"siglip_pixel_values_256.pt\"\n+        elif image_size == 384:\n+            filename = \"siglip_pixel_values_384.pt\"\n+        elif image_size == 512:\n+            filename = \"siglip_pixel_values_512.pt\"\n+        else:\n+            raise ValueError(\"Image size not supported\")\n \n-    print(\"Mean of original pixel values:\", original_pixel_values.mean())\n-    print(\"Mean of new pixel values:\", inputs.pixel_values.mean())\n+        filepath = hf_hub_download(repo_id=\"nielsr/test-image\", filename=filename, repo_type=\"dataset\")\n+        original_pixel_values = torch.load(filepath)\n+        filepath = hf_hub_download(repo_id=\"nielsr/test-image\", filename=\"siglip_input_ids.pt\", repo_type=\"dataset\")\n+        original_input_ids = torch.load(filepath)\n \n-    # note: we're testing with original pixel values here since we don't have exact pixel values\n-    with torch.no_grad():\n-        outputs = model(input_ids=inputs.input_ids, pixel_values=original_pixel_values)\n+        if \"i18n\" not in model_name:\n+            assert inputs.input_ids.tolist() == original_input_ids.tolist()\n \n-    # with torch.no_grad():\n-    #     outputs = model(input_ids=inputs.input_ids, pixel_values=inputs.pixel_values)\n+        print(\"Mean of original pixel values:\", original_pixel_values.mean())\n+        print(\"Mean of new pixel values:\", inputs.pixel_values.mean())\n \n-    print(outputs.logits_per_image[:3, :3])\n+        # note: we're testing with original pixel values here since we don't have exact pixel values\n+        with torch.no_grad():\n+            outputs = model(input_ids=original_input_ids, pixel_values=original_pixel_values)\n+        print(outputs.logits_per_image[:3, :3])\n \n-    probs = torch.sigmoid(outputs.logits_per_image)  # these are the probabilities\n-    print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n-    print(f\"{probs[0][1]:.1%} that image 0 is '{texts[1]}'\")\n+        probs = torch.sigmoid(outputs.logits_per_image)  # these are the probabilities\n+        print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n+        print(f\"{probs[0][1]:.1%} that image 0 is '{texts[1]}'\")\n \n-    if verify_logits:\n         if model_name == \"siglip-base-patch16-224\":\n             expected_slice = torch.tensor(\n                 [[-2.9621, -2.1672], [-0.2713, 0.2910]],\n@@ -375,15 +495,16 @@ def convert_siglip_checkpoint(model_name, pytorch_dump_folder_path, verify_logit\n         print(\"Looks ok!\")\n \n     if pytorch_dump_folder_path is not None:\n-        Path(pytorch_dump_folder_path).mkdir(exist_ok=True)\n+        pytorch_dump_folder_path = os.path.join(pytorch_dump_folder_path, model_name)\n+        os.makedirs(pytorch_dump_folder_path, exist_ok=True)\n         print(f\"Saving model {model_name} to {pytorch_dump_folder_path}\")\n         model.save_pretrained(pytorch_dump_folder_path)\n         print(f\"Saving processor to {pytorch_dump_folder_path}\")\n         processor.save_pretrained(pytorch_dump_folder_path)\n \n     if push_to_hub:\n-        model.push_to_hub(f\"nielsr/{model_name}\")\n-        processor.push_to_hub(f\"nielsr/{model_name}\")\n+        model.push_to_hub(f\"s0225/{model_name}\", private=True)\n+        processor.push_to_hub(f\"s0225/{model_name}\", private=True)\n \n \n if __name__ == \"__main__\":\n@@ -401,7 +522,7 @@ def convert_siglip_checkpoint(model_name, pytorch_dump_folder_path, verify_logit\n     )\n     parser.add_argument(\n         \"--verify_logits\",\n-        action=\"store_false\",\n+        action=\"store_true\",\n         help=\"Whether to verify logits against the original implementation.\",\n     )\n     parser.add_argument("
        },
        {
            "sha": "9c54ed9c03fcbbdee8f1abc1fba7089afc06ed17",
            "filename": "src/transformers/models/siglip/modeling_siglip.py",
            "status": "modified",
            "additions": 9,
            "deletions": 14,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fmodeling_siglip.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -471,15 +471,9 @@ def forward(\n         # Flash attention requires the input to have the shape\n         # batch_size x seq_length x head_dim x hidden_dim\n         # therefore we just need to keep the original shape\n-        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n-\n-        # TODO: These transpose are quite inefficient but Flash Attention requires the layout [batch_size, sequence_length, num_heads, head_dim]. We would need to refactor the KV cache\n-        # to be able to avoid many of these transpose/reshape/view.\n-        query_states = query_states.transpose(1, 2)\n-        key_states = key_states.transpose(1, 2)\n-        value_states = value_states.transpose(1, 2)\n+        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n+        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n+        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n \n         dropout_rate = self.dropout if self.training else 0.0\n \n@@ -936,7 +930,7 @@ def __init__(self, config: SiglipTextConfig):\n         self.encoder = SiglipEncoder(config)\n         self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n \n-        self.head = nn.Linear(embed_dim, embed_dim)\n+        self.head = nn.Linear(embed_dim, config.projection_size)\n         self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n \n     @add_start_docstrings_to_model_forward(SIGLIP_TEXT_INPUTS_DOCSTRING)\n@@ -1415,10 +1409,11 @@ def forward(\n         text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n \n         # cosine similarity as logits\n-        logits_per_text = (\n-            torch.matmul(text_embeds, image_embeds.t().to(text_embeds.device)) * self.logit_scale.exp()\n-            + self.logit_bias\n-        )\n+        logits_per_text = torch.matmul(text_embeds, image_embeds.t().to(text_embeds.device))\n+\n+        logit_scale, logit_bias = self.logit_scale.to(text_embeds.device), self.logit_bias.to(text_embeds.device)\n+        logits_per_text = logits_per_text * logit_scale.exp() + logit_bias\n+\n         logits_per_image = logits_per_text.t()\n \n         loss = None"
        },
        {
            "sha": "7a37cebabfe75af28204fbdcf84ac2ac96a81247",
            "filename": "src/transformers/models/siglip/processing_siglip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -41,7 +41,7 @@ class SiglipProcessor(ProcessorMixin):\n \n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"SiglipImageProcessor\"\n-    tokenizer_class = \"SiglipTokenizer\"\n+    tokenizer_class = \"AutoTokenizer\"\n \n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n@@ -113,7 +113,7 @@ def __call__(\n             image_features = self.image_processor(images, return_tensors=return_tensors)\n \n         if text is not None and images is not None:\n-            encoding[\"pixel_values\"] = image_features.pixel_values\n+            encoding.update(image_features)\n             return encoding\n         elif text is not None:\n             return encoding"
        },
        {
            "sha": "fe5b732b951367cd130d7edd3299eda74f9dc267",
            "filename": "src/transformers/models/siglip2/__init__.py",
            "status": "added",
            "additions": 30,
            "deletions": 0,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2F__init__.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -0,0 +1,30 @@\n+# Copyright 2025 The HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_siglip2 import *\n+    from .image_processing_siglip2 import *\n+    from .image_processing_siglip2_fast import *\n+    from .modeling_siglip2 import *\n+    from .processing_siglip2 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "6cb379c670adeea5f1a511f8561006c89a21ae76",
            "filename": "src/transformers/models/siglip2/configuration_siglip2.py",
            "status": "added",
            "additions": 277,
            "deletions": 0,
            "changes": 277,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconfiguration_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconfiguration_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconfiguration_siglip2.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -0,0 +1,277 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/siglip2/modular_siglip2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_siglip2.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig\n+from ...utils import logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+class Siglip2TextConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Siglip2TextModel`]. It is used to instantiate a\n+    Siglip2 text encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the text encoder of the Siglip2\n+    [google/siglip2-base-patch16-224](https://huggingface.co/google/siglip2-base-patch16-224) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 32000):\n+            Vocabulary size of the Siglip2 text model. Defines the number of different tokens that can be represented by\n+            the `inputs_ids` passed when calling [`Siglip2Model`].\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 3072):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 64):\n+            The maximum sequence length that this model might ever be used with. Typically set this to something large\n+            just in case (e.g., 512 or 1024 or 2048).\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        pad_token_id (`int`, *optional*, defaults to 1):\n+            The id of the padding token in the vocabulary.\n+        bos_token_id (`int`, *optional*, defaults to 49406):\n+            The id of the beginning-of-sequence token in the vocabulary.\n+        eos_token_id (`int`, *optional*, defaults to 49407):\n+            The id of the end-of-sequence token in the vocabulary.\n+        projection_size (`int`, *optional*, defaults to `hidden_size`):\n+            The size of the projection head.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Siglip2TextConfig, Siglip2TextModel\n+\n+    >>> # Initializing a Siglip2TextConfig with google/siglip2-base-patch16-224 style configuration\n+    >>> configuration = Siglip2TextConfig()\n+\n+    >>> # Initializing a Siglip2TextModel (with random weights) from the google/siglip2-base-patch16-224 style configuration\n+    >>> model = Siglip2TextModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"siglip2_text_model\"\n+    base_config_key = \"text_config\"\n+\n+    def __init__(\n+        self,\n+        vocab_size=32000,\n+        hidden_size=768,\n+        intermediate_size=3072,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        max_position_embeddings=64,\n+        hidden_act=\"gelu_pytorch_tanh\",\n+        layer_norm_eps=1e-6,\n+        attention_dropout=0.0,\n+        # This differs from `CLIPTokenizer`'s default and from openai/siglip2\n+        # See https://github.com/huggingface/transformers/pull/24773#issuecomment-1632287538\n+        pad_token_id=1,\n+        bos_token_id=49406,\n+        eos_token_id=49407,\n+        projection_size=None,\n+        **kwargs,\n+    ):\n+        super().__init__(pad_token_id=pad_token_id, bos_token_id=bos_token_id, eos_token_id=eos_token_id, **kwargs)\n+\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.max_position_embeddings = max_position_embeddings\n+        self.layer_norm_eps = layer_norm_eps\n+        self.hidden_act = hidden_act\n+        self.attention_dropout = attention_dropout\n+        self.projection_size = projection_size if projection_size is not None else hidden_size\n+\n+\n+class Siglip2VisionConfig(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Siglip2VisionModel`]. It is used to instantiate a\n+    Siglip2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the vision encoder of the Siglip2\n+    [google/siglip2-base-patch16-naflex](https://huggingface.co/google/siglip2-base-patch16-naflex) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 3072):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input images.\n+        num_patches (`int`, *optional*, defaults to 256):\n+            The number of patches in the image with the size of (`patch_size`, `patch_size`).\n+            The image is resized to fill maximum of this number of patches, and to preserve\n+            the aspect ratio. In case the resulted number of patches is lower, the image is\n+            padded in \"patch\" dimension.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Siglip2VisionConfig, Siglip2VisionModel\n+\n+    >>> # Initializing a Siglip2VisionConfig with google/siglip2-base-patch16-naflex style configuration\n+    >>> configuration = Siglip2VisionConfig()\n+\n+    >>> # Initializing a Siglip2VisionModel (with random weights) from the google/siglip2-base-patch16-naflex style configuration\n+    >>> model = Siglip2VisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    model_type = \"siglip2_vision_model\"\n+    base_config_key = \"vision_config\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=768,\n+        intermediate_size=3072,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        num_channels=3,\n+        num_patches=256,\n+        patch_size=16,\n+        hidden_act=\"gelu_pytorch_tanh\",\n+        layer_norm_eps=1e-6,\n+        attention_dropout=0.0,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.num_channels = num_channels\n+        self.patch_size = patch_size\n+        self.attention_dropout = attention_dropout\n+        self.layer_norm_eps = layer_norm_eps\n+        self.hidden_act = hidden_act\n+        self.num_patches = num_patches\n+\n+\n+class Siglip2Config(PretrainedConfig):\n+    r\"\"\"\n+    [`Siglip2Config`] is the configuration class to store the configuration of a [`Siglip2Model`]. It is used to\n+    instantiate a Siglip2 model according to the specified arguments, defining the text model and vision model configs.\n+    Instantiating a configuration with the defaults will yield a similar configuration to that of the Siglip2\n+    [google/siglip2-base-patch16-224](https://huggingface.co/google/siglip2-base-patch16-224) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        text_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`Siglip2TextConfig`].\n+        vision_config (`dict`, *optional*):\n+            Dictionary of configuration options used to initialize [`Siglip2VisionConfig`].\n+        kwargs (*optional*):\n+            Dictionary of keyword arguments.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Siglip2Config, Siglip2Model\n+\n+    >>> # Initializing a Siglip2Config with google/siglip2-base-patch16-224 style configuration\n+    >>> configuration = Siglip2Config()\n+\n+    >>> # Initializing a Siglip2Model (with random weights) from the google/siglip2-base-patch16-224 style configuration\n+    >>> model = Siglip2Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+\n+    >>> # We can also initialize a Siglip2Config from a Siglip2TextConfig and a Siglip2VisionConfig\n+    >>> from transformers import Siglip2TextConfig, Siglip2VisionConfig\n+\n+    >>> # Initializing a Siglip2Text and Siglip2Vision configuration\n+    >>> config_text = Siglip2TextConfig()\n+    >>> config_vision = Siglip2VisionConfig()\n+\n+    >>> config = Siglip2Config.from_text_vision_configs(config_text, config_vision)\n+    ```\"\"\"\n+\n+    model_type = \"siglip2\"\n+    sub_configs = {\"text_config\": Siglip2TextConfig, \"vision_config\": Siglip2VisionConfig}\n+\n+    def __init__(self, text_config=None, vision_config=None, **kwargs):\n+        super().__init__(**kwargs)\n+\n+        if text_config is None:\n+            text_config = {}\n+            logger.info(\"`text_config` is `None`. Initializing the `Siglip2TextConfig` with default values.\")\n+\n+        if vision_config is None:\n+            vision_config = {}\n+            logger.info(\"`vision_config` is `None`. initializing the `Siglip2VisionConfig` with default values.\")\n+\n+        self.text_config = Siglip2TextConfig(**text_config)\n+        self.vision_config = Siglip2VisionConfig(**vision_config)\n+\n+        self.initializer_factor = 1.0\n+\n+    @classmethod\n+    def from_text_vision_configs(cls, text_config: Siglip2TextConfig, vision_config: Siglip2VisionConfig, **kwargs):\n+        r\"\"\"\n+        Instantiate a [`Siglip2Config`] (or a derived class) from siglip2 text model configuration and siglip2 vision\n+        model configuration.\n+\n+        Returns:\n+            [`Siglip2Config`]: An instance of a configuration object\n+        \"\"\"\n+\n+        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n+\n+\n+__all__ = [\"Siglip2Config\", \"Siglip2TextConfig\", \"Siglip2VisionConfig\"]"
        },
        {
            "sha": "819596498996c1af9623f07e4cf2dea1f663ccef",
            "filename": "src/transformers/models/siglip2/convert_siglip2_to_hf.py",
            "status": "added",
            "additions": 438,
            "deletions": 0,
            "changes": 438,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconvert_siglip2_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconvert_siglip2_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconvert_siglip2_to_hf.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -0,0 +1,438 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Convert Siglip2 checkpoints from the original repository.\n+\n+URL: https://github.com/google-research/big_vision/tree/main\n+\"\"\"\n+\n+import argparse\n+import collections\n+import os\n+import re\n+\n+import numpy as np\n+import torch\n+from huggingface_hub import hf_hub_download\n+from PIL import Image, ImageDraw\n+\n+from transformers import GemmaTokenizerFast, Siglip2Config, Siglip2ImageProcessorFast, Siglip2Model, Siglip2Processor\n+from transformers.utils import logging\n+\n+\n+logging.set_verbosity_info()\n+logger = logging.get_logger(__name__)\n+\n+\n+COMMON_CONFIG_PARAMS = {\n+    \"base\": {\n+        \"hidden_size\": 768,\n+        \"intermediate_size\": 3072,\n+        \"num_hidden_layers\": 12,\n+        \"num_attention_heads\": 12,\n+    },\n+    \"large\": {\n+        \"hidden_size\": 1024,\n+        \"intermediate_size\": 4096,\n+        \"num_hidden_layers\": 24,\n+        \"num_attention_heads\": 16,\n+    },\n+    \"so400m\": {\n+        \"hidden_size\": 1152,\n+        \"intermediate_size\": 4304,\n+        \"num_hidden_layers\": 27,\n+        \"num_attention_heads\": 16,\n+    },\n+}\n+\n+MODEL_NAME_TO_CHECKPOINT_PATH = {\n+    # base checkpoints\n+    \"siglip2-base-patch16-naflex\": \"gv-hf/siglip2/siglip2_b16_naflex.npz\",\n+    \"siglip2-so400m-patch16-naflex\": \"gv-hf/siglip2/siglip2_so400m16_naflex.npz\",\n+}\n+\n+# fmt: off\n+EXPECTED_OUTPUTS = {\n+    \"siglip2-base-patch16-naflex\": torch.tensor([\n+        [  1.0195,  -0.0280,  -1.4468],\n+        [ -4.5395,  -6.2269,  -1.5667],\n+        [  4.1757,   5.0358,   3.5159],\n+        [  9.4264,  10.1879,   6.3353],\n+        [  2.4409,   3.1058,   4.5491],\n+        [-12.3230, -13.7355, -13.4632],\n+        [  1.1520,   1.1687,  -1.9647],\n+    ]),\n+    \"siglip2-so400m-patch16-naflex\": torch.tensor([\n+        [  0.9422,   0.5540,  -2.4405],\n+        [ -7.3522,  -9.4931,  -6.3499],\n+        [  5.7852,   6.7288,   7.7893],\n+        [  9.9881,  10.8136,   9.2121],\n+        [  5.3660,   5.7746,   8.4130],\n+        [-12.7218, -14.2631, -13.6442],\n+        [  0.6384,   0.4278,  -0.9022],\n+    ]),\n+}\n+# fmt: on\n+\n+# fmt: off\n+ORIGINAL_TO_CONVERTED_KEY_MAPPING = {\n+    # Vision embeddings\n+    r\"params/img/embedding/kernel\":                                                                         r\"vision_model.embeddings.patch_embedding.weight\",\n+    r\"params/img/embedding/bias\":                                                                           r\"vision_model.embeddings.patch_embedding.bias\",\n+    r\"params/img/pos_embedding\":                                                                            r\"vision_model.embeddings.position_embedding.weight\",\n+    # Vision encoder\n+    r\"params/img/Transformer/encoderblock_(\\d+)/LayerNorm_0/scale\":                                         r\"vision_model.encoder.layers.\\1.layer_norm1.weight\",\n+    r\"params/img/Transformer/encoderblock_(\\d+)/LayerNorm_0/bias\":                                          r\"vision_model.encoder.layers.\\1.layer_norm1.bias\",\n+    r\"params/img/Transformer/encoderblock_(\\d+)/LayerNorm_1/scale\":                                         r\"vision_model.encoder.layers.\\1.layer_norm2.weight\",\n+    r\"params/img/Transformer/encoderblock_(\\d+)/LayerNorm_1/bias\":                                          r\"vision_model.encoder.layers.\\1.layer_norm2.bias\",\n+    r\"params/img/Transformer/encoderblock_(\\d+)/MlpBlock_0/Dense_0/kernel\":                                 r\"vision_model.encoder.layers.\\1.mlp.fc1.weight\",\n+    r\"params/img/Transformer/encoderblock_(\\d+)/MlpBlock_0/Dense_0/bias\":                                   r\"vision_model.encoder.layers.\\1.mlp.fc1.bias\",\n+    r\"params/img/Transformer/encoderblock_(\\d+)/MlpBlock_0/Dense_1/kernel\":                                 r\"vision_model.encoder.layers.\\1.mlp.fc2.weight\",\n+    r\"params/img/Transformer/encoderblock_(\\d+)/MlpBlock_0/Dense_1/bias\":                                   r\"vision_model.encoder.layers.\\1.mlp.fc2.bias\",\n+    r\"params/img/Transformer/encoderblock_(\\d+)/MultiHeadDotProductAttention_0/(q|k|v|out)[a-z]*/kernel\":   r\"vision_model.encoder.layers.\\1.self_attn.\\2_proj.weight\",\n+    r\"params/img/Transformer/encoderblock_(\\d+)/MultiHeadDotProductAttention_0/(q|k|v|out)[a-z]*/bias\":     r\"vision_model.encoder.layers.\\1.self_attn.\\2_proj.bias\",\n+    # Vision norm\n+    r\"params/img/Transformer/encoder_norm/scale\":                                                           r\"vision_model.post_layernorm.weight\",\n+    r\"params/img/Transformer/encoder_norm/bias\":                                                            r\"vision_model.post_layernorm.bias\",\n+    # Vision head\n+    r\"params/img/MAPHead_0/probe\":                                                                          r\"vision_model.head.probe\",\n+    r\"params/img/MAPHead_0/LayerNorm_0/scale\":                                                              r\"vision_model.head.layernorm.weight\",\n+    r\"params/img/MAPHead_0/LayerNorm_0/bias\":                                                               r\"vision_model.head.layernorm.bias\",\n+    r\"params/img/MAPHead_0/MlpBlock_0/Dense_0/kernel\":                                                      r\"vision_model.head.mlp.fc1.weight\",\n+    r\"params/img/MAPHead_0/MlpBlock_0/Dense_0/bias\":                                                        r\"vision_model.head.mlp.fc1.bias\",\n+    r\"params/img/MAPHead_0/MlpBlock_0/Dense_1/kernel\":                                                      r\"vision_model.head.mlp.fc2.weight\",\n+    r\"params/img/MAPHead_0/MlpBlock_0/Dense_1/bias\":                                                        r\"vision_model.head.mlp.fc2.bias\",\n+    r\"params/img/MAPHead_0/MultiHeadDotProductAttention_0/out/kernel\":                                      r\"vision_model.head.attention.out_proj.weight\",\n+    r\"params/img/MAPHead_0/MultiHeadDotProductAttention_0/out/bias\":                                        r\"vision_model.head.attention.out_proj.bias\",\n+    r\"params/img/MAPHead_0/MultiHeadDotProductAttention_0/qkv/kernel\":                                      r\"vision_model.head.attention.in_proj_weight\",\n+    r\"params/img/MAPHead_0/MultiHeadDotProductAttention_0/qkv/bias\":                                        r\"vision_model.head.attention.in_proj_bias\",\n+    # Text embeddings\n+    r\"params/txt/Embed_0/embedding\":                                                                        r\"text_model.embeddings.token_embedding.weight\",\n+    r\"params/txt/pos_embedding\":                                                                            r\"text_model.embeddings.position_embedding.weight\",\n+    # Text encoder\n+    r\"params/txt/Encoder_0/encoderblock_(\\d+)/LayerNorm_0/scale\":                                           r\"text_model.encoder.layers.\\1.layer_norm1.weight\",\n+    r\"params/txt/Encoder_0/encoderblock_(\\d+)/LayerNorm_0/bias\":                                            r\"text_model.encoder.layers.\\1.layer_norm1.bias\",\n+    r\"params/txt/Encoder_0/encoderblock_(\\d+)/LayerNorm_1/scale\":                                           r\"text_model.encoder.layers.\\1.layer_norm2.weight\",\n+    r\"params/txt/Encoder_0/encoderblock_(\\d+)/LayerNorm_1/bias\":                                            r\"text_model.encoder.layers.\\1.layer_norm2.bias\",\n+    r\"params/txt/Encoder_0/encoderblock_(\\d+)/MlpBlock_0/Dense_0/kernel\":                                   r\"text_model.encoder.layers.\\1.mlp.fc1.weight\",\n+    r\"params/txt/Encoder_0/encoderblock_(\\d+)/MlpBlock_0/Dense_0/bias\":                                     r\"text_model.encoder.layers.\\1.mlp.fc1.bias\",\n+    r\"params/txt/Encoder_0/encoderblock_(\\d+)/MlpBlock_0/Dense_1/kernel\":                                   r\"text_model.encoder.layers.\\1.mlp.fc2.weight\",\n+    r\"params/txt/Encoder_0/encoderblock_(\\d+)/MlpBlock_0/Dense_1/bias\":                                     r\"text_model.encoder.layers.\\1.mlp.fc2.bias\",\n+    r\"params/txt/Encoder_0/encoderblock_(\\d+)/MultiHeadDotProductAttention_0/(q|k|v|out)[a-z]*/kernel\":     r\"text_model.encoder.layers.\\1.self_attn.\\2_proj.weight\",\n+    r\"params/txt/Encoder_0/encoderblock_(\\d+)/MultiHeadDotProductAttention_0/(q|k|v|out)[a-z]*/bias\":       r\"text_model.encoder.layers.\\1.self_attn.\\2_proj.bias\",\n+    # Text encoder norm and head\n+    r\"params/txt/Encoder_0/encoder_norm/scale\":                                                             r\"text_model.final_layer_norm.weight\",\n+    r\"params/txt/Encoder_0/encoder_norm/bias\":                                                              r\"text_model.final_layer_norm.bias\",\n+    r\"params/txt/head/kernel\":                                                                              r\"text_model.head.weight\",\n+    r\"params/txt/head/bias\":                                                                                r\"text_model.head.bias\",\n+    # learned temperature and bias\n+    r\"params/t\":                                                                                            r\"logit_scale\",\n+    r\"params/b\":                                                                                            r\"logit_bias\",\n+}\n+# fmt: on\n+\n+\n+# --------------------------------------------------------------------------------------------\n+# Model objects: configuration, tokenizer, image processor\n+# --------------------------------------------------------------------------------------------\n+\n+\n+def get_siglip2_config(model_name: str) -> Siglip2Config:\n+    \"\"\"\n+    Create a configuration for the Siglip2 model based on the model name.\n+    \"\"\"\n+\n+    _, variant, patch, _ = model_name.split(\"-\")\n+    patch_size = int(patch[-2:])\n+    num_patches = 256\n+\n+    common_options = COMMON_CONFIG_PARAMS[variant]\n+    vision_config = {\n+        \"patch_size\": patch_size,\n+        \"num_patches\": num_patches,\n+        **common_options,\n+    }\n+    text_config = {\n+        \"vocab_size\": 256_000,\n+        **common_options,\n+    }\n+    config = Siglip2Config(\n+        vision_config=vision_config,\n+        text_config=text_config,\n+    )\n+    return config\n+\n+\n+def get_siglip2_tokenizer() -> GemmaTokenizerFast:\n+    # Load pretrained tokenizer\n+    gemma_checkpoint = \"google/gemma-7b\"\n+    tokenizer = GemmaTokenizerFast.from_pretrained(\n+        gemma_checkpoint,\n+        add_bos_token=False,\n+        add_eos_token=True,\n+        padding_side=\"right\",\n+        do_lower_case=True,\n+        # important: make tokenizer NOT return attention_mask since original one doesn't require it\n+        model_input_names=[\"input_ids\"],\n+    )\n+    return tokenizer\n+\n+\n+def get_siglip2_image_processor(patch_size: int, max_num_patches: int) -> Siglip2ImageProcessorFast:\n+    image_processor = Siglip2ImageProcessorFast(\n+        patch_size=patch_size,\n+        max_num_patches=max_num_patches,\n+        do_resize=True,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        resample=Image.Resampling.BILINEAR,\n+    )\n+    return image_processor\n+\n+\n+# --------------------------------------------------------------------------------------------\n+# Helper functions for state dict conversion\n+# --------------------------------------------------------------------------------------------\n+\n+\n+def flatten_nested_dict(params: dict, parent_key: str = \"\", sep: str = \"/\") -> dict:\n+    \"\"\"\n+    Flatten a nested original checkpoint dictionary into a flat dictionary.\n+    \"\"\"\n+    items = []\n+    for k, v in params.items():\n+        new_key = parent_key + sep + k if parent_key else k\n+        if isinstance(v, collections.abc.MutableMapping):\n+            items.extend(flatten_nested_dict(v, new_key, sep=sep).items())\n+        else:\n+            items.append((new_key, v))\n+    return dict(items)\n+\n+\n+def split_encoderblock_layers(state_dict: dict) -> dict:\n+    \"\"\"\n+    Split the encoderblock weight into layers. In some cases they are concatenated in\n+    the original checkpoints.\n+    \"\"\"\n+    # Make shallow copy\n+    state_dict = state_dict.copy()\n+    # Split encoderblock weight into layers\n+    keys = list(state_dict.keys())\n+    for key in keys:\n+        if \"/encoderblock/\" in key:\n+            weight = state_dict.pop(key)\n+            for i, weight_i in enumerate(weight):\n+                new_name = key.replace(\"encoderblock\", f\"encoderblock_{i}\")\n+                state_dict[new_name] = weight_i\n+    return state_dict\n+\n+\n+def merge_qkv_for_head(state_dict: dict, config: Siglip2Config) -> dict:\n+    \"\"\"\n+    Merge the q/k/v weights and biases for the attention head.\n+    \"\"\"\n+    # Make shallow copy\n+    state_dict = state_dict.copy()\n+    # Read and process q/k/v weights and biases\n+    qkv_weights, qkv_biases = [], []\n+    for name in [\"query\", \"key\", \"value\"]:\n+        prefix = f\"params/img/MAPHead_0/MultiHeadDotProductAttention_0/{name}\"\n+        weight = state_dict.pop(f\"{prefix}/kernel\").reshape(-1, config.vision_config.hidden_size)\n+        bias = state_dict.pop(f\"{prefix}/bias\").reshape(-1)\n+        qkv_weights.append(weight)\n+        qkv_biases.append(bias)\n+\n+    # Combine into single tensors\n+    state_dict[\"params/img/MAPHead_0/MultiHeadDotProductAttention_0/qkv/kernel\"] = np.concatenate(qkv_weights, axis=1)\n+    state_dict[\"params/img/MAPHead_0/MultiHeadDotProductAttention_0/qkv/bias\"] = np.concatenate(qkv_biases, axis=0)\n+    return state_dict\n+\n+\n+def convert_old_keys_to_new_keys(state_dict_keys: list) -> dict:\n+    \"\"\"\n+    This function should be applied only once, on the concatenated keys to efficiently rename using\n+    the key mappings.\n+    \"\"\"\n+    output_dict = {}\n+    if state_dict_keys is not None:\n+        old_text = \"\\n\".join(state_dict_keys)\n+        new_text = old_text\n+        for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING.items():\n+            if replacement is None:\n+                new_text = re.sub(pattern, \"\", new_text)  # an empty line\n+                continue\n+            new_text = re.sub(pattern, replacement, new_text)\n+        output_dict = dict(zip(old_text.split(\"\\n\"), new_text.split(\"\\n\")))\n+    return output_dict\n+\n+\n+# --------------------------------------------------------------------------------------------\n+# Helper functions for model verification\n+# --------------------------------------------------------------------------------------------\n+\n+\n+def create_image(width, height):\n+    \"\"\"\n+    Helper function to create an image with a blue circle on a red background.\n+    \"\"\"\n+    image = Image.new(\"RGB\", (width, height), color=\"red\")\n+    draw = ImageDraw.Draw(image)\n+    center_x = image.width // 2\n+    center_y = image.height // 2\n+    radius = min(center_x, center_y) // 8 * 7\n+    draw.ellipse(\n+        (center_x - radius, center_y - radius, center_x + radius, center_y + radius),\n+        fill=\"blue\",\n+        outline=\"green\",\n+        width=image.width // 20,\n+    )\n+    return image\n+\n+\n+def prepare_inputs():\n+    \"\"\"\n+    Prepare inputs for the model.\n+    \"\"\"\n+    text = [\n+        \"circle\",\n+        \"ellipsoid\",\n+        \"blue circle on red background\",\n+        \"blue circle with green border on red background\",\n+        \"green circle on red background\",\n+        \"a dog\",\n+        \"a blue dog with a green border on a red background\",\n+    ]\n+    img224 = create_image(224, 224)\n+    img1024 = create_image(1024, 1024)\n+    img224_1024 = create_image(1024, 224)\n+\n+    images = [img224, img1024, img224_1024]\n+    return text, images\n+\n+\n+# --------------------------------------------------------------------------------------------\n+# Convert model\n+# --------------------------------------------------------------------------------------------\n+\n+\n+@torch.no_grad()\n+def convert_siglip2_checkpoint(model_name, pytorch_dump_folder_path, verify_logits=True, push_to_hub=False):\n+    \"\"\"\n+    Copy/paste/tweak model's weights to our Siglip2 structure.\n+    \"\"\"\n+\n+    # Define Siglip2 configuration\n+    config = get_siglip2_config(model_name)\n+\n+    checkpoint = MODEL_NAME_TO_CHECKPOINT_PATH[model_name]\n+    if not os.path.exists(checkpoint):\n+        org, repo_id, *filepath = checkpoint.split(\"/\")\n+        checkpoint = hf_hub_download(repo_id=f\"{org}/{repo_id}\", filename=\"/\".join(filepath))\n+\n+    print(f\"Loading checkpoint from {checkpoint}...\")\n+    data = np.load(checkpoint)\n+    state_dict = flatten_nested_dict(data)\n+    state_dict = split_encoderblock_layers(state_dict)\n+    state_dict = merge_qkv_for_head(state_dict, config)\n+\n+    # Rename and transform weights\n+    print(\"Renaming and transforming weights...\")\n+\n+    original_keys = list(state_dict.keys())\n+    hf_keys = convert_old_keys_to_new_keys(original_keys)\n+\n+    new_state_dict = {}\n+    for original_key in original_keys:\n+        new_key = hf_keys[original_key]\n+        parameter = state_dict.pop(original_key)\n+\n+        hidden_size = config.vision_config.hidden_size if \"vision\" in new_key else config.text_config.hidden_size\n+\n+        if any(k in new_key for k in (\"out_proj\", \"q_proj\", \"k_proj\", \"v_proj\", \"position_embedding\")):\n+            parameter = parameter.reshape(-1, hidden_size)\n+\n+        # Transpose every weight except for position_embedding and token_embedding\n+        if new_key.endswith(\"weight\") and \"position_embedding\" not in new_key and \"token_embedding\" not in new_key:\n+            parameter = parameter.T\n+\n+        # Reshape every bias\n+        if new_key.endswith(\"bias\"):\n+            parameter = parameter.reshape(-1)\n+\n+        new_state_dict[new_key] = torch.from_numpy(parameter)\n+\n+    # load HuggingFace model\n+    print(\"Loading HuggingFace model...\")\n+    model = Siglip2Model(config).eval()\n+    model.load_state_dict(new_state_dict)\n+\n+    # Create processor\n+    print(\"Creating processor...\")\n+    # TODO: update with more checkpoints\n+    tokenizer = get_siglip2_tokenizer()\n+    image_processor = get_siglip2_image_processor(config.vision_config.patch_size, max_num_patches=256)\n+    processor = Siglip2Processor(image_processor=image_processor, tokenizer=tokenizer)\n+\n+    # Verify logits\n+    if verify_logits:\n+        print(f\"Verifying logits for {model_name}...\")\n+        text, images = prepare_inputs()\n+        inputs = processor(text=text, images=images, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n+        outputs = model(**inputs)\n+        torch.testing.assert_close(outputs.logits_per_text, EXPECTED_OUTPUTS[model_name], atol=1e-3, rtol=1e-3)\n+\n+    # Save model\n+    if pytorch_dump_folder_path is not None:\n+        dst_dir = os.path.join(pytorch_dump_folder_path, model_name)\n+        print(f\"Saving model {model_name} to {dst_dir}...\")\n+        model.save_pretrained(dst_dir)\n+        print(f\"Saving processor to {dst_dir}...\")\n+        processor.save_pretrained(dst_dir)\n+\n+    if push_to_hub:\n+        print(f\"Pushing model and processor for {model_name} to the HuggingFace Hub...\")\n+        model.push_to_hub(f\"qubvel-hf/{model_name}\", private=True)\n+        processor.push_to_hub(f\"qubvel-hf/{model_name}\", private=True)\n+\n+\n+if __name__ == \"__main__\":\n+    parser = argparse.ArgumentParser()\n+    # Required parameters\n+    parser.add_argument(\n+        \"--model_name\",\n+        default=\"siglip2-base-patch16-naflex\",\n+        type=str,\n+        choices=MODEL_NAME_TO_CHECKPOINT_PATH.keys(),\n+        help=\"Name of the model you'd like to convert.\",\n+    )\n+    parser.add_argument(\n+        \"--pytorch_dump_folder_path\",\n+        default=\"checkpoints/\",\n+        type=str,\n+        help=\"Path to the output PyTorch model directory.\",\n+    )\n+    parser.add_argument(\n+        \"--verify_logits\",\n+        action=\"store_true\",\n+        help=\"Whether to verify logits against the original implementation.\",\n+    )\n+    parser.add_argument(\n+        \"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the converted model to the ü§ó hub.\"\n+    )\n+\n+    args = parser.parse_args()\n+    convert_siglip2_checkpoint(args.model_name, args.pytorch_dump_folder_path, args.verify_logits, args.push_to_hub)"
        },
        {
            "sha": "6278010319b918060cdb502bd75a299376ede6e9",
            "filename": "src/transformers/models/siglip2/image_processing_siglip2.py",
            "status": "added",
            "additions": 343,
            "deletions": 0,
            "changes": 343,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -0,0 +1,343 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Image processor class for SigLIP2.\"\"\"\n+\n+import math\n+from functools import lru_cache\n+from typing import List, Optional, Tuple, Union\n+\n+import numpy as np\n+\n+from ...image_processing_utils import BaseImageProcessor, BatchFeature\n+from ...image_transforms import (\n+    convert_to_rgb,\n+    resize,\n+    to_channel_dimension_format,\n+)\n+from ...image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    infer_channel_dimension_format,\n+    is_scaled_image,\n+    make_flat_list_of_images,\n+    to_numpy_array,\n+    valid_images,\n+    validate_preprocess_arguments,\n+)\n+from ...utils import TensorType, filter_out_non_signature_kwargs, is_vision_available, logging\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+\n+@lru_cache(maxsize=256)\n+def get_image_size_for_max_num_patches(\n+    image_height: int, image_width: int, patch_size: int, max_num_patches: int, eps: float = 1e-5\n+) -> Tuple[int, int]:\n+    \"\"\"\n+    Determine image size based on max number of patches, ensure dimensions are divisible by patch size and image is at least 1 patch.\n+\n+    Args:\n+        image_height (`int`):\n+            Original image height.\n+        image_width (`int`):\n+            Original image width.\n+        patch_size (`int`):\n+            Patch size for processing.\n+        max_num_patches (`int`):\n+            Maximum number of patches.\n+        eps (`float`):\n+            Small threshold for binary search.\n+\n+    Returns:\n+        Tuple: (target_height, target_width)\n+    \"\"\"\n+\n+    def get_scaled_image_size(scale: float, size: int, patch_size: int) -> int:\n+        scaled_size = size * scale\n+        scaled_size = math.ceil(scaled_size / patch_size) * patch_size  # make divisible by patch_size\n+        scaled_size = max(patch_size, scaled_size)  # ensure at least 1 patch\n+        return int(scaled_size)\n+\n+    # Binary search for optimal scale\n+    scale_min, scale_max = eps / 10, 100.0\n+    while (scale_max - scale_min) >= eps:\n+        scale = (scale_min + scale_max) / 2\n+        target_height = get_scaled_image_size(scale, image_height, patch_size)\n+        target_width = get_scaled_image_size(scale, image_width, patch_size)\n+        num_patches = (target_height / patch_size) * (target_width / patch_size)\n+\n+        if num_patches <= max_num_patches:\n+            scale_min = scale\n+        else:\n+            scale_max = scale\n+\n+    scale = scale_min\n+    target_height = get_scaled_image_size(scale, image_height, patch_size)\n+    target_width = get_scaled_image_size(scale, image_width, patch_size)\n+    return target_height, target_width\n+\n+\n+def convert_image_to_patches(image: np.ndarray, patch_size: int) -> np.ndarray:\n+    \"\"\"\n+    Convert 3D array image of shape (image_height, image_width, num_channels) into 2D array of patches of shape\n+    (num_patches_height * num_patches_width, patch_size * patch_size * num_channels).\n+    \"\"\"\n+    image_height, image_width, num_channels = image.shape\n+    num_patches_height = image_height // patch_size\n+    num_patches_width = image_width // patch_size\n+    patched_image = image.reshape(num_patches_height, patch_size, num_patches_width, patch_size, num_channels)\n+    patched_image = patched_image.transpose(0, 2, 1, 3, 4)\n+    patched_image = patched_image.reshape(num_patches_height * num_patches_width, -1)\n+    return patched_image\n+\n+\n+def pad_along_first_dim(array: np.ndarray, target_length: int, pad_value: int = 0) -> Tuple[np.ndarray, np.ndarray]:\n+    \"\"\"\n+    Pad the array along the first dimension.\n+    \"\"\"\n+    current_length = array.shape[0]\n+    padding_length = target_length - current_length\n+    mask = np.ones((target_length,), dtype=np.int32)\n+    if padding_length > 0:\n+        paddings = [(0, padding_length)] + [(0, 0)] * (array.ndim - 1)\n+        array = np.pad(array, paddings, mode=\"constant\", constant_values=pad_value)\n+        mask[-padding_length:] = 0\n+    return array, mask\n+\n+\n+class Siglip2ImageProcessor(BaseImageProcessor):\n+    r\"\"\"\n+    Constructs a SigLIP2 image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's dimensions to fit `max_num_patches` according to given `patch_size`.\n+            Can be overridden by `do_resize` in the `preprocess` method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image by the specified mean and standard deviation. Can be overridden by\n+            `do_normalize` in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.5, 0.5, 0.5]`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `[0.5, 0.5, 0.5]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch the image will be split to.\n+        max_num_patches (`int`, *optional*, defaults to 256):\n+            The image will be resized to have at most this number of patches,\n+            and then padded in \"patch\" dimension to match this number exactly.\n+    \"\"\"\n+\n+    model_input_names = [\"pixel_values\", \"pixel_attention_mask\", \"spatial_shapes\"]\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        do_rescale: bool = True,\n+        rescale_factor: float = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        patch_size: int = 16,\n+        max_num_patches: int = 256,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        image_mean = image_mean if image_mean is not None else [0.5, 0.5, 0.5]\n+        image_std = image_std if image_std is not None else [0.5, 0.5, 0.5]\n+\n+        self.do_resize = do_resize\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+        self.patch_size = patch_size\n+        self.max_num_patches = max_num_patches\n+\n+    @filter_out_non_signature_kwargs()\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        patch_size: Optional[int] = None,\n+        max_num_patches: Optional[int] = None,\n+    ) -> \"Image.Image\":\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n+                Patch size for processing, same as the patch size used in the model.\n+            max_num_patches (`int`, *optional*, defaults to `self.max_num_patches`):\n+                Maximum number of patches per image, the image will be resized to have at most this number of patches.\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        patch_size = patch_size if patch_size is not None else self.patch_size\n+        max_num_patches = max_num_patches if max_num_patches is not None else self.max_num_patches\n+\n+        # Explicitly specify data format to be channels last for image preprocessing.\n+        # Image processor does not support different output formats, because it returns patches.\n+        data_format = ChannelDimension.LAST\n+\n+        images = make_flat_list_of_images(images)\n+\n+        if not valid_images(images):\n+            raise ValueError(\n+                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n+                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n+            )\n+        validate_preprocess_arguments(\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            do_normalize=do_normalize,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+        )\n+        if do_convert_rgb:\n+            images = [convert_to_rgb(image) for image in images]\n+\n+        # All transformations expect numpy arrays.\n+        images = [to_numpy_array(image) for image in images]\n+\n+        if do_rescale and is_scaled_image(images[0]):\n+            logger.warning_once(\n+                \"It looks like you are trying to rescale already rescaled images. If the input\"\n+                \" images have pixel values between 0 and 1, set `do_rescale=False` to avoid rescaling them again.\"\n+            )\n+\n+        if input_data_format is None:\n+            # We assume that all images have the same channel dimension format.\n+            input_data_format = infer_channel_dimension_format(images[0])\n+\n+        pixel_masks = []\n+        pixel_values = []\n+        spatial_shapes = []\n+\n+        for image in images:\n+            image = to_channel_dimension_format(image, data_format, input_channel_dim=input_data_format)\n+\n+            if do_resize:\n+                height, width = get_image_size_for_max_num_patches(\n+                    image_height=image.shape[0],\n+                    image_width=image.shape[1],\n+                    patch_size=patch_size,\n+                    max_num_patches=max_num_patches,\n+                )\n+                image = resize(image=image, size=(height, width), resample=resample, input_data_format=data_format)\n+\n+            if do_rescale:\n+                image = self.rescale(image=image, scale=rescale_factor, input_data_format=data_format)\n+\n+            if do_normalize:\n+                image = self.normalize(image=image, mean=image_mean, std=image_std, input_data_format=data_format)\n+\n+            patches = convert_image_to_patches(image, patch_size)\n+            patches, mask = pad_along_first_dim(patches, max_num_patches)\n+            num_patches_height = image.shape[0] // patch_size\n+            num_patches_width = image.shape[1] // patch_size\n+\n+            spatial_shapes.append((num_patches_height, num_patches_width))\n+            pixel_values.append(patches)\n+            pixel_masks.append(mask)\n+\n+        batch_feature = BatchFeature(\n+            data={\n+                \"pixel_values\": pixel_values,\n+                \"pixel_attention_mask\": pixel_masks,\n+                \"spatial_shapes\": spatial_shapes,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+\n+        return batch_feature\n+\n+\n+__all__ = [\"Siglip2ImageProcessor\"]"
        },
        {
            "sha": "3cb2015e369590cf5e774e0052a1ecf816e55a27",
            "filename": "src/transformers/models/siglip2/image_processing_siglip2_fast.py",
            "status": "added",
            "additions": 322,
            "deletions": 0,
            "changes": 322,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fimage_processing_siglip2_fast.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -0,0 +1,322 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for SigLIP2.\"\"\"\n+\n+import math\n+from functools import lru_cache\n+from typing import List, Optional, Tuple, Union\n+\n+import torch\n+\n+from ...image_processing_utils import BatchFeature\n+from ...image_processing_utils_fast import BaseImageProcessorFast\n+from ...image_utils import (\n+    ChannelDimension,\n+    ImageInput,\n+    PILImageResampling,\n+    SizeDict,\n+    TensorType,\n+)\n+from ...utils import (\n+    filter_out_non_signature_kwargs,\n+    is_torch_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+\n+@lru_cache(maxsize=256)\n+# Copied from transformers.models.siglip2.image_processing_siglip2.get_image_size_for_max_num_patches\n+def get_image_size_for_max_num_patches(\n+    image_height: int, image_width: int, patch_size: int, max_num_patches: int, eps: float = 1e-5\n+) -> Tuple[int, int]:\n+    \"\"\"\n+    Determine image size based on max number of patches, ensure dimensions are divisible by patch size and image is at least 1 patch.\n+\n+    Args:\n+        image_height (`int`):\n+            Original image height.\n+        image_width (`int`):\n+            Original image width.\n+        patch_size (`int`):\n+            Patch size for processing.\n+        max_num_patches (`int`):\n+            Maximum number of patches.\n+        eps (`float`):\n+            Small threshold for binary search.\n+\n+    Returns:\n+        Tuple: (target_height, target_width)\n+    \"\"\"\n+\n+    def get_scaled_image_size(scale: float, size: int, patch_size: int) -> int:\n+        scaled_size = size * scale\n+        scaled_size = math.ceil(scaled_size / patch_size) * patch_size  # make divisible by patch_size\n+        scaled_size = max(patch_size, scaled_size)  # ensure at least 1 patch\n+        return int(scaled_size)\n+\n+    # Binary search for optimal scale\n+    scale_min, scale_max = eps / 10, 100.0\n+    while (scale_max - scale_min) >= eps:\n+        scale = (scale_min + scale_max) / 2\n+        target_height = get_scaled_image_size(scale, image_height, patch_size)\n+        target_width = get_scaled_image_size(scale, image_width, patch_size)\n+        num_patches = (target_height / patch_size) * (target_width / patch_size)\n+\n+        if num_patches <= max_num_patches:\n+            scale_min = scale\n+        else:\n+            scale_max = scale\n+\n+    scale = scale_min\n+    target_height = get_scaled_image_size(scale, image_height, patch_size)\n+    target_width = get_scaled_image_size(scale, image_width, patch_size)\n+    return target_height, target_width\n+\n+\n+def convert_image_to_patches(image: \"torch.Tensor\", patch_size: int) -> \"torch.Tensor\":\n+    \"\"\"\n+    Convert 3D tensor image of shape (num_channels, image_height, image_width) into 2D tensor of patches of shape\n+    (num_patches_height * num_patches_width, patch_size * patch_size * num_channels).\n+    \"\"\"\n+    num_channels, image_height, image_width = image.shape\n+    num_patches_height = image_height // patch_size\n+    num_patches_width = image_width // patch_size\n+    patched_image = image.reshape(num_channels, num_patches_height, patch_size, num_patches_width, patch_size)\n+    patched_image = patched_image.permute(1, 3, 2, 4, 0)\n+    patched_image = patched_image.reshape(num_patches_height * num_patches_width, -1)\n+    return patched_image\n+\n+\n+def pad_along_first_dim(\n+    tensor: \"torch.Tensor\", target_length: int, pad_value: int = 0\n+) -> Tuple[\"torch.Tensor\", \"torch.Tensor\"]:\n+    \"\"\"\n+    Pad the tensor along the first dimension.\n+    \"\"\"\n+    current_length = tensor.shape[0]\n+    padding_length = target_length - current_length\n+    mask = torch.ones((target_length,), dtype=torch.int32)\n+    if padding_length > 0:\n+        padding = [0, 0] * (tensor.ndim - 1) + [0, padding_length]\n+        tensor = torch.nn.functional.pad(tensor, padding, mode=\"constant\", value=pad_value)\n+        mask[-padding_length:] = 0\n+    return tensor, mask\n+\n+\n+class Siglip2ImageProcessorFast(BaseImageProcessorFast):\n+    r\"\"\"\n+    Constructs a fast SigLIP2 image processor.\n+\n+    Args:\n+        do_resize (`bool`, *optional*, defaults to `True`):\n+            Whether to resize the image's dimensions to fit `max_num_patches` according to given `patch_size`.\n+            Can be overridden by `do_resize` in the `preprocess` method.\n+        resample (`PILImageResampling`, *optional*, defaults to `Resampling.BILINEAR`):\n+            Resampling filter to use if resizing the image. Can be overridden by `resample` in the `preprocess` method.\n+        do_rescale (`bool`, *optional*, defaults to `True`):\n+            Whether to rescale the image by the specified scale `rescale_factor`. Can be overridden by `do_rescale` in\n+            the `preprocess` method.\n+        rescale_factor (`int` or `float`, *optional*, defaults to `1/255`):\n+            Scale factor to use if rescaling the image. Can be overridden by `rescale_factor` in the `preprocess`\n+            method.\n+        do_normalize (`bool`, *optional*, defaults to `True`):\n+            Whether to normalize the image by the specified mean and standard deviation. Can be overridden by\n+            `do_normalize` in the `preprocess` method.\n+        image_mean (`float` or `List[float]`, *optional*, defaults to `[0.5, 0.5, 0.5]`):\n+            Mean to use if normalizing the image. This is a float or list of floats the length of the number of\n+            channels in the image. Can be overridden by the `image_mean` parameter in the `preprocess` method.\n+        image_std (`float` or `List[float]`, *optional*, defaults to `[0.5, 0.5, 0.5]`):\n+            Standard deviation to use if normalizing the image. This is a float or list of floats the length of the\n+            number of channels in the image. Can be overridden by the `image_std` parameter in the `preprocess` method.\n+            Can be overridden by the `image_std` parameter in the `preprocess` method.\n+        do_convert_rgb (`bool`, *optional*, defaults to `True`):\n+            Whether to convert the image to RGB.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch the image will be split to.\n+        max_num_patches (`int`, *optional*, defaults to 256):\n+            The image will be resized to have at most this number of patches,\n+            and then padded in \"patch\" dimension to match this number exactly.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        do_resize: bool = True,\n+        resample: PILImageResampling = PILImageResampling.BILINEAR,\n+        do_rescale: bool = True,\n+        rescale_factor: float = 1 / 255,\n+        do_normalize: bool = True,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        patch_size: int = 16,\n+        max_num_patches: int = 256,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+\n+        image_mean = image_mean if image_mean is not None else [0.5, 0.5, 0.5]\n+        image_std = image_std if image_std is not None else [0.5, 0.5, 0.5]\n+\n+        self.do_resize = do_resize\n+        self.resample = resample\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.do_convert_rgb = do_convert_rgb\n+        self.patch_size = patch_size\n+        self.max_num_patches = max_num_patches\n+\n+    @filter_out_non_signature_kwargs()\n+    def preprocess(\n+        self,\n+        images: ImageInput,\n+        do_resize: Optional[bool] = None,\n+        resample: Optional[PILImageResampling] = None,\n+        do_rescale: Optional[bool] = None,\n+        rescale_factor: Optional[float] = None,\n+        do_normalize: Optional[bool] = None,\n+        image_mean: Optional[Union[float, List[float]]] = None,\n+        image_std: Optional[Union[float, List[float]]] = None,\n+        return_tensors: Optional[Union[str, TensorType]] = None,\n+        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n+        do_convert_rgb: Optional[bool] = None,\n+        patch_size: Optional[int] = None,\n+        max_num_patches: Optional[int] = None,\n+        device: Union[\"torch.device\", str] = \"cpu\",\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Preprocess an image or batch of images.\n+\n+        Args:\n+            images (`ImageInput`):\n+                Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255. If\n+                passing in images with pixel values between 0 and 1, set `do_rescale=False`.\n+            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n+                Whether to resize the image.\n+            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n+                Size of the image after resizing.\n+            resample (`int`, *optional*, defaults to `self.resample`):\n+                Resampling filter to use if resizing the image. This can be one of the enum `PILImageResampling`. Only\n+                has an effect if `do_resize` is set to `True`.\n+            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n+                Whether to rescale the image.\n+            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n+                Rescale factor to rescale the image by if `do_rescale` is set to `True`.\n+            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n+                Whether to normalize the image.\n+            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n+                Image mean to use for normalization. Only has an effect if `do_normalize` is set to `True`.\n+            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n+                Image standard deviation to use for normalization. Only has an effect if `do_normalize` is set to\n+                `True`.\n+            return_tensors (`str` or `TensorType`, *optional*):\n+                The type of tensors to return. Can be one of:\n+                - Unset: Return a list of `np.ndarray`.\n+                - `TensorType.TENSORFLOW` or `'tf'`: Return a batch of type `tf.Tensor`.\n+                - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n+                - `TensorType.NUMPY` or `'np'`: Return a batch of type `np.ndarray`.\n+                - `TensorType.JAX` or `'jax'`: Return a batch of type `jax.numpy.ndarray`.\n+            input_data_format (`ChannelDimension` or `str`, *optional*):\n+                The channel dimension format for the input image. If unset, the channel dimension format is inferred\n+                from the input image. Can be one of:\n+                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n+                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n+                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n+            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n+                Whether to convert the image to RGB.\n+            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n+                Patch size for processing, same as the patch size used in the model.\n+            max_num_patches (`int`, *optional*, defaults to `self.max_num_patches`):\n+                Maximum number of patches per image, the image will be resized to have at most this number of patches.\n+        \"\"\"\n+        do_resize = do_resize if do_resize is not None else self.do_resize\n+        resample = resample if resample is not None else self.resample\n+        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n+        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n+        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n+        image_mean = image_mean if image_mean is not None else self.image_mean\n+        image_std = image_std if image_std is not None else self.image_std\n+        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n+        patch_size = patch_size if patch_size is not None else self.patch_size\n+        max_num_patches = max_num_patches if max_num_patches is not None else self.max_num_patches\n+\n+        image_mean = tuple(image_mean) if isinstance(image_mean, list) else image_mean\n+        image_std = tuple(image_std) if isinstance(image_std, list) else image_std\n+\n+        image_mean, image_std, interpolation = self._prepare_process_arguments(\n+            do_normalize=do_normalize,\n+            do_rescale=do_rescale,\n+            rescale_factor=rescale_factor,\n+            image_mean=image_mean,\n+            image_std=image_std,\n+            resample=resample,\n+        )\n+\n+        images = self._prepare_input_images(\n+            images=images,\n+            do_convert_rgb=do_convert_rgb,\n+            input_data_format=input_data_format,\n+            device=device,\n+        )\n+\n+        pixel_masks = []\n+        pixel_values = []\n+        spatial_shapes = []\n+\n+        for image in images:\n+            if do_resize:\n+                height, width = get_image_size_for_max_num_patches(\n+                    image_height=image.shape[1],\n+                    image_width=image.shape[2],\n+                    patch_size=patch_size,\n+                    max_num_patches=max_num_patches,\n+                )\n+                side_dict = SizeDict(height=height, width=width)\n+                image = self.resize(image=image, size=side_dict, interpolation=interpolation)\n+\n+            image = self.rescale_and_normalize(image, do_rescale, rescale_factor, do_normalize, image_mean, image_std)\n+\n+            # (num_channels, height, width) -> (num_patches, patch_size * patch_size * num_channels)\n+            patches = convert_image_to_patches(image, patch_size)\n+            patches, mask = pad_along_first_dim(patches, max_num_patches)\n+\n+            num_patches_height = image.shape[1] // patch_size\n+            num_patches_width = image.shape[2] // patch_size\n+\n+            spatial_shapes.append((num_patches_height, num_patches_width))\n+            pixel_values.append(patches)\n+            pixel_masks.append(mask)\n+\n+        pixel_values = torch.stack(pixel_values)\n+        pixel_masks = torch.stack(pixel_masks)\n+        spatial_shapes = torch.tensor(spatial_shapes)\n+\n+        batch_feature = BatchFeature(\n+            data={\n+                \"pixel_values\": pixel_values,\n+                \"pixel_attention_mask\": pixel_masks,\n+                \"spatial_shapes\": spatial_shapes,\n+            },\n+            tensor_type=return_tensors,\n+        )\n+        return batch_feature\n+\n+\n+__all__ = [\"Siglip2ImageProcessorFast\"]"
        },
        {
            "sha": "4785ea9f0177e6c0904ebb34fe4dba50dbfd1342",
            "filename": "src/transformers/models/siglip2/modeling_siglip2.py",
            "status": "added",
            "additions": 1634,
            "deletions": 0,
            "changes": 1634,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodeling_siglip2.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -0,0 +1,1634 @@\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+#           This file was automatically generated from src/transformers/models/siglip2/modular_siglip2.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_siglip2.py file directly. One of our CI enforces this.\n+#                üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®üö®\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+import math\n+import warnings\n+from dataclasses import dataclass\n+from typing import Any, Optional, Tuple, Union\n+\n+import numpy as np\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+from torch.nn.init import _calculate_fan_in_and_fan_out\n+\n+from ...activations import ACT2FN\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling, ImageClassifierOutput\n+from ...modeling_utils import PreTrainedModel\n+from ...utils import (\n+    ModelOutput,\n+    add_start_docstrings,\n+    add_start_docstrings_to_model_forward,\n+    is_flash_attn_2_available,\n+    is_flash_attn_greater_or_equal_2_10,\n+    logging,\n+    replace_return_docstrings,\n+)\n+from .configuration_siglip2 import Siglip2Config, Siglip2TextConfig, Siglip2VisionConfig\n+\n+\n+if is_flash_attn_2_available():\n+    from ...modeling_flash_attention_utils import _flash_attention_forward\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+# General docstring\n+_CONFIG_FOR_DOC = \"Siglip2Config\"\n+\n+\n+@dataclass\n+class Siglip2VisionOutput(ModelOutput):\n+    \"\"\"\n+    Base class for vision model's outputs that also contains image embeddings of the pooling of the last hidden states.\n+\n+    Args:\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+            The image embeddings obtained by applying the projection layer to the pooler_output.\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    image_embeds: Optional[torch.FloatTensor] = None\n+    last_hidden_state: torch.FloatTensor = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@dataclass\n+class Siglip2TextOutput(ModelOutput):\n+    \"\"\"\n+    Base class for text model's outputs that also contains a pooling of the last hidden states.\n+\n+    Args:\n+        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim)` *optional* returned when model is initialized with `with_projection=True`):\n+            The text embeddings obtained by applying the projection layer to the pooler_output.\n+        last_hidden_state (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+            Sequence of hidden-states at the output of the last layer of the model.\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings, if the model has an embedding layer, +\n+            one for the output of each layer) of shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each layer) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+    \"\"\"\n+\n+    text_embeds: Optional[torch.FloatTensor] = None\n+    last_hidden_state: torch.FloatTensor = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+\n+@dataclass\n+class Siglip2Output(ModelOutput):\n+    \"\"\"\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `return_loss` is `True`):\n+            Contrastive loss for image-text similarity.\n+        logits_per_image (`torch.FloatTensor` of shape `(image_batch_size, text_batch_size)`):\n+            The scaled dot product scores between `image_embeds` and `text_embeds`. This represents the image-text\n+            similarity scores.\n+        logits_per_text (`torch.FloatTensor` of shape `(text_batch_size, image_batch_size)`):\n+            The scaled dot product scores between `text_embeds` and `image_embeds`. This represents the text-image\n+            similarity scores.\n+        text_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+            The text embeddings obtained by applying the projection layer to the pooled output of [`Siglip2TextModel`].\n+        image_embeds (`torch.FloatTensor` of shape `(batch_size, output_dim`):\n+            The image embeddings obtained by applying the projection layer to the pooled output of [`Siglip2VisionModel`].\n+        text_model_output (`BaseModelOutputWithPooling`):\n+            The output of the [`Siglip2TextModel`].\n+        vision_model_output (`BaseModelOutputWithPooling`):\n+            The output of the [`Siglip2VisionModel`].\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits_per_image: torch.FloatTensor = None\n+    logits_per_text: torch.FloatTensor = None\n+    text_embeds: torch.FloatTensor = None\n+    image_embeds: torch.FloatTensor = None\n+    text_model_output: BaseModelOutputWithPooling = None\n+    vision_model_output: BaseModelOutputWithPooling = None\n+\n+    def to_tuple(self) -> Tuple[Any]:\n+        return tuple(\n+            self[k] if k not in [\"text_model_output\", \"vision_model_output\"] else getattr(self, k).to_tuple()\n+            for k in self.keys()\n+        )\n+\n+\n+class Siglip2VisionEmbeddings(nn.Module):\n+    def __init__(self, config: Siglip2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.patch_size = config.patch_size\n+\n+        self.patch_embedding = nn.Linear(\n+            in_features=config.num_channels * self.patch_size * self.patch_size,\n+            out_features=self.embed_dim,\n+        )\n+\n+        self.num_patches = config.num_patches\n+        self.position_embedding_size = int(self.num_patches**0.5)\n+        self.position_embedding = nn.Embedding(self.num_patches, self.embed_dim)\n+\n+    @staticmethod\n+    def resize_positional_embeddings(\n+        positional_embeddings: torch.Tensor,\n+        spatial_shapes: torch.LongTensor,\n+        max_length: int,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize positional embeddings to image-specific size and pad to a fixed size.\n+\n+        Args:\n+            positional_embeddings (`torch.Tensor`):\n+                Position embeddings of shape (height, width, embed_dim)\n+            spatial_shapes (`torch.LongTensor`):\n+                Spatial shapes of shape (batch_size, 2) to resize the positional embeddings to\n+            max_length (`int`):\n+                Maximum length of the positional embeddings to pad resized positional embeddings to\n+\n+        Returns:\n+            `torch.Tensor`: Embeddings of shape (batch_size, max_length, embed_dim)\n+        \"\"\"\n+        batch_size = spatial_shapes.shape[0]\n+        embed_dim = positional_embeddings.shape[-1]\n+        source_dtype = positional_embeddings.dtype\n+\n+        resulted_positional_embeddings = torch.empty(\n+            (batch_size, max_length, embed_dim),\n+            device=positional_embeddings.device,\n+            dtype=source_dtype,\n+        )\n+\n+        # (height, width, embed_dim) -> (1, embed_dim, height, width) for interpolation\n+        positional_embeddings = positional_embeddings.permute(2, 0, 1).unsqueeze(0)\n+\n+        # Upcast to float32 on CPU because antialias is not supported for bfloat16/float16 on CPU\n+        if positional_embeddings.device.type == \"cpu\":\n+            positional_embeddings = positional_embeddings.to(torch.float32)\n+\n+        for i in range(batch_size):\n+            # (1, dim, height, width) -> (1, dim, target_height, target_width)\n+            height, width = spatial_shapes[i]\n+            resized_embeddings = F.interpolate(\n+                positional_embeddings,\n+                size=(height, width),\n+                mode=\"bilinear\",\n+                align_corners=False,\n+                antialias=True,\n+            )\n+\n+            # (1, dim, target_height, target_width) -> (target_height * target_width, dim)\n+            resized_embeddings = resized_embeddings.reshape(embed_dim, height * width).transpose(0, 1)\n+\n+            # Cast to original dtype\n+            resized_embeddings = resized_embeddings.to(source_dtype)\n+\n+            resulted_positional_embeddings[i, : height * width] = resized_embeddings\n+            resulted_positional_embeddings[i, height * width :] = resized_embeddings[0]\n+\n+        return resulted_positional_embeddings\n+\n+    def forward(self, pixel_values: torch.FloatTensor, spatial_shapes: torch.LongTensor) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            pixel_values (`torch.FloatTensor`):\n+                Pixel values of shape (batch_size, max_num_patches, num_channels * patch_size * patch_size)\n+            spatial_shapes (`List[Tuple[int, int]]`):\n+                Spatial shapes of shape (batch_size, 2) to resize the positional embeddings to\n+        \"\"\"\n+\n+        # Apply patch embeddings to already patchified pixel values\n+        target_dtype = self.patch_embedding.weight.dtype\n+        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))\n+\n+        # Get positional resized and padded positional embeddings\n+        positional_embeddings = self.position_embedding.weight.reshape(\n+            self.position_embedding_size, self.position_embedding_size, -1\n+        )\n+        resized_positional_embeddings = self.resize_positional_embeddings(\n+            positional_embeddings, spatial_shapes, max_length=pixel_values.shape[1]\n+        )\n+\n+        # Add positional embeddings to patch embeddings\n+        embeddings = patch_embeds + resized_positional_embeddings\n+        return embeddings\n+\n+\n+class Siglip2Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.num_heads = config.num_attention_heads\n+        self.head_dim = self.embed_dim // self.num_heads\n+        if self.head_dim * self.num_heads != self.embed_dim:\n+            raise ValueError(\n+                f\"embed_dim must be divisible by num_heads (got `embed_dim`: {self.embed_dim} and `num_heads`:\"\n+                f\" {self.num_heads}).\"\n+            )\n+        self.scale = self.head_dim**-0.5\n+        self.dropout = config.attention_dropout\n+\n+        self.k_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.v_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.q_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+        self.out_proj = nn.Linear(self.embed_dim, self.embed_dim)\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        \"\"\"Input shape: Batch x Time x Channel\"\"\"\n+\n+        batch_size, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        k_v_seq_len = key_states.shape[-2]\n+        attn_weights = torch.matmul(query_states, key_states.transpose(2, 3)) * self.scale\n+\n+        if attn_weights.size() != (batch_size, self.num_heads, q_len, k_v_seq_len):\n+            raise ValueError(\n+                f\"Attention weights should be of size {(batch_size, self.num_heads, q_len, k_v_seq_len)}, but is\"\n+                f\" {attn_weights.size()}\"\n+            )\n+\n+        if attention_mask is not None:\n+            if attention_mask.size() != (batch_size, 1, q_len, k_v_seq_len):\n+                raise ValueError(\n+                    f\"Attention mask should be of size {(batch_size, 1, q_len, k_v_seq_len)}, but is {attention_mask.size()}\"\n+                )\n+            attn_weights = attn_weights + attention_mask\n+\n+        # upcast attention to fp32\n+        attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query_states.dtype)\n+        attn_weights = nn.functional.dropout(attn_weights, p=self.dropout, training=self.training)\n+        attn_output = torch.matmul(attn_weights, value_states)\n+\n+        if attn_output.size() != (batch_size, self.num_heads, q_len, self.head_dim):\n+            raise ValueError(\n+                f\"`attn_output` should be of size {(batch_size, self.num_heads, q_len, self.head_dim)}, but is\"\n+                f\" {attn_output.size()}\"\n+            )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, attn_weights\n+\n+\n+class Siglip2FlashAttention2(Siglip2Attention):\n+    \"\"\"\n+    Siglip2Attention flash attention module. This module inherits from `Siglip2Attention` as the weights of the module stays\n+    untouched. The only required change would be on the forward pass where it needs to correctly call the public API of\n+    flash attention and deal with padding tokens in case the input contains any of them.\n+    \"\"\"\n+\n+    is_causal = False\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+\n+        # TODO: Should be removed once Flash Attention for RoCm is bumped to 2.1.\n+        # flash_attn<2.1 generates top-left aligned causal mask, while what is needed here is bottom-right alignement, that was made default for flash_attn>=2.1. This attribute is used to handle this difference. Reference: https://github.com/Dao-AILab/flash-attention/releases/tag/v2.1.0.\n+        # Beware that with flash_attn<2.1, using q_seqlen != k_seqlen (except for the case q_seqlen == 1) produces a wrong mask (top-left).\n+        self._flash_attn_uses_top_left_mask = not is_flash_attn_greater_or_equal_2_10()\n+\n+    # Adapted from transformers.models.llama.modeling_llama.LlamaFlashAttention2.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.LongTensor] = None,\n+        output_attentions: bool = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n+        output_attentions = False\n+\n+        batch_size, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        # Flash attention requires the input to have the shape\n+        # batch_size x seq_length x head_dim x hidden_dim\n+        # therefore we just need to keep the original shape\n+        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n+        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n+        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim)\n+\n+        dropout_rate = self.dropout if self.training else 0.0\n+\n+        # In PEFT, usually we cast the layer norms in float32 for training stability reasons\n+        # therefore the input hidden states gets silently casted in float32. Hence, we need\n+        # cast them back in the correct dtype just to be sure everything works as expected.\n+        # This might slowdown training & inference so it is recommended to not cast the LayerNorms\n+        # in fp32.\n+\n+        input_dtype = query_states.dtype\n+        if input_dtype == torch.float32:\n+            if torch.is_autocast_enabled():\n+                target_dtype = torch.get_autocast_gpu_dtype()\n+            # Handle the case where the model is quantized\n+            elif hasattr(self.config, \"_pre_quantization_dtype\"):\n+                target_dtype = self.config._pre_quantization_dtype\n+            else:\n+                target_dtype = self.q_proj.weight.dtype\n+\n+            logger.warning_once(\n+                f\"The input hidden states seems to be silently casted in float32, this might be related to\"\n+                f\" the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in\"\n+                f\" {target_dtype}.\"\n+            )\n+\n+            query_states = query_states.to(target_dtype)\n+            key_states = key_states.to(target_dtype)\n+            value_states = value_states.to(target_dtype)\n+\n+        attn_output = _flash_attention_forward(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            q_len,\n+            dropout=dropout_rate,\n+            is_causal=self.is_causal,\n+            use_top_left_mask=self._flash_attn_uses_top_left_mask,\n+        )\n+\n+        attn_output = attn_output.reshape(batch_size, q_len, self.embed_dim).contiguous()\n+        attn_output = self.out_proj(attn_output)\n+\n+        if not output_attentions:\n+            attn_weights = None\n+\n+        return attn_output, attn_weights\n+\n+\n+class Siglip2SdpaAttention(Siglip2Attention):\n+    \"\"\"\n+    Siglip2 attention module using torch.nn.functional.scaled_dot_product_attention. This module inherits from\n+    `Siglip2Attention` as the weights of the module stays untouched. The only changes are on the forward pass to adapt to\n+    SDPA API.\n+    \"\"\"\n+\n+    is_causal = False\n+\n+    # Adapted from Siglip2Attention.forward and transformers.models.llama.modeling_llama.LlamaSdpaAttention.forward\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        if output_attentions:\n+            # TODO: Improve this warning with e.g. `model.config.attn_implementation = \"manual\"` once this is implemented.\n+            logger.warning_once(\n+                \"Siglip2Model is using Siglip2SdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to the manual attention implementation, \"\n+                'but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n+            )\n+            return super().forward(\n+                hidden_states=hidden_states,\n+                attention_mask=attention_mask,\n+                output_attentions=output_attentions,\n+            )\n+\n+        batch_size, q_len, _ = hidden_states.size()\n+\n+        query_states = self.q_proj(hidden_states)\n+        key_states = self.k_proj(hidden_states)\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        key_states = key_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+        value_states = value_states.view(batch_size, q_len, self.num_heads, self.head_dim).transpose(1, 2)\n+\n+        # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n+        # Reference: https://github.com/pytorch/pytorch/issues/112577.\n+        if query_states.device.type == \"cuda\" and attention_mask is not None:\n+            query_states = query_states.contiguous()\n+            key_states = key_states.contiguous()\n+            value_states = value_states.contiguous()\n+\n+        # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n+        # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n+        is_causal = True if self.is_causal and q_len > 1 else False\n+\n+        attn_output = torch.nn.functional.scaled_dot_product_attention(\n+            query_states,\n+            key_states,\n+            value_states,\n+            attn_mask=attention_mask,\n+            dropout_p=self.dropout if self.training else 0.0,\n+            is_causal=is_causal,\n+        )\n+\n+        attn_output = attn_output.transpose(1, 2).contiguous()\n+        attn_output = attn_output.view(batch_size, q_len, self.embed_dim)\n+\n+        attn_output = self.out_proj(attn_output)\n+\n+        return attn_output, None\n+\n+\n+class Siglip2MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.activation_fn = ACT2FN[config.hidden_act]\n+        self.fc1 = nn.Linear(config.hidden_size, config.intermediate_size)\n+        self.fc2 = nn.Linear(config.intermediate_size, config.hidden_size)\n+\n+    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n+        hidden_states = self.fc1(hidden_states)\n+        hidden_states = self.activation_fn(hidden_states)\n+        hidden_states = self.fc2(hidden_states)\n+        return hidden_states\n+\n+\n+SIGLIP2_ATTENTION_CLASSES = {\n+    \"eager\": Siglip2Attention,\n+    \"flash_attention_2\": Siglip2FlashAttention2,\n+    \"sdpa\": Siglip2SdpaAttention,\n+}\n+\n+\n+class Siglip2EncoderLayer(nn.Module):\n+    def __init__(self, config: Siglip2Config):\n+        super().__init__()\n+        self.embed_dim = config.hidden_size\n+        self.self_attn = SIGLIP2_ATTENTION_CLASSES[config._attn_implementation](config=config)\n+        self.layer_norm1 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+        self.mlp = Siglip2MLP(config)\n+        self.layer_norm2 = nn.LayerNorm(self.embed_dim, eps=config.layer_norm_eps)\n+\n+    # Ignore copy\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: torch.Tensor,\n+        output_attentions: Optional[bool] = False,\n+    ) -> Tuple[torch.FloatTensor]:\n+        \"\"\"\n+        Args:\n+            hidden_states (`torch.FloatTensor`):\n+                Input to the layer of shape `(batch, seq_len, embed_dim)`.\n+            attention_mask (`torch.FloatTensor`):\n+                Attention mask of shape `(batch, 1, q_len, k_v_seq_len)` where padding elements are indicated by very large negative values.\n+            output_attentions (`bool`, *optional*, defaults to `False`):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+        \"\"\"\n+        residual = hidden_states\n+\n+        hidden_states = self.layer_norm1(hidden_states)\n+        hidden_states, attn_weights = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+        )\n+        hidden_states = residual + hidden_states\n+\n+        residual = hidden_states\n+        hidden_states = self.layer_norm2(hidden_states)\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        outputs = (hidden_states,)\n+\n+        if output_attentions:\n+            outputs += (attn_weights,)\n+\n+        return outputs\n+\n+\n+class Siglip2Encoder(nn.Module):\n+    \"\"\"\n+    Transformer encoder consisting of `config.num_hidden_layers` self attention layers. Each layer is a\n+    [`Siglip2EncoderLayer`].\n+\n+    Args:\n+        config: Siglip2Config\n+    \"\"\"\n+\n+    def __init__(self, config: Siglip2Config):\n+        super().__init__()\n+        self.config = config\n+        self.layers = nn.ModuleList([Siglip2EncoderLayer(config) for _ in range(config.num_hidden_layers)])\n+        self.gradient_checkpointing = False\n+\n+    # Ignore copy\n+    def forward(\n+        self,\n+        inputs_embeds,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutput]:\n+        r\"\"\"\n+        Args:\n+            inputs_embeds (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n+                Optionally, instead of passing `input_ids` you can choose to directly pass an embedded representation.\n+                This is useful if you want more control over how to convert `input_ids` indices into associated vectors\n+                than the model's internal embedding lookup matrix.\n+            attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+                - 1 for tokens that are **not masked**,\n+                - 0 for tokens that are **masked**.\n+\n+                [What are attention masks?](../glossary#attention-mask)\n+            output_attentions (`bool`, *optional*):\n+                Whether or not to return the attentions tensors of all attention layers. See `attentions` under\n+                returned tensors for more detail.\n+            output_hidden_states (`bool`, *optional*):\n+                Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors\n+                for more detail.\n+            return_dict (`bool`, *optional*):\n+                Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        encoder_states = () if output_hidden_states else None\n+        all_attentions = () if output_attentions else None\n+\n+        hidden_states = inputs_embeds\n+        for encoder_layer in self.layers:\n+            if output_hidden_states:\n+                encoder_states = encoder_states + (hidden_states,)\n+            if self.gradient_checkpointing and self.training:\n+                layer_outputs = self._gradient_checkpointing_func(\n+                    encoder_layer.__call__,\n+                    hidden_states,\n+                    attention_mask,\n+                    output_attentions,\n+                )\n+            else:\n+                layer_outputs = encoder_layer(\n+                    hidden_states,\n+                    attention_mask,\n+                    output_attentions=output_attentions,\n+                )\n+\n+            hidden_states = layer_outputs[0]\n+\n+            if output_attentions:\n+                all_attentions = all_attentions + (layer_outputs[1],)\n+\n+        if output_hidden_states:\n+            encoder_states = encoder_states + (hidden_states,)\n+\n+        if not return_dict:\n+            return tuple(v for v in [hidden_states, encoder_states, all_attentions] if v is not None)\n+        return BaseModelOutput(\n+            last_hidden_state=hidden_states, hidden_states=encoder_states, attentions=all_attentions\n+        )\n+\n+\n+SIGLIP2_VISION_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+            Whether to interpolate the pre-trained position encodings.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+class Siglip2VisionTransformer(nn.Module):\n+    def __init__(self, config: Siglip2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        embed_dim = config.hidden_size\n+\n+        self.embeddings = Siglip2VisionEmbeddings(config)\n+        self.encoder = Siglip2Encoder(config)\n+        self.post_layernorm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+        self.use_head = True if not hasattr(config, \"vision_use_head\") else config.vision_use_head\n+        if self.use_head:\n+            self.head = Siglip2MultiheadAttentionPoolingHead(config)\n+        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+\n+    @add_start_docstrings_to_model_forward(SIGLIP2_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Siglip2VisionConfig)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        attention_mask: torch.Tensor,\n+        spatial_shapes: torch.LongTensor,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        Returns:\n+\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        hidden_states = self.embeddings(pixel_values, spatial_shapes)\n+\n+        if attention_mask is not None and not self._use_flash_attention_2:\n+            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n+            encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+        else:\n+            encoder_attention_mask = attention_mask\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+\n+        pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n+        if not return_dict:\n+            return (last_hidden_state, pooler_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooler_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+class Siglip2TextEmbeddings(nn.Module):\n+    def __init__(self, config: Siglip2TextConfig):\n+        super().__init__()\n+        embed_dim = config.hidden_size\n+\n+        self.token_embedding = nn.Embedding(config.vocab_size, embed_dim)\n+        self.position_embedding = nn.Embedding(config.max_position_embeddings, embed_dim)\n+\n+        # position_ids (1, len position emb) is contiguous in memory and exported when serialized\n+        self.register_buffer(\n+            \"position_ids\", torch.arange(config.max_position_embeddings).expand((1, -1)), persistent=False\n+        )\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+    ) -> torch.Tensor:\n+        seq_length = input_ids.shape[-1] if input_ids is not None else inputs_embeds.shape[-2]\n+        max_position_embedding = self.position_embedding.weight.shape[0]\n+\n+        if seq_length > max_position_embedding:\n+            raise ValueError(\n+                f\"Sequence length must be less than max_position_embeddings (got `sequence length`: \"\n+                f\"{seq_length} and max_position_embeddings: {max_position_embedding}\"\n+            )\n+\n+        if position_ids is None:\n+            position_ids = self.position_ids[:, :seq_length]\n+\n+        if inputs_embeds is None:\n+            inputs_embeds = self.token_embedding(input_ids)\n+\n+        position_embeddings = self.position_embedding(position_ids)\n+        embeddings = inputs_embeds + position_embeddings\n+\n+        return embeddings\n+\n+\n+def _trunc_normal_(tensor, mean, std, a, b):\n+    # Cut & paste from PyTorch official master until it's in a few official releases - RW\n+    # Method based on https://people.sc.fsu.edu/~jburkardt/presentations/truncated_normal.pdf\n+    def norm_cdf(x):\n+        # Computes standard normal cumulative distribution function\n+        return (1.0 + math.erf(x / math.sqrt(2.0))) / 2.0\n+\n+    if (mean < a - 2 * std) or (mean > b + 2 * std):\n+        warnings.warn(\n+            \"mean is more than 2 std from [a, b] in nn.init.trunc_normal_. \"\n+            \"The distribution of values may be incorrect.\",\n+            stacklevel=2,\n+        )\n+\n+    # Values are generated by using a truncated uniform distribution and\n+    # then using the inverse CDF for the normal distribution.\n+    # Get upper and lower cdf values\n+    l = norm_cdf((a - mean) / std)\n+    u = norm_cdf((b - mean) / std)\n+\n+    # Uniformly fill tensor with values from [l, u], then translate to\n+    # [2l-1, 2u-1].\n+    tensor.uniform_(2 * l - 1, 2 * u - 1)\n+\n+    # Use inverse cdf transform for normal distribution to get truncated\n+    # standard normal\n+    tensor.erfinv_()\n+\n+    # Transform to proper mean, std\n+    tensor.mul_(std * math.sqrt(2.0))\n+    tensor.add_(mean)\n+\n+    # Clamp to ensure it's in the proper range\n+    tensor.clamp_(min=a, max=b)\n+\n+\n+def trunc_normal_tf_(\n+    tensor: torch.Tensor, mean: float = 0.0, std: float = 1.0, a: float = -2.0, b: float = 2.0\n+) -> torch.Tensor:\n+    \"\"\"Fills the input Tensor with values drawn from a truncated\n+    normal distribution. The values are effectively drawn from the\n+    normal distribution :math:`\\\\mathcal{N}(\\text{mean}, \\text{std}^2)`\n+    with values outside :math:`[a, b]` redrawn until they are within\n+    the bounds. The method used for generating the random values works\n+    best when :math:`a \\\\leq \\text{mean} \\\\leq b`.\n+\n+    NOTE: this 'tf' variant behaves closer to Tensorflow / JAX impl where the\n+    bounds [a, b] are applied when sampling the normal distribution with mean=0, std=1.0\n+    and the result is subsequently scaled and shifted by the mean and std args.\n+\n+    Args:\n+        tensor: an n-dimensional `torch.Tensor`\n+        mean: the mean of the normal distribution\n+        std: the standard deviation of the normal distribution\n+        a: the minimum cutoff value\n+        b: the maximum cutoff value\n+    \"\"\"\n+    with torch.no_grad():\n+        _trunc_normal_(tensor, 0, 1.0, a, b)\n+        tensor.mul_(std).add_(mean)\n+\n+\n+def variance_scaling_(tensor, scale=1.0, mode=\"fan_in\", distribution=\"normal\"):\n+    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n+    if mode == \"fan_in\":\n+        denom = fan_in\n+    elif mode == \"fan_out\":\n+        denom = fan_out\n+    elif mode == \"fan_avg\":\n+        denom = (fan_in + fan_out) / 2\n+\n+    variance = scale / denom\n+\n+    if distribution == \"truncated_normal\":\n+        # constant is stddev of standard normal truncated to (-2, 2)\n+        trunc_normal_tf_(tensor, std=math.sqrt(variance) / 0.87962566103423978)\n+    elif distribution == \"normal\":\n+        with torch.no_grad():\n+            tensor.normal_(std=math.sqrt(variance))\n+    elif distribution == \"uniform\":\n+        bound = math.sqrt(3 * variance)\n+        with torch.no_grad():\n+            tensor.uniform_(-bound, bound)\n+    else:\n+        raise ValueError(f\"invalid distribution {distribution}\")\n+\n+\n+def lecun_normal_(tensor):\n+    variance_scaling_(tensor, mode=\"fan_in\", distribution=\"truncated_normal\")\n+\n+\n+def default_flax_embed_init(tensor):\n+    variance_scaling_(tensor, mode=\"fan_in\", distribution=\"normal\")\n+\n+\n+SIGLIP2_TEXT_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+class Siglip2TextTransformer(nn.Module):\n+    def __init__(self, config: Siglip2TextConfig):\n+        super().__init__()\n+        self.config = config\n+        embed_dim = config.hidden_size\n+        self.embeddings = Siglip2TextEmbeddings(config)\n+        self.encoder = Siglip2Encoder(config)\n+        self.final_layer_norm = nn.LayerNorm(embed_dim, eps=config.layer_norm_eps)\n+\n+        self.head = nn.Linear(embed_dim, config.projection_size)\n+        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+\n+    @add_start_docstrings_to_model_forward(SIGLIP2_TEXT_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Siglip2TextConfig)\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        Returns:\n+\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        if input_ids is None:\n+            raise ValueError(\"You have to specify input_ids\")\n+\n+        input_shape = input_ids.size()\n+        input_ids = input_ids.view(-1, input_shape[-1])\n+\n+        hidden_states = self.embeddings(input_ids=input_ids, position_ids=position_ids)\n+\n+        # note: Siglip2's text model does not use a causal mask, unlike the original CLIP model.\n+        # expand attention_mask\n+        if attention_mask is not None and not self._use_flash_attention_2:\n+            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n+            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.final_layer_norm(last_hidden_state)\n+\n+        # Assuming \"sticky\" EOS tokenization, last token is always EOS.\n+        pooled_output = last_hidden_state[:, -1, :]\n+        pooled_output = self.head(pooled_output)\n+\n+        if not return_dict:\n+            return (last_hidden_state, pooled_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooled_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+SIGLIP2_START_DOCSTRING = r\"\"\"\n+    This model inherits from [`PreTrainedModel`]. Check the superclass documentation for the generic methods the\n+    library implements for all its model (such as downloading or saving, resizing the input embeddings, pruning heads\n+    etc.)\n+\n+    This model is also a PyTorch [torch.nn.Module](https://pytorch.org/docs/stable/nn.html#torch.nn.Module) subclass.\n+    Use it as a regular PyTorch Module and refer to the PyTorch documentation for all matter related to general usage\n+    and behavior.\n+\n+    Parameters:\n+        config ([`Siglip2Config`]): Model configuration class with all the parameters of the model.\n+            Initializing with a config file does not load the weights associated with the model, only the\n+            configuration. Check out the [`~PreTrainedModel.from_pretrained`] method to load the model weights.\n+\"\"\"\n+\n+SIGLIP2_INPUTS_DOCSTRING = r\"\"\"\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. Padding will be ignored by default should you provide\n+            it.\n+\n+            Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n+            [`PreTrainedTokenizer.__call__`] for details.\n+\n+            [What are input IDs?](../glossary#input-ids)\n+        attention_mask (`torch.Tensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n+        position_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Indices of positions of each input sequence tokens in the position embeddings. Selected in the range `[0,\n+            config.max_position_embeddings - 1]`.\n+\n+            [What are position IDs?](../glossary#position-ids)\n+        pixel_values (`torch.FloatTensor` of shape `(batch_size, num_channels, height, width)`):\n+            Pixel values. Padding will be ignored by default should you provide it. Pixel values can be obtained using\n+            [`AutoImageProcessor`]. See [`CLIPImageProcessor.__call__`] for details.\n+        return_loss (`bool`, *optional*):\n+            Whether or not to return the contrastive loss.\n+        output_attentions (`bool`, *optional*):\n+            Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n+            tensors for more detail.\n+        output_hidden_states (`bool`, *optional*):\n+            Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n+            more detail.\n+        interpolate_pos_encoding (`bool`, *optional*, defaults to `False`):\n+            Whether to interpolate the pre-trained position encodings.\n+        return_dict (`bool`, *optional*):\n+            Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+\"\"\"\n+\n+\n+class Siglip2PreTrainedModel(PreTrainedModel):\n+    \"\"\"\n+    An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n+    models.\n+    \"\"\"\n+\n+    config_class = Siglip2Config\n+    base_model_prefix = \"siglip2\"\n+    supports_gradient_checkpointing = True\n+\n+    _no_split_modules = [\n+        \"Siglip2TextEmbeddings\",\n+        \"Siglip2EncoderLayer\",\n+        \"Siglip2VisionEmbeddings\",\n+        \"Siglip2EncoderLayer\",\n+        \"Siglip2MultiheadAttentionPoolingHead\",\n+    ]\n+    _supports_flash_attn_2 = True\n+    _supports_sdpa = True\n+\n+    def _init_weights(self, module):\n+        \"\"\"Initialize the weights\"\"\"\n+        if isinstance(module, Siglip2VisionEmbeddings):\n+            width = (\n+                self.config.vision_config.hidden_size\n+                if isinstance(self.config, Siglip2Config)\n+                else self.config.hidden_size\n+            )\n+            nn.init.normal_(module.position_embedding.weight, std=1 / np.sqrt(width))\n+        elif isinstance(module, nn.Embedding):\n+            default_flax_embed_init(module.weight)\n+        elif isinstance(module, Siglip2Attention):\n+            nn.init.xavier_uniform_(module.q_proj.weight)\n+            nn.init.xavier_uniform_(module.k_proj.weight)\n+            nn.init.xavier_uniform_(module.v_proj.weight)\n+            nn.init.xavier_uniform_(module.out_proj.weight)\n+            nn.init.zeros_(module.q_proj.bias)\n+            nn.init.zeros_(module.k_proj.bias)\n+            nn.init.zeros_(module.v_proj.bias)\n+            nn.init.zeros_(module.out_proj.bias)\n+        elif isinstance(module, Siglip2MLP):\n+            nn.init.xavier_uniform_(module.fc1.weight)\n+            nn.init.xavier_uniform_(module.fc2.weight)\n+            nn.init.normal_(module.fc1.bias, std=1e-6)\n+            nn.init.normal_(module.fc2.bias, std=1e-6)\n+        elif isinstance(module, Siglip2MultiheadAttentionPoolingHead):\n+            nn.init.xavier_uniform_(module.probe.data)\n+            nn.init.xavier_uniform_(module.attention.in_proj_weight.data)\n+            nn.init.zeros_(module.attention.in_proj_bias.data)\n+        elif isinstance(module, Siglip2Model):\n+            logit_scale_init = torch.log(torch.tensor(1.0))\n+            module.logit_scale.data.fill_(logit_scale_init)\n+            module.logit_bias.data.zero_()\n+        elif isinstance(module, Siglip2ForImageClassification):\n+            nn.init.normal_(\n+                module.classifier.weight,\n+                std=self.config.vision_config.hidden_size**-0.5 * self.config.initializer_factor,\n+            )\n+        elif isinstance(module, (nn.Linear, nn.Conv2d)):\n+            lecun_normal_(module.weight)\n+            if module.bias is not None:\n+                nn.init.zeros_(module.bias)\n+        elif isinstance(module, nn.LayerNorm):\n+            module.bias.data.zero_()\n+            module.weight.data.fill_(1.0)\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The text model from Siglip2 without any head or projection on top.\"\"\",\n+    SIGLIP2_START_DOCSTRING,\n+)\n+class Siglip2TextModel(Siglip2PreTrainedModel):\n+    config_class = Siglip2TextConfig\n+\n+    def __init__(self, config: Siglip2TextConfig):\n+        super().__init__(config)\n+        self.text_model = Siglip2TextTransformer(config)\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.text_model.embeddings.token_embedding\n+\n+    def set_input_embeddings(self, value):\n+        self.text_model.embeddings.token_embedding = value\n+\n+    @add_start_docstrings_to_model_forward(SIGLIP2_TEXT_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Siglip2TextConfig)\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Siglip2TextModel\n+\n+        >>> model = Siglip2TextModel.from_pretrained(\"google/siglip2-base-patch16-224\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"google/siglip2-base-patch16-224\")\n+\n+        >>> # important: make sure to set padding=\"max_length\" as that's how the model was trained\n+        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=\"max_length\", return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> last_hidden_state = outputs.last_hidden_state\n+        >>> pooled_output = outputs.pooler_output  # pooled (EOS token) states\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        return self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+\n+class Siglip2MultiheadAttentionPoolingHead(nn.Module):\n+    \"\"\"Multihead Attention Pooling.\"\"\"\n+\n+    def __init__(self, config: Siglip2VisionConfig):\n+        super().__init__()\n+\n+        self.probe = nn.Parameter(torch.randn(1, 1, config.hidden_size))\n+        self.attention = torch.nn.MultiheadAttention(config.hidden_size, config.num_attention_heads, batch_first=True)\n+        self.layernorm = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)\n+        self.mlp = Siglip2MLP(config)\n+        self.num_heads = config.num_attention_heads\n+\n+    def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor] = None):\n+        batch_size = hidden_state.shape[0]\n+        probe = self.probe.repeat(batch_size, 1, 1)\n+\n+        if attention_mask is not None:\n+            target_len, source_len = probe.shape[1], hidden_state.shape[1]\n+            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_state.dtype, target_len)\n+            attention_mask = attention_mask.repeat(1, self.num_heads, target_len, 1)\n+            attention_mask = attention_mask.reshape(-1, target_len, source_len)\n+\n+        hidden_state = self.attention(probe, hidden_state, hidden_state, attn_mask=attention_mask)[0]\n+\n+        residual = hidden_state\n+        hidden_state = self.layernorm(hidden_state)\n+        hidden_state = residual + self.mlp(hidden_state)\n+\n+        return hidden_state[:, 0]\n+\n+\n+@add_start_docstrings(\n+    \"\"\"The vision model from Siglip2 without any head or projection on top.\"\"\",\n+    SIGLIP2_START_DOCSTRING,\n+)\n+class Siglip2VisionModel(Siglip2PreTrainedModel):\n+    config_class = Siglip2VisionConfig\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(self, config: Siglip2VisionConfig):\n+        super().__init__(config)\n+\n+        self.vision_model = Siglip2VisionTransformer(config)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        return self.vision_model.embeddings.patch_embedding\n+\n+    @add_start_docstrings_to_model_forward(SIGLIP2_VISION_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=BaseModelOutputWithPooling, config_class=Siglip2VisionConfig)\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        pixel_attention_mask: torch.Tensor,\n+        spatial_shapes: torch.LongTensor,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, Siglip2VisionModel\n+\n+        >>> model = Siglip2VisionModel.from_pretrained(\"google/siglip2-base-patch16-224\")\n+        >>> processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> outputs = model(**inputs)\n+        >>> last_hidden_state = outputs.last_hidden_state\n+        >>> pooled_output = outputs.pooler_output  # pooled features\n+        ```\"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        return self.vision_model(\n+            pixel_values=pixel_values,\n+            attention_mask=pixel_attention_mask,\n+            spatial_shapes=spatial_shapes,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+\n+@add_start_docstrings(SIGLIP2_START_DOCSTRING)\n+class Siglip2Model(Siglip2PreTrainedModel):\n+    config_class = Siglip2Config\n+\n+    def __init__(self, config: Siglip2Config):\n+        super().__init__(config)\n+\n+        if not isinstance(config.text_config, Siglip2TextConfig):\n+            raise TypeError(\n+                \"config.text_config is expected to be of type Siglip2TextConfig but is of type\"\n+                f\" {type(config.text_config)}.\"\n+            )\n+\n+        if not isinstance(config.vision_config, Siglip2VisionConfig):\n+            raise TypeError(\n+                \"config.vision_config is expected to be of type Siglip2VisionConfig but is of type\"\n+                f\" {type(config.vision_config)}.\"\n+            )\n+\n+        text_config = config.text_config\n+        vision_config = config.vision_config\n+\n+        # First, initialize the text and vision models with proper attention implementation\n+        text_model = Siglip2TextModel._from_config(text_config)\n+        vision_model = Siglip2VisionModel._from_config(vision_config)\n+\n+        # Second, get the text and vision submodules (for backward compatibility)\n+        self.text_model = text_model.text_model\n+        self.vision_model = vision_model.vision_model\n+\n+        self.logit_scale = nn.Parameter(torch.randn(1))\n+        self.logit_bias = nn.Parameter(torch.randn(1))\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(SIGLIP2_TEXT_INPUTS_DOCSTRING)\n+    def get_text_features(\n+        self,\n+        input_ids: Optional[torch.Tensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> torch.FloatTensor:\n+        r\"\"\"\n+        Returns:\n+            text_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The text embeddings obtained by\n+            applying the projection layer to the pooled output of [`Siglip2TextModel`].\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, AutoModel\n+        >>> import torch\n+\n+        >>> model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-224\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"google/siglip2-base-patch16-224\")\n+\n+        >>> # important: make sure to set padding=\"max_length\" as that's how the model was trained\n+        >>> inputs = tokenizer([\"a photo of a cat\", \"a photo of a dog\"], padding=\"max_length\", return_tensors=\"pt\")\n+        >>> with torch.no_grad():\n+        ...     text_features = model.get_text_features(**inputs)\n+        ```\"\"\"\n+        # Use Siglip2 model's config for some fields (if specified) instead of those of vision & text components.\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        text_outputs = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        pooled_output = text_outputs[1]\n+\n+        return pooled_output\n+\n+    @add_start_docstrings_to_model_forward(SIGLIP2_VISION_INPUTS_DOCSTRING)\n+    def get_image_features(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_attention_mask: Optional[torch.Tensor] = None,\n+        spatial_shapes: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> torch.FloatTensor:\n+        r\"\"\"\n+        Returns:\n+            image_features (`torch.FloatTensor` of shape `(batch_size, output_dim`): The image embeddings obtained by\n+            applying the projection layer to the pooled output of [`Siglip2VisionModel`].\n+\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, AutoModel\n+        >>> import torch\n+\n+        >>> model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-224\")\n+        >>> processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> inputs = processor(images=image, return_tensors=\"pt\")\n+\n+        >>> with torch.no_grad():\n+        ...     image_features = model.get_image_features(**inputs)\n+        ```\"\"\"\n+        # Use Siglip2Model's config for some fields (if specified) instead of those of vision & text components.\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            attention_mask=pixel_attention_mask,\n+            spatial_shapes=spatial_shapes,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        pooled_output = vision_outputs[1]\n+\n+        return pooled_output\n+\n+    @add_start_docstrings_to_model_forward(SIGLIP2_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=Siglip2Output, config_class=Siglip2Config)\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_attention_mask: Optional[torch.Tensor] = None,\n+        spatial_shapes: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        return_loss: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, Siglip2Output]:\n+        r\"\"\"\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from PIL import Image\n+        >>> import requests\n+        >>> from transformers import AutoProcessor, AutoModel\n+        >>> import torch\n+\n+        >>> model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-224\")\n+        >>> processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n+\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> texts = [\"a photo of 2 cats\", \"a photo of 2 dogs\"]\n+        >>> # important: we pass `padding=max_length` since the model was trained with this\n+        >>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\")\n+\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+\n+        >>> logits_per_image = outputs.logits_per_image\n+        >>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n+        >>> print(f\"{probs[0][0]:.1%} that image 0 is '{texts[0]}'\")\n+        31.9% that image 0 is 'a photo of 2 cats'\n+        ```\"\"\"\n+        # Use Siglip2 model's config for some fields (if specified) instead of those of vision & text components.\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            attention_mask=pixel_attention_mask,\n+            spatial_shapes=spatial_shapes,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        text_outputs = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        image_embeds = vision_outputs[1]\n+        text_embeds = text_outputs[1]\n+\n+        # normalized features\n+        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n+        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n+\n+        # cosine similarity as logits\n+        logits_per_text = torch.matmul(text_embeds, image_embeds.t().to(text_embeds.device))\n+\n+        logit_scale, logit_bias = self.logit_scale.to(text_embeds.device), self.logit_bias.to(text_embeds.device)\n+        logits_per_text = logits_per_text * logit_scale.exp() + logit_bias\n+\n+        logits_per_image = logits_per_text.t()\n+\n+        loss = None\n+        if return_loss:\n+            # Adapted from https://github.com/google-research/big_vision/blob/01edb81a4716f93a48be43b3a4af14e29cdb3a7f/big_vision/trainers/proj/image_text/siglip2.py#L287\n+            eye = torch.eye(logits_per_text.size(0), device=logits_per_text.device)\n+            m1_diag1 = -torch.ones_like(logits_per_text) + 2 * eye\n+            loglik = torch.nn.functional.logsigmoid(m1_diag1 * logits_per_text)\n+            nll = -torch.sum(loglik, dim=-1)\n+            loss = nll.mean()\n+\n+        if not return_dict:\n+            output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return Siglip2Output(\n+            loss=loss,\n+            logits_per_image=logits_per_image,\n+            logits_per_text=logits_per_text,\n+            text_embeds=text_embeds,\n+            image_embeds=image_embeds,\n+            text_model_output=text_outputs,\n+            vision_model_output=vision_outputs,\n+        )\n+\n+\n+@add_start_docstrings(\n+    \"\"\"\n+    Siglip2 vision encoder with an image classification head on top (a linear layer on top of the pooled final hidden states of\n+    the patch tokens) e.g. for ImageNet.\n+    \"\"\",\n+    SIGLIP2_START_DOCSTRING,\n+)\n+class Siglip2ForImageClassification(Siglip2PreTrainedModel):\n+    main_input_name = \"pixel_values\"\n+\n+    def __init__(self, config: Siglip2Config) -> None:\n+        super().__init__(config)\n+\n+        self.num_labels = config.num_labels\n+\n+        # Create the vision model with proper attention\n+        # and take only vision_model submodule (for backward compatibility)\n+        vision_model = Siglip2VisionModel._from_config(config.vision_config)\n+        self.vision_model = vision_model.vision_model\n+\n+        # Classifier head\n+        self.classifier = (\n+            nn.Linear(config.vision_config.hidden_size, config.num_labels) if config.num_labels > 0 else nn.Identity()\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(SIGLIP2_INPUTS_DOCSTRING)\n+    @replace_return_docstrings(output_type=ImageClassifierOutput, config_class=_CONFIG_FOR_DOC)\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_attention_mask: Optional[torch.Tensor] = None,\n+        spatial_shapes: Optional[torch.LongTensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[tuple, ImageClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+\n+        Returns:\n+\n+        Examples:\n+\n+        ```python\n+        >>> from transformers import AutoImageProcessor, Siglip2ForImageClassification\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> import requests\n+\n+        >>> torch.manual_seed(3)  # doctest: +IGNORE_RESULT\n+        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(url, stream=True).raw)\n+\n+        >>> # note: we are loading a `Siglip2Model` from the hub here,\n+        >>> # so the head will be randomly initialized, hence the predictions will be random if seed is not set above.\n+        >>> image_processor = AutoImageProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n+        >>> model = Siglip2ForImageClassification.from_pretrained(\"google/siglip2-base-patch16-224\")\n+\n+        >>> inputs = image_processor(images=image, return_tensors=\"pt\")\n+        >>> outputs = model(**inputs)\n+        >>> logits = outputs.logits\n+        >>> # model predicts one of the two classes\n+        >>> predicted_class_idx = logits.argmax(-1).item()\n+        >>> print(\"Predicted class:\", model.config.id2label[predicted_class_idx])\n+        Predicted class: LABEL_1\n+        ```\"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.vision_model(\n+            pixel_values,\n+            attention_mask=pixel_attention_mask,\n+            spatial_shapes=spatial_shapes,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        # average pool the patch tokens\n+        if pixel_attention_mask is not None:\n+            pool_mask = pixel_attention_mask[..., None].to(sequence_output.device)\n+            sequence_output = torch.sum(sequence_output * pool_mask, dim=1) / torch.sum(pool_mask, dim=1)\n+        else:\n+            sequence_output = torch.mean(sequence_output, dim=1)\n+\n+        # apply classifier\n+        logits = self.classifier(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(logits.device)\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[2:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return ImageClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"Siglip2Model\",\n+    \"Siglip2PreTrainedModel\",\n+    \"Siglip2TextModel\",\n+    \"Siglip2VisionModel\",\n+    \"Siglip2ForImageClassification\",\n+]"
        },
        {
            "sha": "6fac0030511f825ca32c1c8ea63c4661c99d7983",
            "filename": "src/transformers/models/siglip2/modular_siglip2.py",
            "status": "added",
            "additions": 537,
            "deletions": 0,
            "changes": 537,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fmodular_siglip2.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -0,0 +1,537 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from typing import Optional, Tuple, Union\n+\n+import torch\n+import torch.nn as nn\n+import torch.nn.functional as F\n+from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n+\n+from transformers.models.siglip.configuration_siglip import SiglipConfig, SiglipTextConfig, SiglipVisionConfig\n+from transformers.models.siglip.modeling_siglip import (\n+    BaseModelOutputWithPooling,\n+    ImageClassifierOutput,\n+    SiglipForImageClassification,\n+    SiglipModel,\n+    SiglipMultiheadAttentionPoolingHead,\n+    SiglipOutput,\n+    SiglipPreTrainedModel,\n+    SiglipTextModel,\n+    SiglipTextModelOutput,\n+    SiglipVisionModel,\n+    SiglipVisionModelOutput,\n+    SiglipVisionTransformer,\n+)\n+\n+from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+\n+\n+class Siglip2TextConfig(SiglipTextConfig):\n+    pass\n+\n+\n+class Siglip2VisionConfig(SiglipVisionConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Siglip2VisionModel`]. It is used to instantiate a\n+    Siglip2 vision encoder according to the specified arguments, defining the model architecture. Instantiating a\n+    configuration with the defaults will yield a similar configuration to that of the vision encoder of the Siglip2\n+    [google/siglip2-base-patch16-naflex](https://huggingface.co/google/siglip2-base-patch16-naflex) architecture.\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+    Args:\n+        hidden_size (`int`, *optional*, defaults to 768):\n+            Dimensionality of the encoder layers and the pooler layer.\n+        intermediate_size (`int`, *optional*, defaults to 3072):\n+            Dimensionality of the \"intermediate\" (i.e., feed-forward) layer in the Transformer encoder.\n+        num_hidden_layers (`int`, *optional*, defaults to 12):\n+            Number of hidden layers in the Transformer encoder.\n+        num_attention_heads (`int`, *optional*, defaults to 12):\n+            Number of attention heads for each attention layer in the Transformer encoder.\n+        num_channels (`int`, *optional*, defaults to 3):\n+            Number of channels in the input images.\n+        num_patches (`int`, *optional*, defaults to 256):\n+            The number of patches in the image with the size of (`patch_size`, `patch_size`).\n+            The image is resized to fill maximum of this number of patches, and to preserve\n+            the aspect ratio. In case the resulted number of patches is lower, the image is\n+            padded in \"patch\" dimension.\n+        patch_size (`int`, *optional*, defaults to 16):\n+            The size (resolution) of each patch.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"gelu_pytorch_tanh\"`):\n+            The non-linear activation function (function or string) in the encoder and pooler. If string, `\"gelu\"`,\n+            `\"relu\"`, `\"selu\"` and `\"gelu_new\"` `\"quick_gelu\"` are supported.\n+        layer_norm_eps (`float`, *optional*, defaults to 1e-06):\n+            The epsilon used by the layer normalization layers.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+\n+    Example:\n+\n+    ```python\n+    >>> from transformers import Siglip2VisionConfig, Siglip2VisionModel\n+\n+    >>> # Initializing a Siglip2VisionConfig with google/siglip2-base-patch16-naflex style configuration\n+    >>> configuration = Siglip2VisionConfig()\n+\n+    >>> # Initializing a Siglip2VisionModel (with random weights) from the google/siglip2-base-patch16-naflex style configuration\n+    >>> model = Siglip2VisionModel(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\"\"\"\n+\n+    def __init__(\n+        self,\n+        hidden_size=768,\n+        intermediate_size=3072,\n+        num_hidden_layers=12,\n+        num_attention_heads=12,\n+        num_channels=3,\n+        num_patches=256,\n+        patch_size=16,\n+        hidden_act=\"gelu_pytorch_tanh\",\n+        layer_norm_eps=1e-6,\n+        attention_dropout=0.0,\n+        **kwargs,\n+    ):\n+        super().__init__(**kwargs)\n+        self.num_patches = num_patches\n+        del self.image_size\n+\n+\n+class Siglip2Config(SiglipConfig):\n+    pass\n+\n+\n+class Siglip2VisionOutput(SiglipVisionModelOutput):\n+    pass\n+\n+\n+class Siglip2TextOutput(SiglipTextModelOutput):\n+    pass\n+\n+\n+class Siglip2Output(SiglipOutput):\n+    pass\n+\n+\n+class Siglip2VisionEmbeddings(nn.Module):\n+    def __init__(self, config: Siglip2VisionConfig):\n+        super().__init__()\n+        self.config = config\n+        self.embed_dim = config.hidden_size\n+        self.patch_size = config.patch_size\n+\n+        self.patch_embedding = nn.Linear(\n+            in_features=config.num_channels * self.patch_size * self.patch_size,\n+            out_features=self.embed_dim,\n+        )\n+\n+        self.num_patches = config.num_patches\n+        self.position_embedding_size = int(self.num_patches**0.5)\n+        self.position_embedding = nn.Embedding(self.num_patches, self.embed_dim)\n+\n+    @staticmethod\n+    def resize_positional_embeddings(\n+        positional_embeddings: torch.Tensor,\n+        spatial_shapes: torch.LongTensor,\n+        max_length: int,\n+    ) -> torch.Tensor:\n+        \"\"\"\n+        Resize positional embeddings to image-specific size and pad to a fixed size.\n+\n+        Args:\n+            positional_embeddings (`torch.Tensor`):\n+                Position embeddings of shape (height, width, embed_dim)\n+            spatial_shapes (`torch.LongTensor`):\n+                Spatial shapes of shape (batch_size, 2) to resize the positional embeddings to\n+            max_length (`int`):\n+                Maximum length of the positional embeddings to pad resized positional embeddings to\n+\n+        Returns:\n+            `torch.Tensor`: Embeddings of shape (batch_size, max_length, embed_dim)\n+        \"\"\"\n+        batch_size = spatial_shapes.shape[0]\n+        embed_dim = positional_embeddings.shape[-1]\n+        source_dtype = positional_embeddings.dtype\n+\n+        resulted_positional_embeddings = torch.empty(\n+            (batch_size, max_length, embed_dim),\n+            device=positional_embeddings.device,\n+            dtype=source_dtype,\n+        )\n+\n+        # (height, width, embed_dim) -> (1, embed_dim, height, width) for interpolation\n+        positional_embeddings = positional_embeddings.permute(2, 0, 1).unsqueeze(0)\n+\n+        # Upcast to float32 on CPU because antialias is not supported for bfloat16/float16 on CPU\n+        if positional_embeddings.device.type == \"cpu\":\n+            positional_embeddings = positional_embeddings.to(torch.float32)\n+\n+        for i in range(batch_size):\n+            # (1, dim, height, width) -> (1, dim, target_height, target_width)\n+            height, width = spatial_shapes[i]\n+            resized_embeddings = F.interpolate(\n+                positional_embeddings,\n+                size=(height, width),\n+                mode=\"bilinear\",\n+                align_corners=False,\n+                antialias=True,\n+            )\n+\n+            # (1, dim, target_height, target_width) -> (target_height * target_width, dim)\n+            resized_embeddings = resized_embeddings.reshape(embed_dim, height * width).transpose(0, 1)\n+\n+            # Cast to original dtype\n+            resized_embeddings = resized_embeddings.to(source_dtype)\n+\n+            resulted_positional_embeddings[i, : height * width] = resized_embeddings\n+            resulted_positional_embeddings[i, height * width :] = resized_embeddings[0]\n+\n+        return resulted_positional_embeddings\n+\n+    def forward(self, pixel_values: torch.FloatTensor, spatial_shapes: torch.LongTensor) -> torch.Tensor:\n+        \"\"\"\n+        Args:\n+            pixel_values (`torch.FloatTensor`):\n+                Pixel values of shape (batch_size, max_num_patches, num_channels * patch_size * patch_size)\n+            spatial_shapes (`List[Tuple[int, int]]`):\n+                Spatial shapes of shape (batch_size, 2) to resize the positional embeddings to\n+        \"\"\"\n+\n+        # Apply patch embeddings to already patchified pixel values\n+        target_dtype = self.patch_embedding.weight.dtype\n+        patch_embeds = self.patch_embedding(pixel_values.to(dtype=target_dtype))\n+\n+        # Get positional resized and padded positional embeddings\n+        positional_embeddings = self.position_embedding.weight.reshape(\n+            self.position_embedding_size, self.position_embedding_size, -1\n+        )\n+        resized_positional_embeddings = self.resize_positional_embeddings(\n+            positional_embeddings, spatial_shapes, max_length=pixel_values.shape[1]\n+        )\n+\n+        # Add positional embeddings to patch embeddings\n+        embeddings = patch_embeds + resized_positional_embeddings\n+        return embeddings\n+\n+\n+class Siglip2VisionTransformer(SiglipVisionTransformer):\n+    def __init__(self, config: Siglip2VisionConfig):\n+        super().__init__()\n+        self._use_flash_attention_2 = config._attn_implementation == \"flash_attention_2\"\n+\n+    # Update: add `spatial_shapes` and `attention_mask`\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        attention_mask: torch.Tensor,\n+        spatial_shapes: torch.LongTensor,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        r\"\"\"\n+        Returns:\n+\n+        \"\"\"\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        hidden_states = self.embeddings(pixel_values, spatial_shapes)\n+\n+        if attention_mask is not None and not self._use_flash_attention_2:\n+            # [batch_size, seq_len] -> [batch_size, 1, tgt_seq_len, src_seq_len]\n+            encoder_attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_states.dtype)\n+        else:\n+            encoder_attention_mask = attention_mask\n+\n+        encoder_outputs = self.encoder(\n+            inputs_embeds=hidden_states,\n+            attention_mask=encoder_attention_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        last_hidden_state = encoder_outputs[0]\n+        last_hidden_state = self.post_layernorm(last_hidden_state)\n+\n+        pooler_output = self.head(last_hidden_state, attention_mask) if self.use_head else None\n+        if not return_dict:\n+            return (last_hidden_state, pooler_output) + encoder_outputs[1:]\n+\n+        return BaseModelOutputWithPooling(\n+            last_hidden_state=last_hidden_state,\n+            pooler_output=pooler_output,\n+            hidden_states=encoder_outputs.hidden_states,\n+            attentions=encoder_outputs.attentions,\n+        )\n+\n+\n+class Siglip2PreTrainedModel(SiglipPreTrainedModel):\n+    pass\n+\n+\n+class Siglip2TextModel(SiglipTextModel):\n+    pass\n+\n+\n+class Siglip2MultiheadAttentionPoolingHead(SiglipMultiheadAttentionPoolingHead):\n+    def __init__(self, config: Siglip2VisionConfig):\n+        super().__init__(config)\n+        self.num_heads = config.num_attention_heads\n+\n+    def forward(self, hidden_state: torch.Tensor, attention_mask: Optional[torch.Tensor] = None) -> torch.Tensor:\n+        batch_size = hidden_state.shape[0]\n+        probe = self.probe.repeat(batch_size, 1, 1)\n+\n+        if attention_mask is not None:\n+            target_len, source_len = probe.shape[1], hidden_state.shape[1]\n+            attention_mask = _prepare_4d_attention_mask(attention_mask, hidden_state.dtype, target_len)\n+            attention_mask = attention_mask.repeat(1, self.num_heads, target_len, 1)\n+            attention_mask = attention_mask.reshape(-1, target_len, source_len)\n+\n+        hidden_state = self.attention(probe, hidden_state, hidden_state, attn_mask=attention_mask)[0]\n+\n+        residual = hidden_state\n+        hidden_state = self.layernorm(hidden_state)\n+        hidden_state = residual + self.mlp(hidden_state)\n+\n+        return hidden_state[:, 0]\n+\n+\n+class Siglip2VisionModel(SiglipVisionModel):\n+    # Update: add `spatial_shapes` and `pixel_attention_mask`\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        pixel_attention_mask: torch.Tensor,\n+        spatial_shapes: torch.LongTensor,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, BaseModelOutputWithPooling]:\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        return self.vision_model(\n+            pixel_values=pixel_values,\n+            attention_mask=pixel_attention_mask,\n+            spatial_shapes=spatial_shapes,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+\n+class Siglip2Model(SiglipModel):\n+    # Update: add `spatial_shapes` and `pixel_attention_mask`\n+    def get_image_features(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_attention_mask: Optional[torch.Tensor] = None,\n+        spatial_shapes: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> torch.FloatTensor:\n+        # Use Siglip2Model's config for some fields (if specified) instead of those of vision & text components.\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            attention_mask=pixel_attention_mask,\n+            spatial_shapes=spatial_shapes,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        pooled_output = vision_outputs[1]\n+\n+        return pooled_output\n+\n+    # Update: add `spatial_shapes` and `pixel_attention_mask`\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        pixel_attention_mask: Optional[torch.Tensor] = None,\n+        spatial_shapes: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        return_loss: Optional[bool] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, Siglip2Output]:\n+        # Use Siglip2 model's config for some fields (if specified) instead of those of vision & text components.\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        vision_outputs = self.vision_model(\n+            pixel_values=pixel_values,\n+            attention_mask=pixel_attention_mask,\n+            spatial_shapes=spatial_shapes,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        text_outputs = self.text_model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        image_embeds = vision_outputs[1]\n+        text_embeds = text_outputs[1]\n+\n+        # normalized features\n+        image_embeds = image_embeds / image_embeds.norm(p=2, dim=-1, keepdim=True)\n+        text_embeds = text_embeds / text_embeds.norm(p=2, dim=-1, keepdim=True)\n+\n+        # cosine similarity as logits\n+        logits_per_text = torch.matmul(text_embeds, image_embeds.t().to(text_embeds.device))\n+\n+        logit_scale, logit_bias = self.logit_scale.to(text_embeds.device), self.logit_bias.to(text_embeds.device)\n+        logits_per_text = logits_per_text * logit_scale.exp() + logit_bias\n+\n+        logits_per_image = logits_per_text.t()\n+\n+        loss = None\n+        if return_loss:\n+            # Adapted from https://github.com/google-research/big_vision/blob/01edb81a4716f93a48be43b3a4af14e29cdb3a7f/big_vision/trainers/proj/image_text/siglip2.py#L287\n+            eye = torch.eye(logits_per_text.size(0), device=logits_per_text.device)\n+            m1_diag1 = -torch.ones_like(logits_per_text) + 2 * eye\n+            loglik = torch.nn.functional.logsigmoid(m1_diag1 * logits_per_text)\n+            nll = -torch.sum(loglik, dim=-1)\n+            loss = nll.mean()\n+\n+        if not return_dict:\n+            output = (logits_per_image, logits_per_text, text_embeds, image_embeds, text_outputs, vision_outputs)\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return Siglip2Output(\n+            loss=loss,\n+            logits_per_image=logits_per_image,\n+            logits_per_text=logits_per_text,\n+            text_embeds=text_embeds,\n+            image_embeds=image_embeds,\n+            text_model_output=text_outputs,\n+            vision_model_output=vision_outputs,\n+        )\n+\n+\n+class Siglip2ForImageClassification(SiglipForImageClassification):\n+    # Update: add `spatial_shapes` and `pixel_attention_mask`\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.Tensor] = None,\n+        pixel_attention_mask: Optional[torch.Tensor] = None,\n+        spatial_shapes: Optional[torch.LongTensor] = None,\n+        labels: Optional[torch.Tensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[tuple, ImageClassifierOutput]:\n+        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n+        output_hidden_states = (\n+            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n+        )\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.vision_model(\n+            pixel_values,\n+            attention_mask=pixel_attention_mask,\n+            spatial_shapes=spatial_shapes,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            return_dict=return_dict,\n+        )\n+\n+        sequence_output = outputs[0]\n+\n+        # average pool the patch tokens\n+        if pixel_attention_mask is not None:\n+            pool_mask = pixel_attention_mask[..., None].to(sequence_output.device)\n+            sequence_output = torch.sum(sequence_output * pool_mask, dim=1) / torch.sum(pool_mask, dim=1)\n+        else:\n+            sequence_output = torch.mean(sequence_output, dim=1)\n+\n+        # apply classifier\n+        logits = self.classifier(sequence_output)\n+\n+        loss = None\n+        if labels is not None:\n+            # move labels to correct device to enable model parallelism\n+            labels = labels.to(logits.device)\n+            if self.config.problem_type is None:\n+                if self.num_labels == 1:\n+                    self.config.problem_type = \"regression\"\n+                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n+                    self.config.problem_type = \"single_label_classification\"\n+                else:\n+                    self.config.problem_type = \"multi_label_classification\"\n+\n+            if self.config.problem_type == \"regression\":\n+                loss_fct = MSELoss()\n+                if self.num_labels == 1:\n+                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n+                else:\n+                    loss = loss_fct(logits, labels)\n+            elif self.config.problem_type == \"single_label_classification\":\n+                loss_fct = CrossEntropyLoss()\n+                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n+            elif self.config.problem_type == \"multi_label_classification\":\n+                loss_fct = BCEWithLogitsLoss()\n+                loss = loss_fct(logits, labels)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[2:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return ImageClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\n+    \"Siglip2Config\",\n+    \"Siglip2TextConfig\",\n+    \"Siglip2VisionConfig\",\n+    \"Siglip2Model\",\n+    \"Siglip2PreTrainedModel\",\n+    \"Siglip2TextModel\",\n+    \"Siglip2VisionModel\",\n+    \"Siglip2ForImageClassification\",\n+]"
        },
        {
            "sha": "4f4ec33f2f196201e7582ceda12efa6ee2872a33",
            "filename": "src/transformers/models/siglip2/processing_siglip2.py",
            "status": "added",
            "additions": 171,
            "deletions": 0,
            "changes": 171,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -0,0 +1,171 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"\n+Image/Text processor class for SigLIP2.\n+\"\"\"\n+\n+from typing import List, Optional, Union\n+\n+from ...feature_extraction_utils import BatchFeature\n+from ...image_utils import ImageInput\n+from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n+from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+\n+\n+class Siglip2ImagesKwargs(ImagesKwargs, total=False):\n+    max_num_patches: Optional[int]\n+    patch_size: Optional[int]\n+\n+\n+class Siglip2ProcessorKwargs(ProcessingKwargs, total=False):\n+    images_kwargs: Siglip2ImagesKwargs\n+\n+    _defaults = {\n+        \"text_kwargs\": {\n+            \"padding\": \"max_length\",\n+            \"truncation\": True,\n+            \"max_length\": 64,\n+        },\n+        \"images_kwargs\": {\n+            \"max_num_patches\": 256,\n+            \"patch_size\": 16,\n+        },\n+    }\n+\n+\n+class Siglip2Processor(ProcessorMixin):\n+    r\"\"\"\n+    Constructs a Siglip2 processor which wraps a Siglip2 image processor and a Gemma tokenizer into a single processor.\n+\n+    [`Siglip2Processor`] offers all the functionalities of [`Siglip2ImageProcessor`] and [`GemmaTokenizerFast`]. See the\n+    [`~Siglip2Processor.__call__`] and [`~Siglip2Processor.decode`] for more information.\n+\n+    Args:\n+        image_processor ([`Siglip2ImageProcessor`]):\n+            The image processor is a required input.\n+        tokenizer ([`GemmaTokenizerFast`]):\n+            The tokenizer is a required input.\n+    \"\"\"\n+\n+    attributes = [\"image_processor\", \"tokenizer\"]\n+\n+    image_processor_class = \"AutoImageProcessor\"\n+    tokenizer_class = \"AutoTokenizer\"\n+\n+    def __init__(self, image_processor, tokenizer):\n+        super().__init__(image_processor, tokenizer)\n+\n+    def __call__(\n+        self,\n+        images: Optional[Union[ImageInput, List[ImageInput], List[List[ImageInput]]]] = None,\n+        text: Optional[Union[TextInput, \"PreTokenizedInput\", List[TextInput], List[\"PreTokenizedInput\"]]] = None,\n+        audio=None,\n+        videos=None,\n+        **kwargs: Unpack[Siglip2ProcessorKwargs],\n+    ) -> BatchFeature:\n+        \"\"\"\n+        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n+        and `kwargs` arguments to GemmaTokenizerFast's [`~GemmaTokenizerFast.__call__`] if `text` is not `None` to encode\n+        the text. To prepare the image(s), this method forwards the `images` argument to\n+        Siglip2ImageProcessor's [`~Siglip2ImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n+        of the above two methods for more information.\n+\n+        Args:\n+            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n+                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n+                tensor. Both channels-first and channels-last formats are supported.\n+            text (`str`, `List[str]`, `List[List[str]]`):\n+                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n+                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `max_length`):\n+                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n+                index) among:\n+                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n+                  acceptable input length for the model if that argument is not provided.\n+                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n+                  sequence if provided).\n+                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n+                  lengths).\n+            max_length (`int`, *optional*, defaults to 64):\n+                Maximum length of the returned list and optionally padding length (see above).\n+            truncation (`bool`, *optional*, defaults to `True`):\n+                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n+            return_tensors (`str` or [`~utils.TensorType`], *optional*, defaults to `'pt'`):\n+                If set, will return tensors of a particular framework. Acceptable values are:\n+\n+                - `'tf'`: Return TensorFlow `tf.constant` objects.\n+                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+                - `'np'`: Return NumPy `np.ndarray` objects.\n+                - `'jax'`: Return JAX `jnp.ndarray` objects.\n+\n+        Returns:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n+\n+            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n+            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n+              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+              `None`).\n+            - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n+            - **pixel_attention_mask** -- Attention mask for the pixel values. Returned when `images` is not `None`.\n+            - **spatial_shapes** -- The number of horizontal and vertical patches per image.\n+              Returned when `images` is not `None`.\n+        \"\"\"\n+        output_kwargs = self._merge_kwargs(\n+            Siglip2ProcessorKwargs,\n+            tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n+            **kwargs,\n+        )\n+\n+        if text is None and images is None:\n+            raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n+\n+        if text is not None:\n+            encoding = self.tokenizer(text, **output_kwargs[\"text_kwargs\"])\n+\n+        if images is not None:\n+            image_features = self.image_processor(images, **output_kwargs[\"images_kwargs\"])\n+\n+        if text is not None and images is not None:\n+            encoding.update(image_features)\n+            return encoding\n+        elif text is not None:\n+            return encoding\n+        else:\n+            return_tensors = output_kwargs[\"common_kwargs\"][\"return_tensors\"]\n+            return BatchFeature(data=dict(**image_features), tensor_type=return_tensors)\n+\n+    def decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to Siglip2Tokenizer's [`~PreTrainedTokenizer.decode`]. Please refer to\n+        the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.decode(*args, **kwargs)\n+\n+    def batch_decode(self, *args, **kwargs):\n+        \"\"\"\n+        This method forwards all its arguments to Siglip2Tokenizer's [`~PreTrainedTokenizer.batch_decode`]. Please\n+        refer to the docstring of this method for more information.\n+        \"\"\"\n+        return self.tokenizer.batch_decode(*args, **kwargs)\n+\n+    @property\n+    def model_input_names(self):\n+        tokenizer_input_names = self.tokenizer.model_input_names\n+        image_processor_input_names = self.image_processor.model_input_names\n+        return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n+\n+\n+__all__ = [\"Siglip2Processor\"]"
        },
        {
            "sha": "7e49ba0efaafc3cc89200651e0820e4fa8ef7701",
            "filename": "src/transformers/pipelines/zero_shot_image_classification.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fzero_shot_image_classification.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -145,8 +145,11 @@ def preprocess(\n             inputs = inputs.to(self.torch_dtype)\n         inputs[\"candidate_labels\"] = candidate_labels\n         sequences = [hypothesis_template.format(x) for x in candidate_labels]\n-        padding = \"max_length\" if self.model.config.model_type == \"siglip\" else True\n-        text_inputs = self.tokenizer(sequences, return_tensors=self.framework, padding=padding, **tokenizer_kwargs)\n+        tokenizer_default_kwargs = {\"padding\": True}\n+        if \"siglip\" in self.model.config.model_type:\n+            tokenizer_default_kwargs.update(padding=\"max_length\", max_length=64, truncation=True)\n+        tokenizer_default_kwargs.update(tokenizer_kwargs)\n+        text_inputs = self.tokenizer(sequences, return_tensors=self.framework, **tokenizer_default_kwargs)\n         inputs[\"text_inputs\"] = [text_inputs]\n         return inputs\n \n@@ -170,7 +173,7 @@ def _forward(self, model_inputs):\n     def postprocess(self, model_outputs):\n         candidate_labels = model_outputs.pop(\"candidate_labels\")\n         logits = model_outputs[\"logits\"][0]\n-        if self.framework == \"pt\" and self.model.config.model_type == \"siglip\":\n+        if self.framework == \"pt\" and \"siglip\" in self.model.config.model_type:\n             probs = torch.sigmoid(logits).squeeze(-1)\n             scores = probs.tolist()\n             if not isinstance(scores, list):"
        },
        {
            "sha": "d409238588d01d427fc3b0a2f65b6b034de3577f",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 35,
            "deletions": 0,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -8849,6 +8849,41 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class Siglip2ForImageClassification(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Siglip2Model(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Siglip2PreTrainedModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Siglip2TextModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n+class Siglip2VisionModel(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class SmolVLMForConditionalGeneration(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "f393a8f1265df4b8fa2768b195dfb583519b4139",
            "filename": "src/transformers/utils/dummy_torchvision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_torchvision_objects.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -107,6 +107,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torchvision\"])\n \n \n+class Siglip2ImageProcessorFast(metaclass=DummyObject):\n+    _backends = [\"torchvision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torchvision\"])\n+\n+\n class ViTImageProcessorFast(metaclass=DummyObject):\n     _backends = [\"torchvision\"]\n "
        },
        {
            "sha": "92906e005f90f6c40818c206045b516c9acf8295",
            "filename": "src/transformers/utils/dummy_vision_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_vision_objects.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -639,6 +639,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"vision\"])\n \n \n+class Siglip2ImageProcessor(metaclass=DummyObject):\n+    _backends = [\"vision\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"vision\"])\n+\n+\n class SmolVLMImageProcessor(metaclass=DummyObject):\n     _backends = [\"vision\"]\n "
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/siglip2/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/tests%2Fmodels%2Fsiglip2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/tests%2Fmodels%2Fsiglip2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2F__init__.py?ref=a957b7911a758d54597914b4479fe6e81424d64f"
        },
        {
            "sha": "dd96db9c5671d43321cfe7265972ddd4ed228a25",
            "filename": "tests/models/siglip2/test_image_processing_siglip2.py",
            "status": "added",
            "additions": 200,
            "deletions": 0,
            "changes": 200,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/tests%2Fmodels%2Fsiglip2%2Ftest_image_processing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/tests%2Fmodels%2Fsiglip2%2Ftest_image_processing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_image_processing_siglip2.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -0,0 +1,200 @@\n+# coding=utf-8\n+# Copyright 2024 HuggingFace Inc.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import unittest\n+\n+import requests\n+\n+from transformers.testing_utils import require_torch, require_vision\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n+\n+from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_vision_available():\n+    from PIL import Image\n+\n+    from transformers import Siglip2ImageProcessor\n+\n+\n+if is_torchvision_available():\n+    from transformers import Siglip2ImageProcessorFast\n+\n+\n+class Siglip2ImageProcessingTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=7,\n+        num_channels=3,\n+        image_size=18,\n+        min_resolution=30,\n+        max_resolution=400,\n+        do_resize=True,\n+        size=None,\n+        do_rescale=True,\n+        rescale_factor=1 / 255,\n+        do_normalize=True,\n+        image_mean=[0.5, 0.5, 0.5],\n+        image_std=[0.5, 0.5, 0.5],\n+        resample=None,\n+        patch_size=16,\n+        max_num_patches=256,\n+    ):\n+        size = size if size is not None else {\"height\": 18, \"width\": 18}\n+        resample = resample if resample is not None else Image.Resampling.BILINEAR\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_channels = num_channels\n+        self.image_size = image_size\n+        self.min_resolution = min_resolution\n+        self.max_resolution = max_resolution\n+        self.do_resize = do_resize\n+        self.size = size\n+        self.do_rescale = do_rescale\n+        self.rescale_factor = rescale_factor\n+        self.do_normalize = do_normalize\n+        self.image_mean = image_mean\n+        self.image_std = image_std\n+        self.resample = resample\n+        self.patch_size = patch_size\n+        self.max_num_patches = max_num_patches\n+\n+    def prepare_image_processor_dict(self):\n+        return {\n+            \"do_resize\": self.do_resize,\n+            \"do_rescale\": self.do_rescale,\n+            \"rescale_factor\": self.rescale_factor,\n+            \"do_normalize\": self.do_normalize,\n+            \"image_mean\": self.image_mean,\n+            \"image_std\": self.image_std,\n+            \"resample\": self.resample,\n+            \"patch_size\": self.patch_size,\n+            \"max_num_patches\": self.max_num_patches,\n+        }\n+\n+    def expected_output_image_shape(self, images):\n+        return self.max_num_patches, self.patch_size * self.patch_size * self.num_channels\n+\n+    def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n+        return prepare_image_inputs(\n+            batch_size=self.batch_size,\n+            num_channels=self.num_channels,\n+            min_resolution=self.min_resolution,\n+            max_resolution=self.max_resolution,\n+            equal_resolution=equal_resolution,\n+            numpify=numpify,\n+            torchify=torchify,\n+        )\n+\n+\n+@require_torch\n+@require_vision\n+# Copied from tests.models.clip.test_image_processing_clip.CLIPImageProcessingTest with CLIP->Siglip2\n+class Siglip2ImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n+    image_processing_class = Siglip2ImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = Siglip2ImageProcessorFast if is_torchvision_available() else None\n+\n+    def setUp(self):\n+        super().setUp()\n+        self.image_processor_tester = Siglip2ImageProcessingTester(self)\n+\n+    @property\n+    def image_processor_dict(self):\n+        return self.image_processor_tester.prepare_image_processor_dict()\n+\n+    # Ignore copy\n+    def test_image_processor_properties(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"resample\"))\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_normalize\"))\n+            self.assertTrue(hasattr(image_processing, \"image_mean\"))\n+            self.assertTrue(hasattr(image_processing, \"image_std\"))\n+            self.assertTrue(hasattr(image_processing, \"patch_size\"))\n+            self.assertTrue(hasattr(image_processing, \"max_num_patches\"))\n+\n+    # Ignore copy\n+    def test_image_processor_from_dict_with_kwargs(self):\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.max_num_patches, 256)\n+            self.assertEqual(image_processor.patch_size, 16)\n+\n+            image_processor = self.image_processing_class.from_dict(\n+                self.image_processor_dict, patch_size=32, max_num_patches=512\n+            )\n+            self.assertEqual(image_processor.patch_size, 32)\n+            self.assertEqual(image_processor.max_num_patches, 512)\n+\n+    @unittest.skip(reason=\"not supported\")\n+    # Ignore copy\n+    def test_call_numpy_4_channels(self):\n+        pass\n+\n+    # increase mean tolerance to 1e-3 -> 2e-3\n+    # Ignore copy\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+        torch.testing.assert_close(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1, rtol=1e-1)\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 2e-3\n+        )\n+\n+    # increase mean tolerance to 1e-3 -> 2e-3\n+    # Ignore copy\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n+\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        torch.testing.assert_close(encoding_slow.pixel_values, encoding_fast.pixel_values, atol=1e-1, rtol=1e-1)\n+        self.assertLessEqual(\n+            torch.mean(torch.abs(encoding_slow.pixel_values - encoding_fast.pixel_values)).item(), 2e-3\n+        )"
        },
        {
            "sha": "dea49ececa9b54ac2ff6488a8ed828d3c21fa852",
            "filename": "tests/models/siglip2/test_modeling_siglip2.py",
            "status": "added",
            "additions": 989,
            "deletions": 0,
            "changes": 989,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -0,0 +1,989 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Siglip2 model.\"\"\"\n+\n+import inspect\n+import tempfile\n+import unittest\n+from typing import Tuple\n+\n+import numpy as np\n+from parameterized import parameterized\n+from pytest import mark\n+\n+from transformers import Siglip2Config, Siglip2TextConfig, Siglip2VisionConfig\n+from transformers.testing_utils import (\n+    require_flash_attn,\n+    require_torch,\n+    require_torch_gpu,\n+    require_torch_sdpa,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n+from transformers.utils import (\n+    is_torch_available,\n+    is_torch_bf16_available_on_device,\n+    is_torch_fp16_available_on_device,\n+    is_torch_sdpa_available,\n+    is_vision_available,\n+)\n+\n+from ...test_configuration_common import ConfigTester\n+from ...test_modeling_common import (\n+    ModelTesterMixin,\n+    floats_tensor,\n+    ids_tensor,\n+    is_flaky,\n+    random_attention_mask,\n+)\n+from ...test_pipeline_mixin import PipelineTesterMixin\n+\n+\n+if is_torch_available():\n+    import torch\n+    from torch import nn\n+\n+    from transformers import Siglip2ForImageClassification, Siglip2Model, Siglip2TextModel, Siglip2VisionModel\n+\n+if is_torch_sdpa_available():\n+    from torch.nn.attention import SDPBackend, sdpa_kernel\n+\n+if is_vision_available():\n+    from PIL import Image, ImageDraw\n+\n+    from transformers import Siglip2Processor\n+\n+\n+class Siglip2ModelTesterMixin(ModelTesterMixin):\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # Load the model with SDPA\n+                model_sdpa = model_class.from_pretrained(tmpdirname)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                # Load model with eager attention\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    attn_implementation=\"eager\",\n+                )\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+            # SigLip has one shared cls attr for all models, so we assign both submodels heer\n+            vision_attn = text_attn = \"sdpa\" if model._supports_sdpa else \"eager\"\n+\n+            if hasattr(model_sdpa, \"vision_model\") and hasattr(model_sdpa, \"text_model\"):\n+                self.assertTrue(model_sdpa.vision_model.config._attn_implementation == vision_attn)\n+                self.assertTrue(model_sdpa.text_model.config._attn_implementation == text_attn)\n+                self.assertTrue(model_eager.vision_model.config._attn_implementation == \"eager\")\n+                self.assertTrue(model_eager.text_model.config._attn_implementation == \"eager\")\n+\n+            self.assertTrue(model_sdpa.config._attn_implementation == \"sdpa\")\n+            self.assertTrue(model_eager.config._attn_implementation == \"eager\")\n+\n+            for name, submodule in model_eager.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    raise ValueError(\"The eager model should not have SDPA attention layers\")\n+\n+            has_sdpa = False\n+            for name, submodule in model_sdpa.named_modules():\n+                class_name = submodule.__class__.__name__\n+                if \"SdpaAttention\" in class_name or \"SdpaSelfAttention\" in class_name:\n+                    has_sdpa = True\n+                    break\n+            if not has_sdpa and model_sdpa.config.model_type != \"falcon\":\n+                raise ValueError(\"The SDPA model should have SDPA attention layers\")\n+\n+    def test_eager_matches_sdpa_inference(\n+        self,\n+        torch_dtype: str,\n+        use_attention_mask_options: Tuple[bool, ...] = (True, False),\n+        logit_keys: Tuple[str, ...] = (\"logits_per_image\", \"logits_per_text\", \"image_embeds\", \"text_embeds\"),\n+    ):\n+        if not self.all_model_classes[0]._supports_sdpa:\n+            self.skipTest(f\"{self.all_model_classes[0].__name__} does not support SDPA\")\n+\n+        if torch_dtype == \"float16\" and not is_torch_fp16_available_on_device(torch_device):\n+            self.skipTest(f\"float16 not supported on {torch_device} (on the specific device currently used)\")\n+\n+        if torch_dtype == \"bfloat16\" and not is_torch_bf16_available_on_device(torch_device):\n+            self.skipTest(\n+                f\"bfloat16 not supported on {torch_device} (on the specific device currently used, e.g. Nvidia T4 GPU)\"\n+            )\n+\n+        # Convert to torch dtype\n+        dtypes = {\n+            \"float16\": torch.float16,\n+            \"bfloat16\": torch.bfloat16,\n+            \"float32\": torch.float32,\n+        }\n+        torch_dtype = dtypes[torch_dtype]\n+\n+        atols = {\n+            torch.float32: 1e-5,\n+            torch.bfloat16: 3e-2,\n+            torch.float16: 5e-3,\n+        }\n+        rtols = {\n+            torch.float32: 1e-4,\n+            torch.bfloat16: 3e-2,\n+            torch.float16: 5e-3,\n+        }\n+\n+        atol = atols[torch_dtype]\n+        rtol = rtols[torch_dtype]\n+\n+        def get_mean_reldiff(msg, current_case, x, ref, atol, rtol):\n+            return f\"{msg} {current_case}: mean relative difference: {((x - ref).abs() / (ref.abs() + 1e-12)).mean():.3e}, torch atol = {atol}, torch rtol = {rtol}\"\n+\n+        for model_class in self.all_model_classes:\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            model = model_class(config)\n+\n+            with tempfile.TemporaryDirectory() as tmpdirname:\n+                model.save_pretrained(tmpdirname)\n+\n+                # Load the model with SDPA\n+                model_sdpa = model_class.from_pretrained(tmpdirname, torch_dtype=torch_dtype)\n+                model_sdpa = model_sdpa.eval().to(torch_device)\n+\n+                # Load model with eager attention\n+                model_eager = model_class.from_pretrained(\n+                    tmpdirname,\n+                    torch_dtype=torch_dtype,\n+                    attn_implementation=\"eager\",\n+                )\n+                model_eager = model_eager.eval().to(torch_device)\n+\n+            # We use these for loops instead of parameterized.expand just for the interest of avoiding loading/saving the model each time,\n+            # but it would be nicer to have an efficient way to use parameterized.expand\n+            cases = [\n+                (use_mask, output_attentions, sdpa_backend, batch_size)\n+                for use_mask in use_attention_mask_options\n+                for output_attentions in [True, False]\n+                for sdpa_backend in [\n+                    SDPBackend.MATH,\n+                    [SDPBackend.FLASH_ATTENTION, SDPBackend.MATH],\n+                    [SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH],\n+                    [SDPBackend.FLASH_ATTENTION, SDPBackend.EFFICIENT_ATTENTION, SDPBackend.MATH],\n+                ]\n+                for batch_size in [1, 5]\n+            ]\n+            fail_cases = []\n+\n+            for use_mask, output_attentions, sdpa_backend, batch_size in cases:\n+                processed_inputs = inputs_dict.copy()\n+\n+                # convert to torch_dtype\n+                if \"pixel_values\" in processed_inputs:\n+                    processed_inputs[\"pixel_values\"] = processed_inputs[\"pixel_values\"].to(torch_dtype)\n+\n+                # slice for different batch sizes\n+                for key in processed_inputs.keys():\n+                    if isinstance(processed_inputs[key], (torch.Tensor, list, tuple)):\n+                        processed_inputs[key] = processed_inputs[key][:batch_size]\n+\n+                # set attention mask with left padding\n+                if not use_mask:\n+                    processed_inputs.pop(\"attention_mask\", None)\n+                else:\n+                    dummy_attention_mask = processed_inputs[\"attention_mask\"]\n+                    dummy_attention_mask[:] = 1\n+                    dummy_attention_mask[:, :1] = 0\n+                    processed_inputs[\"attention_mask\"] = dummy_attention_mask\n+\n+                processed_inputs[\"output_attentions\"] = output_attentions\n+                processed_inputs[\"output_hidden_states\"] = True\n+\n+                current_case = (\n+                    f\"padding_side=left, use_mask={use_mask}, batch_size={batch_size}, sdpa_backend={sdpa_backend}\"\n+                )\n+\n+                prepared_inputs = self._prepare_for_class(processed_inputs, model_class)\n+\n+                with torch.no_grad():\n+                    try:\n+                        with sdpa_kernel(sdpa_backend):\n+                            outputs_eager = model_eager(**prepared_inputs)\n+                            outputs_sdpa = model_sdpa(**prepared_inputs)\n+                    except Exception as e:\n+                        fail_cases.append(f\"{current_case}: {e}\")\n+                        continue\n+\n+                for key in logit_keys:\n+                    eager_logits = outputs_eager[key]\n+                    sdpa_logits = outputs_sdpa[key]\n+\n+                    if use_mask:\n+                        eager_logits = eager_logits[:, 1:]\n+                        sdpa_logits = sdpa_logits[:, 1:]\n+\n+                    is_close = torch.allclose(eager_logits, sdpa_logits, atol=atol, rtol=rtol)\n+                    if not is_close:\n+                        fail_cases.append(get_mean_reldiff(key, current_case, sdpa_logits, eager_logits, atol, rtol))\n+\n+            self.assertTrue(len(fail_cases) == 0, \"\\n\".join(fail_cases))\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    @slow\n+    def test_flash_attn_2_inference_equivalence(self):\n+        dtype = torch.float16\n+\n+        for model_class in self.all_model_classes:\n+            if not model_class._supports_flash_attn_2:\n+                self.skipTest(f\"{model_class.__name__} does not support Flash Attention 2\")\n+\n+            # Prepare inputs\n+            config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n+            if \"pixel_values\" in inputs_dict:\n+                inputs_dict[\"pixel_values\"] = inputs_dict[\"pixel_values\"].to(dtype)\n+\n+            # Separate masks\n+            attention_masks = {}\n+            if \"attention_mask\" in inputs_dict:\n+                # attention_masks[\"attention_mask\"] = inputs_dict.pop(\"attention_mask\")\n+                inputs_dict[\"attention_mask\"] = None\n+            if \"pixel_attention_mask\" in inputs_dict:\n+                attention_masks[\"pixel_attention_mask\"] = inputs_dict.pop(\"pixel_attention_mask\")\n+                inputs_dict[\"pixel_attention_mask\"] = None\n+\n+            # Save and load model with flash attention 2 and eager attentions\n+            with tempfile.TemporaryDirectory() as tmp_dir:\n+                model = model_class(config)\n+                model.save_pretrained(tmp_dir)\n+\n+                model = model_class.from_pretrained(tmp_dir, torch_dtype=dtype)\n+                model_fa = model_class.from_pretrained(\n+                    tmp_dir, torch_dtype=dtype, attn_implementation=\"flash_attention_2\"\n+                )\n+\n+            model_fa.to(torch_device)\n+            model.to(torch_device)\n+\n+            # Run forward pass without attention masks\n+            with torch.no_grad():\n+                outputs = model(**inputs_dict, output_hidden_states=True)\n+                outputs_fa = model_fa(**inputs_dict, output_hidden_states=True)\n+\n+            # Choose which key to compare\n+            key = [k for k in [\"logits\", \"logits_per_image\", \"last_hidden_state\"] if k in outputs][0]\n+\n+            torch.testing.assert_close(outputs[key], outputs_fa[key], atol=4e-2, rtol=4e-2)\n+\n+            # Run forward pass with attention masks\n+            inputs_dict.update(attention_masks)\n+            with torch.no_grad():\n+                outputs = model(**inputs_dict, output_hidden_states=True)\n+                outputs_fa = model_fa(**inputs_dict, output_hidden_states=True)\n+\n+            output_tensor = outputs[key]\n+            output_tensor_fa = outputs_fa[key]\n+\n+            # Mask out padded tokens, they are different for SDPA and Flash Attention 2\n+            if key == \"last_hidden_state\" and \"pixel_attention_mask\" in inputs_dict:\n+                output_tensor = output_tensor * inputs_dict[\"pixel_attention_mask\"][..., None]\n+                output_tensor_fa = output_tensor_fa * inputs_dict[\"pixel_attention_mask\"][..., None]\n+            elif key == \"last_hidden_state\" and inputs_dict.get(\"attention_mask\", None) is not None:\n+                output_tensor = output_tensor * inputs_dict[\"attention_mask\"][..., None]\n+                output_tensor_fa = output_tensor_fa * inputs_dict[\"attention_mask\"][..., None]\n+\n+            torch.testing.assert_close(output_tensor, output_tensor_fa, atol=4e-2, rtol=4e-2)\n+\n+            # Check with inference + dropout\n+            model.train()\n+            _ = model_fa(**inputs_dict, output_hidden_states=True)\n+\n+    @unittest.skip(reason=\"Siglip2 has default right padding (tested in test_flash_attn_2_inference_equivalence)\")\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        pass\n+\n+    @unittest.skip(reason=\"SDPA can't dispatch on flash with not None `attention_mask`\")\n+    def test_sdpa_can_dispatch_on_flash(self):\n+        pass\n+\n+\n+class Siglip2VisionModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=12,\n+        num_patches=16,\n+        image_num_patches=24,\n+        patch_size=2,\n+        num_channels=3,\n+        is_training=True,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        dropout=0.1,\n+        attention_dropout=0.1,\n+        initializer_range=0.02,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.num_patches = num_patches\n+        self.patch_size = patch_size\n+        self.num_channels = num_channels\n+        self.is_training = is_training\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.initializer_range = initializer_range\n+        self.scope = scope\n+        self.seq_length = image_num_patches\n+\n+    def prepare_config_and_inputs(self):\n+        pixel_values = floats_tensor(\n+            [self.batch_size, self.seq_length, self.num_channels * self.patch_size * self.patch_size]\n+        )\n+        pixel_attention_mask = torch.zeros(self.batch_size, self.seq_length, device=torch_device, dtype=torch.long)\n+\n+        spatial_shapes = [\n+            (height, width)\n+            for height in range(1, self.seq_length)\n+            for width in range(1, self.seq_length)\n+            if height * width <= self.seq_length\n+        ] * self.batch_size\n+        spatial_shapes = spatial_shapes[: self.batch_size]\n+        spatial_shapes = torch.tensor(spatial_shapes, device=torch_device, dtype=torch.long)\n+\n+        for i, (height, width) in enumerate(spatial_shapes):\n+            pixel_attention_mask[i, : height * width] = 1\n+\n+        config = self.get_config()\n+\n+        return config, pixel_values, pixel_attention_mask, spatial_shapes\n+\n+    def get_config(self):\n+        return Siglip2VisionConfig(\n+            num_patches=self.num_patches,\n+            patch_size=self.patch_size,\n+            num_channels=self.num_channels,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+            initializer_range=self.initializer_range,\n+        )\n+\n+    def create_and_check_model(self, config, pixel_values, pixel_attention_mask, spatial_shapes):\n+        model = Siglip2VisionModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(pixel_values, pixel_attention_mask, spatial_shapes)\n+\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config, pixel_values, pixel_attention_mask, spatial_shapes = self.prepare_config_and_inputs()\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"pixel_attention_mask\": pixel_attention_mask,\n+            \"spatial_shapes\": spatial_shapes,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Siglip2VisionModelTest(Siglip2ModelTesterMixin, unittest.TestCase):\n+    \"\"\"\n+    Here we also overwrite some of the tests of test_modeling_common.py, as SIGLIP2 does not use input_ids, inputs_embeds,\n+    attention_mask and seq_length.\n+    \"\"\"\n+\n+    all_model_classes = (Siglip2VisionModel,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_head_masking = False\n+    # MP works but offload doesn't work when the MultiheadAttention is offloaded\n+    # TODO: One potential solution would be to add to set preload_module_classes = [\"Siglip2MultiheadAttentionPoolingHead\"]\n+    # in the dispatch_model function\n+    test_cpu_offload = False\n+    test_disk_offload_safetensors = False\n+    test_disk_offload_bin = False\n+\n+    def setUp(self):\n+        self.model_tester = Siglip2VisionModelTester(self)\n+        self.config_tester = ConfigTester(\n+            self, config_class=Siglip2VisionConfig, has_text_modality=False, hidden_size=37\n+        )\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    @unittest.skip(reason=\"SIGLIP2 does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    def test_model_get_set_embeddings(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            self.assertIsInstance(model.get_input_embeddings(), (nn.Module))\n+            x = model.get_output_embeddings()\n+            self.assertTrue(x is None or isinstance(x, nn.Linear))\n+\n+    def test_forward_signature(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        for model_class in self.all_model_classes:\n+            model = model_class(config)\n+            signature = inspect.signature(model.forward)\n+            # signature.parameters is an OrderedDict => so arg_names order is deterministic\n+            arg_names = [*signature.parameters.keys()]\n+\n+            expected_arg_names = [\"pixel_values\"]\n+            self.assertListEqual(arg_names[:1], expected_arg_names)\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"Siglip2VisionModel does not support standalone training\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2VisionModel does not support standalone training\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2VisionModel does not support standalone training\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2VisionModel does not support standalone training\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2VisionModel has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_from_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2VisionModel has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_to_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2 uses the same initialization scheme as the Flax original implementation\")\n+    def test_initialization(self):\n+        pass\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"google/siglip2-base-patch16-naflex\"\n+        model = Siglip2VisionModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @require_torch_sdpa\n+    @slow\n+    @is_flaky()\n+    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n+        super().test_eager_matches_sdpa_inference(\n+            torch_dtype=torch_dtype,\n+            logit_keys=(\"pooler_output\", \"last_hidden_state\"),\n+            use_attention_mask_options=(False,),\n+        )\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n+\n+class Siglip2TextModelTester:\n+    def __init__(\n+        self,\n+        parent,\n+        batch_size=12,\n+        seq_length=7,\n+        is_training=True,\n+        use_input_mask=True,\n+        use_labels=True,\n+        vocab_size=99,\n+        hidden_size=32,\n+        num_hidden_layers=2,\n+        num_attention_heads=4,\n+        intermediate_size=37,\n+        dropout=0.1,\n+        attention_dropout=0.1,\n+        max_position_embeddings=512,\n+        initializer_range=0.02,\n+        scope=None,\n+    ):\n+        self.parent = parent\n+        self.batch_size = batch_size\n+        self.seq_length = seq_length\n+        self.is_training = is_training\n+        self.use_input_mask = use_input_mask\n+        self.use_labels = use_labels\n+        self.vocab_size = vocab_size\n+        self.hidden_size = hidden_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+        self.intermediate_size = intermediate_size\n+        self.dropout = dropout\n+        self.attention_dropout = attention_dropout\n+        self.max_position_embeddings = max_position_embeddings\n+        self.initializer_range = initializer_range\n+        self.scope = scope\n+\n+    def prepare_config_and_inputs(self):\n+        input_ids = ids_tensor([self.batch_size, self.seq_length], self.vocab_size)\n+\n+        input_mask = None\n+        if self.use_input_mask:\n+            input_mask = random_attention_mask([self.batch_size, self.seq_length])\n+\n+        if input_mask is not None:\n+            batch_size, seq_length = input_mask.shape\n+            rnd_start_indices = np.random.randint(1, seq_length - 1, size=(batch_size,))\n+            for batch_idx, start_index in enumerate(rnd_start_indices):\n+                input_mask[batch_idx, :start_index] = 1\n+                input_mask[batch_idx, start_index:] = 0\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, input_mask\n+\n+    def get_config(self):\n+        return Siglip2TextConfig(\n+            vocab_size=self.vocab_size,\n+            hidden_size=self.hidden_size,\n+            num_hidden_layers=self.num_hidden_layers,\n+            num_attention_heads=self.num_attention_heads,\n+            intermediate_size=self.intermediate_size,\n+            dropout=self.dropout,\n+            attention_dropout=self.attention_dropout,\n+            max_position_embeddings=self.max_position_embeddings,\n+            initializer_range=self.initializer_range,\n+        )\n+\n+    def create_and_check_model(self, config, input_ids, input_mask):\n+        model = Siglip2TextModel(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        with torch.no_grad():\n+            result = model(input_ids, attention_mask=input_mask)\n+            result = model(input_ids)\n+        self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, self.seq_length, self.hidden_size))\n+        self.parent.assertEqual(result.pooler_output.shape, (self.batch_size, self.hidden_size))\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, input_mask = config_and_inputs\n+        inputs_dict = {\"input_ids\": input_ids, \"attention_mask\": input_mask}\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Siglip2TextModelTest(Siglip2ModelTesterMixin, unittest.TestCase):\n+    all_model_classes = (Siglip2TextModel,) if is_torch_available() else ()\n+    fx_compatible = False\n+    test_resize_embeddings = False\n+    test_pruning = False\n+    test_head_masking = False\n+    model_split_percents = [0.5, 0.8, 0.9]\n+\n+    def setUp(self):\n+        self.model_tester = Siglip2TextModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Siglip2TextConfig, hidden_size=37)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"Siglip2TextModel does not support standalone training\")\n+    def test_training(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2TextModel does not support standalone training\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2TextModel does not support standalone training\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2TextModel does not support standalone training\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2 does not use inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2TextModel has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_from_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2TextModel has no base class and is not available in MODEL_MAPPING\")\n+    def test_save_load_fast_init_to_base(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2 uses the same initialization scheme as the Flax original implementation\")\n+    def test_initialization(self):\n+        pass\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"google/siglip2-base-patch16-naflex\"\n+        model = Siglip2TextModel.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @require_torch_sdpa\n+    @slow\n+    @is_flaky()\n+    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n+        super().test_eager_matches_sdpa_inference(\n+            torch_dtype=torch_dtype,\n+            logit_keys=(\"pooler_output\", \"last_hidden_state\"),\n+            use_attention_mask_options=(False, True),\n+        )\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n+\n+class Siglip2ModelTester:\n+    def __init__(self, parent, text_kwargs=None, vision_kwargs=None, is_training=True):\n+        if text_kwargs is None:\n+            text_kwargs = {}\n+        if vision_kwargs is None:\n+            vision_kwargs = {}\n+\n+        self.parent = parent\n+        self.text_model_tester = Siglip2TextModelTester(parent, **text_kwargs)\n+        self.vision_model_tester = Siglip2VisionModelTester(parent, **vision_kwargs)\n+        self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n+        self.is_training = is_training\n+\n+    def prepare_config_and_inputs(self):\n+        text_config, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n+        vision_config, pixel_values, pixel_attention_mask, spatial_shapes = (\n+            self.vision_model_tester.prepare_config_and_inputs()\n+        )\n+\n+        config = self.get_config()\n+\n+        return config, input_ids, attention_mask, pixel_values, pixel_attention_mask, spatial_shapes\n+\n+    def get_config(self):\n+        return Siglip2Config.from_text_vision_configs(\n+            self.text_model_tester.get_config(),\n+            self.vision_model_tester.get_config(),\n+        )\n+\n+    def create_and_check_model(\n+        self, config, input_ids, attention_mask, pixel_values, pixel_attention_mask, spatial_shapes\n+    ):\n+        model = Siglip2Model(config).to(torch_device).eval()\n+        with torch.no_grad():\n+            result = model(input_ids, pixel_values, pixel_attention_mask, spatial_shapes, attention_mask)\n+        self.parent.assertEqual(\n+            result.logits_per_image.shape, (self.vision_model_tester.batch_size, self.text_model_tester.batch_size)\n+        )\n+        self.parent.assertEqual(\n+            result.logits_per_text.shape, (self.text_model_tester.batch_size, self.vision_model_tester.batch_size)\n+        )\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, input_ids, attention_mask, pixel_values, pixel_attention_mask, spatial_shapes = config_and_inputs\n+        inputs_dict = {\n+            \"input_ids\": input_ids,\n+            \"pixel_values\": pixel_values,\n+            \"pixel_attention_mask\": pixel_attention_mask,\n+            \"spatial_shapes\": spatial_shapes,\n+            \"attention_mask\": attention_mask,\n+            \"position_ids\": None,\n+            \"return_loss\": False,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Siglip2ModelTest(Siglip2ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (Siglip2Model,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"feature-extraction\": Siglip2Model} if is_torch_available() else {}\n+    fx_compatible = False\n+    test_head_masking = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_attention_outputs = False\n+    # MP works but offload doesn't work when the MultiheadAttention is offloaded\n+    # TODO: One potential solution would be to add to set preload_module_classes = [\"Siglip2MultiheadAttentionPoolingHead\"]\n+    # in the dispatch_model function\n+    test_cpu_offload = False\n+    test_disk_offload_safetensors = False\n+    test_disk_offload_bin = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = Siglip2ModelTester(self)\n+        self.config_tester = ConfigTester(self, config_class=Siglip2Config, has_text_modality=False)\n+\n+    def test_config(self):\n+        self.config_tester.run_common_tests()\n+\n+    def test_model(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_model(*config_and_inputs)\n+\n+    @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n+    def test_hidden_states_output(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Inputs_embeds is tested in individual model tests\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Retain_grad is tested in individual model tests\")\n+    def test_retain_grad_hidden_states_attentions(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2Model does not have input/output embeddings\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2 uses the same initialization scheme as the Flax original implementation\")\n+    def test_initialization(self):\n+        pass\n+\n+    def test_load_vision_text_config(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # Save Siglip2Config and check if we can load Siglip2VisionConfig from it\n+        with tempfile.TemporaryDirectory() as tmp_dir_name:\n+            config.save_pretrained(tmp_dir_name)\n+            vision_config = Siglip2VisionConfig.from_pretrained(tmp_dir_name)\n+            self.assertDictEqual(config.vision_config.to_dict(), vision_config.to_dict())\n+\n+        # Save Siglip2Config and check if we can load Siglip2TextConfig from it\n+        with tempfile.TemporaryDirectory() as tmp_dir_name:\n+            config.save_pretrained(tmp_dir_name)\n+            text_config = Siglip2TextConfig.from_pretrained(tmp_dir_name)\n+            self.assertDictEqual(config.text_config.to_dict(), text_config.to_dict())\n+\n+    @slow\n+    def test_model_from_pretrained(self):\n+        model_name = \"google/siglip2-base-patch16-naflex\"\n+        model = Siglip2Model.from_pretrained(model_name)\n+        self.assertIsNotNone(model)\n+\n+    @require_flash_attn\n+    @require_torch_gpu\n+    @mark.flash_attn_test\n+    def test_flash_attn_2_inference_equivalence_right_padding(self):\n+        self.skipTest(\"Siglip2 does not support right padding\")\n+\n+    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @require_torch_sdpa\n+    @slow\n+    @is_flaky()\n+    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n+        super().test_eager_matches_sdpa_inference(\n+            torch_dtype=torch_dtype,\n+            logit_keys=(\"logits_per_image\", \"logits_per_text\", \"image_embeds\", \"text_embeds\"),\n+            use_attention_mask_options=(False, True),\n+        )\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n+\n+class Siglip2ForImageClassificationModelTester(Siglip2ModelTester):\n+    def __init__(self, parent):\n+        super().__init__(parent)\n+        self.batch_size = self.vision_model_tester.batch_size\n+        self.num_hidden_layers = self.vision_model_tester.num_hidden_layers\n+        self.hidden_size = self.vision_model_tester.hidden_size\n+        self.seq_length = self.vision_model_tester.seq_length\n+\n+    def prepare_config_and_inputs(self):\n+        _, pixel_values, pixel_attention_mask, spatial_shapes = self.vision_model_tester.prepare_config_and_inputs()\n+        config = self.get_config()\n+\n+        return config, pixel_values, pixel_attention_mask, spatial_shapes\n+\n+    def prepare_config_and_inputs_for_common(self):\n+        config_and_inputs = self.prepare_config_and_inputs()\n+        config, pixel_values, pixel_attention_mask, spatial_shapes = config_and_inputs\n+        inputs_dict = {\n+            \"pixel_values\": pixel_values,\n+            \"pixel_attention_mask\": pixel_attention_mask,\n+            \"spatial_shapes\": spatial_shapes,\n+        }\n+        return config, inputs_dict\n+\n+\n+@require_torch\n+class Siglip2ForImageClassificationModelTest(Siglip2ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n+    all_model_classes = (Siglip2ForImageClassification,) if is_torch_available() else ()\n+    pipeline_model_mapping = {\"image-classification\": Siglip2ForImageClassification} if is_torch_available() else {}\n+    fx_compatible = False\n+    test_head_masking = False\n+    test_pruning = False\n+    test_resize_embeddings = False\n+    test_attention_outputs = False\n+    # MP works but offload doesn't work when the MultiheadAttention is offloaded\n+    # TODO: One potential solution would be to add to set preload_module_classes = [\"Siglip2MultiheadAttentionPoolingHead\"]\n+    # in the dispatch_model function\n+    test_cpu_offload = False\n+    test_disk_offload_safetensors = False\n+    test_disk_offload_bin = False\n+    _is_composite = True\n+\n+    def setUp(self):\n+        self.model_tester = Siglip2ForImageClassificationModelTester(self)\n+\n+    @unittest.skip(reason=\"Siglip2ForImageClassification does not support inputs_embeds\")\n+    def test_inputs_embeds(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2ForImageClassification does not support inputs_embeds\")\n+    def test_model_get_set_embeddings(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2ForImageClassification does not support gradient checkpointing yet\")\n+    def test_training_gradient_checkpointing(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2ForImageClassification does not support gradient checkpointing yet\")\n+    def test_training_gradient_checkpointing_use_reentrant(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2ForImageClassification does not support gradient checkpointing yet\")\n+    def test_training_gradient_checkpointing_use_reentrant_false(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Siglip2 uses the same initialization scheme as the Flax original implementation\")\n+    def test_initialization(self):\n+        pass\n+\n+    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    @require_torch_sdpa\n+    @slow\n+    @is_flaky()\n+    def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n+        super().test_eager_matches_sdpa_inference(\n+            torch_dtype=torch_dtype, logit_keys=(\"logits\",), use_attention_mask_options=(False,)\n+        )\n+\n+    @require_torch_sdpa\n+    def test_sdpa_can_dispatch_composite_models(self):\n+        super().test_sdpa_can_dispatch_composite_models()\n+\n+\n+# Draw a circle on an images with different aspect ratios\n+def prepare_images():\n+    shapes = [(224, 224), (1024, 1024), (224, 1024)]\n+    images = []\n+    for height, width in shapes:\n+        image = Image.new(\"RGB\", (width, height), color=\"red\")\n+        draw = ImageDraw.Draw(image)\n+        center_x = image.width // 2\n+        center_y = image.height // 2\n+        radius = min(center_x, center_y) // 8 * 7\n+        draw.ellipse(\n+            (center_x - radius, center_y - radius, center_x + radius, center_y + radius),\n+            fill=\"blue\",\n+            outline=\"green\",\n+            width=image.width // 20,\n+        )\n+        images.append(image)\n+    return images\n+\n+\n+@require_vision\n+@require_torch\n+class Siglip2ModelIntegrationTest(unittest.TestCase):\n+    @slow\n+    def test_inference(self):\n+        model_name = \"google/siglip2-base-patch16-naflex\"\n+        model = Siglip2Model.from_pretrained(model_name).to(torch_device)\n+        processor = Siglip2Processor.from_pretrained(model_name)\n+\n+        images = prepare_images()\n+        text = [\n+            \"circle\",\n+            \"ellipsoid\",\n+            \"blue circle on red background\",\n+            \"blue circle with green border on red background\",\n+            \"green circle on red background\",\n+            \"a dog\",\n+            \"a blue dog with a green border on a red background\",\n+        ]\n+\n+        inputs = processor(text=text, images=images, return_tensors=\"pt\")\n+        inputs = inputs.to(torch_device)\n+\n+        # forward pass\n+        with torch.no_grad():\n+            outputs = model(**inputs)\n+\n+        logits_per_image = outputs.logits_per_image\n+        logits_per_text = outputs.logits_per_text\n+\n+        # verify the logits shape\n+        self.assertEqual(\n+            logits_per_image.shape,\n+            torch.Size((inputs.pixel_values.shape[0], inputs.input_ids.shape[0])),\n+        )\n+        self.assertEqual(\n+            logits_per_text.shape,\n+            torch.Size((inputs.input_ids.shape[0], inputs.pixel_values.shape[0])),\n+        )\n+\n+        # verify the logits values\n+        # fmt: off\n+        expected_logits_per_text = torch.tensor(\n+            [\n+                [  1.0195,  -0.0280,  -1.4468],\n+                [ -4.5395,  -6.2269,  -1.5667],\n+                [  4.1757,   5.0358,   3.5159],\n+                [  9.4264,  10.1879,   6.3353],\n+                [  2.4409,   3.1058,   4.5491],\n+                [-12.3230, -13.7355, -13.4632],\n+                [  1.1520,   1.1687,  -1.9647],\n+            ]\n+        ).to(torch_device)\n+        # fmt: on\n+\n+        torch.testing.assert_close(outputs.logits_per_text, expected_logits_per_text, rtol=1e-3, atol=1e-3)"
        },
        {
            "sha": "d0adee987d52c273659624e3998923abdec77124",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -1175,6 +1175,10 @@ def _create_and_check_torchscript(self, config, inputs_dict):\n                         traced_model = torch.jit.trace(\n                             model, (pixel_values, prompt_pixel_values, prompt_masks), check_trace=False\n                         )  # when traced model is checked, an error is produced due to name mangling\n+                    elif \"Siglip2\" in model_class.__name__:\n+                        outputs = model(**inputs)\n+                        example_inputs = [t for t in inputs.values() if isinstance(t, torch.Tensor)]\n+                        traced_model = torch.jit.trace(model, example_inputs, check_trace=False)\n                     else:\n                         main_input = inputs[main_input_name]\n \n@@ -3035,6 +3039,7 @@ def test_mismatched_shapes_have_properly_initialized_weights(self):\n                 \"wav2vec2.masked_spec_embed\",\n                 \"Wav2Vec2ForSequenceClassification\",\n                 \"CLIPForImageClassification\",\n+                \"Siglip2ForImageClassification\",\n                 \"RegNetForImageClassification\",\n                 \"ResNetForImageClassification\",\n                 \"UniSpeechSatForSequenceClassification\","
        },
        {
            "sha": "3b3dddf9cf63d15dd85eb30747a1939181fa44c1",
            "filename": "utils/check_repo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a957b7911a758d54597914b4479fe6e81424d64f/utils%2Fcheck_repo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a957b7911a758d54597914b4479fe6e81424d64f/utils%2Fcheck_repo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_repo.py?ref=a957b7911a758d54597914b4479fe6e81424d64f",
            "patch": "@@ -334,6 +334,8 @@\n     \"SegGptForImageSegmentation\",\n     \"SiglipVisionModel\",\n     \"SiglipTextModel\",\n+    \"Siglip2VisionModel\",\n+    \"Siglip2TextModel\",\n     \"ChameleonVQVAE\",  # no autoclass for VQ-VAE models\n     \"VitPoseForPoseEstimation\",\n     \"CLIPTextModel\","
        }
    ],
    "stats": {
        "total": 5692,
        "additions": 5570,
        "deletions": 122
    }
}