{
    "author": "zucchini-nlp",
    "message": "Clean-up deprecated code (#33446)\n\n* update\r\n\r\n* update modeling",
    "sha": "c8ea67532451be7fcecdf4bdb5df579dc61b88ac",
    "files": [
        {
            "sha": "92af404cdbef1165a110f7d7b4a39687870b7c1e",
            "filename": "src/transformers/models/fuyu/configuration_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8ea67532451be7fcecdf4bdb5df579dc61b88ac/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8ea67532451be7fcecdf4bdb5df579dc61b88ac/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fconfiguration_fuyu.py?ref=c8ea67532451be7fcecdf4bdb5df579dc61b88ac",
            "patch": "@@ -14,8 +14,6 @@\n # limitations under the License.\n \"\"\"Fuyu model configuration\"\"\"\n \n-import warnings\n-\n from ...configuration_utils import PretrainedConfig\n from ...utils import logging\n from ..auto import CONFIG_MAPPING\n@@ -207,20 +205,3 @@ def _rope_scaling_validation(self):\n             )\n         if rope_scaling_factor is None or not isinstance(rope_scaling_factor, float) or rope_scaling_factor <= 1.0:\n             raise ValueError(f\"`rope_scaling`'s factor field must be a float > 1, got {rope_scaling_factor}\")\n-\n-    @property\n-    def vocab_size(self):\n-        warnings.warn(\n-            \"The `vocab_size` attribute is deprecated and will be removed in v4.44, Please use `text_config.vocab_size` instead.\",\n-            FutureWarning,\n-        )\n-        return self._vocab_size\n-\n-    @vocab_size.setter\n-    def vocab_size(self, value):\n-        self._vocab_size = value\n-\n-    def to_dict(self):\n-        output = super().to_dict()\n-        output.pop(\"_vocab_size\", None)\n-        return output"
        },
        {
            "sha": "089313b03b7b60043cc562753024f0a9ed7dcc0f",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8ea67532451be7fcecdf4bdb5df579dc61b88ac/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8ea67532451be7fcecdf4bdb5df579dc61b88ac/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=c8ea67532451be7fcecdf4bdb5df579dc61b88ac",
            "patch": "@@ -183,15 +183,6 @@ def get_decoder(self):\n     def tie_weights(self):\n         return self.language_model.tie_weights()\n \n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        # TODO: config.vocab_size is deprecated and will be removed in v4.43.\n-        # `resize_token_embeddings` should work from `modeling_utils.py``\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n-        self.config.text_config.vocab_size = model_embeds.num_embeddings\n-        self.config.vocab_size = model_embeds.num_embeddings\n-        self.vocab_size = model_embeds.num_embeddings\n-        return model_embeds\n-\n     def gather_continuous_embeddings(\n         self,\n         word_embeddings: torch.Tensor,\n@@ -254,8 +245,8 @@ def forward(\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+                config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n \n         Returns:\n "
        },
        {
            "sha": "64598436dbbf1f08f3afa567b23a7bcf51d7f74e",
            "filename": "src/transformers/models/paligemma/configuration_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8ea67532451be7fcecdf4bdb5df579dc61b88ac/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8ea67532451be7fcecdf4bdb5df579dc61b88ac/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fconfiguration_paligemma.py?ref=c8ea67532451be7fcecdf4bdb5df579dc61b88ac",
            "patch": "@@ -141,20 +141,7 @@ def ignore_index(self):\n     def ignore_index(self, value):\n         self._ignore_index = value\n \n-    @property\n-    def vocab_size(self):\n-        warnings.warn(\n-            \"The `vocab_size` attribute is deprecated and will be removed in v4.44, Please use `text_config.vocab_size` instead.\",\n-            FutureWarning,\n-        )\n-        return self._vocab_size\n-\n-    @vocab_size.setter\n-    def vocab_size(self, value):\n-        self._vocab_size = value\n-\n     def to_dict(self):\n         output = super().to_dict()\n-        output.pop(\"_vocab_size\", None)\n         output.pop(\"_ignore_index\", None)\n         return output"
        },
        {
            "sha": "71bc7c331339daee36419ed7649ec2169b0bbf81",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8ea67532451be7fcecdf4bdb5df579dc61b88ac/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8ea67532451be7fcecdf4bdb5df579dc61b88ac/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=c8ea67532451be7fcecdf4bdb5df579dc61b88ac",
            "patch": "@@ -53,7 +53,7 @@ class PaliGemmaCausalLMOutputWithPast(ModelOutput):\n     Args:\n         loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n             Language modeling loss (for next-token prediction).\n-        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n+        logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n             Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n         past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n             Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n@@ -283,15 +283,6 @@ def get_decoder(self):\n     def tie_weights(self):\n         return self.language_model.tie_weights()\n \n-    def resize_token_embeddings(self, new_num_tokens: Optional[int] = None, pad_to_multiple_of=None) -> nn.Embedding:\n-        # TODO: config.vocab_size is deprecated and will be removed in v4.43.\n-        # `resize_token_embeddings` should work from `modeling_utils.py``\n-        model_embeds = self.language_model.resize_token_embeddings(new_num_tokens, pad_to_multiple_of)\n-        self.config.text_config.vocab_size = model_embeds.num_embeddings\n-        self.config.vocab_size = model_embeds.num_embeddings\n-        self.vocab_size = model_embeds.num_embeddings\n-        return model_embeds\n-\n     def _update_causal_mask(\n         self, attention_mask, token_type_ids, inputs_embeds, past_key_values, cache_position, is_training: bool = False\n     ):\n@@ -362,8 +353,8 @@ def forward(\n         Args:\n             labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\n-                config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n-                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.vocab_size]`.\n+                config.text_config.vocab_size]` or -100 (see `input_ids` docstring). Tokens with indices set to `-100` are ignored\n+                (masked), the loss is only computed for the tokens with labels in `[0, ..., config.text_config.vocab_size]`.\n \n             num_logits_to_keep (`int`, *optional*):\n                 Calculate logits for the last `num_logits_to_keep` tokens. If `0`, calculate logits for all"
        }
    ],
    "stats": {
        "total": 60,
        "additions": 5,
        "deletions": 55
    }
}