{
    "author": "Cyrilvallez",
    "message": "Fix T5Gemma module structure (#42145)\n\n* fix modular\n\n* oupsi typo",
    "sha": "3760afb21c1f4c14aa48c41d1b025702777b1c53",
    "files": [
        {
            "sha": "ebb5f6a5e87481e411bac1798ce75d80e1d35967",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 29,
            "deletions": 3,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/3760afb21c1f4c14aa48c41d1b025702777b1c53/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3760afb21c1f4c14aa48c41d1b025702777b1c53/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=3760afb21c1f4c14aa48c41d1b025702777b1c53",
            "patch": "@@ -448,11 +448,28 @@ def forward(\n         return hidden_states\n \n \n-class T5GemmaDecoderLayer(T5GemmaEncoderLayer):\n+class T5GemmaDecoderLayer(GradientCheckpointingLayer):\n     \"\"\"Decoder sub-layer: an extra cross-attention layer.\"\"\"\n \n     def __init__(self, config, layer_idx: int):\n-        super().__init__(config, layer_idx)\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.attention_type = config.layer_types[layer_idx]\n+\n+        self.self_attn = T5GemmaSelfAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.pre_self_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_self_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.mlp = T5GemmaMLP(config)\n+        self.pre_feedforward_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.dropout = nn.Dropout(config.dropout_rate)\n         self.cross_attn = T5GemmaCrossAttention(config=config, layer_idx=layer_idx)\n         self.pre_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -732,7 +749,7 @@ def forward(\n         )\n \n \n-class T5GemmaDecoder(T5GemmaEncoder):\n+class T5GemmaDecoder(T5GemmaPreTrainedModel):\n     _can_record_outputs = {\n         \"attentions\": OutputRecorder(T5GemmaSelfAttention, index=1),\n         \"cross_attentions\": OutputRecorder(T5GemmaCrossAttention, index=1),\n@@ -741,11 +758,20 @@ class T5GemmaDecoder(T5GemmaEncoder):\n \n     def __init__(self, config):\n         super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.norm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.gradient_checkpointing = False\n+\n         self.layers = nn.ModuleList(\n             [T5GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n+        self.dropout = nn.Dropout(config.dropout_rate)\n         self.rotary_emb = T5GemmaRotaryEmbedding(config=config)\n \n+        # Initialize weights and apply final processing\n         self.post_init()\n \n     @check_model_inputs()"
        },
        {
            "sha": "a4c1d5083e7443507f8ea975090cd4654553bdf4",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 29,
            "deletions": 3,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/3760afb21c1f4c14aa48c41d1b025702777b1c53/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3760afb21c1f4c14aa48c41d1b025702777b1c53/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=3760afb21c1f4c14aa48c41d1b025702777b1c53",
            "patch": "@@ -517,11 +517,28 @@ def forward(\n         return hidden_states\n \n \n-class T5GemmaDecoderLayer(T5GemmaEncoderLayer):\n+class T5GemmaDecoderLayer(GradientCheckpointingLayer):\n     \"\"\"Decoder sub-layer: an extra cross-attention layer.\"\"\"\n \n     def __init__(self, config, layer_idx: int):\n-        super().__init__(config, layer_idx)\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.attention_type = config.layer_types[layer_idx]\n+\n+        self.self_attn = T5GemmaSelfAttention(\n+            config=config,\n+            layer_idx=layer_idx,\n+        )\n+        self.pre_self_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_self_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.mlp = T5GemmaMLP(config)\n+        self.pre_feedforward_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+        self.dropout = nn.Dropout(config.dropout_rate)\n         self.cross_attn = T5GemmaCrossAttention(config=config, layer_idx=layer_idx)\n         self.pre_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.post_cross_attn_layernorm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n@@ -770,7 +787,7 @@ def forward(\n         )\n \n \n-class T5GemmaDecoder(T5GemmaEncoder):\n+class T5GemmaDecoder(T5GemmaPreTrainedModel):\n     _can_record_outputs = {\n         \"attentions\": OutputRecorder(T5GemmaSelfAttention, index=1),\n         \"cross_attentions\": OutputRecorder(T5GemmaCrossAttention, index=1),\n@@ -779,11 +796,20 @@ class T5GemmaDecoder(T5GemmaEncoder):\n \n     def __init__(self, config):\n         super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.norm = T5GemmaRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.gradient_checkpointing = False\n+\n         self.layers = nn.ModuleList(\n             [T5GemmaDecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n         )\n+        self.dropout = nn.Dropout(config.dropout_rate)\n         self.rotary_emb = T5GemmaRotaryEmbedding(config=config)\n \n+        # Initialize weights and apply final processing\n         self.post_init()\n \n     @check_model_inputs()"
        }
    ],
    "stats": {
        "total": 64,
        "additions": 58,
        "deletions": 6
    }
}