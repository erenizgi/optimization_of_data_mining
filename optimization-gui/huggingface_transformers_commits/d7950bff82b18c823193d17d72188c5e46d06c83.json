{
    "author": "yonigozlan",
    "message": "uniformize processor Mllama (#33876)\n\n* uniformize processor Mllama\r\n\r\n* nit syntax\r\n\r\n* nit",
    "sha": "d7950bff82b18c823193d17d72188c5e46d06c83",
    "files": [
        {
            "sha": "eb092f021f6368bf2d5e4e949ce33a4350b726b0",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7950bff82b18c823193d17d72188c5e46d06c83/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7950bff82b18c823193d17d72188c5e46d06c83/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=d7950bff82b18c823193d17d72188c5e46d06c83",
            "patch": "@@ -23,7 +23,6 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import (\n-    BatchEncoding,\n     PreTokenizedInput,\n     TextInput,\n )\n@@ -226,8 +225,10 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, List[TextInput], List[PreTokenizedInput]]] = None,\n+        audio=None,\n+        videos=None,\n         **kwargs: Unpack[MllamaProcessorKwargs],\n-    ) -> BatchEncoding:\n+    ) -> BatchFeature:\n         \"\"\"\n         Main method to prepare text(s) and image(s) to be fed as input to the model. This method forwards the `text`\n         arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] if `text` is not `None` to encode\n@@ -250,7 +251,7 @@ def __call__(\n                     - `'np'`: Return NumPy `np.ndarray` objects.\n                     - `'jax'`: Return JAX `jnp.ndarray` objects.\n         Returns:\n-            [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n+            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n \n             - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n             - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n@@ -323,9 +324,9 @@ def __call__(\n             data[\"cross_attention_mask\"] = cross_attention_mask\n \n         return_tensors = common_kwargs.pop(\"return_tensors\", None)\n-        batch_encoding = BatchFeature(data=data, tensor_type=return_tensors)\n+        batch_feature = BatchFeature(data=data, tensor_type=return_tensors)\n \n-        return batch_encoding\n+        return batch_feature\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "a48a7a2e6da4d223ef0783d09d29c36c2824176d",
            "filename": "tests/models/mllama/test_processor_mllama.py",
            "status": "modified",
            "additions": 53,
            "deletions": 25,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/d7950bff82b18c823193d17d72188c5e46d06c83/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d7950bff82b18c823193d17d72188c5e46d06c83/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processor_mllama.py?ref=d7950bff82b18c823193d17d72188c5e46d06c83",
            "patch": "@@ -13,32 +13,44 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+import shutil\n+import tempfile\n import unittest\n+from typing import Optional\n \n import numpy as np\n \n from transformers import MllamaProcessor\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_vision_available\n \n+from ...test_processing_common import ProcessorTesterMixin\n+\n \n if is_vision_available():\n     from PIL import Image\n \n \n @require_torch\n @require_vision\n-class MllamaProcessorTest(unittest.TestCase):\n+class MllamaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n+    processor_class = MllamaProcessor\n+\n     def setUp(self):\n-        self.checkpoint = \"hf-internal-testing/mllama-11b\"  # TODO: change\n-        self.processor = MllamaProcessor.from_pretrained(self.checkpoint)\n+        self.checkpoint = \"hf-internal-testing/mllama-11b\"\n+        processor = MllamaProcessor.from_pretrained(self.checkpoint)\n         self.image1 = Image.new(\"RGB\", (224, 220))\n         self.image2 = Image.new(\"RGB\", (512, 128))\n-        self.image_token = self.processor.image_token\n-        self.image_token_id = self.processor.image_token_id\n-        self.pad_token_id = self.processor.tokenizer.pad_token_id\n-        self.bos_token = self.processor.bos_token\n-        self.bos_token_id = self.processor.tokenizer.bos_token_id\n+        self.image_token = processor.image_token\n+        self.image_token_id = processor.image_token_id\n+        self.pad_token_id = processor.tokenizer.pad_token_id\n+        self.bos_token = processor.bos_token\n+        self.bos_token_id = processor.tokenizer.bos_token_id\n+        self.tmpdirname = tempfile.mkdtemp()\n+        processor.save_pretrained(self.tmpdirname)\n+\n+    def tearDown(self):\n+        shutil.rmtree(self.tmpdirname)\n \n     def test_apply_chat_template(self):\n         # Message contains content which a mix of lists with images and image urls and string\n@@ -64,8 +76,8 @@ def test_apply_chat_template(self):\n                 ],\n             },\n         ]\n-\n-        rendered = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+        processor = MllamaProcessor.from_pretrained(self.tmpdirname)\n+        rendered = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n \n         expected_rendered = (\n             \"<|begin_of_text|>\"\n@@ -96,7 +108,7 @@ def test_apply_chat_template(self):\n                 ],\n             },\n         ]\n-        input_ids = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        input_ids = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n         expected_ids = [\n             128000,  # <|begin_of_text|>\n             128006,  # <|start_header_id|>\n@@ -142,15 +154,15 @@ def test_apply_chat_template(self):\n             }\n         ]\n \n-        rendered = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n+        rendered = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=False)\n         expected_rendered = (\n             \"<|begin_of_text|><|start_header_id|>user<|end_header_id|>\\n\\n\"\n             \"Describe this image in two sentences<|image|> Test sentence   <|image|>ok\\n<|eot_id|>\"\n             \"<|start_header_id|>assistant<|end_header_id|>\\n\\n\"\n         )\n         self.assertEqual(rendered, expected_rendered)\n \n-        input_ids = self.processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n+        input_ids = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True)\n         # fmt: off\n         expected_ids = [\n             128000, 128006, 882, 128007, 271, 75885, 420, 2217, 304, 1403, 23719, 128256,\n@@ -176,18 +188,19 @@ def test_apply_chat_template(self):\n             }\n         ]\n \n-        rendered_list = self.processor.apply_chat_template(messages_list, add_generation_prompt=True, tokenize=False)\n-        rendered_str = self.processor.apply_chat_template(messages_str, add_generation_prompt=True, tokenize=False)\n+        rendered_list = processor.apply_chat_template(messages_list, add_generation_prompt=True, tokenize=False)\n+        rendered_str = processor.apply_chat_template(messages_str, add_generation_prompt=True, tokenize=False)\n         self.assertEqual(rendered_list, rendered_str)\n \n     def test_process_interleaved_images_prompts_image_splitting(self):\n+        processor = MllamaProcessor.from_pretrained(self.tmpdirname)\n         # Test that a single image is processed correctly\n-        inputs = self.processor(images=self.image2, size={\"width\": 224, \"height\": 224})\n+        inputs = processor(images=self.image2, size={\"width\": 224, \"height\": 224})\n         self.assertEqual(inputs[\"pixel_values\"].shape, (1, 1, 4, 3, 224, 224))\n \n         # Test that text is processed correctly\n         text = \"<|begin_of_text|>This is a test sentence.<|end_of_text|>\"\n-        inputs = self.processor(text=text)\n+        inputs = processor(text=text)\n         expected_ids = [128000, 2028, 374, 264, 1296, 11914, 13, 128001]\n         self.assertEqual(inputs[\"input_ids\"][0], expected_ids)\n         self.assertEqual(inputs[\"attention_mask\"][0], [1] * len(expected_ids))\n@@ -197,7 +210,7 @@ def test_process_interleaved_images_prompts_image_splitting(self):\n         image_str = \"<|image|>\"\n         text_str = \"This is a test sentence.\"\n         text = image_str + text_str\n-        inputs = self.processor(\n+        inputs = processor(\n             text=text,\n             images=self.image1,\n             size={\"width\": 128, \"height\": 128},\n@@ -225,7 +238,7 @@ def test_process_interleaved_images_prompts_image_splitting(self):\n         ]\n         # fmt: onn\n         images = [[self.image1], [self.image1, self.image2]]\n-        inputs = self.processor(text=text, images=images, padding=True, size={\"width\": 256, \"height\": 256})\n+        inputs = processor(text=text, images=images, padding=True, size={\"width\": 256, \"height\": 256})\n \n         self.assertEqual(inputs[\"pixel_values\"].shape, (2, 2, 4, 3, 256, 256))\n         for input_ids_i, attention_mask_i, expected_ids_i in zip(inputs[\"input_ids\"], inputs[\"attention_mask\"], expected_ids):\n@@ -264,34 +277,49 @@ def test_process_interleaved_images_prompts_image_error(self):\n             \"This is a test sentence.\",\n             \"In this other sentence we try some good things\",\n         ]\n-        inputs = self.processor(text=text, images=None, padding=True)\n+        processor = MllamaProcessor.from_pretrained(self.tmpdirname)\n+        inputs = processor(text=text, images=None, padding=True)\n         self.assertIsNotNone(inputs[\"input_ids\"])\n \n         text = [\n             \"This is a test sentence.<|image|>\",\n             \"In this other sentence we try some good things\",\n         ]\n         with self.assertRaises(ValueError):\n-            self.processor(text=text, images=None, padding=True)\n+            processor(text=text, images=None, padding=True)\n \n         images = [[self.image1], []]\n         with self.assertRaises(ValueError):\n-            self.processor(text=text, images=images, padding=True)\n+            processor(text=text, images=images, padding=True)\n \n         text = [\n             \"This is a test sentence.<|image|>\",\n             \"In this other sentence we try some good things<|image|>\",\n         ]\n         with self.assertRaises(ValueError):\n-            self.processor(text=text, images=None, padding=True)\n+            processor(text=text, images=None, padding=True)\n \n         text = [\n             \"This is a test sentence.<|image|>\",\n             \"In this other sentence we try some good things<|image|>\",\n         ]\n         images = [[self.image1], [self.image2]]\n-        inputs = self.processor(text=text, images=images, padding=True)\n+        inputs = processor(text=text, images=images, padding=True)\n \n         images = [[self.image1, self.image2], []]\n         with self.assertRaises(ValueError):\n-            self.processor(text=text, images=None, padding=True)\n+            processor(text=text, images=None, padding=True)\n+\n+    # Override as MllamaProcessor needs image tokens in prompts\n+    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n+        if batch_size is None:\n+            return \"lower newer <|image|>\"\n+\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+\n+        if batch_size == 1:\n+            return [\"lower newer <|image|>\"]\n+        return [\"lower newer <|image|>\", \"<|image|> upper older longer string\"] + [\"<|image|> lower newer\"] * (\n+            batch_size - 2\n+        )"
        }
    ],
    "stats": {
        "total": 89,
        "additions": 59,
        "deletions": 30
    }
}