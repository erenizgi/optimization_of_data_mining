{
    "author": "gante",
    "message": "[generate] remove docs of a feature that no longer exists (#40895)",
    "sha": "5af248b3e3bf854e884c53f05b17ed1e0ca2ed24",
    "files": [
        {
            "sha": "ff9089cdaa3223b5e60bcf43f9ec3415b9c67f2e",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/5af248b3e3bf854e884c53f05b17ed1e0ca2ed24/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5af248b3e3bf854e884c53f05b17ed1e0ca2ed24/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=5af248b3e3bf854e884c53f05b17ed1e0ca2ed24",
            "patch": "@@ -183,36 +183,6 @@ text\n  'My favorite all time favorite condiment is ketchup. I love it on everything. I love it on my eggs, my fries, my chicken, my burgers, my hot dogs, my sandwiches, my salads, my p']\n ```\n \n-</hfoption>\n-<hfoption id=\"3. compile entire generate function\">\n-\n-Compiling the entire [`~GenerationMixin.generate`] function also compiles the input preparation logit processor operations, and more, in addition to the forward pass. With this approach, you don't need to initialize [`StaticCache`] or set the [cache_implementation](https://hf.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig.cache_implementation) parameter.\n-\n-```py\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n-import torch\n-import os\n-os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"  # To prevent long warnings :)\n-\n-tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b\")\n-model = AutoModelForCausalLM.from_pretrained(\"google/gemma-2b\", dtype=\"auto\", device_map=\"auto\")\n-\n-model.generate = torch.compile(model.generate, mode=\"reduce-overhead\", fullgraph=True)\n-input_text = \"The theory of special relativity states \"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device.type)\n-\n-outputs = model.generate(**input_ids)\n-print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n-['The theory of special relativity states 1. The speed of light is constant in all inertial reference']\n-```\n-\n-This usage pattern is more appropriate for unique hardware or use cases, but there are several drawbacks to consider.\n-\n-1. Compilation is much slower.\n-2. Parameters must be configured through [`GenerationConfig`].\n-3. Many warnings and exceptions are suppressed. We recommend testing the uncompiled model first.\n-4. Many features are unavailable at the moment. For example, generation does not stop if an `EOS` token is selected.\n-\n </hfoption>\n </hfoptions>\n "
        }
    ],
    "stats": {
        "total": 30,
        "additions": 0,
        "deletions": 30
    }
}