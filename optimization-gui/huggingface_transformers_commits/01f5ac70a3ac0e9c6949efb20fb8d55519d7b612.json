{
    "author": "ydshieh",
    "message": "flash attn pytest marker (#41781)\n\n* flash attn marker\n\n* 111\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
    "files": [
        {
            "sha": "910e9fcc176635f1bba29c39732c79cc27b40ea3",
            "filename": "conftest.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/conftest.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/conftest.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/conftest.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -87,6 +87,8 @@ def pytest_configure(config):\n     config.addinivalue_line(\"markers\", \"not_device_test: mark the tests always running on cpu\")\n     config.addinivalue_line(\"markers\", \"torch_compile_test: mark test which tests torch compile functionality\")\n     config.addinivalue_line(\"markers\", \"torch_export_test: mark test which tests torch export functionality\")\n+    config.addinivalue_line(\"markers\", \"flash_attn_test: mark test which tests flash attention functionality\")\n+    config.addinivalue_line(\"markers\", \"flash_attn_3_test: mark test which tests flash attention 3 functionality\")\n \n     os.environ[\"DISABLE_SAFETENSORS_CONVERSION\"] = \"true\"\n "
        },
        {
            "sha": "fb7754d652d93f02093077ed119b5677f971741a",
            "filename": "tests/models/exaone4/test_modeling_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -120,6 +120,7 @@ def test_model_generation_sdpa(self):\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT, text)\n \n+    @pytest.mark.flash_attn_test\n     @slow\n     @require_torch_accelerator\n     @require_flash_attn"
        },
        {
            "sha": "4d958aff70073070ec650e849c22d6ed1ee7cfa1",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -643,6 +643,7 @@ def test_integration_test_4bit_batch2(self):\n         self.assertEqual(batched_generated_texts[0], generated_text_0[0])\n         self.assertEqual(batched_generated_texts[1], generated_text_1[0])\n \n+    @pytest.mark.flash_attn_test\n     @require_flash_attn\n     @require_torch_gpu\n     @require_bitsandbytes"
        },
        {
            "sha": "1a41fecd582d2bf1357c053f5289d5579c8b12ed",
            "filename": "tests/models/ministral/test_modeling_ministral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fministral%2Ftest_modeling_ministral.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -208,6 +208,7 @@ def test_export_text_with_hybrid_cache(self):\n \n         self.assertEqual(export_generated_text, eager_generated_text)\n \n+    @pytest.mark.flash_attn_test\n     @require_flash_attn\n     @slow\n     def test_past_sliding_window_generation(self):"
        },
        {
            "sha": "ac4797102a78b6ddba0ec9c0988043280751f8d4",
            "filename": "tests/models/mistral/test_modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral%2Ftest_modeling_mistral.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -300,6 +300,7 @@ def test_compile_static_cache(self):\n         static_compiled_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, static_compiled_text)\n \n+    @pytest.mark.flash_attn_test\n     @parameterized.expand([(\"flash_attention_2\",), (\"sdpa\",), (\"flex_attention\",), (\"eager\",)])\n     @require_flash_attn\n     @slow"
        },
        {
            "sha": "2c016d861da53fa1f28a160a4e8fc386d97a6196",
            "filename": "tests/models/qwen2/test_modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2%2Ftest_modeling_qwen2.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -274,6 +274,7 @@ def test_export_static_cache(self):\n         ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)\n \n+    @pytest.mark.flash_attn_test\n     @require_flash_attn\n     @slow\n     def test_3b_generation(self):"
        },
        {
            "sha": "b54bf3f6d514d63b831b723d87487e053cb13a60",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -816,6 +816,7 @@ def test_small_model_integration_test_w_audio(self):\n     @slow\n     @require_flash_attn\n     @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n             \"Qwen/Qwen2.5-Omni-7B\","
        },
        {
            "sha": "4a4950c5104fe1cc27bd57a86a453e8b529063ff",
            "filename": "tests/models/qwen2_5_vl/test_modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_modeling_qwen2_5_vl.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -17,6 +17,7 @@\n import tempfile\n import unittest\n \n+import pytest\n import requests\n \n from transformers import (\n@@ -630,6 +631,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n     @slow\n     @require_flash_attn\n     @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n             \"Qwen/Qwen2.5-VL-7B-Instruct\",\n@@ -658,6 +660,7 @@ def test_small_model_integration_test_batch_flashatt2(self):\n     @slow\n     @require_flash_attn\n     @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n         model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n             \"Qwen/Qwen2.5-VL-7B-Instruct\","
        },
        {
            "sha": "e06c2872ed5dda28eba4db47b9431107b66ee404",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -18,6 +18,7 @@\n import tempfile\n import unittest\n \n+import pytest\n import requests\n \n from transformers import (\n@@ -562,6 +563,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n     @slow\n     @require_flash_attn\n     @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen2VLForConditionalGeneration.from_pretrained(\n             \"Qwen/Qwen2-VL-7B-Instruct\",\n@@ -589,6 +591,7 @@ def test_small_model_integration_test_batch_flashatt2(self):\n     @slow\n     @require_flash_attn\n     @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n         model = Qwen2VLForConditionalGeneration.from_pretrained(\n             \"Qwen/Qwen2-VL-7B-Instruct\","
        },
        {
            "sha": "06223b474bebad2a75fbe3c39672685b45614b5d",
            "filename": "tests/models/qwen3/test_modeling_qwen3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3%2Ftest_modeling_qwen3.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -266,6 +266,7 @@ def test_export_static_cache(self):\n \n     @require_flash_attn\n     @slow\n+    @pytest.mark.flash_attn_test\n     def test_600m_generation(self):\n         model_id = \"Qwen/Qwen3-0.6B-Base\"\n         tokenizer = AutoTokenizer.from_pretrained(model_id)"
        },
        {
            "sha": "e752406e0a2a7b97b068bca23f80f6a1e68f0c7f",
            "filename": "tests/models/qwen3_omni_moe/test_modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_modeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_modeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_omni_moe%2Ftest_modeling_qwen3_omni_moe.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -848,6 +848,7 @@ def test_small_model_integration_test_w_audio(self):\n     @slow\n     @require_flash_attn\n     @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n             \"Qwen/Qwen2.5-Omni-7B\","
        },
        {
            "sha": "ea48ef000d42e3f33add31ac03da2277a968548c",
            "filename": "tests/models/qwen3_vl_moe/test_modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen3_vl_moe%2Ftest_modeling_qwen3_vl_moe.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -16,6 +16,8 @@\n import copy\n import unittest\n \n+import pytest\n+\n from transformers import (\n     AutoProcessor,\n     Qwen3VLMoeConfig,\n@@ -513,6 +515,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n     @slow\n     @require_flash_attn\n     @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n             \"Qwen/Qwen3-VL-30B-A3B-Instruct\",\n@@ -545,6 +548,7 @@ def test_small_model_integration_test_batch_flashatt2(self):\n     @slow\n     @require_flash_attn\n     @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n         model = Qwen3VLMoeForConditionalGeneration.from_pretrained(\n             \"Qwen/Qwen3-VL-30B-A3B-Instruct\","
        },
        {
            "sha": "5ad0cf9d7d4c2761eb79f1915727bb557ee1cb14",
            "filename": "tests/models/video_llama_3/test_modeling_video_llama_3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvideo_llama_3%2Ftest_modeling_video_llama_3.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -20,6 +20,7 @@\n import unittest\n \n import numpy as np\n+import pytest\n import requests\n import torch.nn as nn\n from parameterized import parameterized\n@@ -907,6 +908,7 @@ def test_small_model_integration_test_batch_different_resolutions(self):\n \n     @require_flash_attn\n     @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_flashatt2(self):\n         model = VideoLlama3ForConditionalGeneration.from_pretrained(\n             \"lkhl/VideoLLaMA3-2B-Image-HF\",\n@@ -933,6 +935,7 @@ def test_small_model_integration_test_batch_flashatt2(self):\n \n     @require_flash_attn\n     @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n     def test_small_model_integration_test_batch_wo_image_flashatt2(self):\n         model = VideoLlama3ForConditionalGeneration.from_pretrained(\n             \"lkhl/VideoLLaMA3-2B-Image-HF\","
        },
        {
            "sha": "78c694a848fcef06831a35e29a89e170decbdd8d",
            "filename": "tests/quantization/autoawq/test_awq.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/01f5ac70a3ac0e9c6949efb20fb8d55519d7b612/tests%2Fquantization%2Fautoawq%2Ftest_awq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fautoawq%2Ftest_awq.py?ref=01f5ac70a3ac0e9c6949efb20fb8d55519d7b612",
            "patch": "@@ -16,6 +16,8 @@\n import tempfile\n import unittest\n \n+import pytest\n+\n from transformers import AutoConfig, AutoModelForCausalLM, AutoTokenizer, AwqConfig, OPTForCausalLM\n from transformers.testing_utils import (\n     backend_empty_cache,\n@@ -369,6 +371,7 @@ def test_fused_modules_to_not_convert(self):\n     )\n     @require_flash_attn\n     @require_torch_gpu\n+    @pytest.mark.flash_attn_test\n     def test_generation_fused(self):\n         \"\"\"\n         Test generation quality for fused models - single batch case\n@@ -391,6 +394,7 @@ def test_generation_fused(self):\n \n         self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION)\n \n+    @pytest.mark.flash_attn_test\n     @require_flash_attn\n     @require_torch_gpu\n     @unittest.skipIf(\n@@ -443,6 +447,7 @@ def test_generation_llava_fused(self):\n \n         self.assertEqual(outputs[0][\"generated_text\"], EXPECTED_OUTPUT)\n \n+    @pytest.mark.flash_attn_test\n     @require_flash_attn\n     @require_torch_multi_gpu\n     @unittest.skipIf(\n@@ -484,6 +489,7 @@ def test_generation_custom_model(self):\n         outputs = model.generate(**inputs, max_new_tokens=12)\n         self.assertEqual(tokenizer.decode(outputs[0], skip_special_tokens=True), self.EXPECTED_GENERATION_CUSTOM_MODEL)\n \n+    @pytest.mark.flash_attn_test\n     @require_flash_attn\n     @require_torch_multi_gpu\n     @unittest.skip(reason=\"Not enough GPU memory on CI runners\")"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 29,
        "deletions": 0
    }
}