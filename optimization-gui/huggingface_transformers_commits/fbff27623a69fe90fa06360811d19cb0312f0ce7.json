{
    "author": "Rocketknight1",
    "message": "Add warning for stop string edge case (#33169)\n\n* Add warning for edge case\r\n\r\n* make fixup",
    "sha": "fbff27623a69fe90fa06360811d19cb0312f0ce7",
    "files": [
        {
            "sha": "7e98b11cf01a1c0919926a43baf1214f9c40b2ef",
            "filename": "src/transformers/generation/stopping_criteria.py",
            "status": "modified",
            "additions": 8,
            "deletions": 1,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/fbff27623a69fe90fa06360811d19cb0312f0ce7/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fbff27623a69fe90fa06360811d19cb0312f0ce7/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fstopping_criteria.py?ref=fbff27623a69fe90fa06360811d19cb0312f0ce7",
            "patch": "@@ -348,7 +348,14 @@ def _stop_string_create_embedding_vec(token_list, token_indices, stop_strings) -\n         # we need a fallback to handle this case\n         max_valid_positions = max(all_valid_positions) if all_valid_positions else 1\n         # There should always be at least one valid end_len, however, so no fallback needed here\n-        max_valid_end_lens = max(len(val) for positions in token_end_overlaps.values() for val in positions.values())\n+        valid_end_lens = [len(val) for positions in token_end_overlaps.values() for val in positions.values()]\n+        if not valid_end_lens:\n+            raise ValueError(\n+                \"Stop string preprocessing was unable to identify tokens matching one or more of the \"\n+                \"supplied stop string(s). This is most often caused by the stop \"\n+                \"strings containing unusual characters that are not in the tokenizer vocabulary.\"\n+            )\n+        max_valid_end_lens = max(valid_end_lens)\n         vec_size = len(stop_strings) * (max_valid_positions + max_valid_end_lens) + 1\n         gather_vec = np.full((len(token_list), vec_size), dtype=np.int32, fill_value=-1)\n "
        }
    ],
    "stats": {
        "total": 9,
        "additions": 8,
        "deletions": 1
    }
}