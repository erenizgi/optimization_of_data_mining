{
    "author": "jiqing-feng",
    "message": "fix static cache data type miss-match (#34799)\n\n* fix gptj data type missmatch\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* add low precision static cache tests\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix format\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix low-precision static cache tests\r\n\r\n* fix format\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* avoid config change\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* change data type convert in cache copy\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* fix comment\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n* cast key value after k v out\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\r\n\r\n---------\r\n\r\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "a464afbe2ac6b1d866d6850fa509f9e8dd244a92",
    "files": [
        {
            "sha": "f3f0bd6fe5458f1b25b85af357c797c5ee9ec150",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a464afbe2ac6b1d866d6850fa509f9e8dd244a92/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a464afbe2ac6b1d866d6850fa509f9e8dd244a92/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=a464afbe2ac6b1d866d6850fa509f9e8dd244a92",
            "patch": "@@ -1217,6 +1217,8 @@ def update(\n \n         k_out = self.key_cache[layer_idx]\n         v_out = self.value_cache[layer_idx]\n+        key_states = key_states.to(k_out.dtype)\n+        value_states = value_states.to(v_out.dtype)\n \n         if cache_position is None:\n             k_out.copy_(key_states)"
        },
        {
            "sha": "0605ea7939714c9035adb5ff5398e5833a592a5a",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 33,
            "deletions": 28,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/a464afbe2ac6b1d866d6850fa509f9e8dd244a92/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a464afbe2ac6b1d866d6850fa509f9e8dd244a92/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=a464afbe2ac6b1d866d6850fa509f9e8dd244a92",
            "patch": "@@ -1901,36 +1901,41 @@ def test_generate_with_static_cache(self):\n             seq_length = main_input.shape[-1]\n             max_new_tokens = 20\n \n-            model = model_class(config).to(torch_device).eval()\n-            generation_kwargs = {\n-                \"max_new_tokens\": max_new_tokens,\n-                \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n-                \"output_scores\": True,\n-                \"use_cache\": True,\n-            }\n-\n-            static_cache_generation = model.generate(**generation_kwargs, **inputs_dict, cache_implementation=\"static\")\n+            for dtype in (torch.float32, torch.float16):\n+                model = model_class(config).to(torch_device).to(dtype).eval()\n+                generation_kwargs = {\n+                    \"max_new_tokens\": max_new_tokens,\n+                    \"return_dict_in_generate\": True,  # Required to return `past_key_values`\n+                    \"output_scores\": True,\n+                    \"use_cache\": True,\n+                }\n \n-            # Check 1: The cache shapes must match the expected shapes\n-            max_cache_len = seq_length + max_new_tokens\n-            config = config.text_config if hasattr(config, \"text_config\") else config\n-            head_dim = (\n-                config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n-            )\n-            num_key_value_heads = (\n-                config.num_attention_heads\n-                if getattr(config, \"num_key_value_heads\", None) is None\n-                else config.num_key_value_heads\n-            )\n-            num_hidden_layers = config.num_hidden_layers\n-            cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n-            self.assertTrue(isinstance(static_cache_generation.past_key_values, StaticCache))\n-            self.assertTrue(len(static_cache_generation.past_key_values.key_cache) == num_hidden_layers)\n-            self.assertTrue(static_cache_generation.past_key_values.key_cache[0].shape == cache_shape)\n+                static_cache_generation = model.generate(\n+                    **generation_kwargs, **inputs_dict, cache_implementation=\"static\"\n+                )\n \n-            # Check 2: The outputs must be similar to the case with dynamic cache\n-            dynamic_cache_generation = model.generate(**generation_kwargs, **inputs_dict)\n-            self._check_similar_generate_outputs(dynamic_cache_generation, static_cache_generation)\n+                # Check 1: The cache shapes must match the expected shapes\n+                max_cache_len = seq_length + max_new_tokens\n+                text_config = config.text_config if hasattr(config, \"text_config\") else config\n+                head_dim = (\n+                    text_config.head_dim\n+                    if hasattr(text_config, \"head_dim\")\n+                    else text_config.hidden_size // text_config.num_attention_heads\n+                )\n+                num_key_value_heads = (\n+                    text_config.num_attention_heads\n+                    if getattr(text_config, \"num_key_value_heads\", None) is None\n+                    else text_config.num_key_value_heads\n+                )\n+                num_hidden_layers = text_config.num_hidden_layers\n+                cache_shape = (batch_size, num_key_value_heads, max_cache_len, head_dim)\n+                self.assertTrue(isinstance(static_cache_generation.past_key_values, StaticCache))\n+                self.assertTrue(len(static_cache_generation.past_key_values.key_cache) == num_hidden_layers)\n+                self.assertTrue(static_cache_generation.past_key_values.key_cache[0].shape == cache_shape)\n+\n+                # Check 2: The outputs must be similar to the case with dynamic cache\n+                dynamic_cache_generation = model.generate(**generation_kwargs, **inputs_dict)\n+                self._check_similar_generate_outputs(dynamic_cache_generation, static_cache_generation)\n \n     @require_optimum_quanto\n     @pytest.mark.generate"
        }
    ],
    "stats": {
        "total": 63,
        "additions": 35,
        "deletions": 28
    }
}