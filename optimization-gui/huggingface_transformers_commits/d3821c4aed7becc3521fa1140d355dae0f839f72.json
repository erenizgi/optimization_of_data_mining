{
    "author": "Rocketknight1",
    "message": "Make audio classification pipeline spec-compliant and add test (#33730)\n\n* Make audio classification pipeline spec-compliant and add test\r\n\r\n* Check that test actually running in CI\r\n\r\n* Try a different pipeline for the CI\r\n\r\n* Move the test so it gets triggered\r\n\r\n* Move it again, this time into task_tests!\r\n\r\n* make fixup\r\n\r\n* indentation fix\r\n\r\n* comment\r\n\r\n* Move everything from testing_utils to test_pipeline_mixin\r\n\r\n* Add output testing too\r\n\r\n* revert small diff with main\r\n\r\n* make fixup\r\n\r\n* Clarify comment\r\n\r\n* Update tests/pipelines/test_pipelines_audio_classification.py\r\n\r\nCo-authored-by: Lucain <lucainp@gmail.com>\r\n\r\n* Update tests/test_pipeline_mixin.py\r\n\r\nCo-authored-by: Lucain <lucainp@gmail.com>\r\n\r\n* Rename function and js_args -> hub_args\r\n\r\n* Cleanup the spec recursion\r\n\r\n* Check keys for all outputs\r\n\r\n---------\r\n\r\nCo-authored-by: Lucain <lucainp@gmail.com>",
    "sha": "d3821c4aed7becc3521fa1140d355dae0f839f72",
    "files": [
        {
            "sha": "089c32d502d121307f8f1b5f95100817b09f99bd",
            "filename": "src/transformers/pipelines/audio_classification.py",
            "status": "modified",
            "additions": 22,
            "deletions": 3,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3821c4aed7becc3521fa1140d355dae0f839f72/src%2Ftransformers%2Fpipelines%2Faudio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3821c4aed7becc3521fa1140d355dae0f839f72/src%2Ftransformers%2Fpipelines%2Faudio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Faudio_classification.py?ref=d3821c4aed7becc3521fa1140d355dae0f839f72",
            "patch": "@@ -126,6 +126,11 @@ def __call__(\n                 The number of top labels that will be returned by the pipeline. If the provided number is `None` or\n                 higher than the number of labels available in the model configuration, it will default to the number of\n                 labels.\n+            function_to_apply(`str`, *optional*, defaults to \"softmax\"):\n+                The function to apply to the model output. By default, the pipeline will apply the softmax function to\n+                the output of the model. Valid options: [\"softmax\", \"sigmoid\", \"none\"]. Note that passing Python's\n+                built-in `None` will default to \"softmax\", so you need to pass the string \"none\" to disable any\n+                post-processing.\n \n         Return:\n             A list of `dict` with the following keys:\n@@ -135,13 +140,22 @@ def __call__(\n         \"\"\"\n         return super().__call__(inputs, **kwargs)\n \n-    def _sanitize_parameters(self, top_k=None, **kwargs):\n+    def _sanitize_parameters(self, top_k=None, function_to_apply=None, **kwargs):\n         # No parameters on this pipeline right now\n         postprocess_params = {}\n         if top_k is not None:\n             if top_k > self.model.config.num_labels:\n                 top_k = self.model.config.num_labels\n             postprocess_params[\"top_k\"] = top_k\n+        if function_to_apply is not None:\n+            if function_to_apply not in [\"softmax\", \"sigmoid\", \"none\"]:\n+                raise ValueError(\n+                    f\"Invalid value for `function_to_apply`: {function_to_apply}. \"\n+                    \"Valid options are ['softmax', 'sigmoid', 'none']\"\n+                )\n+            postprocess_params[\"function_to_apply\"] = function_to_apply\n+        else:\n+            postprocess_params[\"function_to_apply\"] = \"softmax\"\n         return {}, {}, postprocess_params\n \n     def preprocess(self, inputs):\n@@ -203,8 +217,13 @@ def _forward(self, model_inputs):\n         model_outputs = self.model(**model_inputs)\n         return model_outputs\n \n-    def postprocess(self, model_outputs, top_k=5):\n-        probs = model_outputs.logits[0].softmax(-1)\n+    def postprocess(self, model_outputs, top_k=5, function_to_apply=\"softmax\"):\n+        if function_to_apply == \"softmax\":\n+            probs = model_outputs.logits[0].softmax(-1)\n+        elif function_to_apply == \"sigmoid\":\n+            probs = model_outputs.logits[0].sigmoid()\n+        else:\n+            probs = model_outputs.logits[0]\n         scores, ids = probs.topk(top_k)\n \n         scores = scores.tolist()"
        },
        {
            "sha": "a8e36b4a07212ec893d5ab24a9c01decc29de0c2",
            "filename": "tests/pipelines/test_pipelines_audio_classification.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3821c4aed7becc3521fa1140d355dae0f839f72/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3821c4aed7becc3521fa1140d355dae0f839f72/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fpipelines%2Ftest_pipelines_audio_classification.py?ref=d3821c4aed7becc3521fa1140d355dae0f839f72",
            "patch": "@@ -13,8 +13,10 @@\n # limitations under the License.\n \n import unittest\n+from dataclasses import fields\n \n import numpy as np\n+from huggingface_hub import AudioClassificationOutputElement\n \n from transformers import MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING, TF_MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING\n from transformers.pipelines import AudioClassificationPipeline, pipeline\n@@ -66,6 +68,11 @@ def run_pipeline_test(self, audio_classifier, examples):\n \n         self.run_torchaudio(audio_classifier)\n \n+        spec_output_keys = {field.name for field in fields(AudioClassificationOutputElement)}\n+        for single_output in output:\n+            output_keys = set(single_output.keys())\n+            self.assertEqual(spec_output_keys, output_keys, msg=\"Pipeline output keys do not match HF Hub spec!\")\n+\n     @require_torchaudio\n     def run_torchaudio(self, audio_classifier):\n         import datasets"
        },
        {
            "sha": "74e685fb112c29f145235a1332b7d4f18de29e24",
            "filename": "tests/test_pipeline_mixin.py",
            "status": "modified",
            "additions": 101,
            "deletions": 0,
            "changes": 101,
            "blob_url": "https://github.com/huggingface/transformers/blob/d3821c4aed7becc3521fa1140d355dae0f839f72/tests%2Ftest_pipeline_mixin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d3821c4aed7becc3521fa1140d355dae0f839f72/tests%2Ftest_pipeline_mixin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_pipeline_mixin.py?ref=d3821c4aed7becc3521fa1140d355dae0f839f72",
            "patch": "@@ -14,12 +14,20 @@\n # limitations under the License.\n \n import copy\n+import inspect\n import json\n import os\n import random\n+import re\n import unittest\n+from dataclasses import fields, is_dataclass\n from pathlib import Path\n+from textwrap import dedent\n+from typing import get_args\n \n+from huggingface_hub import AudioClassificationInput\n+\n+from transformers.pipelines import AudioClassificationPipeline\n from transformers.testing_utils import (\n     is_pipeline_test,\n     require_decord,\n@@ -92,6 +100,12 @@\n     \"zero-shot-object-detection\": {\"test\": ZeroShotObjectDetectionPipelineTests},\n }\n \n+task_to_pipeline_and_spec_mapping = {\n+    # Adding a task to this list will cause its pipeline input signature to be checked against the corresponding\n+    # task spec in the HF Hub\n+    \"audio-classification\": (AudioClassificationPipeline, AudioClassificationInput),\n+}\n+\n for task, task_info in pipeline_test_mapping.items():\n     test = task_info[\"test\"]\n     task_info[\"mapping\"] = {\n@@ -175,6 +189,9 @@ def run_task_tests(self, task, torch_dtype=\"float32\"):\n             self.run_model_pipeline_tests(\n                 task, repo_name, model_architecture, tokenizer_names, processor_names, commit, torch_dtype\n             )\n+        if task in task_to_pipeline_and_spec_mapping:\n+            pipeline, hub_spec = task_to_pipeline_and_spec_mapping[task]\n+            compare_pipeline_args_to_hub_spec(pipeline, hub_spec)\n \n     def run_model_pipeline_tests(\n         self, task, repo_name, model_architecture, tokenizer_names, processor_names, commit, torch_dtype=\"float32\"\n@@ -685,3 +702,87 @@ def validate_test_components(test_case, task, model, tokenizer, processor):\n             raise ValueError(\n                 \"Could not determine `vocab_size` from model configuration while `tokenizer` is not `None`.\"\n             )\n+\n+\n+def get_arg_names_from_hub_spec(hub_spec, first_level=True):\n+    # This util is used in pipeline tests, to verify that a pipeline's documented arguments\n+    # match the Hub specification for that task\n+    arg_names = []\n+    for field in fields(hub_spec):\n+        # Recurse into nested fields, but max one level\n+        if is_dataclass(field.type):\n+            arg_names.extend([field.name for field in fields(field.type)])\n+            continue\n+        # Next, catch nested fields that are part of a Union[], which is usually caused by Optional[]\n+        for param_type in get_args(field.type):\n+            if is_dataclass(param_type):\n+                # Again, recurse into nested fields, but max one level\n+                arg_names.extend([field.name for field in fields(param_type)])\n+                break\n+        else:\n+            # Finally, this line triggers if it's not a nested field\n+            arg_names.append(field.name)\n+    return arg_names\n+\n+\n+def parse_args_from_docstring_by_indentation(docstring):\n+    # This util is used in pipeline tests, to extract the argument names from a google-format docstring\n+    # to compare them against the Hub specification for that task. It uses indentation levels as a primary\n+    # source of truth, so these have to be correct!\n+    docstring = dedent(docstring)\n+    lines_by_indent = [\n+        (len(line) - len(line.lstrip()), line.strip()) for line in docstring.split(\"\\n\") if line.strip()\n+    ]\n+    args_lineno = None\n+    args_indent = None\n+    args_end = None\n+    for lineno, (indent, line) in enumerate(lines_by_indent):\n+        if line == \"Args:\":\n+            args_lineno = lineno\n+            args_indent = indent\n+            continue\n+        elif args_lineno is not None and indent == args_indent:\n+            args_end = lineno\n+            break\n+    if args_lineno is None:\n+        raise ValueError(\"No args block to parse!\")\n+    elif args_end is None:\n+        args_block = lines_by_indent[args_lineno + 1 :]\n+    else:\n+        args_block = lines_by_indent[args_lineno + 1 : args_end]\n+    outer_indent_level = min(line[0] for line in args_block)\n+    outer_lines = [line for line in args_block if line[0] == outer_indent_level]\n+    arg_names = [re.match(r\"(\\w+)\\W\", line[1]).group(1) for line in outer_lines]\n+    return arg_names\n+\n+\n+def compare_pipeline_args_to_hub_spec(pipeline_class, hub_spec):\n+    docstring = inspect.getdoc(pipeline_class.__call__).strip()\n+    docstring_args = set(parse_args_from_docstring_by_indentation(docstring))\n+    hub_args = set(get_arg_names_from_hub_spec(hub_spec))\n+\n+    # Special casing: We allow the name of this arg to differ\n+    js_generate_args = [js_arg for js_arg in hub_args if js_arg.startswith(\"generate\")]\n+    docstring_generate_args = [\n+        docstring_arg for docstring_arg in docstring_args if docstring_arg.startswith(\"generate\")\n+    ]\n+    if (\n+        len(js_generate_args) == 1\n+        and len(docstring_generate_args) == 1\n+        and js_generate_args != docstring_generate_args\n+    ):\n+        hub_args.remove(js_generate_args[0])\n+        docstring_args.remove(docstring_generate_args[0])\n+\n+    if hub_args != docstring_args:\n+        error = [f\"{pipeline_class.__name__} differs from JS spec {hub_spec.__name__}\"]\n+        matching_args = hub_args & docstring_args\n+        huggingface_hub_only = hub_args - docstring_args\n+        transformers_only = docstring_args - hub_args\n+        if matching_args:\n+            error.append(f\"Matching args: {matching_args}\")\n+        if huggingface_hub_only:\n+            error.append(f\"Huggingface Hub only: {huggingface_hub_only}\")\n+        if transformers_only:\n+            error.append(f\"Transformers only: {transformers_only}\")\n+        raise ValueError(\"\\n\".join(error))"
        }
    ],
    "stats": {
        "total": 133,
        "additions": 130,
        "deletions": 3
    }
}