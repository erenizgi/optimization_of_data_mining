{
    "author": "gante",
    "message": "Fix chat (#39128)",
    "sha": "ea0ea392e57f8816f9ab8e5f740577a0343a1594",
    "files": [
        {
            "sha": "91979590046034c4f39bc3e49d1656c5f9aede85",
            "filename": "src/transformers/commands/chat.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ea0ea392e57f8816f9ab8e5f740577a0343a1594/src%2Ftransformers%2Fcommands%2Fchat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ea0ea392e57f8816f9ab8e5f740577a0343a1594/src%2Ftransformers%2Fcommands%2Fchat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcommands%2Fchat.py?ref=ea0ea392e57f8816f9ab8e5f740577a0343a1594",
            "patch": "@@ -636,7 +636,6 @@ def run(self):\n     async def _inner_run(self):\n         if self.spawn_backend:\n             serve_args = ServeArguments(\n-                model_revision=self.args.model_revision,\n                 device=self.args.device,\n                 torch_dtype=self.args.torch_dtype,\n                 trust_remote_code=self.args.trust_remote_code,\n@@ -649,13 +648,13 @@ async def _inner_run(self):\n                 port=self.args.port,\n                 log_level=\"error\",\n             )\n-            serve_args.model_name_or_path = self.args.model_name_or_path\n             serve_command = ServeCommand(serve_args)\n \n             thread = Thread(target=serve_command.run)\n             thread.daemon = True\n             thread.start()\n \n+        model = self.args.model_name_or_path + \"@\" + self.args.model_revision\n         host = \"http://localhost\" if self.args.host == \"localhost\" else self.args.host\n         client = AsyncInferenceClient(f\"{host}:{self.args.port}\")\n \n@@ -709,7 +708,11 @@ async def _inner_run(self):\n                 stream = client.chat_completion(\n                     chat,\n                     stream=True,\n-                    extra_body={\"request_id\": request_id, \"generation_config\": {**generation_config.to_dict()}},\n+                    extra_body={\n+                        \"request_id\": request_id,\n+                        \"generation_config\": {**generation_config.to_dict()},\n+                        \"model\": model,\n+                    },\n                 )\n \n                 model_output, request_id = await interface.stream_output(stream)"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 6,
        "deletions": 3
    }
}