{
    "author": "llllvvuu",
    "message": "fix(qwen3_moe): pass kwargs to self_attn (#38691)\n\nThis is needed to avoid `.item()` calls in `_flash_attention_forward`.",
    "sha": "b84ebb7f3cb6b0e2b24277944f30ed11e4d83c57",
    "files": [
        {
            "sha": "d4ba6c3656906483649ad3dd354be20720c98c09",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b84ebb7f3cb6b0e2b24277944f30ed11e4d83c57/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b84ebb7f3cb6b0e2b24277944f30ed11e4d83c57/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=b84ebb7f3cb6b0e2b24277944f30ed11e4d83c57",
            "patch": "@@ -355,6 +355,7 @@ def forward(\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n "
        },
        {
            "sha": "6f821d5b81446021550d72dfbfe0cdac9323ac44",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b84ebb7f3cb6b0e2b24277944f30ed11e4d83c57/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b84ebb7f3cb6b0e2b24277944f30ed11e4d83c57/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=b84ebb7f3cb6b0e2b24277944f30ed11e4d83c57",
            "patch": "@@ -189,6 +189,7 @@ def forward(\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n+            **kwargs,\n         )\n         hidden_states = residual + hidden_states\n "
        }
    ],
    "stats": {
        "total": 2,
        "additions": 2,
        "deletions": 0
    }
}