{
    "author": "zucchini-nlp",
    "message": "Offloaded cache: fix generate (#34921)\n\n* fix cache impl\r\n\r\n* require_torch_gpu\r\n\r\n* fix mamba\r\n\r\n* fix copies",
    "sha": "5e8c1d713d0e8177163dbbc2a30d72f099fb48a6",
    "files": [
        {
            "sha": "9d4d90f11221dbcf57362df291d8677d1b2549f8",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 46,
            "deletions": 16,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=5e8c1d713d0e8177163dbbc2a30d72f099fb48a6",
            "patch": "@@ -1140,13 +1140,13 @@ def __init__(\n         layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         super().__init__()\n-        if max_batch_size is not None:\n+        if batch_size is not None:\n             logger.warning_once(\n-                f\"The 'max_batch_size' argument of {self.__class__.__name__} is deprecated and will be removed in \"\n-                \"v4.46. Use the more precisely named 'batch_size' argument instead.\"\n+                f\"The 'batch_size' argument of {self.__class__.__name__} is deprecated and will be removed in \"\n+                \"v4.49. Use the more precisely named 'max_batch_size' argument instead.\"\n             )\n \n-        self.batch_size = batch_size or max_batch_size\n+        self.max_batch_size = batch_size or max_batch_size\n         self.max_cache_len = config.max_position_embeddings if max_cache_len is None else max_cache_len\n \n         # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n@@ -1254,6 +1254,14 @@ def reset(self):\n             self.key_cache[layer_idx].zero_()\n             self.value_cache[layer_idx].zero_()\n \n+    @property\n+    def batch_size(self):\n+        logger.warning_once(\n+            f\"The 'batch_size' attribute of {self.__class__.__name__} is deprecated and will be removed in \"\n+            \"v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\"\n+        )\n+        return self.max_batch_size\n+\n \n class SlidingWindowCache(StaticCache):\n     \"\"\"\n@@ -1626,10 +1634,10 @@ def __init__(\n         layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         super().__init__()\n-        if max_batch_size is not None:\n+        if batch_size is not None:\n             logger.warning_once(\n-                f\"The 'max_batch_size' argument of {self.__class__.__name__} is deprecated and will be removed in \"\n-                \"v4.46. Use the more precisely named 'batch_size' argument instead.\"\n+                f\"The 'batch_size' argument of {self.__class__.__name__} is deprecated and will be removed in \"\n+                \"v4.49. Use the more precisely named 'max_batch_size' argument instead.\"\n             )\n         if not hasattr(config, \"sliding_window\") or config.sliding_window is None:\n             raise ValueError(\n@@ -1638,7 +1646,7 @@ def __init__(\n                 \"config and it's not set to None.\"\n             )\n         self.max_cache_len = max_cache_len\n-        self.batch_size = batch_size or max_batch_size\n+        self.max_batch_size = batch_size or max_batch_size\n         # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n         self.head_dim = (\n             config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n@@ -1758,6 +1766,14 @@ def reset(self):\n             self.key_cache[layer_idx].zero_()\n             self.value_cache[layer_idx].zero_()\n \n+    @property\n+    def batch_size(self):\n+        logger.warning_once(\n+            f\"The 'batch_size' attribute of {self.__class__.__name__} is deprecated and will be removed in \"\n+            \"v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\"\n+        )\n+        return self.max_batch_size\n+\n \n class MambaCache:\n     \"\"\"\n@@ -1815,28 +1831,28 @@ def __init__(\n         device: Optional[Union[torch.device, str]] = None,\n         max_batch_size: Optional[int] = None,\n     ):\n-        if max_batch_size is not None:\n+        if batch_size is not None:\n             logger.warning_once(\n-                f\"The 'max_batch_size' argument of {self.__class__.__name__} is deprecated and will be removed in \"\n-                \"v4.46. Use the more precisely named 'batch_size' argument instead.\"\n+                f\"The 'batch_size' argument of {self.__class__.__name__} is deprecated and will be removed in \"\n+                \"v4.49. Use the more precisely named 'max_batch_size' argument instead.\"\n             )\n         self.dtype = dtype\n-        self.batch_size = batch_size or max_batch_size\n+        self.max_batch_size = batch_size or max_batch_size\n         self.intermediate_size = config.intermediate_size\n         self.ssm_state_size = config.state_size\n         self.conv_kernel_size = config.conv_kernel\n \n         self.conv_states: torch.Tensor = torch.zeros(\n             config.num_hidden_layers,\n-            self.batch_size,\n+            self.max_batch_size,\n             self.intermediate_size,\n             self.conv_kernel_size,\n             device=device,\n             dtype=dtype,\n         )\n         self.ssm_states: torch.Tensor = torch.zeros(\n             config.num_hidden_layers,\n-            self.batch_size,\n+            self.max_batch_size,\n             self.intermediate_size,\n             self.ssm_state_size,\n             device=device,\n@@ -1866,6 +1882,14 @@ def reset(self):\n         self.conv_states.zero_()\n         self.ssm_states.zero_()\n \n+    @property\n+    def batch_size(self):\n+        logger.warning_once(\n+            f\"The 'batch_size' attribute of {self.__class__.__name__} is deprecated and will be removed in \"\n+            \"v4.49. Use the more precisely named 'self.max_batch_size' attribute instead.\"\n+        )\n+        return self.max_batch_size\n+\n \n class OffloadedStaticCache(StaticCache):\n     \"\"\"\n@@ -1887,6 +1911,9 @@ class OffloadedStaticCache(StaticCache):\n             The default `dtype` to use when initializing the cache.\n         offload_device (`Union[str, torch.device]`, *optional*, defaults to `cpu`):\n             The device to offload to. Defaults to CPU.\n+        layer_device_map (`Dict[int, Union[str, torch.device, int]]`, *optional*):\n+            Mapping between the layers and its device. This is required when you are manually initializing the cache and the model is splitted between differents gpus.\n+            You can know which layers mapped to which device by checking the associated device_map: `model.hf_device_map`.\n \n     Attributes:\n         key_cache (`List[torch.Tensor]`):\n@@ -1933,18 +1960,21 @@ def __init__(\n         device: Union[str, torch.device],\n         dtype: Optional[torch.dtype] = None,\n         offload_device: Union[str, torch.device] = torch.device(\"cpu\"),\n+        layer_device_map: Optional[Dict[int, Union[str, torch.device, int]]] = None,\n     ) -> None:\n         self.max_batch_size = max_batch_size\n         self.max_cache_len = config.max_position_embeddings if max_cache_len is None else max_cache_len\n-        self.device = torch.device(device)\n+        self.device = torch.device(device) if layer_device_map is None else layer_device_map[0]\n         self.offload_device = torch.device(offload_device)\n         self.dtype = dtype if dtype is not None else torch.float32\n \n         # Some model define a custom `head_dim` != config.hidden_size // config.num_attention_heads\n         head_dim = config.head_dim if hasattr(config, \"head_dim\") else config.hidden_size // config.num_attention_heads\n \n         num_key_value_heads = (\n-            config.num_attention_heads if config.num_key_value_heads is None else config.num_key_value_heads\n+            config.num_attention_heads\n+            if getattr(config, \"num_key_value_heads\", None) is None\n+            else config.num_key_value_heads\n         )\n \n         cache_shape = (max_batch_size, num_key_value_heads, self.max_cache_len, head_dim)"
        },
        {
            "sha": "30a632aa8cca6a44887192a213ad69c770bd103c",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=5e8c1d713d0e8177163dbbc2a30d72f099fb48a6",
            "patch": "@@ -72,7 +72,9 @@\n         \"mamba\": MambaCache,\n     }\n     QUANT_BACKEND_CLASSES_MAPPING = {\"quanto\": QuantoQuantizedCache, \"HQQ\": HQQQuantizedCache}\n-    ALL_CACHE_IMPLEMENTATIONS = list(NEED_SETUP_CACHE_CLASSES_MAPPING.keys()) + list(NEEDS_CACHE_CONFIG.keys())\n+    ALL_CACHE_IMPLEMENTATIONS = (\n+        list(NEED_SETUP_CACHE_CLASSES_MAPPING.keys()) + list(NEEDS_CACHE_CONFIG.keys()) + [\"offloaded\"]\n+    )\n \n \n class GenerationMode(ExplicitEnum):"
        },
        {
            "sha": "05e39c4a9b56b5c103ea7a92d2235e57b38dda8e",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=5e8c1d713d0e8177163dbbc2a30d72f099fb48a6",
            "patch": "@@ -1610,7 +1610,7 @@ def _get_cache(\n         need_new_cache = (\n             not hasattr(self, \"_cache\")\n             or (not isinstance(cache_to_check, cache_cls))\n-            or cache_to_check.batch_size != batch_size\n+            or cache_to_check.max_batch_size != batch_size\n         )\n         if cache_implementation != \"mamba\":\n             need_new_cache = need_new_cache or cache_to_check.max_cache_len < max_cache_len\n@@ -1666,7 +1666,7 @@ def get_layer_device_map(execution_device_map: Optional[dict] = None):\n \n             cache_kwargs = {\n                 \"config\": self.config.get_text_config(),\n-                \"batch_size\": batch_size,\n+                \"max_batch_size\": batch_size,\n                 \"max_cache_len\": max_cache_len,\n                 \"device\": device,\n                 \"dtype\": cache_dtype,"
        },
        {
            "sha": "063e9a3da8fdade6c7d64c05efd06eb5ce598a67",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 26,
            "deletions": 0,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=5e8c1d713d0e8177163dbbc2a30d72f099fb48a6",
            "patch": "@@ -1880,6 +1880,32 @@ def test_new_cache_format(self, num_beams, do_sample):\n                             )\n                         )\n \n+    @parameterized.expand([(\"offloaded\",)])  # (\"offloaded_static\",) TODO: @raushan fixme in some models (eg T5)\n+    @require_torch_gpu\n+    @pytest.mark.generate\n+    def test_offloaded_cache_implementation(self, cache_implementation):\n+        \"\"\"Tests we can generate by indicating `cache_implementation` for each possible cache class\"\"\"\n+        for model_class in self.all_generative_model_classes:\n+            if not model_class._supports_cache_class:\n+                self.skipTest(reason=\"This model does not support the new cache format\")\n+\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+\n+            model = model_class(config).to(torch_device).eval()\n+            generation_kwargs = {\n+                \"max_new_tokens\": 5,\n+                \"use_cache\": True,\n+                \"cache_implementation\": cache_implementation,\n+            }\n+\n+            legacy_results = model.generate(**generation_kwargs, **inputs_dict)\n+\n+            # Most cache classes have their own tests except for some that are tested here\n+            # The ones here do not need special treatment when passing `cache_implementation`\n+            # and are not bound to specific models only\n+            new_results = model.generate(**generation_kwargs, **inputs_dict)\n+            self.assertListEqual(legacy_results.tolist(), new_results.tolist())\n+\n     @pytest.mark.generate\n     def test_generate_with_static_cache(self):\n         \"\"\""
        },
        {
            "sha": "cfd64aee5368f32e5111fc341f635f3646e27ee9",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=5e8c1d713d0e8177163dbbc2a30d72f099fb48a6",
            "patch": "@@ -16,7 +16,9 @@\n \n import unittest\n \n+import pytest\n import requests\n+from parameterized import parameterized\n \n from transformers import (\n     AutoProcessor,\n@@ -365,6 +367,12 @@ def test_sdpa_can_compile_dynamic(self):\n     def test_model_parallelism(self):\n         pass\n \n+    @parameterized.expand([(\"offloaded\",)])\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"Offloaded cache seems to not work with mllama's kv cache type\")\n+    def test_offloaded_cache_implementation(self, cache_implementation):\n+        pass\n+\n     def test_generate_text_only_with_cache(self):\n         \"\"\"\n         Tests that our cached generation with text-only inputs works. When mllama was introduced, this feature"
        },
        {
            "sha": "9389c4f47def1b7e5fd37e0e05d499e55834990b",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5e8c1d713d0e8177163dbbc2a30d72f099fb48a6/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=5e8c1d713d0e8177163dbbc2a30d72f099fb48a6",
            "patch": "@@ -567,6 +567,12 @@ def test_training_gradient_checkpointing_use_reentrant_false(self):\n     def test_generate_with_head_masking(self):\n         pass\n \n+    @parameterized.expand([(\"offloaded\",)])\n+    @pytest.mark.generate\n+    @unittest.skip(reason=\"Whisper doesnt work with offloaded cache implementation yet\")\n+    def test_offloaded_cache_implementation(self, cache_implementation):\n+        pass\n+\n     @require_torch_fp16\n     def test_generate_fp16(self):\n         config, input_dict = self.model_tester.prepare_config_and_inputs()"
        }
    ],
    "stats": {
        "total": 110,
        "additions": 91,
        "deletions": 19
    }
}