{
    "author": "zucchini-nlp",
    "message": "Prefill-related logic in input preparation for generation (#42088)\n\n* add prefill arg in generation\n\n* add a slow test\n\n* fix copies\n\n* can be like this but checking special tokens isn't good\n\n* ig this solves the issue with assisted_gen+prefill\n\n* update overwritten `prepare_inpits_for_generation`\n\n* prefill is actually when we have no cache at all.. Try this for now\n\n* first iteration is not always techincally same as prefill\n\n* fix?\n\n* fix now?\n\n* update bloom\n\n* fix smth\n\n* make style\n\n* fix copies and skip test\n\n* fix copies\n\n* tiny updates after a review\n\n* fix other slow tests\n\n* fix copies\n\n* do not pass the same kwargs twice in prefill\n\n* oops\n\n* have to revert? prob fails only on dgx\n\n* adjust slow test again\n\n* address comments\n\n* fix copies",
    "sha": "ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
    "files": [
        {
            "sha": "b978ae3f11c199ad9122a209fad1e6e8a200176a",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 62,
            "deletions": 19,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -41,7 +41,9 @@\n class CandidateGenerator:\n     \"\"\"Abstract base class for all candidate generators that can be applied during assisted generation.\"\"\"\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n+    def get_candidates(\n+        self, input_ids: torch.LongTensor, is_first_iteration: bool\n+    ) -> tuple[torch.LongTensor, torch.FloatTensor]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -200,7 +202,9 @@ def __init__(\n             self.probs = []\n             self.matches = []\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n+    def get_candidates(\n+        self, input_ids: torch.LongTensor, is_first_iteration: bool\n+    ) -> tuple[torch.LongTensor, torch.FloatTensor]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -221,7 +225,7 @@ def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor,\n         # Update past key values and masks\n         self._update_past_and_masks(input_ids)\n         # Generate candidates\n-        generation_args = self._prepare_generation_args(input_ids, min_new_tokens, max_new_tokens)\n+        generation_args = self._prepare_generation_args(input_ids, min_new_tokens, max_new_tokens, is_first_iteration)\n         candidate_ids, candidate_logits = self._generate_candidates(generation_args)\n         return candidate_ids, candidate_logits\n \n@@ -309,19 +313,46 @@ def _update_past_and_masks(\n \n         return has_past_key_values\n \n-    def _prepare_generation_args(self, input_ids: torch.LongTensor, min_new_tokens: int, max_new_tokens: int) -> dict:\n+    def _prepare_generation_args(\n+        self, input_ids: torch.LongTensor, min_new_tokens: int, max_new_tokens: int, is_first_iteration: bool\n+    ) -> dict:\n         \"\"\"Prepare arguments for the generation call.\"\"\"\n-        return {\n-            self.input_ids_key: input_ids,\n-            \"min_new_tokens\": min_new_tokens,\n-            \"max_new_tokens\": max_new_tokens,\n-            \"generation_config\": self.generation_config,\n-            \"logits_processor\": self.logits_processor,\n-        }\n+        # Generate candidates. Run prefill-specific logic in first generation and prepare model kwargs.\n+        # Some models prepare inputs differently depending on first vs subsequent iterations.(e.g. VLMs)\n+        # Assisted generation however calls internally `self.generate()` many times and technically will\n+        # lead to many `is_first_iteration's`. This way we can call prefill only once per assistant model\n+        if is_first_iteration:\n+            generation_args = self.assistant_model._get_initial_cache_position(\n+                input_ids.shape[1], input_ids.device, self.assistant_kwargs.copy()\n+            )\n+            generation_args = self.assistant_model.prepare_inputs_for_generation(\n+                input_ids, is_first_iteration=True, **generation_args\n+            )\n+            # NOTE: `prepare_inputs_for_generation` creates inputs that can't be used when continuing generation with past-cache\n+            # therefore we manually re-assign full input ids and other args. It is a known issue, due to legacy reasons we\n+            # have to pass whole input ids to `generate()` including past tokens which are in encoded in cache\n+            generation_args[self.input_ids_key] = input_ids\n+            for model_input_name in [\"position_ids\", \"token_type_ids\", \"decoder_position_ids\", \"cache_position\"]:\n+                generation_args.pop(model_input_name, None)\n+        else:\n+            generation_args = {self.input_ids_key: input_ids}\n+        generation_args.update(\n+            {\n+                \"min_new_tokens\": min_new_tokens,\n+                \"max_new_tokens\": max_new_tokens,\n+                \"generation_config\": self.generation_config,\n+                \"logits_processor\": self.logits_processor,\n+            }\n+        )\n+\n+        generation_args.update(\n+            {k: self.assistant_kwargs[k] for k in self.assistant_kwargs if k not in generation_args}\n+        )\n+        return generation_args\n \n     def _generate_candidates(self, generation_args: dict) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n         \"\"\"Generate candidate sequences using the assistant model.\"\"\"\n-        assistant_output = self.assistant_model.generate(**generation_args, **self.assistant_kwargs)\n+        assistant_output = self.assistant_model.generate(**generation_args)\n         self.assistant_kwargs[\"past_key_values\"] = assistant_output.past_key_values\n         if (\n             is_sklearn_available()\n@@ -499,7 +530,9 @@ def convert_source_tokens_to_target_tokens(\n         dest_ids = destination_tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n         return dest_ids.to(input_ids.device)\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n+    def get_candidates(\n+        self, input_ids: torch.LongTensor, is_first_iteration: bool\n+    ) -> tuple[torch.LongTensor, torch.FloatTensor]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -525,10 +558,12 @@ def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor,\n         min_new_tokens = max(min(max_new_tokens, self.main_model_min_length - assistant_input_ids.shape[-1]), 0)\n \n         self._update_past_and_masks(assistant_input_ids, remove_from_pkv)\n-        generation_args = self._prepare_generation_args(assistant_input_ids, min_new_tokens, max_new_tokens)\n+        generation_args = self._prepare_generation_args(\n+            assistant_input_ids, min_new_tokens, max_new_tokens, is_first_iteration\n+        )\n         self.assistant_kwargs.pop(\"attention_mask\", None)\n \n-        assistant_output = self.assistant_model.generate(**generation_args, **self.assistant_kwargs)\n+        assistant_output = self.assistant_model.generate(**generation_args)\n         new_target_ids = self._process_assistant_outputs(input_ids, assistant_output.sequences)\n \n         # Update state\n@@ -924,7 +959,9 @@ def __init__(\n         self._target_seq_len_with_candidates: int = 0\n         self._prev_assistant_ids: torch.LongTensor | None = None\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n+    def get_candidates(\n+        self, input_ids: torch.LongTensor, is_first_iteration: bool\n+    ) -> tuple[torch.LongTensor, torch.FloatTensor]:\n         \"\"\"\n         Simplified version of get_candidates that uses the translator cache for token conversion.\n         \"\"\"\n@@ -936,7 +973,9 @@ def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor,\n             return input_ids, None\n \n         self._update_past_and_masks(assistant_input_ids, num_added_tokens=num_added_tokens)\n-        generation_args = self._prepare_generation_args(assistant_input_ids, min_new_tokens, max_new_tokens)\n+        generation_args = self._prepare_generation_args(\n+            assistant_input_ids, min_new_tokens, max_new_tokens, is_first_iteration\n+        )\n \n         # Ensure scores are returned\n         generation_args[\"generation_config\"].output_scores = True\n@@ -1050,7 +1089,9 @@ def __init__(\n         if self.max_matching_ngram_size <= 0 or self.num_output_tokens <= 0:\n             raise ValueError(\"Invalid max_matching_ngram_size or num_output_tokens\")\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n+    def get_candidates(\n+        self, input_ids: torch.LongTensor, is_first_iteration: bool\n+    ) -> tuple[torch.LongTensor, torch.FloatTensor]:\n         \"\"\"\n         Fetches the candidates to be tried for the current input.\n \n@@ -1207,7 +1248,9 @@ def __init__(\n         self.assistant_early_exit = self.generation_config.assistant_early_exit\n         self.generation_config.assistant_early_exit = None\n \n-    def get_candidates(self, input_ids: torch.LongTensor) -> tuple[torch.LongTensor, torch.FloatTensor | None]:\n+    def get_candidates(\n+        self, input_ids: torch.LongTensor, is_first_iteration: bool\n+    ) -> tuple[torch.LongTensor, torch.FloatTensor]:\n         # Temporarily sets the number of hidden layers to the early exit value\n         base_model = getattr(self.assistant_model, self.assistant_model.base_model_prefix)\n         original_num_hidden_layers = base_model.config.num_hidden_layers"
        },
        {
            "sha": "cf9497d2a1f1b2e53c84295d7f251f5dcc9b5b12",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 24,
            "deletions": 8,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -596,6 +596,7 @@ def prepare_inputs_for_generation(\n         attention_mask: torch.LongTensor | None = None,\n         inputs_embeds: torch.FloatTensor | None = None,\n         cache_position: torch.LongTensor | None = None,\n+        is_first_iteration: bool | None = False,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -631,7 +632,7 @@ def prepare_inputs_for_generation(\n         input_ids_key = \"decoder_input_ids\" if self.config.is_encoder_decoder else \"input_ids\"\n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step for every prompt.\n         if not self.config.is_encoder_decoder:\n-            if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n+            if inputs_embeds is not None and is_first_iteration:\n                 model_inputs[input_ids_key] = None\n                 model_inputs[\"inputs_embeds\"] = inputs_embeds\n             else:\n@@ -711,6 +712,7 @@ def prepare_inputs_for_generation(\n                     past_key_values=past_key_values,\n                     position_ids=position_ids,\n                     token_type_ids=token_type_ids,\n+                    is_first_iteration=is_first_iteration,\n                 )\n             else:\n                 attention_mask = causal_mask_creation_function(\n@@ -2835,8 +2837,14 @@ def _sample(\n             else self.__call__\n         )\n \n-        prefill_consumed = False\n-        outputs = self._prefill(input_ids, generation_config, model_kwargs)\n+        # Assisted generation completes the prefill stage in candidate generator so that\n+        # we don't have several `prefill` calls in one generation loop. Skip `_prefill` for assistants\n+        if not generation_config.is_assistant:\n+            outputs = self._prefill(input_ids, generation_config, model_kwargs)\n+            prefill_consumed = False\n+        else:\n+            model_kwargs = self._get_initial_cache_position(input_ids.shape[1], input_ids.device, model_kwargs)\n+            prefill_consumed = True\n \n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n             if prefill_consumed:\n@@ -3313,9 +3321,15 @@ def _beam_search(\n         )\n         beam_indices = running_beam_indices.detach().clone()\n \n-        prefill_consumed = False\n         flat_running_sequences = input_ids\n-        model_outputs = self._prefill(input_ids, generation_config, model_kwargs)\n+        # Assisted generation completes the prefill stage in candidate generator so that\n+        # we don't have several `prefill` calls in one generation loop. Skip `_prefill` for assistants\n+        if not generation_config.is_assistant:\n+            model_outputs = self._prefill(input_ids, generation_config, model_kwargs)\n+            prefill_consumed = False\n+        else:\n+            model_kwargs = self._get_initial_cache_position(input_ids.shape[1], input_ids.device, model_kwargs)\n+            prefill_consumed = True\n \n         # 4. run the generation loop\n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n@@ -3621,7 +3635,7 @@ def _assisted_decoding(\n             cur_len = input_ids.shape[1]\n \n             #  1. Fetch candidate sequences from a `CandidateGenerator` and move to the correct device\n-            candidate_input_ids, candidate_logits = candidate_generator.get_candidates(input_ids)\n+            candidate_input_ids, candidate_logits = candidate_generator.get_candidates(input_ids, is_first_iteration)\n             candidate_input_ids = candidate_input_ids.to(self.device)\n             if candidate_logits is not None:\n                 candidate_logits = candidate_logits.to(self.device)\n@@ -3648,7 +3662,9 @@ def _assisted_decoding(\n                     dim=0,\n                 )\n \n-            model_inputs = self.prepare_inputs_for_generation(candidate_input_ids, **candidate_kwargs)\n+            model_inputs = self.prepare_inputs_for_generation(\n+                candidate_input_ids, is_first_iteration=is_first_iteration, **candidate_kwargs\n+            )\n             if \"logits_to_keep\" in model_inputs:\n                 model_inputs[\"logits_to_keep\"] = candidate_length + 1\n \n@@ -3811,7 +3827,7 @@ def _assisted_decoding(\n     def _prefill(self, input_ids: torch.LongTensor, generation_config: GenerationConfig, model_kwargs):\n         if generation_config.prefill_chunk_size is None:\n             model_kwargs = self._get_initial_cache_position(input_ids.shape[1], input_ids.device, model_kwargs)\n-            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n+            model_inputs = self.prepare_inputs_for_generation(input_ids, is_first_iteration=True, **model_kwargs)\n             return self(**model_inputs, return_dict=True)\n         else:  # Chunked prefill\n             # Even if we are not compiling the forward, flex is always compiled when used. With chunked prefill, we may"
        },
        {
            "sha": "8da900f49c05fa3aa4c11f61983dfe10e0a63e6c",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1203,6 +1203,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         model_inputs = super().prepare_inputs_for_generation(\n@@ -1212,12 +1213,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"pixel_mask\"] = pixel_mask\n "
        },
        {
            "sha": "01b7921837d2759d0545002a360ecd74069ff490",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1500,6 +1500,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         model_inputs = super().prepare_inputs_for_generation(\n@@ -1509,12 +1510,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"pixel_mask\"] = pixel_mask\n "
        },
        {
            "sha": "529902f98c15fcd5f9a481e73e7ff48850f43aaa",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -471,6 +471,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -482,12 +483,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "227116947bfdb1936c47a86ed4981c5f6fa2d985",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1488,6 +1488,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n@@ -1520,7 +1521,7 @@ def prepare_inputs_for_generation(\n                 position_ids = position_ids[:, -input_ids.shape[1] :]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and empty_past_kv:\n+        if inputs_embeds is not None and is_first_iteration:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases"
        },
        {
            "sha": "fe474710da77fbb239911e101441874e629982fe",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1151,6 +1151,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n@@ -1183,7 +1184,7 @@ def prepare_inputs_for_generation(\n                 position_ids = position_ids[:, -input_ids.shape[1] :]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and empty_past_kv:\n+        if inputs_embeds is not None and is_first_iteration:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases"
        },
        {
            "sha": "0ce1fc1b3f10434aea467e7d91ca9eef2bcc97d2",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 13,
            "deletions": 44,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -714,36 +714,21 @@ def prepare_inputs_for_generation(\n         inputs_embeds=None,\n         cache_position=None,\n         use_cache=True,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten because of the fixed-shape attention mask creation\n \n-        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n-        # Exception 1: when passing input_embeds, input_ids may be missing entries\n-        # Exception 2: some generation methods do special slicing of input_ids, so we don't need to do it here\n-        # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n-        #              (we can't check exception 3 while compiling)\n-        # Exception 4: If input_embeds are passed then slice it through `cache_position`, to keep only the unprocessed tokens and\n-        # generate the first token for each sequence. Later use the generated Input ids for continuation.\n-        if past_key_values is not None:\n-            if inputs_embeds is not None and input_ids.shape[1] == 0:  # Exception 4\n-                inputs_embeds = inputs_embeds[:, -cache_position.shape[0] :]\n-            elif (\n-                inputs_embeds is not None  # Exception 1\n-                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n-            ):\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and len(cache_position) == inputs_embeds.shape[1]:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            # This `clone` call is needed to avoid recapturing cuda graphs with `torch.compile`'s  `mode=\"reduce-overhead`, as otherwise the\n-            # input `position_ids` would have various stride during the decoding. Here, simply using `.contiguous()` is not sufficient as in\n-            # the batch size = 1 case, `position_ids` is already contiguous but with varying stride which retriggers a capture.\n-            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n+            **kwargs,\n+        )\n \n         # This part differs from other models because BLOOM needs a 2D mask to construct alibi tensor\n         # The only difference is the usage of 2D instead of 4D mask, but the shape will be static\n@@ -753,24 +738,8 @@ def prepare_inputs_for_generation(\n             diff = target_length - seq_length\n \n             new_attn_mask = torch.zeros(batch_size, diff, device=attention_mask.device, dtype=attention_mask.dtype)\n-            attention_mask = torch.cat(\n-                [attention_mask, new_attn_mask],\n-                dim=-1,\n-            )\n-\n-        model_inputs.update(\n-            {\n-                \"cache_position\": cache_position,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-            }\n-        )\n-\n-        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n-        for key, value in kwargs.items():\n-            if key not in model_inputs:\n-                model_inputs[key] = value\n+            attention_mask = torch.cat([attention_mask, new_attn_mask], dim=-1)\n+            model_inputs[\"attention_mask\"] = attention_mask\n \n         return model_inputs\n "
        },
        {
            "sha": "e77c25922b59fba2416d03cb7420173b88ab0d89",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1123,6 +1123,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1136,12 +1137,15 @@ def prepare_inputs_for_generation(\n             cache_position=cache_position,\n             position_ids=position_ids,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] != 0:\n-            # If we're in cached decoding stage, pixel values should be `None` because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if not is_first_iteration and use_cache:\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = None\n \n         return model_inputs"
        },
        {
            "sha": "9f3468d3313f4af6d2f976ed8bd023685c1b74fd",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1318,6 +1318,7 @@ def prepare_inputs_for_generation(\n         inputs_embeds=None,\n         conditioning_embeds=None,\n         cache_position=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten: has `conditioning_embeds`-related logic\n@@ -1329,9 +1330,10 @@ def prepare_inputs_for_generation(\n             past_key_values=past_key_values,\n             inputs_embeds=inputs_embeds,\n             cache_position=cache_position,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n-        if conditioning_embeds is not None and cache_position[0] != 0:\n+        if conditioning_embeds is not None and not is_first_iteration:\n             model_inputs[\"position_ids\"] = torch.tensor([input_ids_length], dtype=torch.long, device=input_ids.device)\n \n         return model_inputs"
        },
        {
            "sha": "ab75bc21bfd1c6584b6ba22aeaa865dbb3a7999d",
            "filename": "src/transformers/models/cohere2_vision/modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fmodeling_cohere2_vision.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -376,6 +376,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -387,12 +388,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "5a5bb85b3da2bc2e3dd497bb5a85c77967c0444d",
            "filename": "src/transformers/models/csm/generation_csm.py",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -207,26 +207,25 @@ def _sample(\n             else self.__call__\n         )\n \n-        is_prefill = True\n-        while self._has_unfinished_sequences(\n-            this_peer_finished,\n-            synced_gpus,\n-            device=input_ids.device,\n-        ):\n-            # prepare model inputs\n-            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n-\n-            # prepare variable output controls (note: some models won't accept all output controls)\n-            model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n-            # *************** Csm specific ***************\n-            model_inputs.update({\"output_hidden_states\": True})\n-            # ============================================\n+        # *************** Csm specific ***************\n+        model_kwargs.update({\"output_hidden_states\": True})\n \n-            if is_prefill:\n-                outputs = self(**model_inputs, return_dict=True)\n-                is_prefill = False\n-            else:\n+        # Assisted generation completes the prefill stage in candidate generator so that\n+        # we don't have several `prefill` calls in one generation loop. Skip `_prefill` for assistants\n+        if not generation_config.is_assistant:\n+            outputs = self._prefill(input_ids, generation_config, model_kwargs)\n+            prefill_consumed = False\n+        else:\n+            model_kwargs = self._get_initial_cache_position(input_ids.shape[1], input_ids.device, model_kwargs)\n+            prefill_consumed = True\n+\n+        while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n+            if prefill_consumed:\n+                model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n+                # prepare variable output controls (note: some models won't accept all output controls)\n+                model_inputs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n                 outputs = model_forward(**model_inputs, return_dict=True)\n+            prefill_consumed = True\n \n             # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n             model_kwargs = self._update_model_kwargs_for_generation("
        },
        {
            "sha": "be79b8a6d0176afdee93b9bfd7cbc91e0ebc389c",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -480,7 +480,9 @@ def forward(\n             attentions=transformer_outputs.attentions,\n         )\n \n-    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, use_cache=None, **kwargs):\n+    def prepare_inputs_for_generation(\n+        self, input_ids, past_key_values=None, use_cache=None, is_first_iteration=False, **kwargs\n+    ):\n         # Overwritten -- inputs_embeds not working properly\n \n         # only last tokens for inputs_ids if past is defined in kwargs"
        },
        {
            "sha": "b3470ad9eb1672d25d27c421d4da60767c67fdaf",
            "filename": "src/transformers/models/deepseek_vl/modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodeling_deepseek_vl.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -315,6 +315,7 @@ def prepare_inputs_for_generation(\n         inputs_embeds=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- extra custom processing\n@@ -326,12 +327,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-        # Otherwise we need pixel values to be passed to model\n-        if cache_position[0] == 0:\n+        # Pixel values are used only in the first iteration if available\n+        # In subsquent iterations, they are already merged with text and cached\n+        # NOTE: first iteration doesn't have to be prefill, it can be the first\n+        # iteration with a question and cached system prompt (continue generate from cache)\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "1b746c5a55ef18eca367c3a0e24a31b044b6cfae",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodeling_deepseek_vl_hybrid.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -473,6 +473,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         model_inputs = super().prepare_inputs_for_generation(\n@@ -482,12 +483,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"high_res_pixel_values\"] = high_res_pixel_values\n "
        },
        {
            "sha": "7785f3ba56b3e255deb93ef75db354dac23703ae",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -410,6 +410,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         model_inputs = super().prepare_inputs_for_generation(\n@@ -419,12 +420,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"high_res_pixel_values\"] = high_res_pixel_values\n "
        },
        {
            "sha": "9bc013c5bbe3ee8ae3dc2fd4e7097979ebf44d4a",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1619,6 +1619,7 @@ def prepare_inputs_for_generation(\n         position_ids=None,\n         use_cache=True,\n         pixel_values=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1632,10 +1633,11 @@ def prepare_inputs_for_generation(\n             position_ids=position_ids,\n             pixel_values=pixel_values,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n \n         return model_inputs"
        },
        {
            "sha": "29223eb3ce4540bffda54a764a5de3ffc00c225a",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1171,6 +1171,7 @@ def prepare_inputs_for_generation(\n         position_ids=None,\n         use_cache=True,\n         pixel_values=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1184,10 +1185,11 @@ def prepare_inputs_for_generation(\n             position_ids=position_ids,\n             pixel_values=pixel_values,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n \n         return model_inputs"
        },
        {
            "sha": "41a5afa301516f6b6e4ec304d3d15ddd127b4854",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1595,6 +1595,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- has a unique cache type, `FalconHybridMambaAttentionDynamicCache`\n@@ -1632,7 +1633,7 @@ def prepare_inputs_for_generation(\n                 position_ids = position_ids[:, -input_ids.shape[1] :]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and empty_past_kv:\n+        if inputs_embeds is not None and is_first_iteration:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases"
        },
        {
            "sha": "c7024844d1d2a80a6b721dba19848aa6dde396fe",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1302,6 +1302,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- has a unique cache type, `FalconHybridMambaAttentionDynamicCache`\n@@ -1339,7 +1340,7 @@ def prepare_inputs_for_generation(\n                 position_ids = position_ids[:, -input_ids.shape[1] :]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and empty_past_kv:\n+        if inputs_embeds is not None and is_first_iteration:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases"
        },
        {
            "sha": "07ded6832bbc9c20100f509cf9a4b3642936b793",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -814,6 +814,7 @@ def prepare_inputs_for_generation(\n         cache_params: Optional[FalconMambaCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n+        is_first_iteration: Optional[bool] = False,\n         **kwargs,\n     ):\n         # Overwritten -- uses `cache_params` as opposed to `past_key_values`"
        },
        {
            "sha": "8e5c8bfafd7903d0365a9d63f8ec34d7043cda97",
            "filename": "src/transformers/models/fast_vlm/modeling_fast_vlm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodeling_fast_vlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodeling_fast_vlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffast_vlm%2Fmodeling_fast_vlm.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -430,6 +430,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -441,12 +442,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "68a4733e4a6060ad43406d618b831d9c8327f560",
            "filename": "src/transformers/models/florence2/modeling_florence2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodeling_florence2.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -950,6 +950,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -961,12 +962,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "8547544a24266b2abe96f60e6c58beebadee9c93",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -359,6 +359,7 @@ def prepare_inputs_for_generation(\n         image_patches=None,\n         image_patches_indices=None,\n         cache_position=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -371,10 +372,11 @@ def prepare_inputs_for_generation(\n             image_patches=image_patches,\n             image_patches_indices=image_patches_indices,\n             cache_position=cache_position,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and kwargs.get(\"use_cache\", True):\n             # set image_patches and image_patches_indices to `None` for decoding stage\n             model_inputs[\"image_patches_indices\"] = None\n             model_inputs[\"image_patches\"] = None"
        },
        {
            "sha": "088b9db12fc902a4f3dc7298ae378fe7d1bf2d7d",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 6,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -765,6 +765,7 @@ def create_causal_mask_mapping(\n     token_type_ids: Optional[torch.Tensor] = None,\n     pixel_values: Optional[torch.FloatTensor] = None,\n     is_training: bool = False,\n+    is_first_iteration: Optional[bool] = None,\n     **kwargs,\n ) -> dict:\n     \"\"\"\n@@ -787,8 +788,12 @@ def create_causal_mask_mapping(\n     # NOTE: this `may_have_image_input` logic is not flawless, it fails when we're using a cache eagerly initialized\n     # (e.g. compiled prefill) AND `pixel_values` are not provided (i.e. the image data is provided through other\n     # means). Determining prefill in that case requires checking data values, which is not compile-compatible.\n-    may_have_image_input = past_key_values is None or not past_key_values.is_initialized or pixel_values is not None\n-    if token_type_ids is not None and may_have_image_input:\n+    is_first_iteration = (\n+        is_first_iteration\n+        if is_first_iteration is not None\n+        else (past_key_values is None or not past_key_values.is_initialized or pixel_values is not None)\n+    )\n+    if token_type_ids is not None and is_first_iteration:\n         # We need to pass an additional mask function to account for token type ids, and it needs to be an `or` (to\n         # undo the causal masking)\n \n@@ -1134,6 +1139,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         logits_to_keep=None,\n         labels=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- custom `position_ids` and `pixel_values` handling\n@@ -1147,12 +1153,15 @@ def prepare_inputs_for_generation(\n             use_cache=use_cache,\n             logits_to_keep=logits_to_keep,\n             token_type_ids=token_type_ids,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-        # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always\n-        if cache_position[0] == 0:\n+        # Pixel values are used only in the first iteration if available\n+        # In subsquent iterations, they are already merged with text and cached\n+        # NOTE: first iteration doesn't have to be prefill, it can be the first\n+        # iteration with a question and cached system prompt (continue generate from cache). NOTE: use_cache=False needs pixel_values always\n+        if is_first_iteration or not use_cache:\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs\n@@ -1166,6 +1175,7 @@ def create_masks_for_generate(\n         past_key_values: Optional[Cache],\n         position_ids: Optional[torch.Tensor],\n         token_type_ids: Optional[torch.Tensor] = None,\n+        is_first_iteration: Optional[bool] = False,\n         **kwargs,\n     ) -> dict:\n         # Uses the overwritten `create_masks_for_generate` with `token_type_ids` masking\n@@ -1177,7 +1187,7 @@ def create_masks_for_generate(\n             past_key_values,\n             position_ids,\n             token_type_ids,\n-            pixel_values=kwargs.get(\"pixel_values\"),\n+            is_first_iteration=is_first_iteration,\n             **{k: v for k, v in kwargs.items() if k != \"pixel_values\"},\n         )\n "
        },
        {
            "sha": "93418fc4faf064052e433672da81e43480ebd282",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 14,
            "deletions": 5,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -745,6 +745,7 @@ def create_causal_mask_mapping(\n     token_type_ids: Optional[torch.Tensor] = None,\n     pixel_values: Optional[torch.FloatTensor] = None,\n     is_training: bool = False,\n+    is_first_iteration: Optional[bool] = None,\n     **kwargs,\n ) -> dict:\n     \"\"\"\n@@ -767,8 +768,12 @@ def create_causal_mask_mapping(\n     # NOTE: this `may_have_image_input` logic is not flawless, it fails when we're using a cache eagerly initialized\n     # (e.g. compiled prefill) AND `pixel_values` are not provided (i.e. the image data is provided through other\n     # means). Determining prefill in that case requires checking data values, which is not compile-compatible.\n-    may_have_image_input = past_key_values is None or not past_key_values.is_initialized or pixel_values is not None\n-    if token_type_ids is not None and may_have_image_input:\n+    is_first_iteration = (\n+        is_first_iteration\n+        if is_first_iteration is not None\n+        else (past_key_values is None or not past_key_values.is_initialized or pixel_values is not None)\n+    )\n+    if token_type_ids is not None and is_first_iteration:\n         # We need to pass an additional mask function to account for token type ids, and it needs to be an `or` (to\n         # undo the causal masking)\n \n@@ -1016,6 +1021,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         logits_to_keep=None,\n         labels=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- custom `position_ids` and `pixel_values` handling\n@@ -1029,12 +1035,15 @@ def prepare_inputs_for_generation(\n             use_cache=use_cache,\n             logits_to_keep=logits_to_keep,\n             token_type_ids=token_type_ids,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-        # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always\n-        if cache_position[0] == 0:\n+        # Pixel values are used only in the first iteration if available\n+        # In subsquent iterations, they are already merged with text and cached\n+        # NOTE: first iteration doesn't have to be prefill, it can be the first\n+        # iteration with a question and cached system prompt (continue generate from cache). NOTE: use_cache=False needs pixel_values always\n+        if is_first_iteration or not use_cache:\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "4f16e3fff40a7dc2d26765e0dff3939c4469da01",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -2337,6 +2337,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         logits_to_keep=None,\n         labels=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- custom `position_ids` and `pixel_values` handling\n@@ -2350,13 +2351,14 @@ def prepare_inputs_for_generation(\n             use_cache=use_cache,\n             logits_to_keep=logits_to_keep,\n             token_type_ids=token_type_ids,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         # If we're in cached decoding stage, multimodal inputs should be None because input ids do not contain special\n         # tokens anymore. Otherwise multimodal inputs should be passed to model.\n         # NOTE: use_cache=False always needs pixel_values, input_features, and input_features_mask\n-        if cache_position[0] == 0:\n+        if is_first_iteration or not use_cache:\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"input_features\"] = input_features\n             model_inputs[\"input_features_mask\"] = input_features_mask"
        },
        {
            "sha": "3a4890d9d64638ffda6a864f90323e480d633c83",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -2583,6 +2583,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         logits_to_keep=None,\n         labels=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- custom `position_ids` and `pixel_values` handling\n@@ -2596,13 +2597,14 @@ def prepare_inputs_for_generation(\n             use_cache=use_cache,\n             logits_to_keep=logits_to_keep,\n             token_type_ids=token_type_ids,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         # If we're in cached decoding stage, multimodal inputs should be None because input ids do not contain special\n         # tokens anymore. Otherwise multimodal inputs should be passed to model.\n         # NOTE: use_cache=False always needs pixel_values, input_features, and input_features_mask\n-        if cache_position[0] == 0:\n+        if is_first_iteration or not use_cache:\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"input_features\"] = input_features\n             model_inputs[\"input_features_mask\"] = input_features_mask"
        },
        {
            "sha": "cc8cbcf84405fcea8a9440381feace2b31c0cc38",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 178,
            "deletions": 126,
            "changes": 304,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -26,8 +26,9 @@\n from ... import initialization as init\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache\n+from ...configuration_utils import PreTrainedConfig\n from ...generation import GenerationMixin\n-from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n+from ...masking_utils import create_masks_for_generate\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n     BaseModelOutput,\n@@ -69,6 +70,104 @@ class GitVisionModelOutput(ModelOutput):\n     attentions: Optional[tuple[torch.FloatTensor, ...]] = None\n \n \n+# Copied from transformers.models.gemma3.modeling_gemma3.token_type_ids_mask_function\n+def token_type_ids_mask_function(\n+    token_type_ids: Optional[torch.Tensor],\n+    image_group_ids: Optional[torch.Tensor],\n+) -> Optional[Callable]:\n+    \"\"\"\n+    This function adds the correct offsets to the `q_idx` and `kv_idx` as the torch API can only accept lengths,\n+    not start and end indices.\n+    \"\"\"\n+    # Do not return an additional mask in this case\n+    if token_type_ids is None:\n+        return None\n+\n+    def inner_mask(batch_idx: int, head_idx: int, q_idx: int, kv_idx: int) -> bool:\n+        # If it's 1 for both query and key/value, we are in an image block\n+        # NOTE: static cache shape goes beyond input seq length, while token_type_ids.shape[1] == input seq length\n+        # Since vmap doesn't support `if statement` we workaround it with `torch.where`\n+        safe_q_idx = torch.where(q_idx < token_type_ids.shape[1], q_idx, 0)\n+        safe_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], kv_idx, 0)\n+\n+        token_type_ids_at_q_idx = token_type_ids[batch_idx, safe_q_idx]\n+        token_type_ids_at_q_idx = torch.where(q_idx < token_type_ids.shape[1], token_type_ids_at_q_idx, 0)\n+\n+        token_type_ids_at_kv_idx = token_type_ids[batch_idx, safe_kv_idx]\n+        token_type_ids_at_kv_idx = torch.where(kv_idx < token_type_ids.shape[1], token_type_ids_at_kv_idx, 0)\n+\n+        image_group_ids_at_q_idx = image_group_ids[batch_idx, safe_q_idx]\n+        image_group_ids_at_q_idx = torch.where(q_idx < image_group_ids.shape[1], image_group_ids_at_q_idx, -1)\n+\n+        image_group_ids_at_kv_idx = image_group_ids[batch_idx, safe_kv_idx]\n+        image_group_ids_at_kv_idx = torch.where(kv_idx < image_group_ids.shape[1], image_group_ids_at_kv_idx, -1)\n+\n+        is_image_block = (token_type_ids_at_q_idx == 1) & (token_type_ids_at_kv_idx == 1)\n+        same_image_block = image_group_ids_at_q_idx == image_group_ids_at_kv_idx\n+\n+        # This is bidirectional attention whenever we are dealing with image tokens\n+        return is_image_block & same_image_block\n+\n+    return inner_mask\n+\n+\n+# Copied from transformers.models.gemma3.modeling_gemma3.create_causal_mask_mapping\n+def create_causal_mask_mapping(\n+    config: PreTrainedConfig,\n+    input_embeds: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    cache_position: torch.Tensor,\n+    past_key_values: Optional[Cache],\n+    position_ids: Optional[torch.Tensor],\n+    token_type_ids: Optional[torch.Tensor] = None,\n+    pixel_values: Optional[torch.FloatTensor] = None,\n+    is_training: bool = False,\n+    is_first_iteration: Optional[bool] = None,\n+    **kwargs,\n+) -> dict:\n+    \"\"\"\n+    Overwrites the base `create_masks_for_generate` with `token_type_ids` masking to create the causal mask mapping\n+    for all kinds of forward passes. Gemma3 uses a bidirectional mask for images.\n+\n+    Uses `pixel_values` as an optional input to disambiguate edge cases.\n+    \"\"\"\n+    if is_training and token_type_ids is None:\n+        raise ValueError(\"`token_type_ids` is required as a model input when training\")\n+\n+    mask_kwargs = {\n+        \"config\": config.get_text_config(),\n+        \"input_embeds\": input_embeds,\n+        \"attention_mask\": attention_mask,\n+        \"cache_position\": cache_position,\n+        \"past_key_values\": past_key_values,\n+        \"position_ids\": position_ids,\n+    }\n+    # NOTE: this `may_have_image_input` logic is not flawless, it fails when we're using a cache eagerly initialized\n+    # (e.g. compiled prefill) AND `pixel_values` are not provided (i.e. the image data is provided through other\n+    # means). Determining prefill in that case requires checking data values, which is not compile-compatible.\n+    is_first_iteration = (\n+        is_first_iteration\n+        if is_first_iteration is not None\n+        else (past_key_values is None or not past_key_values.is_initialized or pixel_values is not None)\n+    )\n+    if token_type_ids is not None and is_first_iteration:\n+        # We need to pass an additional mask function to account for token type ids, and it needs to be an `or` (to\n+        # undo the causal masking)\n+\n+        # First find where a new image block starts: 1 if image and previous not image\n+        # The images cannot attend to future images, but can attend to all prev images and to itself bidirectionally\n+        is_image = (token_type_ids == 1).to(cache_position.device)\n+        is_previous_image = nn.functional.pad(is_image, (1, 0), value=0)[:, :-1]\n+        new_image_start = is_image & ~is_previous_image\n+        image_group_ids = torch.cumsum(new_image_start.int(), dim=1) - 1\n+        image_group_ids = torch.where(is_image, image_group_ids, -1)\n+        mask_kwargs[\"or_mask_function\"] = token_type_ids_mask_function(\n+            token_type_ids.to(cache_position.device), image_group_ids\n+        )\n+\n+    return create_masks_for_generate(**mask_kwargs)\n+\n+\n class GitEmbeddings(nn.Module):\n     \"\"\"Construct the embeddings from word and position embeddings.\"\"\"\n \n@@ -148,17 +247,15 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n-        output_attentions: Optional[bool] = False,\n-        pixel_values_present: Optional[bool] = False,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> tuple[torch.Tensor]:\n-        batch_size, seq_length, _ = hidden_states.shape\n+        batch_size = hidden_states.shape[0]\n         query_layer = (\n             self.query(hidden_states)\n             .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n             .transpose(1, 2)\n         )\n \n-        cutoff = self.image_patch_tokens if pixel_values_present else 0\n         key_layer = (\n             self.key(hidden_states)\n             .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)\n@@ -170,12 +267,9 @@ def forward(\n             .transpose(1, 2)\n         )\n         if past_key_values is not None:\n-            # NOTE: like in other caches, we store the text component. In GIT it means we discard the image component.\n-            key_layer_past, value_layer_past = past_key_values.update(\n-                key_layer[:, :, cutoff:, :], value_layer[:, :, cutoff:, :], self.layer_idx\n+            key_layer, value_layer = past_key_values.update(\n+                key_layer, value_layer, self.layer_idx, cache_kwargs={\"cache_position\": cache_position}\n             )\n-            key_layer = torch.cat([key_layer[:, :, :cutoff, :], key_layer_past], dim=2)\n-            value_layer = torch.cat([value_layer[:, :, :cutoff, :], value_layer_past], dim=2)\n \n         # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n         attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n@@ -232,15 +326,14 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-        pixel_values_present: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         attn_output, self_attn_weights = self.self(\n             hidden_states,\n             attention_mask,\n             past_key_values,\n-            output_attentions,\n-            pixel_values_present,\n+            cache_position=cache_position,\n         )\n         attention_output = self.output(attn_output, hidden_states)\n         return attention_output, self_attn_weights\n@@ -291,16 +384,16 @@ def forward(\n         hidden_states: torch.Tensor,\n         attention_mask: Optional[torch.FloatTensor] = None,\n         past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         output_attentions: Optional[bool] = False,\n-        pixel_values_present: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n         attention_output, self_attention_weights = self.attention(\n             hidden_states,\n             attention_mask,\n             output_attentions=output_attentions,\n             past_key_values=past_key_values,\n-            pixel_values_present=pixel_values_present,\n+            cache_position=cache_position,\n         )\n \n         layer_output = apply_chunking_to_forward(\n@@ -329,8 +422,8 @@ def forward(\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = False,\n         output_hidden_states: Optional[bool] = False,\n-        pixel_values_present: Optional[bool] = False,\n         return_dict: Optional[bool] = True,\n+        cache_position: Optional[torch.Tensor] = None,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPast]:\n         if self.gradient_checkpointing and self.training:\n             if use_cache:\n@@ -353,7 +446,7 @@ def forward(\n                 attention_mask,\n                 past_key_values,\n                 output_attentions,\n-                pixel_values_present,\n+                cache_position,\n             )\n \n             hidden_states = layer_outputs[0]\n@@ -906,62 +999,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.embeddings.word_embeddings = value\n \n-    def _generate_future_mask(self, size: int, dtype: torch.dtype, device: torch.device) -> torch.Tensor:\n-        # Default mask is for forward direction. Flip for backward direction.\n-        mask = torch.triu(torch.ones(size, size, device=device, dtype=dtype), diagonal=1)\n-        mask = mask.masked_fill(mask == 1, float(\"-inf\"))\n-        return mask\n-\n-    def create_attention_mask(self, tgt, memory, tgt_mask, past_key_values_length, memory_key_padding_mask=None):\n-        num_tgt = tgt.shape[1]\n-        num_memory = memory.shape[1]\n-        device = tgt.device\n-        dtype = tgt.dtype\n-        top_left = torch.zeros((num_memory, num_memory), device=device, dtype=dtype)\n-        top_right = torch.full(\n-            (num_memory, num_tgt + past_key_values_length),\n-            float(\"-inf\"),\n-            device=tgt.device,\n-            dtype=dtype,\n-        )\n-        bottom_left = torch.zeros(\n-            (num_tgt, num_memory),\n-            dtype=dtype,\n-            device=tgt_mask.device,\n-        )\n-\n-        if past_key_values_length > 0:\n-            tgt_mask = torch.zeros(\n-                (tgt_mask.shape[0], tgt_mask.shape[0] + past_key_values_length),\n-                dtype=dtype,\n-                device=tgt_mask.device,\n-            )\n-\n-        left = torch.cat((top_left, bottom_left), dim=0)\n-        right = torch.cat((top_right, tgt_mask.to(dtype)), dim=0)\n-\n-        full_attention_mask = torch.cat((left, right), dim=1)[None, :]\n-\n-        if memory_key_padding_mask is None:\n-            memory_key_padding_mask = torch.full((memory.shape[0], memory.shape[1]), fill_value=False, device=device)\n-        # if it is False, it means valid. That is, it is not a padding\n-        if memory_key_padding_mask.dtype != torch.bool:\n-            raise ValueError(\"Memory key padding mask must be a boolean tensor.\")\n-        zero_negative_infinity = torch.zeros_like(memory_key_padding_mask, dtype=tgt.dtype)\n-        zero_negative_infinity[memory_key_padding_mask] = float(\"-inf\")\n-        full_attention_mask = full_attention_mask.expand(\n-            (memory_key_padding_mask.shape[0], num_memory + num_tgt, num_memory + past_key_values_length + num_tgt)\n-        )\n-        full_attention_mask = full_attention_mask.clone()\n-        origin_left = full_attention_mask[:, :, :num_memory]\n-        update = zero_negative_infinity[:, None, :]\n-        full_attention_mask[:, :, :num_memory] = origin_left + update\n-\n-        # add axis for multi-head\n-        full_attention_mask = full_attention_mask[:, None, :, :]\n-\n-        return full_attention_mask\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -976,6 +1013,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], BaseModelOutputWithPooling]:\n         r\"\"\"\n@@ -1008,15 +1046,6 @@ def forward(\n \n         if input_ids is not None and inputs_embeds is not None:\n             raise ValueError(\"You cannot specify both input_ids and inputs_embeds at the same time\")\n-        elif input_ids is not None:\n-            self.warn_if_padding_and_no_attention_mask(input_ids, attention_mask)\n-            input_shape = input_ids.size()\n-        elif inputs_embeds is not None:\n-            input_shape = inputs_embeds.size()[:-1]\n-        else:\n-            raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n-\n-        seq_length = input_shape[1]\n \n         # past_key_values_length\n         past_key_values_length = 0\n@@ -1027,7 +1056,23 @@ def forward(\n                 else past_key_values.get_seq_length()\n             )\n \n-        projected_visual_features = None\n+        embedding_output = self.embeddings(\n+            input_ids=input_ids,\n+            position_ids=position_ids,\n+            inputs_embeds=inputs_embeds,\n+            past_key_values_length=past_key_values_length,\n+        )\n+\n+        if cache_position is None:\n+            cache_position = torch.arange(\n+                past_key_values_length,\n+                past_key_values_length + embedding_output.shape[1],\n+                device=embedding_output.device,\n+            )\n+\n+        # Always create `token_type_ids` so we can re-use Gemma3 style mask preparation fn\n+        token_type_ids = torch.zeros_like(embedding_output, dtype=torch.int)[..., 0]\n+\n         if pixel_values is not None:\n             if pixel_values.ndim == 4:\n                 # here we assume pixel_values is of shape (batch_size, num_channels, height, width)\n@@ -1053,60 +1098,54 @@ def forward(\n \n             projected_visual_features = self.visual_projection(visual_features)\n \n-        embedding_output = self.embeddings(\n-            input_ids=input_ids,\n-            position_ids=position_ids,\n-            inputs_embeds=inputs_embeds,\n-            past_key_values_length=past_key_values_length,\n-        )\n-\n-        if projected_visual_features is None:\n-            projected_visual_features = torch.zeros(\n-                (embedding_output.shape[0], 0, embedding_output.shape[2]),\n-                dtype=embedding_output.dtype,\n-                device=embedding_output.device,\n+            # Repeat visual features to match embedding batch size.\n+            projected_visual_features = projected_visual_features.repeat(\n+                embedding_output.size(0) // projected_visual_features.size(0), 1, 1\n             )\n \n-        # Repeat visual features to match embedding batch size.\n-        projected_visual_features = projected_visual_features.repeat(\n-            embedding_output.size(0) // projected_visual_features.size(0), 1, 1\n-        )\n-\n-        # concatenate patch token and text token embeddings\n-        hidden_states = torch.cat((projected_visual_features, embedding_output), dim=1)\n-\n-        # By default, an additive causal mask is created\n-        # for masking the future (one direction).\n-        tgt_mask = self._generate_future_mask(seq_length, embedding_output.dtype, embedding_output.device)\n+            # concatenate patch token and text token embeddings\n+            embedding_output = torch.cat((projected_visual_features, embedding_output), dim=1)\n+            image_token_type_ids = torch.ones_like(projected_visual_features, dtype=torch.int)[..., 0]\n+            token_type_ids = torch.cat([image_token_type_ids, token_type_ids], dim=-1)\n+            cache_position = torch.arange(embedding_output.shape[1], device=embedding_output.device, dtype=torch.int)\n+            if attention_mask is not None:\n+                attention_mask = torch.cat([torch.ones_like(image_token_type_ids), attention_mask], dim=-1)\n+        elif past_key_values is not None and input_ids.shape[1] == 1:\n+            # Expand attention mask and cache position with image tokens because GIT doesn't add image\n+            # placeholder tokens when processing. Doesn't worth the refactor, low usage!\n+            cache_position = torch.tensor(\n+                [past_key_values_length], dtype=cache_position.dtype, device=cache_position.device\n+            )\n+            extended_attention_mask = torch.ones(\n+                (attention_mask.shape[0], past_key_values_length - attention_mask.shape[1] + 1),\n+                dtype=attention_mask.dtype,\n+                device=attention_mask.device,\n+            )\n+            attention_mask = torch.cat([extended_attention_mask, attention_mask], dim=-1)\n \n-        # Create an attention mask of shape (batch_size, 1, tgt_seq_len, src_seq_len)\n-        combined_attention_mask = self.create_attention_mask(\n-            tgt=embedding_output,\n-            memory=projected_visual_features,\n-            tgt_mask=tgt_mask,\n-            past_key_values_length=past_key_values_length,\n+        # Images attend each other bidirectionally while text remains causal\n+        causal_mask = create_causal_mask_mapping(\n+            self.config,\n+            embedding_output,\n+            attention_mask,\n+            cache_position,\n+            past_key_values,\n+            None,\n+            token_type_ids,\n+            pixel_values,\n         )\n \n-        if attention_mask is not None:\n-            # if the user provides an attention mask, we add it to the default one\n-            # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n-            expanded_attn_mask = _prepare_4d_attention_mask(\n-                attention_mask, embedding_output.dtype, tgt_len=input_shape[-1]\n-            ).to(embedding_output.device)\n-            if past_key_values_length > 0:\n-                expanded_attn_mask = expanded_attn_mask[:, :, -past_key_values_length:, :]\n-            else:\n-                combined_attention_mask[:, :, -input_shape[1] :, -input_shape[1] :] += expanded_attn_mask\n+        hidden_states = embedding_output\n \n         encoder_outputs = self.encoder(\n             hidden_states,\n-            attention_mask=combined_attention_mask,\n+            attention_mask=causal_mask,\n             past_key_values=past_key_values,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n-            pixel_values_present=pixel_values is not None,\n+            cache_position=cache_position,\n         )\n         sequence_output = encoder_outputs[0]\n \n@@ -1160,6 +1199,7 @@ def forward(\n         interpolate_pos_encoding: bool = False,\n         return_dict: Optional[bool] = None,\n         logits_to_keep: Union[int, torch.Tensor] = 0,\n+        cache_position: Optional[torch.Tensor] = None,\n         **kwargs,\n     ) -> Union[tuple[torch.Tensor], CausalLMOutputWithPast]:\n         r\"\"\"\n@@ -1309,6 +1349,7 @@ def forward(\n             output_hidden_states=output_hidden_states,\n             interpolate_pos_encoding=interpolate_pos_encoding,\n             return_dict=return_dict,\n+            cache_position=cache_position,\n         )\n \n         hidden_states = outputs[0]\n@@ -1342,7 +1383,15 @@ def forward(\n         )\n \n     def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, attention_mask=None, use_cache=None, **kwargs\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        pixel_values=None,\n+        attention_mask=None,\n+        use_cache=None,\n+        cache_position=None,\n+        is_first_iteration=False,\n+        **kwargs,\n     ):\n         # Overwritten -- `git` has special cache handling and doesn't support generating from `inputs_embeds` atm\n \n@@ -1367,11 +1416,14 @@ def prepare_inputs_for_generation(\n         model_inputs = {\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n-            \"pixel_values\": kwargs.get(\"pixel_values\"),\n             \"past_key_values\": past_key_values,\n             \"use_cache\": use_cache,\n+            \"cache_position\": cache_position,\n         }\n \n+        if is_first_iteration or not use_cache:\n+            model_inputs[\"pixel_values\"] = pixel_values\n+\n         # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n         for key, value in kwargs.items():\n             if key not in model_inputs:"
        },
        {
            "sha": "e3ec9e6e605404cb726b5271584790dc5143b698",
            "filename": "src/transformers/models/glm46v/modeling_glm46v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm46v%2Fmodeling_glm46v.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -639,6 +639,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_grid_thw=None,\n         video_grid_thw=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -655,13 +656,14 @@ def prepare_inputs_for_generation(\n             image_grid_thw=image_grid_thw,\n             video_grid_thw=video_grid_thw,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         # GLM-4.1V position_ids are prepareed with rope_deltas in forward\n         model_inputs[\"position_ids\"] = None\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "2b527c0c6fd8ce4d0ee50cc91f113c5b9749fcdd",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1495,6 +1495,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_grid_thw=None,\n         video_grid_thw=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1511,13 +1512,14 @@ def prepare_inputs_for_generation(\n             image_grid_thw=image_grid_thw,\n             video_grid_thw=video_grid_thw,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         # GLM-4.1V position_ids are prepareed with rope_deltas in forward\n         model_inputs[\"position_ids\"] = None\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "040e32ea3751bb3b4b9bc7407a74dfddc3df0296",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1420,6 +1420,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_grid_thw=None,\n         video_grid_thw=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1436,13 +1437,14 @@ def prepare_inputs_for_generation(\n             image_grid_thw=image_grid_thw,\n             video_grid_thw=video_grid_thw,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         # GLM-4.1V position_ids are prepareed with rope_deltas in forward\n         model_inputs[\"position_ids\"] = None\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "728ae8ec8d3483d6d7d7101136b25782806764bf",
            "filename": "src/transformers/models/glm4v_moe/modeling_glm4v_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodeling_glm4v_moe.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1768,6 +1768,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_grid_thw=None,\n         video_grid_thw=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1784,13 +1785,14 @@ def prepare_inputs_for_generation(\n             image_grid_thw=image_grid_thw,\n             video_grid_thw=video_grid_thw,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         # GLM-4.1V position_ids are prepareed with rope_deltas in forward\n         model_inputs[\"position_ids\"] = None\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "174dd34523bed062f61c9d5dddedfb0361dc321b",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -797,6 +797,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -808,12 +809,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "ee12fecf1ed96e1b9e96ed84e2f4ef6dd3330fa4",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -470,6 +470,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward audio inputs to the model\n@@ -481,13 +482,14 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         # If we're in cached decoding stage, input_features should be None because\n         # input ids do not contain special audio token anymore Otherwise we need\n         # input feature values to be passed to the model\n-        if cache_position[0] == 0:\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n             model_inputs[\"input_features\"] = input_features\n         return model_inputs\n "
        },
        {
            "sha": "47b80286582560ad4f7602f285ffa7254df9d8a7",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1553,6 +1553,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n@@ -1585,7 +1586,7 @@ def prepare_inputs_for_generation(\n                 position_ids = position_ids[:, -input_ids.shape[1] :]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and empty_past_kv:\n+        if inputs_embeds is not None and is_first_iteration:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases"
        },
        {
            "sha": "10fcfb3d3534822317d4743dbe2320c93f848332",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -309,6 +309,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- has a unique cache type, `HybridMambaAttentionDynamicCache`\n@@ -341,7 +342,7 @@ def prepare_inputs_for_generation(\n                 position_ids = position_ids[:, -input_ids.shape[1] :]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and empty_past_kv:\n+        if inputs_embeds is not None and is_first_iteration:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases"
        },
        {
            "sha": "25afb43696bfe7721006b410c439177bbae52f6b",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1119,6 +1119,7 @@ def prepare_inputs_for_generation(\n         pixel_attention_mask=None,\n         image_hidden_states=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- there are mutually exclusive inputs (if the logic to make `image_hidden_states` take\n@@ -1134,10 +1135,11 @@ def prepare_inputs_for_generation(\n             pixel_attention_mask=pixel_attention_mask,\n             image_hidden_states=image_hidden_states,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if image_hidden_states is not None or cache_position[0] != 0:\n+        if image_hidden_states is not None or not is_first_iteration:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_attention_mask\"] = None\n "
        },
        {
            "sha": "7a4ed8453cbe0cd381125866de0288669bc34c7e",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -889,6 +889,7 @@ def prepare_inputs_for_generation(\n         pixel_attention_mask=None,\n         image_hidden_states=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- there are mutually exclusive inputs (if the logic to make `image_hidden_states` take\n@@ -904,10 +905,11 @@ def prepare_inputs_for_generation(\n             pixel_attention_mask=pixel_attention_mask,\n             image_hidden_states=image_hidden_states,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if image_hidden_states is not None or cache_position[0] != 0:\n+        if image_hidden_states is not None or not is_first_iteration:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_attention_mask\"] = None\n "
        },
        {
            "sha": "4b8aa15c669ea63a200d52cbc598b99395ae53b2",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -897,6 +897,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -908,12 +909,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "bd81f3be1027b99dd27fae4a86d9add51f618edd",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1254,6 +1254,7 @@ def prepare_inputs_for_generation(\n         inputs_embeds=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- extra custom processing\n@@ -1265,12 +1266,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-        # Otherwise we need pixel values to be passed to model\n-        if cache_position[0] == 0:\n+        # Pixel values are used only in the first iteration if available\n+        # In subsquent iterations, they are already merged with text and cached\n+        # NOTE: first iteration doesn't have to be prefill, it can be the first\n+        # iteration with a question and cached system prompt (continue generate from cache)\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "6159dcb7e39c0cb6b033b9f72cdea87a832393cb",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1071,6 +1071,7 @@ def prepare_inputs_for_generation(\n         inputs_embeds=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- extra custom processing\n@@ -1082,12 +1083,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-        # Otherwise we need pixel values to be passed to model\n-        if cache_position[0] == 0:\n+        # Pixel values are used only in the first iteration if available\n+        # In subsquent iterations, they are already merged with text and cached\n+        # NOTE: first iteration doesn't have to be prefill, it can be the first\n+        # iteration with a question and cached system prompt (continue generate from cache)\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "d421c5dd5c71fa3c223be7e75aae2ba0cb7e604d",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 2,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1389,12 +1389,16 @@ def prepare_inputs_for_generation(\n         inputs_embeds=None,\n         use_cache=None,\n         cache_position=None,\n+        is_first_iteration=False,\n         **model_kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n \n-        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-        if cache_position[0] != 0:\n+        # Pixel values are used only in the first iteration if available\n+        # In subsquent iterations, they are already merged with text and cached\n+        # NOTE: first iteration doesn't have to be prefill, it can be the first\n+        # iteration with a question and cached system prompt (continue generate from cache)\n+        if not is_first_iteration and use_cache:\n             image_embeds = None\n             image_embeds_position_mask = None\n \n@@ -1419,6 +1423,7 @@ def prepare_inputs_for_generation(\n             inputs_embeds=inputs_embeds,\n             use_cache=use_cache,\n             cache_position=cache_position,\n+            is_first_iteration=is_first_iteration,\n             **model_kwargs,\n         )\n         # Kosmos2 has offset for position ids, so we need to create them correctly in PositionEmbedding layer"
        },
        {
            "sha": "93d814357f39b39ccbe9fbcffd1771bcd3b0ba04",
            "filename": "src/transformers/models/kosmos2_5/modeling_kosmos2_5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fmodeling_kosmos2_5.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1608,6 +1608,7 @@ def prepare_inputs_for_generation(\n         use_cache=None,\n         cache_position=None,\n         position_ids=None,\n+        is_first_iteration=False,\n         **model_kwargs,\n     ):\n         input_shape = input_ids.shape\n@@ -1812,6 +1813,7 @@ def prepare_inputs_for_generation(\n         use_cache=None,\n         cache_position=None,\n         position_ids=None,\n+        is_first_iteration=False,\n         **model_kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1825,10 +1827,11 @@ def prepare_inputs_for_generation(\n             use_cache=use_cache,\n             cache_position=cache_position,\n             position_ids=position_ids,\n+            is_first_iteration=is_first_iteration,\n             **model_kwargs,\n         )\n \n-        if cache_position[0] == 0:\n+        if is_first_iteration or not use_cache:\n             # If we're in cached decoding stage, `flattened_patches` should be `None` because `input_ids` do not contain special image token anymore\n             # Otherwise we need `flattened_patches` to be passed to model\n             model_inputs[\"flattened_patches\"] = flattened_patches"
        },
        {
            "sha": "725d7cdc7415978405a3d12f83e9936b05d94336",
            "filename": "src/transformers/models/lfm2_vl/modeling_lfm2_vl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fmodeling_lfm2_vl.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -448,6 +448,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -459,12 +460,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "6902cb86bc88f35389cb847457290bfe9a6aceb9",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1387,6 +1387,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1398,12 +1399,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "9a3d6890f753e6db1c2d92eebe09e3fd88039bb2",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -438,6 +438,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -449,12 +450,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "b11944b77da655d777e603f0c97a663680f1b965",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -692,6 +692,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -703,12 +704,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-        # Otherwise we need pixel values to be passed to model\n-        if cache_position[0] == 0:\n+        # Pixel values are used only in the first iteration if available\n+        # In subsquent iterations, they are already merged with text and cached\n+        # NOTE: first iteration doesn't have to be prefill, it can be the first\n+        # iteration with a question and cached system prompt (continue generate from cache)\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"image_sizes\"] = image_sizes\n "
        },
        {
            "sha": "53f1c0cdba94e98258829a7fd7e009d3d3cad6ab",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -868,6 +868,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- extra custom processing\n@@ -879,12 +880,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-        # Otherwise we need pixel values to be passed to model\n-        if cache_position[0] == 0:\n+        # Pixel values are used only in the first iteration if available\n+        # In subsquent iterations, they are already merged with text and cached\n+        # NOTE: first iteration doesn't have to be prefill, it can be the first\n+        # iteration with a question and cached system prompt (continue generate from cache)\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n             model_inputs[\"image_sizes\"] = image_sizes"
        },
        {
            "sha": "780706b70a9ec8aed06099d6287d3cbbfc3ff23a",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -693,6 +693,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- extra custom processing\n@@ -704,12 +705,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-        # Otherwise we need pixel values to be passed to model\n-        if cache_position[0] == 0:\n+        # Pixel values are used only in the first iteration if available\n+        # In subsquent iterations, they are already merged with text and cached\n+        # NOTE: first iteration doesn't have to be prefill, it can be the first\n+        # iteration with a question and cached system prompt (continue generate from cache)\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n             model_inputs[\"image_sizes\"] = image_sizes"
        },
        {
            "sha": "dc706ccdcb57d052421cc447d145ac9777f1b5c1",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -846,6 +846,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -857,12 +858,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"image_sizes\"] = image_sizes\n             model_inputs[\"pixel_values_videos\"] = pixel_values_videos"
        },
        {
            "sha": "aff9fb52b8abb0aa3db9d2d3c438cd2583b0ce57",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -697,6 +697,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -708,12 +709,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"image_sizes\"] = image_sizes\n             model_inputs[\"pixel_values_videos\"] = pixel_values_videos"
        },
        {
            "sha": "afb347b6a74ab6252fa4dac732d131209e539255",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -750,6 +750,7 @@ def prepare_inputs_for_generation(\n         cache_params: Optional[MambaCache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.LongTensor] = None,\n+        is_first_iteration: Optional[bool] = False,\n         **kwargs,\n     ):\n         # Overwritten -- uses `cache_params` as opposed to `past_key_values`"
        },
        {
            "sha": "09dddc60a3994b45386214304277e2e7458b2add",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -955,6 +955,7 @@ def prepare_inputs_for_generation(\n         cache_params: Optional[Mamba2Cache] = None,\n         cache_position: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n+        is_first_iteration: Optional[bool] = False,\n         **kwargs,\n     ):\n         # Overwritten -- uses `cache_params` as opposed to `past_key_values`"
        },
        {
            "sha": "12c890bd28fcca0ebd12d14ebfaaf2b094c429d0",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -491,6 +491,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -502,12 +503,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "71408e610ff519e5219023f087851340b6914217",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1730,6 +1730,7 @@ def prepare_inputs_for_generation(\n         use_cache=False,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1747,12 +1748,13 @@ def prepare_inputs_for_generation(\n             cross_attention_mask=cross_attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         # If we're in pre-fill or cacheless decoding step, then we need pixel_values and aspect ratios\n         # to compute image hidden states, otherwise they are cached within each cross attn layer\n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"aspect_ratio_ids\"] = None\n             model_inputs[\"aspect_ratio_mask\"] = None"
        },
        {
            "sha": "247a9f2da7dd204ea865e19f88eb369a6f505863",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 16,
            "deletions": 48,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -2180,6 +2180,7 @@ def prepare_inputs_for_generation(\n         user_delay_pattern_mask=None,\n         moshi_delay_pattern_mask=None,\n         kwargs_depth_decoder=None,\n+        is_first_iteration=False,\n         blank_user_audio_codes: Optional[torch.FloatTensor] = None,\n         **kwargs,\n     ):\n@@ -2191,49 +2192,21 @@ def prepare_inputs_for_generation(\n         # Exception 3: with synced GPUs cache_position may go out of bounds, but we only want dummy token in that case.\n         # (we can't check exception 3 while compiling)\n \n-        if past_key_values is not None:\n-            if (\n-                inputs_embeds is not None  # Exception 1\n-                or cache_position[-1] >= input_ids.shape[1]  # Exception 3\n-            ):\n-                input_ids = input_ids[:, -cache_position.shape[0] :]\n-            elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n-                input_ids = input_ids[:, cache_position]\n-\n-        # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and cache_position[0] == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": None}\n-        else:\n-            model_inputs = {\"input_ids\": input_ids, \"inputs_embeds\": None}\n-\n-        if isinstance(past_key_values, StaticCache) and attention_mask.ndim == 2:\n-            if model_inputs[\"inputs_embeds\"] is not None:\n-                batch_size, sequence_length, _ = inputs_embeds.shape\n-                device = inputs_embeds.device\n-            else:\n-                batch_size, sequence_length = input_ids.shape\n-                device = input_ids.device\n-\n-            attention_mask = self.decoder.model._prepare_4d_causal_attention_mask_with_cache_position(\n-                attention_mask,\n-                sequence_length=sequence_length,\n-                target_length=past_key_values.get_max_cache_shape(),\n-                dtype=self.decoder.lm_head.weight.dtype,\n-                device=device,\n-                cache_position=cache_position,\n-                batch_size=batch_size,\n-                config=self.config,\n-                past_key_values=past_key_values,\n-            )\n-\n-        model_inputs.update(\n-            {\n-                \"position_ids\": position_ids,\n-                \"past_key_values\": past_key_values,\n-                \"use_cache\": use_cache,\n-                \"attention_mask\": attention_mask,\n-                \"cache_position\": cache_position,\n-            }\n+        model_inputs = super().prepare_inputs_for_generation(\n+            input_ids,\n+            past_key_values=past_key_values,\n+            attention_mask=attention_mask,\n+            inputs_embeds=inputs_embeds,\n+            cache_position=cache_position,\n+            position_ids=position_ids,\n+            use_cache=use_cache,\n+            logits_to_keep=logits_to_keep,\n+            user_delay_pattern_mask=user_delay_pattern_mask,\n+            moshi_delay_pattern_mask=moshi_delay_pattern_mask,\n+            kwargs_depth_decoder=kwargs_depth_decoder,\n+            is_first_iteration=is_first_iteration,\n+            blank_user_audio_codes=blank_user_audio_codes,\n+            **kwargs,\n         )\n \n         # 2. Now that everything is prepared, generate audio_codes using the depth decoder\n@@ -2272,11 +2245,6 @@ def prepare_inputs_for_generation(\n             model_inputs[\"input_ids\"] = None\n             model_inputs[\"inputs_embeds\"] = inputs_embeds\n \n-        # Forward ALL kwargs that are uninitialized (e.g. `use_cache`).\n-        for key, value in kwargs.items():\n-            if key not in model_inputs:\n-                model_inputs[key] = value\n-\n         return model_inputs\n \n     def _update_model_kwargs_for_generation("
        },
        {
            "sha": "fcdeccc49be101f3b76a59175df909b644b8ca7b",
            "filename": "src/transformers/models/ovis2/modeling_ovis2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fmodeling_ovis2.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -788,6 +788,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -799,12 +800,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "79c7a6bf1d2aeff1eda36f85a56fd1385af80aff",
            "filename": "src/transformers/models/paddleocr_vl/modeling_paddleocr_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodeling_paddleocr_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodeling_paddleocr_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaddleocr_vl%2Fmodeling_paddleocr_vl.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1478,6 +1478,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_grid_thw=None,\n         video_grid_thw=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1494,6 +1495,7 @@ def prepare_inputs_for_generation(\n             image_grid_thw=image_grid_thw,\n             video_grid_thw=video_grid_thw,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n@@ -1525,7 +1527,7 @@ def prepare_inputs_for_generation(\n             text_positions = model_inputs[\"position_ids\"][None, ...]\n             model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n-        if model_inputs[\"cache_position\"][0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "faa89d34fafa8ffeffe9ff0960c321fbcf19aa56",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 25,
            "deletions": 17,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -149,7 +149,8 @@ def create_causal_mask_mapping(\n     position_ids: Optional[torch.Tensor],\n     token_type_ids: Optional[torch.Tensor] = None,\n     pixel_values: Optional[torch.FloatTensor] = None,\n-    is_training: bool = False,\n+    is_training: Optional[bool] = False,\n+    is_first_iteration: Optional[bool] = None,\n     **kwargs,\n ) -> dict:\n     \"\"\"\n@@ -169,31 +170,33 @@ def create_causal_mask_mapping(\n         \"past_key_values\": past_key_values,\n         \"position_ids\": position_ids,\n     }\n-    # NOTE: this `is_prompt` logic is not flawless, it fails when we're using a cache eagerly initialized\n-    # (e.g. compiled prefill) AND `pixel_values` are not provided (i.e. the image data is provided through other\n-    # means). Determining prefill in that case requires checking data values, which is not compile-compatible.\n-    maybe_is_prompt = past_key_values is None or not past_key_values.is_initialized or pixel_values is not None\n-\n-    if maybe_is_prompt:\n+    # Infer if prefill or decoding stage, if the flag isn't passed. This happens only when the mask is constructed\n+    # from `forward` call. If users run a `forward` call, we have no option to infer `is_first_iteration` because users may be\n+    # running generation with custom loop. Thus we need to infer it in a `non-perfect` way\n+    # NOTE: Determining prefill in that case requires checking data values, which is not compile-compatible.\n+    is_first_iteration = (\n+        is_first_iteration\n+        if is_first_iteration\n+        else (past_key_values is None or not past_key_values.is_initialized or pixel_values is not None)\n+    )\n+\n+    if is_first_iteration or not kwargs.get(\"use_cache\", True):\n         if token_type_ids is not None:\n             # The logic bellow was originally written for Gemma3, where `token_type_ids` is reversed. Let's reverse\n             # it to then use exactly the same logic.\n             token_type_ids = 1 - token_type_ids\n         else:\n             logger.warning_once(\n-                \"The input may be the prompt, but `token_type_ids` is not provided. We recommend \"\n+                \"It is a prefill stage but The `token_type_ids` is not provided. We recommend \"\n                 \"passing `token_type_ids` to the model to prevent bad attention masking.\"\n             )\n-            # BC: when NOT training, use bidirectional mask if sequence length > 1. Otherwise, use the default causal\n-            # mask. This is incorrect in some advanced use cases, hence the warning above.\n             # NOTE: this branch can't be reached when training because `token_type_ids` is required as a model input.\n-            if input_embeds.shape[1] > 1:\n-                token_type_ids = torch.ones_like(input_embeds)[:, :, 0]\n+            token_type_ids = torch.ones_like(input_embeds)[:, :, 0]\n \n     # Logic originally copied from Gemma3. It holds up for Paligemma as well because Paligemma assumes up to one image\n     # per prompt AND we reverse `token_type_ids` above. Gemma3 uses a bidirectional mask for images, tagged through\n     # `token_type_ids` 1s.\n-    if token_type_ids is not None and maybe_is_prompt:\n+    if token_type_ids is not None and is_first_iteration:\n         # We need to pass an additional mask function to account for token type ids, and it needs to be an `or` (to\n         # undo the causal masking)\n \n@@ -550,6 +553,7 @@ def prepare_inputs_for_generation(\n         use_cache=True,\n         logits_to_keep=None,\n         labels=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- custom `position_ids` and `pixel_values` handling\n@@ -563,16 +567,19 @@ def prepare_inputs_for_generation(\n             use_cache=use_cache,\n             logits_to_keep=logits_to_keep,\n             token_type_ids=token_type_ids,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         # position_ids in Paligemma are 1-indexed\n         if model_inputs.get(\"position_ids\") is not None:\n             model_inputs[\"position_ids\"] += 1\n \n-        # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-        # Otherwise we need pixel values to be passed to model. NOTE: use_cache=False needs pixel_values always\n-        if cache_position[0] == 0:\n+        # Pixel values are used only in the first iteration if available\n+        # In subsquent iterations, they are already merged with text and cached\n+        # NOTE: first iteration doesn't have to be prefill, it can be the first\n+        # iteration with a question and cached system prompt (continue generate from cache). NOTE: use_cache=False needs pixel_values always\n+        if is_first_iteration or not use_cache:\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs\n@@ -586,6 +593,7 @@ def create_masks_for_generate(\n         past_key_values: Optional[Cache],\n         position_ids: Optional[torch.Tensor],\n         token_type_ids: Optional[torch.Tensor] = None,\n+        is_first_iteration: Optional[bool] = False,\n         **kwargs,\n     ) -> dict:\n         # Uses the overwritten `create_masks_for_generate` with `token_type_ids` masking\n@@ -597,7 +605,7 @@ def create_masks_for_generate(\n             past_key_values,\n             position_ids,\n             token_type_ids,\n-            pixel_values=kwargs.get(\"pixel_values\"),\n+            is_first_iteration=is_first_iteration,\n             **{k: v for k, v in kwargs.items() if k != \"pixel_values\"},\n         )\n "
        },
        {
            "sha": "f8ba4db46fbcdb9a34c44b9c1a5c9f40289c6946",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -451,6 +451,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -462,12 +463,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n         return model_inputs"
        },
        {
            "sha": "00f10e01767f94ee9552517a64b97e23e1be4027",
            "filename": "src/transformers/models/perception_lm/modular_perception_lm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodular_perception_lm.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -293,6 +293,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -304,12 +305,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n             model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n         return model_inputs"
        },
        {
            "sha": "13d3f6f4500ca0bb3f7257e1a04ab1d7317fa6ba",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1848,6 +1848,7 @@ def prepare_inputs_for_generation(\n         past_key_values=None,\n         attention_mask=None,\n         use_cache=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- our tests complain if we use GenerationMixin.prepare_inputs_for_generation\n@@ -1856,7 +1857,7 @@ def prepare_inputs_for_generation(\n         if attention_mask is None:\n             attention_mask = input_ids.new_ones(input_ids.shape)\n \n-        if past_key_values is not None and past_key_values.get_seq_length() > 0:\n+        if past_key_values is not None and not is_first_iteration:\n             input_ids = input_ids[:, -1:]\n         # first step, decoder_cached_states are empty\n         model_inputs = {"
        },
        {
            "sha": "ff4ee4d26fd580c68becd90b8d3ba14cd306a3e1",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -2104,6 +2104,7 @@ def prepare_inputs_for_generation(\n         feature_attention_mask=None,\n         use_audio_in_video=False,\n         video_second_per_grid=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         model_inputs = super().prepare_inputs_for_generation(\n@@ -2122,12 +2123,13 @@ def prepare_inputs_for_generation(\n             feature_attention_mask=feature_attention_mask,\n             use_audio_in_video=use_audio_in_video,\n             video_second_per_grid=video_second_per_grid,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         model_inputs[\"position_ids\"] = None\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n             model_inputs[\"input_features\"] = None"
        },
        {
            "sha": "6d0c3f9bcc6960fadd29cc6aaa7b41f727d382e9",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -2407,6 +2407,7 @@ def prepare_inputs_for_generation(\n         feature_attention_mask=None,\n         use_audio_in_video=False,\n         video_second_per_grid=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         model_inputs = super().prepare_inputs_for_generation(\n@@ -2425,12 +2426,13 @@ def prepare_inputs_for_generation(\n             feature_attention_mask=feature_attention_mask,\n             use_audio_in_video=use_audio_in_video,\n             video_second_per_grid=video_second_per_grid,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         model_inputs[\"position_ids\"] = None\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n             model_inputs[\"input_features\"] = None"
        },
        {
            "sha": "1f52aa42167b45bcfb8ba545b4cea0d810f9a1ef",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1536,6 +1536,7 @@ def prepare_inputs_for_generation(\n         image_grid_thw=None,\n         video_grid_thw=None,\n         second_per_grid_ts=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1553,6 +1554,7 @@ def prepare_inputs_for_generation(\n             video_grid_thw=video_grid_thw,\n             second_per_grid_ts=second_per_grid_ts,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n@@ -1562,7 +1564,7 @@ def prepare_inputs_for_generation(\n             # When compiling, we can't check tensor values thus we check only input length\n             # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n             # models currently cannot do assisted decoding\n-            if cache_position[0] == 0 or self.model.rope_deltas is None:\n+            if (cache_position[0] == 0 or not use_cache) or self.model.rope_deltas is None:\n                 vision_positions, rope_deltas = self.model.get_rope_index(\n                     model_inputs.get(\"input_ids\", None),\n                     image_grid_thw=image_grid_thw,\n@@ -1585,7 +1587,7 @@ def prepare_inputs_for_generation(\n             text_positions = model_inputs[\"position_ids\"][None, ...]\n             model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "a319263bed3c6ff9e976d46ec9a61ee0fc5122fa",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -778,6 +778,7 @@ def prepare_inputs_for_generation(\n         image_grid_thw=None,\n         video_grid_thw=None,\n         second_per_grid_ts=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -795,6 +796,7 @@ def prepare_inputs_for_generation(\n             video_grid_thw=video_grid_thw,\n             second_per_grid_ts=second_per_grid_ts,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n@@ -804,7 +806,7 @@ def prepare_inputs_for_generation(\n             # When compiling, we can't check tensor values thus we check only input length\n             # It is safe to assume that `length!=1` means we're in pre-fill because compiled\n             # models currently cannot do assisted decoding\n-            if cache_position[0] == 0 or self.model.rope_deltas is None:\n+            if (cache_position[0] == 0 or not use_cache) or self.model.rope_deltas is None:\n                 vision_positions, rope_deltas = self.model.get_rope_index(\n                     model_inputs.get(\"input_ids\", None),\n                     image_grid_thw=image_grid_thw,\n@@ -827,7 +829,7 @@ def prepare_inputs_for_generation(\n             text_positions = model_inputs[\"position_ids\"][None, ...]\n             model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "4051096abe5ef947620e75d7d0af0245c8bae0fa",
            "filename": "src/transformers/models/qwen2_audio/modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fmodeling_qwen2_audio.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -848,11 +848,11 @@ def prepare_inputs_for_generation(self, *args, **kwargs):\n         # Overwritten -- we should not pass input_features when we are in cached decoding stage\n \n         input_features = kwargs.pop(\"input_features\", None)\n-        cache_position = kwargs.get(\"cache_position\")\n+        is_first_iteration = kwargs.get(\"is_first_iteration\", False)\n \n         model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n \n-        if cache_position is not None and cache_position[0] == 0:\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n             # input_features should only be passed when we are not in cached decoding stage\n             model_inputs[\"input_features\"] = input_features\n "
        },
        {
            "sha": "68d9fbed254283684a9b2f27c061c10c003df8e6",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1427,6 +1427,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_grid_thw=None,\n         video_grid_thw=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1443,6 +1444,7 @@ def prepare_inputs_for_generation(\n             image_grid_thw=image_grid_thw,\n             video_grid_thw=video_grid_thw,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n@@ -1474,7 +1476,7 @@ def prepare_inputs_for_generation(\n             text_positions = model_inputs[\"position_ids\"][None, ...]\n             model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n-        if model_inputs[\"cache_position\"][0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "ff3a5ef41c774f24e155f35e7ca3bbc1c8f161ab",
            "filename": "src/transformers/models/qwen3_omni_moe/modeling_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 19,
            "deletions": 4,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodeling_qwen3_omni_moe.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -2268,6 +2268,7 @@ def prepare_inputs_for_generation(\n         feature_attention_mask=None,\n         use_audio_in_video=False,\n         video_second_per_grid=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         model_inputs = super().prepare_inputs_for_generation(\n@@ -2286,12 +2287,13 @@ def prepare_inputs_for_generation(\n             feature_attention_mask=feature_attention_mask,\n             use_audio_in_video=use_audio_in_video,\n             video_second_per_grid=video_second_per_grid,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n         model_inputs[\"position_ids\"] = None\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n             model_inputs[\"input_features\"] = None\n@@ -3233,18 +3235,31 @@ def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_\n         return model_kwargs\n \n     def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, cache_position=None, **kwargs\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        is_first_iteration=False,\n+        **kwargs,\n     ):\n         hidden_states = kwargs.pop(\"hidden_states\", None)\n         inputs = super().prepare_inputs_for_generation(\n-            input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs\n+            input_ids,\n+            past_key_values,\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            is_first_iteration=is_first_iteration,\n+            **kwargs,\n         )\n \n         # Qwen3-Omni will prepare position ids in forward with deltas\n         inputs[\"position_ids\"] = None\n \n         # TODO(raushan, gante): Refactor this part to a utility function\n-        if cache_position[0] != 0:\n+        if not is_first_iteration and kwargs.get(\"use_cache\", True):\n             input_ids = input_ids[:, -1:]\n             generation_step = kwargs.get(\"generation_step\")\n             trailing_text_hidden = kwargs.get(\"trailing_text_hidden\")"
        },
        {
            "sha": "79ae9f95c3747603558a1acec26f0a6744519ebd",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 16,
            "deletions": 3,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -2055,18 +2055,31 @@ def _update_model_kwargs_for_generation(self, outputs, model_kwargs, is_encoder_\n         return model_kwargs\n \n     def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, attention_mask=None, inputs_embeds=None, cache_position=None, **kwargs\n+        self,\n+        input_ids,\n+        past_key_values=None,\n+        attention_mask=None,\n+        inputs_embeds=None,\n+        cache_position=None,\n+        is_first_iteration=False,\n+        **kwargs,\n     ):\n         hidden_states = kwargs.pop(\"hidden_states\", None)\n         inputs = super().prepare_inputs_for_generation(\n-            input_ids, past_key_values, attention_mask, inputs_embeds, cache_position, **kwargs\n+            input_ids,\n+            past_key_values,\n+            attention_mask,\n+            inputs_embeds,\n+            cache_position,\n+            is_first_iteration=is_first_iteration,\n+            **kwargs,\n         )\n \n         # Qwen3-Omni will prepare position ids in forward with deltas\n         inputs[\"position_ids\"] = None\n \n         # TODO(raushan, gante): Refactor this part to a utility function\n-        if cache_position[0] != 0:\n+        if not is_first_iteration and kwargs.get(\"use_cache\", True):\n             input_ids = input_ids[:, -1:]\n             generation_step = kwargs.get(\"generation_step\")\n             trailing_text_hidden = kwargs.get(\"trailing_text_hidden\")"
        },
        {
            "sha": "da6fb7bc4ba3792def9f70128bb93ce0c455633a",
            "filename": "src/transformers/models/qwen3_vl/modeling_qwen3_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodeling_qwen3_vl.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1418,6 +1418,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_grid_thw=None,\n         video_grid_thw=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1434,6 +1435,7 @@ def prepare_inputs_for_generation(\n             image_grid_thw=image_grid_thw,\n             video_grid_thw=video_grid_thw,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n@@ -1465,7 +1467,7 @@ def prepare_inputs_for_generation(\n             text_positions = model_inputs[\"position_ids\"][None, ...]\n             model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "1fb2a6993b1071a55225d42f79484e3b8f05e752",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1199,6 +1199,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_grid_thw=None,\n         video_grid_thw=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1215,6 +1216,7 @@ def prepare_inputs_for_generation(\n             image_grid_thw=image_grid_thw,\n             video_grid_thw=video_grid_thw,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n@@ -1246,7 +1248,7 @@ def prepare_inputs_for_generation(\n             text_positions = model_inputs[\"position_ids\"][None, ...]\n             model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "881fbcc44266e349f9277c13c40efdb2dc8968c6",
            "filename": "src/transformers/models/qwen3_vl_moe/modeling_qwen3_vl_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl_moe%2Fmodeling_qwen3_vl_moe.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1642,6 +1642,7 @@ def prepare_inputs_for_generation(\n         pixel_values_videos=None,\n         image_grid_thw=None,\n         video_grid_thw=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -1658,6 +1659,7 @@ def prepare_inputs_for_generation(\n             image_grid_thw=image_grid_thw,\n             video_grid_thw=video_grid_thw,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n@@ -1689,7 +1691,7 @@ def prepare_inputs_for_generation(\n             text_positions = model_inputs[\"position_ids\"][None, ...]\n             model_inputs[\"position_ids\"] = torch.cat([text_positions, vision_positions], dim=0)\n \n-        if cache_position[0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "a8b4c4cd7eff13943adfe077042f4b6e5bcacf18",
            "filename": "src/transformers/models/reformer/modeling_reformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Freformer%2Fmodeling_reformer.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -2247,7 +2247,7 @@ def forward(\n         )\n \n     def prepare_inputs_for_generation(\n-        self, input_ids, past_key_values=None, use_cache=None, num_hashes=None, **kwargs\n+        self, input_ids, past_key_values=None, use_cache=None, num_hashes=None, is_first_iteration=False, **kwargs\n     ):\n         # Overitten -- different expected inputs/outputs\n "
        },
        {
            "sha": "70c6f5eb8b6c4a17f03a2e00955f91708c240e0b",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -708,7 +708,7 @@ def forward(\n             attentions=outputs.attentions,\n         )\n \n-    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, **model_kwargs):\n+    def prepare_inputs_for_generation(self, input_ids, attention_mask=None, is_first_iteration=False, **model_kwargs):\n         input_shape = input_ids.shape\n         effective_batch_size = input_shape[0]\n "
        },
        {
            "sha": "f720a0ddba7c4db318bc4df78d15511526f065ff",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -855,6 +855,7 @@ def prepare_inputs_for_generation(\n         pixel_attention_mask=None,\n         image_hidden_states=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- there are mutually exclusive inputs (if the logic to make `image_hidden_states` take\n@@ -870,10 +871,11 @@ def prepare_inputs_for_generation(\n             pixel_attention_mask=pixel_attention_mask,\n             image_hidden_states=image_hidden_states,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if image_hidden_states is not None or cache_position[0] != 0:\n+        if image_hidden_states is not None or not is_first_iteration:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_attention_mask\"] = None\n "
        },
        {
            "sha": "a8eec7e010d43e6eca442f6cb4210f374a760a2b",
            "filename": "src/transformers/models/video_llama_3/modeling_video_llama_3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodeling_video_llama_3.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -864,6 +864,7 @@ def prepare_inputs_for_generation(\n         video_grid_thw: Optional[torch.LongTensor] = None,\n         video_merge_sizes: Optional[torch.LongTensor] = None,\n         video_compression_mask: Optional[torch.BoolTensor] = None,\n+        is_first_iteration: Optional[bool] = False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -883,10 +884,11 @@ def prepare_inputs_for_generation(\n             video_merge_sizes=video_merge_sizes,\n             video_compression_mask=video_compression_mask,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if model_inputs[\"cache_position\"][0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "92a5776d4e51d6ff2337a4c0ee3ea65713c3178e",
            "filename": "src/transformers/models/video_llama_3/modular_video_llama_3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -849,6 +849,7 @@ def prepare_inputs_for_generation(\n         video_grid_thw: Optional[torch.LongTensor] = None,\n         video_merge_sizes: Optional[torch.LongTensor] = None,\n         video_compression_mask: Optional[torch.BoolTensor] = None,\n+        is_first_iteration: Optional[bool] = False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -868,10 +869,11 @@ def prepare_inputs_for_generation(\n             video_merge_sizes=video_merge_sizes,\n             video_compression_mask=video_compression_mask,\n             use_cache=use_cache,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if model_inputs[\"cache_position\"][0] != 0:\n+        if not is_first_iteration and use_cache:\n             model_inputs[\"pixel_values\"] = None\n             model_inputs[\"pixel_values_videos\"] = None\n "
        },
        {
            "sha": "1f79c5b6f09ae2181ede4328fa9c9c0b83c8aad9",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -599,6 +599,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -610,12 +611,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values_images\"] = pixel_values_images\n             model_inputs[\"pixel_values_videos\"] = pixel_values_videos\n "
        },
        {
            "sha": "e39ec8bf90b4cfca8d3ad66217009bf84ba45eba",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -415,6 +415,7 @@ def prepare_inputs_for_generation(\n         attention_mask=None,\n         cache_position=None,\n         logits_to_keep=None,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- in specific circumstances we don't want to forward image inputs to the model\n@@ -426,12 +427,15 @@ def prepare_inputs_for_generation(\n             attention_mask=attention_mask,\n             cache_position=cache_position,\n             logits_to_keep=logits_to_keep,\n+            is_first_iteration=is_first_iteration,\n             **kwargs,\n         )\n \n-        if cache_position[0] == 0:\n-            # If we're in cached decoding stage, pixel values should be None because input ids do not contain special image token anymore\n-            # Otherwise we need pixel values to be passed to model\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n+            # Pixel values are used only in the first iteration if available\n+            # In subsquent iterations, they are already merged with text and cached\n+            # NOTE: first iteration doesn't have to be prefill, it can be the first\n+            # iteration with a question and cached system prompt (continue generate from cache)\n             model_inputs[\"pixel_values\"] = pixel_values\n \n         return model_inputs"
        },
        {
            "sha": "4084934c653affc6ccf49b5699e01cc7e1dcf000",
            "filename": "src/transformers/models/voxtral/modeling_voxtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -505,11 +505,11 @@ def prepare_inputs_for_generation(self, *args, **kwargs):\n         # Overwritten -- we should not pass input_features when we are in cached decoding stage\n \n         input_features = kwargs.pop(\"input_features\", None)\n-        cache_position = kwargs.get(\"cache_position\")\n+        is_first_iteration = kwargs.get(\"is_first_iteration\", False)\n \n         model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n \n-        if cache_position is not None and cache_position[0] == 0:\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n             # input_features should only be passed when we are not in cached decoding stage\n             model_inputs[\"input_features\"] = input_features\n "
        },
        {
            "sha": "d2cf20d4321a5436d427b20c9bd59226ce3f4ea1",
            "filename": "src/transformers/models/voxtral/modular_voxtral.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -267,11 +267,11 @@ def prepare_inputs_for_generation(self, *args, **kwargs):\n         # Overwritten -- we should not pass input_features when we are in cached decoding stage\n \n         input_features = kwargs.pop(\"input_features\", None)\n-        cache_position = kwargs.get(\"cache_position\")\n+        is_first_iteration = kwargs.get(\"is_first_iteration\", False)\n \n         model_inputs = super().prepare_inputs_for_generation(*args, **kwargs)\n \n-        if cache_position is not None and cache_position[0] == 0:\n+        if is_first_iteration or not kwargs.get(\"use_cache\", True):\n             # input_features should only be passed when we are not in cached decoding stage\n             model_inputs[\"input_features\"] = input_features\n "
        },
        {
            "sha": "1e2ef238192130212936f8f7f6b6d55fbf89bf09",
            "filename": "src/transformers/models/xlm/modeling_xlm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm%2Fmodeling_xlm.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -945,7 +945,7 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.pred_layer.proj = new_embeddings\n \n-    def prepare_inputs_for_generation(self, input_ids, **kwargs):\n+    def prepare_inputs_for_generation(self, input_ids, is_first_iteration=False, **kwargs):\n         # Overwritten -- this model uses config options to prepare inputs\n \n         mask_token_id = self.config.mask_token_id"
        },
        {
            "sha": "77023c1bd66f2754b1cd962987fedf556e652022",
            "filename": "src/transformers/models/xlnet/modeling_xlnet.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlnet%2Fmodeling_xlnet.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1244,7 +1244,9 @@ def get_output_embeddings(self):\n     def set_output_embeddings(self, new_embeddings):\n         self.lm_loss = new_embeddings\n \n-    def prepare_inputs_for_generation(self, input_ids, past_key_values=None, use_mems=None, **kwargs):\n+    def prepare_inputs_for_generation(\n+        self, input_ids, past_key_values=None, use_mems=None, is_first_iteration=False, **kwargs\n+    ):\n         # Overwritten -- this model has unique input preparation\n \n         # Add dummy token at the end (no attention on this one)"
        },
        {
            "sha": "c70df5f500f3a9992e52056d52230aa920b4d7d0",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1099,6 +1099,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- has a unique cache type, `ZambaHybridDynamicCache`\n@@ -1132,7 +1133,7 @@ def prepare_inputs_for_generation(\n                 position_ids = position_ids[:, -input_ids.shape[1] :]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and empty_past_kv:\n+        if inputs_embeds is not None and is_first_iteration:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases"
        },
        {
            "sha": "29bad05ae85b85a5c0d844c449bef1b40d11e5ee",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -1545,6 +1545,7 @@ def prepare_inputs_for_generation(\n         cache_position=None,\n         position_ids=None,\n         use_cache=True,\n+        is_first_iteration=False,\n         **kwargs,\n     ):\n         # Overwritten -- has a unique cache type, `Zamba2HybridDynamicCache`\n@@ -1578,7 +1579,7 @@ def prepare_inputs_for_generation(\n                 position_ids = position_ids[:, -input_ids.shape[1] :]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and empty_past_kv:\n+        if inputs_embeds is not None and is_first_iteration:\n             model_inputs = {\"inputs_embeds\": inputs_embeds}\n         else:\n             model_inputs = {\"input_ids\": input_ids.contiguous()}  # `contiguous()` needed for compilation use cases"
        },
        {
            "sha": "58803ca93a788fb4d7619a6c564a88529aebed34",
            "filename": "tests/generation/test_candidate_generator.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/tests%2Fgeneration%2Ftest_candidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/tests%2Fgeneration%2Ftest_candidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_candidate_generator.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -274,7 +274,7 @@ def test_basic_generation(self):\n         input_text = \"The quick brown fox\"\n         input_ids = self.target_tokenizer.encode(input_text, return_tensors=\"pt\")\n         self.generator.input_ids = input_ids\n-        candidates, scores = self.generator.get_candidates(input_ids)\n+        candidates, scores = self.generator.get_candidates(input_ids, is_first_iteration=True)\n \n         self.assertIsNotNone(candidates)\n         self.assertIsNotNone(scores)\n@@ -295,7 +295,7 @@ def test_mismatched_vocabularies(self):\n         )\n         input_ids = torch.tensor([[self.target_tokenizer.convert_tokens_to_ids(missing_token)]])\n         self.generator.input_ids = input_ids\n-        candidates, _ = self.generator.get_candidates(input_ids)\n+        candidates, _ = self.generator.get_candidates(input_ids, is_first_iteration=True)\n         self.assertIsNotNone(candidates)\n \n     def test_speculation_depth(self):\n@@ -305,14 +305,14 @@ def test_speculation_depth(self):\n \n         for depth in [1, 8, 17]:\n             self.generator.num_assistant_tokens = depth\n-            candidates, _ = self.generator.get_candidates(input_ids)\n+            candidates, _ = self.generator.get_candidates(input_ids, is_first_iteration=True)\n             self.assertLessEqual(candidates.shape[1] - input_ids.shape[1], depth)\n \n     def test_device_consistency(self):\n         \"\"\"Test handling of inputs on different devices\"\"\"\n         input_ids = torch.tensor([[1, 2, 3]]).to(torch_device)\n         self.generator.input_ids = input_ids\n-        candidates, _ = self.generator.get_candidates(input_ids)\n+        candidates, _ = self.generator.get_candidates(input_ids, is_first_iteration=True)\n         self.assertEqual(candidates.device, input_ids.device)\n \n     def test_usd_vs_vanilla_sampling(cls):"
        },
        {
            "sha": "747b2c4d173bdc9037bf7ce123c29823b26009a5",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 23,
            "deletions": 1,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -899,7 +899,7 @@ def test_prompt_lookup_decoding_stops_at_eos(self):\n         candidate_generator = PromptLookupCandidateGenerator(\n             eos_token_id=eos_token_id, num_output_tokens=4, max_matching_ngram_size=1\n         )\n-        output_prompt_lookup = candidate_generator.get_candidates(input_ids)[0]\n+        output_prompt_lookup = candidate_generator.get_candidates(input_ids, is_first_iteration=None)[0]\n \n         # PLD shouldn't propose any new tokens based on eos-match\n         self.assertTrue(output_prompt_lookup.shape[-1] == 10)\n@@ -1319,6 +1319,15 @@ def test_generate_continue_from_past_key_values(self):\n                         mode=\"constant\",\n                         value=1,\n                     )\n+            # Pop multimodal data since they are already cached and we'll raise an error\n+            # if there are multimodal data which don't belong anywhere inside `text_tokens`\n+            keys_to_pop = []\n+            for key in inputs:\n+                if (\"pixel\" in key or key in [\"image_patches\", \"input_feature\"]) and key != model.main_input_name:\n+                    keys_to_pop.append(key)\n+            for key in keys_to_pop:\n+                inputs.pop(key)\n+\n             first_caches_scores = outputs_cached.scores\n             outputs_cached = model.generate(**inputs, **generate_kwargs, max_new_tokens=1)\n             full_cached_scores = first_caches_scores + outputs_cached.scores\n@@ -2915,6 +2924,19 @@ def test_transition_scores_group_beam_search_encoder_decoder(self):\n \n         torch.testing.assert_close(transition_scores_sum, outputs.sequences_scores, rtol=1e-3, atol=1e-3)\n \n+    @slow\n+    def test_generate_inputs_embeds_one_token(self):\n+        \"Tests that we can generate legible text from a single token input embedding. See #41863 for details\"\n+        model = AutoModelForCausalLM.from_pretrained(\"gpt2\").to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n+        inputs_embeds = model.get_input_embeddings()(torch.tensor([[tokenizer.bos_token_id]], device=torch_device))\n+\n+        output = model.generate(\n+            inputs_embeds=inputs_embeds, do_sample=False, max_length=15, pad_token_id=tokenizer.eos_token_id\n+        )\n+        text = tokenizer.batch_decode(output, skip_special_tokens=True)[0]\n+        self.assertEqual(text, \"\\nThe first time I saw the new version of the game, I\")\n+\n     @slow\n     def test_green_red_watermark_generation(self):\n         model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)"
        },
        {
            "sha": "269747b32359616ce801b02997a1c4a1e5c851b9",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 30,
            "deletions": 2,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -15,6 +15,7 @@\n import inspect\n import unittest\n \n+import pytest\n from huggingface_hub import hf_hub_download\n \n from transformers import GitConfig, GitProcessor, GitVisionConfig, is_torch_available, is_vision_available\n@@ -414,6 +415,33 @@ def test_batched_generate_captioning(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester._test_batched_generate_captioning(*config_and_inputs)\n \n+    @pytest.mark.generate\n+    def test_past_key_values_format(self):\n+        \"\"\"\n+        Test that the KV cache is formatted correctly.\n+        Having a standard KV cache format is important for a consistent API (and for advanced generation methods).\n+        \"\"\"\n+        # GIT seq length shape depends on image inputs, overwrite\n+\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            # If it doesn't support cache, skip the test\n+            decoder_config = config.get_text_config(decoder=True)\n+\n+            model = model_class(config).to(torch_device)\n+            model = model.eval()\n+            if \"use_cache\" not in inputs:\n+                inputs[\"use_cache\"] = True\n+            outputs = model(**inputs)\n+\n+            cache = outputs[\"past_key_values\"]\n+            batch_size, seq_length = inputs[\"input_ids\"].shape[:2]\n+            image_length = int((config.vision_config.image_size / config.vision_config.patch_size) ** 2 + 1)\n+\n+            # Check the format\n+            self._check_past_key_values_for_generate(batch_size, cache, seq_length + image_length, decoder_config)\n+\n     def _check_attentions_for_generate(\n         self, batch_size, attentions, prompt_length, output_length, config, decoder_past_key_values\n     ):\n@@ -497,7 +525,7 @@ def test_inference_image_captioning(self):\n         self.assertEqual(outputs.sequences.shape, expected_shape)\n         self.assertEqual(generated_caption, \"two cats laying on a pink blanket\")\n         self.assertTrue(outputs.scores[-1].shape, expected_shape)\n-        expected_slice = torch.tensor([-0.8805, -0.8803, -0.8799], device=torch_device)\n+        expected_slice = torch.tensor([-0.8131, -0.8128, -0.8124], device=torch_device)\n         torch.testing.assert_close(outputs.scores[-1][0, :3], expected_slice, rtol=1e-4, atol=1e-4)\n \n     def test_visual_question_answering(self):\n@@ -540,7 +568,7 @@ def test_batched_generation(self):\n         generated_ids = model.generate(pixel_values=pixel_values, input_ids=input_ids, max_length=50)\n         generated_captions = processor.batch_decode(generated_ids, skip_special_tokens=True)\n \n-        self.assertEqual(generated_captions, [\"two cats sleeping on a pink blanket next to remotes.\"] * 2)\n+        self.assertEqual(generated_captions, [\"two cats sleeping on a couch\"] * 2)\n \n     @slow\n     def test_inference_interpolate_pos_encoding(self):"
        },
        {
            "sha": "a69bcc37b5e0f46c0c7a350f20c23c942dcb5f09",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 55,
            "deletions": 0,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -495,6 +495,61 @@ def test_generate_without_input_ids(self):\n     def test_generate_continue_from_inputs_embeds(self):\n         pass\n \n+    @pytest.mark.generate\n+    def test_generate_continue_from_past_key_values(self):\n+        \"\"\"Overwrite because IDEFICS needs image attention mask to be also processed\"\"\"\n+\n+        # Tests that we can continue generating from past key values, returned from a previous `generate` call\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            # Let's make it always:\n+            # 1. use cache (for obvious reasons)\n+            # 2. generate to max length (which can be achieved by setting the eos token to an invalid value), which\n+            #    would make the test flaky (e.g. EOS is generated on iteration 1 on both generations, but the\n+            #    continuation would force it to generate beyond an EOS token)\n+            # 3. ignore `token_type_ids` for simplicity\n+            # 4. ignore `forced_eos_token_id`, which requires further manipulation of the continuation inputs and is\n+            #    active by default on some models\n+            # 5. ignore `encoder_no_repeat_ngram_size`, which is set by default in some encoder-decoder models. When\n+            #    we use their decoder as a stand-alone model, `encoder_no_repeat_ngram_size` actually prevents\n+            #    repetition exclusively from the prompt. This test relies on comparing one call vs 2 calls\n+            #    with cache, what is considered a prompt is different in the two cases.\n+\n+            model = model_class(config).to(torch_device)\n+            model.eval()\n+            model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1\n+            model.generation_config.forced_eos_token_id = None\n+            model.generation_config.encoder_no_repeat_ngram_size = 0\n+            model.generation_config.use_cache = True\n+\n+            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values\n+            outputs = model.generate(**inputs, do_sample=False, max_new_tokens=4, return_dict_in_generate=True)\n+\n+            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens). Note that the\n+            # inputs may need to be tweaked across `generate` calls (like the attention mask).\n+            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=3, return_dict_in_generate=True)\n+\n+            # Continue from the tokens generated above, preparing the inputs accordingly\n+            inputs[\"past_key_values\"] = outputs_cached.past_key_values\n+            new_attention_len = outputs_cached.sequences.shape[-1]\n+            inputs[\"input_ids\"] = outputs_cached.sequences\n+            if \"attention_mask\" in inputs:\n+                inputs[\"attention_mask\"] = torch.nn.functional.pad(\n+                    inputs[\"attention_mask\"],\n+                    (0, new_attention_len - inputs[\"attention_mask\"].shape[1]),\n+                    mode=\"constant\",\n+                    value=1,\n+                )\n+            if \"image_attention_mask\" in inputs:\n+                inputs[\"image_attention_mask\"] = inputs[\"image_attention_mask\"][:, -1:, :]\n+\n+            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=1, return_dict_in_generate=True)\n+\n+            # The two sets of generated text and past kv should be equal to each other\n+            self.assertListEqual(outputs.sequences.tolist(), outputs_cached.sequences.tolist())\n+            self._check_caches_are_equal(outputs.past_key_values, outputs_cached.past_key_values)\n+\n     def test_attention_outputs(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True"
        },
        {
            "sha": "8e681b91718eb1f4cabfd9a291b25e8cf72ddaa6",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 25,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ccc7d90bcb4a94cd0888925c6abac941f3b8151d/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=ccc7d90bcb4a94cd0888925c6abac941f3b8151d",
            "patch": "@@ -510,31 +510,6 @@ def test_resize_embeddings_untied(self):\n             # Check that the model can still do a forward pass successfully (every parameter should be resized)\n             model(**self._prepare_for_class(inputs_dict, model_class))\n \n-    def test_inputs_embeds_matches_input_ids_with_generate(self):\n-        # overwrite because IDEFICS needs ids and embeds at the input to be not None\n-        config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        for model_class in self.all_model_classes:\n-            model = model_class(config)\n-            model.to(torch_device)\n-            model.eval()\n-\n-            inputs = copy.deepcopy(self._prepare_for_class(inputs_dict, model_class))\n-            pad_token_id = config.pad_token_id if config.pad_token_id is not None else 1\n-\n-            wte = model.get_input_embeddings()\n-\n-            input_ids = inputs[\"input_ids\"]\n-            # some models infer position ids/attn mask differently when input ids\n-            # by check if pad_token let's make sure no padding is in input ids\n-            not_pad_token_id = pad_token_id + 1 if max(0, pad_token_id - 1) == 0 else pad_token_id - 1\n-            input_ids[input_ids == pad_token_id] = not_pad_token_id\n-            del inputs[\"input_ids\"]\n-            inputs_embeds = wte(input_ids)\n-            out_ids = model.generate(input_ids=input_ids, **inputs, max_new_tokens=2)\n-            out_embeds = model.generate(input_ids=input_ids, inputs_embeds=inputs_embeds, **inputs, max_new_tokens=2)\n-\n-            torch.testing.assert_close(out_embeds, out_ids)\n-\n \n @require_torch\n class Idefics2ForConditionalGenerationIntegrationTest(unittest.TestCase):"
        }
    ],
    "stats": {
        "total": 1300,
        "additions": 835,
        "deletions": 465
    }
}