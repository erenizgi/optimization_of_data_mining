{
    "author": "zhanluxianshen",
    "message": "Fix some typos about benchmark scripts. (#37027)\n\nSigned-off-by: zhanluxianshen <zhanluxianshen@163.com>",
    "sha": "c90e6e9625801f4271efbec1826b5c357b9f3eee",
    "files": [
        {
            "sha": "3935f02b389d841367d16f11bbf7c6775fb09a14",
            "filename": "benchmark/README.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c90e6e9625801f4271efbec1826b5c357b9f3eee/benchmark%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c90e6e9625801f4271efbec1826b5c357b9f3eee/benchmark%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2FREADME.md?ref=c90e6e9625801f4271efbec1826b5c357b9f3eee",
            "patch": "@@ -12,7 +12,7 @@ def run_benchmark(logger: Logger, branch: str, commit_id: str, commit_msg: str,\n \n ## Writing metrics to the database\n \n-`MetricRecorder` is thread-safe, in the sense of the python [`Thread`](https://docs.python.org/3/library/threading.html#threading.Thread). This means you can start a background thread to do the readings on the device measurements while not blocking the main thread to execute the model measurements.\n+`MetricsRecorder` is thread-safe, in the sense of the python [`Thread`](https://docs.python.org/3/library/threading.html#threading.Thread). This means you can start a background thread to do the readings on the device measurements while not blocking the main thread to execute the model measurements.\n \n cf [`llama.py`](./llama.py) to see an example of this in practice.\n "
        },
        {
            "sha": "6c036fdd6939ae25a299ed2eac777ec58a838ecf",
            "filename": "benchmark/benchmarks_entrypoint.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c90e6e9625801f4271efbec1826b5c357b9f3eee/benchmark%2Fbenchmarks_entrypoint.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c90e6e9625801f4271efbec1826b5c357b9f3eee/benchmark%2Fbenchmarks_entrypoint.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fbenchmarks_entrypoint.py?ref=c90e6e9625801f4271efbec1826b5c357b9f3eee",
            "patch": "@@ -3,7 +3,6 @@\n import logging\n import os\n from typing import Dict\n-import psycopg2\n import sys\n \n from psycopg2.extras import Json"
        },
        {
            "sha": "c7ad049e6f67c52197caab7127e95bc6d49d4054",
            "filename": "benchmark/llama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c90e6e9625801f4271efbec1826b5c357b9f3eee/benchmark%2Fllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c90e6e9625801f4271efbec1826b5c357b9f3eee/benchmark%2Fllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark%2Fllama.py?ref=c90e6e9625801f4271efbec1826b5c357b9f3eee",
            "patch": "@@ -215,7 +215,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n             torch.cuda.synchronize()\n             end = perf_counter()\n             time_to_second_token = end - start\n-            logger.info(f\"completed second compile generation in: {time_to_first_token}s\")\n+            logger.info(f\"completed second compile generation in: {time_to_second_token}s\")\n             cache_position += 1\n             all_generated_tokens += next_token.clone().detach().cpu().tolist()\n \n@@ -227,7 +227,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n             torch.cuda.synchronize()\n             end = perf_counter()\n             time_to_third_token = end - start\n-            logger.info(f\"completed third compile forward in: {time_to_first_token}s\")\n+            logger.info(f\"completed third compile forward in: {time_to_third_token}s\")\n             cache_position += 1\n             all_generated_tokens += next_token.clone().detach().cpu().tolist()\n \n@@ -298,7 +298,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n             output = model.generate(**inputs, past_key_values=past_key_values)\n             end = perf_counter()\n             third_compile_generate_time = end - start\n-            logger.info(f\"completed second compile generation in: {third_compile_generate_time}s\")\n+            logger.info(f\"completed third compile generation in: {third_compile_generate_time}s\")\n             logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n \n             past_key_values = StaticCache(\n@@ -313,7 +313,7 @@ def decode_one_token(model, cur_token, cache_position, past_key_values):\n             output = model.generate(**inputs, past_key_values=past_key_values)\n             end = perf_counter()\n             fourth_compile_generate_time = end - start\n-            logger.info(f\"completed second compile generation in: {fourth_compile_generate_time}s\")\n+            logger.info(f\"completed fourth compile generation in: {fourth_compile_generate_time}s\")\n             logger.info(f\"generated: {tokenizer.batch_decode(output.cpu().tolist())}\")\n \n         metrics_recorder.collect_model_measurements("
        }
    ],
    "stats": {
        "total": 11,
        "additions": 5,
        "deletions": 6
    }
}