{
    "author": "ydshieh",
    "message": "Fix flaky test execution caused by `Thread` (#34966)\n\nfix\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "5f8b24ee12a4254c00d9dff7bdc451f94fdc2aba",
    "files": [
        {
            "sha": "fe6b600a86c1cc0f553f1bb68cc8b723fc91b63c",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5f8b24ee12a4254c00d9dff7bdc451f94fdc2aba/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5f8b24ee12a4254c00d9dff7bdc451f94fdc2aba/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=5f8b24ee12a4254c00d9dff7bdc451f94fdc2aba",
            "patch": "@@ -29,7 +29,7 @@\n from contextlib import contextmanager\n from dataclasses import dataclass\n from functools import partial, wraps\n-from threading import Thread\n+from multiprocessing import Process\n from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Union\n from zipfile import is_zipfile\n \n@@ -3839,11 +3839,11 @@ def from_pretrained(\n                                     **has_file_kwargs,\n                                 }\n                                 if not has_file(pretrained_model_name_or_path, safe_weights_name, **has_file_kwargs):\n-                                    Thread(\n+                                    Process(\n                                         target=auto_conversion,\n                                         args=(pretrained_model_name_or_path,),\n                                         kwargs={\"ignore_errors_during_conversion\": True, **cached_file_kwargs},\n-                                        name=\"Thread-autoconversion\",\n+                                        name=\"Process-auto_conversion\",\n                                     ).start()\n                         else:\n                             # Otherwise, no PyTorch file was found, maybe there is a TF or Flax model file."
        }
    ],
    "stats": {
        "total": 6,
        "additions": 3,
        "deletions": 3
    }
}