{
    "author": "Sai-Suraj-27",
    "message": "Fixed failing `BioGPT` batch generation test (#42677)\n\nFixed failing BioGPT batch generation test",
    "sha": "e8e142de378e15626ace04b02256a9d8ee158550",
    "files": [
        {
            "sha": "aec7093030a8407087b114e54f7961cad84058e7",
            "filename": "tests/models/biogpt/test_modeling_biogpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/e8e142de378e15626ace04b02256a9d8ee158550/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e8e142de378e15626ace04b02256a9d8ee158550/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbiogpt%2Ftest_modeling_biogpt.py?ref=e8e142de378e15626ace04b02256a9d8ee158550",
            "patch": "@@ -335,7 +335,9 @@ def test_batch_generation(self):\n \n         num_paddings = inputs_non_padded.shape[-1] - inputs[\"attention_mask\"][-1].long().sum().item()\n         inputs_padded = tokenizer(sentences[1], return_tensors=\"pt\").input_ids.to(torch_device)\n-        output_padded = model.generate(input_ids=inputs_padded, max_length=model.config.max_length - num_paddings)\n+        output_padded = model.generate(\n+            input_ids=inputs_padded, max_length=model.generation_config.max_length - num_paddings\n+        )\n \n         batch_out_sentence = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n         non_padded_sentence = tokenizer.decode(output_non_padded[0], skip_special_tokens=True)"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 3,
        "deletions": 1
    }
}