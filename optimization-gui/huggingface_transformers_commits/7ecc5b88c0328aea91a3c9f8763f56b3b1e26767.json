{
    "author": "eljandoubi",
    "message": "Add image classifier donut & update loss calculation for all swins  (#37224)\n\n* add classifier head to donut\n\n* add to transformers __init__\n\n* add to auto model\n\n* fix typo\n\n* add loss for image classification\n\n* add checkpoint\n\n* remove no needed import\n\n* reoder import\n\n* format\n\n* consistency\n\n* add test of classifier\n\n* add doc\n\n* try ignore\n\n* update loss for all swin models",
    "sha": "7ecc5b88c0328aea91a3c9f8763f56b3b1e26767",
    "files": [
        {
            "sha": "6e3bd3c51eab3402c7b61e6bec1c33685a85a5a0",
            "filename": "docs/source/en/model_doc/donut.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdonut.md?ref=7ecc5b88c0328aea91a3c9f8763f56b3b1e26767",
            "patch": "@@ -226,3 +226,8 @@ print(answer)\n \n [[autodoc]] DonutSwinModel\n     - forward\n+\n+## DonutSwinForImageClassification\n+\n+[[autodoc]] transformers.DonutSwinForImageClassification\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "71bb1e2bd84a90cadc09a386304f0676a14553fd",
            "filename": "src/transformers/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2F__init__.py?ref=7ecc5b88c0328aea91a3c9f8763f56b3b1e26767",
            "patch": "@@ -2304,6 +2304,7 @@\n     )\n     _import_structure[\"models.donut\"].extend(\n         [\n+            \"DonutSwinForImageClassification\",\n             \"DonutSwinModel\",\n             \"DonutSwinPreTrainedModel\",\n         ]\n@@ -7457,6 +7458,7 @@\n             DistilBertPreTrainedModel,\n         )\n         from .models.donut import (\n+            DonutSwinForImageClassification,\n             DonutSwinModel,\n             DonutSwinPreTrainedModel,\n         )"
        },
        {
            "sha": "98ecbe76520c4acb4e59f20a3776674ecea5ad01",
            "filename": "src/transformers/loss/loss_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Floss%2Floss_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Floss%2Floss_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Floss%2Floss_utils.py?ref=7ecc5b88c0328aea91a3c9f8763f56b3b1e26767",
            "patch": "@@ -145,6 +145,7 @@ def ForTokenClassification(logits: torch.Tensor, labels, config, **kwargs):\n     \"ForMaskedLM\": ForMaskedLMLoss,\n     \"ForQuestionAnswering\": ForQuestionAnsweringLoss,\n     \"ForSequenceClassification\": ForSequenceClassificationLoss,\n+    \"ForImageClassification\": ForSequenceClassificationLoss,\n     \"ForTokenClassification\": ForTokenClassification,\n     \"ForSegmentation\": ForSegmentationLoss,\n     \"ForObjectDetection\": ForObjectDetectionLoss,"
        },
        {
            "sha": "84df5dc363a93bb3f96969ab8e9b6ce9f33412f8",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=7ecc5b88c0328aea91a3c9f8763f56b3b1e26767",
            "patch": "@@ -707,6 +707,7 @@\n         (\"dinat\", \"DinatForImageClassification\"),\n         (\"dinov2\", \"Dinov2ForImageClassification\"),\n         (\"dinov2_with_registers\", \"Dinov2WithRegistersForImageClassification\"),\n+        (\"donut-swin\", \"DonutSwinForImageClassification\"),\n         (\n             \"efficientformer\",\n             ("
        },
        {
            "sha": "b06be3bcf61ae7f9db4b3adc9f8892704d6c3835",
            "filename": "src/transformers/models/donut/modeling_donut_swin.py",
            "status": "modified",
            "additions": 130,
            "deletions": 3,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fmodeling_donut_swin.py?ref=7ecc5b88c0328aea91a3c9f8763f56b3b1e26767",
            "patch": "@@ -49,6 +49,10 @@\n _CHECKPOINT_FOR_DOC = \"https://huggingface.co/naver-clova-ix/donut-base\"\n _EXPECTED_OUTPUT_SHAPE = [1, 49, 768]\n \n+# Image classification docstring\n+_IMAGE_CLASS_CHECKPOINT = \"eljandoubi/donut-base-encoder\"\n+_IMAGE_CLASS_EXPECTED_OUTPUT = \"tabby, tabby cat\"\n+\n \n @dataclass\n # Copied from transformers.models.swin.modeling_swin.SwinEncoderOutput with Swin->DonutSwin\n@@ -121,6 +125,43 @@ class DonutSwinModelOutput(ModelOutput):\n     reshaped_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n \n \n+@dataclass\n+# Copied from transformers.models.swin.modeling_swin.SwinImageClassifierOutput with Swin->DonutSwin\n+class DonutSwinImageClassifierOutput(ModelOutput):\n+    \"\"\"\n+    DonutSwin outputs for image classification.\n+\n+    Args:\n+        loss (`torch.FloatTensor` of shape `(1,)`, *optional*, returned when `labels` is provided):\n+            Classification (or regression if config.num_labels==1) loss.\n+        logits (`torch.FloatTensor` of shape `(batch_size, config.num_labels)`):\n+            Classification (or regression if config.num_labels==1) scores (before SoftMax).\n+        hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+            shape `(batch_size, sequence_length, hidden_size)`.\n+\n+            Hidden-states of the model at the output of each layer plus the initial embedding outputs.\n+        attentions (`tuple(torch.FloatTensor)`, *optional*, returned when `output_attentions=True` is passed or when `config.output_attentions=True`):\n+            Tuple of `torch.FloatTensor` (one for each stage) of shape `(batch_size, num_heads, sequence_length,\n+            sequence_length)`.\n+\n+            Attentions weights after the attention softmax, used to compute the weighted average in the self-attention\n+            heads.\n+        reshaped_hidden_states (`tuple(torch.FloatTensor)`, *optional*, returned when `output_hidden_states=True` is passed or when `config.output_hidden_states=True`):\n+            Tuple of `torch.FloatTensor` (one for the output of the embeddings + one for the output of each stage) of\n+            shape `(batch_size, hidden_size, height, width)`.\n+\n+            Hidden-states of the model at the output of each layer plus the initial embedding outputs reshaped to\n+            include the spatial dimensions.\n+    \"\"\"\n+\n+    loss: Optional[torch.FloatTensor] = None\n+    logits: torch.FloatTensor = None\n+    hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    attentions: Optional[Tuple[torch.FloatTensor, ...]] = None\n+    reshaped_hidden_states: Optional[Tuple[torch.FloatTensor, ...]] = None\n+\n+\n # Copied from transformers.models.swin.modeling_swin.window_partition\n def window_partition(input_feature, window_size):\n     \"\"\"\n@@ -845,15 +886,15 @@ def forward(\n         )\n \n \n-# Copied from transformers.models.swin.modeling_swin.SwinPreTrainedModel with Swin->DonutSwin\n+# Copied from transformers.models.swin.modeling_swin.SwinPreTrainedModel with Swin->DonutSwin,swin->donut\n class DonutSwinPreTrainedModel(PreTrainedModel):\n     \"\"\"\n     An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained\n     models.\n     \"\"\"\n \n     config_class = DonutSwinConfig\n-    base_model_prefix = \"swin\"\n+    base_model_prefix = \"donut\"\n     main_input_name = \"pixel_values\"\n     supports_gradient_checkpointing = True\n     _no_split_modules = [\"DonutSwinStage\"]\n@@ -1015,4 +1056,90 @@ def forward(\n         )\n \n \n-__all__ = [\"DonutSwinModel\", \"DonutSwinPreTrainedModel\"]\n+@add_start_docstrings(\n+    \"\"\"\n+    DonutSwin Model transformer with an image classification head on top (a linear layer on top of the final hidden state of\n+    the [CLS] token) e.g. for ImageNet.\n+\n+    <Tip>\n+\n+        Note that it's possible to fine-tune DonutSwin on higher resolution images than the ones it has been trained on, by\n+        setting `interpolate_pos_encoding` to `True` in the forward of the model. This will interpolate the pre-trained\n+        position embeddings to the higher resolution.\n+\n+    </Tip>\n+    \"\"\",\n+    SWIN_START_DOCSTRING,\n+)\n+# Copied from transformers.models.swin.modeling_swin.SwinForImageClassification with Swin->DonutSwin,swin->donut\n+class DonutSwinForImageClassification(DonutSwinPreTrainedModel):\n+    def __init__(self, config):\n+        super().__init__(config)\n+\n+        self.num_labels = config.num_labels\n+        self.donut = DonutSwinModel(config)\n+\n+        # Classifier head\n+        self.classifier = (\n+            nn.Linear(self.donut.num_features, config.num_labels) if config.num_labels > 0 else nn.Identity()\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @add_start_docstrings_to_model_forward(SWIN_INPUTS_DOCSTRING)\n+    @add_code_sample_docstrings(\n+        checkpoint=_IMAGE_CLASS_CHECKPOINT,\n+        output_type=DonutSwinImageClassifierOutput,\n+        config_class=_CONFIG_FOR_DOC,\n+        expected_output=_IMAGE_CLASS_EXPECTED_OUTPUT,\n+    )\n+    def forward(\n+        self,\n+        pixel_values: Optional[torch.FloatTensor] = None,\n+        head_mask: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        output_attentions: Optional[bool] = None,\n+        output_hidden_states: Optional[bool] = None,\n+        interpolate_pos_encoding: bool = False,\n+        return_dict: Optional[bool] = None,\n+    ) -> Union[Tuple, DonutSwinImageClassifierOutput]:\n+        r\"\"\"\n+        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            Labels for computing the image classification/regression loss. Indices should be in `[0, ...,\n+            config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\n+            `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n+        \"\"\"\n+        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n+\n+        outputs = self.donut(\n+            pixel_values,\n+            head_mask=head_mask,\n+            output_attentions=output_attentions,\n+            output_hidden_states=output_hidden_states,\n+            interpolate_pos_encoding=interpolate_pos_encoding,\n+            return_dict=return_dict,\n+        )\n+\n+        pooled_output = outputs[1]\n+\n+        logits = self.classifier(pooled_output)\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=logits, config=self.config)\n+\n+        if not return_dict:\n+            output = (logits,) + outputs[2:]\n+            return ((loss,) + output) if loss is not None else output\n+\n+        return DonutSwinImageClassifierOutput(\n+            loss=loss,\n+            logits=logits,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+            reshaped_hidden_states=outputs.reshaped_hidden_states,\n+        )\n+\n+\n+__all__ = [\"DonutSwinModel\", \"DonutSwinPreTrainedModel\", \"DonutSwinForImageClassification\"]"
        },
        {
            "sha": "e155874d8f0a5be783e22735d0e76c075809d940",
            "filename": "src/transformers/models/swin/modeling_swin.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin%2Fmodeling_swin.py?ref=7ecc5b88c0328aea91a3c9f8763f56b3b1e26767",
            "patch": "@@ -23,7 +23,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BackboneOutput\n@@ -1285,26 +1284,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=logits, config=self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "1e07af70159ecec2b8420ce1f584a3111d2eab9d",
            "filename": "src/transformers/models/swinv2/modeling_swinv2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 21,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswinv2%2Fmodeling_swinv2.py?ref=7ecc5b88c0328aea91a3c9f8763f56b3b1e26767",
            "patch": "@@ -23,7 +23,6 @@\n import torch\n import torch.utils.checkpoint\n from torch import Tensor, nn\n-from torch.nn import BCEWithLogitsLoss, CrossEntropyLoss, MSELoss\n \n from ...activations import ACT2FN\n from ...modeling_outputs import BackboneOutput\n@@ -1339,26 +1338,7 @@ def forward(\n \n         loss = None\n         if labels is not None:\n-            if self.config.problem_type is None:\n-                if self.num_labels == 1:\n-                    self.config.problem_type = \"regression\"\n-                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n-                    self.config.problem_type = \"single_label_classification\"\n-                else:\n-                    self.config.problem_type = \"multi_label_classification\"\n-\n-            if self.config.problem_type == \"regression\":\n-                loss_fct = MSELoss()\n-                if self.num_labels == 1:\n-                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n-                else:\n-                    loss = loss_fct(logits, labels)\n-            elif self.config.problem_type == \"single_label_classification\":\n-                loss_fct = CrossEntropyLoss()\n-                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n-            elif self.config.problem_type == \"multi_label_classification\":\n-                loss_fct = BCEWithLogitsLoss()\n-                loss = loss_fct(logits, labels)\n+            loss = self.loss_function(logits=logits, labels=labels, pooled_logits=logits, config=self.config)\n \n         if not return_dict:\n             output = (logits,) + outputs[2:]"
        },
        {
            "sha": "b0959e1c8db5536c8ff6b35a5f46f9539d3d5f03",
            "filename": "src/transformers/utils/dummy_pt_objects.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fdummy_pt_objects.py?ref=7ecc5b88c0328aea91a3c9f8763f56b3b1e26767",
            "patch": "@@ -3829,6 +3829,13 @@ def __init__(self, *args, **kwargs):\n         requires_backends(self, [\"torch\"])\n \n \n+class DonutSwinForImageClassification(metaclass=DummyObject):\n+    _backends = [\"torch\"]\n+\n+    def __init__(self, *args, **kwargs):\n+        requires_backends(self, [\"torch\"])\n+\n+\n class DonutSwinModel(metaclass=DummyObject):\n     _backends = [\"torch\"]\n "
        },
        {
            "sha": "078331389b8c9ffa003308fcf22a8946ad985e83",
            "filename": "tests/models/donut/test_modeling_donut_swin.py",
            "status": "modified",
            "additions": 29,
            "deletions": 3,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7ecc5b88c0328aea91a3c9f8763f56b3b1e26767/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdonut%2Ftest_modeling_donut_swin.py?ref=7ecc5b88c0328aea91a3c9f8763f56b3b1e26767",
            "patch": "@@ -29,7 +29,7 @@\n     import torch\n     from torch import nn\n \n-    from transformers import DonutSwinModel\n+    from transformers import DonutSwinForImageClassification, DonutSwinModel\n \n \n class DonutSwinModelTester:\n@@ -129,6 +129,24 @@ def create_and_check_model(self, config, pixel_values, labels):\n \n         self.parent.assertEqual(result.last_hidden_state.shape, (self.batch_size, expected_seq_len, expected_dim))\n \n+    def create_and_check_for_image_classification(self, config, pixel_values, labels):\n+        config.num_labels = self.type_sequence_label_size\n+        model = DonutSwinForImageClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values, labels=labels)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n+\n+        # test greyscale images\n+        config.num_channels = 1\n+        model = DonutSwinForImageClassification(config)\n+        model.to(torch_device)\n+        model.eval()\n+\n+        pixel_values = floats_tensor([self.batch_size, 1, self.image_size, self.image_size])\n+        result = model(pixel_values)\n+        self.parent.assertEqual(result.logits.shape, (self.batch_size, self.type_sequence_label_size))\n+\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         (\n@@ -142,8 +160,12 @@ def prepare_config_and_inputs_for_common(self):\n \n @require_torch\n class DonutSwinModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n-    all_model_classes = (DonutSwinModel,) if is_torch_available() else ()\n-    pipeline_model_mapping = {\"image-feature-extraction\": DonutSwinModel} if is_torch_available() else {}\n+    all_model_classes = (DonutSwinModel, DonutSwinForImageClassification) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\"image-feature-extraction\": DonutSwinModel, \"image-classification\": DonutSwinForImageClassification}\n+        if is_torch_available()\n+        else {}\n+    )\n     fx_compatible = True\n \n     test_pruning = False\n@@ -167,6 +189,10 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def test_for_image_classification(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_for_image_classification(*config_and_inputs)\n+\n     @unittest.skip(reason=\"DonutSwin does not use inputs_embeds\")\n     def test_inputs_embeds(self):\n         pass"
        }
    ],
    "stats": {
        "total": 225,
        "additions": 177,
        "deletions": 48
    }
}