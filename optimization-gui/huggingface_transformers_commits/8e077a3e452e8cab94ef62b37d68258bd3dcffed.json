{
    "author": "zucchini-nlp",
    "message": "Fix re-compilations for cross attention cache (#39788)\n\nfix recompilations for cross attn cache",
    "sha": "8e077a3e452e8cab94ef62b37d68258bd3dcffed",
    "files": [
        {
            "sha": "3a81f3e284be1636b38d173014bb396dad5945c7",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -485,8 +485,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "90ed959176add30659f12a51b406c9ba9c092170",
            "filename": "src/transformers/models/bert/modeling_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert%2Fmodeling_bert.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -248,8 +248,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n@@ -378,8 +378,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)"
        },
        {
            "sha": "10ba8baca0d9a2dc1e25bf312c3c7d3d44e39d69",
            "filename": "src/transformers/models/bert_generation/modeling_bert_generation.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbert_generation%2Fmodeling_bert_generation.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -110,8 +110,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose("
        },
        {
            "sha": "8112f102002631eec9839c8e55ad41e11e79b307",
            "filename": "src/transformers/models/big_bird/modeling_big_bird.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbig_bird%2Fmodeling_big_bird.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -338,8 +338,8 @@ def forward(\n         attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n         if is_cross_attention and past_key_value is not None and past_key_value.get_seq_length(self.layer_idx) > 0:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.key_cache[self.layer_idx]\n-            value_layer = past_key_value.value_cache[self.layer_idx]\n+            key_layer = past_key_value.layers[self.layer_idx].keys\n+            value_layer = past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)"
        },
        {
            "sha": "40013e4f32462df7188059171cf69b90fc4c2f01",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -150,8 +150,8 @@ def forward(\n         attention_mask = encoder_attention_mask if is_cross_attention else attention_mask\n         if is_cross_attention and past_key_value is not None and past_key_value.get_seq_length(self.layer_idx) > 0:\n             # reuse k,v, cross_attentions\n-            key_layer = past_key_value.key_cache[self.layer_idx]\n-            value_layer = past_key_value.value_cache[self.layer_idx]\n+            key_layer = past_key_value.layers[self.layer_idx].keys\n+            value_layer = past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)"
        },
        {
            "sha": "bdf26fe39f54be5e4f0e5bfda0b6be3cf3b4599d",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -176,8 +176,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)"
        },
        {
            "sha": "650543adab566feac11da60b11b52b86ae0435cf",
            "filename": "src/transformers/models/bridgetower/modeling_bridgetower.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fmodeling_bridgetower.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -460,8 +460,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose("
        },
        {
            "sha": "bb983794ab74c98e778b92648edf46b1609330ba",
            "filename": "src/transformers/models/camembert/modeling_camembert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcamembert%2Fmodeling_camembert.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -198,8 +198,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n@@ -329,8 +329,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)"
        },
        {
            "sha": "9f472b02505a7f32f3e570a408719c7f6a1c60a6",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_text.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -198,8 +198,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose("
        },
        {
            "sha": "0bf12f0511adde98493dd8f112c0e71f96d6fa95",
            "filename": "src/transformers/models/electra/modeling_electra.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Felectra%2Fmodeling_electra.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -255,8 +255,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose("
        },
        {
            "sha": "98cdda20ef22bbd5c008e6bfe3503a4e735ba066",
            "filename": "src/transformers/models/ernie/modeling_ernie.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie%2Fmodeling_ernie.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -184,8 +184,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose("
        },
        {
            "sha": "8a7b3e898840bad6e19cbf5d773e563cfe115892",
            "filename": "src/transformers/models/fsmt/modeling_fsmt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fmodeling_fsmt.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -800,8 +800,8 @@ def forward(\n         current_states = key if self.encoder_decoder_attention else query\n         if self.encoder_decoder_attention and layer_state is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "b45a2810cc03ec05d87dddfdb086b39f384e1df0",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -205,8 +205,8 @@ def forward(\n                 )\n             if layer_past is not None and is_updated:\n                 # reuse k,v, cross_attentions\n-                key = curr_past_key_value.key_cache[self.layer_idx]\n-                value = curr_past_key_value.value_cache[self.layer_idx]\n+                key = curr_past_key_value.layers[self.layer_idx].keys\n+                value = curr_past_key_value.layers[self.layer_idx].values\n             else:\n                 query = self.q_attn(hidden_states).view(*input_shape, -1, self.head_dim).transpose(1, 2)\n                 key, value = self.c_attn(encoder_hidden_states).split((self.head_dim, self.head_dim), dim=-1)"
        },
        {
            "sha": "2e13db7f29d9679671b3b558b8f20428b92a59ec",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -368,8 +368,8 @@ def forward(\n             if layer_past is not None and is_updated:\n                 # reuse k,v, cross_attentions, and compute only q\n                 query = self.q_attn(hidden_states)\n-                key = curr_past_key_value.key_cache[self.layer_idx]\n-                value = curr_past_key_value.value_cache[self.layer_idx]\n+                key = curr_past_key_value.layers[self.layer_idx].keys\n+                value = curr_past_key_value.layers[self.layer_idx].values\n             else:\n                 query = self.q_attn(hidden_states)\n                 key, value = self.c_attn(current_states).split(self.split_size, dim=2)"
        },
        {
            "sha": "57419c150c18cb4ab965c73125e54506b1d5ff63",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -744,8 +744,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "639fa3df4da06b9050f471a4d8a8e27afe4ad65f",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -824,8 +824,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "e358499c60b411a1a380df8794509afe272f840e",
            "filename": "src/transformers/models/megatron_bert/modeling_megatron_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmegatron_bert%2Fmodeling_megatron_bert.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -237,8 +237,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose("
        },
        {
            "sha": "97c16b7e2516629e4fcd4a983bde95167a4ae309",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -262,8 +262,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)"
        },
        {
            "sha": "92cad3caccbef14f873fdec9df2d43f2176145a7",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -270,8 +270,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)"
        },
        {
            "sha": "1102b3c2f736cc7cd93da923821c5cf3e64378bb",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -158,8 +158,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "5e6dc2d52d8ac66836dbc689f35afaa48a0cfa5e",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -584,8 +584,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)"
        },
        {
            "sha": "57a41b33c56a0e33412ad3e490c9bf335d1637d9",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -475,8 +475,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.key_proj(current_states)\n             value_states = self.value_proj(current_states)"
        },
        {
            "sha": "dc9c5f86eaea7d22e87dcf1dda80abe82164b056",
            "filename": "src/transformers/models/rembert/modeling_rembert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frembert%2Fmodeling_rembert.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -253,8 +253,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)"
        },
        {
            "sha": "26a10273a5fe1ca061fffea0ec3f519c4690ee27",
            "filename": "src/transformers/models/roberta/modeling_roberta.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta%2Fmodeling_roberta.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -197,8 +197,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n@@ -328,8 +328,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)"
        },
        {
            "sha": "c36f029cf35e3d2cd97153010064aa5615106c4d",
            "filename": "src/transformers/models/roberta_prelayernorm/modeling_roberta_prelayernorm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froberta_prelayernorm%2Fmodeling_roberta_prelayernorm.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -196,8 +196,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose("
        },
        {
            "sha": "a4e736c9f4f4d1e0dd46bec5740c95e9bc20e762",
            "filename": "src/transformers/models/roc_bert/modeling_roc_bert.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froc_bert%2Fmodeling_roc_bert.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -311,8 +311,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose("
        },
        {
            "sha": "5a816efb6169ad76b1270ce19739973e7b5d87c6",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -247,8 +247,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)"
        },
        {
            "sha": "6fb9c2ab3ba6efbb4b578946db474f68e92e7150",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -1062,8 +1062,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "c7ab1db016b6f38d710763a36dd9127fddf0cb71",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -928,8 +928,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "81b36c8bf94f2aabaa86229c3299bdaccdf35efa",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -286,8 +286,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states).view(*kv_input_shape).transpose(1, 2)\n             value_states = self.v_proj(current_states).view(*kv_input_shape).transpose(1, 2)"
        },
        {
            "sha": "afb5dba86cee187e3d7442afdf730184fd8eabba",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -912,8 +912,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "b8a681a0aacd9679c27486cb2ec49eb818a83b3b",
            "filename": "src/transformers/models/tapas/modeling_tapas.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftapas%2Fmodeling_tapas.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -332,8 +332,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)"
        },
        {
            "sha": "0ea823b4cafe4f9c24cd6841e4d6bfa90c80c539",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -212,8 +212,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "7f54c3fac89597634902bbefac006575a36cfaff",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -169,8 +169,8 @@ def forward(\n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_states = curr_past_key_value.key_cache[self.layer_idx]\n-            value_states = curr_past_key_value.value_cache[self.layer_idx]\n+            key_states = curr_past_key_value.layers[self.layer_idx].keys\n+            value_states = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_states = self.k_proj(current_states)\n             value_states = self.v_proj(current_states)"
        },
        {
            "sha": "d1c70b60289954dd9a3778539d4f2e7bf5461e61",
            "filename": "src/transformers/models/xlm_roberta/modeling_xlm_roberta.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta%2Fmodeling_xlm_roberta.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -198,8 +198,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n@@ -329,8 +329,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)"
        },
        {
            "sha": "62793c38bca8c2084e52273a228f23f744362af6",
            "filename": "src/transformers/models/xlm_roberta_xl/modeling_xlm_roberta_xl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxlm_roberta_xl%2Fmodeling_xlm_roberta_xl.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -195,8 +195,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose(\n@@ -326,8 +326,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = (\n                 self.key(current_states)"
        },
        {
            "sha": "6b4ac64f4e96653d889efdb9049d25368f2074a5",
            "filename": "src/transformers/models/xmod/modeling_xmod.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e077a3e452e8cab94ef62b37d68258bd3dcffed/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxmod%2Fmodeling_xmod.py?ref=8e077a3e452e8cab94ef62b37d68258bd3dcffed",
            "patch": "@@ -195,8 +195,8 @@ def forward(\n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n             # reuse k,v, cross_attentions\n-            key_layer = curr_past_key_value.key_cache[self.layer_idx]\n-            value_layer = curr_past_key_value.value_cache[self.layer_idx]\n+            key_layer = curr_past_key_value.layers[self.layer_idx].keys\n+            value_layer = curr_past_key_value.layers[self.layer_idx].values\n         else:\n             key_layer = self.key(current_states)\n             key_layer = key_layer.view(batch_size, -1, self.num_attention_heads, self.attention_head_size).transpose("
        }
    ],
    "stats": {
        "total": 168,
        "additions": 84,
        "deletions": 84
    }
}