{
    "author": "LysandreJik",
    "message": "Helium documentation fixes (#36170)\n\n* Helium documentation fixes\r\n\r\n* Update helium.md\r\n\r\n* Update helium.md\r\n\r\n* Update helium.md",
    "sha": "c82319b493889aaa60912319369e33dd049420fc",
    "files": [
        {
            "sha": "b830c0a72be706c2f2d67236246b351d21d9aa9c",
            "filename": "docs/source/en/model_doc/helium.md",
            "status": "modified",
            "additions": 4,
            "deletions": 8,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/c82319b493889aaa60912319369e33dd049420fc/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c82319b493889aaa60912319369e33dd049420fc/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fhelium.md?ref=c82319b493889aaa60912319369e33dd049420fc",
            "patch": "@@ -107,24 +107,20 @@ Tips:\n   \n ## Usage tips\n \n-`Helium` can be found on the [Huggingface Hub](https://huggingface.co/collections/kyutai/helium-1-preview)\n+`Helium` can be found on the [Huggingface Hub](https://huggingface.co/models?other=helium)\n \n In the following, we demonstrate how to use `helium-1-preview` for the inference. \n \n ```python\n >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n >>> device = \"cuda\" # the device to load the model onto\n \n->>> model = AutoModelForCausalLM.from_pretrained(\"helium-1-preview\", device_map=\"auto\")\n->>> tokenizer = AutoTokenizer.from_pretrained(\"helium-1-preview\")\n+>>> model = AutoModelForCausalLM.from_pretrained(\"kyutai/helium-1-preview-2b\", device_map=\"auto\")\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"kyutai/helium-1-preview-2b\")\n \n >>> prompt = \"Give me a short introduction to large language model.\"\n \n->>> messages = [{\"role\": \"user\", \"content\": prompt}]\n-\n->>> text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n-\n->>> model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n+>>> model_inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n \n >>> generated_ids = model.generate(model_inputs.input_ids, max_new_tokens=512, do_sample=True)\n "
        }
    ],
    "stats": {
        "total": 12,
        "additions": 4,
        "deletions": 8
    }
}