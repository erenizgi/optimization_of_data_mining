{
    "author": "dggaytan",
    "message": "Removing extra space in large command for speech-pretraining example (#38705)\n\nRemoving extra space in Large command",
    "sha": "4f650040a68c915c4e9fa70c4a7a62714e471d65",
    "files": [
        {
            "sha": "3475eb1b48293bbd12b4fccc9d1756c05c6ed084",
            "filename": "examples/pytorch/speech-pretraining/README.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f650040a68c915c4e9fa70c4a7a62714e471d65/examples%2Fpytorch%2Fspeech-pretraining%2FREADME.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f650040a68c915c4e9fa70c4a7a62714e471d65/examples%2Fpytorch%2Fspeech-pretraining%2FREADME.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fspeech-pretraining%2FREADME.md?ref=4f650040a68c915c4e9fa70c4a7a62714e471d65",
            "patch": "@@ -129,7 +129,7 @@ To pre-train `\"large-sized\"` Wav2Vec2 model, *e.g.* [facebook/wav2vec2-large-lv6\n on [librispeech_asr](https://huggingface.co/datasets/librispeech_asr), the following command can be run:\n \n ```bash\n-accelerate launch run_wav2vec2_pretraining_no_trainer.py \\ \n+accelerate launch run_wav2vec2_pretraining_no_trainer.py \\\n \t--dataset_name=librispeech_asr \\\n \t--dataset_config_names clean clean other \\\n \t--dataset_split_names train.100 train.360 train.500 \\\n@@ -141,7 +141,7 @@ accelerate launch run_wav2vec2_pretraining_no_trainer.py \\\n \t--weight_decay=0.01 \\\n \t--max_duration_in_seconds=20.0 \\\n \t--min_duration_in_seconds=2.0 \\\n-\t--model_name_or_path=./ \n+\t--model_name_or_path=./ \\\n \t--logging_steps=1 \\\n \t--saving_steps=10000 \\\n \t--per_device_train_batch_size=2 \\"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}