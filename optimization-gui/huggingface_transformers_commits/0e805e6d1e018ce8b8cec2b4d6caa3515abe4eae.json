{
    "author": "MekkCyber",
    "message": "Skipping aqlm non working inference tests till fix merged (#34865)",
    "sha": "0e805e6d1e018ce8b8cec2b4d6caa3515abe4eae",
    "files": [
        {
            "sha": "8195d975711ab52ef3acb02051ed4101bfeba070",
            "filename": "tests/quantization/aqlm_integration/test_aqlm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e805e6d1e018ce8b8cec2b4d6caa3515abe4eae/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e805e6d1e018ce8b8cec2b4d6caa3515abe4eae/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Faqlm_integration%2Ftest_aqlm.py?ref=0e805e6d1e018ce8b8cec2b4d6caa3515abe4eae",
            "patch": "@@ -17,6 +17,7 @@\n import importlib\n import tempfile\n import unittest\n+from unittest import skip\n \n from packaging import version\n \n@@ -142,6 +143,9 @@ def test_quantized_model_conversion(self):\n \n         self.assertEqual(nb_linears - 1, nb_aqlm_linear)\n \n+    @skip(\n+        \"inference doesn't work with quantized aqlm models using torch.Any type with recent torch versions. Waiting for the fix from AQLM side\"\n+    )\n     def test_quantized_model(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly\n@@ -158,6 +162,9 @@ def test_raise_if_non_quantized(self):\n         with self.assertRaises(ValueError):\n             _ = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n \n+    @skip(\n+        \"inference doesn't work with quantized aqlm models using torch.Any type with recent torch versions. Waiting for the fix from AQLM side\"\n+    )\n     def test_save_pretrained(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly after being saved and loaded\n@@ -171,6 +178,9 @@ def test_save_pretrained(self):\n             output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens)\n             self.assertEqual(self.tokenizer.decode(output[0], skip_special_tokens=True), self.EXPECTED_OUTPUT)\n \n+    @skip(\n+        \"inference doesn't work with quantized aqlm models using torch.Any type with recent torch versions. Waiting for the fix from AQLM side\"\n+    )\n     @require_torch_multi_gpu\n     def test_quantized_model_multi_gpu(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 10,
        "additions": 10,
        "deletions": 0
    }
}