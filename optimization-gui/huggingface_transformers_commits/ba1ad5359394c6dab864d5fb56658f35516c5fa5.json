{
    "author": "Wauplin",
    "message": "Use hfh's is_offline_mode helper (#42657)\n\n* Use hfh's is_offline_mode helper\n\n* fix tests",
    "sha": "ba1ad5359394c6dab864d5fb56658f35516c5fa5",
    "files": [
        {
            "sha": "cac0b3ee3d8ddaf25c7045c3e5d6a13b1d9b1c79",
            "filename": "examples/pytorch/summarization/run_summarization.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -45,6 +45,7 @@\n import numpy as np\n from datasets import load_dataset\n from filelock import FileLock\n+from huggingface_hub import is_offline_mode\n \n import transformers\n from transformers import (\n@@ -61,7 +62,7 @@\n     Seq2SeqTrainingArguments,\n     set_seed,\n )\n-from transformers.utils import check_min_version, is_offline_mode\n+from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n "
        },
        {
            "sha": "bb402e933c5f8d7b46777f4f72693b7179d5abe3",
            "filename": "examples/pytorch/summarization/run_summarization_no_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fsummarization%2Frun_summarization_no_trainer.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -51,7 +51,7 @@\n from accelerate.utils import set_seed\n from datasets import load_dataset\n from filelock import FileLock\n-from huggingface_hub import HfApi\n+from huggingface_hub import HfApi, is_offline_mode\n from torch.utils.data import DataLoader\n from tqdm.auto import tqdm\n \n@@ -66,7 +66,7 @@\n     SchedulerType,\n     get_scheduler,\n )\n-from transformers.utils import check_min_version, is_offline_mode\n+from transformers.utils import check_min_version\n from transformers.utils.versions import require_version\n \n "
        },
        {
            "sha": "6702c47aec8a29598008f686e615b436269308ec",
            "filename": "setup.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/setup.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/setup.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/setup.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -112,7 +112,7 @@\n     \"GitPython<3.1.19\",\n     \"hf-doc-builder>=0.3.0\",\n     \"hf_xet\",\n-    \"huggingface-hub>=1.0.0,<2.0\",\n+    \"huggingface-hub>=1.2.1,<2.0\",\n     \"importlib_metadata\",\n     \"ipadic>=1.0.0,<2.0\",\n     \"jinja2>=3.1.0\","
        },
        {
            "sha": "00033733d75f54000787e1224634c0bd1fd3216e",
            "filename": "src/transformers/dependency_versions_table.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fdependency_versions_table.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fdependency_versions_table.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdependency_versions_table.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -22,7 +22,7 @@\n     \"GitPython\": \"GitPython<3.1.19\",\n     \"hf-doc-builder\": \"hf-doc-builder>=0.3.0\",\n     \"hf_xet\": \"hf_xet\",\n-    \"huggingface-hub\": \"huggingface-hub>=1.0.0,<2.0\",\n+    \"huggingface-hub\": \"huggingface-hub>=1.2.1,<2.0\",\n     \"importlib_metadata\": \"importlib_metadata\",\n     \"ipadic\": \"ipadic>=1.0.0,<2.0\",\n     \"jinja2\": \"jinja2>=3.1.0\","
        },
        {
            "sha": "d797831a26d144f651b9276010edcbbe07702df3",
            "filename": "src/transformers/dynamic_module_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fdynamic_module_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fdynamic_module_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdynamic_module_utils.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -30,15 +30,14 @@\n from types import ModuleType\n from typing import Any, Optional, Union\n \n-from huggingface_hub import try_to_load_from_cache\n+from huggingface_hub import is_offline_mode, try_to_load_from_cache\n from packaging import version\n \n from .utils import (\n     HF_MODULES_CACHE,\n     TRANSFORMERS_DYNAMIC_MODULE_NAME,\n     cached_file,\n     extract_commit_hash,\n-    is_offline_mode,\n     logging,\n )\n from .utils.import_utils import VersionComparison, split_package_version"
        },
        {
            "sha": "ed7a0978b4e16e183e12b0ec2e0456ec7ee98279",
            "filename": "src/transformers/feature_extraction_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Ffeature_extraction_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffeature_extraction_utils.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -22,7 +22,7 @@\n from typing import TYPE_CHECKING, Any, Optional, TypeVar, Union\n \n import numpy as np\n-from huggingface_hub import create_repo\n+from huggingface_hub import create_repo, is_offline_mode\n \n from .dynamic_module_utils import custom_object_save\n from .utils import (\n@@ -32,7 +32,6 @@\n     TensorType,\n     copy_func,\n     is_numpy_array,\n-    is_offline_mode,\n     is_torch_available,\n     is_torch_device,\n     is_torch_dtype,"
        },
        {
            "sha": "0b1dae92625518870f0f35744ff8ea8e189912e3",
            "filename": "src/transformers/file_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Ffile_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Ffile_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ffile_utils.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -68,7 +68,6 @@\n     is_in_notebook,\n     is_ipex_available,\n     is_librosa_available,\n-    is_offline_mode,\n     is_onnx_available,\n     is_pandas_available,\n     is_phonemizer_available,"
        },
        {
            "sha": "cd59b56e229672d0844c4e28a28330d284a0c97a",
            "filename": "src/transformers/image_processing_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fimage_processing_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fimage_processing_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fimage_processing_base.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -18,7 +18,7 @@\n from typing import Any, Optional, TypeVar, Union\n \n import numpy as np\n-from huggingface_hub import create_repo\n+from huggingface_hub import create_repo, is_offline_mode\n \n from .dynamic_module_utils import custom_object_save\n from .feature_extraction_utils import BatchFeature as BaseBatchFeature\n@@ -28,7 +28,6 @@\n     PROCESSOR_NAME,\n     PushToHubMixin,\n     copy_func,\n-    is_offline_mode,\n     logging,\n     safe_load_json_file,\n )"
        },
        {
            "sha": "4cdafea12154b12321c8e956a418f95c969f89c4",
            "filename": "src/transformers/modelcard.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fmodelcard.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fmodelcard.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodelcard.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -23,7 +23,7 @@\n \n import httpx\n import yaml\n-from huggingface_hub import model_info\n+from huggingface_hub import is_offline_mode, model_info\n from huggingface_hub.errors import OfflineModeIsEnabled\n from huggingface_hub.utils import HFValidationError\n \n@@ -50,7 +50,6 @@\n     MODEL_CARD_NAME,\n     cached_file,\n     is_datasets_available,\n-    is_offline_mode,\n     is_tokenizers_available,\n     is_torch_available,\n     logging,"
        },
        {
            "sha": "6c73d54f7a44e59757b7b99c66b60b375418a540",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -36,7 +36,7 @@\n from zipfile import is_zipfile\n \n import torch\n-from huggingface_hub import create_repo, split_torch_state_dict_into_shards\n+from huggingface_hub import create_repo, is_offline_mode, split_torch_state_dict_into_shards\n from packaging import version\n from safetensors import safe_open\n from safetensors.torch import save_file as safe_save_file\n@@ -110,7 +110,6 @@\n     is_flash_attn_2_available,\n     is_flash_attn_3_available,\n     is_kernels_available,\n-    is_offline_mode,\n     is_torch_flex_attn_available,\n     is_torch_greater_or_equal,\n     is_torch_mlu_available,"
        },
        {
            "sha": "d8ed322133097ab5ecdae948590dfa0fe43b0a3a",
            "filename": "src/transformers/pipelines/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fpipelines%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2F__init__.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -18,7 +18,7 @@\n from pathlib import Path\n from typing import TYPE_CHECKING, Any, Optional, Union\n \n-from huggingface_hub import model_info\n+from huggingface_hub import is_offline_mode, model_info\n \n from ..configuration_utils import PreTrainedConfig\n from ..dynamic_module_utils import get_class_from_dynamic_module\n@@ -38,7 +38,6 @@\n     extract_commit_hash,\n     find_adapter_config_file,\n     is_kenlm_available,\n-    is_offline_mode,\n     is_peft_available,\n     is_pyctcdecode_available,\n     is_torch_available,"
        },
        {
            "sha": "da846e12da7fb7f05b65eba53e50d038a6057c28",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -28,7 +28,7 @@\n \n import numpy as np\n import typing_extensions\n-from huggingface_hub import create_repo\n+from huggingface_hub import create_repo, is_offline_mode\n from huggingface_hub.dataclasses import validate_typed_dict\n from huggingface_hub.errors import EntryNotFoundError\n \n@@ -54,7 +54,6 @@\n     cached_file,\n     copy_func,\n     direct_transformers_import,\n-    is_offline_mode,\n     is_torch_available,\n     list_repo_templates,\n     logging,"
        },
        {
            "sha": "9670573cac67171f4e8fb5f7d41565d3fe724f05",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -33,7 +33,7 @@\n from typing import TYPE_CHECKING, Any, NamedTuple, Optional, Union\n \n import numpy as np\n-from huggingface_hub import create_repo, list_repo_files\n+from huggingface_hub import create_repo, is_offline_mode, list_repo_files\n from packaging import version\n \n from . import __version__\n@@ -51,7 +51,6 @@\n     extract_commit_hash,\n     is_mlx_available,\n     is_numpy_array,\n-    is_offline_mode,\n     is_protobuf_available,\n     is_tokenizers_available,\n     is_torch_available,"
        },
        {
            "sha": "183a2cf797a3072899329f571cb507b2aea88a6d",
            "filename": "src/transformers/tokenization_utils_tokenizers.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Ftokenization_utils_tokenizers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_tokenizers.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -24,6 +24,7 @@\n from typing import Any, Optional, Union\n \n import tokenizers.pre_tokenizers as pre_tokenizers_fast\n+from huggingface_hub import is_offline_mode\n from tokenizers import AddedToken, processors\n from tokenizers import Encoding as EncodingFast\n from tokenizers import Tokenizer as TokenizerFast\n@@ -42,7 +43,7 @@\n     TextInput,\n     TruncationStrategy,\n )\n-from .utils import PaddingStrategy, add_end_docstrings, is_offline_mode, logging\n+from .utils import PaddingStrategy, add_end_docstrings, logging\n \n \n logger = logging.get_logger(__name__)"
        },
        {
            "sha": "229a3c5df35000a549099c616cc05dcd16b5164e",
            "filename": "src/transformers/utils/__init__.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Futils%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Futils%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2F__init__.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -91,7 +91,6 @@\n     extract_commit_hash,\n     has_file,\n     http_user_agent,\n-    is_offline_mode,\n     list_repo_templates,\n     try_to_load_from_cache,\n )"
        },
        {
            "sha": "fa63195783531dc1068a474747bb465d4061d21f",
            "filename": "src/transformers/utils/hub.py",
            "status": "modified",
            "additions": 1,
            "deletions": 7,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Futils%2Fhub.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Futils%2Fhub.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fhub.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -37,6 +37,7 @@\n     create_repo,\n     hf_hub_download,\n     hf_hub_url,\n+    is_offline_mode,\n     list_repo_tree,\n     snapshot_download,\n     try_to_load_from_cache,\n@@ -83,13 +84,6 @@ class DownloadKwargs(TypedDict, total=False):\n     commit_hash: str | None\n \n \n-def is_offline_mode():\n-    # Import inside the function so test patches on `huggingface_hub.constants` are picked up.\n-    from huggingface_hub import constants as hf_hub_constants\n-\n-    return hf_hub_constants.HF_HUB_OFFLINE\n-\n-\n # Determine default cache directory.\n # The best way to set the cache path is with the environment variable HF_HOME. For more details, check out this\n # documentation page: https://huggingface.co/docs/huggingface_hub/package_reference/environment_variables."
        },
        {
            "sha": "d73bbce889f1dae7cf4937ac6f43f2102b7f9809",
            "filename": "src/transformers/video_processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fvideo_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/src%2Ftransformers%2Fvideo_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fvideo_processing_utils.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -22,7 +22,7 @@\n from typing import Any, Optional, Union\n \n import numpy as np\n-from huggingface_hub import create_repo\n+from huggingface_hub import create_repo, is_offline_mode\n from huggingface_hub.dataclasses import validate_typed_dict\n \n from .dynamic_module_utils import custom_object_save\n@@ -44,7 +44,6 @@\n     TensorType,\n     add_start_docstrings,\n     copy_func,\n-    is_offline_mode,\n     is_torch_available,\n     is_torchcodec_available,\n     is_torchvision_v2_available,"
        },
        {
            "sha": "0cf8ae251b4c447d950a3ae8fb686430fe23bbfe",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -315,16 +315,16 @@ class TestOffline(unittest.TestCase):\n         def test_offline(self):\n             with tempfile.TemporaryDirectory() as tmpdir:\n                 # First offline load should fail\n-                with patch(\"transformers.utils.hub.is_offline_mode\", return_value=True):\n+                with patch(\"huggingface_hub.constants.HF_HUB_OFFLINE\", True):\n                     with pytest.raises(OSError):\n                         AutoModelForImageClassification.from_pretrained(TINY_IMAGE_CLASSIF, cache_dir=tmpdir)\n \n                 # Enable online mode for download\n-                with patch(\"transformers.utils.hub.is_offline_mode\", return_value=False):\n+                with patch(\"huggingface_hub.constants.HF_HUB_OFFLINE\", False):\n                     snapshot_download(TINY_IMAGE_CLASSIF, cache_dir=tmpdir)\n \n                 # Load again in offline mode - should work now\n-                with patch(\"transformers.utils.hub.is_offline_mode\", return_value=True):\n+                with patch(\"huggingface_hub.constants.HF_HUB_OFFLINE\", True):\n                     AutoModelForImageClassification.from_pretrained(TINY_IMAGE_CLASSIF, cache_dir=tmpdir)\n \n         def test_local_files_only(self):"
        },
        {
            "sha": "20f1690bb719e3ea1e7c3e36b9a250e2f3d751f1",
            "filename": "tests/utils/test_offline.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ba1ad5359394c6dab864d5fb56658f35516c5fa5/tests%2Futils%2Ftest_offline.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ba1ad5359394c6dab864d5fb56658f35516c5fa5/tests%2Futils%2Ftest_offline.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_offline.py?ref=ba1ad5359394c6dab864d5fb56658f35516c5fa5",
            "patch": "@@ -182,7 +182,7 @@ def test_is_offline_mode(self):\n         \"\"\"\n         Test `is_offline_mode` helper (should respect both HF_HUB_OFFLINE and legacy TRANSFORMERS_OFFLINE env vars)\n         \"\"\"\n-        load = \"from transformers.utils import is_offline_mode\"\n+        load = \"from huggingface_hub import is_offline_mode\"\n         run = \"print(is_offline_mode())\"\n \n         stdout, _ = self._execute_with_env(load, run)"
        }
    ],
    "stats": {
        "total": 59,
        "additions": 22,
        "deletions": 37
    }
}