{
    "author": "zucchini-nlp",
    "message": "Blip: get/set input embeddings correctly (#34152)\n\n* set-get embeds\r\n\r\n* add tests\r\n\r\n* fix tests\r\n\r\n* remove\r\n\r\n* return dict True\r\n\r\n* fix tests\r\n\r\n* why did i remove this\r\n\r\n* enabel torchscript tests",
    "sha": "6beb3f1691ca13556c7421b2c0503068906137a2",
    "files": [
        {
            "sha": "b623d2a8adb17be6eedfc5595e73e89d17257c83",
            "filename": "src/transformers/models/blip/modeling_blip.py",
            "status": "modified",
            "additions": 24,
            "deletions": 7,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/6beb3f1691ca13556c7421b2c0503068906137a2/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6beb3f1691ca13556c7421b2c0503068906137a2/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip.py?ref=6beb3f1691ca13556c7421b2c0503068906137a2",
            "patch": "@@ -795,6 +795,12 @@ def __init__(self, config: BlipConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_input_embeddings(self):\n+        return self.text_model.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.text_model.set_input_embeddings(value)\n+\n     @add_start_docstrings_to_model_forward(BLIP_TEXT_INPUTS_DOCSTRING)\n     def get_text_features(\n         self,\n@@ -1053,8 +1059,11 @@ def __init__(self, config: BlipConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self) -> nn.Module:\n-        return self.vision_model.embeddings.patch_embedding\n+    def get_input_embeddings(self):\n+        return self.text_decoder.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.text_decoder.set_input_embeddings(value)\n \n     @add_start_docstrings_to_model_forward(BLIP_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BlipForConditionalGenerationModelOutput, config_class=BlipVisionConfig)\n@@ -1117,7 +1126,8 @@ def forward(\n         )\n \n         if not return_dict:\n-            outputs = (outputs[0], outputs[1], image_embeds, vision_outputs[0]) + vision_outputs[2:]\n+            outputs = (outputs[0], outputs[1]) if labels is not None else (outputs[0],)\n+            outputs += (image_embeds, vision_outputs[0]) + vision_outputs[2:]\n             return tuple(output for output in outputs if output is not None)\n \n         return BlipForConditionalGenerationModelOutput(\n@@ -1232,8 +1242,12 @@ def __init__(self, config: BlipConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self) -> nn.Module:\n-        return self.vision_model.embeddings.patch_embedding\n+    def set_input_embeddings(self, value):\n+        self.text_encoder.set_input_embeddings(value)\n+\n+    def get_input_embeddings(self):\n+        # This will return shared embeddings if they are shared else specific to encoder.\n+        return self.text_encoder.get_input_embeddings()\n \n     @add_start_docstrings_to_model_forward(BLIP_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BlipTextVisionModelOutput, config_class=BlipVisionConfig)\n@@ -1474,8 +1488,11 @@ def __init__(self, config: BlipConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self) -> nn.Module:\n-        return self.vision_model.embeddings.patch_embedding\n+    def get_input_embeddings(self):\n+        return self.text_encoder.get_input_embeddings()\n+\n+    def set_input_embeddings(self, value):\n+        self.text_encoder.set_input_embeddings(value)\n \n     @add_start_docstrings_to_model_forward(BLIP_VISION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=BlipTextVisionModelOutput, config_class=BlipVisionConfig)"
        },
        {
            "sha": "97a4f523380bc554697832ee81ef2c933742a36c",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/6beb3f1691ca13556c7421b2c0503068906137a2/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6beb3f1691ca13556c7421b2c0503068906137a2/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=6beb3f1691ca13556c7421b2c0503068906137a2",
            "patch": "@@ -817,6 +817,12 @@ def __init__(self, config):\n         self.cls = BlipTextOnlyMLMHead(config)\n         self.label_smoothing = config.label_smoothing\n \n+    def get_input_embeddings(self):\n+        return self.bert.get_input_embeddings()\n+\n+    def set_input_embeddings(self, new_embeddings):\n+        self.bert.set_input_embeddings(new_embeddings)\n+\n     def get_output_embeddings(self):\n         return self.cls.predictions.decoder\n "
        },
        {
            "sha": "4c06d85b50df6a8e9d1283cbd113a4cc9f412be8",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 6,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/6beb3f1691ca13556c7421b2c0503068906137a2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6beb3f1691ca13556c7421b2c0503068906137a2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=6beb3f1691ca13556c7421b2c0503068906137a2",
            "patch": "@@ -1768,11 +1768,12 @@ def forward(\n                 decoder_attention_mask=decoder_attention_mask,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n+                return_dict=True,  # toggle for easier access to loss/logits below\n                 labels=labels,\n             )\n-            loss = outputs.loss if return_dict else outputs[0]\n-            logits = outputs.logits if return_dict else outputs[1]\n+            loss = outputs.loss\n+            logits = outputs.logits\n+            outputs = outputs.to_tuple() if not return_dict else outputs\n \n         if not return_dict:\n             output = (logits, vision_outputs, query_outputs, outputs)\n@@ -1810,6 +1811,12 @@ def __init__(self, config: Blip2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_input_embeddings(self):\n+        return self.embeddings.word_embeddings\n+\n+    def set_input_embeddings(self, value):\n+        self.embeddings.word_embeddings = value\n+\n     @add_start_docstrings_to_model_forward(BLIP_2_TEXT_WITH_PROJECTION_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Blip2TextModelOutput, config_class=Blip2Config)\n     def forward(\n@@ -2233,11 +2240,12 @@ def forward(\n                 decoder_attention_mask=decoder_attention_mask,\n                 output_attentions=output_attentions,\n                 output_hidden_states=output_hidden_states,\n-                return_dict=return_dict,\n+                return_dict=True,  # toggle for easier access to loss/logits below\n                 labels=labels,\n             )\n-            loss = outputs.loss if return_dict else outputs[0]\n-            logits = outputs.logits if return_dict else outputs[1]\n+            loss = outputs.loss\n+            logits = outputs.logits\n+            outputs = outputs.to_tuple() if not return_dict else outputs\n \n         if not return_dict:\n             output = (logits, vision_outputs, query_outputs, outputs)\n@@ -2389,6 +2397,12 @@ def __init__(self, config: Blip2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_input_embeddings(self):\n+        return self.embeddings.word_embeddings\n+\n+    def set_input_embeddings(self, value):\n+        self.embeddings.word_embeddings = value\n+\n     @add_start_docstrings_to_model_forward(BLIP2_IMAGE_TEXT_RETRIEVAL_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=Blip2ImageTextMatchingModelOutput, config_class=Blip2Config)\n     def forward("
        },
        {
            "sha": "d542757cbf879ffdc9d1c04831e5ea6638c797d9",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/6beb3f1691ca13556c7421b2c0503068906137a2/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6beb3f1691ca13556c7421b2c0503068906137a2/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=6beb3f1691ca13556c7421b2c0503068906137a2",
            "patch": "@@ -444,7 +444,7 @@ class BlipModelTest(ModelTesterMixin, PipelineTesterMixin, unittest.TestCase):\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n-    test_resize_embeddings = False\n+    test_resize_embeddings = True\n     test_attention_outputs = False\n \n     def setUp(self):\n@@ -738,7 +738,6 @@ def prepare_config_and_inputs_for_common(self):\n         config, input_ids, attention_mask, pixel_values = config_and_inputs\n         inputs_dict = {\n             \"input_ids\": input_ids,\n-            \"labels\": input_ids,\n             \"attention_mask\": attention_mask,\n             \"pixel_values\": pixel_values,\n         }\n@@ -787,10 +786,10 @@ def prepare_config_and_inputs_for_common(self):\n         config, input_ids, attention_mask, pixel_values = config_and_inputs\n         inputs_dict = {\n             \"input_ids\": input_ids,\n-            \"labels\": input_ids,\n             \"decoder_input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n             \"pixel_values\": pixel_values,\n+            \"labels\": input_ids,\n         }\n         return config, inputs_dict\n \n@@ -802,7 +801,7 @@ class BlipVQAModelTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n-    test_resize_embeddings = False\n+    test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False\n \n@@ -811,7 +810,6 @@ def setUp(self):\n \n     def _prepare_inputs_for_vqa(self):\n         _, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n-        inputs_dict[\"labels\"] = inputs_dict[\"input_ids\"]\n         inputs_dict[\"decoder_input_ids\"] = inputs_dict[\"input_ids\"]\n         inputs_dict.pop(\"return_loss\")\n         return inputs_dict\n@@ -882,7 +880,7 @@ class BlipTextRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n-    test_resize_embeddings = False\n+    test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False\n \n@@ -1110,7 +1108,7 @@ class BlipTextImageModelTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n-    test_resize_embeddings = False\n+    test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False\n "
        },
        {
            "sha": "d91adf1bd4104f888921cdefc75e9a64f47c90d3",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 229,
            "deletions": 10,
            "changes": 239,
            "blob_url": "https://github.com/huggingface/transformers/blob/6beb3f1691ca13556c7421b2c0503068906137a2/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6beb3f1691ca13556c7421b2c0503068906137a2/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=6beb3f1691ca13556c7421b2c0503068906137a2",
            "patch": "@@ -15,6 +15,7 @@\n \"\"\"Testing suite for the PyTorch BLIP-2 model.\"\"\"\n \n import inspect\n+import os\n import tempfile\n import unittest\n \n@@ -32,7 +33,7 @@\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torch_sdpa_available, is_vision_available\n \n from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n@@ -443,7 +444,6 @@ def prepare_config_and_inputs_for_common(self):\n             \"pixel_values\": pixel_values,\n             \"input_ids\": input_ids,\n             \"attention_mask\": attention_mask,\n-            \"labels\": input_ids,\n         }\n         return config, inputs_dict\n \n@@ -456,7 +456,7 @@ class Blip2ForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationT\n     test_pruning = False\n     test_resize_embeddings = False\n     test_attention_outputs = False\n-    test_torchscript = False\n+    test_torchscript = True\n     _is_composite = True\n \n     def setUp(self):\n@@ -466,6 +466,116 @@ def test_for_conditional_generation(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_conditional_generation(*config_and_inputs)\n \n+    def _create_and_check_torchscript(self, config, inputs_dict):\n+        # overwrite because BLIP requires ipnut ids and pixel values as input\n+        if not self.test_torchscript:\n+            self.skipTest(reason=\"test_torchscript is set to `False`\")\n+\n+        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n+        configs_no_init.torchscript = True\n+        for model_class in self.all_model_classes:\n+            for attn_implementation in [\"eager\", \"sdpa\"]:\n+                if attn_implementation == \"sdpa\" and (not model_class._supports_sdpa or not is_torch_sdpa_available()):\n+                    continue\n+\n+                configs_no_init._attn_implementation = attn_implementation\n+                model = model_class(config=configs_no_init)\n+                model.to(torch_device)\n+                model.eval()\n+                inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+                main_input_name = model_class.main_input_name\n+\n+                try:\n+                    if model.config.is_encoder_decoder:\n+                        model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n+                        main_input = inputs[main_input_name]\n+                        input_ids = inputs[\"input_ids\"]\n+                        attention_mask = inputs[\"attention_mask\"]\n+                        decoder_input_ids = inputs[\"decoder_input_ids\"]\n+                        decoder_attention_mask = inputs[\"decoder_attention_mask\"]\n+                        model(main_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask)\n+                        traced_model = torch.jit.trace(\n+                            model, (main_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask)\n+                        )\n+                    else:\n+                        main_input = inputs[main_input_name]\n+                        input_ids = inputs[\"input_ids\"]\n+\n+                        if model.config._attn_implementation == \"sdpa\":\n+                            trace_input = {main_input_name: main_input, \"input_ids\": input_ids}\n+\n+                            if \"attention_mask\" in inputs:\n+                                trace_input[\"attention_mask\"] = inputs[\"attention_mask\"]\n+                            else:\n+                                self.skipTest(reason=\"testing SDPA without attention_mask is not supported\")\n+\n+                            model(main_input, attention_mask=inputs[\"attention_mask\"])\n+                            # example_kwarg_inputs was introduced in torch==2.0, but it is fine here since SDPA has a requirement on torch>=2.1.\n+                            traced_model = torch.jit.trace(model, example_kwarg_inputs=trace_input)\n+                        else:\n+                            model(main_input, input_ids)\n+                            traced_model = torch.jit.trace(model, (main_input, input_ids))\n+                except RuntimeError:\n+                    self.fail(\"Couldn't trace module.\")\n+\n+                with tempfile.TemporaryDirectory() as tmp_dir_name:\n+                    pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n+\n+                    try:\n+                        torch.jit.save(traced_model, pt_file_name)\n+                    except Exception:\n+                        self.fail(\"Couldn't save module.\")\n+\n+                    try:\n+                        loaded_model = torch.jit.load(pt_file_name)\n+                    except Exception:\n+                        self.fail(\"Couldn't load module.\")\n+\n+                model.to(torch_device)\n+                model.eval()\n+\n+                loaded_model.to(torch_device)\n+                loaded_model.eval()\n+\n+                model_state_dict = model.state_dict()\n+                loaded_model_state_dict = loaded_model.state_dict()\n+\n+                non_persistent_buffers = {}\n+                for key in loaded_model_state_dict.keys():\n+                    if key not in model_state_dict.keys():\n+                        non_persistent_buffers[key] = loaded_model_state_dict[key]\n+\n+                loaded_model_state_dict = {\n+                    key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n+                }\n+\n+                self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n+\n+                model_buffers = list(model.buffers())\n+                for non_persistent_buffer in non_persistent_buffers.values():\n+                    found_buffer = False\n+                    for i, model_buffer in enumerate(model_buffers):\n+                        if torch.equal(non_persistent_buffer, model_buffer):\n+                            found_buffer = True\n+                            break\n+\n+                    self.assertTrue(found_buffer)\n+                    model_buffers.pop(i)\n+\n+                models_equal = True\n+                for layer_name, p1 in model_state_dict.items():\n+                    if layer_name in loaded_model_state_dict:\n+                        p2 = loaded_model_state_dict[layer_name]\n+                        if p1.data.ne(p2.data).sum() > 0:\n+                            models_equal = False\n+\n+                self.assertTrue(models_equal)\n+\n+                # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n+                # (Even with this call, there are still memory leak by ~0.04MB)\n+                self.clear_torch_jit_class_registry()\n+\n     @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass\n@@ -754,7 +864,6 @@ def prepare_config_and_inputs_for_common(self):\n             \"attention_mask\": attention_mask,\n             \"decoder_input_ids\": decoder_input_ids,\n             \"decoder_attention_mask\": decoder_attention_mask,\n-            \"labels\": labels,\n         }\n         return config, inputs_dict\n \n@@ -775,9 +884,9 @@ class Blip2ModelTest(ModelTesterMixin, PipelineTesterMixin, GenerationTesterMixi\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n-    test_resize_embeddings = False\n+    test_resize_embeddings = True\n     test_attention_outputs = False\n-    test_torchscript = False\n+    test_torchscript = True\n     _is_composite = True\n \n     # TODO: Fix the failed tests\n@@ -804,6 +913,116 @@ def test_for_conditional_generation(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_for_conditional_generation(*config_and_inputs)\n \n+    def _create_and_check_torchscript(self, config, inputs_dict):\n+        # overwrite because BLIP requires ipnut ids and pixel values as input\n+        if not self.test_torchscript:\n+            self.skipTest(reason=\"test_torchscript is set to `False`\")\n+\n+        configs_no_init = _config_zero_init(config)  # To be sure we have no Nan\n+        configs_no_init.torchscript = True\n+        for model_class in self.all_model_classes:\n+            for attn_implementation in [\"eager\", \"sdpa\"]:\n+                if attn_implementation == \"sdpa\" and (not model_class._supports_sdpa or not is_torch_sdpa_available()):\n+                    continue\n+\n+                configs_no_init._attn_implementation = attn_implementation\n+                model = model_class(config=configs_no_init)\n+                model.to(torch_device)\n+                model.eval()\n+                inputs = self._prepare_for_class(inputs_dict, model_class)\n+\n+                main_input_name = model_class.main_input_name\n+\n+                try:\n+                    if model.config.is_encoder_decoder:\n+                        model.config.use_cache = False  # FSTM still requires this hack -> FSTM should probably be refactored similar to BART afterward\n+                        main_input = inputs[main_input_name]\n+                        input_ids = inputs[\"input_ids\"]\n+                        attention_mask = inputs[\"attention_mask\"]\n+                        decoder_input_ids = inputs[\"decoder_input_ids\"]\n+                        decoder_attention_mask = inputs[\"decoder_attention_mask\"]\n+                        model(main_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask)\n+                        traced_model = torch.jit.trace(\n+                            model, (main_input, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask)\n+                        )\n+                    else:\n+                        main_input = inputs[main_input_name]\n+                        input_ids = inputs[\"input_ids\"]\n+\n+                        if model.config._attn_implementation == \"sdpa\":\n+                            trace_input = {main_input_name: main_input, \"input_ids\": input_ids}\n+\n+                            if \"attention_mask\" in inputs:\n+                                trace_input[\"attention_mask\"] = inputs[\"attention_mask\"]\n+                            else:\n+                                self.skipTest(reason=\"testing SDPA without attention_mask is not supported\")\n+\n+                            model(main_input, attention_mask=inputs[\"attention_mask\"])\n+                            # example_kwarg_inputs was introduced in torch==2.0, but it is fine here since SDPA has a requirement on torch>=2.1.\n+                            traced_model = torch.jit.trace(model, example_kwarg_inputs=trace_input)\n+                        else:\n+                            model(main_input, input_ids)\n+                            traced_model = torch.jit.trace(model, (main_input, input_ids))\n+                except RuntimeError:\n+                    self.fail(\"Couldn't trace module.\")\n+\n+                with tempfile.TemporaryDirectory() as tmp_dir_name:\n+                    pt_file_name = os.path.join(tmp_dir_name, \"traced_model.pt\")\n+\n+                    try:\n+                        torch.jit.save(traced_model, pt_file_name)\n+                    except Exception:\n+                        self.fail(\"Couldn't save module.\")\n+\n+                    try:\n+                        loaded_model = torch.jit.load(pt_file_name)\n+                    except Exception:\n+                        self.fail(\"Couldn't load module.\")\n+\n+                model.to(torch_device)\n+                model.eval()\n+\n+                loaded_model.to(torch_device)\n+                loaded_model.eval()\n+\n+                model_state_dict = model.state_dict()\n+                loaded_model_state_dict = loaded_model.state_dict()\n+\n+                non_persistent_buffers = {}\n+                for key in loaded_model_state_dict.keys():\n+                    if key not in model_state_dict.keys():\n+                        non_persistent_buffers[key] = loaded_model_state_dict[key]\n+\n+                loaded_model_state_dict = {\n+                    key: value for key, value in loaded_model_state_dict.items() if key not in non_persistent_buffers\n+                }\n+\n+                self.assertEqual(set(model_state_dict.keys()), set(loaded_model_state_dict.keys()))\n+\n+                model_buffers = list(model.buffers())\n+                for non_persistent_buffer in non_persistent_buffers.values():\n+                    found_buffer = False\n+                    for i, model_buffer in enumerate(model_buffers):\n+                        if torch.equal(non_persistent_buffer, model_buffer):\n+                            found_buffer = True\n+                            break\n+\n+                    self.assertTrue(found_buffer)\n+                    model_buffers.pop(i)\n+\n+                models_equal = True\n+                for layer_name, p1 in model_state_dict.items():\n+                    if layer_name in loaded_model_state_dict:\n+                        p2 = loaded_model_state_dict[layer_name]\n+                        if p1.data.ne(p2.data).sum() > 0:\n+                            models_equal = False\n+\n+                self.assertTrue(models_equal)\n+\n+                # Avoid memory leak. Without this, each call increase RAM usage by ~20MB.\n+                # (Even with this call, there are still memory leak by ~0.04MB)\n+                self.clear_torch_jit_class_registry()\n+\n     @unittest.skip(reason=\"Hidden_states is tested in individual model tests\")\n     def test_hidden_states_output(self):\n         pass\n@@ -942,7 +1161,7 @@ def test_get_text_features(self):\n     def test_get_image_features(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n-        keys_to_pop = [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"]\n+        keys_to_pop = [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\"]\n \n         for key in keys_to_pop:\n             inputs_dict.pop(key)\n@@ -962,7 +1181,7 @@ def test_get_image_features(self):\n     def test_get_qformer_features(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n \n-        keys_to_pop = [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\", \"labels\"]\n+        keys_to_pop = [\"input_ids\", \"attention_mask\", \"decoder_input_ids\", \"decoder_attention_mask\"]\n \n         for key in keys_to_pop:\n             inputs_dict.pop(key)\n@@ -1072,7 +1291,7 @@ class Blip2TextModelWithProjectionTest(ModelTesterMixin, unittest.TestCase):\n     test_pruning = False\n     test_head_masking = False\n \n-    test_resize_embeddings = False\n+    test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False\n \n@@ -1396,7 +1615,7 @@ class Blip2TextRetrievalModelTest(ModelTesterMixin, unittest.TestCase):\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n-    test_resize_embeddings = False\n+    test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False\n "
        },
        {
            "sha": "a9dba06dab823ccbc1c48e3ef25a48bf6e6b607c",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6beb3f1691ca13556c7421b2c0503068906137a2/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6beb3f1691ca13556c7421b2c0503068906137a2/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=6beb3f1691ca13556c7421b2c0503068906137a2",
            "patch": "@@ -459,7 +459,7 @@ class InstructBlipForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, Gene\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n-    test_resize_embeddings = False\n+    test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False\n     _is_composite = True"
        },
        {
            "sha": "ce25571d29333e784651436e6cabcff18f385338",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6beb3f1691ca13556c7421b2c0503068906137a2/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6beb3f1691ca13556c7421b2c0503068906137a2/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=6beb3f1691ca13556c7421b2c0503068906137a2",
            "patch": "@@ -479,7 +479,7 @@ class InstructBlipVideoForConditionalGenerationDecoderOnlyTest(\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n-    test_resize_embeddings = False\n+    test_resize_embeddings = True\n     test_attention_outputs = False\n     test_torchscript = False\n     _is_composite = True"
        },
        {
            "sha": "13c4d5155be4454c9d6d3af9eb2bccf9276e5678",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/6beb3f1691ca13556c7421b2c0503068906137a2/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6beb3f1691ca13556c7421b2c0503068906137a2/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=6beb3f1691ca13556c7421b2c0503068906137a2",
            "patch": "@@ -1811,6 +1811,7 @@ def test_resize_tokens_embeddings(self):\n             original_config,\n             inputs_dict,\n         ) = self.model_tester.prepare_config_and_inputs_for_common()\n+        inputs_dict.pop(\"labels\", None)\n \n         for model_class in self.all_model_classes:\n             config = copy.deepcopy(original_config)\n@@ -1988,6 +1989,7 @@ def test_resize_embeddings_untied(self):\n \n         original_config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         original_config.tie_word_embeddings = False\n+        inputs_dict.pop(\"labels\", None)\n \n         # if model cannot untied embeddings -> leave test\n         if original_config.tie_word_embeddings:"
        }
    ],
    "stats": {
        "total": 320,
        "additions": 288,
        "deletions": 32
    }
}