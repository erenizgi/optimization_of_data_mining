{
    "author": "zucchini-nlp",
    "message": "BLIP: enable generation tests (#34174)\n\n* blip2 tests\r\n\r\n* instructblips\r\n\r\n* copies\r\n\r\n* fix slow tests\r\n\r\n* fix\r\n\r\n* uncomment this\r\n\r\n* clean up after rebase\r\n\r\n* should be model main input\r\n\r\n* fix overwritten tests\r\n\r\n* oops len should be multiple of frame number\r\n\r\n* style\r\n\r\n* fix some tests",
    "sha": "4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6",
    "files": [
        {
            "sha": "08e42d1c8f70cbb496b4c6960cc132c4e44836fd",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 17,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6",
            "patch": "@@ -2342,24 +2342,11 @@ def generate(\n                 )\n                 generate_kwargs[\"min_length\"] = generate_kwargs.get(\"min_length\", 0) + language_model_inputs.shape[1]\n \n-        outputs = self.language_model.generate(\n-            inputs_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-            **generate_kwargs,\n-        )\n-\n-        # this is a temporary workaround to be consistent with other generation models and\n-        # have BOS as the first token, even though under the hood we are calling LM with embeds\n+        inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}\n         if not self.language_model.config.is_encoder_decoder:\n-            bos_tokens = (\n-                torch.LongTensor([[self.config.text_config.bos_token_id]])\n-                .repeat(batch_size, 1)\n-                .to(image_embeds.device)\n-            )\n-            if not isinstance(outputs, torch.Tensor):\n-                outputs.sequences = torch.cat([bos_tokens, outputs.sequences], dim=-1)\n-            else:\n-                outputs = torch.cat([bos_tokens, outputs], dim=-1)\n+            inputs[\"input_ids\"] = input_ids\n+\n+        outputs = self.language_model.generate(**inputs, **generate_kwargs)\n         return outputs\n \n "
        },
        {
            "sha": "a78a3b6687742948dfa0dc39a2fb2ed61d4341a3",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 21,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6",
            "patch": "@@ -1625,27 +1625,10 @@ def generate(\n                 )\n                 generate_kwargs[\"min_length\"] = generate_kwargs.get(\"min_length\", 0) + language_model_inputs.shape[1]\n \n-        outputs = self.language_model.generate(\n-            inputs_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-            **generate_kwargs,\n-        )\n-\n-        # this is a temporary workaround to be consistent with other generation models and\n-        # have BOS as the first token, even though under the hood we are calling LM with embeds\n+        inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}\n         if not self.language_model.config.is_encoder_decoder:\n-            # the InstructBLIP authors used inconsistent tokenizer/model files during training,\n-            # with the tokenizer's bos token being set to </s> which has ID=2,\n-            # whereas the model's text config has bos token id = 0\n-            bos_token_id = (\n-                2\n-                if self.config.text_config.architectures[0] == \"LLaMAForCausalLM\"\n-                else self.config.text_config.bos_token_id\n-            )\n-            bos_tokens = torch.LongTensor([[bos_token_id]]).repeat(batch_size, 1).to(image_embeds.device)\n-            if not isinstance(outputs, torch.Tensor):\n-                outputs.sequences = torch.cat([bos_tokens, outputs.sequences], dim=-1)\n-            else:\n-                outputs = torch.cat([bos_tokens, outputs], dim=-1)\n+            inputs[\"input_ids\"] = input_ids\n+\n+        outputs = self.language_model.generate(**inputs, **generate_kwargs)\n \n         return outputs"
        },
        {
            "sha": "90fc2113973a95865daee9b6a1b84eeb0ba51296",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 21,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6",
            "patch": "@@ -1660,27 +1660,10 @@ def generate(\n                 )\n                 generate_kwargs[\"min_length\"] = generate_kwargs.get(\"min_length\", 0) + language_model_inputs.shape[1]\n \n-        outputs = self.language_model.generate(\n-            inputs_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-            **generate_kwargs,\n-        )\n-\n-        # this is a temporary workaround to be consistent with other generation models and\n-        # have BOS as the first token, even though under the hood we are calling LM with embeds\n+        inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}\n         if not self.language_model.config.is_encoder_decoder:\n-            # the InstructBLIP authors used inconsistent tokenizer/model files during training,\n-            # with the tokenizer's bos token being set to </s> which has ID=2,\n-            # whereas the model's text config has bos token id = 0\n-            bos_token_id = (\n-                2\n-                if self.config.text_config.architectures[0] == \"LLaMAForCausalLM\"\n-                else self.config.text_config.bos_token_id\n-            )\n-            bos_tokens = torch.LongTensor([[bos_token_id]]).repeat(batch_size, 1).to(image_embeds.device)\n-            if not isinstance(outputs, torch.Tensor):\n-                outputs.sequences = torch.cat([bos_tokens, outputs.sequences], dim=-1)\n-            else:\n-                outputs = torch.cat([bos_tokens, outputs], dim=-1)\n+            inputs[\"input_ids\"] = input_ids\n+\n+        outputs = self.language_model.generate(**inputs, **generate_kwargs)\n \n         return outputs"
        },
        {
            "sha": "63c6c486854c574b56df0220ef807f46ac2f925a",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 4,
            "deletions": 21,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6",
            "patch": "@@ -468,27 +468,10 @@ def generate(\n                 )\n                 generate_kwargs[\"min_length\"] = generate_kwargs.get(\"min_length\", 0) + language_model_inputs.shape[1]\n \n-        outputs = self.language_model.generate(\n-            inputs_embeds=inputs_embeds,\n-            attention_mask=attention_mask,\n-            **generate_kwargs,\n-        )\n-\n-        # this is a temporary workaround to be consistent with other generation models and\n-        # have BOS as the first token, even though under the hood we are calling LM with embeds\n+        inputs = {\"inputs_embeds\": inputs_embeds, \"attention_mask\": attention_mask}\n         if not self.language_model.config.is_encoder_decoder:\n-            # the InstructBLIP authors used inconsistent tokenizer/model files during training,\n-            # with the tokenizer's bos token being set to </s> which has ID=2,\n-            # whereas the model's text config has bos token id = 0\n-            bos_token_id = (\n-                2\n-                if self.config.text_config.architectures[0] == \"LLaMAForCausalLM\"\n-                else self.config.text_config.bos_token_id\n-            )\n-            bos_tokens = torch.LongTensor([[bos_token_id]]).repeat(batch_size, 1).to(image_embeds.device)\n-            if not isinstance(outputs, torch.Tensor):\n-                outputs.sequences = torch.cat([bos_tokens, outputs.sequences], dim=-1)\n-            else:\n-                outputs = torch.cat([bos_tokens, outputs], dim=-1)\n+            inputs[\"input_ids\"] = input_ids\n+\n+        outputs = self.language_model.generate(**inputs, **generate_kwargs)\n \n         return outputs"
        },
        {
            "sha": "3bd8ce4b59c9b1d86af9601120523ef811a5eb2d",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6",
            "patch": "@@ -96,6 +96,7 @@\n \n \n class GenerationTesterMixin:\n+    input_name = \"input_ids\"\n     model_tester = None\n     all_generative_model_classes = ()\n     max_new_tokens = 3"
        },
        {
            "sha": "1ec9c2e1c07cdd17e1dc0812e1e7480d77823bdb",
            "filename": "tests/models/blip_2/test_modeling_blip_2.py",
            "status": "modified",
            "additions": 214,
            "deletions": 6,
            "changes": 220,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip_2%2Ftest_modeling_blip_2.py?ref=4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6",
            "patch": "@@ -20,7 +20,9 @@\n import unittest\n \n import numpy as np\n+import pytest\n import requests\n+from parameterized import parameterized\n \n from transformers import CONFIG_MAPPING, Blip2Config, Blip2QFormerConfig, Blip2VisionConfig\n from transformers.testing_utils import (\n@@ -392,7 +394,14 @@ def get_config(self):\n # this model tester uses a decoder-only language model (OPT)\n class Blip2ForConditionalGenerationDecoderOnlyModelTester:\n     def __init__(\n-        self, parent, vision_kwargs=None, qformer_kwargs=None, text_kwargs=None, is_training=True, num_query_tokens=10\n+        self,\n+        parent,\n+        vision_kwargs=None,\n+        qformer_kwargs=None,\n+        text_kwargs=None,\n+        is_training=True,\n+        num_query_tokens=10,\n+        image_token_index=4,\n     ):\n         if vision_kwargs is None:\n             vision_kwargs = {}\n@@ -406,14 +415,24 @@ def __init__(\n         self.qformer_model_tester = Blip2QFormerModelTester(parent, **qformer_kwargs)\n         self.text_model_tester = Blip2TextModelDecoderOnlyTester(parent, **text_kwargs)\n         self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n-        self.seq_length = self.text_model_tester.seq_length  # need seq_length for common tests\n+        self.seq_length = self.text_model_tester.seq_length + num_query_tokens  # need seq_length for common tests\n         self.is_training = is_training\n         self.num_query_tokens = num_query_tokens\n+        self.image_token_index = image_token_index\n \n     def prepare_config_and_inputs(self):\n         _, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n         _, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n \n+        vision_tokens = (\n+            torch.ones((input_ids.shape[0], self.num_query_tokens), device=torch_device, dtype=input_ids.dtype)\n+            * self.image_token_index\n+        )\n+        input_ids[input_ids == self.image_token_index] = self.text_model_tester.pad_token_id\n+        input_ids = torch.cat([vision_tokens, input_ids], dim=-1)\n+        vision_attention_mask = torch.ones_like(vision_tokens)\n+        attention_mask = torch.cat([vision_attention_mask, attention_mask], dim=-1)\n+\n         config = self.get_config()\n \n         return config, input_ids, attention_mask, pixel_values\n@@ -424,6 +443,7 @@ def get_config(self):\n             qformer_config=self.qformer_model_tester.get_config(),\n             text_config=self.text_model_tester.get_config(),\n             num_query_tokens=self.num_query_tokens,\n+            image_token_index=self.image_token_index,\n         )\n \n     def create_and_check_for_conditional_generation(self, config, input_ids, attention_mask, pixel_values):\n@@ -451,6 +471,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class Blip2ForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (Blip2ForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (Blip2ForConditionalGeneration,) if is_torch_available() else ()\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n@@ -693,6 +714,192 @@ def test_model_from_pretrained(self):\n         model = Blip2ForConditionalGeneration.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    # overwrite because BLIP internally calls LM.generate() with embeds thus it cannot operate in no cache format\n+    def _check_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n+        use_cache = True  # force this to be True in case False is passed\n+\n+        input_batch_size = int(output.sequences.shape[0] / num_return_sequences)\n+        internal_batch_size = (\n+            input_batch_size * num_beams if num_beams > 1 else input_batch_size * num_return_sequences\n+        )\n+\n+        seq_length = getattr(self.model_tester, \"seq_length\", None)\n+        seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_length)\n+        seq_length = getattr(self.model_tester, \"text_seq_length\", seq_length)\n+\n+        config = config.text_config if hasattr(config, \"text_config\") else config\n+\n+        gen_len = (\n+            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n+        )\n+\n+        # in some models we subsample the sequence length in inner layers\n+        if hasattr(self.model_tester, \"get_subsampled_output_lengths\"):\n+            seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n+\n+        # scores\n+        self._check_scores(internal_batch_size, output.scores, length=gen_len, config=config)\n+\n+        # unprocessed logits\n+        self._check_logits(internal_batch_size, output.logits, config=config)\n+\n+        # Attentions\n+        if self.has_attentions:\n+            if config.is_encoder_decoder:\n+                # encoder\n+                self._check_encoder_attention_for_generate(\n+                    output.encoder_attentions, input_batch_size, config, seq_length\n+                )\n+                # decoder\n+                self._check_attentions_for_generate(\n+                    internal_batch_size,\n+                    output.decoder_attentions,\n+                    min_length=1,\n+                    max_length=output.sequences.shape[-1],\n+                    config=config,\n+                    use_cache=use_cache,\n+                )\n+            else:\n+                # if use_cache first input is equal to no use_cache, so skip here\n+                attentions = output.attentions if not use_cache else output.attentions[1:]\n+                min_length = seq_length if not use_cache else seq_length + 1\n+                self._check_attentions_for_generate(\n+                    internal_batch_size,\n+                    attentions=attentions,\n+                    min_length=min_length,\n+                    max_length=output.sequences.shape[-1],\n+                    config=config,\n+                    use_cache=use_cache,\n+                )\n+\n+        # Hidden States\n+        if config.is_encoder_decoder:\n+            # encoder\n+            self._check_encoder_hidden_states_for_generate(\n+                output.encoder_hidden_states, input_batch_size, config, seq_length\n+            )\n+\n+            # decoder\n+            self._check_hidden_states_for_generate(\n+                internal_batch_size,\n+                output.decoder_hidden_states,\n+                min_length=1,\n+                max_length=output.sequences.shape[-1],\n+                config=config,\n+                use_cache=use_cache,\n+            )\n+        else:\n+            # if use_cache first input is equal to no use_cache, so skip here\n+            hidden_states = output.hidden_states if not use_cache else output.hidden_states[1:]\n+            min_length = seq_length if not use_cache else seq_length + 1\n+            self._check_hidden_states_for_generate(\n+                internal_batch_size,\n+                hidden_states,\n+                min_length=min_length,\n+                max_length=output.sequences.shape[-1],\n+                config=config,\n+                use_cache=use_cache,\n+            )\n+\n+        # Past Key Value States\n+        if use_cache:\n+            past_key_values = output.past_key_values\n+            past_sequence_length = output.sequences.shape[-1] - 1\n+            self._check_past_key_values_for_generate(\n+                internal_batch_size,\n+                past_key_values,\n+                seq_length=past_sequence_length,\n+                config=config,\n+            )\n+\n+    # overwrite because BLIP2 cannot generate only from input ids, and requires pixel values in all cases to be present\n+    @pytest.mark.generate\n+    def test_left_padding_compatibility(self):\n+        # NOTE: left-padding results in small numerical differences. This is expected.\n+        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n+\n+        # First, filter out models that don't support left padding\n+        # - The model must have generative capabilities\n+        if len(self.all_generative_model_classes) == 0:\n+            self.skipTest(reason=\"No generative architecture available for this model.\")\n+\n+        # - The model must support padding\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"This model doesn't support padding.\")\n+\n+        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n+        decoder_only_classes = []\n+        for model_class in self.all_generative_model_classes:\n+            config, _ = self.prepare_config_and_inputs_for_generate()\n+            if config.is_encoder_decoder:\n+                continue\n+            else:\n+                decoder_only_classes.append(model_class)\n+        if len(decoder_only_classes) == 0:\n+            self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n+\n+        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n+        #   added support for it yet. We skip these models for now.\n+        has_encoder_attributes = any(\n+            attr_name\n+            for attr_name in config.to_dict().keys()\n+            if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n+        )\n+        if has_encoder_attributes:\n+            self.skipTest(\n+                reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n+            )\n+\n+        # Then, test left-padding\n+        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n+            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n+            if \"position_ids\" in signature:\n+                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n+                position_ids.masked_fill_(attention_mask == 0, 1)\n+                model_kwargs[\"position_ids\"] = position_ids\n+            if \"cache_position\" in signature:\n+                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n+                model_kwargs[\"cache_position\"] = cache_position\n+            return model_kwargs\n+\n+        for model_class in decoder_only_classes:\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            input_ids = inputs_dict[\"input_ids\"]\n+            attention_mask = inputs_dict.get(\"attention_mask\")\n+            pixel_values = inputs_dict[\"pixel_values\"]\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(input_ids)\n+\n+            model = model_class(config).to(torch_device).eval()\n+            signature = inspect.signature(model.forward).parameters.keys()\n+\n+            # no cache as some models require special cache classes to be init outside forward\n+            model.generation_config.use_cache = False\n+\n+            # Without padding\n+            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n+            next_logits_wo_padding = model(**model_kwargs, pixel_values=pixel_values).logits[:, -1, :]\n+\n+            # With left-padding (length 32)\n+            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n+            pad_token_id = (\n+                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n+            )\n+            pad_size = (input_ids.shape[0], 32)\n+            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n+            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n+            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n+            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n+            next_logits_with_padding = model(**model_kwargs, pixel_values=pixel_values).logits[:, -1, :]\n+\n+            # They should result in very similar logits\n+            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-5))\n+\n+    @unittest.skip(\"BLIP2 cannot generate only from input ids, and requires pixel values in all cases to be present\")\n+    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n+    def test_generate_from_inputs_embeds(self, _, num_beams):\n+        pass\n+\n \n # this class is based on `T5ModelTester` found in tests/models/t5/test_modeling_t5.py\n class Blip2TextModelTester:\n@@ -1780,6 +1987,7 @@ def test_inference_opt(self):\n         generated_text = processor.batch_decode(predictions, skip_special_tokens=True)[0].strip()\n \n         # Test output\n+        print(predictions[0].tolist(), generated_text)\n         self.assertEqual(predictions[0].tolist(), [2, 102, 693, 2828, 15, 5, 4105, 19, 10, 2335, 50118])\n         self.assertEqual(\"a woman sitting on the beach with a dog\", generated_text)\n \n@@ -1794,9 +2002,9 @@ def test_inference_opt(self):\n         # Test output\n         self.assertEqual(\n             predictions[0].tolist(),\n-            [2, 24, 18, 45, 10, 343, 6, 24, 18, 10, 4105, 50118],\n+            [2, 45641, 35, 61, 343, 16, 42, 116, 31652, 35, 24, 18, 45, 10, 343, 6, 24, 18, 10, 4105, 50118],\n         )\n-        self.assertEqual(generated_text, \"it's not a city, it's a beach\")\n+        self.assertEqual(generated_text, \"Question: which city is this? Answer: it's not a city, it's a beach\")\n \n     def test_inference_interpolate_pos_encoding(self):\n         processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n@@ -1905,9 +2113,9 @@ def test_inference_opt_multi_accelerator(self):\n         # Test output\n         self.assertEqual(\n             predictions[0].tolist(),\n-            [2, 24, 18, 45, 10, 343, 6, 24, 18, 10, 4105, 50118],\n+            [2, 45641, 35, 61, 343, 16, 42, 116, 31652, 35, 24, 18, 45, 10, 343, 6, 24, 18, 10, 4105, 50118],\n         )\n-        self.assertEqual(generated_text, \"it's not a city, it's a beach\")\n+        self.assertEqual(generated_text, \"Question: which city is this? Answer: it's not a city, it's a beach\")\n \n     @require_torch_multi_accelerator\n     def test_inference_t5_multi_accelerator(self):"
        },
        {
            "sha": "f06caeb03778ee9f4b534c7d30d6e782511fa5d4",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 218,
            "deletions": 5,
            "changes": 223,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6",
            "patch": "@@ -19,7 +19,9 @@\n import unittest\n \n import numpy as np\n+import pytest\n import requests\n+from parameterized import parameterized\n \n from transformers import (\n     CONFIG_MAPPING,\n@@ -320,7 +322,7 @@ def __init__(\n         hidden_act=\"gelu\",\n         hidden_dropout_prob=0.1,\n         attention_probs_dropout_prob=0.1,\n-        max_position_embeddings=20,\n+        max_position_embeddings=100,\n         eos_token_id=2,\n         pad_token_id=1,\n         bos_token_id=0,\n@@ -384,7 +386,14 @@ def get_config(self):\n # this model tester uses a decoder-only language model (OPT)\n class InstructBlipForConditionalGenerationDecoderOnlyModelTester:\n     def __init__(\n-        self, parent, vision_kwargs=None, qformer_kwargs=None, text_kwargs=None, is_training=True, num_query_tokens=10\n+        self,\n+        parent,\n+        vision_kwargs=None,\n+        qformer_kwargs=None,\n+        text_kwargs=None,\n+        is_training=True,\n+        num_query_tokens=10,\n+        image_token_index=4,\n     ):\n         if vision_kwargs is None:\n             vision_kwargs = {}\n@@ -398,16 +407,25 @@ def __init__(\n         self.qformer_model_tester = InstructBlipQFormerModelTester(parent, **qformer_kwargs)\n         self.text_model_tester = InstructBlipTextModelDecoderOnlyTester(parent, **text_kwargs)\n         self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n-        self.seq_length = self.text_model_tester.seq_length  # need seq_length for common tests\n+        self.seq_length = self.text_model_tester.seq_length + num_query_tokens  # need seq_length for common tests\n         self.is_training = is_training\n         self.num_query_tokens = num_query_tokens\n+        self.image_token_index = image_token_index\n \n     def prepare_config_and_inputs(self):\n         _, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n         _, _, _, qformer_input_ids, qformer_attention_mask = self.qformer_model_tester.prepare_config_and_inputs()\n         _, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n \n         config = self.get_config()\n+        vision_tokens = (\n+            torch.ones((input_ids.shape[0], self.num_query_tokens), device=torch_device, dtype=input_ids.dtype)\n+            * self.image_token_index\n+        )\n+        input_ids[input_ids == self.image_token_index] = self.text_model_tester.pad_token_id\n+        input_ids = torch.cat([vision_tokens, input_ids], dim=-1)\n+        vision_attention_mask = torch.ones_like(vision_tokens)\n+        attention_mask = torch.cat([vision_attention_mask, attention_mask], dim=-1)\n \n         return config, input_ids, attention_mask, qformer_input_ids, qformer_attention_mask, pixel_values\n \n@@ -417,6 +435,7 @@ def get_config(self):\n             qformer_config=self.qformer_model_tester.get_config(),\n             text_config=self.text_model_tester.get_config(),\n             num_query_tokens=self.num_query_tokens,\n+            image_token_index=self.image_token_index,\n         )\n \n     def create_and_check_for_conditional_generation(\n@@ -455,6 +474,7 @@ def prepare_config_and_inputs_for_common(self):\n @require_torch\n class InstructBlipForConditionalGenerationDecoderOnlyTest(ModelTesterMixin, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (InstructBlipForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (InstructBlipForConditionalGeneration,) if is_torch_available() else ()\n     pipeline_model_mapping = {\"image-text-to-text\": InstructBlipForConditionalGeneration}\n     fx_compatible = False\n     test_head_masking = False\n@@ -532,6 +552,199 @@ def test_model_from_pretrained(self):\n         model = InstructBlipForConditionalGeneration.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    # overwrite because InstructBLIP internally calls LM.generate() with embeds thus it cannot operate in no cache format\n+    def _check_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n+        use_cache = True  # force this to be True in case False is passed\n+\n+        input_batch_size = int(output.sequences.shape[0] / num_return_sequences)\n+        internal_batch_size = (\n+            input_batch_size * num_beams if num_beams > 1 else input_batch_size * num_return_sequences\n+        )\n+\n+        seq_length = getattr(self.model_tester, \"seq_length\", None)\n+        seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_length)\n+        seq_length = getattr(self.model_tester, \"text_seq_length\", seq_length)\n+\n+        config = config.text_config if hasattr(config, \"text_config\") else config\n+\n+        gen_len = (\n+            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n+        )\n+\n+        # in some models we subsample the sequence length in inner layers\n+        if hasattr(self.model_tester, \"get_subsampled_output_lengths\"):\n+            seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n+\n+        # scores\n+        self._check_scores(internal_batch_size, output.scores, length=gen_len, config=config)\n+\n+        # unprocessed logits\n+        self._check_logits(internal_batch_size, output.logits, config=config)\n+\n+        # Attentions\n+        if self.has_attentions:\n+            if config.is_encoder_decoder:\n+                # encoder\n+                self._check_encoder_attention_for_generate(\n+                    output.encoder_attentions, input_batch_size, config, seq_length\n+                )\n+                # decoder\n+                self._check_attentions_for_generate(\n+                    internal_batch_size,\n+                    output.decoder_attentions,\n+                    min_length=1,\n+                    max_length=output.sequences.shape[-1],\n+                    config=config,\n+                    use_cache=use_cache,\n+                )\n+            else:\n+                # if use_cache first input is equal to no use_cache, so skip here\n+                attentions = output.attentions if not use_cache else output.attentions[1:]\n+                min_length = seq_length if not use_cache else seq_length + 1\n+                self._check_attentions_for_generate(\n+                    internal_batch_size,\n+                    attentions=attentions,\n+                    min_length=min_length,\n+                    max_length=output.sequences.shape[-1],\n+                    config=config,\n+                    use_cache=use_cache,\n+                )\n+\n+        # Hidden States\n+        if config.is_encoder_decoder:\n+            # encoder\n+            self._check_encoder_hidden_states_for_generate(\n+                output.encoder_hidden_states, input_batch_size, config, seq_length\n+            )\n+\n+            # decoder\n+            self._check_hidden_states_for_generate(\n+                internal_batch_size,\n+                output.decoder_hidden_states,\n+                min_length=1,\n+                max_length=output.sequences.shape[-1],\n+                config=config,\n+                use_cache=use_cache,\n+            )\n+        else:\n+            # if use_cache first input is equal to no use_cache, so skip here\n+            hidden_states = output.hidden_states if not use_cache else output.hidden_states[1:]\n+            min_length = seq_length if not use_cache else seq_length + 1\n+            self._check_hidden_states_for_generate(\n+                internal_batch_size,\n+                hidden_states,\n+                min_length=min_length,\n+                max_length=output.sequences.shape[-1],\n+                config=config,\n+                use_cache=use_cache,\n+            )\n+\n+        # Past Key Value States\n+        if use_cache:\n+            past_key_values = output.past_key_values\n+            past_sequence_length = output.sequences.shape[-1] - 1\n+            self._check_past_key_values_for_generate(\n+                internal_batch_size,\n+                past_key_values,\n+                seq_length=past_sequence_length,\n+                config=config,\n+            )\n+\n+    # overwrite because InstructBLIP cannot generate only from input ids, and requires `pixel` values and `qformer_input_ids` in all cases to be present\n+    @pytest.mark.generate\n+    def test_left_padding_compatibility(self):\n+        # NOTE: left-padding results in small numerical differences. This is expected.\n+        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n+\n+        # First, filter out models that don't support left padding\n+        # - The model must have generative capabilities\n+        if len(self.all_generative_model_classes) == 0:\n+            self.skipTest(reason=\"No generative architecture available for this model.\")\n+\n+        # - The model must support padding\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"This model doesn't support padding.\")\n+\n+        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n+        decoder_only_classes = []\n+        for model_class in self.all_generative_model_classes:\n+            config, _ = self.prepare_config_and_inputs_for_generate()\n+            if config.is_encoder_decoder:\n+                continue\n+            else:\n+                decoder_only_classes.append(model_class)\n+        if len(decoder_only_classes) == 0:\n+            self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n+\n+        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n+        #   added support for it yet. We skip these models for now.\n+        has_encoder_attributes = any(\n+            attr_name\n+            for attr_name in config.to_dict().keys()\n+            if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n+        )\n+        if has_encoder_attributes:\n+            self.skipTest(\n+                reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n+            )\n+\n+        # Then, test left-padding\n+        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n+            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n+            if \"position_ids\" in signature:\n+                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n+                position_ids.masked_fill_(attention_mask == 0, 1)\n+                model_kwargs[\"position_ids\"] = position_ids\n+            if \"cache_position\" in signature:\n+                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n+                model_kwargs[\"cache_position\"] = cache_position\n+            return model_kwargs\n+\n+        for model_class in decoder_only_classes:\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            input_ids = inputs_dict[\"input_ids\"]\n+            attention_mask = inputs_dict.get(\"attention_mask\")\n+            pixel_values = inputs_dict[\"pixel_values\"]\n+            qformer_input_ids = inputs_dict[\"qformer_input_ids\"]\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(input_ids)\n+\n+            model = model_class(config).to(torch_device).eval()\n+            signature = inspect.signature(model.forward).parameters.keys()\n+\n+            # no cache as some models require special cache classes to be init outside forward\n+            model.generation_config.use_cache = False\n+\n+            # Without padding\n+            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n+            next_logits_wo_padding = model(\n+                **model_kwargs, pixel_values=pixel_values, qformer_input_ids=qformer_input_ids\n+            ).logits[:, -1, :]\n+\n+            # With left-padding (length 32)\n+            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n+            pad_token_id = (\n+                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n+            )\n+            pad_size = (input_ids.shape[0], 32)\n+            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n+            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n+            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n+            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n+            next_logits_with_padding = model(\n+                **model_kwargs, pixel_values=pixel_values, qformer_input_ids=qformer_input_ids\n+            ).logits[:, -1, :]\n+\n+            # They should result in very similar logits\n+            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-5))\n+\n+    @unittest.skip(\n+        \"InstructBLIP cannot generate only from input ids, and requires pixel values in all cases to be present\"\n+    )\n+    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n+    def test_generate_from_inputs_embeds(self, _, num_beams):\n+        pass\n+\n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n@@ -632,12 +845,12 @@ def test_inference_vicuna_7b(self):\n         outputs = model.generate(**inputs, max_new_tokens=30)\n         generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n \n-        expected_outputs = [2, 450, 22910, 9565, 310, 445, 1967, 338, 393, 263, 767, 338, 13977, 292, 22095, 373, 278, 1250, 310, 263, 13328, 20134, 29963, 1550, 372, 338, 19500, 1623, 263, 19587, 4272]  # fmt: off\n+        expected_outputs = [2, 1724, 338, 22910, 1048, 445, 1967, 29973, 450, 22910, 9565, 310, 445, 1967, 338, 393, 263, 767, 338, 13977, 292, 22095, 373, 278, 1250, 310, 263, 13328, 20134, 29963, 1550, 19500, 373, 263, 19587, 4272, 11952, 29889]  # fmt: off\n \n         self.assertEqual(outputs[0].tolist(), expected_outputs)\n         self.assertEqual(\n             generated_text,\n-            \"The unusual aspect of this image is that a man is ironing clothes on the back of a yellow SUV while it is driving down a busy city\",\n+            \"What is unusual about this image? The unusual aspect of this image is that a man is ironing clothes on the back of a yellow SUV while driving on a busy city street.\",\n         )\n \n     def test_inference_flant5_xl(self):"
        },
        {
            "sha": "7e0bf4eaf0a20db10a12e4935b79f13f7abb94d3",
            "filename": "tests/models/instructblipvideo/test_modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 222,
            "deletions": 5,
            "changes": 227,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblipvideo%2Ftest_modeling_instructblipvideo.py?ref=4cc0813e28c3ea1f0e1257d6079658c5f71d3dc6",
            "patch": "@@ -19,7 +19,9 @@\n import unittest\n \n import numpy as np\n+import pytest\n from huggingface_hub import hf_hub_download\n+from parameterized import parameterized\n \n from transformers import (\n     CONFIG_MAPPING,\n@@ -398,7 +400,14 @@ def get_config(self):\n # this model tester uses a decoder-only language model (OPT)\n class InstructBlipVideoForConditionalGenerationDecoderOnlyModelTester:\n     def __init__(\n-        self, parent, vision_kwargs=None, qformer_kwargs=None, text_kwargs=None, is_training=True, num_query_tokens=10\n+        self,\n+        parent,\n+        vision_kwargs=None,\n+        qformer_kwargs=None,\n+        text_kwargs=None,\n+        is_training=True,\n+        num_query_tokens=10,\n+        video_token_index=4,\n     ):\n         if vision_kwargs is None:\n             vision_kwargs = {}\n@@ -412,17 +421,30 @@ def __init__(\n         self.qformer_model_tester = InstructBlipVideoQFormerModelTester(parent, **qformer_kwargs)\n         self.text_model_tester = InstructBlipVideoTextModelDecoderOnlyTester(parent, **text_kwargs)\n         self.batch_size = self.text_model_tester.batch_size  # need bs for batching_equivalence test\n-        self.seq_length = self.text_model_tester.seq_length  # need seq_length for common tests\n+        self.frames = self.vision_model_tester.frames\n+        # need seq_length for common tests\n+        self.seq_length = self.text_model_tester.seq_length + (num_query_tokens * self.frames)\n         self.is_training = is_training\n         self.num_query_tokens = num_query_tokens\n+        self.video_token_index = video_token_index\n \n     def prepare_config_and_inputs(self):\n         _, pixel_values = self.vision_model_tester.prepare_config_and_inputs()\n         _, _, _, qformer_input_ids, qformer_attention_mask = self.qformer_model_tester.prepare_config_and_inputs()\n         _, input_ids, attention_mask = self.text_model_tester.prepare_config_and_inputs()\n-        frames = self.vision_model_tester.frames\n         _, c, h, w = pixel_values.shape\n-        pixel_values = pixel_values.reshape(-1, frames, c, h, w)\n+        pixel_values = pixel_values.reshape(-1, self.frames, c, h, w)\n+\n+        vision_tokens = (\n+            torch.ones(\n+                (input_ids.shape[0], self.num_query_tokens * self.frames), device=torch_device, dtype=input_ids.dtype\n+            )\n+            * self.video_token_index\n+        )\n+        input_ids[input_ids == self.video_token_index] = self.text_model_tester.pad_token_id\n+        input_ids = torch.cat([vision_tokens, input_ids], dim=-1)\n+        vision_attention_mask = torch.ones_like(vision_tokens)\n+        attention_mask = torch.cat([vision_attention_mask, attention_mask], dim=-1)\n \n         config = self.get_config()\n \n@@ -434,6 +456,7 @@ def get_config(self):\n             qformer_config=self.qformer_model_tester.get_config(),\n             text_config=self.text_model_tester.get_config(),\n             num_query_tokens=self.num_query_tokens,\n+            video_token_index=self.video_token_index,\n         )\n \n     def create_and_check_for_conditional_generation(\n@@ -476,6 +499,7 @@ class InstructBlipVideoForConditionalGenerationDecoderOnlyTest(\n     ModelTesterMixin, GenerationTesterMixin, unittest.TestCase\n ):\n     all_model_classes = (InstructBlipVideoForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (InstructBlipVideoForConditionalGeneration,) if is_torch_available() else ()\n     fx_compatible = False\n     test_head_masking = False\n     test_pruning = False\n@@ -552,6 +576,199 @@ def test_model_from_pretrained(self):\n         model = InstructBlipVideoForConditionalGeneration.from_pretrained(model_name)\n         self.assertIsNotNone(model)\n \n+    # overwrite because InstructBLIPVideo internally calls LM.generate() with embeds thus it cannot operate in no cache format\n+    def _check_outputs(self, output, config, use_cache=False, num_return_sequences=1, num_beams=1):\n+        use_cache = True  # force this to be True in case False is passed\n+\n+        input_batch_size = int(output.sequences.shape[0] / num_return_sequences)\n+        internal_batch_size = (\n+            input_batch_size * num_beams if num_beams > 1 else input_batch_size * num_return_sequences\n+        )\n+\n+        seq_length = getattr(self.model_tester, \"seq_length\", None)\n+        seq_length = getattr(self.model_tester, \"encoder_seq_length\", seq_length)\n+        seq_length = getattr(self.model_tester, \"text_seq_length\", seq_length)\n+\n+        config = config.text_config if hasattr(config, \"text_config\") else config\n+\n+        gen_len = (\n+            output.sequences.shape[-1] - 1 if config.is_encoder_decoder else output.sequences.shape[-1] - seq_length\n+        )\n+\n+        # in some models we subsample the sequence length in inner layers\n+        if hasattr(self.model_tester, \"get_subsampled_output_lengths\"):\n+            seq_length = self.model_tester.get_subsampled_output_lengths(seq_length)\n+\n+        # scores\n+        self._check_scores(internal_batch_size, output.scores, length=gen_len, config=config)\n+\n+        # unprocessed logits\n+        self._check_logits(internal_batch_size, output.logits, config=config)\n+\n+        # Attentions\n+        if self.has_attentions:\n+            if config.is_encoder_decoder:\n+                # encoder\n+                self._check_encoder_attention_for_generate(\n+                    output.encoder_attentions, input_batch_size, config, seq_length\n+                )\n+                # decoder\n+                self._check_attentions_for_generate(\n+                    internal_batch_size,\n+                    output.decoder_attentions,\n+                    min_length=1,\n+                    max_length=output.sequences.shape[-1],\n+                    config=config,\n+                    use_cache=use_cache,\n+                )\n+            else:\n+                # if use_cache first input is equal to no use_cache, so skip here\n+                attentions = output.attentions if not use_cache else output.attentions[1:]\n+                min_length = seq_length if not use_cache else seq_length + 1\n+                self._check_attentions_for_generate(\n+                    internal_batch_size,\n+                    attentions=attentions,\n+                    min_length=min_length,\n+                    max_length=output.sequences.shape[-1],\n+                    config=config,\n+                    use_cache=use_cache,\n+                )\n+\n+        # Hidden States\n+        if config.is_encoder_decoder:\n+            # encoder\n+            self._check_encoder_hidden_states_for_generate(\n+                output.encoder_hidden_states, input_batch_size, config, seq_length\n+            )\n+\n+            # decoder\n+            self._check_hidden_states_for_generate(\n+                internal_batch_size,\n+                output.decoder_hidden_states,\n+                min_length=1,\n+                max_length=output.sequences.shape[-1],\n+                config=config,\n+                use_cache=use_cache,\n+            )\n+        else:\n+            # if use_cache first input is equal to no use_cache, so skip here\n+            hidden_states = output.hidden_states if not use_cache else output.hidden_states[1:]\n+            min_length = seq_length if not use_cache else seq_length + 1\n+            self._check_hidden_states_for_generate(\n+                internal_batch_size,\n+                hidden_states,\n+                min_length=min_length,\n+                max_length=output.sequences.shape[-1],\n+                config=config,\n+                use_cache=use_cache,\n+            )\n+\n+        # Past Key Value States\n+        if use_cache:\n+            past_key_values = output.past_key_values\n+            past_sequence_length = output.sequences.shape[-1] - 1\n+            self._check_past_key_values_for_generate(\n+                internal_batch_size,\n+                past_key_values,\n+                seq_length=past_sequence_length,\n+                config=config,\n+            )\n+\n+    # overwrite because InstructBLIPVideo cannot generate only from input ids, and requires `pixel` values and `qformer_input_ids` in all cases to be present\n+    @pytest.mark.generate\n+    def test_left_padding_compatibility(self):\n+        # NOTE: left-padding results in small numerical differences. This is expected.\n+        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n+\n+        # First, filter out models that don't support left padding\n+        # - The model must have generative capabilities\n+        if len(self.all_generative_model_classes) == 0:\n+            self.skipTest(reason=\"No generative architecture available for this model.\")\n+\n+        # - The model must support padding\n+        if not self.has_attentions:\n+            self.skipTest(reason=\"This model doesn't support padding.\")\n+\n+        # - The model must be a decoder-only architecture (encoder-based architectures use right-padding)\n+        decoder_only_classes = []\n+        for model_class in self.all_generative_model_classes:\n+            config, _ = self.prepare_config_and_inputs_for_generate()\n+            if config.is_encoder_decoder:\n+                continue\n+            else:\n+                decoder_only_classes.append(model_class)\n+        if len(decoder_only_classes) == 0:\n+            self.skipTest(reason=\"No decoder-only architecture available for this model.\")\n+\n+        # - Decoder-only architectures derived from encoder-decoder models could support it in theory, but we haven't\n+        #   added support for it yet. We skip these models for now.\n+        has_encoder_attributes = any(\n+            attr_name\n+            for attr_name in config.to_dict().keys()\n+            if attr_name.startswith(\"encoder\") and attr_name != \"encoder_no_repeat_ngram_size\"\n+        )\n+        if has_encoder_attributes:\n+            self.skipTest(\n+                reason=\"The decoder-only derived from encoder-decoder models are not expected to support left-padding.\"\n+            )\n+\n+        # Then, test left-padding\n+        def _prepare_model_kwargs(input_ids, attention_mask, signature):\n+            model_kwargs = {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n+            if \"position_ids\" in signature:\n+                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n+                position_ids.masked_fill_(attention_mask == 0, 1)\n+                model_kwargs[\"position_ids\"] = position_ids\n+            if \"cache_position\" in signature:\n+                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n+                model_kwargs[\"cache_position\"] = cache_position\n+            return model_kwargs\n+\n+        for model_class in decoder_only_classes:\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            input_ids = inputs_dict[\"input_ids\"]\n+            attention_mask = inputs_dict.get(\"attention_mask\")\n+            pixel_values = inputs_dict[\"pixel_values\"]\n+            qformer_input_ids = inputs_dict[\"qformer_input_ids\"]\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(input_ids)\n+\n+            model = model_class(config).to(torch_device).eval()\n+            signature = inspect.signature(model.forward).parameters.keys()\n+\n+            # no cache as some models require special cache classes to be init outside forward\n+            model.generation_config.use_cache = False\n+\n+            # Without padding\n+            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, signature)\n+            next_logits_wo_padding = model(\n+                **model_kwargs, pixel_values=pixel_values, qformer_input_ids=qformer_input_ids\n+            ).logits[:, -1, :]\n+\n+            # With left-padding (length 32)\n+            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n+            pad_token_id = (\n+                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n+            )\n+            pad_size = (input_ids.shape[0], 32)\n+            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n+            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n+            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n+            model_kwargs = _prepare_model_kwargs(padded_input_ids, padded_attention_mask, signature)\n+            next_logits_with_padding = model(\n+                **model_kwargs, pixel_values=pixel_values, qformer_input_ids=qformer_input_ids\n+            ).logits[:, -1, :]\n+\n+            # They should result in very similar logits\n+            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-5))\n+\n+    @unittest.skip(\n+        \"InstructBLIPVideo cannot generate only from input ids, and requires pixel values in all cases to be present\"\n+    )\n+    @parameterized.expand([(\"greedy\", 1), (\"beam search\", 2)])\n+    def test_generate_from_inputs_embeds(self, _, num_beams):\n+        pass\n+\n     @require_torch_sdpa\n     def test_sdpa_can_dispatch_composite_models(self):\n         \"\"\"\n@@ -643,7 +860,7 @@ def test_inference_vicuna_7b(self):\n         generated_text = processor.batch_decode(outputs, skip_special_tokens=True)[0].strip()\n         self.assertEqual(\n             generated_text,\n-            \"a baby girl wearing glasses is reading a book on the bed 1080p\",\n+            \"Explain what is happening in this short video. a baby girl wearing glasses is reading a book on the bed 1080p\",\n         )\n \n     def test_expansion_in_processing(self):"
        }
    ],
    "stats": {
        "total": 767,
        "additions": 671,
        "deletions": 96
    }
}