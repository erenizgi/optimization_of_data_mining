{
    "author": "torotoki",
    "message": "Fix crash when executing MambaCache sample code (#40557)\n\n* Fix the sample code of MambaCache\n\n* Update automatically generated code\n\n* Fix FalconMambaCache documents\n\n* minor doc fixes\n\n---------\n\nCo-authored-by: Joao Gante <joao@huggingface.co>",
    "sha": "d09fdf5e52294f1e34ef981da368174724ff65b5",
    "files": [
        {
            "sha": "90958725b6c4b4f5643e63dc6c58b8a1370d9e20",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/d09fdf5e52294f1e34ef981da368174724ff65b5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d09fdf5e52294f1e34ef981da368174724ff65b5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=d09fdf5e52294f1e34ef981da368174724ff65b5",
            "patch": "@@ -71,7 +71,8 @@ class FalconMambaCache:\n         config (`PretrainedConfig):\n             The configuration file defining the shape-related attributes required to initialize the static cache.\n         max_batch_size (`int`):\n-            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a smaller batch size is used.\n+            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if\n+            a smaller batch size is used.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float16`):\n             The default `dtype` to use when initializing the layer.\n         device (`torch.device` or `str`, *optional*):\n@@ -80,18 +81,19 @@ class FalconMambaCache:\n     Example:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, FalconMambaForCausalLM, FalconMambaCache\n \n-        >>> model = FalconMambaForCausalLM.from_pretrained(\"state-spaces/falcon_mamba-130m-hf\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"state-spaces/falcon_mamba-130m-hf\")\n+        >>> model = FalconMambaForCausalLM.from_pretrained(\"tiiuae/falcon-mamba-7b\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b\")\n \n         >>> inputs = tokenizer(text=\"My name is FalconMamba\", return_tensors=\"pt\")\n \n         >>> # Prepare a cache class and pass it to model's forward\n-        >>> past_key_values = FalconMambaCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values\n-        FalconMambaCache()\n+        >>> cache_params = FalconMambaCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n+        >>> cache_position = torch.arange(len(inputs[\"input_ids\"][0]), device=model.device)  # sequence length\n+        >>> outputs = model(**inputs, cache_params=cache_params, cache_position=cache_position, use_cache=True)\n+        >>> outputs.cache_params\n         ```\n     \"\"\"\n "
        },
        {
            "sha": "f000f3f8271cc78f6df2c04c704450bb5b1e35dd",
            "filename": "src/transformers/models/falcon_mamba/modular_falcon_mamba.py",
            "status": "modified",
            "additions": 33,
            "deletions": 0,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/d09fdf5e52294f1e34ef981da368174724ff65b5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d09fdf5e52294f1e34ef981da368174724ff65b5/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodular_falcon_mamba.py?ref=d09fdf5e52294f1e34ef981da368174724ff65b5",
            "patch": "@@ -201,6 +201,39 @@ def __init__(\n \n \n class FalconMambaCache(MambaCache):\n+    \"\"\"\n+    Cache for falcon_mamba model which does not have attention mechanism and key value states.\n+\n+    Arguments:\n+        config (`PretrainedConfig):\n+            The configuration file defining the shape-related attributes required to initialize the static cache.\n+        max_batch_size (`int`):\n+            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if\n+            a smaller batch size is used.\n+        dtype (`torch.dtype`, *optional*, defaults to `torch.float16`):\n+            The default `dtype` to use when initializing the layer.\n+        device (`torch.device` or `str`, *optional*):\n+            The device on which the cache should be initialized. Should be the same as the layer.\n+\n+    Example:\n+\n+        ```python\n+        >>> import torch\n+        >>> from transformers import AutoTokenizer, FalconMambaForCausalLM, FalconMambaCache\n+\n+        >>> model = FalconMambaForCausalLM.from_pretrained(\"tiiuae/falcon-mamba-7b\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"tiiuae/falcon-mamba-7b\")\n+\n+        >>> inputs = tokenizer(text=\"My name is FalconMamba\", return_tensors=\"pt\")\n+\n+        >>> # Prepare a cache class and pass it to model's forward\n+        >>> cache_params = FalconMambaCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n+        >>> cache_position = torch.arange(len(inputs[\"input_ids\"][0]), device=model.device)  # sequence length\n+        >>> outputs = model(**inputs, cache_params=cache_params, cache_position=cache_position, use_cache=True)\n+        >>> outputs.cache_params\n+        ```\n+    \"\"\"\n+\n     pass\n \n "
        },
        {
            "sha": "45a02bdd9505f07ee8e68e96f511c87615d0d8b5",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d09fdf5e52294f1e34ef981da368174724ff65b5/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d09fdf5e52294f1e34ef981da368174724ff65b5/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=d09fdf5e52294f1e34ef981da368174724ff65b5",
            "patch": "@@ -64,7 +64,8 @@ class MambaCache:\n         config (`PretrainedConfig):\n             The configuration file defining the shape-related attributes required to initialize the static cache.\n         max_batch_size (`int`):\n-            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if a smaller batch size is used.\n+            The maximum batch size with which the model will be used. Note that a new instance must be instantiated if\n+            a smaller batch size is used.\n         dtype (`torch.dtype`, *optional*, defaults to `torch.float16`):\n             The default `dtype` to use when initializing the layer.\n         device (`torch.device` or `str`, *optional*):\n@@ -73,6 +74,7 @@ class MambaCache:\n     Example:\n \n         ```python\n+        >>> import torch\n         >>> from transformers import AutoTokenizer, MambaForCausalLM, MambaCache\n \n         >>> model = MambaForCausalLM.from_pretrained(\"state-spaces/mamba-130m-hf\")\n@@ -81,10 +83,10 @@ class MambaCache:\n         >>> inputs = tokenizer(text=\"My name is Mamba\", return_tensors=\"pt\")\n \n         >>> # Prepare a cache class and pass it to model's forward\n-        >>> past_key_values = MambaCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n-        >>> outputs = model(**inputs, past_key_values=past_key_values, use_cache=True)\n-        >>> outputs.past_key_values\n-        MambaCache()\n+        >>> cache_params = MambaCache(config=model.config, max_batch_size=1, device=model.device, dtype=model.dtype)\n+        >>> cache_position = torch.arange(len(inputs[\"input_ids\"][0]), device=model.device)  # sequence length\n+        >>> outputs = model(**inputs, cache_params=cache_params, cache_position=cache_position, use_cache=True)\n+        >>> outputs.cache_params\n         ```\n     \"\"\"\n "
        }
    ],
    "stats": {
        "total": 61,
        "additions": 49,
        "deletions": 12
    }
}