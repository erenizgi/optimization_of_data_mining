{
    "author": "remi-or",
    "message": "Modernbert fixes (#38912)\n\n* Removed deprecated argument in modernbert RotaryEmbedding\n\n* Skip test_sdpa_can_dispatch_on_flash for modernbert\n\n---------\n\nCo-authored-by: ivarflakstad <69173633+ivarflakstad@users.noreply.github.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "9bcdd5cde9411477cba66bc9e6d1c59e80b60b60",
    "files": [
        {
            "sha": "05fb1af62b2c04a67286b9a20e33cc29c0f180cf",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bcdd5cde9411477cba66bc9e6d1c59e80b60b60/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bcdd5cde9411477cba66bc9e6d1c59e80b60b60/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=9bcdd5cde9411477cba66bc9e6d1c59e80b60b60",
            "patch": "@@ -154,7 +154,7 @@ def __init__(\n             up to max_seqlen. If the max_seqlen, device, or dtype during training/inference differ,\n             the cos_sin_cache will be recomputed during the forward pass.\n         \"\"\"\n-        super().__init__(dim=dim, base=base, pos_idx_in_fp32=True, device=device, interleaved=False)\n+        super().__init__(dim=dim, base=base, device=device, interleaved=False)\n         self.max_seqlen = max_seqlen\n \n         if max_seqlen is not None and device is not None and dtype is not None:"
        },
        {
            "sha": "a707c659fbf4f20d2b2fe2487207f5f7dfe96f8e",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bcdd5cde9411477cba66bc9e6d1c59e80b60b60/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bcdd5cde9411477cba66bc9e6d1c59e80b60b60/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=9bcdd5cde9411477cba66bc9e6d1c59e80b60b60",
            "patch": "@@ -417,7 +417,7 @@ def __init__(\n             up to max_seqlen. If the max_seqlen, device, or dtype during training/inference differ,\n             the cos_sin_cache will be recomputed during the forward pass.\n         \"\"\"\n-        super().__init__(dim=dim, base=base, pos_idx_in_fp32=True, device=device, interleaved=False)\n+        super().__init__(dim=dim, base=base, device=device, interleaved=False)\n         self.max_seqlen = max_seqlen\n \n         if max_seqlen is not None and device is not None and dtype is not None:"
        },
        {
            "sha": "4e2555b57ef10e00139c74e68f9d6bca7076b06b",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/9bcdd5cde9411477cba66bc9e6d1c59e80b60b60/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9bcdd5cde9411477cba66bc9e6d1c59e80b60b60/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=9bcdd5cde9411477cba66bc9e6d1c59e80b60b60",
            "patch": "@@ -3795,6 +3795,10 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 self.skipTest(\n                     \"PaliGemma-like models currently (transformers==4.41.0) requires an attention_mask input\"\n                 )\n+            if config.model_type in [\"modernbert\"]:\n+                self.skipTest(\n+                    reason=\"ModernBert currently (transformers==4.52.0) automatically adds an attention_mask input\"\n+                )\n             if config.model_type in [\"idefics\", \"idefics2\", \"idefics3\"]:\n                 self.skipTest(reason=\"Idefics currently (transformers==4.39.1) requires an image_attention_mask input\")\n             if config.model_type in [\"sam\"]:"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 6,
        "deletions": 2
    }
}