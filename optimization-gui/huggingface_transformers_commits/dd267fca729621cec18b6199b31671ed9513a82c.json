{
    "author": "junejae",
    "message": "Add T5 GGUF loading support (#33389)\n\n* add: GGUFT5Converter\r\n\r\n* add: tensormapping for t5\r\n\r\n* add: test code for t5\r\n\r\n* fix: Remove whitespace from blank line\r\n\r\n* add: t5 fp16 tests\r\n\r\n* fix: whitespace formatting\r\n\r\n* fix: minor formatting\r\n\r\n* fix: testing every weights",
    "sha": "dd267fca729621cec18b6199b31671ed9513a82c",
    "files": [
        {
            "sha": "20531b990bc341d1d9374932cf2e0d89361a5ec9",
            "filename": "docs/source/en/gguf.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd267fca729621cec18b6199b31671ed9513a82c/docs%2Fsource%2Fen%2Fgguf.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd267fca729621cec18b6199b31671ed9513a82c/docs%2Fsource%2Fen%2Fgguf.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgguf.md?ref=dd267fca729621cec18b6199b31671ed9513a82c",
            "patch": "@@ -85,6 +85,7 @@ For now the supported model architectures are the architectures that have been v\n - StableLM\n - GPT2\n - Starcoder2\n+- T5\n \n ## Example usage\n "
        },
        {
            "sha": "4a2740fcb30e1c597d40d2cc29fa461f79f980ce",
            "filename": "src/transformers/integrations/ggml.py",
            "status": "modified",
            "additions": 125,
            "deletions": 3,
            "changes": 128,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd267fca729621cec18b6199b31671ed9513a82c/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd267fca729621cec18b6199b31671ed9513a82c/src%2Ftransformers%2Fintegrations%2Fggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fggml.py?ref=dd267fca729621cec18b6199b31671ed9513a82c",
            "patch": "@@ -21,11 +21,11 @@\n from array import array\n \n import numpy as np\n-from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers\n-from tokenizers.models import BPE\n+from tokenizers import Tokenizer, decoders, normalizers, pre_tokenizers, processors\n+from tokenizers.models import BPE, Unigram\n \n from .. import AddedToken\n-from ..convert_slow_tokenizer import GPT2Converter, LlamaConverter, Qwen2Converter\n+from ..convert_slow_tokenizer import GPT2Converter, LlamaConverter, Qwen2Converter, T5Converter\n from ..utils import logging\n from ..utils.logging import tqdm\n \n@@ -148,6 +148,51 @@\n         \".output.\": \".lm_head.\",\n         \"output_norm\": \"ln_f\",\n     },\n+    \"t5\": {\n+        \"token_embd\": \"shared\",\n+        \"dec.blk.{bid}.attn_q\": \"decoder.block.{bid}.layer.0.SelfAttention.q\",\n+        \"dec.blk.{bid}.attn_k\": \"decoder.block.{bid}.layer.0.SelfAttention.k\",\n+        \"dec.blk.{bid}.attn_v\": \"decoder.block.{bid}.layer.0.SelfAttention.v\",\n+        \"dec.blk.{bid}.attn_o\": \"decoder.block.{bid}.layer.0.SelfAttention.o\",\n+        \"dec.blk.{bid}.attn_rel_b\": \"decoder.block.{bid}.layer.0.SelfAttention.relative_attention_bias\",\n+        \"dec.blk.{bid}.attn_norm\": \"decoder.block.{bid}.layer.0.layer_norm\",\n+        \"dec.blk.{bid}.cross_attn_q\": \"decoder.block.{bid}.layer.1.EncDecAttention.q\",\n+        \"dec.blk.{bid}.cross_attn_k\": \"decoder.block.{bid}.layer.1.EncDecAttention.k\",\n+        \"dec.blk.{bid}.cross_attn_v\": \"decoder.block.{bid}.layer.1.EncDecAttention.v\",\n+        \"dec.blk.{bid}.cross_attn_o\": \"decoder.block.{bid}.layer.1.EncDecAttention.o\",\n+        \"dec.blk.{bid}.cross_attn_norm\": \"decoder.block.{bid}.layer.1.layer_norm\",\n+        \"dec.blk.{bid}.ffn_gate\": \"decoder.block.{bid}.layer.2.DenseReluDense.wi_0\",\n+        \"dec.blk.{bid}.ffn_up\": \"decoder.block.{bid}.layer.2.DenseReluDense.wi_1\",\n+        \"dec.blk.{bid}.ffn_down\": \"decoder.block.{bid}.layer.2.DenseReluDense.wo\",\n+        \"dec.blk.{bid}.ffn_norm\": \"decoder.block.{bid}.layer.2.layer_norm\",\n+        \"dec.output_norm\": \"decoder.final_layer_norm\",\n+        \"enc.blk.{bid}.attn_q\": \"encoder.block.{bid}.layer.0.SelfAttention.q\",\n+        \"enc.blk.{bid}.attn_k\": \"encoder.block.{bid}.layer.0.SelfAttention.k\",\n+        \"enc.blk.{bid}.attn_v\": \"encoder.block.{bid}.layer.0.SelfAttention.v\",\n+        \"enc.blk.{bid}.attn_o\": \"encoder.block.{bid}.layer.0.SelfAttention.o\",\n+        \"enc.blk.{bid}.attn_rel_b\": \"encoder.block.{bid}.layer.0.SelfAttention.relative_attention_bias\",\n+        \"enc.blk.{bid}.attn_norm\": \"encoder.block.{bid}.layer.0.layer_norm\",\n+        \"enc.blk.{bid}.ffn_gate\": \"encoder.block.{bid}.layer.1.DenseReluDense.wi_0\",\n+        \"enc.blk.{bid}.ffn_up\": \"encoder.block.{bid}.layer.1.DenseReluDense.wi_1\",\n+        \"enc.blk.{bid}.ffn_down\": \"encoder.block.{bid}.layer.1.DenseReluDense.wo\",\n+        \"enc.blk.{bid}.ffn_norm\": \"encoder.block.{bid}.layer.1.layer_norm\",\n+        \"enc.output_norm\": \"encoder.final_layer_norm\",\n+        \"output.weight\": \"lm_head.weight\",\n+    },\n+    \"t5encoder\": {\n+        \"token_embd\": \"shared\",\n+        \"enc.blk.{bid}.attn_q\": \"encoder.block.{bid}.layer.0.SelfAttention.q\",\n+        \"enc.blk.{bid}.attn_k\": \"encoder.block.{bid}.layer.0.SelfAttention.k\",\n+        \"enc.blk.{bid}.attn_v\": \"encoder.block.{bid}.layer.0.SelfAttention.v\",\n+        \"enc.blk.{bid}.attn_o\": \"encoder.block.{bid}.layer.0.SelfAttention.o\",\n+        \"enc.blk.{bid}.attn_rel_b\": \"encoder.block.{bid}.layer.0.SelfAttention.relative_attention_bias\",\n+        \"enc.blk.{bid}.attn_norm\": \"encoder.block.{bid}.layer.0.layer_norm\",\n+        \"enc.blk.{bid}.ffn_gate\": \"encoder.block.{bid}.layer.1.DenseReluDense.wi_0\",\n+        \"enc.blk.{bid}.ffn_up\": \"encoder.block.{bid}.layer.1.DenseReluDense.wi_1\",\n+        \"enc.blk.{bid}.ffn_down\": \"encoder.block.{bid}.layer.1.DenseReluDense.wo\",\n+        \"enc.blk.{bid}.ffn_norm\": \"encoder.block.{bid}.layer.1.layer_norm\",\n+        \"enc.output_norm\": \"encoder.final_layer_norm\",\n+    },\n     \"stablelm\": {\n         \"token_embd\": \"model.embed_tokens\",\n         \"blk\": \"model.layers\",\n@@ -287,6 +332,19 @@\n         \"vocab_size\": \"vocab_size\",\n         \"attention.layer_norm_epsilon\": \"layer_norm_epsilon\",\n     },\n+    \"t5\": {\n+        \"context_length\": \"n_positions\",\n+        \"block_count\": \"num_layers\",\n+        \"feed_forward_length\": \"d_ff\",\n+        \"embedding_length\": \"d_model\",\n+        \"attention.key_length\": \"d_kv\",\n+        \"attention.head_count\": \"num_heads\",\n+        \"attention.head_count_kv\": \"num_key_value_heads\",\n+        \"attention.layer_norm_epsilon\": \"layer_norm_epsilon\",\n+        \"attention.relative_buckets_count\": \"relative_attention_num_buckets\",\n+        \"decoder_start_token_id\": \"decoder_start_token_id\",\n+        \"vocab_size\": \"vocab_size\",\n+    },\n     \"stablelm\": {\n         \"context_length\": \"max_position_embeddings\",\n         \"block_count\": \"num_hidden_layers\",\n@@ -636,6 +694,69 @@ def converted(self) -> Tokenizer:\n         return tokenizer\n \n \n+class GGUFT5Converter(T5Converter):\n+    def __init__(self, tokenizer_dict):\n+        # set dummy data to avoid unnecessary merges calculation\n+        tokenizer_dict[\"merges\"] = [\"dummy text\"]\n+\n+        self.proto = GGUFTokenizerSkeleton(tokenizer_dict)\n+        self.token2id = {k: v for v, k in enumerate(self.proto.tokens)}\n+        self.original_tokenizer = self.proto\n+        self.additional_kwargs = {}\n+\n+    def vocab(self, proto):\n+        return list(zip(proto.tokens, proto.scores))\n+\n+    def normalizer(self, proto):\n+        if getattr(self.original_tokenizer, \"legacy\", True):\n+            sequence = []\n+            if getattr(self.original_tokenizer, \"add_prefix_space\", True):\n+                sequence += [normalizers.Prepend(prepend=\"▁\")]\n+            sequence += [normalizers.Replace(pattern=\" \", content=\"▁\")]\n+            return normalizers.Sequence(sequence)\n+        return None  # non-legacy, no normalizer\n+\n+    def post_processor(self):\n+        return processors.TemplateProcessing(\n+            single=[\"$A\", \"</s>\"],\n+            pair=[\"$A\", \"</s>\", \"$B\", \"</s>\"],\n+            special_tokens=[\n+                (\"</s>\", self.token2id[\"</s>\"]),\n+            ],\n+        )\n+\n+    def converted(self) -> Tokenizer:\n+        vocab_scores = self.vocab(self.proto)\n+        tokenizer = Tokenizer(\n+            Unigram(\n+                vocab_scores,\n+                unk_id=self.proto.unk_token_id,\n+                byte_fallback=False,\n+            )\n+        )\n+\n+        # Tokenizer assemble\n+        normalizer = self.normalizer(self.proto)\n+        if normalizer is not None:\n+            tokenizer.normalizer = normalizer\n+\n+        replacement = \"▁\"\n+        add_prefix_space = True\n+        if hasattr(self.original_tokenizer, \"add_prefix_space\"):\n+            add_prefix_space = self.original_tokenizer.add_prefix_space\n+\n+        pre_tokenizer = self.pre_tokenizer(replacement, add_prefix_space)\n+        if pre_tokenizer is not None:\n+            tokenizer.pre_tokenizer = pre_tokenizer\n+\n+        tokenizer.decoder = self.decoder(replacement, add_prefix_space)\n+        post_processor = self.post_processor()\n+        if post_processor:\n+            tokenizer.post_processor = post_processor\n+\n+        return tokenizer\n+\n+\n GGUF_TO_FAST_CONVERTERS = {\n     \"llama\": GGUFLlamaConverter,\n     \"qwen2\": GGUFQwen2Converter,\n@@ -646,6 +767,7 @@ def converted(self) -> Tokenizer:\n     \"stablelm\": GGUFGPTConverter,\n     \"gpt2\": GGUFGPTConverter,\n     \"starcoder2\": GGUFGPTConverter,\n+    \"t5\": GGUFT5Converter,\n }\n \n "
        },
        {
            "sha": "171b2f4d15b122eb00867341529d5884827f9793",
            "filename": "src/transformers/modeling_gguf_pytorch_utils.py",
            "status": "modified",
            "additions": 15,
            "deletions": 2,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd267fca729621cec18b6199b31671ed9513a82c/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd267fca729621cec18b6199b31671ed9513a82c/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_gguf_pytorch_utils.py?ref=dd267fca729621cec18b6199b31671ed9513a82c",
            "patch": "@@ -94,6 +94,12 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n     # to add this patch to ensure things work correctly on our side.\n     if \"llama\" in architecture and \"mistral\" in model_name:\n         updated_architecture = \"mistral\"\n+    # FIXME: Currnetly this implementation is only for flan-t5 architecture.\n+    # It needs to be developed for supporting legacy t5.\n+    elif \"t5\" in architecture or \"t5encoder\" in architecture:\n+        parsed_parameters[\"config\"][\"tie_word_embeddings\"] = False\n+        parsed_parameters[\"config\"][\"is_gated_act\"] = True\n+        updated_architecture = \"t5\"\n     else:\n         updated_architecture = architecture\n \n@@ -191,6 +197,13 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n                 else:\n                     weights = reverse_reshape_bias(weights, num_heads, n_embed)\n \n+            bid = None\n+            if architecture in (\"t5\", \"t5encoder\"):\n+                for chunk in name.split(\".\"):\n+                    if chunk.isdigit():\n+                        bid = int(chunk)\n+                        break\n+\n             if architecture == \"gpt2\":\n                 if (\n                     \"attn_qkv.weight\" in name\n@@ -209,8 +222,8 @@ def load_gguf_checkpoint(gguf_checkpoint_path, return_tensors=False):\n                     continue\n \n             for tensor_name in tensor_key_mapping:\n-                if tensor_name in name:\n-                    name = name.replace(tensor_name, tensor_key_mapping[tensor_name])\n+                if tensor_name.format(bid=bid) in name:\n+                    name = name.replace(tensor_name.format(bid=bid), tensor_key_mapping[tensor_name].format(bid=bid))\n \n             # Use copy to avoid errors with numpy and pytorch\n             parsed_parameters[\"tensors\"][name] = torch.from_numpy(np.copy(weights))"
        },
        {
            "sha": "4c3fa95055963774c350f4897f199cdb1724f8d2",
            "filename": "src/transformers/models/t5/tokenization_t5_fast.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd267fca729621cec18b6199b31671ed9513a82c/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd267fca729621cec18b6199b31671ed9513a82c/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Ftokenization_t5_fast.py?ref=dd267fca729621cec18b6199b31671ed9513a82c",
            "patch": "@@ -117,7 +117,7 @@ def __init__(\n             kwargs[\"from_slow\"] = True\n \n         super().__init__(\n-            vocab_file,\n+            vocab_file=vocab_file,\n             tokenizer_file=tokenizer_file,\n             eos_token=eos_token,\n             unk_token=unk_token,"
        },
        {
            "sha": "ddc791e96a648996af9c676983de63922646231d",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 55,
            "deletions": 1,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/dd267fca729621cec18b6199b31671ed9513a82c/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dd267fca729621cec18b6199b31671ed9513a82c/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=dd267fca729621cec18b6199b31671ed9513a82c",
            "patch": "@@ -15,7 +15,7 @@\n import tempfile\n import unittest\n \n-from transformers import AddedToken, AutoModelForCausalLM, AutoTokenizer\n+from transformers import AddedToken, AutoModelForCausalLM, AutoModelForSeq2SeqLM, AutoTokenizer\n from transformers.testing_utils import (\n     require_gguf,\n     require_torch_gpu,\n@@ -48,6 +48,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     falcon7b_model_id = \"xaviviro/falcon-7b-quantized-gguf\"\n     falcon40b_model_id = \"maddes8cht/tiiuae-falcon-40b-gguf\"\n     original_flacon7b_model_id = \"tiiuae/falcon-7b\"\n+    t5_model_id = \"repetitio/flan-t5-small\"\n+    original_t5_model_id = \"google/flan-t5-small\"\n     stablelm_model_id = \"afrideva/stablelm-3b-4e1t-GGUF\"\n     stablelm2_model_id = \"afrideva/stablelm-2-1_6b-GGUF\"\n     original_stablelm2_model_id = \"stabilityai/stablelm-2-1_6b\"\n@@ -92,6 +94,8 @@ class GgufIntegrationTests(unittest.TestCase):\n     q2_k_falcon7b_model_id = \"falcon-7b-q2_k.gguf\"\n     fp16_falcon7b_model_id = \"falcon-7b-fp16.gguf\"\n     q2_k_falcon40b_model_id = \"tiiuae-falcon-40b-Q2_K.gguf\"\n+    fp16_t5_model_id = \"flan-t5-small-f16.gguf\"\n+    q8_0_t5_model_id = \"flan-t5-small-q8_0.gguf\"\n     fp16_qwen2moe_model_id = \"Qwen1.5-MoE-A2.7B.gguf\"\n     fp16_gpt2_model_id = \"gpt2.f16.gguf\"\n     q8_gpt2_model_id = \"gpt2.Q8_0.gguf\"\n@@ -487,6 +491,56 @@ def test_bloom_weights_conversion_fp16(self):\n                 self.assertTrue(quantized_param.shape == original_param.shape)\n                 torch.testing.assert_close(quantized_param, original_param)\n \n+    def test_t5_f16(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.t5_model_id, gguf_file=self.fp16_t5_model_id)\n+        model = AutoModelForSeq2SeqLM.from_pretrained(\n+            self.t5_model_id, gguf_file=self.fp16_t5_model_id, device_map=\"auto\", torch_dtype=torch.float16\n+        )\n+\n+        T5_EXAMPLE_TEXT = \"translate English to German: How old are you?\"\n+\n+        text = tokenizer(T5_EXAMPLE_TEXT, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Wie ich er?\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_t5_q8_0(self):\n+        tokenizer = AutoTokenizer.from_pretrained(self.t5_model_id, gguf_file=self.q8_0_t5_model_id)\n+        model = AutoModelForSeq2SeqLM.from_pretrained(\n+            self.t5_model_id, gguf_file=self.q8_0_t5_model_id, device_map=\"auto\", torch_dtype=torch.float16\n+        )\n+\n+        T5_EXAMPLE_TEXT = \"translate English to German: How old are you?\"\n+\n+        text = tokenizer(T5_EXAMPLE_TEXT, return_tensors=\"pt\").to(torch_device)\n+        out = model.generate(**text, max_new_tokens=10)\n+\n+        EXPECTED_TEXT = \"Wie ich er?\"\n+        self.assertEqual(tokenizer.decode(out[0], skip_special_tokens=True), EXPECTED_TEXT)\n+\n+    def test_t5_weights_conversion_fp16(self):\n+        quantized_model = AutoModelForSeq2SeqLM.from_pretrained(\n+            self.t5_model_id,\n+            gguf_file=self.fp16_t5_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+        original_model = AutoModelForSeq2SeqLM.from_pretrained(\n+            self.original_t5_model_id,\n+            device_map=\"auto\",\n+            torch_dtype=torch.float16,\n+        )\n+\n+        quantized_state_dict = quantized_model.state_dict()\n+        original_state_dict = original_model.state_dict()\n+\n+        for (quantized_name, quantized_param), (original_name, original_param) in zip(\n+            quantized_state_dict.items(), original_state_dict.items()\n+        ):\n+            self.assertTrue(quantized_param.shape == original_param.shape)\n+            torch.testing.assert_close(quantized_param, original_param, rtol=5e-04, atol=5e-04)\n+\n     def test_gpt2_q8(self):\n         tokenizer = AutoTokenizer.from_pretrained(self.gpt2_model_id, gguf_file=self.q8_gpt2_model_id)\n         model = AutoModelForCausalLM.from_pretrained("
        }
    ],
    "stats": {
        "total": 204,
        "additions": 197,
        "deletions": 7
    }
}