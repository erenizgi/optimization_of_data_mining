{
    "author": "molbap",
    "message": "add test scanner (#39419)\n\n* add test scanner\n\n* add doc + license\n\n* refactor for only 1 tree traversal\n\n* add back test of only one method\n\n* document single method scan\n\n* format\n\n* fixup generate tests\n\n* minor fix\n\n* fixup\n\n* fixup doc",
    "sha": "b9ee5282464acdef0cb275bd05456af8dcfa8a62",
    "files": [
        {
            "sha": "262113575f4207d90fd83bf56a533f7f6f1d0077",
            "filename": "docs/source/en/internal/model_debugging_utils.md",
            "status": "modified",
            "additions": 111,
            "deletions": 0,
            "changes": 111,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9ee5282464acdef0cb275bd05456af8dcfa8a62/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9ee5282464acdef0cb275bd05456af8dcfa8a62/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Finternal%2Fmodel_debugging_utils.md?ref=b9ee5282464acdef0cb275bd05456af8dcfa8a62",
            "patch": "@@ -247,3 +247,114 @@ first and last layer will be shown. This is useful when some layers (typically c\n layers.\n \n [[autodoc]] model_addition_debugger_context\n+\n+## Analyzer of skipped tests\n+\n+### Scan skipped tests - for model adders and maintainers\n+\n+This small util is a power user tool intended for model adders and maintainers. It lists all test methods\n+existing in `test_modeling_common.py`, inherited by all model tester classes, and scans the repository to measure\n+how many tests are being skipped and for which models. \n+\n+### Rationale\n+\n+When porting models to transformers, tests fail as they should, and sometimes `test_modeling_common` feels irreconcilable with the peculiarities of our brand new model. But how can we be sure we're not breaking everything by adding a seemingly innocent skip?\n+\n+This utility:\n+- scans all test_modeling_common methods\n+- looks for times where a method is skipped\n+- returns a summary json you can load as a DataFrame/inspect\n+\n+**For instance test_inputs_embeds is skipped in a whooping 39% proportion at the time of writing this util.**\n+\n+![download-icon](https://huggingface.co/datasets/huggingface/documentation-images/resolve/f7f671f69b88ce4967e19179172c248958d35742/transformers/tests_skipped_visualisation.png)\n+\n+\n+### Usage \n+\n+You can run the skipped test analyzer in two ways:\n+\n+#### Full scan (default)\n+\n+From the root of `transformers` repo, scans all common test methods and outputs the results to a JSON file (default: `all_tests_scan_result.json`).\n+\n+```bash\n+python utils/scan_skipped_tests.py --output_dir path/to/output\n+```\n+\n+- `--output_dir` (optional): Directory where the JSON results will be saved. Defaults to the current directory.\n+\n+**Example output:**\n+\n+```\n+üî¨ Parsing 331 model test files once each...\n+üìù Aggregating 224 tests...\n+  (224/224) test_update_candidate_strategy_with_matches_1es_3d_is_nonecodet_schedule_fa_kwargs\n+‚úÖ Scan complete.\n+\n+üìÑ JSON saved to /home/pablo/git/transformers/all_tests_scan_result.json\n+\n+```\n+\n+And it will generate `all_tests_scan_result.json` file that you can inspect. The JSON is indexed by method name, and each entry follows this schema, indicating the origin as well (from `common`or `GenerationMixin`.)\n+\n+```json\n+{\n+  \"<method_name>\": {\n+    \"origin\": \"<test suite>\"\n+    \"models_ran\": [\"<model_name>\", ...],\n+    \"models_skipped\": [\"<model_name>\", ...],\n+    \"skipped_proportion\": <float>,\n+    \"reasons_skipped\": [\"<model_name>: <reason>\",\n+      ...\n+    ]\n+  },\n+  ...\n+}\n+```\n+\n+Which you can visualise as above with e.g. `pandas`\n+\n+```python\n+df = pd.read_json('all_tests_scan_result.json').T\n+df.sort_values(by=['skipped_proportion'], ascending=False)\n+\n+```\n+\n+### Scan a single test method\n+\n+You can focus on a specific test method using `--test_method_name`:\n+\n+```bash\n+$ python utils/scan_skipped_tests.py --test_method_name test_inputs_embeds --output_dir path/to/output\n+```\n+\n+- `--test_method_name`: Name of the test method to scan (e.g., `test_inputs_embeds`).\n+- `--output_dir` (optional): Directory where the JSON result will be saved.\n+\n+**Example output:**\n+\n+```bash\n+$ python utils/scan_skipped_tests.py --test_method_name test_inputs_embeds\n+\n+üî¨ Parsing 331 model test files once each...\n+\n+== test_inputs_embeds ==\n+\n+Ran    : 199/323\n+Skipped : 124/323 (38.4%)\n+ - aimv2: Aimv2 does not use inputs_embeds\n+ - align: Inputs_embeds is tested in individual model tests\n+ - altclip: Inputs_embeds is tested in individual model tests\n+ - audio_spectrogram_transformer: AST does not use inputs_embeds\n+ - beit: BEiT does not use inputs_embeds\n+ - bit: Bit does not use inputs_embeds\n+ - blip: Blip does not use inputs_embeds\n+ - blip_2: Inputs_embeds is tested in individual model tests\n+ - bridgetower: \n+ - canine: CANINE does not have a get_input_embeddings() method.\n+ - ...\n+\n+üìÑ JSON saved to /home/pablo/git/transformers/scan_test_inputs_embeds.json\n+\n+```\n\\ No newline at end of file"
        },
        {
            "sha": "20d794f733dc20ed3897359dc6175c0986155d18",
            "filename": "utils/scan_skipped_tests.py",
            "status": "added",
            "additions": 199,
            "deletions": 0,
            "changes": 199,
            "blob_url": "https://github.com/huggingface/transformers/blob/b9ee5282464acdef0cb275bd05456af8dcfa8a62/utils%2Fscan_skipped_tests.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b9ee5282464acdef0cb275bd05456af8dcfa8a62/utils%2Fscan_skipped_tests.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fscan_skipped_tests.py?ref=b9ee5282464acdef0cb275bd05456af8dcfa8a62",
            "patch": "@@ -0,0 +1,199 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import argparse\n+import json\n+import re\n+from pathlib import Path\n+\n+\n+REPO_ROOT = Path().cwd()\n+\n+COMMON_TEST_FILES: list[tuple[Path, str]] = [\n+    (REPO_ROOT / \"tests/test_modeling_common.py\", \"common\"),\n+    (REPO_ROOT / \"tests/generation/test_utils.py\", \"GenerationMixin\"),\n+]\n+\n+MODELS_DIR = REPO_ROOT / \"tests/models\"\n+\n+\n+def get_common_tests(file_paths_with_origin: list[tuple[Path, str]]) -> dict[str, str]:\n+    \"\"\"Extract all common test function names (e.g., 'test_forward').\"\"\"\n+    tests_with_origin: dict[str, str] = {}\n+    for file_path, origin_tag in file_paths_with_origin:\n+        if not file_path.is_file():\n+            continue\n+        content = file_path.read_text(encoding=\"utf-8\")\n+        for test_name in re.findall(r\"^\\s*def\\s+(test_[A-Za-z0-9_]+)\", content, re.MULTILINE):\n+            tests_with_origin[test_name] = origin_tag\n+    return tests_with_origin\n+\n+\n+def get_models_and_test_files(models_dir: Path) -> tuple[list[str], list[Path]]:\n+    if not models_dir.is_dir():\n+        raise FileNotFoundError(f\"Models directory not found at {models_dir}\")\n+    test_files: list[Path] = sorted(models_dir.rglob(\"test_modeling_*.py\"))\n+    model_names: list[str] = sorted({file_path.parent.name for file_path in test_files})\n+    return model_names, test_files\n+\n+\n+def _extract_reason_from_decorators(decorators_block: str) -> str:\n+    \"\"\"Extracts the reason string from a decorator block, if any.\"\"\"\n+    reason_match = re.search(r'reason\\s*=\\s*[\"\\'](.*?)[\"\\']', decorators_block)\n+    if reason_match:\n+        return reason_match.group(1)\n+    reason_match = re.search(r'\\((?:.*?,\\s*)?[\"\\'](.*?)[\"\\']\\)', decorators_block)\n+    if reason_match:\n+        return reason_match.group(1)\n+    return decorators_block.strip().split(\"\\n\")[-1].strip()\n+\n+\n+def extract_test_info(file_content: str) -> dict[str, tuple[str, str]]:\n+    \"\"\"\n+    Parse a test file once and return a mapping of test functions to their\n+    status and skip reason, e.g. {'test_forward': ('SKIPPED', 'too slow')}.\n+    \"\"\"\n+    result: dict[str, tuple[str, str]] = {}\n+    pattern = re.compile(r\"((?:^\\s*@.*?\\n)*?)^\\s*def\\s+(test_[A-Za-z0-9_]+)\\b\", re.MULTILINE)\n+    for decorators_block, test_name in pattern.findall(file_content):\n+        if \"skip\" in decorators_block:\n+            result[test_name] = (\"SKIPPED\", _extract_reason_from_decorators(decorators_block))\n+        else:\n+            result[test_name] = (\"RAN\", \"\")\n+    return result\n+\n+\n+def build_model_overrides(model_test_files: list[Path]) -> dict[str, dict[str, tuple[str, str]]]:\n+    \"\"\"Return *model_name ‚Üí {test_name ‚Üí (status, reason)}* mapping.\"\"\"\n+    model_overrides: dict[str, dict[str, tuple[str, str]]] = {}\n+    for file_path in model_test_files:\n+        model_name = file_path.parent.name\n+        file_content = file_path.read_text(encoding=\"utf-8\")\n+        model_overrides.setdefault(model_name, {}).update(extract_test_info(file_content))\n+    return model_overrides\n+\n+\n+def save_json(obj: dict, output_path: Path) -> None:\n+    output_path.parent.mkdir(parents=True, exist_ok=True)\n+    output_path.write_text(json.dumps(obj, indent=2), encoding=\"utf-8\")\n+\n+\n+def summarize_single_test(\n+    test_name: str,\n+    model_names: list[str],\n+    model_overrides: dict[str, dict[str, tuple[str, str]]],\n+) -> dict[str, object]:\n+    \"\"\"Print a concise terminal summary for *test_name* and return the raw data.\"\"\"\n+    models_ran, models_skipped, reasons_for_skipping = [], [], []\n+    for model_name in model_names:\n+        status, reason = model_overrides.get(model_name, {}).get(test_name, (\"RAN\", \"\"))\n+        if status == \"SKIPPED\":\n+            models_skipped.append(model_name)\n+            reasons_for_skipping.append(f\"{model_name}: {reason}\")\n+        else:\n+            models_ran.append(model_name)\n+\n+    total_models = len(model_names)\n+    skipped_ratio = len(models_skipped) / total_models if total_models else 0.0\n+\n+    print(f\"\\n== {test_name} ==\")\n+    print(f\"Ran    : {len(models_ran)}/{total_models}\")\n+    print(f\"Skipped : {len(models_skipped)}/{total_models} ({skipped_ratio:.1%})\")\n+    for reason_entry in reasons_for_skipping[:10]:\n+        print(f\" - {reason_entry}\")\n+    if len(reasons_for_skipping) > 10:\n+        print(\" - ...\")\n+\n+    return {\n+        \"models_ran\": sorted(models_ran),\n+        \"models_skipped\": sorted(models_skipped),\n+        \"skipped_proportion\": round(skipped_ratio, 4),\n+        \"reasons_skipped\": sorted(reasons_for_skipping),\n+    }\n+\n+\n+def summarize_all_tests(\n+    tests_with_origin: dict[str, str],\n+    model_names: list[str],\n+    model_overrides: dict[str, dict[str, tuple[str, str]]],\n+) -> dict[str, object]:\n+    \"\"\"Return aggregated data for every discovered common test.\"\"\"\n+    results: dict[str, object] = {}\n+    total_models = len(model_names)\n+    test_names = list(tests_with_origin)\n+\n+    print(f\"üìù Aggregating {len(test_names)} tests...\")\n+    for index, test_fn in enumerate(test_names, 1):\n+        print(f\"  ({index}/{len(test_names)}) {test_fn}\", end=\"\\r\")\n+        models_ran, models_skipped, reasons_for_skipping = [], [], []\n+        for model_name in model_names:\n+            status, reason = model_overrides.get(model_name, {}).get(test_fn, (\"RAN\", \"\"))\n+            if status == \"SKIPPED\":\n+                models_skipped.append(model_name)\n+                reasons_for_skipping.append(f\"{model_name}: {reason}\")\n+            else:\n+                models_ran.append(model_name)\n+\n+        skipped_ratio = len(models_skipped) / total_models if total_models else 0.0\n+        results[test_fn] = {\n+            \"origin\": tests_with_origin[test_fn],\n+            \"models_ran\": sorted(models_ran),\n+            \"models_skipped\": sorted(models_skipped),\n+            \"skipped_proportion\": round(skipped_ratio, 4),\n+            \"reasons_skipped\": sorted(reasons_for_skipping),\n+        }\n+    print(\"\\n‚úÖ Scan complete.\")\n+    return results\n+\n+\n+def main() -> None:\n+    parser = argparse.ArgumentParser(\n+        description=\"Scan model tests for overridden or skipped common or generat tests.\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        default=\".\",\n+        help=\"Directory for JSON output (default: %(default)s)\",\n+    )\n+    parser.add_argument(\n+        \"--test_method_name\",\n+        help=\"Scan only this test method (single‚Äëtest mode)\",\n+    )\n+    args = parser.parse_args()\n+\n+    output_dir = Path(args.output_dir).expanduser()\n+    test_method_name = args.test_method_name\n+\n+    tests_with_origin = get_common_tests(COMMON_TEST_FILES)\n+    if test_method_name:\n+        tests_with_origin = {test_method_name: tests_with_origin.get(test_method_name, \"unknown\")}\n+\n+    model_names, model_test_files = get_models_and_test_files(MODELS_DIR)\n+    print(f\"üî¨ Parsing {len(model_test_files)} model test files once each...\")\n+    model_overrides = build_model_overrides(model_test_files)\n+\n+    if test_method_name:\n+        data = summarize_single_test(test_method_name, model_names, model_overrides)\n+        json_path = output_dir / f\"scan_{test_method_name}.json\"\n+    else:\n+        data = summarize_all_tests(tests_with_origin, model_names, model_overrides)\n+        json_path = output_dir / \"all_tests_scan_result.json\"\n+    save_json(data, json_path)\n+    print(f\"\\nüìÑ JSON saved to {json_path.resolve()}\")\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        }
    ],
    "stats": {
        "total": 310,
        "additions": 310,
        "deletions": 0
    }
}