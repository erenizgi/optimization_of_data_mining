{
    "author": "qubvel",
    "message": "Fix loading models with mismatched sizes (#36463)\n\n* Fix loading model with mismatched sizes\n\n* trigger tests",
    "sha": "02776d2c6aa997c5b81f28f2edf38df9967253be",
    "files": [
        {
            "sha": "e602d7ca8ccea13be9bc155a8f02ef66a6a4f927",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/02776d2c6aa997c5b81f28f2edf38df9967253be/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/02776d2c6aa997c5b81f28f2edf38df9967253be/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=02776d2c6aa997c5b81f28f2edf38df9967253be",
            "patch": "@@ -4907,7 +4907,9 @@ def _load_pretrained_model(\n                     model_to_load, state_dict, start_prefix\n                 )\n                 # at this point the state dict should be on cpu, we don't need to actually read it\n-                fixed_state_dict = model_to_load._fix_state_dict_keys_on_load(state_dict)\n+                mismatched_names = [name for name, _, _ in mismatched_keys]\n+                fixed_state_dict = {k: v for k, v in state_dict.items() if k not in mismatched_names}\n+                fixed_state_dict = model_to_load._fix_state_dict_keys_on_load(fixed_state_dict)\n                 model_to_load.load_state_dict(fixed_state_dict, strict=False, assign=assign_to_params_buffers)\n         else:\n             # This should always be a list but, just to be sure."
        }
    ],
    "stats": {
        "total": 4,
        "additions": 3,
        "deletions": 1
    }
}