{
    "author": "zucchini-nlp",
    "message": "CI: fix failures (#34371)\n\nfix",
    "sha": "b29c24ff1ed130d717c59b58091cfedb652872d0",
    "files": [
        {
            "sha": "0fe89676b92d63a4cdc4eec622f6439423807395",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/b29c24ff1ed130d717c59b58091cfedb652872d0/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b29c24ff1ed130d717c59b58091cfedb652872d0/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=b29c24ff1ed130d717c59b58091cfedb652872d0",
            "patch": "@@ -561,6 +561,7 @@ def forward(\n             )\n \n         video_features = None\n+        num_frames = 0\n         if pixel_values_videos is not None:\n             video_features, num_frames = self.get_video_features(\n                 pixel_values_videos=pixel_values_videos, vision_feature_layer=vision_feature_layer"
        },
        {
            "sha": "a33be021353f72f8ea3a842da5fcff788bde6fce",
            "filename": "tests/models/instructblip/test_modeling_instructblip.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b29c24ff1ed130d717c59b58091cfedb652872d0/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b29c24ff1ed130d717c59b58091cfedb652872d0/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finstructblip%2Ftest_modeling_instructblip.py?ref=b29c24ff1ed130d717c59b58091cfedb652872d0",
            "patch": "@@ -621,7 +621,7 @@ def test_inference_vicuna_7b(self):\n             logits = model(**inputs).logits\n \n         expected_slice = torch.tensor(\n-            [[-3.3926, -12.2969, 8.4922], [-5.0195, -11.9531, 8.1406], [-4.0039, -13.3594, 9.2578]],\n+            [[-3.3047, -12.0625, 8.4922], [-4.9258, -11.7578, 8.1406], [-3.9297, -13.5000, 9.2500]],\n             device=torch_device,\n         )\n "
        }
    ],
    "stats": {
        "total": 3,
        "additions": 2,
        "deletions": 1
    }
}