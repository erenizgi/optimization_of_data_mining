{
    "author": "ydshieh",
    "message": "Better CI (#38552)\n\nbetter CI\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "5009252a05144f439e76502083c4380c33683054",
    "files": [
        {
            "sha": "fe1f18f42b9916c87b926f837f5f4cf6ec5db0ae",
            "filename": ".github/workflows/build-docker-images.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 39,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/5009252a05144f439e76502083c4380c33683054/.github%2Fworkflows%2Fbuild-docker-images.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5009252a05144f439e76502083c4380c33683054/.github%2Fworkflows%2Fbuild-docker-images.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fbuild-docker-images.yml?ref=5009252a05144f439e76502083c4380c33683054",
            "patch": "@@ -19,7 +19,7 @@ concurrency:\n \n jobs:\n   latest-docker:\n-    name: \"Latest PyTorch + TensorFlow [dev]\"\n+    name: \"Latest PyTorch [dev]\"\n     runs-on:\n       group: aws-general-8-plus\n     steps:\n@@ -267,44 +267,6 @@ jobs:\n           status: ${{ job.status }}\n           slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n \n-  latest-tensorflow:\n-    name: \"Latest TensorFlow [dev]\"\n-    # Push CI doesn't need this image\n-    if: inputs.image_postfix != '-push-ci'\n-    runs-on:\n-      group: aws-general-8-plus\n-    steps:\n-      -\n-        name: Set up Docker Buildx\n-        uses: docker/setup-buildx-action@v3\n-      -\n-        name: Check out code\n-        uses: actions/checkout@v4\n-      -\n-        name: Login to DockerHub\n-        uses: docker/login-action@v3\n-        with:\n-          username: ${{ secrets.DOCKERHUB_USERNAME }}\n-          password: ${{ secrets.DOCKERHUB_PASSWORD }}\n-      -\n-        name: Build and push\n-        uses: docker/build-push-action@v5\n-        with:\n-          context: ./docker/transformers-tensorflow-gpu\n-          build-args: |\n-            REF=main\n-          push: true\n-          tags: huggingface/transformers-tensorflow-gpu\n-\n-      - name: Post to Slack\n-        if: always()\n-        uses: huggingface/hf-workflows/.github/actions/post-slack@main\n-        with:\n-          slack_channel: ${{ secrets.CI_SLACK_CHANNEL_DOCKER }}\n-          title: ðŸ¤— Results of the huggingface/transformers-tensorflow-gpu build\n-          status: ${{ job.status }}\n-          slack_token: ${{ secrets.SLACK_CIFEEDBACK_BOT_TOKEN }}\n-\n   latest-pytorch-deepspeed-amd:\n     name: \"PyTorch + DeepSpeed (AMD) [dev]\"\n     runs-on:"
        },
        {
            "sha": "d6e36e9004462f1003cfecb0bb37f75904859ea6",
            "filename": ".github/workflows/self-scheduled-caller.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/5009252a05144f439e76502083c4380c33683054/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5009252a05144f439e76502083c4380c33683054/.github%2Fworkflows%2Fself-scheduled-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-caller.yml?ref=5009252a05144f439e76502083c4380c33683054",
            "patch": "@@ -69,18 +69,6 @@ jobs:\n       report_repo_id: hf-internal-testing/transformers_daily_ci\n     secrets: inherit\n \n-  tf-pipeline:\n-    name: TF pipeline CI\n-    uses: ./.github/workflows/self-scheduled.yml\n-    with:\n-      job: run_pipelines_tf_gpu\n-      slack_report_channel: \"#transformers-ci-daily-pipeline-tf\"\n-      runner: daily-ci\n-      docker: huggingface/transformers-tensorflow-gpu\n-      ci_event: Daily CI\n-      report_repo_id: hf-internal-testing/transformers_daily_ci\n-    secrets: inherit\n-\n   example-ci:\n     name: Example CI\n     uses: ./.github/workflows/self-scheduled.yml"
        },
        {
            "sha": "5ad51bc008a74266c5b78da66024ccec6999bdaa",
            "filename": ".github/workflows/self-scheduled.yml",
            "status": "modified",
            "additions": 0,
            "deletions": 70,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/5009252a05144f439e76502083c4380c33683054/.github%2Fworkflows%2Fself-scheduled.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5009252a05144f439e76502083c4380c33683054/.github%2Fworkflows%2Fself-scheduled.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled.yml?ref=5009252a05144f439e76502083c4380c33683054",
            "patch": "@@ -209,75 +209,6 @@ jobs:\n           name: ${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports\n           path: /transformers/reports/${{ env.machine_type }}_run_pipelines_torch_gpu_test_reports\n \n-  run_pipelines_tf_gpu:\n-    if: ${{ inputs.job == 'run_pipelines_tf_gpu' }}\n-    name: TensorFlow pipelines\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        machine_type: [aws-g4dn-4xlarge-cache, aws-g4dn-12xlarge-cache]\n-    runs-on:\n-      group: '${{ matrix.machine_type }}'\n-    container:\n-      image: huggingface/transformers-tensorflow-gpu\n-      options: --gpus all --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: Update clone\n-        working-directory: /transformers\n-        run: |\n-          git fetch && git checkout ${{ github.sha }}\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: NVIDIA-SMI\n-        run: |\n-          nvidia-smi\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Set `machine_type` for report and artifact names\n-        working-directory: /transformers\n-        shell: bash\n-        run: |\n-          echo \"${{ matrix.machine_type }}\"\n-\n-          if [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-4xlarge-cache\" ]; then\n-            machine_type=single-gpu\n-          elif [ \"${{ matrix.machine_type }}\" = \"aws-g4dn-12xlarge-cache\" ]; then\n-            machine_type=multi-gpu\n-          else\n-            machine_type=${{ matrix.machine_type }}\n-          fi\n-\n-          echo \"$machine_type\"\n-          echo \"machine_type=$machine_type\" >> $GITHUB_ENV\n-\n-      - name: Run all pipeline tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 1 -v --dist=loadfile --make-reports=${{ env.machine_type }}_run_pipelines_tf_gpu_test_reports tests/pipelines\n-\n-      - name: Failure short reports\n-        if: ${{ always() }}\n-        run: |\n-          cat /transformers/reports/${{ env.machine_type }}_run_pipelines_tf_gpu_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ env.machine_type }}_run_pipelines_tf_gpu_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ env.machine_type }}_run_pipelines_tf_gpu_test_reports\n-          path: /transformers/reports/${{ env.machine_type }}_run_pipelines_tf_gpu_test_reports\n-\n   run_examples_gpu:\n     if: ${{ inputs.job == 'run_examples_gpu' }}\n     name: Examples directory\n@@ -571,7 +502,6 @@ jobs:\n       run_models_gpu,\n       run_trainer_and_fsdp_gpu,\n       run_pipelines_torch_gpu,\n-      run_pipelines_tf_gpu,\n       run_examples_gpu,\n       run_torch_cuda_extensions_gpu,\n       run_quantization_torch_gpu,"
        },
        {
            "sha": "f0b43e23ec3454633bc9e1c5dd184abce301a736",
            "filename": "docker/transformers-all-latest-gpu/Dockerfile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5009252a05144f439e76502083c4380c33683054/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "raw_url": "https://github.com/huggingface/transformers/raw/5009252a05144f439e76502083c4380c33683054/docker%2Ftransformers-all-latest-gpu%2FDockerfile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docker%2Ftransformers-all-latest-gpu%2FDockerfile?ref=5009252a05144f439e76502083c4380c33683054",
            "patch": "@@ -28,7 +28,7 @@ RUN git clone https://github.com/huggingface/transformers && cd transformers &&\n # 1. Put several commands in a single `RUN` to avoid image/layer exporting issue. Could be revised in the future.\n # 2. Regarding `torch` part, We might need to specify proper versions for `torchvision` and `torchaudio`.\n #    Currently, let's not bother to specify their versions explicitly (so installed with their latest release versions).\n-RUN python3 -m pip install --no-cache-dir -U tensorflow==2.13 protobuf==3.20.3 \"tensorflow_text<2.16\" \"tensorflow_probability<0.22\" && python3 -m pip install --no-cache-dir -e ./transformers[dev,onnxruntime] && [ ${#PYTORCH} -gt 0 -a \"$PYTORCH\" != \"pre\" ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch'; echo \"export VERSION='$VERSION'\" >> ~/.profile && echo torch=$VERSION && [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/$CUDA || python3 -m pip install --no-cache-dir -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA\n+RUN python3 -m pip install --no-cache-dir -e ./transformers[dev,onnxruntime] && [ ${#PYTORCH} -gt 0 -a \"$PYTORCH\" != \"pre\" ] && VERSION='torch=='$PYTORCH'.*' ||  VERSION='torch'; echo \"export VERSION='$VERSION'\" >> ~/.profile && echo torch=$VERSION && [ \"$PYTORCH\" != \"pre\" ] && python3 -m pip install --no-cache-dir -U $VERSION torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/$CUDA || python3 -m pip install --no-cache-dir -U --pre torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/nightly/$CUDA && python3 -m pip uninstall -y tensorflow tensorflow_text tensorflow_probability\n \n RUN python3 -m pip uninstall -y flax jax\n "
        }
    ],
    "stats": {
        "total": 124,
        "additions": 2,
        "deletions": 122
    }
}