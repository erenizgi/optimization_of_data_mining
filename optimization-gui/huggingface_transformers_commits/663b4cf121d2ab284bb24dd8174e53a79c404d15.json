{
    "author": "molbap",
    "message": "tiny fix for deepseekocr support [vllm] (#42423)\n\n* tiny fix\n\n* fix usage\n\n* only need the config\n\n* remove that too\n\n* fixup",
    "sha": "663b4cf121d2ab284bb24dd8174e53a79c404d15",
    "files": [
        {
            "sha": "042781485449fcb4691bf1c71847b3b55eb86a25",
            "filename": "src/transformers/models/deepseek_v2/configuration_deepseek_v2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/663b4cf121d2ab284bb24dd8174e53a79c404d15/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/663b4cf121d2ab284bb24dd8174e53a79c404d15/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fconfiguration_deepseek_v2.py?ref=663b4cf121d2ab284bb24dd8174e53a79c404d15",
            "patch": "@@ -101,6 +101,9 @@ class DeepseekV2Config(PreTrainedConfig):\n             Number of selected groups per token for expert selection.\n         topk_method (`str`, *optional*, defaults to `\"greedy\"`):\n             The method used for selecting top-k experts in the routed gate mechanism.\n+        norm_topk_prob (`bool`, *optional*, defaults to `False`):\n+            Whether to renormalize the router probabilities when `top_k > 1`. This flag is kept for backward\n+            compatibility with previously released checkpoints and runtimes relying on the legacy DeepSeek config.\n         v_head_dim (`int`, *optional*, defaults to 128):\n             The dimension of value projections in the attention layers.\n         num_experts_per_tok (`int`, *optional*):\n@@ -169,6 +172,7 @@ def __init__(\n         routed_scaling_factor: Optional[float] = 1.0,\n         topk_group: Optional[int] = None,\n         topk_method: Optional[str] = \"greedy\",\n+        norm_topk_prob: Optional[bool] = False,\n         v_head_dim: Optional[int] = 128,\n         num_experts_per_tok: Optional[int] = None,\n         moe_intermediate_size: Optional[int] = 1407,\n@@ -185,6 +189,7 @@ def __init__(\n         self.routed_scaling_factor = routed_scaling_factor\n         self.topk_group = topk_group\n         self.topk_method = topk_method\n+        self.norm_topk_prob = norm_topk_prob\n         self.v_head_dim = v_head_dim\n         self.num_experts_per_tok = num_experts_per_tok\n         self.moe_intermediate_size = moe_intermediate_size"
        },
        {
            "sha": "510be857637424faf30c7de18180afc1760908bb",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/663b4cf121d2ab284bb24dd8174e53a79c404d15/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/663b4cf121d2ab284bb24dd8174e53a79c404d15/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=663b4cf121d2ab284bb24dd8174e53a79c404d15",
            "patch": "@@ -119,6 +119,9 @@ class DeepseekV2Config(LlamaConfig):\n             Number of selected groups per token for expert selection.\n         topk_method (`str`, *optional*, defaults to `\"greedy\"`):\n             The method used for selecting top-k experts in the routed gate mechanism.\n+        norm_topk_prob (`bool`, *optional*, defaults to `False`):\n+            Whether to renormalize the router probabilities when `top_k > 1`. This flag is kept for backward\n+            compatibility with previously released checkpoints and runtimes relying on the legacy DeepSeek config.\n         v_head_dim (`int`, *optional*, defaults to 128):\n             The dimension of value projections in the attention layers.\n         num_experts_per_tok (`int`, *optional*):\n@@ -182,6 +185,7 @@ def __init__(\n         routed_scaling_factor: Optional[float] = 1.0,\n         topk_group: Optional[int] = None,\n         topk_method: Optional[str] = \"greedy\",\n+        norm_topk_prob: Optional[bool] = False,\n         v_head_dim: Optional[int] = 128,\n         num_experts_per_tok: Optional[int] = None,\n         moe_intermediate_size: Optional[int] = 1407,\n@@ -198,6 +202,7 @@ def __init__(\n         self.routed_scaling_factor = routed_scaling_factor\n         self.topk_group = topk_group\n         self.topk_method = topk_method\n+        self.norm_topk_prob = norm_topk_prob\n         self.v_head_dim = v_head_dim\n         self.num_experts_per_tok = num_experts_per_tok\n         self.moe_intermediate_size = moe_intermediate_size"
        },
        {
            "sha": "692c9861dd2927f5754a5768752b1afadd3c005e",
            "filename": "utils/check_config_attributes.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/663b4cf121d2ab284bb24dd8174e53a79c404d15/utils%2Fcheck_config_attributes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/663b4cf121d2ab284bb24dd8174e53a79c404d15/utils%2Fcheck_config_attributes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_config_attributes.py?ref=663b4cf121d2ab284bb24dd8174e53a79c404d15",
            "patch": "@@ -314,6 +314,7 @@\n     \"GemmaConfig\": [\"tie_word_embeddings\"],\n     \"CsmConfig\": [\"tie_codebooks_embeddings\"],\n     \"LayoutXLMConfig\": True,\n+    \"DeepseekV2Config\": [\"norm_topk_prob\"],\n }\n \n "
        }
    ],
    "stats": {
        "total": 11,
        "additions": 11,
        "deletions": 0
    }
}