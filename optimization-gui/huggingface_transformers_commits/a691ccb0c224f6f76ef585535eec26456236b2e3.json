{
    "author": "ydshieh",
    "message": "Change back to `Thread` for SF conversion (#35236)\n\n* fix\r\n\r\n* fix\r\n\r\n* fix\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "a691ccb0c224f6f76ef585535eec26456236b2e3",
    "files": [
        {
            "sha": "c86559e62f94ea90a41dbb12b5129d3512ea9c0d",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a691ccb0c224f6f76ef585535eec26456236b2e3/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a691ccb0c224f6f76ef585535eec26456236b2e3/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=a691ccb0c224f6f76ef585535eec26456236b2e3",
            "patch": "@@ -29,7 +29,7 @@\n from contextlib import contextmanager\n from dataclasses import dataclass\n from functools import partial, wraps\n-from multiprocessing import Process\n+from threading import Thread\n from typing import Any, Callable, Dict, List, Optional, Set, Tuple, Type, TypeVar, Union\n from zipfile import is_zipfile\n \n@@ -3825,11 +3825,11 @@ def from_pretrained(\n                                     **has_file_kwargs,\n                                 }\n                                 if not has_file(pretrained_model_name_or_path, safe_weights_name, **has_file_kwargs):\n-                                    Process(\n+                                    Thread(\n                                         target=auto_conversion,\n                                         args=(pretrained_model_name_or_path,),\n                                         kwargs={\"ignore_errors_during_conversion\": True, **cached_file_kwargs},\n-                                        name=\"Process-auto_conversion\",\n+                                        name=\"Thread-auto_conversion\",\n                                     ).start()\n                         else:\n                             # Otherwise, no PyTorch file was found, maybe there is a TF or Flax model file."
        },
        {
            "sha": "f1612d3ea57c98fd1d383887cfbeb4e2882d3963",
            "filename": "src/transformers/safetensors_conversion.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a691ccb0c224f6f76ef585535eec26456236b2e3/src%2Ftransformers%2Fsafetensors_conversion.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a691ccb0c224f6f76ef585535eec26456236b2e3/src%2Ftransformers%2Fsafetensors_conversion.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fsafetensors_conversion.py?ref=a691ccb0c224f6f76ef585535eec26456236b2e3",
            "patch": "@@ -67,7 +67,7 @@ def get_conversion_pr_reference(api: HfApi, model_id: str, **kwargs):\n     # security breaches.\n     pr = previous_pr(api, model_id, pr_title, token=token)\n \n-    if pr is None or (not private and pr.author != \"SFConvertBot\"):\n+    if pr is None or (not private and pr.author != \"SFconvertbot\"):\n         spawn_conversion(token, private, model_id)\n         pr = previous_pr(api, model_id, pr_title, token=token)\n     else:"
        },
        {
            "sha": "409f274d41eb171abb02e37fc480535f82631119",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 19,
            "deletions": 2,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/a691ccb0c224f6f76ef585535eec26456236b2e3/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a691ccb0c224f6f76ef585535eec26456236b2e3/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=a691ccb0c224f6f76ef585535eec26456236b2e3",
            "patch": "@@ -28,6 +28,7 @@\n import subprocess\n import sys\n import tempfile\n+import threading\n import time\n import unittest\n from collections import defaultdict\n@@ -2311,12 +2312,28 @@ class RequestCounter:\n \n     def __enter__(self):\n         self._counter = defaultdict(int)\n-        self.patcher = patch.object(urllib3.connectionpool.log, \"debug\", wraps=urllib3.connectionpool.log.debug)\n+        self._thread_id = threading.get_ident()\n+        self._extra_info = []\n+\n+        def patched_with_thread_info(func):\n+            def wrap(*args, **kwargs):\n+                self._extra_info.append(threading.get_ident())\n+                return func(*args, **kwargs)\n+\n+            return wrap\n+\n+        self.patcher = patch.object(\n+            urllib3.connectionpool.log, \"debug\", side_effect=patched_with_thread_info(urllib3.connectionpool.log.debug)\n+        )\n         self.mock = self.patcher.start()\n         return self\n \n     def __exit__(self, *args, **kwargs) -> None:\n-        for call in self.mock.call_args_list:\n+        assert len(self.mock.call_args_list) == len(self._extra_info)\n+\n+        for thread_id, call in zip(self._extra_info, self.mock.call_args_list):\n+            if thread_id != self._thread_id:\n+                continue\n             log = call.args[0] % call.args[1:]\n             for method in (\"HEAD\", \"GET\", \"POST\", \"PUT\", \"DELETE\", \"CONNECT\", \"OPTIONS\", \"TRACE\", \"PATCH\"):\n                 if method in log:"
        }
    ],
    "stats": {
        "total": 29,
        "additions": 23,
        "deletions": 6
    }
}