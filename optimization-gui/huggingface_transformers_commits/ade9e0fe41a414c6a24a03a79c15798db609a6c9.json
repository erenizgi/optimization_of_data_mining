{
    "author": "alankashkash",
    "message": "Corrected max number for bf16 in transformer/docs (#33658)\n\nUpdate perf_train_gpu_one.md\r\n\r\nper issue https://github.com/huggingface/hub-docs/issues/1425 max number for bf16 should be 65,504 not 65,535",
    "sha": "ade9e0fe41a414c6a24a03a79c15798db609a6c9",
    "files": [
        {
            "sha": "5d7ae312ee0ff0ad83373058e0b813267d1b8c1e",
            "filename": "docs/source/en/perf_train_gpu_one.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/ade9e0fe41a414c6a24a03a79c15798db609a6c9/docs%2Fsource%2Fen%2Fperf_train_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/ade9e0fe41a414c6a24a03a79c15798db609a6c9/docs%2Fsource%2Fen%2Fperf_train_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperf_train_gpu_one.md?ref=ade9e0fe41a414c6a24a03a79c15798db609a6c9",
            "patch": "@@ -186,7 +186,7 @@ If you prefer to use ðŸ¤— Accelerate, find the ðŸ¤— Accelerate example [further\n \n If you have access to an Ampere or newer hardware you can use bf16 for mixed precision training and evaluation. While \n bf16 has a worse precision than fp16, it has a much bigger dynamic range. In fp16 the biggest number you can have \n-is `65535` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \n+is `65504` and any number above that will result in an overflow. A bf16 number can be as large as `3.39e+38` (!) which \n is about the same as fp32 - because both have 8-bits used for the numerical range.\n \n You can enable BF16 in the ðŸ¤— Trainer with:"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}