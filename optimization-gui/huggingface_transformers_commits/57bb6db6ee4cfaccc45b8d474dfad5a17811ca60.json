{
    "author": "romitjain",
    "message": "Skipping pytree registration in case fsdp is enabled (#40075)\n\n* Skipping pytree registration in case fsdp is enabled\n\n* Beauty changes\n\n* Beauty changes\n\n* Moved the is_fsdp_available function to import utils\n\n* Moved is_fsdp_available to integrations.fsdp\n\n* Skipping pytree registration in case fsdp is enabled\n\n* Beauty changes\n\n* Beauty changes\n\n* Moved the is_fsdp_available function to import utils\n\n* Moved is_fsdp_available to integrations.fsdp\n\n* Added pytree registration inside dynamic cache class\n\n* Making ci/cd lords happy\n\n* Adding a check if DynamicCache is already a leaf\n\n* Adding try/catch for multiple initializations of DynamicCache in test suites\n\n* Moving dynamic cache pytree registration to executorch\n\n* Adding try catch back",
    "sha": "57bb6db6ee4cfaccc45b8d474dfad5a17811ca60",
    "files": [
        {
            "sha": "e52774e8c13c938428502c8dc564ba55cef13e45",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 50,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/57bb6db6ee4cfaccc45b8d474dfad5a17811ca60/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57bb6db6ee4cfaccc45b8d474dfad5a17811ca60/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=57bb6db6ee4cfaccc45b8d474dfad5a17811ca60",
            "patch": "@@ -4,8 +4,6 @@\n \n import torch\n \n-from transformers.pytorch_utils import is_torch_greater_or_equal_than_2_6\n-\n from .configuration_utils import PretrainedConfig\n from .utils import (\n     is_hqq_available,\n@@ -1072,54 +1070,6 @@ def from_legacy_cache(cls, past_key_values: tuple[tuple[torch.Tensor, torch.Tens\n         return cache\n \n \n-# Utilities for `DynamicCache` <> torch.export support\n-\n-if is_torch_greater_or_equal(\"2.3\"):\n-\n-    def _get_cache_dict(cache: DynamicCache):\n-        if any(not isinstance(layer, (DynamicLayer, DynamicSlidingWindowLayer)) for layer in cache.layers):\n-            raise RuntimeError(\"This pytree flattening function should only be applied to DynamicCache\")\n-\n-        if not is_torch_greater_or_equal_than_2_6:\n-            logger.warning_once(\n-                \"DynamicCache + torch.export is tested on torch 2.6.0+ and may not work on earlier versions.\"\n-            )\n-\n-        return {\n-            \"key_cache\": [layer.keys for layer in cache.layers if layer.keys is not None],\n-            \"value_cache\": [layer.values for layer in cache.layers if layer.values is not None],\n-        }\n-\n-    def _unflatten_dynamic_cache(\n-        values,\n-        context: torch.utils._pytree.Context,\n-    ):\n-        dictionary = torch.utils._pytree._dict_unflatten(values, context)\n-        cache = DynamicCache()\n-        # Reconstruct layers from keys and values lists\n-        key_list = dictionary.get(\"key_cache\", [])\n-        value_list = dictionary.get(\"value_cache\", [])\n-        for idx in range(max(len(key_list), len(value_list))):\n-            key = key_list[idx] if idx < len(key_list) else None\n-            value = value_list[idx] if idx < len(value_list) else None\n-            cache.update(key, value, idx)\n-        return cache\n-\n-    torch.utils._pytree.register_pytree_node(\n-        DynamicCache,\n-        lambda dynamic_cache: torch.utils._pytree._dict_flatten(_get_cache_dict(dynamic_cache)),\n-        _unflatten_dynamic_cache,\n-        serialized_type_name=f\"{DynamicCache.__module__}.{DynamicCache.__name__}\",\n-        flatten_with_keys_fn=lambda dynamic_cache: torch.utils._pytree._dict_flatten_with_keys(\n-            _get_cache_dict(dynamic_cache)\n-        ),\n-    )\n-    # TODO (tmanlaibaatar) This won't be needed in torch 2.7.\n-    torch.fx._pytree.register_pytree_flatten_spec(\n-        DynamicCache, lambda cache, spec: torch.fx._pytree._dict_flatten_spec(_get_cache_dict(cache), spec)\n-    )\n-\n-\n class OffloadedCache(Cache):\n     \"\"\"\n     A drop-in replacement for DynamicCache that conserves accelerator (GPU, XPU) memory at the expense of more CPU memory."
        },
        {
            "sha": "a141eab45532df02eb5eab7cc764cacb71fd5bfd",
            "filename": "src/transformers/integrations/__init__.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/57bb6db6ee4cfaccc45b8d474dfad5a17811ca60/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57bb6db6ee4cfaccc45b8d474dfad5a17811ca60/src%2Ftransformers%2Fintegrations%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2F__init__.py?ref=57bb6db6ee4cfaccc45b8d474dfad5a17811ca60",
            "patch": "@@ -55,7 +55,7 @@\n     \"eetq\": [\"replace_with_eetq_linear\"],\n     \"fbgemm_fp8\": [\"FbgemmFp8Linear\", \"FbgemmFp8Llama4TextExperts\", \"replace_with_fbgemm_fp8_linear\"],\n     \"finegrained_fp8\": [\"FP8Linear\", \"replace_with_fp8_linear\"],\n-    \"fsdp\": [\"is_fsdp_managed_module\"],\n+    \"fsdp\": [\"is_fsdp_enabled\", \"is_fsdp_managed_module\"],\n     \"ggml\": [\n         \"GGUF_CONFIG_MAPPING\",\n         \"GGUF_TOKENIZER_MAPPING\",\n@@ -204,7 +204,7 @@\n     from .eetq import replace_with_eetq_linear\n     from .fbgemm_fp8 import FbgemmFp8Linear, FbgemmFp8Llama4TextExperts, replace_with_fbgemm_fp8_linear\n     from .finegrained_fp8 import FP8Linear, replace_with_fp8_linear\n-    from .fsdp import is_fsdp_managed_module\n+    from .fsdp import is_fsdp_enabled, is_fsdp_managed_module\n     from .ggml import (\n         GGUF_CONFIG_MAPPING,\n         GGUF_TOKENIZER_MAPPING,"
        },
        {
            "sha": "2ae73f37212ee6798bb8fd20fc87bee4eb472c17",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 70,
            "deletions": 2,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/57bb6db6ee4cfaccc45b8d474dfad5a17811ca60/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57bb6db6ee4cfaccc45b8d474dfad5a17811ca60/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=57bb6db6ee4cfaccc45b8d474dfad5a17811ca60",
            "patch": "@@ -15,7 +15,14 @@\n \n import torch\n \n-from ..cache_utils import DynamicCache, EncoderDecoderCache, HybridCache, StaticCache\n+from ..cache_utils import (\n+    DynamicCache,\n+    DynamicLayer,\n+    DynamicSlidingWindowLayer,\n+    EncoderDecoderCache,\n+    HybridCache,\n+    StaticCache,\n+)\n from ..generation.configuration_utils import GenerationConfig\n from ..masking_utils import (\n     ALL_MASK_ATTENTION_FUNCTIONS,\n@@ -24,7 +31,11 @@\n     prepare_padding_mask,\n )\n from ..modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ..pytorch_utils import is_torch_greater_or_equal, is_torch_greater_or_equal_than_2_3\n+from ..pytorch_utils import (\n+    is_torch_greater_or_equal,\n+    is_torch_greater_or_equal_than_2_3,\n+    is_torch_greater_or_equal_than_2_6,\n+)\n \n \n # Add this to src/transformers/integrations/executorch.py\n@@ -824,6 +835,8 @@ def __init__(self, model, max_static_cache_length, batch_size):\n         self.static_cache.early_initialization(batch_size, num_heads, head_dim, torch.float32, \"cpu\")\n         self.cache = EncoderDecoderCache(self.static_cache, DynamicCache())\n \n+        register_dynamic_cache_export_support()\n+\n         # Register cache buffers to make them exportable\n         for i in range(len(self.static_cache)):\n             self.register_buffer(f\"key_cache_{i}\", self.static_cache.layers[i].keys, persistent=False)\n@@ -996,6 +1009,8 @@ def export_with_dynamic_cache(\n     ALL_ATTENTION_FUNCTIONS.register(\"sdpa_without_vmap\", ALL_ATTENTION_FUNCTIONS[\"sdpa\"])\n     model.config._attn_implementation = \"sdpa_without_vmap\"\n \n+    register_dynamic_cache_export_support()\n+\n     with torch.no_grad():\n         exported_program = torch.export.export(\n             model,\n@@ -1011,6 +1026,59 @@ def export_with_dynamic_cache(\n         return exported_program\n \n \n+def register_dynamic_cache_export_support():\n+    \"\"\"\n+    Utilities for `DynamicCache` <> torch.export support\n+    \"\"\"\n+\n+    try:\n+        torch.utils._pytree.register_pytree_node(\n+            DynamicCache,\n+            lambda dynamic_cache: torch.utils._pytree._dict_flatten(_get_cache_dict(dynamic_cache)),\n+            _unflatten_dynamic_cache,\n+            serialized_type_name=f\"{DynamicCache.__module__}.{DynamicCache.__name__}\",\n+            flatten_with_keys_fn=lambda dynamic_cache: torch.utils._pytree._dict_flatten_with_keys(\n+                _get_cache_dict(dynamic_cache)\n+            ),\n+        )\n+        # TODO (tmanlaibaatar) This won't be needed in torch 2.7.\n+        torch.fx._pytree.register_pytree_flatten_spec(\n+            DynamicCache,\n+            lambda cache, spec: torch.fx._pytree._dict_flatten_spec(_get_cache_dict(cache), spec),\n+        )\n+    # Catching this in case there are multiple runs for some test runs\n+    except ValueError as e:\n+        if \"already registered as pytree node\" not in str(e):\n+            raise\n+\n+\n+def _get_cache_dict(cache: DynamicCache):\n+    \"\"\"Convert cache to dictionary format for pytree operations.\"\"\"\n+    if any(not isinstance(layer, (DynamicLayer, DynamicSlidingWindowLayer)) for layer in cache.layers):\n+        raise RuntimeError(\"This pytree flattening function should only be applied to DynamicCache\")\n+\n+    if not is_torch_greater_or_equal_than_2_6:\n+        logging.warning(\"DynamicCache + torch.export is tested on torch 2.6.0+ and may not work on earlier versions.\")\n+\n+    return {\n+        \"key_cache\": [layer.keys for layer in cache.layers if layer.keys is not None],\n+        \"value_cache\": [layer.values for layer in cache.layers if layer.values is not None],\n+    }\n+\n+\n+def _unflatten_dynamic_cache(values, context: torch.utils._pytree.Context):\n+    dictionary = torch.utils._pytree._dict_unflatten(values, context)\n+    cache = DynamicCache()\n+    # Reconstruct layers from keys and values lists\n+    key_list = dictionary.get(\"key_cache\", [])\n+    value_list = dictionary.get(\"value_cache\", [])\n+    for idx in range(max(len(key_list), len(value_list))):\n+        key = key_list[idx] if idx < len(key_list) else None\n+        value = value_list[idx] if idx < len(value_list) else None\n+        cache.update(key, value, idx)\n+    return cache\n+\n+\n def sdpa_mask_without_vmap(\n     batch_size: int,\n     cache_position: torch.Tensor,"
        },
        {
            "sha": "240268b751242cbf6ef04648e2b4807d41f02180",
            "filename": "src/transformers/integrations/fsdp.py",
            "status": "modified",
            "additions": 16,
            "deletions": 1,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/57bb6db6ee4cfaccc45b8d474dfad5a17811ca60/src%2Ftransformers%2Fintegrations%2Ffsdp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57bb6db6ee4cfaccc45b8d474dfad5a17811ca60/src%2Ftransformers%2Fintegrations%2Ffsdp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffsdp.py?ref=57bb6db6ee4cfaccc45b8d474dfad5a17811ca60",
            "patch": "@@ -13,9 +13,10 @@\n # limitations under the License.\n from __future__ import annotations\n \n+import os\n from typing import TYPE_CHECKING\n \n-from ..utils import is_torch_available\n+from ..utils import is_torch_available, strtobool\n \n \n if TYPE_CHECKING:\n@@ -36,3 +37,17 @@ def is_fsdp_managed_module(module: nn.Module) -> bool:\n     return isinstance(module, torch.distributed.fsdp.FullyShardedDataParallel) or getattr(\n         module, \"_is_fsdp_managed_module\", False\n     )\n+\n+\n+def is_fsdp_enabled():\n+    if is_torch_available():\n+        import torch\n+\n+        return (\n+            torch.distributed.is_available()\n+            and torch.distributed.is_initialized()\n+            and strtobool(os.environ.get(\"ACCELERATE_USE_FSDP\", \"False\")) == 1\n+            and strtobool(os.environ.get(\"FSDP_CPU_RAM_EFFICIENT_LOADING\", \"False\")) == 1\n+        )\n+\n+    return False"
        },
        {
            "sha": "f3d6d92ad1011163c60aeb26d5b09ea598455842",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 11,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/57bb6db6ee4cfaccc45b8d474dfad5a17811ca60/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/57bb6db6ee4cfaccc45b8d474dfad5a17811ca60/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=57bb6db6ee4cfaccc45b8d474dfad5a17811ca60",
            "patch": "@@ -54,7 +54,7 @@\n from .distributed import DistributedConfig\n from .dynamic_module_utils import custom_object_save\n from .generation import CompileConfig, GenerationConfig\n-from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled\n+from .integrations import PeftAdapterMixin, deepspeed_config, is_deepspeed_zero3_enabled, is_fsdp_enabled\n from .integrations.accelerate import find_tied_parameters, init_empty_weights\n from .integrations.deepspeed import _load_state_dict_into_zero3_model\n from .integrations.eager_paged import eager_paged_attention_forward\n@@ -124,7 +124,6 @@\n     is_torch_xla_available,\n     is_torch_xpu_available,\n     logging,\n-    strtobool,\n )\n from .utils.generic import _CAN_RECORD_REGISTRY, GeneralInterface, OutputRecorder\n from .utils.hub import create_and_tag_model_card, get_checkpoint_shard_files\n@@ -182,15 +181,6 @@\n     from torch.distributed.tensor import DTensor\n \n \n-def is_fsdp_enabled():\n-    return (\n-        torch.distributed.is_available()\n-        and torch.distributed.is_initialized()\n-        and strtobool(os.environ.get(\"ACCELERATE_USE_FSDP\", \"False\")) == 1\n-        and strtobool(os.environ.get(\"FSDP_CPU_RAM_EFFICIENT_LOADING\", \"False\")) == 1\n-    )\n-\n-\n def is_local_dist_rank_0():\n     return (\n         torch.distributed.is_available()"
        }
    ],
    "stats": {
        "total": 155,
        "additions": 89,
        "deletions": 66
    }
}