{
    "author": "zucchini-nlp",
    "message": "Fix dtype in Paligemma (#40912)\n\n* fix dtypes\n\n* fix copies\n\n* delete unused attr",
    "sha": "cccef4be918f67c7661f265123b1c4fccaf6f19d",
    "files": [
        {
            "sha": "fe92252b9a8012beddbb778405124ea6e993c030",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=cccef4be918f67c7661f265123b1c4fccaf6f19d",
            "patch": "@@ -156,7 +156,8 @@ def forward(\n         vlm_image_hidden_states = vlm_output.image_hidden_states if pixel_values is not None else None\n \n         last_hidden_states = vlm_output[0]  # (batch_size, sequence_length, hidden_size)\n-        embeddings = self.embedding_proj_layer(last_hidden_states)  # (batch_size, sequence_length, dim)\n+        proj_dtype = self.embedding_proj_layer.weight.dtype\n+        embeddings = self.embedding_proj_layer(last_hidden_states.to(proj_dtype))  # (batch_size, sequence_length, dim)\n \n         # L2 normalization\n         embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)  # (batch_size, sequence_length, dim)"
        },
        {
            "sha": "fc0f585531aedd4703bac149ebef450fd529c548",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=cccef4be918f67c7661f265123b1c4fccaf6f19d",
            "patch": "@@ -143,9 +143,6 @@ def forward(\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         \"\"\"\n-        if pixel_values is not None:\n-            pixel_values = pixel_values.to(dtype=self.dtype)  # (batch_size, max_num_patches, pixel_values)\n-\n         # Handle the custom \"pixel_values\" input obtained with `ColQwen2Processor` through unpadding\n         if pixel_values is not None and image_grid_thw is not None:\n             # NOTE: image_grid_thw: (batch_size, 3) where image_grid_thw[i] = (num_patches_h, num_patches_w, temporal_patch_size)\n@@ -182,9 +179,6 @@ def forward(\n                 image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(inputs_embeds.device)\n-\n         vlm_output = self.vlm.model(\n             input_ids=None,\n             position_ids=position_ids,\n@@ -201,7 +195,8 @@ def forward(\n         vlm_hidden_states = vlm_output.hidden_states if output_hidden_states else None\n \n         last_hidden_states = vlm_output[0]  # (batch_size, sequence_length, hidden_size)\n-        embeddings = self.embedding_proj_layer(last_hidden_states)  # (batch_size, sequence_length, dim)\n+        proj_dtype = self.embedding_proj_layer.weight.dtype\n+        embeddings = self.embedding_proj_layer(last_hidden_states.to(proj_dtype))  # (batch_size, sequence_length, dim)\n \n         # L2 normalization\n         embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)  # (batch_size, sequence_length, dim)"
        },
        {
            "sha": "a4684d670d17e4551c57e56434193e0fe8022cc4",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=cccef4be918f67c7661f265123b1c4fccaf6f19d",
            "patch": "@@ -336,9 +336,6 @@ def forward(\n         image_grid_thw (`torch.LongTensor` of shape `(num_images, 3)`, *optional*):\n             The temporal, height and width of feature shape of each image in LLM.\n         \"\"\"\n-        if pixel_values is not None:\n-            pixel_values = pixel_values.to(dtype=self.dtype)  # (batch_size, max_num_patches, pixel_values)\n-\n         # Handle the custom \"pixel_values\" input obtained with `ColQwen2Processor` through unpadding\n         if pixel_values is not None and image_grid_thw is not None:\n             # NOTE: image_grid_thw: (batch_size, 3) where image_grid_thw[i] = (num_patches_h, num_patches_w, temporal_patch_size)\n@@ -375,9 +372,6 @@ def forward(\n                 image_embeds = image_embeds.to(inputs_embeds.device, inputs_embeds.dtype)\n                 inputs_embeds = inputs_embeds.masked_scatter(image_mask, image_embeds)\n \n-            if attention_mask is not None:\n-                attention_mask = attention_mask.to(inputs_embeds.device)\n-\n         vlm_output = self.vlm.model(\n             input_ids=None,\n             position_ids=position_ids,\n@@ -394,7 +388,8 @@ def forward(\n         vlm_hidden_states = vlm_output.hidden_states if output_hidden_states else None\n \n         last_hidden_states = vlm_output[0]  # (batch_size, sequence_length, hidden_size)\n-        embeddings = self.embedding_proj_layer(last_hidden_states)  # (batch_size, sequence_length, dim)\n+        proj_dtype = self.embedding_proj_layer.weight.dtype\n+        embeddings = self.embedding_proj_layer(last_hidden_states.to(proj_dtype))  # (batch_size, sequence_length, dim)\n \n         # L2 normalization\n         embeddings = embeddings / embeddings.norm(dim=-1, keepdim=True)  # (batch_size, sequence_length, dim)"
        },
        {
            "sha": "c7db46bf75743ef7603acba2657120abb965071f",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=cccef4be918f67c7661f265123b1c4fccaf6f19d",
            "patch": "@@ -756,6 +756,10 @@ class Gemma3Model(PaliGemmaModel):\n     # we are filtering the logits/labels so we shouldn't divide the loss based on num_items_in_batch\n     accepts_loss_kwargs = False\n \n+    def __init__(self, config: Gemma3Config):\n+        super().__init__(config)\n+        del self.text_config_dtype\n+\n     def get_image_features(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         \"\"\"\n         Projects the last hidden state from the vision model into language model space."
        },
        {
            "sha": "48de2bb27f7f8f1383c30bd298681f0ec17da46a",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=cccef4be918f67c7661f265123b1c4fccaf6f19d",
            "patch": "@@ -2241,6 +2241,7 @@ class Gemma3nModel(PaliGemmaModel):\n     def __init__(self, config: Gemma3nConfig):\n         super().__init__(config)\n         del self.multi_modal_projector  # Replaced by Gemma3nVisionEmbedder\n+        del self.text_config_dtype\n         self.vocab_size_per_layer_input = config.text_config.vocab_size_per_layer_input\n         self.audio_tower = AutoModel.from_config(config.audio_config)\n         self.embed_vision = Gemma3nMultimodalEmbedder(config.vision_config, config.text_config)"
        },
        {
            "sha": "a165bc22a0de3dfa515956b58a2dc8f0201bb82f",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/cccef4be918f67c7661f265123b1c4fccaf6f19d/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=cccef4be918f67c7661f265123b1c4fccaf6f19d",
            "patch": "@@ -143,6 +143,7 @@ def __init__(self, config: PaliGemmaConfig):\n         self.language_model = language_model\n \n         self.pad_token_id = self.config.pad_token_id if self.config.pad_token_id is not None else -1\n+        self.text_config_dtype = self.config.get_text_config().dtype or self.dtype\n         self.post_init()\n \n     # Copied from transformers.models.llava.modeling_llava.LlavaModel.get_input_embeddings with Llava->PaliGemma\n@@ -174,7 +175,7 @@ def _update_causal_mask(\n             return None\n         is_training = is_training if is_training is not None else self.training\n         using_static_cache = isinstance(past_key_values, StaticCache)\n-        min_dtype = torch.finfo(self.dtype).min\n+        min_dtype = torch.finfo(self.text_config_dtype).min\n         if input_tensor is None:\n             input_tensor = attention_mask\n \n@@ -193,7 +194,10 @@ def _update_causal_mask(\n             return attention_mask\n \n         causal_mask = torch.full(\n-            (sequence_length, target_length), fill_value=min_dtype, dtype=self.dtype, device=cache_position.device\n+            (sequence_length, target_length),\n+            fill_value=min_dtype,\n+            dtype=self.text_config_dtype,\n+            device=cache_position.device,\n         )\n         # Causal diagonal mask only if training, otherwise attend to the whole prefix. Training-specific attn for prefix is handled below\n         if sequence_length != 1:"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 17,
        "deletions": 17
    }
}