{
    "author": "li-plus",
    "message": "Support gradient checkpointing in Qwen2VL ViT (#34724)\n\n* Support gradient checkpointing in Qwen2VL ViT\r\n\r\n* Enable gradient checkpoint tests for Qwen2VL\r\n\r\n* [run-slow] qwen2_vl",
    "sha": "0db91c3c8ddd8ca60c061d6ebdea30dc35e2d9f5",
    "files": [
        {
            "sha": "eabae7b2b0df065a19f813e98bcad0be1e566d62",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 1,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0db91c3c8ddd8ca60c061d6ebdea30dc35e2d9f5/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0db91c3c8ddd8ca60c061d6ebdea30dc35e2d9f5/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=0db91c3c8ddd8ca60c061d6ebdea30dc35e2d9f5",
            "patch": "@@ -1000,6 +1000,7 @@ def __init__(self, config) -> None:\n         self.merger = PatchMerger(\n             dim=config.hidden_size, context_dim=config.embed_dim, spatial_merge_size=config.spatial_merge_size\n         )\n+        self.gradient_checkpointing = False\n \n     def get_dtype(self) -> torch.dtype:\n         return self.blocks[0].mlp.fc2.weight.dtype\n@@ -1046,7 +1047,12 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor) -> torch.\n         cu_seqlens = F.pad(cu_seqlens, (1, 0), value=0)\n \n         for blk in self.blocks:\n-            hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb)\n+            if self.gradient_checkpointing and self.training:\n+                hidden_states = self._gradient_checkpointing_func(\n+                    blk.__call__, hidden_states, cu_seqlens, rotary_pos_emb\n+                )\n+            else:\n+                hidden_states = blk(hidden_states, cu_seqlens=cu_seqlens, rotary_pos_emb=rotary_pos_emb)\n \n         return self.merger(hidden_states)\n "
        },
        {
            "sha": "f2a3719e17b4c6227798ac76583f4ba3bed438af",
            "filename": "tests/models/qwen2_vl/test_modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/0db91c3c8ddd8ca60c061d6ebdea30dc35e2d9f5/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0db91c3c8ddd8ca60c061d6ebdea30dc35e2d9f5/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_modeling_qwen2_vl.py?ref=0db91c3c8ddd8ca60c061d6ebdea30dc35e2d9f5",
            "patch": "@@ -285,24 +285,6 @@ def test_mismatching_num_image_tokens(self):\n             image_grid_thw = torch.cat([image_grid_thw, image_grid_thw], dim=0)\n             _ = model(input_ids=input_ids, pixel_values=pixel_values, image_grid_thw=image_grid_thw)\n \n-    @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing_use_reentrant(self):\n-        pass\n-\n-    @unittest.skip(\n-        reason=\"This architecure seem to not compute gradients properly when using GC, check: https://github.com/huggingface/transformers/pull/27124\"\n-    )\n-    def test_training_gradient_checkpointing_use_reentrant_false(self):\n-        pass\n-\n     @unittest.skip(reason=\"Feedforward chunking is not yet supported\")\n     def test_feed_forward_chunking(self):\n         pass"
        }
    ],
    "stats": {
        "total": 26,
        "additions": 7,
        "deletions": 19
    }
}