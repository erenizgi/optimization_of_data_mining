{
    "author": "zhanluxianshen",
    "message": "Fix qwen2-vl-docs. (#37879)\n\nSigned-off-by: zhanluxianshen <zhanluxianshen@163.com>",
    "sha": "4fc976779e9f8b5584c7d1d2634f9bf637afe619",
    "files": [
        {
            "sha": "a7af98648262c72159556902bbb723c43f22f4ca",
            "filename": "Makefile",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fc976779e9f8b5584c7d1d2634f9bf637afe619/Makefile",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fc976779e9f8b5584c7d1d2634f9bf637afe619/Makefile",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/Makefile?ref=4fc976779e9f8b5584c7d1d2634f9bf637afe619",
            "patch": "@@ -79,7 +79,7 @@ fixup: modified_only_fixup extra_style_checks autogenerate_code repo-consistency\n \n fix-copies:\n \tpython utils/check_copies.py --fix_and_overwrite\n-\tpython utils/check_modular_conversion.py  --fix_and_overwrite\n+\tpython utils/check_modular_conversion.py --fix_and_overwrite\n \tpython utils/check_dummies.py --fix_and_overwrite\n \tpython utils/check_doctest_list.py --fix_and_overwrite\n \tpython utils/check_docstrings.py --fix_and_overwrite"
        },
        {
            "sha": "c414b41faacbd662598242a8865e084fc43d7fb4",
            "filename": "docs/source/en/model_doc/qwen2_5_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/4fc976779e9f8b5584c7d1d2634f9bf637afe619/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/4fc976779e9f8b5584c7d1d2634f9bf637afe619/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md?ref=4fc976779e9f8b5584c7d1d2634f9bf637afe619",
            "patch": "@@ -118,7 +118,7 @@ The example below uses [torchao](../quantization/torchao) to only quantize the w\n \n ```python\n import torch\n-from transformers import TorchAoConfig, Gemma3ForConditionalGeneration, AutoProcessor\n+from transformers import TorchAoConfig, Qwen2_5_VLForConditionalGeneration, AutoProcessor\n \n quantization_config = TorchAoConfig(\"int4_weight_only\", group_size=128)\n model = Qwen2_5_VLForConditionalGeneration.from_pretrained("
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}