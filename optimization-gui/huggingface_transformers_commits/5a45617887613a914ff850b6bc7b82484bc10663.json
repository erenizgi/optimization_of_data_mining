{
    "author": "zRzRzRzRzRzRzR",
    "message": "change apply_rotary_pos_emb of Glmmodel for GLM-Edge Series model (#34629)\n\n* change apply_rotary_pos_emb\r\n\r\n* upload for glm-edge\r\n\r\n* remove useless part\r\n\r\n* follow the suggestion\r\n\r\n* fix\r\n\r\n* format\r\n\r\n* format\r\n\r\n* test\r\n\r\n* format again\r\n\r\n* format again\r\n\r\n* remove modular change\r\n\r\n* remove modular change\r\n\r\n* this apply_rotary_pos_emb need modify?\r\n\r\n* fix with this\r\n\r\n* format\r\n\r\n* format\r\n\r\n* ruff check\r\n\r\n* modify modular_glm failed\r\n\r\n* remove partial_rotary_factor of function  partial_rotary_factor\r\n\r\n* fix wrong change of examples/research_projects\r\n\r\n* revert\r\n\r\n* remove line 118\r\n\r\n* use q_rot",
    "sha": "5a45617887613a914ff850b6bc7b82484bc10663",
    "files": [
        {
            "sha": "de0e80e8c65ba9c11419bb33f3a4ba3582c931da",
            "filename": "src/transformers/models/glm/configuration_glm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a45617887613a914ff850b6bc7b82484bc10663/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a45617887613a914ff850b6bc7b82484bc10663/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fconfiguration_glm.py?ref=5a45617887613a914ff850b6bc7b82484bc10663",
            "patch": "@@ -45,6 +45,7 @@ class GlmConfig(PretrainedConfig):\n             by meanpooling all the original heads within that group. For more details checkout [this\n             paper](https://arxiv.org/pdf/2305.13245.pdf). If it is not specified, will default to\n             `num_attention_heads`.\n+        partial_rotary_factor (`float`, *optional*, defaults to 0.5): The factor of the partial rotary position.\n         head_dim (`int`, *optional*, defaults to 128):\n             The attention head dimension.\n         hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n@@ -93,6 +94,7 @@ def __init__(\n         num_hidden_layers=40,\n         num_attention_heads=32,\n         num_key_value_heads=2,\n+        partial_rotary_factor=0.5,\n         head_dim=128,\n         hidden_act=\"silu\",\n         attention_dropout=0.0,\n@@ -114,6 +116,7 @@ def __init__(\n         self.intermediate_size = intermediate_size\n         self.num_hidden_layers = num_hidden_layers\n         self.num_attention_heads = num_attention_heads\n+        self.partial_rotary_factor = partial_rotary_factor\n         self.head_dim = head_dim\n         self.num_key_value_heads = num_key_value_heads\n         self.hidden_act = hidden_act"
        },
        {
            "sha": "1053f984d7f053f0d4f7ec4c3540542a4be0ca24",
            "filename": "src/transformers/models/glm/convert_glm_weights_to_hf.py",
            "status": "modified",
            "additions": 46,
            "deletions": 25,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a45617887613a914ff850b6bc7b82484bc10663/src%2Ftransformers%2Fmodels%2Fglm%2Fconvert_glm_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a45617887613a914ff850b6bc7b82484bc10663/src%2Ftransformers%2Fmodels%2Fglm%2Fconvert_glm_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fconvert_glm_weights_to_hf.py?ref=5a45617887613a914ff850b6bc7b82484bc10663",
            "patch": "@@ -37,16 +37,28 @@\n # fmt: on\n \n \n-def merge_safetensors(input_dir: str):\n-    all_files = [os.path.join(input_dir, x) for x in os.listdir(input_dir) if x.endswith(\".safetensors\")]\n-    all_files = sorted(all_files, key=lambda x: int(x.rsplit(\"-\", 3)[1]))\n+def load_weights(input_dir: str):\n+    safetensor_files = [os.path.join(input_dir, x) for x in os.listdir(input_dir) if x.endswith(\".safetensors\")]\n+    bin_files = [os.path.join(input_dir, x) for x in os.listdir(input_dir) if x.endswith(\".bin\")]\n \n     all_weights = {}\n-    for file in all_files:\n-        tensors = load_file(file)\n-        all_weights.update(tensors)\n \n-    return all_weights\n+    if safetensor_files:\n+        safetensor_files = sorted(safetensor_files, key=lambda x: int(x.rsplit(\"-\", 3)[1]))\n+        for file in safetensor_files:\n+            tensors = load_file(file)\n+            all_weights.update(tensors)\n+        return all_weights\n+\n+    elif bin_files:\n+        bin_files = sorted(bin_files, key=lambda x: int(x.rsplit(\"-\", 3)[1]))\n+        for file in bin_files:\n+            tensors = torch.load(file, map_location=\"cpu\")\n+            all_weights.update(tensors)\n+        return all_weights\n+\n+    else:\n+        raise ValueError(\"No .safetensors or .bin files found in the specified directory.\")\n \n \n def map_old_key_to_new(old_key):\n@@ -100,7 +112,8 @@ def convert_config(original_config: dict):\n         \"attention_bias\": \"add_qkv_bias\",\n     }\n     similar_keys_to_keep = [\n-        \"num_attention_heads\" \"hidden_size\",\n+        \"num_attention_heads\",\n+        \"hidden_size\",\n         \"attention_dropout\",\n         \"use_cache\",\n         \"eos_token_id\",\n@@ -120,40 +133,43 @@ def convert_config(original_config: dict):\n     return new_config\n \n \n-def convert_glm_tokenizer(input_dir):\n+def convert_glm_tokenizer(input_dir, use_post_processor=False):\n     fast_tok = PreTrainedTokenizerFast.from_pretrained(input_dir, model_input_names=[\"input_ids\", \"attention_mask\"])\n-    # Add the two tokens automatically with post processor\n-    fast_tok._tokenizer.post_processor = processors.Sequence(\n-        [\n-            processors.ByteLevel(trim_offsets=False),\n-            processors.TemplateProcessing(\n-                single=\"[gMASK]:0 <sop>:0 $A:0\",\n-                pair=\"[gMASK]:0 <sop>:0 $A:0 $B:1\",\n-                special_tokens=[(\"[gMASK]\", 151331), (\"<sop>\", 151333)],\n-            ),\n-        ],\n-    )\n-\n+    if use_post_processor:\n+        fast_tok._tokenizer.post_processor = processors.Sequence(\n+            [\n+                processors.ByteLevel(trim_offsets=False),\n+                processors.TemplateProcessing(\n+                    single=\"[gMASK]:0 <sop>:0 $A:0\",\n+                    pair=\"[gMASK]:0 <sop>:0 $A:0 $B:1\",\n+                    special_tokens=[(\"[gMASK]\", 151331), (\"<sop>\", 151333)],\n+                ),\n+            ],\n+        )\n+    else:\n+        fast_tok._tokenizer.post_processor = processors.Sequence(\n+            [processors.ByteLevel(trim_offsets=False)],\n+        )\n     return fast_tok\n \n \n-def convert_glm_model(input_dir, output_dir):\n+def convert_glm_model(input_dir, output_dir, use_post_processor=False):\n     # Load and convert config\n     with open(os.path.join(input_dir, \"config.json\")) as f:\n         original_config = json.load(f)\n     config = convert_config(original_config)\n     config.save_pretrained(output_dir)\n \n     # Load and convert weights\n-    original_state_dict = merge_safetensors(input_dir)\n+    original_state_dict = load_weights(input_dir)\n     new_dict = convert_state_dict(original_state_dict, config)\n     with torch.device(\"meta\"):\n         model = GlmForCausalLM(config)\n     model.load_state_dict(new_dict, strict=True, assign=True)\n     model.save_pretrained(output_dir)\n \n     # Load and convert tokenizer\n-    tokenizer = convert_glm_tokenizer(input_dir)\n+    tokenizer = convert_glm_tokenizer(input_dir, use_post_processor)\n     tokenizer.save_pretrained(output_dir)\n \n \n@@ -169,6 +185,11 @@ def convert_glm_model(input_dir, output_dir):\n         type=str,\n         help=\"Location to write HF model and tokenizer\",\n     )\n+    parser.add_argument(\n+        \"--use_post_processor\",\n+        action=\"store_true\",\n+        help=\"Whether to apply post processor with special tokens\",\n+    )\n \n     args = parser.parse_args()\n-    convert_glm_model(args.input_dir, args.output_dir)\n+    convert_glm_model(args.input_dir, args.output_dir, args.use_post_processor)"
        },
        {
            "sha": "16a724f69464a96f4aea08a055f4029fa497e809",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a45617887613a914ff850b6bc7b82484bc10663/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a45617887613a914ff850b6bc7b82484bc10663/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=5a45617887613a914ff850b6bc7b82484bc10663",
            "patch": "@@ -169,13 +169,14 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     cos = cos[..., : cos.shape[-1] // 2].repeat_interleave(2, dim=-1)\n     sin = sin[..., : sin.shape[-1] // 2].repeat_interleave(2, dim=-1)\n \n-    # Keep half for later concatenation\n-    q, q_pass = q[..., : q.shape[-1] // 2], q[..., q.shape[-1] // 2 :]\n-    k, k_pass = k[..., : k.shape[-1] // 2], k[..., k.shape[-1] // 2 :]\n+    # Keep half or full tensor for later concatenation\n+    rotary_dim = cos.shape[-1]\n+    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]\n+    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]\n \n-    # Apply rotary embeddings on the first half\n-    q_embed = (q * cos) + (rotate_half(q) * sin)\n-    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    # Apply rotary embeddings on the first half or full tensor\n+    q_embed = (q_rot * cos) + (rotate_half(q_rot) * sin)\n+    k_embed = (k_rot * cos) + (rotate_half(k_rot) * sin)\n \n     # Concatenate back to full shape\n     q_embed = torch.cat([q_embed, q_pass], dim=-1)\n@@ -705,7 +706,9 @@ def __init__(self, config: GlmConfig):\n         )\n         self.norm = GlmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = GlmRotaryEmbedding(\n-            dim=config.head_dim // 2, max_position_embeddings=config.max_position_embeddings, base=config.rope_theta\n+            dim=int(config.head_dim * config.partial_rotary_factor),\n+            max_position_embeddings=config.max_position_embeddings,\n+            base=config.rope_theta,\n         )\n         self.gradient_checkpointing = False\n         if getattr(config, \"pretraining_tp\", 1) != 1:"
        },
        {
            "sha": "48605c15d30be33903709c6894e4f9c64714ceee",
            "filename": "src/transformers/models/glm/modular_glm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 7,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/5a45617887613a914ff850b6bc7b82484bc10663/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5a45617887613a914ff850b6bc7b82484bc10663/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodular_glm.py?ref=5a45617887613a914ff850b6bc7b82484bc10663",
            "patch": "@@ -95,13 +95,14 @@ def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n     cos = cos[..., : cos.shape[-1] // 2].repeat_interleave(2, dim=-1)\n     sin = sin[..., : sin.shape[-1] // 2].repeat_interleave(2, dim=-1)\n \n-    # Keep half for later concatenation\n-    q, q_pass = q[..., : q.shape[-1] // 2], q[..., q.shape[-1] // 2 :]\n-    k, k_pass = k[..., : k.shape[-1] // 2], k[..., k.shape[-1] // 2 :]\n+    # Keep half or full tensor for later concatenation\n+    rotary_dim = cos.shape[-1]\n+    q_rot, q_pass = q[..., :rotary_dim], q[..., rotary_dim:]\n+    k_rot, k_pass = k[..., :rotary_dim], k[..., rotary_dim:]\n \n-    # Apply rotary embeddings on the first half\n-    q_embed = (q * cos) + (rotate_half(q) * sin)\n-    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    # Apply rotary embeddings on the first half or full tensor\n+    q_embed = (q_rot * cos) + (rotate_half(q_rot) * sin)\n+    k_embed = (k_rot * cos) + (rotate_half(k_rot) * sin)\n \n     # Concatenate back to full shape\n     q_embed = torch.cat([q_embed, q_pass], dim=-1)\n@@ -152,7 +153,9 @@ def __init__(self, config: GlmConfig):\n         )\n         self.norm = GlmRMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n         self.rotary_emb = GlmRotaryEmbedding(\n-            dim=config.head_dim // 2, max_position_embeddings=config.max_position_embeddings, base=config.rope_theta\n+            dim=int(config.head_dim * config.partial_rotary_factor),\n+            max_position_embeddings=config.max_position_embeddings,\n+            base=config.rope_theta,\n         )\n         self.gradient_checkpointing = False\n "
        }
    ],
    "stats": {
        "total": 108,
        "additions": 69,
        "deletions": 39
    }
}