{
    "author": "yonigozlan",
    "message": "Support `auto_doctring` in Processors (#42101)\n\n* remove attributes and add all missing sub processors to their auto classes\n\n* remove all mentions of .attributes\n\n* cleanup\n\n* fix processor tests\n\n* fix modular\n\n* remove last attributes\n\n* fixup\n\n* fixes after merge\n\n* fix wrong tokenizer in auto florence2\n\n* fix missing audio_processor + nits\n\n* Override __init__ in NewProcessor and change hf-internal-testing-repo (temporarily)\n\n* fix auto tokenizer test\n\n* add init to markup_lm\n\n* update CustomProcessor in custom_processing\n\n* remove print\n\n* nit\n\n* fix test modeling owlv2\n\n* fix test_processing_layoutxlm\n\n* Fix owlv2, wav2vec2, markuplm, voxtral issues\n\n* add support for loading and saving multiple tokenizer natively\n\n* remove exclude_attributes from save_pretrained\n\n* Run slow v2 (#41914)\n\n* Super\n\n* Super\n\n* Super\n\n* Super\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Fix `detectron2` installation in docker files (#41975)\n\n* detectron2 - part 1\n\n* detectron2 - part 2\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Fix `autoawq[kernels]` installation in quantization docker file (#41978)\n\nfix autoawq[kernels]\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* add support for saving encoder only so any parakeet model can be loaded for inference (#41969)\n\n* add support for saving encoder only so any decoder model can be loaded\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* use convolution_bias\n\n* convert modular\n\n* convolution_bias in convertion script\n\n---------\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\n\n* Use indices as position_ids in modernebert (#41789)\n\n* Use indices as position_ids in modernebert\n\n* Move position_ids init to the branch\n\n* test tensor parallel: make tests for dense model more robust (#41968)\n\n* make test forward and backward more robust\n\n* refactor compile part of test tensor parallel\n\n* linting\n\n* pass rank around instead of calling it over and over\n\n* Run slow v2 (#41914)\n\n* Super\n\n* Super\n\n* Super\n\n* Super\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Fix `detectron2` installation in docker files (#41975)\n\n* detectron2 - part 1\n\n* detectron2 - part 2\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Fix `autoawq[kernels]` installation in quantization docker file (#41978)\n\nfix autoawq[kernels]\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* add support for saving encoder only so any parakeet model can be loaded for inference (#41969)\n\n* add support for saving encoder only so any decoder model can be loaded\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\n\n* use convolution_bias\n\n* convert modular\n\n* convolution_bias in convertion script\n\n---------\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\n\n---------\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\n\n* fix: dict[RopeParameters] to dict[str, RopeParameters] (#41963)\n\n* docs: add continuous batching page (#41847)\n\n* docs: add continuous batching page\n\n* docs(cb): add `generate_batch` example\n\n* docs(cb): add `opentelemtry` and `serving` section\n\n* feat: add `TODO` note about opentelemetry dependency\n\n* docs(cb): add supported features\n\n* docs(cb): add unsupported features\n\n* docs(cb): add `ContinuousBatchingManager` example\n\n* docs(cb): x reference CB in optimizing inference\n\n* Fix `torchcodec` version in quantization docker file (#41988)\n\ncheck\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* [kernels]Â Add Tests & CI for kernels (#41765)\n\n* first commit\n\n* add tests\n\n* add kernel config\n\n* add more tests\n\n* add ci\n\n* small fix\n\n* change branch name\n\n* update tests\n\n* nit\n\n* change test name\n\n* revert jobs\n\n* addressing review\n\n* reenable all jobs\n\n* address second review\n\n* Move the Mi355 to regular docker (#41989)\n\n* Move the Mi355 to regular docker\n\n* Disable gfx950 compilation for FA on AMD\n\n* More data in benchmarking (#41848)\n\n* Reduce scope of cross-generate\n\n* Rm generate_sall configs\n\n* Workflow benchmarks more\n\n* Prevent crash when FA is not installed\n\n* fix (CI): Refactor SSH runners (#41991)\n\n* Change ssh runner type\n\n* Add wait step to SSH runner workflow\n\n* Rename wait step to wait2 in ssh-runner.yml\n\n* Remove wait step from ssh-runner.yml\n\nRemoved the wait step from the SSH runner workflow.\n\n* Update runner type for single GPU A10 instance\n\n* Update SSH runner version to 1.90.3\n\n* Add sha256sum to ssh-runner workflow\n\n* Update runner type and remove unused steps\n\n* fix 3 failed test cases for video_llama_3 model on Intel XPU (#41931)\n\n* fix 3 failed test cases for video_llama_3 model on Intel XPU\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* update\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* adjust format\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* update code\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n---------\n\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\n\n* Integrate colqwen2.5 using colqwen2 modelling code (#40600)\n\n* adding option for 2.5\n\n* minor - arg in conversion script\n\n* getting started on modelling.py\n\n* minor - shouldve been using modular\n\n* adressing comments + fixing datatype/device _get method\n\n* minor\n\n* commiting suggestion\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* docs + first test\n\n* ruff fix\n\n* minor fix\n\n* ruff fix\n\n* model fix\n\n* model fix\n\n* fine-grained check, with a hardcoded score from the original Hf implementation.\n\n* minor ruff\n\n* update tests values with CI hardware\n\n* adding 2.5 to conversion script\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: Sahil Kabir <sahilkabir@Sahils-MacBook-Pro.local>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\n\n* Fixed wrong padding value in OWLv2 (#41938)\n\n* Update image_processing_owlv2_fast.py\n\nfixed padding value\n\n* fixed padding value\n\n* Change padding constant value from 0.5 to 0.0\n\n* Fixed missed padding value in modular_owlv2.py\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* Fix `run slow v2`: empty report when there is only one model (#42002)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* [kernels] change import time in KernelConfig (#42004)\n\n* change import time\n\n* style\n\n* DOC Fix typo in argument name: pseudoquant (#41994)\n\nThe correct argument name is pseudoquantization. Since there is no error\non passing wrong arguments name (which is arguably an anti-pattern),\nthis is difficult for users to debug.\n\n* Fix `torch+deepspeed` docker file (#41985)\n\n* fix\n\n* delete\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n* Correct syntax error in trainer.md (#42001)\n\nA comma is missing between two parameters in the signature of compute_loss function.\n\n* Reduce the number of benchmark in the CI (#42008)\n\nChanged how benchmark cfgs are chosen\n\n* Fix continuous batching tests (#42012)\n\n* Fix continuous batching tests\n\n* make fixup\n\n* add back `logging_dir` (#42013)\n\n* add back\n\n* Apply style fixes\n\n---------\n\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\n\n* Fix issue with from pretrained and kwargs in image processors (#41997)\n\n* accept kwargs in image proc from_pretrained\n\n* only use kwargs that are in cls.valid_kwargs\n\n* remove specific logic for _from_auto\n\n* add image_seq_length to Images_kwargs for backward compatibility\n\n* fix missing image kwargs in pix2struct\n\n* Fix default image_rows and image_cols initialization in Idefics3 and SmolVLM processors (#41871)\n\n* Fix default image_rows and image_cols initialization in Idefics3 and SmolVLM processors\n\n* Fix default initialization of image_rows and image_cols in Idefics3 and SmolVLM processors\n\n* Add GLPNImageProcessorFast  (#41725)\n\n* Add GLPNImageProcessorFast for torch backend\n\n* Address review feedback\n\n- Simplified to_dict() method\n- Keep tensors as torch instead of converting to numpy for heterogeneous shapes\n- Removed unnecessary shape guards in post_process_depth_estimation\n- Improved variable names (tgt -> target_size, d -> resized)\n- Removed unnecessary GLPNImageProcessorKwargs class\n\n* Address review feedback\n\n- Simplified to_dict() method\n- Keep tensors as torch instead of converting to numpy for heterogeneous shapes\n- Removed unnecessary shape guards in post_process_depth_estimation\n- Improved variable names (tgt -> target_size, d -> resized)\n- Removed unnecessary GLPNImageProcessorKwargs class\n\n* commits after 2nd review\n\n* Address all review feedback and add explicit batched test\n\n- Simplified to_dict() with descriptive variable names (d->output_dict)\n- Fixed resize operation: changed from crop to proper resize with interpolation\n- Added padding for heterogeneous batch shapes in both slow and fast processors\n- Fused rescale and normalize operations for efficiency\n- Improved all variable names (tgt->target_size, d->depth_4d->resized)\n- Added GLPNImageProcessorKwargs class in slow processor and imported in fast\n- Renamed test_equivalence_slow_fast to test_slow_fast_equivalence\n- Added explicit test_slow_fast_equivalence_batched test\n- All 20 tests passing\n\n* using padding from utils\n\n* simplify glpn image processor fast\n\n* fix docstring\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* add fuyu fast image processors (#41817)\n\n* added fast processor for fuyu (#36978)\n\n* updated docs for fuyu model (#36978)\n\n* updated test_image_processing  and image_processing_fuyu_fast\n\n* updated fuyu.md and image_processing_fuyu_fast (#36978)\n\n* updated test_image_processing_fuyu (#36978)\n\n* formatted image_processing_fuyu_fast and test_image_processing_fuyu (#36978)\n\n* updated tests and fuyu fast image processing (#36978)\n\n* Merge branch 'fuyu-fast-image-processors' of https://github.com/DeXtAr47-oss/transformers into fuyu-fast-image-processors\n\n* fixed format (#36978)\n\n* formatted files (#36978)\n\n* formatted files\n\n* revert unnecessary changes\n\n* clean up and process by group\n\n---------\n\nCo-authored-by: yonigozlan <yoni.gozlan@huggingface.co>\n\n* [kernels] Fix XPU layernorm kernel (#41583)\n\n* fix\n\n* add comment\n\n* better fix\n\n* style\n\n* Update src/transformers/modeling_utils.py\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\n\n* [v5] Deprecate Text2Text and related pipelines (#41996)\n\n* Deprecate Text2Text and related pipelines\n\n* Try a restructure\n\n* make fixup\n\n* logging -> logger\n\n* [FPQuant] MXFP8 and MXFP4 backwards support (#41897)\n\n* FP-Quant backwards\n\n* fp-quant v0.3.0 docker\n\n* availability version bump\n\n* fp_quant==0.3.1\n\n* fp_quant v0.3.2\n\n* add working auto_docstring for processors\n\n* add auto_docstring to processors first part\n\n* add auto_docstring to processors part 2\n\n* modifs after review\n\n* fully working auto_docstring and check_docstring with placeholder docstrings\n\n* Working check_docstrings for Typed dicts\n\n* Add recurring processor args to auto_docstring and add support for removing redundant docstring and placeholders\n\n* replace placeholders with real docstrings\n\n* fix copies\n\n* fixup\n\n* remove unwanted changes\n\n* fix unprotected imports\n\n* Fix unprotected imports\n\n* fix unprotected imports\n\n* Add __call__ to all docs of processors\n\n* nits docs\n\n---------\n\nSigned-off-by: nithinraok <nithinrao.koluguri@gmail.com>\nSigned-off-by: Liu, Kaixuan <kaixuan.liu@intel.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\nCo-authored-by: Nithin Rao <nithinrao.koluguri@gmail.com>\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>\nCo-authored-by: RÃ©mi Ouazan <83456801+remi-or@users.noreply.github.com>\nCo-authored-by: Ferdinand Mom <47445085+3outeille@users.noreply.github.com>\nCo-authored-by: Ryan Mullins <ryanmullins@google.com>\nCo-authored-by: Luc Georges <McPatate@users.noreply.github.com>\nCo-authored-by: Mohamed Mekkouri <93391238+MekkCyber@users.noreply.github.com>\nCo-authored-by: Guillaume LEGENDRE <glegendre01@gmail.com>\nCo-authored-by: kaixuanliu <kaixuan.liu@intel.com>\nCo-authored-by: Sahil Kabir <66221472+sahil-kabir@users.noreply.github.com>\nCo-authored-by: Sahil Kabir <sahilkabir@Sahils-MacBook-Pro.local>\nCo-authored-by: github-actions[bot] <github-actions[bot]@users.noreply.github.com>\nCo-authored-by: James <67161633+gjamesgoenawan@users.noreply.github.com>\nCo-authored-by: Benjamin Bossan <BenjaminBossan@users.noreply.github.com>\nCo-authored-by: Yacklin Wong <139425274+Yacklin@users.noreply.github.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>\nCo-authored-by: MilkClouds <claude@maum.ai>\nCo-authored-by: ARAVINDHAN T <arvindhant01@gmail.com>\nCo-authored-by: Pritam Das <79273068+DeXtAr47-oss@users.noreply.github.com>\nCo-authored-by: Andrei Panferov <andrei@panferov.org>",
    "sha": "c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
    "files": [
        {
            "sha": "3a4358525acc54d1f894ab891a49d1ec590b5858",
            "filename": "docs/source/en/model_doc/align.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Falign.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -166,6 +166,7 @@ for label, score in zip(candidate_labels, probs):\n ## AlignProcessor\n \n [[autodoc]] AlignProcessor\n+    - __call__\n \n ## AlignModel\n "
        },
        {
            "sha": "3c52b8dc2d7f69480622aacd360e76d67446400e",
            "filename": "docs/source/en/model_doc/altclip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faltclip.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -126,3 +126,4 @@ for label, prob in zip(labels, probs[0]):\n ## AltCLIPProcessor\n \n [[autodoc]] AltCLIPProcessor\n+    - __call__\n\\ No newline at end of file"
        },
        {
            "sha": "1d33981b334d3764b00be3396936b0b481205816",
            "filename": "docs/source/en/model_doc/aria.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faria.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -149,6 +149,7 @@ print(response)\n ## AriaProcessor\n \n [[autodoc]] AriaProcessor\n+    - __call__\n \n ## AriaTextConfig\n "
        },
        {
            "sha": "a57897d691e4016b597d50a5ba655f3ac263eb41",
            "filename": "docs/source/en/model_doc/audioflamingo3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Faudioflamingo3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Faudioflamingo3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faudioflamingo3.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -390,6 +390,7 @@ are forwarded, so you can tweak padding or tensor formats just like when calling\n ## AudioFlamingo3Processor\n \n [[autodoc]] AudioFlamingo3Processor\n+    - __call__\n \n ## AudioFlamingo3Encoder\n "
        },
        {
            "sha": "4f3a77007d6aff7b8a53ff3841c54777ea15cefc",
            "filename": "docs/source/en/model_doc/aya_vision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Faya_vision.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -260,6 +260,7 @@ print(processor.tokenizer.decode(generated[0], skip_special_tokens=True))\n ## AyaVisionProcessor\n \n [[autodoc]] AyaVisionProcessor\n+    - __call__\n \n ## AyaVisionConfig\n "
        },
        {
            "sha": "e2a260f8def76d0b14cb210b7b920143a7ebe52a",
            "filename": "docs/source/en/model_doc/blip-2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip-2.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -72,6 +72,7 @@ If you're interested in submitting a resource to be included here, please feel f\n ## Blip2Processor\n \n [[autodoc]] Blip2Processor\n+    - __call__\n \n ## Blip2VisionModel\n "
        },
        {
            "sha": "15e6474da44d3053c1bf0a941220ac0c55e42ede",
            "filename": "docs/source/en/model_doc/blip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fblip.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -99,6 +99,7 @@ Refer to this [notebook](https://github.com/huggingface/notebooks/blob/main/exam\n ## BlipProcessor\n \n [[autodoc]] BlipProcessor\n+    - __call__\n \n ## BlipImageProcessor\n "
        },
        {
            "sha": "3a9f1a707a4652069c5eff98ed607e329238b059",
            "filename": "docs/source/en/model_doc/chameleon.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchameleon.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -182,6 +182,7 @@ model = ChameleonForConditionalGeneration.from_pretrained(\n ## ChameleonProcessor\n \n [[autodoc]] ChameleonProcessor\n+    - __call__\n \n ## ChameleonImageProcessor\n "
        },
        {
            "sha": "5d1b71591d541f43427a357e66f85116fa3719ab",
            "filename": "docs/source/en/model_doc/chinese_clip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fchinese_clip.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -98,6 +98,7 @@ Currently, following scales of pretrained Chinese-CLIP models are available on \n ## ChineseCLIPProcessor\n \n [[autodoc]] ChineseCLIPProcessor\n+    - __call__\n \n ## ChineseCLIPModel\n "
        },
        {
            "sha": "4c2db140bd18094d8399b33b5820ae81a6f3c18e",
            "filename": "docs/source/en/model_doc/clap.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclap.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -79,6 +79,7 @@ print(f\"Text embeddings: {text_features}\")\n ## ClapProcessor\n \n [[autodoc]] ClapProcessor\n+    - __call__\n \n ## ClapModel\n "
        },
        {
            "sha": "c341a6368bf23b72c0f42dae6d61fa6aa036a73f",
            "filename": "docs/source/en/model_doc/clip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclip.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -119,6 +119,7 @@ print(f\"Most likely label: {most_likely_label} with probability: {probs[0][most_\n ## CLIPProcessor\n \n [[autodoc]] CLIPProcessor\n+    - __call__\n \n ## CLIPModel\n "
        },
        {
            "sha": "d305bf690a6389825846ef2b54c3597dcc0aca75",
            "filename": "docs/source/en/model_doc/clipseg.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fclipseg.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -84,6 +84,7 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n ## CLIPSegProcessor\n \n [[autodoc]] CLIPSegProcessor\n+    - __call__\n \n ## CLIPSegModel\n "
        },
        {
            "sha": "49771d1feca4ab3bbb714f05585f789231158b14",
            "filename": "docs/source/en/model_doc/cohere2_vision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -139,3 +139,4 @@ print(outputs)\n ## Cohere2VisionProcessor\n \n [[autodoc]] Cohere2VisionProcessor\n+    - __call__\n\\ No newline at end of file"
        },
        {
            "sha": "42c880ea092662994208d46143352ce3bc046bec",
            "filename": "docs/source/en/model_doc/colpali.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -164,6 +164,7 @@ print(scores)\n ## ColPaliProcessor\n \n [[autodoc]] ColPaliProcessor\n+    - __call__\n \n ## ColPaliForRetrieval\n "
        },
        {
            "sha": "51393122c1dabe7beb5eff965a6cbba4c2f4b58a",
            "filename": "docs/source/en/model_doc/colqwen2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolqwen2.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -189,6 +189,7 @@ processor = ColQwen2Processor.from_pretrained(model_name)\n ## ColQwen2Processor\n \n [[autodoc]] ColQwen2Processor\n+    - __call__\n \n ## ColQwen2ForRetrieval\n "
        },
        {
            "sha": "7c5502849b7eac7d355760fa03fcf3f82b39cf56",
            "filename": "docs/source/en/model_doc/deepseek_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -209,6 +209,7 @@ model = DeepseekVLForConditionalGeneration.from_pretrained(\n ## DeepseekVLProcessor\n \n [[autodoc]] DeepseekVLProcessor\n+    - __call__\n \n ## DeepseekVLImageProcessor\n "
        },
        {
            "sha": "35cf380f95ba75ddb14e09aa36a047775ee83d54",
            "filename": "docs/source/en/model_doc/deepseek_vl_hybrid.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdeepseek_vl_hybrid.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -208,6 +208,7 @@ model = DeepseekVLHybridForConditionalGeneration.from_pretrained(\n ## DeepseekVLHybridProcessor\n \n [[autodoc]] DeepseekVLHybridProcessor\n+    - __call__\n \n ## DeepseekVLHybridImageProcessor\n "
        },
        {
            "sha": "2da028e4665b014446bffa0ec1e2936ef2dcc7e4",
            "filename": "docs/source/en/model_doc/emu3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -155,6 +155,7 @@ for i, image in enumerate(images['pixel_values']):\n ## Emu3Processor\n \n [[autodoc]] Emu3Processor\n+    - __call__\n \n ## Emu3ImageProcessor\n "
        },
        {
            "sha": "71ffc1ba0a9706de69e3f70649479364eb22985c",
            "filename": "docs/source/en/model_doc/ernie4_5_vl_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_vl_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_vl_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fernie4_5_vl_moe.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -201,6 +201,7 @@ print(output_text)\n ## Ernie4_5_VL_MoeProcessor\n \n [[autodoc]] Ernie4_5_VL_MoeProcessor\n+    - __call__\n \n ## Ernie4_5_VL_MoeTextModel\n "
        },
        {
            "sha": "0fb792e0106c9fb0708e262184791fdb94a0894a",
            "filename": "docs/source/en/model_doc/flava.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflava.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -63,6 +63,7 @@ This model was contributed by [aps](https://huggingface.co/aps). The original co\n ## FlavaProcessor\n \n [[autodoc]] FlavaProcessor\n+    - __call__\n \n ## FlavaImageProcessor\n "
        },
        {
            "sha": "bc7dd1368e3d41ed584756cf0a05d52559f9abb5",
            "filename": "docs/source/en/model_doc/florence2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fflorence2.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -171,6 +171,7 @@ print(parsed_answer)\n ## Florence2Processor\n \n [[autodoc]] Florence2Processor\n+    - __call__\n \n ## Florence2Model\n "
        },
        {
            "sha": "381a6a4ff0f60a60a3542953c264752e91ff8b10",
            "filename": "docs/source/en/model_doc/gemma3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -243,6 +243,7 @@ visualizer(\"<img>What is shown in this image?\")\n ## Gemma3Processor\n \n [[autodoc]] Gemma3Processor\n+    - __call__\n \n ## Gemma3TextConfig\n "
        },
        {
            "sha": "edfde4041395ec3d3df1ffade19307eca82df6f7",
            "filename": "docs/source/en/model_doc/gemma3n.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma3n.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -161,6 +161,7 @@ echo -e \"Plants create energy through a process known as\" | transformers run --t\n ## Gemma3nProcessor\n \n [[autodoc]] Gemma3nProcessor\n+    - __call__\n \n ## Gemma3nTextConfig\n "
        },
        {
            "sha": "ab62530a438a1e20b156c0af5548617d2af063b3",
            "filename": "docs/source/en/model_doc/glm46v.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm46v.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm46v.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm46v.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -39,6 +39,7 @@ rendered properly in your Markdown viewer.\n ## Glm46VProcessor\n \n [[autodoc]] Glm46VProcessor\n+    - __call__\n \n ## Glm46VModel\n "
        },
        {
            "sha": "38e43ab5c5c8a5c63cfc89ce869691c41889e422",
            "filename": "docs/source/en/model_doc/glm4v.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -196,6 +196,7 @@ print(output_text)\n ## Glm4vProcessor\n \n [[autodoc]] Glm4vProcessor\n+    - __call__\n \n ## Glm4vVisionModel\n "
        },
        {
            "sha": "a895b778bedbc27ea8bc8b26fd0a230e1f324118",
            "filename": "docs/source/en/model_doc/glmasr.md",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fglmasr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fglmasr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglmasr.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,7 +16,7 @@ limitations under the License.\n âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n \n -->\n-*This model was released on {release_date} and added to Hugging Face Transformers on 2025-12-15.*\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-12-24.*\n \n \n # GlmAsr\n@@ -162,6 +162,7 @@ print(decoded_outputs)\n ## GlmAsrProcessor\n \n [[autodoc]] GlmAsrProcessor\n+    - __call__\n \n ## GlmAsrEncoder\n "
        },
        {
            "sha": "8deb85fb144c4a9c2bba19e1b1975ed10fb95149",
            "filename": "docs/source/en/model_doc/got_ocr2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgot_ocr2.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -281,6 +281,7 @@ alt=\"drawing\" width=\"600\"/>\n ## GotOcr2Processor\n \n [[autodoc]] GotOcr2Processor\n+    - __call__\n \n ## GotOcr2Model\n "
        },
        {
            "sha": "286d62bd93410572d9770e8150c44c896e714ceb",
            "filename": "docs/source/en/model_doc/granite_speech.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranite_speech.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -160,6 +160,7 @@ for i, transcription in enumerate(transcriptions):\n ## GraniteSpeechProcessor\n \n [[autodoc]] GraniteSpeechProcessor\n+    - __call__\n \n ## GraniteSpeechFeatureExtractor\n "
        },
        {
            "sha": "3c31817520307f6dd11a61576dd1fdfbdae6360f",
            "filename": "docs/source/en/model_doc/granitevision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgranitevision.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -85,6 +85,7 @@ This model was contributed by [Alexander Brooks](https://huggingface.co/abrooks9\n ## LlavaNextProcessor\n \n [[autodoc]] LlavaNextProcessor\n+    - __call__\n \n ## LlavaNextForConditionalGeneration\n "
        },
        {
            "sha": "bdae28b076c3e5b8c0d9d0fca7731afabd586573",
            "filename": "docs/source/en/model_doc/grounding-dino.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -114,6 +114,7 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n ## GroundingDinoProcessor\n \n [[autodoc]] GroundingDinoProcessor\n+    - __call__\n     - post_process_grounded_object_detection\n \n ## GroundingDinoConfig"
        },
        {
            "sha": "e2fef6c1e35cbb80a2afdf4dc2454f003e8e2616",
            "filename": "docs/source/en/model_doc/instructblip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finstructblip.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -57,6 +57,7 @@ The attributes can be obtained from model config, as `model.config.num_query_tok\n ## InstructBlipProcessor\n \n [[autodoc]] InstructBlipProcessor\n+    - __call__\n \n ## InstructBlipVisionModel\n "
        },
        {
            "sha": "e2a712697d1da22b479c2767d2fbd556f2bc5bdc",
            "filename": "docs/source/en/model_doc/internvl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Finternvl.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -348,6 +348,7 @@ This example showcases how to handle a batch of chat conversations with interlea\n ## InternVLProcessor\n \n [[autodoc]] InternVLProcessor\n+    - __call__\n \n ## InternVLVideoProcessor\n "
        },
        {
            "sha": "916592489b5fed2bb767c50a6e40b0886c3212d9",
            "filename": "docs/source/en/model_doc/janus.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fjanus.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -205,6 +205,7 @@ for i, image in enumerate(images['pixel_values']):\n ## JanusProcessor\n \n [[autodoc]] JanusProcessor\n+    - __call__\n \n ## JanusImageProcessor\n "
        },
        {
            "sha": "8fa5fd30c5eb3faf61b8e4e46de0ecd48294cd41",
            "filename": "docs/source/en/model_doc/kosmos2_5.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fkosmos2_5.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -224,6 +224,7 @@ print(generated_text[0])\n ## Kosmos2_5Processor\n \n [[autodoc]] Kosmos2_5Processor\n+    - __call__\n \n ## Kosmos2_5Model\n "
        },
        {
            "sha": "de77b984369d3963bbbd7f85bdf11b7092f21be3",
            "filename": "docs/source/en/model_doc/lfm2_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flfm2_vl.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -82,6 +82,7 @@ processor.batch_decode(outputs, skip_special_tokens=True)[0]\n ## Lfm2VlProcessor\n \n [[autodoc]] Lfm2VlProcessor\n+    - __call__\n \n ## Lfm2VlConfig\n "
        },
        {
            "sha": "6ccb3190e4854a4fc1bd9e74b4883a791f3d7039",
            "filename": "docs/source/en/model_doc/llama4.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllama4.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -416,6 +416,7 @@ model = Llama4ForConditionalGeneration.from_pretrained(\n ## Llama4Processor\n \n [[autodoc]] Llama4Processor\n+    - __call__\n \n ## Llama4ImageProcessorFast\n "
        },
        {
            "sha": "2adaa15db8f30f70448c8807bc4abbf34d44a8f0",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -250,6 +250,7 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n ## LlavaProcessor\n \n [[autodoc]] LlavaProcessor\n+    - __call__\n \n ## LlavaModel\n "
        },
        {
            "sha": "11a7a8dfe76cbdcb99f5c56efb95c304200ca9fc",
            "filename": "docs/source/en/model_doc/llava_next.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_next.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -206,6 +206,7 @@ print(processor.decode(output[0], skip_special_tokens=True))\n ## LlavaNextProcessor\n \n [[autodoc]] LlavaNextProcessor\n+    - __call__\n \n ## LlavaNextModel\n "
        },
        {
            "sha": "c271421862fe0b00f7110daf418829952d97156d",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -298,6 +298,7 @@ model = LlavaOnevisionForConditionalGeneration.from_pretrained(\n ## LlavaOnevisionProcessor\n \n [[autodoc]] LlavaOnevisionProcessor\n+    - __call__\n \n ## LlavaOnevisionImageProcessor\n "
        },
        {
            "sha": "e39eaace30e570e6dfa08400001383dc5d2bbabb",
            "filename": "docs/source/en/model_doc/mllama.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmllama.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -108,6 +108,7 @@ print(processor.decode(output[0], skip_special_tokens=True))\n ## MllamaProcessor\n \n [[autodoc]] MllamaProcessor\n+    - __call__\n \n ## MllamaImageProcessor\n "
        },
        {
            "sha": "8c4319710b9c66b2f4ffb76ee58b05aeb9476e7f",
            "filename": "docs/source/en/model_doc/musicgen.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -272,6 +272,7 @@ Tips:\n ## MusicgenProcessor\n \n [[autodoc]] MusicgenProcessor\n+    - __call__\n \n ## MusicgenModel\n "
        },
        {
            "sha": "5b3b8e3a777ec55938d333b099361ef4f757edc0",
            "filename": "docs/source/en/model_doc/musicgen_melody.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmusicgen_melody.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -266,6 +266,7 @@ Tips:\n ## MusicgenMelodyProcessor\n \n [[autodoc]] MusicgenMelodyProcessor\n+    - __call__\n     - get_unconditional_inputs\n \n ## MusicgenMelodyFeatureExtractor"
        },
        {
            "sha": "5aa94ad350c0318ada97231c1c92530076be1452",
            "filename": "docs/source/en/model_doc/omdet-turbo.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fomdet-turbo.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -164,6 +164,7 @@ Detected statue with confidence 0.2 at location [428.1, 205.5, 767.3, 759.5] in\n ## OmDetTurboProcessor\n \n [[autodoc]] OmDetTurboProcessor\n+    - __call__\n     - post_process_grounded_object_detection\n \n ## OmDetTurboForObjectDetection"
        },
        {
            "sha": "87a926e189069331cadc653df1db78017e699b6e",
            "filename": "docs/source/en/model_doc/oneformer.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Foneformer.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -85,6 +85,7 @@ The resource should ideally demonstrate something new instead of duplicating an\n ## OneFormerProcessor\n \n [[autodoc]] OneFormerProcessor\n+    - __call__\n \n ## OneFormerModel\n "
        },
        {
            "sha": "eacce1b30b385dd3414d8cf06d7bdec889e7b8dc",
            "filename": "docs/source/en/model_doc/ovis2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -107,3 +107,4 @@ with torch.inference_mode():\n ## Ovis2Processor\n \n [[autodoc]] Ovis2Processor\n+    - __call__\n\\ No newline at end of file"
        },
        {
            "sha": "cc4df07743161a1b1c9a9b089da3788026e7f539",
            "filename": "docs/source/en/model_doc/paddleocr_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaddleocr_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaddleocr_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaddleocr_vl.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -242,6 +242,7 @@ model = AutoModelForImageTextToText.from_pretrained(\"PaddlePaddle/PaddleOCR-VL\",\n ## PaddleOCRVLProcessor\n \n [[autodoc]] PaddleOCRVLProcessor\n+    - __call__\n \n ## PaddleOCRVisionTransformer\n "
        },
        {
            "sha": "638d5f47ebc2fd78069c75a3d0052eb68ca2cf9c",
            "filename": "docs/source/en/model_doc/paligemma.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpaligemma.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -175,6 +175,7 @@ visualizer(\"<img> What is in this image?\")\n ## PaliGemmaProcessor\n \n [[autodoc]] PaliGemmaProcessor\n+    - __call__\n \n ## PaliGemmaModel\n "
        },
        {
            "sha": "ca99d1e4cd6278e78feffc03deb8e63b5532e0b2",
            "filename": "docs/source/en/model_doc/perception_lm.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fperception_lm.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fperception_lm.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fperception_lm.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -48,6 +48,7 @@ The original code can be found [here](https://github.com/facebookresearch/percep\n ## PerceptionLMProcessor\n \n [[autodoc]] PerceptionLMProcessor\n+    - __call__\n \n ## PerceptionLMImageProcessorFast\n "
        },
        {
            "sha": "a52249640726399deddc8cf9381e151b91c809d1",
            "filename": "docs/source/en/model_doc/phi4_multimodal.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fphi4_multimodal.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -152,6 +152,7 @@ print(f'>>> Response\\n{response}')\n ## Phi4MultimodalProcessor\n \n [[autodoc]] Phi4MultimodalProcessor\n+    - __call__\n \n ## Phi4MultimodalAudioConfig\n "
        },
        {
            "sha": "5bb276999cbd5e18dde89bcb7fe306910eafe0de",
            "filename": "docs/source/en/model_doc/pix2struct.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpix2struct.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -59,6 +59,7 @@ The original code can be found [here](https://github.com/google-research/pix2str\n ## Pix2StructProcessor\n \n [[autodoc]] Pix2StructProcessor\n+    - __call__\n \n ## Pix2StructImageProcessor\n "
        },
        {
            "sha": "53bf01cc5b1ed8ea21b173ae37a105a62c12526d",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -160,3 +160,4 @@ print(output)\n ## PixtralProcessor\n \n [[autodoc]] PixtralProcessor\n+    - __call__\n\\ No newline at end of file"
        },
        {
            "sha": "7b68c6283059305bd91398eb9d94d47e74c9c972",
            "filename": "docs/source/en/model_doc/qwen2_5_omni.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_omni.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -349,6 +349,7 @@ model = Qwen2_5OmniForConditionalGeneration.from_pretrained(\n ## Qwen2_5OmniProcessor\n \n [[autodoc]] Qwen2_5OmniProcessor\n+    - __call__\n \n ## Qwen2_5OmniForConditionalGeneration\n "
        },
        {
            "sha": "69317215731750328a24728c58fd0d969c1953db",
            "filename": "docs/source/en/model_doc/qwen2_5_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_5_vl.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -246,6 +246,7 @@ model = Qwen2_5_VLForConditionalGeneration.from_pretrained(\n ## Qwen2_5_VLProcessor\n \n [[autodoc]] Qwen2_5_VLProcessor\n+    - __call__\n \n ## Qwen2_5_VLTextModel\n "
        },
        {
            "sha": "6960f7f07b034cd32a153bd1cfc4e8c607db3e1e",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -302,6 +302,7 @@ model = Qwen2VLForConditionalGeneration.from_pretrained(\n ## Qwen2VLProcessor\n \n [[autodoc]] Qwen2VLProcessor\n+    - __call__\n \n ## Qwen2VLTextModel\n "
        },
        {
            "sha": "ee07ce1386760b2eedf9da3236b4b92266961489",
            "filename": "docs/source/en/model_doc/qwen3_omni_moe.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_omni_moe.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -389,6 +389,7 @@ model = Qwen3OmniMoeForConditionalGeneration.from_pretrained(\n ## Qwen3OmniMoeProcessor\n \n [[autodoc]] Qwen3OmniMoeProcessor\n+    - __call__\n \n ## Qwen3OmniMoeCode2Wav\n "
        },
        {
            "sha": "856c92cf98971979cb614d312b1eed4eb435693b",
            "filename": "docs/source/en/model_doc/qwen3_vl.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen3_vl.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -92,6 +92,7 @@ print(output_text)\n ## Qwen3VLProcessor\n \n [[autodoc]] Qwen3VLProcessor\n+    - __call__\n \n ## Qwen3VLVideoProcessor\n "
        },
        {
            "sha": "b770e41663e1c7721024bffabd9ddceece4a05a8",
            "filename": "docs/source/en/model_doc/sam.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -143,6 +143,7 @@ alt=\"drawing\" width=\"900\"/>\n ## SamProcessor\n \n [[autodoc]] SamProcessor\n+    - __call__\n \n ## SamImageProcessor\n "
        },
        {
            "sha": "a73d791f63e6bbff7865891b740ba6e6c9b3241d",
            "filename": "docs/source/en/model_doc/sam_hq.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsam_hq.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -132,6 +132,7 @@ A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to h\n ## SamHQProcessor\n \n [[autodoc]] SamHQProcessor\n+    - __call__\n \n ## SamHQVisionModel\n "
        },
        {
            "sha": "55a5d8bf57c8417e8fe82ce0ea24473ca7fa6ae7",
            "filename": "docs/source/en/model_doc/shieldgemma2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fshieldgemma2.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -89,6 +89,7 @@ print(output.probabilities)\n ## ShieldGemma2Processor\n \n [[autodoc]] ShieldGemma2Processor\n+    - __call__\n \n ## ShieldGemma2Config\n "
        },
        {
            "sha": "ba31f837d6d4c4b85ad6916355f64a064df5d7ea",
            "filename": "docs/source/en/model_doc/siglip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -160,6 +160,7 @@ print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n ## SiglipProcessor\n \n [[autodoc]] SiglipProcessor\n+    - __call__\n \n ## SiglipModel\n "
        },
        {
            "sha": "b8fa5d1256b4af168099fd5812eaad1cf6d8da8b",
            "filename": "docs/source/en/model_doc/siglip2.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -195,6 +195,7 @@ print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n ## Siglip2Processor\n \n [[autodoc]] Siglip2Processor\n+    - __call__\n \n ## Siglip2Model\n "
        },
        {
            "sha": "dc5c8f1c2a22a6ca986edbfac73d2181f24e7d44",
            "filename": "docs/source/en/model_doc/video_llama_3.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llama_3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llama_3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llama_3.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -212,6 +212,7 @@ model = VideoLlama3ForConditionalGeneration.from_pretrained(\n ## VideoLlama3Processor\n \n [[autodoc]] VideoLlama3Processor\n+    - __call__\n \n ## VideoLlama3Model\n "
        },
        {
            "sha": "24437684716fc92a23d468be883e574cd39f56e7",
            "filename": "docs/source/en/model_doc/video_llava.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvideo_llava.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -213,6 +213,7 @@ model = VideoLlavaForConditionalGeneration.from_pretrained(\n ## VideoLlavaProcessor\n \n [[autodoc]] VideoLlavaProcessor\n+    - __call__\n \n ## VideoLlavaModel\n "
        },
        {
            "sha": "d4ba9878bdbdac12ae9210de7d21b88b70e93d81",
            "filename": "docs/source/en/model_doc/vision-text-dual-encoder.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-text-dual-encoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-text-dual-encoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvision-text-dual-encoder.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -42,6 +42,7 @@ new zero-shot vision tasks such as image classification or retrieval.\n ## VisionTextDualEncoderProcessor\n \n [[autodoc]] VisionTextDualEncoderProcessor\n+    - __call__\n \n ## VisionTextDualEncoderModel\n "
        },
        {
            "sha": "b22bb18efab1ad418af1cada9ae446a575839f11",
            "filename": "docs/source/en/model_doc/voxtral.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fvoxtral.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -359,6 +359,7 @@ This model was contributed by [Eustache Le Bihan](https://huggingface.co/eustlb)\n ## VoxtralProcessor\n \n [[autodoc]] VoxtralProcessor\n+    - __call__\n \n ## VoxtralEncoder\n "
        },
        {
            "sha": "c0a9cdc0ab6fb1de156fd6284a6d3019c0e0fc5c",
            "filename": "docs/source/en/model_doc/xclip.md",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fxclip.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/docs%2Fsource%2Fen%2Fmodel_doc%2Fxclip.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fxclip.md?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -53,6 +53,7 @@ If you're interested in submitting a resource to be included here, please feel f\n ## XCLIPProcessor\n \n [[autodoc]] XCLIPProcessor\n+    - __call__\n \n ## XCLIPConfig\n "
        },
        {
            "sha": "fa15fcce3de6cc52da5f37f541ffa0fa6cbcd651",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 2,
            "deletions": 29,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,6 +16,7 @@\n \"\"\"\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+from ...utils import auto_docstring\n \n \n class AlignProcessorKwargs(ProcessingKwargs, total=False):\n@@ -28,36 +29,8 @@ class AlignProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class AlignProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs an ALIGN processor which wraps [`EfficientNetImageProcessor`] and\n-    [`BertTokenizer`]/[`BertTokenizerFast`] into a single processor that inherits both the image processor and\n-    tokenizer functionalities. See the [`~AlignProcessor.__call__`] and [`~OwlViTProcessor.decode`] for more\n-    information.\n-    The preferred way of passing kwargs is as a dictionary per modality, see usage example below.\n-        ```python\n-        from transformers import AlignProcessor\n-        from PIL import Image\n-        model_id = \"kakaobrain/align-base\"\n-        processor = AlignProcessor.from_pretrained(model_id)\n-\n-        processor(\n-            images=your_pil_image,\n-            text=[\"What is that?\"],\n-            images_kwargs = {\"crop_size\": {\"height\": 224, \"width\": 224}},\n-            text_kwargs = {\"padding\": \"do_not_pad\"},\n-            common_kwargs = {\"return_tensors\": \"pt\"},\n-        )\n-        ```\n-\n-    Args:\n-        image_processor ([`EfficientNetImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`BertTokenizer`, `BertTokenizerFast`]):\n-            The tokenizer is a required input.\n-\n-    \"\"\"\n-\n     valid_processor_kwargs = AlignProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):"
        },
        {
            "sha": "78fd31be5f227bfc9eef107ce74c0c88ff0dbbec",
            "filename": "src/transformers/models/altclip/processing_altclip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fprocessing_altclip.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,23 +16,11 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class AltCLIPProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a AltCLIP processor which wraps a CLIP image processor and a XLM-Roberta tokenizer into a single\n-    processor.\n-\n-    [`AltCLIPProcessor`] offers all the functionalities of [`CLIPImageProcessor`] and [`XLMRobertaTokenizerFast`]. See\n-    the [`~AltCLIPProcessor.__call__`] and [`~AltCLIPProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`CLIPImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`XLMRobertaTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "01ff43cef5313f6aff2c44feef8ab9269dad74da",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 22,
            "deletions": 27,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -878,6 +878,21 @@ def get_number_of_image_patches(self, height: int, width: int, images_kwargs=Non\n \n \n class AriaImagesKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    split_image (`bool`, *optional*, defaults to `False`):\n+        Whether to split large images into multiple crops. When enabled, images exceeding the maximum size are\n+        divided into overlapping crops that are processed separately and then combined. This allows processing\n+        of very high-resolution images that exceed the model's input size limits.\n+    max_image_size (`int`, *optional*, defaults to `980`):\n+        Maximum image size (in pixels) for a single image crop. Images larger than this will be split into\n+        multiple crops when `split_image=True`, or resized if splitting is disabled. This parameter controls\n+        the maximum resolution of individual image patches processed by the model.\n+    min_image_size (`int`, *optional*):\n+        Minimum image size (in pixels) for a single image crop. Images smaller than this will be upscaled to\n+        meet the minimum requirement. If not specified, images are processed at their original size (subject\n+        to the maximum size constraint).\n+    \"\"\"\n+\n     split_image: bool\n     max_image_size: int\n     min_image_size: int\n@@ -899,28 +914,19 @@ class AriaProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class AriaProcessor(ProcessorMixin):\n-    \"\"\"\n-    AriaProcessor is a processor for the Aria model which wraps the Aria image preprocessor and the LLama slow tokenizer.\n-\n-    Args:\n-        image_processor (`AriaImageProcessor`, *optional*):\n-            The AriaImageProcessor to use for image preprocessing.\n-        tokenizer (`PreTrainedTokenizerBase`, *optional*):\n-            An instance of [`PreTrainedTokenizerBase`]. This should correspond with the model's text model. The tokenizer is a required input.\n-        chat_template (`str`, *optional*):\n-            A Jinja template which will be used to convert lists of messages in a chat into a tokenizable string.\n-        size_conversion (`Dict`, *optional*):\n-            A dictionary indicating size conversions for images.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n         tokenizer: AutoTokenizer | str = None,\n         chat_template: str | None = None,\n         size_conversion: dict[float | int, int] | None = None,\n     ):\n+        r\"\"\"\n+        size_conversion (`Dict`, *optional*):\n+            A dictionary indicating size conversions for images.\n+        \"\"\"\n         if size_conversion is None:\n             size_conversion = {490: 128, 980: 256}\n         self.size_conversion = {int(k): v for k, v in size_conversion.items()}\n@@ -932,25 +938,14 @@ def __init__(\n \n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput],\n         images: ImageInput | None = None,\n         **kwargs: Unpack[AriaProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s).\n-\n-        Args:\n-            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`ImageInput`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n             - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`."
        },
        {
            "sha": "c712c6ef585fae92d5dd17cecee0edf289e8fc03",
            "filename": "src/transformers/models/aria/processing_aria.py",
            "status": "modified",
            "additions": 23,
            "deletions": 28,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fprocessing_aria.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -24,11 +24,26 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_python import PreTokenizedInput, TextInput\n-from ...utils import TensorType\n+from ...utils import TensorType, auto_docstring\n from ..auto import AutoTokenizer\n \n \n class AriaImagesKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    split_image (`bool`, *optional*, defaults to `False`):\n+        Whether to split large images into multiple crops. When enabled, images exceeding the maximum size are\n+        divided into overlapping crops that are processed separately and then combined. This allows processing\n+        of very high-resolution images that exceed the model's input size limits.\n+    max_image_size (`int`, *optional*, defaults to `980`):\n+        Maximum image size (in pixels) for a single image crop. Images larger than this will be split into\n+        multiple crops when `split_image=True`, or resized if splitting is disabled. This parameter controls\n+        the maximum resolution of individual image patches processed by the model.\n+    min_image_size (`int`, *optional*):\n+        Minimum image size (in pixels) for a single image crop. Images smaller than this will be upscaled to\n+        meet the minimum requirement. If not specified, images are processed at their original size (subject\n+        to the maximum size constraint).\n+    \"\"\"\n+\n     split_image: bool\n     max_image_size: int\n     min_image_size: int\n@@ -50,28 +65,19 @@ class AriaProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class AriaProcessor(ProcessorMixin):\n-    \"\"\"\n-    AriaProcessor is a processor for the Aria model which wraps the Aria image preprocessor and the LLama slow tokenizer.\n-\n-    Args:\n-        image_processor (`AriaImageProcessor`, *optional*):\n-            The AriaImageProcessor to use for image preprocessing.\n-        tokenizer (`PreTrainedTokenizerBase`, *optional*):\n-            An instance of [`PreTrainedTokenizerBase`]. This should correspond with the model's text model. The tokenizer is a required input.\n-        chat_template (`str`, *optional*):\n-            A Jinja template which will be used to convert lists of messages in a chat into a tokenizable string.\n-        size_conversion (`Dict`, *optional*):\n-            A dictionary indicating size conversions for images.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n         tokenizer: AutoTokenizer | str = None,\n         chat_template: str | None = None,\n         size_conversion: dict[float | int, int] | None = None,\n     ):\n+        r\"\"\"\n+        size_conversion (`Dict`, *optional*):\n+            A dictionary indicating size conversions for images.\n+        \"\"\"\n         if size_conversion is None:\n             size_conversion = {490: 128, 980: 256}\n         self.size_conversion = {int(k): v for k, v in size_conversion.items()}\n@@ -83,25 +89,14 @@ def __init__(\n \n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput],\n         images: ImageInput | None = None,\n         **kwargs: Unpack[AriaProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s).\n-\n-        Args:\n-            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`ImageInput`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n             - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`."
        },
        {
            "sha": "054f79209daaa679a55101d8d2218a1a6325e985",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -177,6 +177,8 @@\n             (\"sam2\", (None, \"Sam2ImageProcessorFast\")),\n             (\"sam2_video\", (None, \"Sam2ImageProcessorFast\")),\n             (\"sam3\", (None, \"Sam3ImageProcessorFast\")),\n+            (\"sam3_tracker\", (None, \"Sam3ImageProcessorFast\")),\n+            (\"sam3_tracker_video\", (None, \"Sam3ImageProcessorFast\")),\n             (\"sam3_video\", (None, \"Sam3ImageProcessorFast\")),\n             (\"sam_hq\", (\"SamImageProcessor\", \"SamImageProcessorFast\")),\n             (\"segformer\", (\"SegformerImageProcessor\", \"SegformerImageProcessorFast\")),"
        },
        {
            "sha": "b5c3a379cfd7a08d16142f6e2a7b887e4953be5d",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -265,6 +265,8 @@\n         (\"roc_bert\", \"RoCBertTokenizer\"),\n         (\"roformer\", \"RoFormerTokenizer\" if is_tokenizers_available() else None),\n         (\"rwkv\", \"GPTNeoXTokenizer\" if is_tokenizers_available() else None),\n+        (\"sam3\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n+        (\"sam3_video\", \"CLIPTokenizer\" if is_tokenizers_available() else None),\n         (\"seamless_m4t\", \"SeamlessM4TTokenizer\" if is_tokenizers_available() else None),\n         (\"seamless_m4t_v2\", \"SeamlessM4TTokenizer\" if is_tokenizers_available() else None),\n         (\"shieldgemma2\", \"GemmaTokenizer\" if is_tokenizers_available() else None),"
        },
        {
            "sha": "4fdc5dca70a883d27ddb256b61b30cfb7da28042",
            "filename": "src/transformers/models/aya_vision/processing_aya_vision.py",
            "status": "modified",
            "additions": 23,
            "deletions": 49,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fprocessing_aya_vision.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,6 +20,7 @@\n from ...image_utils import ImageInput, make_flat_list_of_images\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class AyaVisionProcessorKwargs(ProcessingKwargs, total=False):\n@@ -35,16 +36,26 @@ class AyaVisionProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class AyaVisionProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a AyaVision processor which wraps a [`AutoImageProcessor`] and\n-    [`PretrainedTokenizerFast`] tokenizer into a single processor that inherits both the image processor and\n-    tokenizer functionalities. See the [`~AyaVisionProcessor.__call__`] and [`~AyaVisionProcessor.decode`] for more information.\n-    Args:\n-        image_processor ([`AutoImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n+    def __init__(\n+        self,\n+        image_processor=None,\n+        tokenizer=None,\n+        patch_size: int = 28,\n+        img_size: int = 364,\n+        image_token=\"<image>\",  # set the default and let users change if they have peculiar special tokens in rare cases\n+        downsample_factor: int = 1,\n+        start_of_img_token=\"<|START_OF_IMG|>\",\n+        end_of_img_token=\"<|END_OF_IMG|>\",\n+        img_patch_token=\"<|IMG_PATCH|>\",\n+        img_line_break_token=\"<|IMG_LINE_BREAK|>\",\n+        tile_token=\"TILE\",\n+        tile_global_token=\"TILE_GLOBAL\",\n+        chat_template=None,\n+        **kwargs,\n+    ):\n+        r\"\"\"\n         patch_size (`int`, *optional*, defaults to 28):\n             The size of image patches for tokenization.\n         img_size (`int`, *optional*, defaults to 364):\n@@ -65,27 +76,7 @@ class AyaVisionProcessor(ProcessorMixin):\n             The token to be used to represent an image patch in the text.\n         tile_global_token (`str`, *optional*, defaults to `\"TILE_GLOBAL\"`):\n             The token to be used to represent the cover image in the text.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n-    def __init__(\n-        self,\n-        image_processor=None,\n-        tokenizer=None,\n-        patch_size: int = 28,\n-        img_size: int = 364,\n-        image_token=\"<image>\",  # set the default and let users change if they have peculiar special tokens in rare cases\n-        downsample_factor: int = 1,\n-        start_of_img_token=\"<|START_OF_IMG|>\",\n-        end_of_img_token=\"<|END_OF_IMG|>\",\n-        img_patch_token=\"<|IMG_PATCH|>\",\n-        img_line_break_token=\"<|IMG_LINE_BREAK|>\",\n-        tile_token=\"TILE\",\n-        tile_global_token=\"TILE_GLOBAL\",\n-        chat_template=None,\n-        **kwargs,\n-    ):\n+        \"\"\"\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n         self.image_token = image_token\n@@ -124,31 +115,14 @@ def _prompt_split_image(self, num_patches):\n         img_string += f\"{self.end_of_img_token}\"\n         return img_string\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         **kwargs: Unpack[AyaVisionProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text.\n-        To prepare the vision inputs, this method forwards the `images` and `kwargs` arguments to\n-        GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "1f3f9504a875bc8b6ee07b35240c55b9edb8b8ce",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -54,17 +54,16 @@ class BambaFlashAttentionKwargs(TypedDict, total=False):\n     Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n     Use cases include padding-free training and fewer `torch.compile` graph breaks.\n \n-    Attributes:\n-        cu_seq_lens_q (`torch.LongTensor`)\n-            Gets cumulative sequence length for query state.\n-        cu_seq_lens_k (`torch.LongTensor`)\n-            Gets cumulative sequence length for key state.\n-        max_length_q (`int`):\n-            Maximum sequence length for query state.\n-        max_length_k (`int`):\n-            Maximum sequence length for key state.\n-        seq_idx (`torch.IntTensor):\n-            Index of each packed sequence.\n+    cu_seq_lens_q (`torch.LongTensor`):\n+        Gets cumulative sequence length for query state.\n+    cu_seq_lens_k (`torch.LongTensor`):\n+        Gets cumulative sequence length for key state.\n+    max_length_q (`int`):\n+        Maximum sequence length for query state.\n+    max_length_k (`int`):\n+        Maximum sequence length for key state.\n+    seq_idx (`torch.IntTensor`):\n+        Index of each packed sequence.\n     \"\"\"\n \n     cu_seq_lens_q: torch.LongTensor"
        },
        {
            "sha": "1c15c916914a4baa4262c94e85efd9441f05a9db",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -63,17 +63,16 @@ class BambaFlashAttentionKwargs(TypedDict, total=False):\n     Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n     Use cases include padding-free training and fewer `torch.compile` graph breaks.\n \n-    Attributes:\n-        cu_seq_lens_q (`torch.LongTensor`)\n-            Gets cumulative sequence length for query state.\n-        cu_seq_lens_k (`torch.LongTensor`)\n-            Gets cumulative sequence length for key state.\n-        max_length_q (`int`):\n-            Maximum sequence length for query state.\n-        max_length_k (`int`):\n-            Maximum sequence length for key state.\n-        seq_idx (`torch.IntTensor):\n-            Index of each packed sequence.\n+    cu_seq_lens_q (`torch.LongTensor`):\n+        Gets cumulative sequence length for query state.\n+    cu_seq_lens_k (`torch.LongTensor`):\n+        Gets cumulative sequence length for key state.\n+    max_length_q (`int`):\n+        Maximum sequence length for query state.\n+    max_length_k (`int`):\n+        Maximum sequence length for key state.\n+    seq_idx (`torch.IntTensor`):\n+        Index of each packed sequence.\n     \"\"\"\n \n     cu_seq_lens_q: torch.LongTensor"
        },
        {
            "sha": "d364f669e1f50a5670c8f58eb1cf72119e8b91ce",
            "filename": "src/transformers/models/bark/processing_bark.py",
            "status": "modified",
            "additions": 17,
            "deletions": 37,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fprocessing_bark.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -24,37 +24,31 @@\n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n from ...utils.hub import cached_file\n from ..auto import AutoTokenizer\n \n \n logger = logging.get_logger(__name__)\n \n \n+@auto_docstring\n class BarkProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Bark processor which wraps a text tokenizer and optional Bark voice presets into a single processor.\n-\n-    Args:\n-        tokenizer ([`PreTrainedTokenizer`]):\n-            An instance of [`PreTrainedTokenizer`].\n-        speaker_embeddings (`dict[dict[str]]`, *optional*):\n-            Optional nested speaker embeddings dictionary. The first level contains voice preset names (e.g\n-            `\"en_speaker_4\"`). The second level contains `\"semantic_prompt\"`, `\"coarse_prompt\"` and `\"fine_prompt\"`\n-            embeddings. The values correspond to the path of the corresponding `np.ndarray`. See\n-            [here](https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c) for\n-            a list of `voice_preset_names`.\n-\n-    \"\"\"\n-\n     preset_shape = {\n         \"semantic_prompt\": 1,  # 1D array of shape (X,)\n         \"coarse_prompt\": 2,  # 2D array of shape (2,X)\n         \"fine_prompt\": 2,  # 2D array of shape (8,X)\n     }\n \n     def __init__(self, tokenizer, speaker_embeddings=None):\n+        r\"\"\"\n+        speaker_embeddings (`dict[dict[str]]`, *optional*):\n+            Optional nested speaker embeddings dictionary. The first level contains voice preset names (e.g\n+            `\"en_speaker_4\"`). The second level contains `\"semantic_prompt\"`, `\"coarse_prompt\"` and `\"fine_prompt\"`\n+            embeddings. The values correspond to the path of the corresponding `np.ndarray`. See\n+            [here](https://suno-ai.notion.site/8b8e8749ed514b0cbf3f699013548683?v=bc67cff786b04b50b3ceb756fd05f68c) for\n+            a list of `voice_preset_names`.\n+        \"\"\"\n         super().__init__(tokenizer)\n \n         self.speaker_embeddings = speaker_embeddings\n@@ -259,6 +253,7 @@ def _verify_speaker_embeddings(self, remove_unavailable: bool = True):\n                 for voice_preset in unavailable_keys:\n                     del self.speaker_embeddings[voice_preset]\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text=None,\n@@ -270,27 +265,12 @@ def __call__(\n         return_token_type_ids=False,\n         **kwargs,\n     ) -> BatchEncoding:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s). This method forwards the `text` and `kwargs`\n-        arguments to the AutoTokenizer's [`~AutoTokenizer.__call__`] to encode the text. The method also proposes a\n-        voice preset which is a dictionary of arrays that conditions `Bark`'s output. `kwargs` arguments are forwarded\n-        to the tokenizer and to `cached_file` method if `voice_preset` is a valid filename.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            voice_preset (`str`, `dict[np.ndarray]`):\n-                The voice preset, i.e the speaker embeddings. It can either be a valid voice_preset name, e.g\n-                `\"en_speaker_1\"`, or directly a dictionary of `np.ndarray` embeddings for each submodel of `Bark`. Or\n-                it can be a valid file name of a local `.npz` single voice preset containing the keys\n-                `\"semantic_prompt\"`, `\"coarse_prompt\"` and `\"fine_prompt\"`.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n+        r\"\"\"\n+        voice_preset (`str`, `dict[np.ndarray]`):\n+            The voice preset, i.e the speaker embeddings. It can either be a valid voice_preset name, e.g\n+            `\"en_speaker_1\"`, or directly a dictionary of `np.ndarray` embeddings for each submodel of `Bark`. Or\n+            it can be a valid file name of a local `.npz` single voice preset containing the keys\n+            `\"semantic_prompt\"`, `\"coarse_prompt\"` and `\"fine_prompt\"`.\n \n         Returns:\n             [`BatchEncoding`]: A [`BatchEncoding`] object containing the output of the `tokenizer`."
        },
        {
            "sha": "30ef6c59357238cce72d6bfb7aa812fa8e78528a",
            "filename": "src/transformers/models/blip/processing_blip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 31,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fprocessing_blip.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,6 +20,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class BlipProcessorKwargs(ProcessingKwargs, total=False):\n@@ -38,48 +39,19 @@ class BlipProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class BlipProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a BLIP processor which wraps a BERT tokenizer and BLIP image processor into a single processor.\n-\n-    [`BlipProcessor`] offers all the functionalities of [`BlipImageProcessor`] and [`BertTokenizerFast`]. See the\n-    docstring of [`~BlipProcessor.__call__`] and [`~BlipProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`BlipImageProcessor`):\n-            An instance of [`BlipImageProcessor`]. The image processor is a required input.\n-        tokenizer (`BertTokenizerFast`):\n-            An instance of ['BertTokenizerFast`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor, tokenizer, **kwargs):\n         tokenizer.return_token_type_ids = False\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         **kwargs: Unpack[BlipProcessorKwargs],\n     ) -> BatchEncoding:\n-        \"\"\"\n-        This method uses [`BlipImageProcessor.__call__`] method to prepare image(s) for the model, and\n-        [`BertTokenizerFast.__call__`] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        Args:\n-            images (`ImageInput`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                    - `'np'`: Return NumPy `np.ndarray` objects.\n-        \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You have to specify either images or text.\")\n "
        },
        {
            "sha": "e035637d86365de1623a460ecc236c74425798c2",
            "filename": "src/transformers/models/blip_2/processing_blip_2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 33,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fprocessing_blip_2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -21,7 +21,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AddedToken, BatchEncoding, PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -43,23 +43,13 @@ class Blip2ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Blip2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a BLIP-2 processor which wraps a BLIP image processor and an OPT/T5 tokenizer into a single processor.\n-\n-    [`BlipProcessor`] offers all the functionalities of [`BlipImageProcessor`] and [`AutoTokenizer`]. See the docstring\n-    of [`~BlipProcessor.__call__`] and [`~BlipProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`BlipImageProcessor`):\n-            An instance of [`BlipImageProcessor`]. The image processor is a required input.\n-        tokenizer (`AutoTokenizer`):\n-            An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n+    def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n+        r\"\"\"\n         num_query_tokens (`int`, *optional*):\n             Number of tokens used by the Qformer as queries, should be same as in model's config.\n-    \"\"\"\n-\n-    def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n+        \"\"\"\n         tokenizer.return_token_type_ids = False\n         if not hasattr(tokenizer, \"image_token\"):\n             self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n@@ -70,30 +60,13 @@ def __init__(self, image_processor, tokenizer, num_query_tokens=None, **kwargs):\n \n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         **kwargs: Unpack[Blip2ProcessorKwargs],\n     ) -> BatchEncoding:\n-        \"\"\"\n-        This method uses [`BlipImageProcessor.__call__`] method to prepare image(s) for the model, and\n-        [`BertTokenizerFast.__call__`] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        Args:\n-            images (`ImageInput`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                    - `'np'`: Return NumPy `np.ndarray` objects.\n-        \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You have to specify either images or text.\")\n         output_kwargs = self._merge_kwargs("
        },
        {
            "sha": "aa0ea7b4c4daa66bffd3757430a252ae7e60081e",
            "filename": "src/transformers/models/bridgetower/processing_bridgetower.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fprocessing_bridgetower.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,6 +16,7 @@\n \"\"\"\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+from ...utils import auto_docstring\n \n \n class BridgeTowerProcessorKwargs(ProcessingKwargs, total=False):\n@@ -37,22 +38,8 @@ class BridgeTowerProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class BridgeTowerProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a BridgeTower processor which wraps a Roberta tokenizer and BridgeTower image processor into a single\n-    processor.\n-\n-    [`BridgeTowerProcessor`] offers all the functionalities of [`BridgeTowerImageProcessor`] and\n-    [`RobertaTokenizerFast`]. See the docstring of [`~BridgeTowerProcessor.__call__`] and\n-    [`~BridgeTowerProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`BridgeTowerImageProcessor`):\n-            An instance of [`BridgeTowerImageProcessor`]. The image processor is a required input.\n-        tokenizer (`RobertaTokenizerFast`):\n-            An instance of ['RobertaTokenizerFast`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     valid_processor_kwargs = BridgeTowerProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):"
        },
        {
            "sha": "4cf108d1371d6bee229931f2936e7a3060fd6f85",
            "filename": "src/transformers/models/bros/processing_bros.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbros%2Fprocessing_bros.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,6 +16,7 @@\n \"\"\"\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+from ...utils import auto_docstring\n \n \n class BrosProcessorKwargs(ProcessingKwargs, total=False):\n@@ -33,18 +34,8 @@ class BrosProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class BrosProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Bros processor which wraps a BERT tokenizer.\n-\n-    [`BrosProcessor`] offers all the functionalities of [`BertTokenizerFast`]. See the docstring of\n-    [`~BrosProcessor.__call__`] and [`~BrosProcessor.decode`] for more information.\n-\n-    Args:\n-        tokenizer (`BertTokenizerFast`, *optional*):\n-            An instance of ['BertTokenizerFast`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     valid_processor_kwargs = BrosProcessorKwargs\n \n     def __init__(self, tokenizer=None, **kwargs):"
        },
        {
            "sha": "24b2b134be483b6c8b62d0e44e0c827955e8ddbe",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 14,
            "deletions": 36,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -29,9 +29,17 @@\n     Unpack,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class ChameleonTextKwargs(TextKwargs, total=False):\n+    \"\"\"\n+    return_for_text_completion (`bool`, *optional*, defaults to `False`):\n+        Whether the processed text is intended for text completion tasks. When `True`, the processor does not\n+        append the separator token (`sep_token`) to the end of the prompt, which is typically used for chat\n+        mode. When `False`, the separator token is appended for proper chat formatting.\n+    \"\"\"\n+\n     return_for_text_completion: bool\n \n \n@@ -49,26 +57,15 @@ class ChameleonProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class ChameleonProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Chameleon processor which wraps a Chameleon image processor and a Chameleon tokenizer into a single\n-    processor.\n-\n-    [`ChameleonProcessor`] offers all the functionalities of [`ChameleonImageProcessor`] and [`LlamaTokenizerFast`].\n-    See the [`~ChameleonProcessor.__call__`] and [`~ChameleonProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`ChameleonImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`]):\n-            The tokenizer is a required input.\n+    def __init__(self, image_processor, tokenizer, image_seq_length: int = 1024, image_token: str = \"<image>\"):\n+        r\"\"\"\n         image_seq_length (`int`, *optional*, defaults to 1024):\n             Sequence length of one image embedding.\n         image_token (`str`, *optional*, defaults to `\"<image>\"`):\n             The special token used to indicate image in the text.\n-    \"\"\"\n-\n-    def __init__(self, image_processor, tokenizer, image_seq_length: int = 1024, image_token: str = \"<image>\"):\n+        \"\"\"\n         self.image_seq_length = image_seq_length\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n         self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n@@ -83,33 +80,14 @@ def __init__(self, image_processor, tokenizer, image_seq_length: int = 1024, ima\n \n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         **kwargs: Unpack[ChameleonProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "71feda46e115a1f3620ac14be54df0afe7c6373d",
            "filename": "src/transformers/models/chinese_clip/processing_chinese_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fprocessing_chinese_clip.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,23 +16,11 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class ChineseCLIPProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Chinese-CLIP processor which wraps a Chinese-CLIP image processor and a Chinese-CLIP tokenizer into a\n-    single processor.\n-\n-    [`ChineseCLIPProcessor`] offers all the functionalities of [`ChineseCLIPImageProcessor`] and [`BertTokenizerFast`].\n-    See the [`~ChineseCLIPProcessor.__call__`] and [`~ChineseCLIPProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`ChineseCLIPImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`BertTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "567aa19ce60775e9f14ed220f4edecb910cf1c21",
            "filename": "src/transformers/models/clap/processing_clap.py",
            "status": "modified",
            "additions": 2,
            "deletions": 14,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fprocessing_clap.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,26 +16,14 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n+@auto_docstring\n class ClapProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a CLAP processor which wraps a CLAP feature extractor and a RoBerta tokenizer into a single processor.\n-\n-    [`ClapProcessor`] offers all the functionalities of [`ClapFeatureExtractor`] and [`RobertaTokenizerFast`]. See the\n-    [`~ClapProcessor.__call__`] and [`~ClapProcessor.decode`] for more information.\n-\n-    Args:\n-        feature_extractor ([`ClapFeatureExtractor`]):\n-            The audio processor is a required input.\n-        tokenizer ([`RobertaTokenizerFast`]):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n "
        },
        {
            "sha": "f69b275c48b248f1e35419ba4f174f5a75d04f21",
            "filename": "src/transformers/models/clip/processing_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fprocessing_clip.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,22 +16,11 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class CLIPProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a CLIP processor which wraps a CLIP image processor and a CLIP tokenizer into a single processor.\n-\n-    [`CLIPProcessor`] offers all the functionalities of [`CLIPImageProcessor`] and [`CLIPTokenizerFast`]. See the\n-    [`~CLIPProcessor.__call__`] and [`~CLIPProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`CLIPImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`AutoTokenizer`], *optional*):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "ad36ba4e14a28ead874a2b28f64e43f519f38680",
            "filename": "src/transformers/models/clipseg/processing_clipseg.py",
            "status": "modified",
            "additions": 8,
            "deletions": 38,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fprocessing_clipseg.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -17,51 +17,21 @@\n \n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class CLIPSegProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a CLIPSeg processor which wraps a CLIPSeg image processor and a CLIP tokenizer into a single processor.\n-\n-    [`CLIPSegProcessor`] offers all the functionalities of [`ViTImageProcessor`] and [`CLIPTokenizerFast`]. See the\n-    [`~CLIPSegProcessor.__call__`] and [`~CLIPSegProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`ViTImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`CLIPTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(self, text=None, images=None, visual_prompt=None, return_tensors=None, **kwargs):\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        ViTImageProcessor's [`~ViTImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring of\n-        the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            visual_prompt (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The visual prompt image or batch of images to be prepared. Each visual prompt image can be a PIL image,\n-                NumPy array or PyTorch tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape\n-                (C, H, W), where C is a number of channels, H and W are image height and width.\n-\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n+        r\"\"\"\n+        visual_prompt (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n+            The visual prompt image or batch of images to be prepared. Each visual prompt image can be a PIL image,\n+            NumPy array or PyTorch tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape\n+            (C, H, W), where C is a number of channels, H and W are image height and width.\n \n         Returns:\n             [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:"
        },
        {
            "sha": "2ca852653e6ea35739af57ad2e66640d10bad482",
            "filename": "src/transformers/models/clvp/processing_clvp.py",
            "status": "modified",
            "additions": 3,
            "deletions": 19,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fprocessing_clvp.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -17,35 +17,19 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n+@auto_docstring\n class ClvpProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a CLVP processor which wraps a CLVP Feature Extractor and a CLVP Tokenizer into a single processor.\n-\n-    [`ClvpProcessor`] offers all the functionalities of [`ClvpFeatureExtractor`] and [`ClvpTokenizer`]. See the\n-    [`~ClvpProcessor.__call__`], [`~ClvpProcessor.decode`] and [`~ClvpProcessor.batch_decode`] for more information.\n-\n-    Args:\n-        feature_extractor (`ClvpFeatureExtractor`):\n-            An instance of [`ClvpFeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer (`ClvpTokenizer`):\n-            An instance of [`ClvpTokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n+    @auto_docstring\n     def __call__(self, *args, **kwargs):\n-        \"\"\"\n-        Forwards the `audio` and `sampling_rate` arguments to [`~ClvpFeatureExtractor.__call__`] and the `text`\n-        argument to [`~ClvpTokenizer.__call__`]. Please refer to the docstring of the above two methods for more\n-        information.\n-        \"\"\"\n         raw_speech = kwargs.pop(\"raw_speech\", None)\n         if raw_speech is not None:\n             logger.warning("
        },
        {
            "sha": "1c01da32dda0969adcd34840a82de4a88ef27ed0",
            "filename": "src/transformers/models/cohere2_vision/processing_cohere2_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 32,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2_vision%2Fprocessing_cohere2_vision.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,6 +20,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class Cohere2VisionProcessorKwargs(ProcessingKwargs, total=False):\n@@ -32,20 +33,8 @@ class Cohere2VisionProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Cohere2VisionProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Cohere2Vision processor which wraps a [`AutoImageProcessor`] and\n-    [`PretrainedTokenizerFast`] tokenizer into a single processor that inherits both the image processor and\n-    tokenizer functionalities. See the [`~Cohere2VisionProcessor.__call__`] and [`~Cohere2VisionProcessor.decode`] for more information.\n-    Args:\n-        image_processor ([`AutoImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -71,31 +60,14 @@ def __init__(\n             ]\n         )\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         **kwargs: Unpack[Cohere2VisionProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text.\n-        To prepare the vision inputs, this method forwards the `images` and `kwargs` arguments to\n-        GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "dec2fee64dea644741529271e62cf3444f9f7d22",
            "filename": "src/transformers/models/colpali/modular_colpali.py",
            "status": "modified",
            "additions": 7,
            "deletions": 46,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodular_colpali.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -44,26 +44,6 @@ class ColPaliProcessorKwargs(ProcessingKwargs, total=False):\n \n \n class ColPaliProcessor(PaliGemmaProcessor):\n-    r\"\"\"\n-    Constructs a ColPali processor which wraps a PaliGemmaProcessor and special methods to process images and queries, as\n-    well as to compute the late-interaction retrieval score.\n-\n-    [`ColPaliProcessor`] offers all the functionalities of [`PaliGemmaProcessor`]. See the [`~PaliGemmaProcessor.__call__`]\n-    for more information.\n-\n-    Args:\n-        image_processor ([`SiglipImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        visual_prompt_prefix (`str`, *optional*, defaults to `\"Describe the image.\"`):\n-            A string that gets tokenized and prepended to the image tokens.\n-        query_prefix (`str`, *optional*, defaults to `\"Question: \"`):\n-            A prefix to be used for the query.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -72,6 +52,12 @@ def __init__(\n         visual_prompt_prefix: str = \"Describe the image.\",\n         query_prefix: str = \"Question: \",\n     ):\n+        r\"\"\"\n+        visual_prompt_prefix (`str`, *optional*, defaults to `\"Describe the image.\"`):\n+            A string that gets tokenized and prepended to the image tokens.\n+        query_prefix (`str`, *optional*, defaults to `\"Question: \"`):\n+            A prefix to be used for the query.\n+        \"\"\"\n         self.visual_prompt_prefix = visual_prompt_prefix\n         self.query_prefix = query_prefix\n         super().__init__(image_processor=image_processor, tokenizer=tokenizer, chat_template=chat_template)\n@@ -91,32 +77,7 @@ def __call__(\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model either (1) one or several texts, either (2) one or several image(s). This method is a custom\n-        wrapper around the PaliGemmaProcessor's [`~PaliGemmaProcessor.__call__`] method adapted for the ColPali model. It cannot process\n-        both text and images at the same time.\n-\n-        When preparing the text(s), this method forwards the `text` and `kwargs` arguments to LlamaTokenizerFast's\n-        [`~LlamaTokenizerFast.__call__`].\n-        When preparing the image(s), this method forwards the `images` and `kwargs` arguments to SiglipImageProcessor's\n-        [`~SiglipImageProcessor.__call__`].\n-        Please refer to the docstring of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n-                number of channels, H and W are image height and width.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "efdccaee4c1d80adf486557dd72420edaa0655e4",
            "filename": "src/transformers/models/colpali/processing_colpali.py",
            "status": "modified",
            "additions": 10,
            "deletions": 47,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fprocessing_colpali.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -25,7 +25,7 @@\n from ...image_utils import ImageInput, make_flat_list_of_images\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AddedToken, PreTokenizedInput, TextInput\n-from ...utils import is_torch_available\n+from ...utils import auto_docstring, is_torch_available\n \n \n if is_torch_available():\n@@ -71,27 +71,8 @@ def build_string_from_input(prompt, bos_token, image_seq_len, image_token, num_i\n     return f\"{image_token * image_seq_len * num_images}{bos_token}{prompt}\\n\"\n \n \n+@auto_docstring\n class ColPaliProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a ColPali processor which wraps a PaliGemmaProcessor and special methods to process images and queries, as\n-    well as to compute the late-interaction retrieval score.\n-\n-    [`ColPaliProcessor`] offers all the functionalities of [`PaliGemmaProcessor`]. See the [`~PaliGemmaProcessor.__call__`]\n-    for more information.\n-\n-    Args:\n-        image_processor ([`SiglipImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        visual_prompt_prefix (`str`, *optional*, defaults to `\"Describe the image.\"`):\n-            A string that gets tokenized and prepended to the image tokens.\n-        query_prefix (`str`, *optional*, defaults to `\"Question: \"`):\n-            A prefix to be used for the query.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -100,6 +81,12 @@ def __init__(\n         visual_prompt_prefix: str = \"Describe the image.\",\n         query_prefix: str = \"Question: \",\n     ):\n+        r\"\"\"\n+        visual_prompt_prefix (`str`, *optional*, defaults to `\"Describe the image.\"`):\n+            A string that gets tokenized and prepended to the image tokens.\n+        query_prefix (`str`, *optional*, defaults to `\"Question: \"`):\n+            A prefix to be used for the query.\n+        \"\"\"\n         self.visual_prompt_prefix = visual_prompt_prefix\n         self.query_prefix = query_prefix\n         if not hasattr(image_processor, \"image_seq_length\"):\n@@ -123,38 +110,14 @@ def __init__(\n \n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: ImageInput | None = None,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[ColPaliProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model either (1) one or several texts, either (2) one or several image(s). This method is a custom\n-        wrapper around the PaliGemmaProcessor's [`~PaliGemmaProcessor.__call__`] method adapted for the ColPali model. It cannot process\n-        both text and images at the same time.\n-\n-        When preparing the text(s), this method forwards the `text` and `kwargs` arguments to LlamaTokenizerFast's\n-        [`~LlamaTokenizerFast.__call__`].\n-        When preparing the image(s), this method forwards the `images` and `kwargs` arguments to SiglipImageProcessor's\n-        [`~SiglipImageProcessor.__call__`].\n-        Please refer to the docstring of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n-                number of channels, H and W are image height and width.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "1e8b9712a46b96b0332fe3e8760183bacabb4847",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 7,
            "deletions": 44,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -45,24 +45,6 @@ class ColQwen2ProcessorKwargs(ProcessingKwargs, total=False):\n \n \n class ColQwen2Processor(ColPaliProcessor):\n-    r\"\"\"\n-    Constructs a ColQwen2 processor which wraps a Qwen2VLProcessor and special methods to process images and queries, as\n-    well as to compute the late-interaction retrieval score.\n-\n-    [`ColQwen2Processor`] offers all the functionalities of [`Qwen2VLProcessor`]. See the [`~Qwen2VLProcessor.__call__`]\n-    for more information.\n-\n-    Args:\n-        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        visual_prompt_prefix (`str`, *optional*): A string that gets tokenized and prepended to the image tokens.\n-        query_prefix (`str`, *optional*): A prefix to be used for the query.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -72,6 +54,12 @@ def __init__(\n         query_prefix: str | None = None,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        visual_prompt_prefix (`str`, *optional*, defaults to `\"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image.<|im_end|><|endoftext|>\"`):\n+            A string that gets tokenized and prepended to the image tokens.\n+        query_prefix (`str`, *optional*, defaults to `\"Query: \"`):\n+            A prefix to be used for the query.\n+        \"\"\"\n         ProcessorMixin.__init__(self, image_processor, tokenizer, chat_template=chat_template)\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n@@ -90,32 +78,7 @@ def __call__(\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[ColQwen2ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model either (1) one or several texts, either (2) one or several image(s). This method is a custom\n-        wrapper around the Qwen2VLProcessor's [`~Qwen2VLProcessor.__call__`] method adapted for the ColQwen2 model. It cannot process\n-        both text and images at the same time.\n-\n-        When preparing the text(s), this method forwards the `text` and `kwargs` arguments to Qwen2TokenizerFast's\n-        [`~Qwen2TokenizerFast.__call__`].\n-        When preparing the image(s), this method forwards the `images` and `kwargs` arguments to Qwen2VLImageProcessor's\n-        [`~Qwen2VLImageProcessor.__call__`].\n-        Please refer to the doctsring of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n-                number of channels, H and W are image height and width.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "9f08c92efe69cbc259dc4f2e8726e36018741359",
            "filename": "src/transformers/models/colqwen2/processing_colqwen2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 45,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fprocessing_colqwen2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -24,7 +24,7 @@\n from ...image_utils import ImageInput, is_valid_image\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_torch_available\n+from ...utils import auto_docstring, is_torch_available\n \n \n if is_torch_available():\n@@ -44,25 +44,8 @@ class ColQwen2ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class ColQwen2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a ColQwen2 processor which wraps a Qwen2VLProcessor and special methods to process images and queries, as\n-    well as to compute the late-interaction retrieval score.\n-\n-    [`ColQwen2Processor`] offers all the functionalities of [`Qwen2VLProcessor`]. See the [`~Qwen2VLProcessor.__call__`]\n-    for more information.\n-\n-    Args:\n-        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        visual_prompt_prefix (`str`, *optional*): A string that gets tokenized and prepended to the image tokens.\n-        query_prefix (`str`, *optional*): A prefix to be used for the query.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -72,6 +55,12 @@ def __init__(\n         query_prefix: str | None = None,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        visual_prompt_prefix (`str`, *optional*, defaults to `\"<|im_start|>user\\n<|vision_start|><|image_pad|><|vision_end|>Describe the image.<|im_end|><|endoftext|>\"`):\n+            A string that gets tokenized and prepended to the image tokens.\n+        query_prefix (`str`, *optional*, defaults to `\"Query: \"`):\n+            A prefix to be used for the query.\n+        \"\"\"\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n@@ -84,38 +73,14 @@ def __init__(\n             query_prefix = \"Query: \"\n         self.query_prefix = query_prefix\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: ImageInput | None = None,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[ColQwen2ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model either (1) one or several texts, either (2) one or several image(s). This method is a custom\n-        wrapper around the Qwen2VLProcessor's [`~Qwen2VLProcessor.__call__`] method adapted for the ColQwen2 model. It cannot process\n-        both text and images at the same time.\n-\n-        When preparing the text(s), this method forwards the `text` and `kwargs` arguments to Qwen2TokenizerFast's\n-        [`~Qwen2TokenizerFast.__call__`].\n-        When preparing the image(s), this method forwards the `images` and `kwargs` arguments to Qwen2VLImageProcessor's\n-        [`~Qwen2VLImageProcessor.__call__`].\n-        Please refer to the doctsring of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n-                number of channels, H and W are image height and width.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "7e5707ee4c299458153e6007d38ec3948a394d47",
            "filename": "src/transformers/models/csm/processing_csm.py",
            "status": "modified",
            "additions": 18,
            "deletions": 60,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fprocessing_csm.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -18,7 +18,7 @@\n \n import numpy as np\n \n-from ...utils import is_soundfile_available, is_torch_available\n+from ...utils import auto_docstring, is_soundfile_available, is_torch_available\n \n \n if is_torch_available():\n@@ -34,6 +34,14 @@\n \n \n class CsmAudioKwargs(AudioKwargs, total=False):\n+    \"\"\"\n+    encoded_length_kwargs (`dict[str, Any]`, *optional*):\n+        Dictionary of keyword arguments used to compute the encoded audio sequence length. This includes parameters\n+        such as `kernel_sizes`, `strides`, `dilations`, and `use_causal_conv` that define the convolutional layers\n+        used in audio encoding. The encoded length is used to determine how many audio tokens to generate for each\n+        audio input in the text sequence.\n+    \"\"\"\n+\n     encoded_length_kwargs: Optional[dict[str, Any]]\n \n \n@@ -58,42 +66,8 @@ class CsmProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class CsmProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Csm processor which wraps [`EncodecFeatureExtractor`] and\n-    [`PretrainedTokenizerFast`] into a single processor that inherits both the audio feature extraction and\n-    tokenizer functionalities. See the [`~CsmProcessor.__call__`] for more\n-    information.\n-    The preferred way of passing kwargs is as a dictionary per modality, see usage example below.\n-        ```python\n-        from transformers import CsmProcessor\n-        from datasets import load_dataset\n-\n-        ds = load_dataset(\"hf-internal-testing/dailytalk-dummy\", split=\"train\")\n-        audio = ds[0][\"audio\"][\"array\"]\n-\n-        processor = CsmProcessor.from_pretrained(\"sesame/csm-1b\")\n-\n-        processor(\n-            text=[\"<|begin_of_text|>[0]What are you working on?<|end_of_text|><|AUDIO|><|audio_eos|><|begin_of_text|>[1]I'm figuring out my budget.<|end_of_text|>\"],\n-            audio=audio,\n-            text_kwargs = {\"padding\": False},\n-            audio_kwargs = {\"sampling_rate\": 16000},\n-            common_kwargs = {\"return_tensors\": \"pt\"},\n-        )\n-        # this should error out because EncodecFeatureExtractor expects a 24kHz audio :)\n-        ```\n-\n-    Args:\n-        feature_extractor ([`EncodecFeatureExtractor`]):\n-            The feature extractor is a required input.\n-        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-\n-    \"\"\"\n-\n     def __init__(\n         self,\n         feature_extractor,\n@@ -188,6 +162,7 @@ def save_audio(\n                 audio_value = audio_value.cpu().float().numpy()\n             sf.write(p, audio_value, sampling_rate)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]],\n@@ -197,31 +172,14 @@ def __call__(\n         **kwargs: Unpack[CsmProcessorKwargs],\n     ):\n         r\"\"\"\n-        Main method to prepare text(s) and audio to be fed as input to the model. This method forwards the `text`\n-        arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode\n-        the text. To prepare the audio, this method forwards the `audio` arguments to\n-        EncodecFeatureExtractor's [`~EncodecFeatureExtractor.__call__`]. Please refer\n-        to the docstring of the above two methods for more information.\n+        output_labels (bool, *optional*, default=False):\n+            Whether to return labels for training. Indices will be in `[config.audio_token_id, -100, -101]`.\n+            - `config.audio_token_id` indicates an audio frame (considering sequence length elements as frames)\n+            - `-100` will be ignored in the loss computation\n+            - `-101` indicates the audio frame will be used only for the backbone model (using the first codebook token as labels)\n+        depth_decoder_labels_ratio (float, *optional*, default=1.0):\n+            The ratio of audio frames to keep for the depth decoder labels.\n \n-        Args:\n-            audio (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The audio or batch of audio to be prepared. Each audio can be a NumPy array or PyTorch\n-                tensor.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            output_labels (bool, *optional*, default=False):\n-                Whether to return labels for training. Indices will be in `[config.audio_token_id, -100, -101]`.\n-                - `config.audio_token_id` indicates an audio frame (considering sequence length elements as frames)\n-                - `-100` will be ignored in the loss computation\n-                - `-101` indicates the audio frame will be used only for the backbone model (using the first codebook token as labels)\n-            depth_decoder_labels_ratio (float, *optional*, default=1.0):\n-                The ratio of audio frames to keep for the depth decoder labels.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                    - `'np'`: Return NumPy `np.ndarray` objects.\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "5c1225e93db8e71b6ca59682c26d286cd625d621",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_audio.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_audio.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -812,10 +812,10 @@ def forward(\n class Data2VecAudioForCTC(Data2VecAudioPreTrainedModel):\n     def __init__(self, config):\n         r\"\"\"\n-        target_lang (`str`, *optional*):\n-            Language id of adapter weights. Adapter weights are stored in the format adapter.<lang>.safetensors or\n-            adapter.<lang>.bin. Only relevant when using an instance of [`Data2VecAudioForCTC`] with adapters. Uses 'eng' by\n-            default.\n+        config ([`Data2VecAudioForCTC`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`]  method to load the model weights.\n         \"\"\"\n         super().__init__(config)\n "
        },
        {
            "sha": "a6f5dda725dadfae5749f43ebb9e4c5a4bf40a32",
            "filename": "src/transformers/models/data2vec/modular_data2vec_audio.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodular_data2vec_audio.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -214,6 +214,12 @@ def forward(self, **super_kwargs):\n \n class Data2VecAudioForCTC(Data2VecAudioPreTrainedModel, Wav2Vec2ForCTC):\n     def __init__(self, config):\n+        r\"\"\"\n+        config ([`Data2VecAudioForCTC`]):\n+            Model configuration class with all the parameters of the model. Initializing with a config file does not\n+            load the weights associated with the model, only the configuration. Check out the\n+            [`~PreTrainedModel.from_pretrained`]  method to load the model weights.\n+        \"\"\"\n         Data2VecAudioPreTrainedModel.__init__(self, config)\n \n         self.data2vec_audio = Data2VecAudioModel(config)"
        },
        {
            "sha": "23c3ed23c0d3f5a6ead22d452cc6920c4a0e4a5c",
            "filename": "src/transformers/models/deepseek_vl/modular_deepseek_vl.py",
            "status": "modified",
            "additions": 7,
            "deletions": 38,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fmodular_deepseek_vl.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -197,63 +197,32 @@ class DeepseekVLProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class DeepseekVLProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a DeepseekVL processor which wraps a DeepseekVL Image Processor and a Llama tokenizer into a single processor.\n-\n-    [`DeepseekVLProcessor`] offers all the functionalities of [`DeepseekVLImageProcessor`] and [`LlamaTokenizerFast`]. See the\n-    [`~DeepseekVLProcessor.__call__`] and [`~DeepseekVLProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`DeepseekVLImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`]):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*):\n-            A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        num_image_tokens (`int`, *optional*, defaults to 576):\n-            The number of special image tokens used as placeholders for visual content in text sequences.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor,\n         tokenizer,\n         chat_template=None,\n         num_image_tokens=576,\n     ):\n+        r\"\"\"\n+        num_image_tokens (`int`, *optional*, defaults to 576):\n+            The number of special image tokens used as placeholders for visual content in text sequences.\n+        \"\"\"\n         self.image_token = tokenizer.image_token\n         self.num_image_tokens = num_image_tokens\n \n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         images: ImageInput | None = None,\n         **kwargs: Unpack[DeepseekVLProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        DeepseekVLImageProcessor's [`~DeepseekVLImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "7057ff152a67a64ae7e34af003303eede6a23277",
            "filename": "src/transformers/models/deepseek_vl/processing_deepseek_vl.py",
            "status": "modified",
            "additions": 8,
            "deletions": 38,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl%2Fprocessing_deepseek_vl.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -23,6 +23,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class DeepseekVLProcessorKwargs(ProcessingKwargs, total=False):\n@@ -32,63 +33,32 @@ class DeepseekVLProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class DeepseekVLProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a DeepseekVL processor which wraps a DeepseekVL Image Processor and a Llama tokenizer into a single processor.\n-\n-    [`DeepseekVLProcessor`] offers all the functionalities of [`DeepseekVLImageProcessor`] and [`LlamaTokenizerFast`]. See the\n-    [`~DeepseekVLProcessor.__call__`] and [`~DeepseekVLProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`DeepseekVLImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`]):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*):\n-            A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        num_image_tokens (`int`, *optional*, defaults to 576):\n-            The number of special image tokens used as placeholders for visual content in text sequences.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor,\n         tokenizer,\n         chat_template=None,\n         num_image_tokens=576,\n     ):\n+        r\"\"\"\n+        num_image_tokens (`int`, *optional*, defaults to 576):\n+            The number of special image tokens used as placeholders for visual content in text sequences.\n+        \"\"\"\n         self.image_token = tokenizer.image_token\n         self.num_image_tokens = num_image_tokens\n \n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         images: ImageInput | None = None,\n         **kwargs: Unpack[DeepseekVLProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        DeepseekVLImageProcessor's [`~DeepseekVLImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "e85da0ad2e7177e0c045b761adb7a4ea4a7a0e99",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 22,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -933,33 +933,14 @@ def __call__(\n         images: ImageInput | None = None,\n         **kwargs: Unpack[DeepseekVLHybridProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        DeepseekVLHybridImageProcessor's [`~DeepseekVLHybridImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n \n             - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n             - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-            `None`).\n+                `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+                `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n         output_kwargs = self._merge_kwargs("
        },
        {
            "sha": "73309c4cbbf5fab1875c91774fc9cf4155b3c98a",
            "filename": "src/transformers/models/deepseek_vl_hybrid/processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 10,
            "deletions": 40,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fprocessing_deepseek_vl_hybrid.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -22,6 +22,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class DeepseekVLHybridProcessorKwargs(ProcessingKwargs, total=False):\n@@ -31,70 +32,39 @@ class DeepseekVLHybridProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class DeepseekVLHybridProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a DeepseekVLHybrid processor which wraps a DeepseekVLHybrid Image Processor and a Llama tokenizer into a single processor.\n-\n-    [`DeepseekVLHybridProcessor`] offers all the functionalities of [`DeepseekVLHybridImageProcessor`] and [`LlamaTokenizerFast`]. See the\n-    [`~DeepseekVLHybridProcessor.__call__`] and [`~DeepseekVLHybridProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`DeepseekVLHybridImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`]):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*):\n-            A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        num_image_tokens (`int`, *optional*, defaults to 576):\n-            The number of special image tokens used as placeholders for visual content in text sequences.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor,\n         tokenizer,\n         chat_template=None,\n         num_image_tokens=576,\n     ):\n+        r\"\"\"\n+        num_image_tokens (`int`, *optional*, defaults to 576):\n+            The number of special image tokens used as placeholders for visual content in text sequences.\n+        \"\"\"\n         self.image_token = tokenizer.image_token\n         self.num_image_tokens = num_image_tokens\n \n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         images: ImageInput | None = None,\n         **kwargs: Unpack[DeepseekVLHybridProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        DeepseekVLHybridImageProcessor's [`~DeepseekVLHybridImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n \n             - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n             - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-            `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-            `None`).\n+                `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+                `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n         output_kwargs = self._merge_kwargs("
        },
        {
            "sha": "b9b78a1ab117d231841ba2d1b629a6a610bdacbd",
            "filename": "src/transformers/models/dia/processing_dia.py",
            "status": "modified",
            "additions": 33,
            "deletions": 21,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fprocessing_dia.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,7 +20,7 @@\n from ...audio_utils import AudioInput, make_list_of_audio\n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import AudioKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n-from ...utils import is_soundfile_available, is_torch_available\n+from ...utils import auto_docstring, is_soundfile_available, is_torch_available\n \n \n if is_torch_available():\n@@ -31,6 +31,26 @@\n \n \n class DiaAudioKwargs(AudioKwargs, total=False):\n+    \"\"\"\n+    bos_token_id (`int`, *optional*, defaults to `1026`):\n+        The token ID used as the beginning-of-sequence token for audio codebooks. This token is prepended to each\n+        audio sequence during encoding.\n+    eos_token_id (`int`, *optional*, defaults to `1024`):\n+        The token ID used as the end-of-sequence token for audio codebooks. This token is appended to audio sequences\n+        during training (when `generation=False`) to mark the end of the audio.\n+    pad_token_id (`int`, *optional*, defaults to `1025`):\n+        The token ID used for padding audio codebook sequences. This token is used to fill positions in the delay\n+        pattern where no valid audio token exists.\n+    delay_pattern (`list[int]`, *optional*, defaults to `[0, 8, 9, 10, 11, 12, 13, 14, 15]`):\n+        A list of delay values (in frames) for each codebook channel. The delay pattern creates temporal offsets\n+        between different codebook channels, allowing the model to capture dependencies across channels. Each value\n+        represents the number of frames to delay that specific channel.\n+    generation (`bool`, *optional*, defaults to `True`):\n+        Whether the processor is being used for generation (text-to-speech) or training. When `True`, the processor\n+        prepares inputs for generation mode where audio is generated from text. When `False`, it prepares inputs for\n+        training where both text and audio are provided.\n+    \"\"\"\n+\n     bos_token_id: int\n     eos_token_id: int\n     pad_token_id: int\n@@ -60,39 +80,31 @@ class DiaProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class DiaProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Dia processor which wraps a [`DiaFeatureExtractor`], [`DiaTokenizer`], and a [`DacModel`] into\n-    a single processor. It inherits, the audio feature extraction, tokenizer, and audio encode/decode functio-\n-    nalities. See [`~DiaProcessor.__call__`], [`~DiaProcessor.encode`], and [`~DiaProcessor.decode`] for more\n-    information.\n-\n-    Args:\n-        feature_extractor (`DiaFeatureExtractor`):\n-            An instance of [`DiaFeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer (`DiaTokenizer`):\n-            An instance of [`DiaTokenizer`]. The tokenizer is a required input.\n-        audio_tokenizer (`DacModel`):\n-            An instance of [`DacModel`] used to encode/decode audio into/from codebooks. It is a required input.\n-    \"\"\"\n-\n     audio_tokenizer_class = \"DacModel\"\n \n     def __init__(self, feature_extractor, tokenizer, audio_tokenizer):\n+        r\"\"\"\n+        audio_tokenizer (`DacModel`):\n+            An instance of [`DacModel`] used to encode/decode audio into/from codebooks. It is is a required input.\n+        \"\"\"\n         super().__init__(feature_extractor, tokenizer, audio_tokenizer=audio_tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: Union[str, list[str]],\n         audio: Optional[AudioInput] = None,\n         output_labels: Optional[bool] = False,\n         **kwargs: Unpack[DiaProcessorKwargs],\n     ):\n-        \"\"\"\n-        Main method to prepare text(s) and audio to be fed as input to the model. The `audio` argument is\n-        forwarded to the DiaFeatureExtractor's [`~DiaFeatureExtractor.__call__`] and subsequently to the\n-        DacModel's [`~DacModel.encode`]. The `text` argument to [`~DiaTokenizer.__call__`]. Please refer\n-        to the docstring of the above methods for more information.\n+        r\"\"\"\n+        output_labels (`bool`, *optional*, defaults to `False`):\n+            Whether to return labels for training. When `True`, the processor generates labels from the decoder input\n+            sequence by shifting it by one position. Labels use special values: `-100` for tokens to ignore in loss\n+            computation (padding and BOS tokens), and `-101` for audio frames used only for the backbone model (when\n+            `depth_decoder_labels_ratio < 1.0`). Cannot be used together with `generation=True`.\n         \"\"\"\n         if not is_torch_available():\n             raise ValueError("
        },
        {
            "sha": "e9cf463f21c919c84ecea02b7a7b986dccef1524",
            "filename": "src/transformers/models/donut/processing_donut.py",
            "status": "modified",
            "additions": 3,
            "deletions": 22,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdonut%2Fprocessing_donut.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -21,7 +21,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n class DonutProcessorKwargs(ProcessingKwargs, total=False):\n@@ -31,37 +31,18 @@ class DonutProcessorKwargs(ProcessingKwargs, total=False):\n logger = logging.get_logger(__name__)\n \n \n+@auto_docstring\n class DonutProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Donut processor which wraps a Donut image processor and an XLMRoBERTa tokenizer into a single\n-    processor.\n-\n-    [`DonutProcessor`] offers all the functionalities of [`DonutImageProcessor`] and\n-    [`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`]. See the [`~DonutProcessor.__call__`] and\n-    [`~DonutProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`DonutImageProcessor`], *optional*):\n-            An instance of [`DonutImageProcessor`]. The image processor is a required input.\n-        tokenizer ([`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`], *optional*):\n-            An instance of [`XLMRobertaTokenizer`/`XLMRobertaTokenizerFast`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         **kwargs: Unpack[DonutProcessorKwargs],\n     ):\n-        \"\"\"\n-        When used in normal mode, this method forwards all its arguments to AutoImageProcessor's\n-        [`~AutoImageProcessor.__call__`] and returns its output. If used in the context\n-        [`~DonutProcessor.as_target_processor`] this method forwards all its arguments to DonutTokenizer's\n-        [`~DonutTokenizer.__call__`]. Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You need to specify either an `images` or `text` input to process.\")\n "
        },
        {
            "sha": "bfe2a75799da6a71df0aae6fa8a46eb3ad6269d4",
            "filename": "src/transformers/models/emu3/image_processing_emu3.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fimage_processing_emu3.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -47,6 +47,13 @@\n \n \n class Emu3ImageProcessorKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    ratio (`str`, *optional*, defaults to `\"1:1\"`):\n+        The ratio of the image to resize the image.\n+    image_area (`int`, *optional*, defaults to `518400`):\n+        The area of the image to resize the image.\n+    \"\"\"\n+\n     ratio: str\n     image_area: int\n "
        },
        {
            "sha": "e90a15eff3be83fe71479cfea3b4174575f71204",
            "filename": "src/transformers/models/emu3/processing_emu3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 39,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fprocessing_emu3.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -21,19 +21,29 @@\n from ...image_utils import ImageInput\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_vision_available\n+from ...utils import auto_docstring, is_vision_available\n+from ...utils.import_utils import requires\n \n \n if is_vision_available():\n-    from .image_processing_emu3 import smart_resize\n+    from .image_processing_emu3 import Emu3ImageProcessorKwargs, smart_resize\n \n \n class Emu3TextKwargs(TextKwargs, total=False):\n+    \"\"\"\n+    return_for_image_generation (`bool`, *optional*, defaults to `False`):\n+        Whether the processed text is intended for image generation tasks. When `True`, the processor prepares\n+        inputs for image generation by appending image start tokens and size information to the prompt, and\n+        images should not be provided. When `False`, the processor prepares inputs for text generation from\n+        images and text, requiring both inputs to be provided.\n+    \"\"\"\n+\n     return_for_image_generation: bool\n \n \n class Emu3ProcessorKwargs(ProcessingKwargs, total=False):\n     text_kwargs: Emu3TextKwargs\n+    images_kwargs: Emu3ImageProcessorKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"return_for_image_generation\": False,\n@@ -46,23 +56,9 @@ class Emu3ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n+@requires(backends=(\"vision\",))\n class Emu3Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Emu3 processor which wraps a Emu3 image processor and a GPT2 tokenizer into a single\n-    processor.\n-\n-    [`Emu3Processor`] offers all the functionalities of [`Emu3ImageProcessor`] and [`GPT2TokenizerFast`].\n-    See the [`~Emu3Processor.__call__`] and [`~Emu3Processor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`Emu3ImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`Emu3TokenizerFast`]):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor,\n@@ -80,33 +76,14 @@ def __init__(\n         self.downsample_ratio = 8\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         **kwargs: Unpack[Emu3ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to Emu3TokenizerFast's [`~Emu3TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "70de5e4d640ab9ad21ecf3174f922887e3b488b3",
            "filename": "src/transformers/models/evolla/processing_evolla.py",
            "status": "modified",
            "additions": 19,
            "deletions": 28,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fprocessing_evolla.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -21,30 +21,23 @@\n from ...processing_utils import (\n     ProcessorMixin,\n )\n+from ...utils import auto_docstring\n \n \n PROTEIN_VALID_KEYS = [\"aa_seq\", \"foldseek\", \"msa\"]\n \n \n+@auto_docstring\n class EvollaProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a EVOLLA processor which wraps a LLama tokenizer and SaProt tokenizer (EsmTokenizer) into a single processor.\n-\n-    [`EvollaProcessor`] offers all the functionalities of [`EsmTokenizer`] and [`LlamaTokenizerFast`]. See the\n-    docstring of [`~EvollaProcessor.__call__`] and [`~EvollaProcessor.decode`] for more information.\n-\n-    Args:\n+    def __init__(self, protein_tokenizer, tokenizer=None, protein_max_length=1024, text_max_length=512, **kwargs):\n+        r\"\"\"\n         protein_tokenizer (`EsmTokenizer`):\n             An instance of [`EsmTokenizer`]. The protein tokenizer is a required input.\n-        tokenizer (`LlamaTokenizerFast`, *optional*):\n-            An instance of [`LlamaTokenizerFast`]. The tokenizer is a required input.\n         protein_max_length (`int`, *optional*, defaults to 1024):\n             The maximum length of the sequence to be generated.\n         text_max_length (`int`, *optional*, defaults to 512):\n             The maximum length of the text to be generated.\n-    \"\"\"\n-\n-    def __init__(self, protein_tokenizer, tokenizer=None, protein_max_length=1024, text_max_length=512, **kwargs):\n+        \"\"\"\n         if protein_tokenizer is None:\n             raise ValueError(\"You need to specify an `protein_tokenizer`.\")\n         if tokenizer is None:\n@@ -93,6 +86,7 @@ def process_text(\n         )\n         return prompt_inputs\n \n+    @auto_docstring\n     def __call__(\n         self,\n         proteins: Optional[Union[list[dict], dict]] = None,\n@@ -101,22 +95,19 @@ def __call__(\n         text_max_length: Optional[int] = None,\n         **kwargs,\n     ):\n-        r\"\"\"This method takes batched or non-batched proteins and messages_list and converts them into format that can be used by\n-        the model.\n-\n-        Args:\n-            proteins (`Union[List[dict], dict]`):\n-                A list of dictionaries or a single dictionary containing the following keys:\n-                    - `\"aa_seq\"` (`str`) -- The amino acid sequence of the protein.\n-                    - `\"foldseek\"` (`str`) -- The foldseek string of the protein.\n-            messages_list (`Union[List[List[dict]], List[dict]]`):\n-                A list of lists of dictionaries or a list of dictionaries containing the following keys:\n-                    - `\"role\"` (`str`) -- The role of the message.\n-                    - `\"content\"` (`str`) -- The content of the message.\n-            protein_max_length (`int`, *optional*, defaults to 1024):\n-                The maximum length of the sequence to be generated.\n-            text_max_length (`int`, *optional*, defaults to 512):\n-                The maximum length of the text.\n+        r\"\"\"\n+        proteins (`Union[List[dict], dict]`):\n+            A list of dictionaries or a single dictionary containing the following keys:\n+                - `\"aa_seq\"` (`str`) -- The amino acid sequence of the protein.\n+                - `\"foldseek\"` (`str`) -- The foldseek string of the protein.\n+        messages_list (`Union[List[List[dict]], List[dict]]`):\n+            A list of lists of dictionaries or a list of dictionaries containing the following keys:\n+                - `\"role\"` (`str`) -- The role of the message.\n+                - `\"content\"` (`str`) -- The content of the message.\n+        protein_max_length (`int`, *optional*, defaults to 1024):\n+            The maximum length of the sequence to be generated.\n+        text_max_length (`int`, *optional*, defaults to 512):\n+            The maximum length of the text.\n \n         Return:\n             a dict with following keys:"
        },
        {
            "sha": "592654a18f0e9fc07fe3c1a1e1ec8abb3459c3ff",
            "filename": "src/transformers/models/flava/processing_flava.py",
            "status": "modified",
            "additions": 2,
            "deletions": 11,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflava%2Fprocessing_flava.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,20 +16,11 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class FlavaProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a FLAVA processor which wraps a FLAVA image processor and a FLAVA tokenizer into a single processor.\n-\n-    [`FlavaProcessor`] offers all the functionalities of [`FlavaImageProcessor`] and [`BertTokenizerFast`]. See the\n-    [`~FlavaProcessor.__call__`] and [`~FlavaProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`FlavaImageProcessor`], *optional*): The image processor is a required input.\n-        tokenizer ([`BertTokenizerFast`], *optional*): The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "dc6fa1755712fbcf5b835e653eae415fd9aeeade",
            "filename": "src/transformers/models/florence2/modular_florence2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 39,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fmodular_florence2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -236,26 +236,8 @@ class Florence2ProcessorKwargs(LlavaProcessorKwargs):\n     pass\n \n \n+@auto_docstring\n class Florence2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Florence2 processor which wraps a Florence2 image processor and a Florence2 tokenizer into a single processor.\n-\n-    [`Florence2Processor`] offers all the functionalities of [`AutoImageProcessor`] and [`BartTokenizerFast`]. See the\n-    [`~Florence2Processor.__call__`] and [`~Florence2Processor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`AutoImageProcessor`, *optional*):\n-            The image processor is a required input.\n-        tokenizer (`Union[BartTokenizer, BartTokenizerFast]`, *optional*):\n-            The tokenizer is a required input.\n-        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n-            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n-            extra tokens appended, no need to set this arg.\n-        post_processor_config (`dict`,  *optional*, defaults to 0):\n-            Task-specific parsing rules for [`Florence2PostProcessor`], e.g. regex patterns,\n-            thresholds, or banned tokens.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -264,6 +246,14 @@ def __init__(\n         post_processor_config: dict | None = None,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n+        post_processor_config (`dict`,  *optional*, defaults to 0):\n+            Task-specific parsing rules for [`Florence2PostProcessor`], e.g. regex patterns,\n+            thresholds, or banned tokens.\n+        \"\"\"\n         self.tasks_answer_post_processing_type = {\n             \"<OCR>\": \"pure_text\",\n             \"<OCR_WITH_REGION>\": \"ocr\",\n@@ -337,32 +327,14 @@ def _construct_prompts(self, text: str | list[str]) -> list[str]:\n             prompts.append(prompt)\n         return prompts\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: ImageInput | None = None,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[Florence2ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to BartTokenizerFast's [`~BartTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "a90bc2a9561f6787a52693a276508087785fa508",
            "filename": "src/transformers/models/florence2/processing_florence2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 40,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fflorence2%2Fprocessing_florence2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -26,7 +26,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_torch_available, logging\n+from ...utils import auto_docstring, is_torch_available, logging\n \n \n if is_torch_available():\n@@ -41,26 +41,8 @@ class Florence2ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Florence2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Florence2 processor which wraps a Florence2 image processor and a Florence2 tokenizer into a single processor.\n-\n-    [`Florence2Processor`] offers all the functionalities of [`AutoImageProcessor`] and [`BartTokenizerFast`]. See the\n-    [`~Florence2Processor.__call__`] and [`~Florence2Processor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`AutoImageProcessor`, *optional*):\n-            The image processor is a required input.\n-        tokenizer (`Union[BartTokenizer, BartTokenizerFast]`, *optional*):\n-            The tokenizer is a required input.\n-        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n-            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n-            extra tokens appended, no need to set this arg.\n-        post_processor_config (`dict`,  *optional*, defaults to 0):\n-            Task-specific parsing rules for [`Florence2PostProcessor`], e.g. regex patterns,\n-            thresholds, or banned tokens.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -69,6 +51,14 @@ def __init__(\n         post_processor_config: dict | None = None,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n+        post_processor_config (`dict`,  *optional*, defaults to 0):\n+            Task-specific parsing rules for [`Florence2PostProcessor`], e.g. regex patterns,\n+            thresholds, or banned tokens.\n+        \"\"\"\n         self.tasks_answer_post_processing_type = {\n             \"<OCR>\": \"pure_text\",\n             \"<OCR_WITH_REGION>\": \"ocr\",\n@@ -142,32 +132,14 @@ def _construct_prompts(self, text: str | list[str]) -> list[str]:\n             prompts.append(prompt)\n         return prompts\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: ImageInput | None = None,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         **kwargs: Unpack[Florence2ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to BartTokenizerFast's [`~BartTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "7882ed9c39dc7857898ed8c0dc489e331fa10a68",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 4,
            "deletions": 30,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -28,7 +28,7 @@\n     Unpack,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_torch_available, logging, requires_backends\n+from ...utils import auto_docstring, is_torch_available, logging, requires_backends\n from ...utils.import_utils import requires\n \n \n@@ -332,20 +332,8 @@ def scale_bbox_to_transformed_image(\n \n \n @requires(backends=(\"vision\",))\n+@auto_docstring\n class FuyuProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Fuyu processor which wraps a Fuyu image processor and a Llama tokenizer into a single processor.\n-\n-    [`FuyuProcessor`] offers all the functionalities of [`FuyuImageProcessor`] and [`TokenizersBackend`]. See the\n-    [`~FuyuProcessor.__call__`] and [`~FuyuProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`FuyuImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`TokenizersBackend`]):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     @classmethod\n     def _load_tokenizer_from_pretrained(\n         cls, sub_processor_type, pretrained_model_name_or_path, subfolder=\"\", **kwargs\n@@ -493,28 +481,14 @@ def get_sample_encoding(\n         }\n         return batch_encoding\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         **kwargs: Unpack[FuyuProcessorKwargs],\n     ) -> \"FuyuBatchFeature\":\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to TokenizersBackend's [`~TokenizersBackend.__call__`] if `text` is not `None` to\n-        encode the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        FuyuImageProcessor's [`~FuyuImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `list[PIL.Image.Image]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-\n+        r\"\"\"\n         Returns:\n             [`FuyuBatchEncoding`]: A [`FuyuBatchEncoding`] with the following fields:\n "
        },
        {
            "sha": "a6538f0e9aebe3f75d4a0236b6264acd880e5c6b",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -21,7 +21,7 @@\n from ...image_utils import ImageInput, make_nested_list_of_images\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import to_py_obj\n+from ...utils import auto_docstring, to_py_obj\n \n \n class Gemma3ProcessorKwargs(ProcessingKwargs, total=False):\n@@ -40,6 +40,7 @@ class Gemma3ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Gemma3Processor(ProcessorMixin):\n     def __init__(\n         self,\n@@ -63,6 +64,7 @@ def __init__(\n             **kwargs,\n         )\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,"
        },
        {
            "sha": "8ed7e9682dbcaea36c8cf412748ae6dce673e450",
            "filename": "src/transformers/models/gemma3n/processing_gemma3n.py",
            "status": "modified",
            "additions": 9,
            "deletions": 21,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fprocessing_gemma3n.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,6 +20,7 @@\n from ...image_utils import ImageInput, make_nested_list_of_images\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class Gemma3nProcessorKwargs(ProcessingKwargs, total=False):\n@@ -28,28 +29,8 @@ class Gemma3nProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Gemma3nProcessor(ProcessorMixin):\n-    \"\"\"\n-    A processor for Gemma 3n, wrapping the full capabilities of a feature extractor, image processor, and tokenizer\n-    into a single processor.\n-\n-    Args:\n-        feature_extractor (`Gemma3nAudioFeatureExtractor`):\n-            Feature extractor that converts raw audio waveforms into MEL spectrograms for the audio encoder. This\n-            should return a `BatchFeature` with `input_features` and `input_features_mask` features.\n-        image_processor (`SiglipImageProcessorFast`):\n-            Image processor that prepares batches of images for the vision encoder. This should return a `BatchFeature`\n-            with a `pixel_values` feature.\n-        tokenizer (`GemmaTokenizerFast`):\n-            The text tokenizer for the model.\n-        chat_template (`string`, *optional*):\n-            A Jinja template for generating text prompts from a set of messages.\n-        audio_seq_length (int, *optional*, defaults to 188):\n-            The number of audio soft tokens that will be added to the text prompt\n-        image_seq_length (int, *optional*, defaults to 256):\n-            The number of image soft tokens that should be added to\n-    \"\"\"\n-\n     def __init__(\n         self,\n         feature_extractor,\n@@ -60,6 +41,12 @@ def __init__(\n         image_seq_length: int = 256,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        audio_seq_length (int, *optional*, defaults to 188):\n+            The number of audio soft tokens that will be added to the text prompt\n+        image_seq_length (int, *optional*, defaults to 256):\n+            The number of image soft tokens that should be added to\n+        \"\"\"\n         self.audio_seq_length = audio_seq_length\n         self.audio_token_id = tokenizer.audio_token_id\n         self.boa_token = tokenizer.boa_token\n@@ -82,6 +69,7 @@ def __init__(\n             **kwargs,\n         )\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,"
        },
        {
            "sha": "2a4399b3c51c9e2c6e5d8e02b9664c99eec006bf",
            "filename": "src/transformers/models/git/processing_git.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,22 +16,11 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class GitProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a GIT processor which wraps a CLIP image processor and a BERT tokenizer into a single processor.\n-\n-    [`GitProcessor`] offers all the functionalities of [`CLIPImageProcessor`] and [`BertTokenizerFast`]. See the\n-    [`~GitProcessor.__call__`] and [`~GitProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`AutoImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`AutoTokenizer`]):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "99bc1cbd8dcdcc1c959a848938a324ecaed98fe3",
            "filename": "src/transformers/models/glm46v/processing_glm46v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 36,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fglm46v%2Fprocessing_glm46v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fglm46v%2Fprocessing_glm46v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm46v%2Fprocessing_glm46v.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -25,7 +25,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n from ...video_utils import VideoInput\n \n \n@@ -43,21 +43,8 @@ class Glm46VProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Glm46VProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a GLM-4V processor which wraps a GLM-4V image processor and a GLM-4 tokenizer into a single processor.\n-    [`~Glm46VProcessor.__call__`] and [`~Glm46VProcessor.decode`] for more information.\n-    Args:\n-        image_processor ([`Glm46VProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`PreTrainedTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`Glm46VVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n@@ -73,34 +60,15 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n         )\n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: ImageInput | None = None,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         videos: VideoInput | None = None,\n         **kwargs: Unpack[Glm46VProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "61edcdc7a390ad1568fbeff7f4b8cda0adfd2d63",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 35,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -1517,20 +1517,6 @@ class Glm4vProcessorKwargs(Qwen2VLProcessorKwargs):\n \n \n class Glm4vProcessor(Qwen2VLProcessor):\n-    r\"\"\"\n-    Constructs a GLM-4V processor which wraps a GLM-4V image processor and a GLM-4 tokenizer into a single processor.\n-    [`~Glm4vProcessor.__call__`] and [`~Glm4vProcessor.decode`] for more information.\n-    Args:\n-        image_processor ([`Glm4vProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`PreTrainedTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`Glm4vVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n         self.image_token = \"<|image|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n@@ -1543,27 +1529,7 @@ def __call__(\n         videos: VideoInput | None = None,\n         **kwargs: Unpack[Glm4vProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "c8fe12e390067e2d3a2c1b5b90ddd98b98cc5e10",
            "filename": "src/transformers/models/glm4v/processing_glm4v.py",
            "status": "modified",
            "additions": 4,
            "deletions": 36,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fprocessing_glm4v.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -24,7 +24,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n from ...video_utils import VideoInput\n \n \n@@ -42,21 +42,8 @@ class Glm4vProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Glm4vProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a GLM-4V processor which wraps a GLM-4V image processor and a GLM-4 tokenizer into a single processor.\n-    [`~Glm4vProcessor.__call__`] and [`~Glm4vProcessor.decode`] for more information.\n-    Args:\n-        image_processor ([`Glm4vProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`PreTrainedTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`Glm4vVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n@@ -72,34 +59,15 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n         )\n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: ImageInput | None = None,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         videos: VideoInput | None = None,\n         **kwargs: Unpack[Glm4vProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "db69e7673f0975037583a1681a23403be15fdc92",
            "filename": "src/transformers/models/got_ocr2/processing_got_ocr2.py",
            "status": "modified",
            "additions": 36,
            "deletions": 54,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fprocessing_got_ocr2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -21,7 +21,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_vision_available, logging\n+from ...utils import auto_docstring, is_vision_available, logging\n \n \n if is_vision_available():\n@@ -31,10 +31,42 @@\n \n \n class GotOcr2TextKwargs(TextKwargs, total=False):\n+    \"\"\"\n+    format (`bool`, *optional*, defaults to `False`):\n+        Whether to request formatted output from the OCR model. When enabled, the model is instructed to return\n+        structured and formatted text output rather than raw OCR results.\n+    \"\"\"\n+\n     format: Optional[bool]\n \n \n class GotOcr2ImagesKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    crop_to_patches (`bool`, *optional*, defaults to `False`):\n+        Whether to crop images into patches before processing. When enabled, large images are divided into\n+        smaller patches for more efficient OCR processing.\n+    min_patches (`int`, *optional*, defaults to `1`):\n+        Minimum number of patches to generate when cropping images. This ensures that even small images are\n+        processed with at least this many patches.\n+    max_patches (`int`, *optional*, defaults to `12`):\n+        Maximum number of patches to generate when cropping images. Large images will be divided into at most\n+        this many patches to control computational complexity.\n+    box (`list`, `tuple[float, float]`, or `tuple[float, float, float, float]`, *optional*):\n+        Bounding box coordinates for OCR region of interest. Can be specified as a single box `[x1, y1, x2, y2]`\n+        or a list of boxes. Coordinates are normalized to the range [0, 1000] based on the image dimensions.\n+        If not provided, OCR is performed on the entire image.\n+    color (`str`, *optional*):\n+        Color filter specification for OCR. When provided, the OCR query is prefixed with the color information\n+        to focus on text of a specific color (e.g., \"red\", \"blue\").\n+    num_image_tokens (`int`, *optional*, defaults to `256`):\n+        Number of image tokens (patches) to use per image. This controls the resolution of the image representation\n+        passed to the model. Higher values provide more detail but increase computational cost.\n+    multi_page (`bool`, *optional*, defaults to `False`):\n+        Whether the input consists of multi-page documents. When enabled, images can be provided as nested lists\n+        where each inner list represents a page, and OCR is performed across all pages with appropriate handling\n+        of page boundaries.\n+    \"\"\"\n+\n     crop_to_patches: bool\n     min_patches: int\n     max_patches: int\n@@ -78,20 +110,8 @@ def preprocess_box_annotation(box: Union[list, tuple], image_size: tuple[int, in\n     return list(box)\n \n \n+@auto_docstring\n class GotOcr2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a GotOcr2 processor which wraps a [`GotOcr2ImageProcessor`] and\n-    [`PretrainedTokenizerFast`] tokenizer into a single processor that inherits both the image processor and\n-    tokenizer functionalities. See the [`~GotOcr2Processor.__call__`] and [`~GotOcr2Processor.decode`] for more information.\n-    Args:\n-        image_processor ([`GotOcr2ImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, chat_template=None, **kwargs):\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n@@ -126,52 +146,14 @@ def _make_list_of_inputs(self, images, text, box, color, multi_page):\n \n         return images, text, box, color\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         **kwargs: Unpack[GotOcr2ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text if `text`\n-        is not `None`, otherwise encode default OCR queries which depends on the `format`, `box`, `color`, `multi_page` and\n-        `crop_to_patches` arguments. To prepare the vision inputs, this method forwards the `images` and `kwargs` arguments to\n-        GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            format (`bool`, *optional*):\n-                If set, will add the format token to the query, and the model will return the OCR result with formatting.\n-            box (`list[float]`, `list[tuple[float, float]]`, `list[tuple[float, float, float, float]]`, *optional*):\n-                The box annotation to be added to the query. If a list of floats or a tuple of floats is provided, it\n-                will be interpreted as [x1, y1, x2, y2]. If a list of tuples is provided, each tuple should be in the\n-                form (x1, y1, x2, y2).\n-            color (`str`, *optional*):\n-                The color annotation to be added to the query. The model will return the OCR result within the box with\n-                the specified color.\n-            multi_page (`bool`, *optional*):\n-                If set, will enable multi-page inference. The model will return the OCR result across multiple pages.\n-            crop_to_patches (`bool`, *optional*):\n-                If set, will crop the image to patches. The model will return the OCR result upon the patch reference.\n-            min_patches (`int`, *optional*):\n-                The minimum number of patches to be cropped from the image. Only used when `crop_to_patches` is set to\n-                `True`.\n-            max_patches (`int`, *optional*):\n-                The maximum number of patches to be cropped from the image. Only used when `crop_to_patches` is set to\n-                `True`.\n-\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "1ffe267e3cf76ad903c42723f26f5995d2a47d7a",
            "filename": "src/transformers/models/granite_speech/processing_granite_speech.py",
            "status": "modified",
            "additions": 9,
            "deletions": 1,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fprocessing_granite_speech.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -18,7 +18,7 @@\n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import ProcessorMixin\n from ...tokenization_python import PreTokenizedInput, TextInput\n-from ...utils import is_torch_available, logging\n+from ...utils import auto_docstring, is_torch_available, logging\n from ...utils.import_utils import requires_backends\n \n \n@@ -28,6 +28,7 @@\n logger = logging.get_logger(__name__)\n \n \n+@auto_docstring\n class GraniteSpeechProcessor(ProcessorMixin):\n     def __init__(\n         self,\n@@ -36,9 +37,16 @@ def __init__(\n         audio_token=\"<|audio|>\",\n         chat_template=None,\n     ):\n+        r\"\"\"\n+        audio_token (`str`, *optional*, defaults to `\"<|audio|>\"`):\n+            The special token used to represent audio in the text sequence. This token serves as a placeholder\n+            that will be replaced with multiple audio tokens based on the actual audio length. The number of\n+            audio tokens inserted depends on the audio feature dimensions extracted by the audio processor.\n+        \"\"\"\n         self.audio_token = tokenizer.audio_token if hasattr(tokenizer, \"audio_token\") else audio_token\n         super().__init__(audio_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]],"
        },
        {
            "sha": "4f227bd0ea6c77c3b845de57fdb92d0bdee7e45e",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -1116,17 +1116,16 @@ class GraniteFlashAttentionKwargs(TypedDict, total=False):\n     Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n     Use cases include padding-free training and fewer `torch.compile` graph breaks.\n \n-    Attributes:\n-        cu_seq_lens_q (`torch.LongTensor`)\n-            Gets cumulative sequence length for query state.\n-        cu_seq_lens_k (`torch.LongTensor`)\n-            Gets cumulative sequence length for key state.\n-        max_length_q (`int`):\n-            Maximum sequence length for query state.\n-        max_length_k (`int`):\n-            Maximum sequence length for key state.\n-        seq_idx (`torch.IntTensor):\n-            Index of each packed sequence.\n+    cu_seq_lens_q (`torch.LongTensor`):\n+        Gets cumulative sequence length for query state.\n+    cu_seq_lens_k (`torch.LongTensor`):\n+        Gets cumulative sequence length for key state.\n+    max_length_q (`int`):\n+        Maximum sequence length for query state.\n+    max_length_k (`int`):\n+        Maximum sequence length for key state.\n+    seq_idx (`torch.IntTensor):\n+        Index of each packed sequence.\n     \"\"\"\n \n     cu_seq_lens_q: torch.LongTensor"
        },
        {
            "sha": "6b985c05a2c1b377ebc5bc71149fbcb4a284f04f",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -46,17 +46,16 @@ class GraniteFlashAttentionKwargs(TypedDict, total=False):\n     Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n     Use cases include padding-free training and fewer `torch.compile` graph breaks.\n \n-    Attributes:\n-        cu_seq_lens_q (`torch.LongTensor`)\n-            Gets cumulative sequence length for query state.\n-        cu_seq_lens_k (`torch.LongTensor`)\n-            Gets cumulative sequence length for key state.\n-        max_length_q (`int`):\n-            Maximum sequence length for query state.\n-        max_length_k (`int`):\n-            Maximum sequence length for key state.\n-        seq_idx (`torch.IntTensor):\n-            Index of each packed sequence.\n+    cu_seq_lens_q (`torch.LongTensor`):\n+        Gets cumulative sequence length for query state.\n+    cu_seq_lens_k (`torch.LongTensor`):\n+        Gets cumulative sequence length for key state.\n+    max_length_q (`int`):\n+        Maximum sequence length for query state.\n+    max_length_k (`int`):\n+        Maximum sequence length for key state.\n+    seq_idx (`torch.IntTensor):\n+        Index of each packed sequence.\n     \"\"\"\n \n     cu_seq_lens_q: torch.LongTensor"
        },
        {
            "sha": "efb03ad06a8752b8a0ba0ba1a7ca0d7b9a444288",
            "filename": "src/transformers/models/granitemoeshared/modular_granitemoeshared.py",
            "status": "modified",
            "additions": 10,
            "deletions": 11,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -38,17 +38,16 @@ class GraniteFlashAttentionKwargs(TypedDict, total=False):\n     Keyword arguments for advanced Flash Attention, causal-conv1d, and mamba_ssm kernel usage.\n     Use cases include padding-free training and fewer `torch.compile` graph breaks.\n \n-    Attributes:\n-        cu_seq_lens_q (`torch.LongTensor`)\n-            Gets cumulative sequence length for query state.\n-        cu_seq_lens_k (`torch.LongTensor`)\n-            Gets cumulative sequence length for key state.\n-        max_length_q (`int`):\n-            Maximum sequence length for query state.\n-        max_length_k (`int`):\n-            Maximum sequence length for key state.\n-        seq_idx (`torch.IntTensor):\n-            Index of each packed sequence.\n+    cu_seq_lens_q (`torch.LongTensor`):\n+        Gets cumulative sequence length for query state.\n+    cu_seq_lens_k (`torch.LongTensor`):\n+        Gets cumulative sequence length for key state.\n+    max_length_q (`int`):\n+        Maximum sequence length for query state.\n+    max_length_k (`int`):\n+        Maximum sequence length for key state.\n+    seq_idx (`torch.IntTensor):\n+        Index of each packed sequence.\n     \"\"\"\n \n     cu_seq_lens_q: torch.LongTensor"
        },
        {
            "sha": "22c51b3c90a6bd562d860d794fb0c61cac55d69d",
            "filename": "src/transformers/models/grounding_dino/processing_grounding_dino.py",
            "status": "modified",
            "additions": 3,
            "deletions": 30,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -22,7 +22,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n-from ...utils import TensorType, is_torch_available\n+from ...utils import TensorType, auto_docstring, is_torch_available\n \n \n if is_torch_available():\n@@ -113,47 +113,20 @@ class GroundingDinoProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class GroundingDinoProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Grounding DINO processor which wraps a Deformable DETR image processor and a BERT tokenizer into a\n-    single processor.\n-\n-    [`GroundingDinoProcessor`] offers all the functionalities of [`GroundingDinoImageProcessor`] and\n-    [`AutoTokenizer`]. See the docstring of [`~GroundingDinoProcessor.__call__`] and [`~GroundingDinoProcessor.decode`]\n-    for more information.\n-\n-    Args:\n-        image_processor (`GroundingDinoImageProcessor`):\n-            An instance of [`GroundingDinoImageProcessor`]. The image processor is a required input.\n-        tokenizer (`AutoTokenizer`):\n-            An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     valid_processor_kwargs = GroundingDinoProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[GroundingDinoProcessorKwargs],\n     ) -> BatchEncoding:\n-        \"\"\"\n-        This method uses [`GroundingDinoImageProcessor.__call__`] method to prepare image(s) for the model, and\n-        [`BertTokenizerFast.__call__`] to prepare text for the model.\n-\n-        Args:\n-            images (`ImageInput`, `list[ImageInput]`, *optional*):\n-                The image or batch of images to be processed. The image might be either PIL image, numpy array or a torch tensor.\n-            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`, *optional*):\n-                Candidate labels to be detected on the image. The text might be one of the following:\n-                - A list of candidate labels (strings) to be detected on the image (e.g. [\"a cat\", \"a dog\"]).\n-                - A batch of candidate labels to be detected on the batch of images (e.g. [[\"a cat\", \"a dog\"], [\"a car\", \"a person\"]]).\n-                - A merged candidate labels string to be detected on the image, separated by \".\" (e.g. \"a cat. a dog.\").\n-                - A batch of merged candidate labels text to be detected on the batch of images (e.g. [\"a cat. a dog.\", \"a car. a person.\"]).\n-        \"\"\"\n         if text is not None:\n             text = self._preprocess_input_text(text)\n         return super().__call__(images=images, text=text, **kwargs)"
        },
        {
            "sha": "b82703635a5b735b850e7b23a3b1dfe69d850c92",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 23,
            "deletions": 35,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -27,7 +27,7 @@\n     Unpack,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_torch_available\n+from ...utils import auto_docstring, is_torch_available\n \n \n if is_torch_available():\n@@ -38,6 +38,16 @@\n \n \n class IdeficsTextKwargs(TextKwargs, total=False):\n+    \"\"\"\n+    add_eos_token (`bool`, *optional*, defaults to `False`):\n+        Whether to add an end-of-sequence token at the end of the text input. When enabled, an EOS token is\n+        appended to mark the end of the text sequence, which is useful for generation tasks.\n+    add_end_of_utterance_token (`bool`, *optional*):\n+        Whether to add an end-of-utterance token to mark the end of a user's message in conversational contexts.\n+        This token helps the model distinguish between different utterances in a multi-turn conversation and is\n+        particularly important for chat-based models.\n+    \"\"\"\n+\n     add_eos_token: Optional[bool]\n     add_end_of_utterance_token: Optional[bool]\n \n@@ -133,25 +143,15 @@ def is_url(string):\n     return all([result.scheme, result.netloc])\n \n \n+@auto_docstring\n class IdeficsProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a IDEFICS processor which wraps a LLama tokenizer and IDEFICS image processor into a single processor.\n-\n-    [`IdeficsProcessor`] offers all the functionalities of [`IdeficsImageProcessor`] and [`LlamaTokenizerFast`]. See\n-    the docstring of [`~IdeficsProcessor.__call__`] and [`~IdeficsProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`IdeficsImageProcessor`):\n-            An instance of [`IdeficsImageProcessor`]. The image processor is a required input.\n-        tokenizer (`LlamaTokenizerFast`):\n-            An instance of [`LlamaTokenizerFast`]. The tokenizer is a required input.\n-        image_size (`int`, *optional*, defaults to 224):\n-            Image size (assuming a square image)\n-        add_end_of_utterance_token (`str`, *optional*):\n-            The string representation of token representing end of utterance\n-    \"\"\"\n-\n     def __init__(self, image_processor, tokenizer=None, image_size=224, add_end_of_utterance_token=None, **kwargs):\n+        r\"\"\"\n+        image_size (int, *optional*, defaults to 224):\n+            The size of the image to be processed.\n+        add_end_of_utterance_token (bool, *optional*, defaults to None):\n+            Whether to add the end of utterance token to the text.\n+        \"\"\"\n         super().__init__(image_processor, tokenizer)\n         self.image_token_id = (\n             tokenizer.image_token_id\n@@ -169,6 +169,7 @@ def __init__(self, image_processor, tokenizer=None, image_size=224, add_end_of_u\n             \"<end_of_utterance>\" in self.tokenizer.special_tokens_map.get(\"additional_special_tokens\", [])\n         )\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Union[ImageInput, list[ImageInput], str, list[str], list[list[str]]] = None,\n@@ -182,29 +183,16 @@ def __call__(\n         ] = None,\n         **kwargs: Unpack[IdeficsProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"This method takes batched or non-batched prompts made of text and images and converts them into prompts that\n-        the model was trained on and prepares the image pixel values for the model to process.\n-\n-        Args:\n-            images (`Union[ImageInput, list[ImageInput], str, list[str], list[list[str]]]`):\n-                either a single image or a batched list of images - can be passed in when text contains only text prompts,\n-                in order to use the image-text-to-text behavior.\n-            text (`Union[list[TextInput], [list[list[TextInput]]]]`):\n-                either a single prompt or a batched list of prompts - see the detailed description immediately after\n-                the end of the arguments doc section.\n-            return_tensors (`str` or `TensorType`, *optional*, defaults to `TensorType.PYTORCH`):\n-                The type of tensors to return. Can be one of:\n-                    - `TensorType.PYTORCH` or `'pt'`: Return a batch of type `torch.Tensor`.\n-\n+        r\"\"\"\n         Returns:\n             a dict with entries: `input_ids`, `attention_mask`, `pixel_values`, `image_attention_mask` which can be\n             directly passed to `model.generate`\n \n-        Detailed explanation:\n+            Detailed explanation:\n \n-        Each entry in `text` is either a text to be passed as is or an image that will be processed.\n+            Each entry in `text` is either a text to be passed as is or an image that will be processed.\n \n-        An image can be either an image object (`PIL.Image`) or a url from which the image can be retrieved.\n+            An image can be either an image object (`PIL.Image`) or a url from which the image can be retrieved.\n \n         When the processor encounters an image it'll inject `<fake_token_around_image><image><fake_token_around_image>`\n         entry into the prompt."
        },
        {
            "sha": "340c9397aa6fd9e70681cbc93a42868064085587",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 65,
            "changes": 73,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -27,7 +27,7 @@\n     Unpack,\n )\n from ...tokenization_utils_base import AddedToken, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n if TYPE_CHECKING:\n@@ -55,29 +55,17 @@ class Idefics2ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Idefics2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a IDEFICS2 processor which wraps a LLama tokenizer and IDEFICS2 image processor into a single processor.\n-\n-    [`IdeficsProcessor`] offers all the functionalities of [`Idefics2ImageProcessor`] and [`LlamaTokenizerFast`]. See\n-    the docstring of [`~IdeficsProcessor.__call__`] and [`~IdeficsProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`Idefics2ImageProcessor`):\n-            An instance of [`Idefics2ImageProcessor`]. The image processor is a required input.\n-        tokenizer (`PreTrainedTokenizerBase`, *optional*):\n-            An instance of [`PreTrainedTokenizerBase`]. This should correspond with the model's text model. The tokenizer is a required input.\n+    def __init__(\n+        self, image_processor, tokenizer=None, image_seq_len: int = 64, chat_template: Optional[str] = None, **kwargs\n+    ):\n+        r\"\"\"\n         image_seq_len (`int`, *optional*, defaults to 64):\n             The length of the image sequence i.e. the number of <image> tokens per image in the input.\n             This parameter is used to build the string from the input prompt and image tokens and should match the\n             config.perceiver_config.resampler_n_latents value for the model used.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n-    def __init__(\n-        self, image_processor, tokenizer=None, image_seq_len: int = 64, chat_template: Optional[str] = None, **kwargs\n-    ):\n+        \"\"\"\n         if not hasattr(tokenizer, \"image_token\"):\n             self.fake_image_token = AddedToken(\"<fake_token_around_image>\", normalized=False, special=True).content\n             self.image_token = AddedToken(\"<image>\", normalized=False, special=True).content\n@@ -107,58 +95,13 @@ def _extract_images_from_prompts(self, prompts):\n             prompt_images.append(images)\n         return prompt_images\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Union[ImageInput, list[ImageInput], list[list[ImageInput]]] = None,\n         text: Union[TextInput, \"PreTokenizedInput\", list[TextInput], list[\"PreTokenizedInput\"]] = None,\n         **kwargs: Unpack[Idefics2ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Processes the input prompts and returns a BatchEncoding.\n-\n-        Example:\n-\n-        ```python\n-        >>> import requests\n-        >>> from transformers import Idefics2Processor\n-        >>> from transformers.image_utils import load_image\n-\n-        >>> processor = Idefics2Processor.from_pretrained(\"HuggingFaceM4/idefics2-8b\", image_seq_len=2)\n-        >>> processor.image_processor.do_image_splitting = False  # Force as False to simplify the example\n-\n-        >>> url1 = \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n-        >>> url2 = \"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\"\n-\n-        >>> image1, image2 = load_image(url1), load_image(url2)\n-        >>> images = [[image1], [image2]]\n-\n-        >>> text = [\n-        ...     \"<image>In this image, we see\",\n-        ...     \"bla bla bla<image>\",\n-        ... ]\n-        >>> outputs = processor(images=images, text=text, return_tensors=\"pt\", padding=True)\n-        >>> input_ids = outputs.input_ids\n-        >>> input_tokens = processor.tokenizer.batch_decode(input_ids)\n-        >>> print(input_tokens)\n-        ['<s><fake_token_around_image><image><image><fake_token_around_image> In this image, we see', '<s> bla bla bla<fake_token_around_image><image><image><fake_token_around_image>']\n-        ```\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`, *optional*):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. If is of type `list[ImageInput]`, it's assumed that this is for a single prompt i.e. of batch size 1.\n-            text (`Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]`, *optional*):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-\n-                Wherever an image token, `<image>` is encountered it is expanded to\n-                `<fake_token_around_image>` + `<image>` * `image_seq_len` * <fake_token_around_image>`.\n-            return_tensors (`Union[str, TensorType]`, *optional*):\n-                If set, will return tensors of a particular framework. See [`PreTrainedTokenizerFast.__call__`] for more\n-                information.\n-\n-        \"\"\"\n         if text is None and images is None:\n             raise ValueError(\"You must provide either `text` or `images`.\")\n "
        },
        {
            "sha": "9e6d4db96b495c71822195d5683ae1a01d09a5d3",
            "filename": "src/transformers/models/idefics3/processing_idefics3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 65,
            "changes": 77,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fprocessing_idefics3.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -25,7 +25,7 @@\n from ...image_utils import ImageInput, is_valid_image, load_image\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AddedToken, BatchEncoding, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n if TYPE_CHECKING:\n@@ -100,29 +100,17 @@ class Idefics3ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Idefics3Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Idefics3 processor which wraps a LLama tokenizer and Idefics3 image processor into a single processor.\n-\n-    [`Idefics3Processor`] offers all the functionalities of [`Idefics3ImageProcessor`] and [`Idefics3TokenizerFast`]. See\n-    the docstring of [`~IdeficsProcessor.__call__`] and [`~IdeficsProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`Idefics3ImageProcessor`):\n-            An instance of [`Idefics3ImageProcessor`]. The image processor is a required input.\n-        tokenizer (`PreTrainedTokenizerBase`, *optional*):\n-            An instance of [`PreTrainedTokenizerBase`]. This should correspond with the model's text model. The tokenizer is a required input.\n+    def __init__(\n+        self, image_processor, tokenizer=None, image_seq_len: int = 169, chat_template: Optional[str] = None, **kwargs\n+    ):\n+        r\"\"\"\n         image_seq_len (`int`, *optional*, defaults to 169):\n             The length of the image sequence i.e. the number of <image> tokens per image in the input.\n             This parameter is used to build the string from the input prompt and image tokens and should match the\n             value the model used. It is computed as: image_seq_len = int(((image_size // patch_size) ** 2) / (scale_factor**2))\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n-    def __init__(\n-        self, image_processor, tokenizer=None, image_seq_len: int = 169, chat_template: Optional[str] = None, **kwargs\n-    ):\n+        \"\"\"\n         self.fake_image_token = AddedToken(\"<fake_token_around_image>\", normalized=False, special=True).content\n         self.image_token = AddedToken(\"<image>\", normalized=False, special=True).content\n         self.end_of_utterance_token = AddedToken(\"<end_of_utterance>\", normalized=False, special=True).content\n@@ -163,59 +151,18 @@ def _extract_images_from_prompts(self, prompts):\n             prompt_images.append(images)\n         return prompt_images\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Union[ImageInput, list[ImageInput], list[list[ImageInput]]] = None,\n         text: Union[TextInput, \"PreTokenizedInput\", list[TextInput], list[\"PreTokenizedInput\"]] = None,\n         image_seq_len: Optional[int] = None,\n         **kwargs: Unpack[Idefics3ProcessorKwargs],\n     ) -> BatchEncoding:\n-        \"\"\"\n-        Processes the input prompts and returns a BatchEncoding.\n-\n-        Example:\n-\n-        ```python\n-        >>> import requests\n-        >>> from transformers import Idefics3Processor\n-        >>> from transformers.image_utils import load_image\n-\n-        >>> processor = Idefics3Processor.from_pretrained(\"HuggingFaceM4/Idefics3-8B-Llama3\")\n-        >>> processor.image_processor.do_image_splitting = False  # Force as False to simplify the example\n-\n-        >>> url1 = \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n-        >>> url2 = \"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\"\n-\n-        >>> image1, image2 = load_image(url1), load_image(url2)\n-        >>> images = [[image1], [image2]]\n-\n-        >>> text = [\n-        ...     \"<image>In this image, we see\",\n-        ...     \"bla bla bla<image>\",\n-        ... ]\n-        >>> outputs = processor(images=images, text=text, return_tensors=\"pt\", padding=True)\n-        >>> input_ids = outputs.input_ids\n-        >>> input_tokens = processor.tokenizer.batch_decode(input_ids)\n-        >>> print(input_tokens)\n-        ['<|begin_of_text|><fake_token_around_image><global-img>((<image>)*169)<fake_token_around_image> In this image, we see', '<|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|begin_of_text|>bla bla bla<fake_token_around_image><global-img>((<image>)*169)<fake_token_around_image>']\n-        ```\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`, *optional*):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. If is of type `list[ImageInput]`, it's assumed that this is for a single prompt i.e. of batch size 1.\n-            text (`Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]`, *optional*):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-                Wherever an image token, `<image>` is encountered it is expanded to\n-                `<fake_token_around_image>` + `<row_x_col_y>` + `<image>` * `image_seq_len` * <fake_token_around_image>`.\n-            image_seq_len (`int`, *optional*):\n-                The length of the image sequence. If not provided, the default value of self.image_seq_len is used.\n-                image_seq_len should be equal to int(((image_size // patch_size) ** 2) / (scale_factor**2))\n-            return_tensors (`Union[str, TensorType]`, *optional*):\n-                If set, will return tensors of a particular framework. See [`PreTrainedTokenizerFast.__call__`] for more\n-                information.\n+        r\"\"\"\n+        image_seq_len (`int`, *optional*):\n+            The length of the image sequence. If not provided, the default value of self.image_seq_len is used.\n+            image_seq_len should be equal to int(((image_size // patch_size) ** 2) / (scale_factor**2))\n         \"\"\"\n         if text is None and images is None:\n             raise ValueError(\"You must provide either `text` or `images`.\")"
        },
        {
            "sha": "16ec43294f025fae18453be3ad2da03f720e0f9f",
            "filename": "src/transformers/models/instructblip/processing_instructblip.py",
            "status": "modified",
            "additions": 8,
            "deletions": 31,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fprocessing_instructblip.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -21,7 +21,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AddedToken, PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -43,26 +43,16 @@ class InstructBlipProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class InstructBlipProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs an InstructBLIP processor which wraps a BLIP image processor and a LLaMa/T5 tokenizer into a single\n-    processor.\n-\n-    [`InstructBlipProcessor`] offers all the functionalities of [`BlipImageProcessor`] and [`AutoTokenizer`]. See the\n-    docstring of [`~BlipProcessor.__call__`] and [`~BlipProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`BlipImageProcessor`):\n-            An instance of [`BlipImageProcessor`]. The image processor is a required input.\n-        tokenizer (`AutoTokenizer`):\n-            An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n+    def __init__(self, image_processor, tokenizer, qformer_tokenizer, num_query_tokens=None, **kwargs):\n+        r\"\"\"\n         qformer_tokenizer (`AutoTokenizer`):\n             An instance of ['PreTrainedTokenizer`]. The Q-Former tokenizer is a required input.\n-        num_query_tokens (`int`, *optional*):\"\n+        num_query_tokens (`int`, *optional*):\n+            \"\n             Number of tokens used by the Qformer as queries, should be same as in model's config.\n-    \"\"\"\n-\n-    def __init__(self, image_processor, tokenizer, qformer_tokenizer, num_query_tokens=None, **kwargs):\n+        \"\"\"\n         if not hasattr(tokenizer, \"image_token\"):\n             self.image_token = AddedToken(\"<image>\", normalized=False, special=True)\n             tokenizer.add_tokens([self.image_token], special_tokens=True)\n@@ -72,26 +62,13 @@ def __init__(self, image_processor, tokenizer, qformer_tokenizer, num_query_toke\n \n         super().__init__(image_processor, tokenizer, qformer_tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[InstructBlipProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        This method uses [`BlipImageProcessor.__call__`] method to prepare image(s) for the model, and\n-        [`BertTokenizerFast.__call__`] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        Args:\n-            images (`ImageInput`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-        \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You have to specify at least images or text.\")\n "
        },
        {
            "sha": "10bfb5ab643c454b402b142f373483a8f43ff3d4",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 22,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -26,33 +26,22 @@\n     TextInput,\n     TruncationStrategy,\n )\n-from ...utils import TensorType, logging\n+from ...utils import TensorType, auto_docstring, logging\n from ...video_utils import VideoInput\n \n \n logger = logging.get_logger(__name__)\n \n \n+@auto_docstring\n class InstructBlipVideoProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs an InstructBLIPVideo processor which wraps a InstructBLIP image processor and a LLaMa/T5 tokenizer into a single\n-    processor.\n-\n-    [`InstructBlipVideoProcessor`] offers all the functionalities of [`InstructBlipVideoVideoProcessor`] and [`AutoTokenizer`]. See the\n-    docstring of [`~InstructBlipVideoProcessor.__call__`] and [`~InstructBlipVideoProcessor.decode`] for more information.\n-\n-    Args:\n-        video_processor (`InstructBlipVideoVideoProcessor`):\n-            An instance of [`InstructBlipVideoVideoProcessor`]. The video processor is a required input.\n-        tokenizer (`AutoTokenizer`):\n-            An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n+    def __init__(self, video_processor, tokenizer, qformer_tokenizer, num_query_tokens=None, **kwargs):\n+        r\"\"\"\n         qformer_tokenizer (`AutoTokenizer`):\n             An instance of ['PreTrainedTokenizer`]. The Q-Former tokenizer is a required input.\n         num_query_tokens (`int`, *optional*):\n             Number of tokens used by the Qformer as queries, should be same as in model's config.\n-    \"\"\"\n-\n-    def __init__(self, video_processor, tokenizer, qformer_tokenizer, num_query_tokens=None, **kwargs):\n+        \"\"\"\n         if not hasattr(tokenizer, \"video_token\"):\n             self.video_token = AddedToken(\"<video>\", normalized=False, special=True)\n             tokenizer.add_tokens([self.video_token], special_tokens=True)\n@@ -61,6 +50,7 @@ def __init__(self, video_processor, tokenizer, qformer_tokenizer, num_query_toke\n         self.num_query_tokens = num_query_tokens\n         super().__init__(video_processor, tokenizer, qformer_tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[VideoInput] = None,\n@@ -81,12 +71,6 @@ def __call__(\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n     ) -> BatchFeature:\n-        \"\"\"\n-        This method uses [`InstructBlipVideoVideoProcessor.__call__`] method to prepare image(s) or video(s) for the model, and\n-        [`BertTokenizerFast.__call__`] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You have to specify at least one of images or text.\")\n "
        },
        {
            "sha": "ca19c94f01a6729958d0f25f75b505cb5147e806",
            "filename": "src/transformers/models/internvl/processing_internvl.py",
            "status": "modified",
            "additions": 9,
            "deletions": 40,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fprocessing_internvl.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,6 +20,7 @@\n from ...image_utils import ImageInput, concatenate_list, make_flat_list_of_images\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n from ...video_utils import VideoInput\n \n \n@@ -38,25 +39,8 @@ class InternVLProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class InternVLProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a InternVL processor which wraps a [`AutoImageProcessor`] and\n-    [`PretrainedTokenizerFast`] tokenizer into a single processor that inherits both the image processor and\n-    tokenizer functionalities. See the [`~InternVLProcessor.__call__`] and [`~InternVLProcessor.decode`] for more information.\n-    Args:\n-        image_processor ([`AutoImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`AutoVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        image_seq_length (`int`, *optional*, defaults to 256):\n-            The number of image token to use per image patch. it should be set so that:\n-            image_seq_length = (config.image_size // config.patch_size) ** 2 * (config.scale_factor**2)\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -66,6 +50,11 @@ def __init__(\n         chat_template=None,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        image_seq_length (`int`, *optional*, defaults to 256):\n+            The number of image token to use per image patch. it should be set so that:\n+            image_seq_length = (config.image_size // config.patch_size) ** 2 * (config.scale_factor**2)\n+        \"\"\"\n         self.image_seq_length = image_seq_length\n         self.start_image_token = tokenizer.start_image_token\n         self.end_image_token = tokenizer.end_image_token\n@@ -142,35 +131,15 @@ def _insert_media_placeholders(\n \n         return processed_text, image_video_patches, image_index, video_index\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[InternVLProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text if `text`\n-        is not `None`, otherwise encode default OCR queries which depends on the `format`, `box`, `color`, `multi_page` and\n-        `crop_to_patches` arguments. To prepare the vision inputs, this method forwards the `images` and `kwargs` arguments to\n-        GotOcr2ImageProcessor's [`~GotOcr2ImageProcessor.__call__`] if `images` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "757fc080264de88dac4c7f7ca5a11382037e329d",
            "filename": "src/transformers/models/janus/processing_janus.py",
            "status": "modified",
            "additions": 15,
            "deletions": 38,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fprocessing_janus.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -21,7 +21,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -34,6 +34,13 @@\n \n \n class JanusTextKwargs(TextKwargs, total=False):\n+    \"\"\"\n+    generation_mode (`str`, *optional*, defaults to `\"text\"`):\n+        The generation mode indicating which modality to generate. Can be one of `\"text\"` or `\"image\"`. When set\n+        to `\"text\"`, the processor prepares inputs for text generation. When set to `\"image\"`, it prepares inputs\n+        for image generation by appending image start tokens to the prompt.\n+    \"\"\"\n+\n     generation_mode: str\n \n \n@@ -45,25 +52,13 @@ class JanusProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class JanusProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Janus processor which wraps a Janus Image Processor and a Llama tokenizer into a single processor.\n-\n-    [`JanusProcessor`] offers all the functionalities of [`JanusImageProcessor`] and [`LlamaTokenizerFast`]. See the\n-    [`~JanusProcessor.__call__`] and [`~JanusProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`JanusImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`]):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        use_default_system_prompt (`str`, *optional*, defaults to `False`):\n-            Use default system prompt for Text Generation.\n-    \"\"\"\n-\n     def __init__(self, image_processor, tokenizer, chat_template=None, use_default_system_prompt=False, **kwargs):\n+        r\"\"\"\n+        use_default_system_prompt (`bool`, *optional*, defaults to `False`):\n+            Use default system prompt for Text Generation.\n+        \"\"\"\n         self.num_image_tokens = 576\n         self.image_token = tokenizer.image_token\n         self.image_start_token = tokenizer.boi_token\n@@ -72,32 +67,14 @@ def __init__(self, image_processor, tokenizer, chat_template=None, use_default_s\n \n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         images: Optional[ImageInput] = None,\n         **kwargs: Unpack[JanusProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        JanusImageProcessor's [`~JanusImageProcessor.__call__`] if `images` is not `None`. Please refer to the doctsring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "f67c1cee42cfb129282fd54506e0b788c0525e18",
            "filename": "src/transformers/models/kosmos2/processing_kosmos2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 36,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fprocessing_kosmos2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -23,6 +23,7 @@\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_python import AddedToken\n from ...tokenization_utils_base import BatchEncoding, TextInput\n+from ...utils import auto_docstring\n \n \n BboxInput = Union[\n@@ -36,12 +37,28 @@\n \n \n class Kosmos2ImagesKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    bboxes (`Union[list[tuple[int]], list[tuple[float]], list[list[tuple[int]]], list[list[tuple[float]]]]`, *optional*):\n+        The bounding bboxes associated to `texts`.\n+    num_image_tokens (`int`, *optional* defaults to 64):\n+        The number of (consecutive) places that are used to mark the placeholders to store image information.\n+        This should be the same as `latent_query_num` in the instance of `Kosmos2Config` you are using.\n+    first_image_token_id (`int`, *optional*):\n+        The token id that will be used for the first place of the subsequence that is reserved to store image\n+        information. If unset, will default to `self.tokenizer.unk_token_id + 1`.\n+    \"\"\"\n+\n     bboxes: Optional[NestedList]  # NOTE: hub validators can't accept `Sequence`\n     num_image_tokens: int\n     first_image_token_id: Optional[int]\n \n \n class Kosmos2TextKwargs(TextKwargs, total=False):\n+    \"\"\"\n+    add_eos_token (`bool`, defaults to `False`):\n+    Whether or not to include `EOS` token id in the encoding when `add_special_tokens=True`.\n+    \"\"\"\n+\n     add_eos_token: bool\n \n \n@@ -66,25 +83,13 @@ class Kosmos2ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Kosmos2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs an KOSMOS-2 processor which wraps a KOSMOS-2 image processor and a KOSMOS-2 tokenizer into a single\n-    processor.\n-\n-    [`Kosmos2Processor`] offers all the functionalities of [`CLIPImageProcessor`] and some functionalities of\n-    [`XLMRobertaTokenizerFast`]. See the docstring of [`~Kosmos2Processor.__call__`] and [`~Kosmos2Processor.decode`]\n-    for more information.\n-\n-    Args:\n-        image_processor (`CLIPImageProcessor`):\n-            An instance of [`CLIPImageProcessor`]. The image processor is a required input.\n-        tokenizer (`XLMRobertaTokenizerFast`):\n-            An instance of ['XLMRobertaTokenizerFast`]. The tokenizer is a required input.\n+    def __init__(self, image_processor, tokenizer, num_patch_index_tokens=1024, *kwargs):\n+        r\"\"\"\n         num_patch_index_tokens (`int`, *optional*, defaults to 1024):\n             The number of tokens that represent patch indices.\n-    \"\"\"\n-\n-    def __init__(self, image_processor, tokenizer, num_patch_index_tokens=1024, *kwargs):\n+        \"\"\"\n         tokenizer.return_token_type_ids = False\n \n         self.eod_token = \"</doc>\"\n@@ -129,32 +134,13 @@ def __init__(self, image_processor, tokenizer, num_patch_index_tokens=1024, *kwa\n \n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, list[TextInput]] = None,\n         **kwargs: Unpack[Kosmos2ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        This method uses [`CLIPImageProcessor.__call__`] method to prepare image(s) for the model, and\n-        [`XLMRobertaTokenizerFast.__call__`] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-\n-        The rest of this documentation shows the arguments specific to `Kosmos2Processor`.\n-\n-        Args:\n-            bboxes (`Union[list[tuple[int]], list[tuple[float]], list[list[tuple[int]]], list[list[tuple[float]]]]`, *optional*):\n-                The bounding bboxes associated to `texts`.\n-            num_image_tokens (`int`, *optional* defaults to 64):\n-                The number of (consecutive) places that are used to mark the placeholders to store image information.\n-                This should be the same as `latent_query_num` in the instance of `Kosmos2Config` you are using.\n-            first_image_token_id (`int`, *optional*):\n-                The token id that will be used for the first place of the subsequence that is reserved to store image\n-                information. If unset, will default to `self.tokenizer.unk_token_id + 1`.\n-            add_eos_token (`bool`, defaults to `False`):\n-                Whether or not to include `EOS` token id in the encoding when `add_special_tokens=True`.\n-        \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You have to specify either images or text.\")\n "
        },
        {
            "sha": "6a097aa74753cd64f758bd18c99a6532502e227b",
            "filename": "src/transformers/models/kosmos2_5/image_processing_kosmos2_5_fast.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fimage_processing_kosmos2_5_fast.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -71,13 +71,6 @@ def __init__(self, **kwargs: Unpack[Kosmos2_5ImageProcessorKwargs]):\n \n     @auto_docstring\n     def preprocess(self, images: ImageInput, **kwargs: Unpack[Kosmos2_5ImageProcessorKwargs]) -> BatchFeature:\n-        r\"\"\"\n-        patch_size (`Dict[str, int]`, *optional*, defaults to `{\"height\": 16, \"width\": 16}`):\n-            The patch size to use for the image. According to Kosmos2_5 paper and code, the patch size is 16x16.\n-        max_patches (`int`, *optional*, defaults to 4096):\n-            The maximum number of patches to extract from the image as per the\n-            [KOSMOS 2.5 paper](https://huggingface.co/papers/2309.11419).\n-        \"\"\"\n         # return super().preprocess(images, **kwargs)\n         # TODO: revert once the issue is fixed: https://huggingface.slack.com/archives/C02TXKQQLE5/p1743411133979019\n         return super().preprocess(images, image_mean=0.0, image_std=0.0, **kwargs)"
        },
        {
            "sha": "62065da6f7703890738af01b6b7e7ddfc697542e",
            "filename": "src/transformers/models/kosmos2_5/processing_kosmos2_5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 24,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2_5%2Fprocessing_kosmos2_5.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -21,7 +21,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import TextInput\n-from ...utils import is_torch_available\n+from ...utils import auto_docstring, is_torch_available\n \n \n if is_torch_available():\n@@ -43,44 +43,26 @@ class Kosmos2_5ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Kosmos2_5Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Kosmos2_5 processor which wraps a PreTrainedTokenizerFast and Kosmos2_5 image processor into a single\n-    processor.\n-\n-    [`Kosmos2_5Processor`] offers all the functionalities of [`Kosmos2_5ImageProcessor`] and [`PreTrainedTokenizerFast`]. See\n-    the docstring of [`~Kosmos2_5Processor.__call__`] and [`~Kosmos2_5Processor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`Kosmos2_5ImageProcessor`):\n-            An instance of [`Kosmos2_5ImageProcessor`]. The image processor is a required input.\n-        tokenizer (`T5Tokenizer`):\n-            An instance of ['T5Tokenizer`]. The tokenizer is a required input.\n+    def __init__(self, image_processor, tokenizer, num_image_tokens: int = 2048):\n+        r\"\"\"\n         num_image_tokens (`int`, *optional*, defaults to 2048):\n             Number of image tokens used as a placeholder.\n-    \"\"\"\n-\n-    def __init__(self, image_processor, tokenizer, num_image_tokens: int = 2048):\n+        \"\"\"\n         self.image_start_token = tokenizer.boi_token  # \"<image>\" : fixed token for the start of image\n         self.image_end_token = tokenizer.eoi_token  # \"</image>\" : fixed token for the end of image\n         self.image_token = tokenizer.image_token  # \"<s>\" : within a <image> ... </image> pair, these <s> tokens indicate they are positions reserved for an image\n         self.num_image_tokens = num_image_tokens\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, list[TextInput]] = None,\n         **kwargs: Unpack[Kosmos2_5ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        This method uses [`Kosmos2_5ImageProcessor.preprocess`] method to prepare image(s) for the model, and\n-        [`PreTrainedTokenizerFast.__call__`] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-\n-        The rest of this documentation shows the arguments specific to `Kosmos2_5Processor`.\n-        \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You have to specify either images or text.\")\n "
        },
        {
            "sha": "8616ac9a5859daf8912eb2616ecf5dee998ab869",
            "filename": "src/transformers/models/kyutai_speech_to_text/processing_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 7,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fprocessing_kyutai_speech_to_text.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -14,6 +14,7 @@\n \n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+from ...utils import auto_docstring\n \n \n class KyutaiSpeechToTextProcessorKwargs(ProcessingKwargs, total=False):\n@@ -25,14 +26,8 @@ class KyutaiSpeechToTextProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class KyutaiSpeechToTextProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Moshi ASR processor which wraps [`EncodecFeatureExtractor`] and\n-    [`PreTrainedTokenizerFast`] into a single processor that inherits both the audio feature extraction and\n-    tokenizer functionalities. See the [`~KyutaiSpeechToTextProcessor.__call__`] for more\n-    information.\n-    \"\"\"\n-\n     valid_processor_kwargs = KyutaiSpeechToTextProcessorKwargs\n \n     def __init__(self, feature_extractor, tokenizer):"
        },
        {
            "sha": "f9838f67d90b4020d746d2064463b75df4c9ba41",
            "filename": "src/transformers/models/lasr/processing_lasr.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flasr%2Fprocessing_lasr.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -23,7 +23,7 @@\n from ...audio_utils import AudioInput, make_list_of_audio\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -45,17 +45,26 @@ class LasrProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class LasrProcessor(ProcessorMixin):\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         audio: AudioInput,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput], None] = None,\n         sampling_rate: Optional[int] = None,\n         **kwargs: Unpack[LasrProcessorKwargs],\n     ):\n+        r\"\"\"\n+        sampling_rate (`int`, *optional*):\n+            The sampling rate of the input audio in Hz. This should match the sampling rate expected by the feature\n+            extractor (defaults to 16000 Hz). If provided, it will be validated against the processor's expected\n+            sampling rate, and an error will be raised if they don't match. If not provided, a warning will be\n+            issued and the default sampling rate will be assumed.\n+        \"\"\"\n         audio = make_list_of_audio(audio)\n \n         output_kwargs = self._merge_kwargs("
        },
        {
            "sha": "5c13854bf1f4c186ec627967dab8839488e4049d",
            "filename": "src/transformers/models/layoutlmv2/processing_layoutlmv2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 30,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fprocessing_layoutlmv2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -19,32 +19,15 @@\n \n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+from ...utils import TensorType, auto_docstring\n \n \n+@auto_docstring\n class LayoutLMv2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a LayoutLMv2 processor which combines a LayoutLMv2 image processor and a LayoutLMv2 tokenizer into a\n-    single processor.\n-\n-    [`LayoutLMv2Processor`] offers all the functionalities you need to prepare data for the model.\n-\n-    It first uses [`LayoutLMv2ImageProcessor`] to resize document images to a fixed size, and optionally applies OCR to\n-    get words and normalized bounding boxes. These are then provided to [`LayoutLMv2Tokenizer`] or\n-    [`LayoutLMv2TokenizerFast`], which turns the words and bounding boxes into token-level `input_ids`,\n-    `attention_mask`, `token_type_ids`, `bbox`. Optionally, one can provide integer `word_labels`, which are turned\n-    into token-level `labels` for token classification tasks (such as FUNSD, CORD).\n-\n-    Args:\n-        image_processor (`LayoutLMv2ImageProcessor`, *optional*):\n-            An instance of [`LayoutLMv2ImageProcessor`]. The image processor is a required input.\n-        tokenizer (`LayoutLMv2Tokenizer` or `LayoutLMv2TokenizerFast`, *optional*):\n-            An instance of [`LayoutLMv2Tokenizer`] or [`LayoutLMv2TokenizerFast`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images,\n@@ -68,16 +51,6 @@ def __call__(\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n     ) -> BatchEncoding:\n-        \"\"\"\n-        This method first forwards the `images` argument to [`~LayoutLMv2ImageProcessor.__call__`]. In case\n-        [`LayoutLMv2ImageProcessor`] was initialized with `apply_ocr` set to `True`, it passes the obtained words and\n-        bounding boxes along with the additional arguments to [`~LayoutLMv2Tokenizer.__call__`] and returns the output,\n-        together with resized `images`. In case [`LayoutLMv2ImageProcessor`] was initialized with `apply_ocr` set to\n-        `False`, it passes the words (`text`/``text_pair`) and `boxes` specified by the user along with the additional\n-        arguments to [`~LayoutLMv2Tokenizer.__call__`] and returns the output, together with resized `images``.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n         # verify input\n         if self.image_processor.apply_ocr and (boxes is not None):\n             raise ValueError("
        },
        {
            "sha": "652aaa0cd074e83d57965350ec08f4aee86bffaf",
            "filename": "src/transformers/models/layoutlmv3/processing_layoutlmv3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 32,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv3%2Fprocessing_layoutlmv3.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -19,32 +19,15 @@\n \n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+from ...utils import TensorType, auto_docstring\n \n \n+@auto_docstring\n class LayoutLMv3Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a LayoutLMv3 processor which combines a LayoutLMv3 image processor and a LayoutLMv3 tokenizer into a\n-    single processor.\n-\n-    [`LayoutLMv3Processor`] offers all the functionalities you need to prepare data for the model.\n-\n-    It first uses [`LayoutLMv3ImageProcessor`] to resize and normalize document images, and optionally applies OCR to\n-    get words and normalized bounding boxes. These are then provided to [`LayoutLMv3Tokenizer`] or\n-    [`LayoutLMv3TokenizerFast`], which turns the words and bounding boxes into token-level `input_ids`,\n-    `attention_mask`, `token_type_ids`, `bbox`. Optionally, one can provide integer `word_labels`, which are turned\n-    into token-level `labels` for token classification tasks (such as FUNSD, CORD).\n-\n-    Args:\n-        image_processor (`LayoutLMv3ImageProcessor`, *optional*):\n-            An instance of [`LayoutLMv3ImageProcessor`]. The image processor is a required input.\n-        tokenizer (`LayoutLMv3Tokenizer` or `LayoutLMv3TokenizerFast`, *optional*):\n-            An instance of [`LayoutLMv3Tokenizer`] or [`LayoutLMv3TokenizerFast`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images,\n@@ -68,18 +51,6 @@ def __call__(\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n     ) -> BatchEncoding:\n-        \"\"\"\n-        This method first forwards the `images` argument to [`~LayoutLMv3ImageProcessor.__call__`]. In case\n-        [`LayoutLMv3ImageProcessor`] was initialized with `apply_ocr` set to `True`, it passes the obtained words and\n-        bounding boxes along with the additional arguments to [`~LayoutLMv3Tokenizer.__call__`] and returns the output,\n-        together with resized and normalized `pixel_values`. In case [`LayoutLMv3ImageProcessor`] was initialized with\n-        `apply_ocr` set to `False`, it passes the words (`text`/``text_pair`) and `boxes` specified by the user along\n-        with the additional arguments to [`~LayoutLMv3Tokenizer.__call__`] and returns the output, together with\n-        resized and normalized `pixel_values`.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n-        # verify input\n         if self.image_processor.apply_ocr and (boxes is not None):\n             raise ValueError(\n                 \"You cannot provide bounding boxes if you initialized the image processor with apply_ocr set to True.\""
        },
        {
            "sha": "39bb79b5de00fd932c20b902fdfabca9ec4a0872",
            "filename": "src/transformers/models/layoutxlm/processing_layoutxlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 30,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutxlm%2Fprocessing_layoutxlm.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -19,32 +19,15 @@\n \n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType\n+from ...utils import TensorType, auto_docstring\n \n \n+@auto_docstring\n class LayoutXLMProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a LayoutXLM processor which combines a LayoutXLM image processor and a LayoutXLM tokenizer into a single\n-    processor.\n-\n-    [`LayoutXLMProcessor`] offers all the functionalities you need to prepare data for the model.\n-\n-    It first uses [`LayoutLMv2ImageProcessor`] to resize document images to a fixed size, and optionally applies OCR to\n-    get words and normalized bounding boxes. These are then provided to [`LayoutXLMTokenizer`] or\n-    [`LayoutXLMTokenizerFast`], which turns the words and bounding boxes into token-level `input_ids`,\n-    `attention_mask`, `token_type_ids`, `bbox`. Optionally, one can provide integer `word_labels`, which are turned\n-    into token-level `labels` for token classification tasks (such as FUNSD, CORD).\n-\n-    Args:\n-        image_processor (`LayoutLMv2ImageProcessor`, *optional*):\n-            An instance of [`LayoutLMv2ImageProcessor`]. The image processor is a required input.\n-        tokenizer (`LayoutXLMTokenizer` or `LayoutXLMTokenizerFast`, *optional*):\n-            An instance of [`LayoutXLMTokenizer`] or [`LayoutXLMTokenizerFast`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images,\n@@ -68,16 +51,6 @@ def __call__(\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n     ) -> BatchEncoding:\n-        \"\"\"\n-        This method first forwards the `images` argument to [`~LayoutLMv2ImagePrpcessor.__call__`]. In case\n-        [`LayoutLMv2ImagePrpcessor`] was initialized with `apply_ocr` set to `True`, it passes the obtained words and\n-        bounding boxes along with the additional arguments to [`~LayoutXLMTokenizer.__call__`] and returns the output,\n-        together with resized `images`. In case [`LayoutLMv2ImagePrpcessor`] was initialized with `apply_ocr` set to\n-        `False`, it passes the words (`text`/``text_pair`) and `boxes` specified by the user along with the additional\n-        arguments to [`~LayoutXLMTokenizer.__call__`] and returns the output, together with resized `images``.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n         # verify input\n         if self.image_processor.apply_ocr and (boxes is not None):\n             raise ValueError("
        },
        {
            "sha": "66163f28f4ba32ef55f484d455154def6858b9d8",
            "filename": "src/transformers/models/lfm2_vl/image_processing_lfm2_vl_fast.py",
            "status": "modified",
            "additions": 31,
            "deletions": 0,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fimage_processing_lfm2_vl_fast.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -172,6 +172,37 @@ class Lfm2VlImageProcessorKwargs(ImagesKwargs, total=False):\n     \"\"\"\n     downsample_factor (`int`, *optional*, defaults to `2`):\n         The downsampling factor for images used when resizing the image.\n+    do_image_splitting (`bool`, *optional*, defaults to `True`):\n+        Whether to split large images into a grid of smaller tiles. When enabled, images exceeding the maximum token\n+        limit are divided into multiple tiles based on `min_tiles` and `max_tiles` constraints.\n+    min_tiles (`int`, *optional*, defaults to `2`):\n+        Minimum number of tiles (width Ã— height) to use when splitting an image into a grid. The grid configuration\n+        is chosen to maintain the original aspect ratio while staying within the `min_tiles` and `max_tiles` range.\n+    max_tiles (`int`, *optional*, defaults to `10`):\n+        Maximum number of tiles (width Ã— height) to use when splitting an image into a grid. The grid configuration\n+        is chosen to maintain the original aspect ratio while staying within the `min_tiles` and `max_tiles` range.\n+    use_thumbnail (`bool`, *optional*, defaults to `True`):\n+        Whether to include a thumbnail version of the image when splitting into tiles. The thumbnail provides a\n+        low-resolution overview of the entire image and is added as an additional patch when the grid has more than\n+        one tile.\n+    min_image_tokens (`int`, *optional*, defaults to `64`):\n+        Minimum number of image tokens (patches) to generate for an image. Images smaller than this threshold will\n+        be upscaled to meet the minimum token requirement.\n+    max_image_tokens (`int`, *optional*, defaults to `256`):\n+        Maximum number of image tokens (patches) allowed for a single image. Images exceeding this limit will be\n+        split into multiple tiles or downscaled accordingly.\n+    encoder_patch_size (`int`, *optional*, defaults to `16`):\n+        The patch size used by the vision encoder. Images are divided into patches of this size, and both height\n+        and width must be divisible by this value (after accounting for the downsampling factor).\n+    tile_size (`int`, *optional*, defaults to `512`):\n+        The size of each tile when splitting large images into a grid. Each tile will be resized to this dimension\n+        before being processed into patches.\n+    max_pixels_tolerance (`float`, *optional*, defaults to `2.0`):\n+        Tolerance factor for determining if an image is too large. An image is considered too large if its pixel\n+        count exceeds `max_image_tokens * encoder_patch_size^2 * downsample_factor^2 * max_pixels_tolerance`.\n+    return_row_col_info (`bool`, *optional*, defaults to `False`):\n+        Whether to return row and column information for each image in the batch. When enabled, the output includes\n+        `image_rows`, `image_cols`, and `image_sizes` fields indicating the grid layout and dimensions of processed images.\n     \"\"\"\n \n     downsample_factor: int"
        },
        {
            "sha": "19d6ca5250f70e8bd96ae5b6672e27d250a498b5",
            "filename": "src/transformers/models/lfm2_vl/processing_lfm2_vl.py",
            "status": "modified",
            "additions": 10,
            "deletions": 28,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -23,13 +23,20 @@\n     Unpack,\n )\n from ...tokenization_utils_base import BatchEncoding, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n class Lfm2VlTextKwargs(TextKwargs, total=False):\n+    \"\"\"\n+    use_image_special_tokens (`bool`, *optional*, defaults to `True`):\n+        Whether to use special image tokens (`<|image_start|>` and `<|image_end|>`) to delimit image sequences\n+        in the text. When enabled, images are wrapped with these tokens to clearly mark image boundaries.\n+        When disabled, only the image token itself is used without delimiters.\n+    \"\"\"\n+\n     use_image_special_tokens: Optional[bool]\n \n \n@@ -48,21 +55,8 @@ class Lfm2VlProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Lfm2VlProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Lfm2Vl processor which wraps a Lfm2Tokenizer tokenizer and Lfm2VlImageProcessor into a single processor.\n-\n-    [`Lfm2VlProcessor`] offers all the functionalities of [`Lfm2ImageProcessor`] and [`Lfm2Tokenizer`].\n-\n-    Args:\n-        image_processor (`Lfm2VlImageProcessor`):\n-             An instance of [`Lfm2VlImageProcessor`]. The image processor is a required input.\n-        tokenizer (`PreTrainedTokenizerBase`):\n-            An instance of [`PreTrainedTokenizerBase`]. This should correspond with the model's text model. The tokenizer is a required input.\n-        chat_template (`str`, *optional*):\n-            A Jinja template which will be used to convert lists of messages in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor,\n@@ -81,25 +75,13 @@ def __init__(\n         self.image_thumbnail_token = getattr(tokenizer, \"image_thumbnail_token\", \"<|img_thumbnail|>\")\n         super().__init__(image_processor, tokenizer, chat_template=chat_template, **kwargs)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[Union[ImageInput, list[ImageInput], list[list[ImageInput]]]] = None,\n         text: Optional[Union[TextInput, list[TextInput]]] = None,\n         **kwargs: Unpack[Lfm2VlProcessorKwargs],\n     ) -> BatchEncoding:\n-        \"\"\"\n-        Processes the input prompts and returns a BatchFeature.\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`, *optional*):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. If is of type `list[ImageInput]`, it's assumed that this is for a single prompt i.e. of batch size 1.\n-            text (`TextInput`, *optional*):\n-                The sequence or batch of sequences to be encoded.\n-                Wherever an image token, `<image>` is encountered it is expanded to a proper sequence of image tokens.\n-            return_tensors (`Optional[str, TensorType]`, *optional*):\n-                If set, will return tensors of a particular framework. See [`PreTrainedTokenizerFast.__call__`] for more\n-                information.\n-        \"\"\"\n         if text is None and images is None:\n             raise ValueError(\"You must provide one of `text` or `images`.\")\n "
        },
        {
            "sha": "058bc40dc1fe9d3f57953c592b8907f65220fe2c",
            "filename": "src/transformers/models/llama4/processing_llama4.py",
            "status": "modified",
            "additions": 31,
            "deletions": 52,
            "changes": 83,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fprocessing_llama4.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,6 +20,7 @@\n \n from ...image_processing_utils import BatchFeature\n from ...image_utils import ImageInput, make_flat_list_of_images\n+from ...utils import auto_docstring\n \n \n class Llama4ProcessorKwargs(ProcessingKwargs, total=False):\n@@ -33,40 +34,8 @@ class Llama4ProcessorKwargs(ProcessingKwargs, total=False):\n chat_template = \"{{- bos_token }}\\n{%- if custom_tools is defined %}\\n    {%- set tools = custom_tools %}\\n{%- endif %}\\n{%- if not tools_in_user_message is defined %}\\n    {%- set tools_in_user_message = true %}\\n{%- endif %}\\n{%- if not date_string is defined %}\\n    {%- if strftime_now is defined %}\\n        {%- set date_string = strftime_now(\\\"%d %b %Y\\\") %}\\n    {%- else %}\\n        {%- set date_string = \\\"26 Jul 2024\\\" %}\\n    {%- endif %}\\n{%- endif %}\\n{%- if not tools is defined %}\\n    {%- set tools = none %}\\n{%- endif %}\\n\\n{#- This block extracts the system message, so we can slot it into the right place. #}\\n{%- if messages[0]['role'] == 'system' %}    \\n    {%- if messages[0]['content'] is string %}\\n        {%- set system_message = messages[0]['content']|trim %}\\n    {%- else %}\\n        {#- FIXME: The processor requires an array, always. #}\\n        {%- set system_message = messages[0]['content'][0]['text']|trim %}\\n    {%- endif %}\\n    {%- set messages = messages[1:] %}\\n    {%- set user_supplied_system_message = true %}\\n{%- else %}\\n    {%- set system_message = \\\"\\\" %}\\n    {%- set user_supplied_system_message = false %}\\n{%- endif %}\\n\\n{#- System message if the user supplied one #}\\n{%- if user_supplied_system_message %}\\n    {{- \\\"<|header_start|>system<|header_end|>\\n\\n\\\" }}\\n    {%- if tools is not none %}\\n        {{- \\\"Environment: ipython\\n\\\" }}\\n    {%- endif %}\\n    {%- if tools is not none and not tools_in_user_message %}\\n        {{- \\\"You have access to the following functions. To call a function, please respond with JSON for a function call.\\\" }}\\n        {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n        {{- \\\"Do not use variables.\\n\\n\\\" }}\\n        {%- for t in tools %}\\n            {{- t | tojson(indent=4) }}\\n            {{- \\\"\\n\\n\\\" }}\\n        {%- endfor %}\\n    {%- endif %}\\n    {{- system_message }}\\n    {{- \\\"<|eot|>\\\" }}\\n{%- endif %}\\n\\n{#- Custom tools are passed in a user message with some extra guidance #}\\n{%- if tools_in_user_message and not tools is none %}\\n    {#- Extract the first user message so we can plug it in here #}\\n    {%- if messages | length != 0 %}\\n        {%- set first_user_message = messages[0]['content']|trim %}\\n        {%- set messages = messages[1:] %}\\n    {%- else %}\\n        {{- raise_exception(\\\"Cannot put tools in the first user message when there's no first user message!\\\") }}\\n{%- endif %}\\n    {{- '<|header_start|>user<|header_end|>\\n\\n' -}}\\n    {{- \\\"Given the following functions, please respond with a JSON for a function call \\\" }}\\n    {{- \\\"with its proper arguments that best answers the given prompt.\\n\\n\\\" }}\\n    {{- 'Respond in the format {\\\"name\\\": function name, \\\"parameters\\\": dictionary of argument name and its value}.' }}\\n    {{- \\\"Do not use variables.\\n\\n\\\" }}\\n    {%- for t in tools %}\\n        {{- t | tojson(indent=4) }}\\n        {{- \\\"\\n\\n\\\" }}\\n    {%- endfor %}\\n    {{- first_user_message + \\\"<|eot|>\\\"}}\\n{%- endif %}\\n\\n{%- for message in messages %}\\n    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}\\n    {{- '<|header_start|>' + message['role'] + '<|header_end|>\\n\\n' }}\\n        {%- if message['content'] is string %}\\n            {{- message['content'] }}\\n        {%- else %}\\n            {%- for content in message['content'] %}\\n                {%- if content['type'] == 'image' %}\\n                    {{- '<|image|>' }}\\n                {%- elif content['type'] == 'text' %}\\n                    {{- content['text'] }}\\n                {%- endif %}\\n            {%- endfor %}\\n        {%- endif %}\\n        {{- \\\"<|eot|>\\\" }}\\n    {%- elif 'tool_calls' in message and message.tool_calls|length > 0 %}\\n       {{- '<|header_start|>assistant<|header_end|>\\n\\n' -}}\\n       {{- '<|python_start|>' }}\\n        {%- if message['content'] is string %}\\n            {{- message['content'] }}\\n        {%- else %}\\n            {%- for content in message['content'] %}\\n                {%- if content['type'] == 'image' %}\\n                    {{- '<|image|>' }}\\n                {%- elif content['type'] == 'text' %}\\n                    {{- content['text'] }}\\n                {%- endif %}\\n            {%- endfor %}\\n        {%- endif %}\\n       {{- '<|python_end|>' }}\\n        {%- for tool_call in message.tool_calls %}\\n           {{- '{\\\"name\\\": \\\"' + tool_call.function.name + '\\\", ' }}\\n           {{- '\\\"parameters\\\": ' }}\\n           {{- tool_call.function.arguments | tojson }}\\n           {{- \\\"}\\\" }}\\n        {%- endfor %}\\n       {{- \\\"<|eot|>\\\" }}\\n    {%- elif message.role == \\\"tool\\\" or message.role == \\\"ipython\\\" %}\\n        {{- \\\"<|header_start|>ipython<|header_end|>\\n\\n\\\" }}\\n        {%- if message.content is mapping or message.content is iterable %}\\n            {{- message.content | tojson }}\\n        {%- else %}\\n            {{- message.content }}\\n        {%- endif %}\\n        {{- \\\"<|eot|>\\\" }}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- '<|header_start|>assistant<|header_end|>\\n\\n' }}\\n{%- endif %}\\n\"\n \n \n+@auto_docstring\n class Llama4Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Llama4 processor which wraps a [`AutoImageProcessor`] and\n-    [`PretrainedTokenizerFast`] tokenizer into a single processor that inherits both the image processor and\n-    tokenizer functionalities. See the [`~Llama4Processor.__call__`] and [`~Llama4Processor.decode`] for more information.\n-    Args:\n-        image_processor ([`AutoImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        patch_size (`int`, *optional*, defaults to 28):\n-            The size of image patches for tokenization.\n-        img_size (`int`, *optional*, defaults to 364):\n-            The size of the image to be tokenized. This should correspond to the size given to the image processor.\n-        image_token (`str`, *optional*, defaults to `\"<|image|>\"`):\n-            The token to be used to represent an image in the text.\n-        downsample_factor (`int`, *optional*, defaults to 1):\n-            The factor by which to scale the patch size.\n-        start_of_img_token (`str`, *optional*, defaults to `\"<|START_OF_IMG|>\"`):\n-            The token to be used to represent the start of an image in the text.\n-        end_of_img_token (`str`, *optional*, defaults to `\"<|END_OF_IMG|>\"`):\n-            The token to be used to represent the end of an image in the text.\n-        img_patch_token (`str`, *optional*, defaults to `\"<|IMG_PATCH|>\"`):\n-            The token to be used to represent an image patch in the text.\n-        img_line_break_token (`str`, *optional*, defaults to `\"<|IMG_LINE_BREAK|>\"`):\n-            The token to be used to represent a line break in the text.\n-        tile_token (`str`, *optional*, defaults to `\"TILE\"`):\n-            The token to be used to represent an image patch in the text.\n-        tile_global_token (`str`, *optional*, defaults to `\"TILE_GLOBAL\"`):\n-            The token to be used to represent the cover image in the text.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -83,6 +52,33 @@ def __init__(\n         chat_template=chat_template,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        patch_size (`int`, *optional*, defaults to 28):\n+            The size of image patches for tokenization.\n+        pixel_shuffle_ratio (`float`, *optional*, defaults to `0.5`):\n+            The ratio used for pixel shuffling when processing images. This controls the downsampling factor\n+            applied to image patches. The actual downsampling ratio is calculated as `1 / (pixel_shuffle_ratio^2)`.\n+        fake_image_token (`str`, *optional*, defaults to `\"<|image|>\"`):\n+            The placeholder token in the text that will be replaced with actual image tokens. This token serves\n+            as a marker indicating where images should be inserted in the text sequence.\n+        image_token (`str`, *optional*, defaults to `\"<|image|>\"`):\n+            The token to be used to represent an image in the text.\n+        start_of_image_token (`str`, *optional*, defaults to `\"<|image_start|>\"`):\n+            The special token that marks the beginning of an image sequence in the text. This token is prepended\n+            to image token sequences to delimit image boundaries.\n+        end_of_image_token (`str`, *optional*, defaults to `\"<|image_end|>\"`):\n+            The special token that marks the end of an image sequence in the text. This token is appended to\n+            image token sequences to delimit image boundaries.\n+        patch_token (`str`, *optional*, defaults to `\"<|patch|>\"`):\n+            The token used to represent individual image patches. Multiple patch tokens are used to represent\n+            the full image, with the number depending on the image size and patch configuration.\n+        tile_x_separator_token (`str`, *optional*, defaults to `\"<|tile_x_separator|>\"`):\n+            The token used to separate tiles (patches) horizontally within an image. This token is inserted\n+            between patches in the same row when images are split into multiple tiles.\n+        tile_y_separator_token (`str`, *optional*, defaults to `\"<|tile_y_separator|>\"`):\n+            The token used to separate tiles (patches) vertically within an image. This token is inserted\n+            between rows of patches when images are split into multiple tiles.\n+        \"\"\"\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n         self.downsample_ratio = int(round(1.0 / (pixel_shuffle_ratio**2)))\n@@ -124,31 +120,14 @@ def _prompt_split_image(self, aspect_ratio, num_patches_per_chunk):\n \n         return img_string\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         **kwargs: Unpack[Llama4ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] to encode the text.\n-        To prepare the vision inputs, this method forwards the `images` and `kwargs` arguments to\n-        Llama4ImageProcessor's [`~Llama4ImageProcessor.__call__`] if `images` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "16ac0807c45f0bde686688d4c5a0fe419bb85203",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 16,
            "deletions": 46,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -28,7 +28,7 @@\n     Unpack,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -40,32 +40,8 @@ class LlavaProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class LlavaProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a LLaVa processor which wraps a LLaVa image processor and a LLaMa tokenizer into a single processor.\n-\n-    [`LlavaProcessor`] offers all the functionalities of [`LlavaImageProcessor`] and [`LlamaTokenizerFast`]. See the\n-    [`~LlavaProcessor.__call__`] and [`~LlavaProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`LlavaImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        patch_size (`int`, *optional*):\n-            Patch size from the vision tower.\n-        vision_feature_select_strategy (`str`, *optional*):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Should be same as in model's config\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n-            Special token used to denote image location.\n-        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n-            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n-            extra tokens appended, no need to set this arg.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -77,39 +53,33 @@ def __init__(\n         num_additional_image_tokens=0,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        patch_size (`int`, *optional*):\n+            Patch size from the vision tower.\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Should be same as in model's config\n+        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n+            Special token used to denote image location.\n+        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n+        \"\"\"\n         self.patch_size = patch_size\n         self.num_additional_image_tokens = num_additional_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n         self.image_token_id = tokenizer.encode(self.image_token, add_special_tokens=False)[0]\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[LlavaProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "992649ba1ce1ecaf561478cef09bfecabf0c1fd5",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 16,
            "deletions": 42,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -29,7 +29,7 @@\n     Unpack,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -47,32 +47,8 @@ class LlavaNextProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class LlavaNextProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a LLaVa-NeXT processor which wraps a LLaVa-NeXT image processor and a LLaMa tokenizer into a single processor.\n-\n-    [`LlavaNextProcessor`] offers all the functionalities of [`LlavaNextImageProcessor`] and [`LlamaTokenizerFast`]. See the\n-    [`~LlavaNextProcessor.__call__`] and [`~LlavaNextProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`LlavaNextImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        patch_size (`int`, *optional*):\n-            Patch size from the vision tower.\n-        vision_feature_select_strategy (`str`, *optional*):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Should be same as in model's config\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n-            Special token used to denote image location.\n-        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n-            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n-            extra tokens appended, no need to set this arg.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -84,6 +60,18 @@ def __init__(\n         num_additional_image_tokens=0,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        patch_size (`int`, *optional*):\n+            Patch size from the vision tower.\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Should be same as in model's config\n+        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n+            Special token used to denote image location.\n+        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n+        \"\"\"\n         self.patch_size = patch_size\n         self.num_additional_image_tokens = num_additional_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n@@ -95,28 +83,14 @@ def __init__(\n         )\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[LlavaNextProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "97f86a49162b05a4f94993efd44a4a895e070954",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 18,
            "deletions": 57,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -24,7 +24,7 @@\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n from ...video_utils import VideoInput\n \n \n@@ -43,37 +43,8 @@ class LlavaNextVideoProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class LlavaNextVideoProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a LLaVa-NeXT-Video processor which wraps a LLaVa-NeXT image processor, LLaVa-NeXT-Video video processor and\n-    a LLaMa tokenizer into a single processor.\n-\n-    [`LlavaNextVideoProcessor`] offers all the functionalities of [`LlavaNextImageProcessor`], [`LlavaNextVideoVideoProcessor`] and\n-    [`LlamaTokenizerFast`]. See the [`~LlavaNextVideoProcessor.__call__`] and [`~LlavaNextVideoProcessor.decode`] for more information.\n-\n-    Args:\n-        video_processor ([`LlavaNextVideoVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        image_processor ([`LlavaNextImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*):\n-            Jinja chat template that will be used in tokenizer's `apply_chat_template`\n-        patch_size (`int`, *optional*):\n-            Patch size from the vision tower.\n-        vision_feature_select_strategy (`str`, *optional*):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Should be same as in model's config\n-        video_token (`str`, *optional*, defaults to `\"<video>\"`):\n-            Special token used to denote video location.\n-        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n-            Special token used to denote image location.\n-        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n-            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n-            extra tokens appended, no need to set this arg.\n-    \"\"\"\n-\n     # video and image processor share same args, but have different processing logic\n     # only image processor config is saved in the hub\n     def __init__(\n@@ -89,6 +60,20 @@ def __init__(\n         num_additional_image_tokens=0,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        patch_size (`int`, *optional*):\n+            Patch size from the vision tower.\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Should be same as in model's config\n+        video_token (`str`, *optional*, defaults to `\"<video>\"`):\n+            Special token used to denote video location.\n+        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n+            Special token used to denote image location.\n+        num_additional_image_tokens (`int`, *optional*, defaults to 0):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n+        \"\"\"\n         self.patch_size = patch_size\n         self.num_additional_image_tokens = num_additional_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n@@ -106,39 +91,15 @@ def __init__(\n         )\n         super().__init__(video_processor, image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[LlavaNextVideoProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. To prepare the video(s),\n-        this method forwards the `videos` and `kwargs` arguments to LlavaNextVideoVideoProcessor's\n-        [`~LlavaNextVideoVideoProcessor.__call__`] if `videos` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "94edd0acef92eea46e07dc53217f7bba0099afc6",
            "filename": "src/transformers/models/llava_onevision/processing_llava_onevision.py",
            "status": "modified",
            "additions": 17,
            "deletions": 47,
            "changes": 64,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fprocessing_llava_onevision.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -26,7 +26,7 @@\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n from ...video_utils import VideoInput\n \n \n@@ -44,35 +44,8 @@ class LlavaOnevisionProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class LlavaOnevisionProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a LLaVa-Onevision processor which wraps a LLaVa-Onevision video processor, LLaVa-NeXT image processor and a LLaMa tokenizer into a single processor.\n-\n-    [`LlavaNextProcessor`] offers all the functionalities of [`LlavaOnevisionVideoProcessor`], [`LlavaOnevisionImageProcessor`] and [`LlamaTokenizerFast`]. See the\n-    [`~LlavaOnevisionVideoProcessor.__call__`], [`~LlavaNextProcessor.__call__`] and [`~LlavaNextProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`LlavaOnevisionImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`LlavaOnevisionVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        num_image_tokens (`int`, *optional*):\n-            Number of image tokens for one imagethat will be returned by vision tower.\n-        vision_feature_select_strategy (`str`, *optional*):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Should be same as in model's config\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n-            Special token used to denote image location.\n-        video_token (`str`, *optional*, defaults to `\"<video>\"`):\n-            Special token used to denote video location.\n-        vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n-            Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -86,6 +59,19 @@ def __init__(\n         vision_aspect_ratio=\"anyres_max_9\",\n         **kwargs,\n     ):\n+        r\"\"\"\n+        num_image_tokens (`int`, *optional*):\n+            Number of image tokens for one imagethat will be returned by vision tower.\n+        vision_feature_select_strategy (`str`, *optional*):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Should be same as in model's config\n+        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n+            Special token used to denote image location.\n+        video_token (`str`, *optional*, defaults to `\"<video>\"`):\n+            Special token used to denote video location.\n+        vision_aspect_ratio (`str`, *optional*, defaults to `\"anyres_max_9\"`):\n+            Aspect ratio used when processong image features. The default value is \"anyres_max_9\".\n+        \"\"\"\n         self.num_image_tokens = num_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n@@ -103,31 +89,15 @@ def __init__(\n         self.vision_aspect_ratio = vision_aspect_ratio\n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[LlavaOnevisionProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        LlavaNextImageProcessor's [`~LlavaNextImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "7e0e2e9f230ea85dabcaaec279108559e1725cd5",
            "filename": "src/transformers/models/markuplm/processing_markuplm.py",
            "status": "modified",
            "additions": 24,
            "deletions": 28,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarkuplm%2Fprocessing_markuplm.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,33 +20,17 @@\n from ...file_utils import TensorType\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding, PaddingStrategy, TruncationStrategy\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class MarkupLMProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a MarkupLM processor which combines a MarkupLM feature extractor and a MarkupLM tokenizer into a single\n-    processor.\n-\n-    [`MarkupLMProcessor`] offers all the functionalities you need to prepare data for the model.\n-\n-    It first uses [`MarkupLMFeatureExtractor`] to extract nodes and corresponding xpaths from one or more HTML strings.\n-    Next, these are provided to [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`], which turns them into token-level\n-    `input_ids`, `attention_mask`, `token_type_ids`, `xpath_tags_seq` and `xpath_subs_seq`.\n-\n-    Args:\n-        feature_extractor (`MarkupLMFeatureExtractor`):\n-            An instance of [`MarkupLMFeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer (`MarkupLMTokenizer` or `MarkupLMTokenizerFast`):\n-            An instance of [`MarkupLMTokenizer`] or [`MarkupLMTokenizerFast`]. The tokenizer is a required input.\n-        parse_html (`bool`, *optional*, defaults to `True`):\n-            Whether or not to use `MarkupLMFeatureExtractor` to parse HTML strings into nodes and corresponding xpaths.\n-    \"\"\"\n-\n     parse_html = True\n \n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         html_strings=None,\n@@ -70,16 +54,28 @@ def __call__(\n         return_tensors: Optional[Union[str, TensorType]] = None,\n         **kwargs,\n     ) -> BatchEncoding:\n-        \"\"\"\n-        This method first forwards the `html_strings` argument to [`~MarkupLMFeatureExtractor.__call__`]. Next, it\n-        passes the `nodes` and `xpaths` along with the additional arguments to [`~MarkupLMTokenizer.__call__`] and\n-        returns the output.\n-\n-        Optionally, one can also provide a `text` argument which is passed along as first sequence.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n         # first, create nodes and xpaths\n+        r\"\"\"\n+        html_strings (`str` or `list[str]`, *optional*):\n+            Raw HTML strings to parse and process. When `parse_html=True` (default), these strings are parsed\n+            to extract nodes and xpaths automatically. If provided, `nodes`, `xpaths`, and `node_labels` should\n+            not be provided. Required when `parse_html=True`.\n+        nodes (`list[list[str]]`, *optional*):\n+            Pre-extracted HTML nodes as a list of lists, where each inner list contains the text content of nodes\n+            for a single document. Required when `parse_html=False`. Should not be provided when `parse_html=True`.\n+        xpaths (`list[list[str]]`, *optional*):\n+            Pre-extracted XPath expressions corresponding to the nodes. Should be a list of lists with the same\n+            structure as `nodes`, where each XPath identifies the location of the corresponding node in the HTML\n+            tree. Required when `parse_html=False`. Should not be provided when `parse_html=True`.\n+        node_labels (`list[list[int]]`, *optional*):\n+            Labels for the nodes, typically used for training or fine-tuning tasks. Should be a list of lists\n+            with the same structure as `nodes`, where each label corresponds to a node. Optional and only used\n+            when `parse_html=False`.\n+        questions (`str` or `list[str]`, *optional*):\n+            Question strings for question-answering tasks. When provided, the tokenizer processes questions\n+            as the first sequence and nodes as the second sequence (text_pair). If a single string is provided,\n+            it is converted to a list to match the batch dimension of the parsed HTML.\n+        \"\"\"\n         if self.parse_html:\n             if html_strings is None:\n                 raise ValueError(\"Make sure to pass HTML strings in case `parse_html` is set to `True`\")"
        },
        {
            "sha": "382acde58f2e486664d521a34bb53f0c91c516b4",
            "filename": "src/transformers/models/mgp_str/processing_mgp_str.py",
            "status": "modified",
            "additions": 3,
            "deletions": 19,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmgp_str%2Fprocessing_mgp_str.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -18,6 +18,7 @@\n from transformers.utils.generic import ExplicitEnum\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import auto_docstring\n from ...utils.import_utils import requires\n \n \n@@ -35,34 +36,17 @@ class DecodeType(ExplicitEnum):\n \n \n @requires(backends=(\"sentencepiece\",))\n+@auto_docstring\n class MgpstrProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a MGP-STR processor which wraps an image processor and MGP-STR tokenizers into a single\n-\n-    [`MgpstrProcessor`] offers all the functionalities of `ViTImageProcessor`] and [`MgpstrTokenizer`]. See the\n-    [`~MgpstrProcessor.__call__`] and [`~MgpstrProcessor.batch_decode`] for more information.\n-\n-    Args:\n-        image_processor (`ViTImageProcessor`, *optional*):\n-            An instance of `ViTImageProcessor`. The image processor is a required input.\n-        tokenizer ([`MgpstrTokenizer`], *optional*):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         self.char_tokenizer = tokenizer\n         self.bpe_tokenizer = AutoTokenizer.from_pretrained(\"openai-community/gpt2\")\n         self.wp_tokenizer = AutoTokenizer.from_pretrained(\"google-bert/bert-base-uncased\")\n \n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(self, text=None, images=None, return_tensors=None, **kwargs):\n-        \"\"\"\n-        When used in normal mode, this method forwards all its arguments to ViTImageProcessor's\n-        [`~ViTImageProcessor.__call__`] and returns its output. This method also forwards the `text` and `kwargs`\n-        arguments to MgpstrTokenizer's [`~MgpstrTokenizer.__call__`] if `text` is not `None` to encode the text. Please\n-        refer to the docstring of the above methods for more information.\n-        \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You need to specify either an `images` or `text` input to process.\")\n "
        },
        {
            "sha": "f855674b4b47bf63242017d457e63e098d42dba9",
            "filename": "src/transformers/models/mllama/processing_mllama.py",
            "status": "modified",
            "additions": 4,
            "deletions": 50,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fprocessing_mllama.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -22,6 +22,7 @@\n from ...image_utils import ImageInput, make_nested_list_of_images\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class MllamaProcessorKwargs(ProcessingKwargs, total=False):\n@@ -165,38 +166,8 @@ def build_string_from_input(prompt: str, bos_token: str, image_token: str) -> st\n     return f\"{image_token * num_image_tokens_on_start}{bos_token}{prompt}\"\n \n \n+@auto_docstring\n class MllamaProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Mllama processor which wraps [`MllamaImageProcessor`] and\n-    [`PretrainedTokenizerFast`] into a single processor that inherits both the image processor and\n-    tokenizer functionalities. See the [`~MllamaProcessor.__call__`] and [`~OwlViTProcessor.decode`] for more\n-    information.\n-    The preferred way of passing kwargs is as a dictionary per modality, see usage example below.\n-        ```python\n-        from transformers import MllamaProcessor\n-        from PIL import Image\n-\n-        processor = MllamaProcessor.from_pretrained(\"meta-llama/Llama-3.2-11B-Vision\")\n-\n-        processor(\n-            images=your_pil_image,\n-            text=[\"<|image|>If I had to write a haiku for this one\"],\n-            images_kwargs = {\"size\": {\"height\": 448, \"width\": 448}},\n-            text_kwargs = {\"padding\": \"right\"},\n-            common_kwargs = {\"return_tensors\": \"pt\"},\n-        )\n-        ```\n-\n-    Args:\n-        image_processor ([`MllamaImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-\n-    \"\"\"\n-\n     def __init__(self, image_processor, tokenizer, chat_template=None):\n         if not hasattr(tokenizer, \"image_token\"):\n             self.image_token = \"<|image|>\"\n@@ -210,31 +181,14 @@ def __init__(self, image_processor, tokenizer, chat_template=None):\n         self.bos_token = tokenizer.bos_token\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         **kwargs: Unpack[MllamaProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare text(s) and image(s) to be fed as input to the model. This method forwards the `text`\n-        arguments to PreTrainedTokenizerFast's [`~PreTrainedTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` arguments to\n-        MllamaImageProcessor's [`~MllamaImageProcessor.__call__`] if `images` is not `None`. Please refer\n-        to the docstring of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                    - `'np'`: Return NumPy `np.ndarray` objects.\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "180f0a2a95eecc662f71a5b53d553b832fd8ad88",
            "filename": "src/transformers/models/musicgen/processing_musicgen.py",
            "status": "modified",
            "additions": 3,
            "deletions": 20,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fprocessing_musicgen.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,36 +20,19 @@\n import numpy as np\n \n from ...processing_utils import ProcessorMixin\n-from ...utils import to_numpy\n+from ...utils import auto_docstring, to_numpy\n \n \n+@auto_docstring\n class MusicgenProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a MusicGen processor which wraps an EnCodec feature extractor and a T5 tokenizer into a single processor\n-    class.\n-\n-    [`MusicgenProcessor`] offers all the functionalities of [`EncodecFeatureExtractor`] and [`TTokenizer`]. See\n-    [`~MusicgenProcessor.__call__`] and [`~MusicgenProcessor.decode`] for more information.\n-\n-    Args:\n-        feature_extractor (`EncodecFeatureExtractor`):\n-            An instance of [`EncodecFeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer (`T5Tokenizer`):\n-            An instance of [`T5Tokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n     def get_decoder_prompt_ids(self, task=None, language=None, no_timestamps=True):\n         return self.tokenizer.get_decoder_prompt_ids(task=task, language=language, no_timestamps=no_timestamps)\n \n+    @auto_docstring\n     def __call__(self, *args, **kwargs):\n-        \"\"\"\n-        Forwards the `audio` argument to EncodecFeatureExtractor's [`~EncodecFeatureExtractor.__call__`] and the `text`\n-        argument to [`~T5Tokenizer.__call__`]. Please refer to the docstring of the above two methods for more\n-        information.\n-        \"\"\"\n         if len(args) > 0:\n             kwargs[\"audio\"] = args[0]\n         return super().__call__(*args, **kwargs)"
        },
        {
            "sha": "cf5cc6088b8ea725a42127ddcca3b5abc7ebe278",
            "filename": "src/transformers/models/musicgen_melody/processing_musicgen_melody.py",
            "status": "modified",
            "additions": 3,
            "deletions": 21,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fprocessing_musicgen_melody.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,40 +20,22 @@\n import numpy as np\n \n from ...processing_utils import ProcessorMixin\n-from ...utils import to_numpy\n+from ...utils import auto_docstring, to_numpy\n from ...utils.import_utils import requires\n \n \n @requires(backends=(\"torchaudio\",))\n+@auto_docstring\n class MusicgenMelodyProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a MusicGen Melody processor which wraps a Wav2Vec2 feature extractor - for raw audio waveform processing - and a T5 tokenizer into a single processor\n-    class.\n-\n-    [`MusicgenProcessor`] offers all the functionalities of [`MusicgenMelodyFeatureExtractor`] and [`T5Tokenizer`]. See\n-    [`~MusicgenProcessor.__call__`] and [`~MusicgenProcessor.decode`] for more information.\n-\n-    Args:\n-        feature_extractor (`MusicgenMelodyFeatureExtractor`):\n-            An instance of [`MusicgenMelodyFeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer (`T5Tokenizer`):\n-            An instance of [`T5Tokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n     # Copied from transformers.models.musicgen.processing_musicgen.MusicgenProcessor.get_decoder_prompt_ids\n     def get_decoder_prompt_ids(self, task=None, language=None, no_timestamps=True):\n         return self.tokenizer.get_decoder_prompt_ids(task=task, language=language, no_timestamps=no_timestamps)\n \n+    @auto_docstring\n     def __call__(self, *args, **kwargs):\n-        \"\"\"\n-        Forwards the `audio` argument to EncodecFeatureExtractor's [`~EncodecFeatureExtractor.__call__`] and the `text`\n-        argument to [`~T5Tokenizer.__call__`]. Please refer to the docstring of the above two methods for more\n-        information.\n-        \"\"\"\n-\n         if len(args) > 0:\n             kwargs[\"audio\"] = args[0]\n         return super().__call__(*args, **kwargs)"
        },
        {
            "sha": "a25b817b3afc955fdb246acdd83d115f41e2c1d3",
            "filename": "src/transformers/models/nougat/processing_nougat.py",
            "status": "modified",
            "additions": 15,
            "deletions": 14,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnougat%2Fprocessing_nougat.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,26 +20,15 @@\n from transformers.tokenization_utils_base import PreTokenizedInput, TextInput, TruncationStrategy\n \n from ...processing_utils import ProcessorMixin\n-from ...utils import PaddingStrategy, TensorType\n+from ...utils import PaddingStrategy, TensorType, auto_docstring\n \n \n+@auto_docstring\n class NougatProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Nougat processor which wraps a Nougat image processor and a Nougat tokenizer into a single processor.\n-\n-    [`NougatProcessor`] offers all the functionalities of [`NougatImageProcessor`] and [`NougatTokenizerFast`]. See the\n-    [`~NougatProcessor.__call__`] and [`~NougatProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`NougatImageProcessor`]):\n-            An instance of [`NougatImageProcessor`]. The image processor is a required input.\n-        tokenizer ([`NougatTokenizerFast`]):\n-            An instance of [`NougatTokenizerFast`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images=None,\n@@ -79,6 +68,18 @@ def __call__(\n         return_length: bool = False,\n         verbose: bool = True,\n     ):\n+        r\"\"\"\n+        do_crop_margin (`bool`, *optional*):\n+            Whether to automatically crop white margins from document images. When enabled, the processor detects\n+            and removes white space around the edges of document pages, which is useful for processing scanned\n+            documents or PDFs with large margins.\n+        do_thumbnail (`bool`, *optional*):\n+            Whether to create a thumbnail version of the image. When enabled, a smaller version of the image is\n+            generated alongside the main processed image, which can be useful for preview or faster processing.\n+        do_align_long_axis (`bool`, *optional*):\n+            Whether to automatically align images so that the longer axis is horizontal. When enabled, portrait\n+            images are rotated to landscape orientation, which is typically better for document processing tasks.\n+        \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You need to specify either an `images` or `text` input to process.\")\n "
        },
        {
            "sha": "9904ede2b4ee134bf7b8f86f333c94717798b3df",
            "filename": "src/transformers/models/omdet_turbo/processing_omdet_turbo.py",
            "status": "modified",
            "additions": 10,
            "deletions": 35,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fprocessing_omdet_turbo.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -25,6 +25,7 @@\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import (\n     TensorType,\n+    auto_docstring,\n     is_torch_available,\n     is_torchvision_available,\n )\n@@ -36,6 +37,13 @@\n \n \n class OmDetTurboTextKwargs(TextKwargs, total=False):\n+    \"\"\"\n+    task (`str`, `list[str]`, `TextInput`, or `PreTokenizedInput`, *optional*):\n+        The detection task description(s) to encode. If not provided, a default task description is generated\n+        from the `text` input (e.g., \"Detect {text}.\"). Can be a single string, a list of strings (one per image),\n+        or pre-tokenized input. The task description guides the model on what objects to detect in the images.\n+    \"\"\"\n+\n     task: Optional[Union[str, list[str], TextInput, PreTokenizedInput]]\n \n \n@@ -198,51 +206,18 @@ def _post_process_boxes_for_image(\n \n \n @requires(backends=(\"vision\", \"torchvision\"))\n+@auto_docstring\n class OmDetTurboProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a OmDet-Turbo processor which wraps a Deformable DETR image processor and an AutoTokenizer into a\n-    single processor.\n-\n-    [`OmDetTurboProcessor`] offers all the functionalities of [`DetrImageProcessor`] and\n-    [`AutoTokenizer`]. See the docstring of [`~OmDetTurboProcessor.__call__`] and [`~OmDetTurboProcessor.decode`]\n-    for more information.\n-\n-    Args:\n-        image_processor (`DetrImageProcessor`):\n-            An instance of [`DetrImageProcessor`]. The image processor is a required input.\n-        tokenizer (`AutoTokenizer`):\n-            An instance of ['PreTrainedTokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[list[str], list[list[str]]]] = None,\n         **kwargs: Unpack[OmDetTurboProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        This method uses [*DetrImageProcessor.__call__] method to prepare image(s) for the model, and\n-        [CLIPTokenizerFast.__call__] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-\n-        Args:\n-            images (`ImageInput`):\n-               Image to preprocess. Expects a single or batch of images with pixel values ranging from 0 to 255.\n-            text (`Union[str, list[str], list[list[str]]]`):\n-                The classes used to limit the scope of the open vocabulary detection. Expects a list of strings or a list\n-                of list of strings. Batched classes can be of different lengths.\n-                Examples: [\"cat\", \"dog\", \"bird\"], [[\"cat\", \"dog\", \"bird\"], [\"hat\", \"person\"], [\"car\"]]\n-        Kwargs:\n-            task (`Union[str, list[str], TextInput, PreTokenizedInput]`):\n-                The grounded text used to guide open vocabulary detection. Expects a single string or a list of strings.\n-                Examples: \"Detect a cat, a dog, and a bird.\",[ \"Detect everything.\", \"Detect trees and flowers.\"]\n-                When not provided, the default task is \"Detect [class1], [class2], [class3]\" etc.\n-            ...\n-        \"\"\"\n         if images is None or text is None:\n             raise ValueError(\"You have to specify both `images` and `text`\")\n "
        },
        {
            "sha": "fef0b2c3b6803f728b9a952713afd96e0591dbef",
            "filename": "src/transformers/models/oneformer/processing_oneformer.py",
            "status": "modified",
            "additions": 28,
            "deletions": 42,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fprocessing_oneformer.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,33 +16,27 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n-from ...utils import is_torch_available\n+from ...utils import auto_docstring, is_torch_available\n \n \n if is_torch_available():\n     import torch\n \n \n+@auto_docstring\n class OneFormerProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs an OneFormer processor which wraps [`OneFormerImageProcessor`] and\n-    [`CLIPTokenizer`]/[`CLIPTokenizerFast`] into a single processor that inherits both the image processor and\n-    tokenizer functionalities.\n-\n-    Args:\n-        image_processor ([`OneFormerImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`CLIPTokenizer`, `CLIPTokenizerFast`]):\n-            The tokenizer is a required input.\n-        max_seq_len (`int`, *optional*, defaults to 77)):\n-            Sequence length for input text list.\n-        task_seq_len (`int`, *optional*, defaults to 77):\n-            Sequence length for input task token.\n-    \"\"\"\n-\n     def __init__(\n         self, image_processor=None, tokenizer=None, max_seq_length: int = 77, task_seq_length: int = 77, **kwargs\n     ):\n+        r\"\"\"\n+        max_seq_length (`int`, *optional*, defaults to `77`):\n+            Maximum sequence length for encoding class names and text inputs. This parameter controls the\n+            maximum number of tokens used when tokenizing class names and other text inputs for the model.\n+        task_seq_length (`int`, *optional*, defaults to `77`):\n+            Maximum sequence length specifically for encoding task descriptions. Task descriptions (e.g.,\n+            \"the task is semantic\") are tokenized with this length limit, which may differ from the general\n+            text sequence length.\n+        \"\"\"\n         self.max_seq_length = max_seq_length\n         self.task_seq_length = task_seq_length\n \n@@ -64,32 +58,24 @@ def _preprocess_text(self, text_list=None, max_length=77):\n         token_inputs = torch.cat(token_inputs, dim=0)\n         return token_inputs\n \n+    @auto_docstring\n     def __call__(self, images=None, task_inputs=None, segmentation_maps=None, **kwargs):\n-        \"\"\"\n-        Main method to prepare for the model one or several task input(s) and image(s). This method forwards the\n-        `task_inputs` and `kwargs` arguments to CLIPTokenizer's [`~CLIPTokenizer.__call__`] if `task_inputs` is not\n-        `None` to encode. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        OneFormerImageProcessor's [`~OneFormerImageProcessor.__call__`] if `images` is not `None`. Please refer to the\n-        docstring of the above two methods for more information.\n-\n-        Args:\n-            task_inputs (`str`, `list[str]`):\n-                The sequence or batch of task_inputs sequences to be encoded. Each sequence can be a string or a list\n-                of strings of the template \"the task is {task}\".\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`,\n-            `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            segmentation_maps (`ImageInput`, *optional*):\n-                The corresponding semantic segmentation maps with the pixel-wise annotations.\n-\n-             (`bool`, *optional*, defaults to `True`):\n-                Whether or not to pad images up to the largest image in a batch and create a pixel mask.\n-\n-                If left to the default, will return a pixel mask that is:\n-\n-                - 1 for pixels that are real (i.e. **not masked**),\n-                - 0 for pixels that are padding (i.e. **masked**).\n+        r\"\"\"\n+        task_inputs (`str` or `list[str]`, *required*):\n+            The task type(s) for segmentation. Must be one or more of `\"semantic\"`, `\"instance\"`, or `\"panoptic\"`.\n+            Can be a single string for a single image, or a list of strings (one per image) for batch processing.\n+            The task type determines which type of segmentation the model will perform on the input images.\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The corresponding semantic segmentation maps with the pixel-wise annotations.\n+\n+            (`bool`, *optional*, defaults to `True`):\n+            Whether or not to pad images up to the largest image in a batch and create a pixel mask.\n+\n+            If left to the default, will return a pixel mask that is:\n+\n+            - 1 for pixels that are real (i.e. **not masked**),\n+            - 0 for pixels that are padding (i.e. **masked**).\n+\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n             - **task_inputs** -- List of token ids to be fed to a model. Returned when `text` is not `None`."
        },
        {
            "sha": "ba7bb1c4fdf2a1382d1e00c83e3475d0d18d67cd",
            "filename": "src/transformers/models/ovis2/processing_ovis2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 36,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fovis2%2Fprocessing_ovis2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -18,7 +18,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -33,26 +33,8 @@ class Ovis2ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Ovis2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Ovis2 processor which wraps Ovis2 image processor and a Qwen2 tokenizer into a single processor.\n-\n-    [`Ovis2Processor`] offers all the functionalities of [`Ovis2VideoProcessor`], [`Ovis2ImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n-    [`~Ovis2Processor.__call__`] and [`~Ovis2Processor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`Ovis2ImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n-            Special token used to denote image location.\n-        image_seq_length (`int`, *optional*, defaults to 256):\n-            The number of image tokens to be used for each image in the input.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -62,6 +44,12 @@ def __init__(\n         image_seq_length=256,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n+            Special token used to denote image location.\n+        image_seq_length (`int`, *optional*, defaults to 256):\n+            The number of image tokens to be used for each image in the input.\n+        \"\"\"\n         self.image_seq_length = image_seq_length\n         self.image_token = tokenizer.image_token if hasattr(tokenizer, \"image_token\") else image_token\n         self.image_token_id = (\n@@ -71,28 +59,14 @@ def __init__(\n         )\n         super().__init__(image_processor, tokenizer, chat_template=chat_template, **kwargs)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[Ovis2ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        Ovis2ImageProcessor's [`~Ovis2ImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "ceacd593071c23291460aee965b73dfba399ace4",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 41,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -28,14 +28,21 @@\n     Unpack,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import TensorType, is_torch_available\n+from ...utils import TensorType, auto_docstring, is_torch_available\n \n \n if TYPE_CHECKING:\n     from .modeling_owlv2 import Owlv2ImageGuidedObjectDetectionOutput, Owlv2ObjectDetectionOutput\n \n \n class Owlv2ImagesKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    query_images (`ImageInput`, *optional*):\n+        Query images to use for image-guided object detection. When provided, these images serve as visual queries\n+        to find similar objects in the main `images`. The query images override any text prompts, and the model\n+        performs image-to-image matching instead of text-to-image matching.\n+    \"\"\"\n+\n     query_images: Optional[ImageInput]\n \n \n@@ -51,60 +58,26 @@ class Owlv2ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Owlv2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs an Owlv2 processor which wraps [`Owlv2ImageProcessor`]/[`Owlv2ImageProcessorFast`] and [`CLIPTokenizer`]/[`CLIPTokenizerFast`] into\n-    a single processor that inherits both the image processor and tokenizer functionalities. See the\n-    [`~OwlViTProcessor.__call__`] and [`~OwlViTProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`Owlv2ImageProcessor`, `Owlv2ImageProcessorFast`]):\n-            The image processor is a required input.\n-        tokenizer ([`CLIPTokenizer`, `CLIPTokenizerFast`]):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor, tokenizer, **kwargs):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     # Copied from transformers.models.owlvit.processing_owlvit.OwlViTProcessor.__call__ with OwlViT->Owlv2\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[Owlv2ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several text(s) and image(s). This method forwards the `text` and\n-        `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode:\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`,\n-            `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            query_images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The query image to be prepared, one query image is expected per target image to be queried. Each image\n-                can be a PIL image, NumPy array or PyTorch tensor. In case of a NumPy array/PyTorch tensor, each image\n-                should be of shape (C, H, W), where C is a number of channels, H and W are image height and width.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n             - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n             - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n+                `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+                `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n             - **query_pixel_values** -- Pixel values of the query images to be fed to a model. Returned when `query_images` is not `None`.\n         \"\"\"\n@@ -145,7 +118,6 @@ def __call__(\n             if return_tensors == \"np\":\n                 input_ids = np.concatenate([encoding[\"input_ids\"] for encoding in encodings], axis=0)\n                 attention_mask = np.concatenate([encoding[\"attention_mask\"] for encoding in encodings], axis=0)\n-\n             elif return_tensors == \"pt\" and is_torch_available():\n                 import torch\n "
        },
        {
            "sha": "06a64a0676f94a98236145d9080cf591742cd00c",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 13,
            "deletions": 40,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -28,14 +28,21 @@\n     Unpack,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import TensorType, is_torch_available\n+from ...utils import TensorType, auto_docstring, is_torch_available\n \n \n if TYPE_CHECKING:\n     from .modeling_owlvit import OwlViTImageGuidedObjectDetectionOutput, OwlViTObjectDetectionOutput\n \n \n class OwlViTImagesKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    query_images (`ImageInput`, *optional*):\n+        Query images to use for image-guided object detection. When provided, these images serve as visual queries\n+        to find similar objects in the main `images`. The query images override any text prompts, and the model\n+        performs image-to-image matching instead of text-to-image matching.\n+    \"\"\"\n+\n     query_images: Optional[ImageInput]\n \n \n@@ -51,59 +58,25 @@ class OwlViTProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class OwlViTProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs an OWL-ViT processor which wraps [`OwlViTImageProcessor`] and [`CLIPTokenizer`]/[`CLIPTokenizerFast`]\n-    into a single processor that inherits both the image processor and tokenizer functionalities. See the\n-    [`~OwlViTProcessor.__call__`] and [`~OwlViTProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`OwlViTImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`CLIPTokenizer`, `CLIPTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[OwlViTProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several text(s) and image(s). This method forwards the `text` and\n-        `kwargs` arguments to CLIPTokenizerFast's [`~CLIPTokenizerFast.__call__`] if `text` is not `None` to encode:\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`,\n-            `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            query_images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The query image to be prepared, one query image is expected per target image to be queried. Each image\n-                can be a PIL image, NumPy array or PyTorch tensor. In case of a NumPy array/PyTorch tensor, each image\n-                should be of shape (C, H, W), where C is a number of channels, H and W are image height and width.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n             - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n             - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n+                `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n+                `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n             - **query_pixel_values** -- Pixel values of the query images to be fed to a model. Returned when `query_images` is not `None`.\n         \"\"\""
        },
        {
            "sha": "48603fa64ccefe3ca8d01e6949f9a8f0b617eac6",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 10,
            "deletions": 60,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -29,7 +29,7 @@\n     Unpack,\n )\n from ...tokenization_utils_base import AddedToken, PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -39,6 +39,12 @@\n \n \n class PaliGemmaTextKwargs(TextKwargs):\n+    \"\"\"\n+    suffix (`str`, `list[str]`, `list[list[str]]`):\n+        The suffixes or batch of suffixes to be encoded. Only necessary for finetuning. See https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md\n+        for more information. If your prompt is \"<image> What is on the image\", the suffix corresponds to the expected prediction \"a cow sitting on a bench\".\n+    \"\"\"\n+\n     suffix: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]]\n \n \n@@ -91,22 +97,8 @@ def build_string_from_input(prompt, bos_token, image_seq_len, image_token, num_i\n     return f\"{image_token * image_seq_len * num_images}{bos_token}{prompt}\\n\"\n \n \n+@auto_docstring\n class PaliGemmaProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a PaliGemma processor which wraps a PaliGemma image processor and a PaliGemma tokenizer into a single processor.\n-\n-    [`PaliGemmaProcessor`] offers all the functionalities of [`SiglipImageProcessor`] and [`GemmaTokenizerFast`]. See the\n-    [`~PaliGemmaProcessor.__call__`] and [`~PaliGemmaProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`SiglipImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`GemmaTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -135,56 +127,14 @@ def __init__(\n \n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[PaliGemmaProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to GemmaTokenizerFast's [`~GemmaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        SiglipImageProcessor's [`~SiglipImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        The usage for PaliGemma fine-tuning preparation is slightly different than usual. suffix passed are suffixes to\n-        the prompt in `text`, and will be placed after the prompt. This is because attention is handled differently for\n-        the prefix and the suffix. For instance,\n-        ```python\n-        image = PIL_cow_image\n-        prompt = \"answer en Where is the cow standing?\"\n-        suffix = \"on the beach\"\n-        inputs = processor(text=prompt, images=image, suffix=suffix)\n-        ```\n-        Here `inputs` will contain the `input_ids` and `token_type_ids` that follow\n-        ```python\n-        inputs[\"input_ids\"][:, 256:]\n-        # tensor([[     2,   6006,    603,    573,  13910,   9980, 235336,    108,    477,   573,   8318]])\n-        inputs[\"token_type_ids\"][:, 256:]\n-        tensor([[0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1]])\n-        ```\n-        Meaning the last three tokens are of \"label\" (\"suffix\") type while the other ones are of \"prefix\" type.\n-\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n-                number of channels, H and W are image height and width.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-            suffix (`str`, `list[str]`, `list[list[str]]`):\n-                The suffixes or batch of suffixes to be encoded. Only necessary for finetuning. See https://github.com/google-research/big_vision/blob/main/big_vision/configs/proj/paligemma/README.md\n-                for more information. If your prompt is \"<image> What is on the image\", the suffix corresponds to the expected prediction \"a cow sitting on a bench\".\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "c97f4f143557927e9109a2e48acc10ef63ae4c61",
            "filename": "src/transformers/models/parakeet/processing_parakeet.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fparakeet%2Fprocessing_parakeet.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,7 +16,7 @@\n from ...audio_utils import AudioInput, make_list_of_audio\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -38,17 +38,26 @@ class ParakeetProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class ParakeetProcessor(ProcessorMixin):\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         audio: AudioInput,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput], None] = None,\n         sampling_rate: Optional[int] = None,\n         **kwargs: Unpack[ParakeetProcessorKwargs],\n     ):\n+        r\"\"\"\n+        sampling_rate (`int`, *optional*):\n+            The sampling rate of the input audio in Hz. This should match the sampling rate expected by the feature\n+            extractor (defaults to 16000 Hz). If provided, it will be validated against the processor's expected\n+            sampling rate, and an error will be raised if they don't match. If not provided, a warning will be\n+            issued and the default sampling rate will be assumed.\n+        \"\"\"\n         audio = make_list_of_audio(audio)\n \n         output_kwargs = self._merge_kwargs("
        },
        {
            "sha": "c7c3381882c71153f6ea5834c6d375e93be77103",
            "filename": "src/transformers/models/perception_lm/processing_perception_lm.py",
            "status": "modified",
            "additions": 10,
            "deletions": 42,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fprocessing_perception_lm.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -23,7 +23,7 @@\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n from ...video_utils import VideoInput\n \n \n@@ -39,28 +39,8 @@ class PerceptionLMProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class PerceptionLMProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a PerceptionLM processor which wraps a PerceptionLM image processor, a PerceptionLM video processor, and a tokenizer into a single processor.\n-\n-    [`PerceptionLMProcessor`] offers all the functionalities of [`PerceptionLMImageProcessorFast`], [`PerceptionLMVideoProcessor`], and the tokenizer (e.g. [`LlamaTokenizerFast`]). See the\n-    [`~PerceptionLMProcessor.__call__`] and [`~PerceptionLMProcessor.decode`] for more information.\n-\n-    Args:\n-        video_processor ([`PerceptionLMVideoProcessor`], *optional*):\n-            The video processor to process video inputs.\n-        image_processor ([`PerceptionLMImageProcessorFast`], *optional*):\n-            The image processor to process image inputs.\n-        tokenizer ([`LlamaTokenizerFast`] or similar, *optional*):\n-            The tokenizer to process text inputs.\n-        patch_size (`int`, *optional*):\n-            Patch size from the vision tower.\n-        chat_template (`str`, *optional*):\n-            A Jinja template which will be used to convert lists of messages in a chat into a tokenizable string.\n-        pooling_ratio (`int`, *optional*, defaults to 2):\n-            Pooling ratio for vision tokens. If not 1, 2D adaptive pooling is applied over projected vision tokens.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         video_processor=None,\n@@ -71,6 +51,12 @@ def __init__(\n         pooling_ratio=2,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        patch_size (`int`, *optional*):\n+            Patch size from the vision tower.\n+        pooling_ratio (`int`, *optional*, defaults to 2):\n+            Pooling ratio for vision tokens. If not 1, 2D adaptive pooling is applied over projected vision tokens.\n+        \"\"\"\n         self.patch_size = patch_size\n         self.pooling_ratio = pooling_ratio\n         self.image_token = tokenizer.image_token\n@@ -79,33 +65,15 @@ def __init__(\n         self.video_token_id = tokenizer.video_token_id\n         super().__init__(video_processor, image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[PerceptionLMProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Prepares a batch containing one or more sequences of text and/or images and/or videos.\n-\n-        If `text` is provided, it is tokenized using the tokenizer.\n-        If `images` is provided, they are processed using the image processor.\n-        If `videos` is provided, they are processed using the video processor.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`, *optional*):\n-                The image or batch of images to be processed. Each image can be a PIL image, NumPy array, or PyTorch tensor.\n-                Both channels-first and channels-last formats are supported.\n-            text (`str`, `List[str]`, *optional*):\n-                The sequence or batch of sequences to be tokenized. Each sequence can be a string.\n-            videos (`Any`, *optional*):\n-                The video or batch of videos to be processed.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "3074b168f771a93a11b69d7e8ceece8317a965e3",
            "filename": "src/transformers/models/phi4_multimodal/processing_phi4_multimodal.py",
            "status": "modified",
            "additions": 4,
            "deletions": 38,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fprocessing_phi4_multimodal.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -24,7 +24,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n@@ -38,26 +38,8 @@ class Phi4MultimodalProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Phi4MultimodalProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Phi4Multimodal processor which raps an image processor, a audio processor, and a GPT tokenizer into a single processor.\n-\n-    [`Phi4MultimodalProcessor`] offers all the functionalities of [`Phi4MultimodalImageProcessorFast`] and [`GPT2Tokenizer`]. See the\n-    [`~Phi4MultimodalProcessor.__call__`] and [`~Phi4MultimodalProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`Phi4MultimodalImageProcessorFast`):\n-            The image processor to use for images.\n-        audio_processor (`Phi4MultimodalFeatureExtractor`):\n-            The audio processor to use for audio inputs.\n-        tokenizer (`GPT2TokenizerFast`):\n-            The tokenizer to use for text.\n-        fake_image_token_pattern (`str`, *optional*, defaults to `r\"<\\|image_\\d+\\|>\"`):\n-            The fake image token pattern.\n-        fake_audio_token_pattern (`str`, *optional*, defaults to `r\"<\\|audio_\\d+\\|>\"`):\n-            The fake audio token pattern.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor,\n@@ -71,31 +53,15 @@ def __init__(\n         self.audio_token_id = tokenizer.audio_token_id\n         super().__init__(image_processor, audio_processor, tokenizer, **kwargs)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: Union[TextInput, list[TextInput]],\n         images: Optional[ImageInput] = None,\n         audio: Optional[AudioInput] = None,\n         **kwargs: Unpack[ProcessingKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forards the `text`\n-        and `kwargs` arguments to GPT2Tokenizer's [`~GPT2Tokenizer.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        Phi4MultimodalImageProcessorFast's [`~Phi4MultimodalImageProcessorFast.__call__`] if `images` is not `None`. Please refer to the doctsring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            audio (`list[Union[np.ndarray, torch.Tensor]]`):\n-                List of the audios to be prepared.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "b2979d492d141c74fe4973b59e89b0f2ea94d60f",
            "filename": "src/transformers/models/pix2struct/processing_pix2struct.py",
            "status": "modified",
            "additions": 3,
            "deletions": 21,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fprocessing_pix2struct.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,7 +20,7 @@\n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n class Pix2StructProcessorKwargs(ProcessingKwargs, total=False):\n@@ -45,37 +45,19 @@ class Pix2StructProcessorKwargs(ProcessingKwargs, total=False):\n logger = logging.get_logger(__name__)\n \n \n+@auto_docstring\n class Pix2StructProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a PIX2STRUCT processor which wraps a BERT tokenizer and PIX2STRUCT image processor into a single\n-    processor.\n-\n-    [`Pix2StructProcessor`] offers all the functionalities of [`Pix2StructImageProcessor`] and [`T5Tokenizer`]. See\n-    the docstring of [`~Pix2StructProcessor.__call__`] and [`~Pix2StructProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`Pix2StructImageProcessor`):\n-            An instance of [`Pix2StructImageProcessor`]. The image processor is a required input.\n-        tokenizer (`T5Tokenizer`):\n-            An instance of ['T5Tokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor, tokenizer):\n         tokenizer.return_token_type_ids = False\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images=None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[Pix2StructProcessorKwargs],\n     ) -> Union[BatchEncoding, BatchFeature]:\n-        \"\"\"\n-        This method uses [`Pix2StructImageProcessor.preprocess`] method to prepare image(s) for the model, and\n-        [`T5Tokenizer.__call__`] to prepare text for the model.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You have to specify either images or text.\")\n "
        },
        {
            "sha": "83bdafdf4174e65643797f49ad9c3588ce818c49",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 16,
            "deletions": 47,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -28,7 +28,7 @@\n     Unpack,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import is_vision_available, logging\n+from ...utils import auto_docstring, is_vision_available, logging\n \n \n if is_vision_available():\n@@ -60,32 +60,8 @@ def is_image_or_image_url(elem):\n     return is_url(elem) or is_valid_image(elem)\n \n \n+@auto_docstring\n class PixtralProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Pixtral processor which wraps a Pixtral image processor and a Pixtral tokenizer into a single processor.\n-\n-    [`PixtralProcessor`] offers all the functionalities of [`CLIPImageProcessor`] and [`LlamaTokenizerFast`]. See the\n-    [`~PixtralProcessor.__call__`] and [`~PixtralProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`PixtralImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        patch_size (`int`, *optional*, defaults to 16):\n-            Patch size from the vision tower.\n-        spatial_merge_size (`int`, *optional*, defaults to 1):\n-            The downsampling factor for the spatial merge operation.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        image_token (`str`, *optional*, defaults to `\"[IMG]\"`):\n-            Special token used to denote image location.\n-        image_break_token (`str`, *optional*, defaults to `\"[IMG_BREAK]\"`):\n-            Special token used to denote the end of a line of pixels in an image.\n-        image_end_token (`str`, *optional*, defaults to `\"[IMG_END]\"`):\n-            Special token used to denote the end of an image input.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -98,6 +74,18 @@ def __init__(\n         image_end_token=\"[IMG_END]\",\n         **kwargs,\n     ):\n+        r\"\"\"\n+        patch_size (`int`, *optional*, defaults to 16):\n+            Patch size from the vision tower.\n+        spatial_merge_size (`int`, *optional*, defaults to 1):\n+            The downsampling factor for the spatial merge operation.\n+        image_token (`str`, *optional*, defaults to `\"[IMG]\"`):\n+            Special token used to denote image location.\n+        image_break_token (`str`, *optional*, defaults to `\"[IMG_BREAK]\"`):\n+            Special token used to denote the end of a line of pixels in an image.\n+        image_end_token (`str`, *optional*, defaults to `\"[IMG_END]\"`):\n+            Special token used to denote the end of an image input.\n+        \"\"\"\n         self.patch_size = patch_size\n         self.spatial_merge_size = spatial_merge_size\n         self.image_token = image_token\n@@ -110,33 +98,14 @@ def __init__(\n         self.image_ids = [self.image_token_id, self.image_break_token_id, self.image_end_token_id]\n         super().__init__(image_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[PixtralProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        CLIPImageProcessor's [`~CLIPImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "b7be77c56b2f11a6148ef2da7c4613ae38775085",
            "filename": "src/transformers/models/pop2piano/processing_pop2piano.py",
            "status": "modified",
            "additions": 16,
            "deletions": 22,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fprocessing_pop2piano.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -21,29 +21,17 @@\n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import ProcessorMixin\n from ...tokenization_python import BatchEncoding, PaddingStrategy, TruncationStrategy\n-from ...utils import TensorType\n+from ...utils import TensorType, auto_docstring\n from ...utils.import_utils import requires\n \n \n @requires(backends=(\"essentia\", \"librosa\", \"pretty_midi\", \"scipy\", \"torch\"))\n+@auto_docstring\n class Pop2PianoProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs an Pop2Piano processor which wraps a Pop2Piano Feature Extractor and Pop2Piano Tokenizer into a single\n-    processor.\n-\n-    [`Pop2PianoProcessor`] offers all the functionalities of [`Pop2PianoFeatureExtractor`] and [`Pop2PianoTokenizer`].\n-    See the docstring of [`~Pop2PianoProcessor.__call__`] and [`~Pop2PianoProcessor.decode`] for more information.\n-\n-    Args:\n-        feature_extractor (`Pop2PianoFeatureExtractor`):\n-            An instance of [`Pop2PianoFeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer (`Pop2PianoTokenizer`):\n-            An instance of ['Pop2PianoTokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         audio: Union[np.ndarray, list[float], list[np.ndarray]] = None,\n@@ -58,15 +46,21 @@ def __call__(\n         verbose: bool = True,\n         **kwargs,\n     ) -> Union[BatchFeature, BatchEncoding]:\n-        \"\"\"\n-        This method uses [`Pop2PianoFeatureExtractor.__call__`] method to prepare log-mel-spectrograms for the model,\n-        and [`Pop2PianoTokenizer.__call__`] to prepare token_ids from notes.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n-\n         # Since Feature Extractor needs both audio and sampling_rate and tokenizer needs both token_ids and\n         # feature_extractor_output, we must check for both.\n+        r\"\"\"\n+        sampling_rate (`int` or `list[int]`, *optional*):\n+            The sampling rate of the input audio in Hz. This should match the sampling rate used by the feature\n+            extractor. If not provided, the default sampling rate from the processor configuration will be used.\n+        steps_per_beat (`int`, *optional*, defaults to `2`):\n+            The number of time steps per musical beat. This parameter controls the temporal resolution of the\n+            musical representation. A higher value provides finer temporal granularity but increases the sequence\n+            length. Used when processing audio to extract musical features.\n+        notes (`list` or `TensorType`, *optional*):\n+            Pre-extracted musical notes in MIDI format. When provided, the processor skips audio feature extraction\n+            and directly processes the notes through the tokenizer. Each note should be represented as a list or\n+            tensor containing pitch, velocity, and timing information.\n+        \"\"\"\n         if (audio is None and sampling_rate is None) and (notes is None):\n             raise ValueError(\n                 \"You have to specify at least audios and sampling_rate in order to use feature extractor or \""
        },
        {
            "sha": "8071241eb6e023e88257624134a66a980f9820b6",
            "filename": "src/transformers/models/qwen2_5_omni/processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 36,
            "deletions": 42,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fprocessing_qwen2_5_omni.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -26,12 +26,46 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, VideosKwargs\n from ...tokenization_utils_base import AudioInput, PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n from ...video_utils import VideoInput\n \n \n # Redefine kwargs for videos because Qwen-Omni uses some kwargs for processing omni\n # and does not use them in video processor class\n class Qwen2_5_OmniVideosKwargs(VideosKwargs, total=False):\n+    \"\"\"\n+    min_pixels (`int`, *optional*):\n+        Minimum number of pixels (height Ã— width) for video frames after resizing. Frames smaller than this\n+        threshold will be upscaled to meet the minimum requirement.\n+    max_pixels (`int`, *optional*):\n+        Maximum number of pixels (height Ã— width) for video frames after resizing. Frames larger than this\n+        threshold will be downscaled to fit within the limit.\n+    patch_size (`int`, *optional*):\n+        The spatial patch size used by the vision encoder. Video frames are divided into patches of this size\n+        in both height and width dimensions.\n+    temporal_patch_size (`int`, *optional*):\n+        The temporal patch size used by the vision encoder. This determines how many consecutive frames are\n+        grouped together as a single temporal patch.\n+    merge_size (`int`, *optional*):\n+        The merge size used for combining spatial patches. Multiple patches are merged together to reduce the\n+        sequence length while maintaining spatial information.\n+    min_frames (`int`, *optional*):\n+        Minimum number of frames to extract from the video. Videos with fewer frames will be padded or repeated\n+        to meet this requirement.\n+    max_frames (`int`, *optional*):\n+        Maximum number of frames to extract from the video. Longer videos will be truncated or sampled to fit\n+        within this limit.\n+    use_audio_in_video (`bool`, *optional*, defaults to `False`):\n+        Whether to incorporate audio information when processing videos. When enabled, audio tokens are\n+        interleaved with video tokens based on temporal alignment, creating a unified multimodal representation.\n+    seconds_per_chunk (`float`, *optional*, defaults to `2.0`):\n+        The duration (in seconds) of each video chunk when splitting long videos. This parameter controls how\n+        videos are divided into temporal segments for processing.\n+    position_id_per_seconds (`int` or `float`, *optional*, defaults to `25`):\n+        The number of position IDs allocated per second of video. This parameter controls the temporal resolution\n+        of position embeddings and is used to align video tokens with audio tokens when `use_audio_in_video=True`.\n+    \"\"\"\n+\n     min_pixels: int\n     max_pixels: int\n     patch_size: int\n@@ -69,25 +103,8 @@ class Qwen2_5OmniProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Qwen2_5OmniProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Qwen2.5Omni processor.\n-    [`Qwen2_5OmniProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`], [`WhisperFeatureExtractor`], and [`Qwen2TokenizerFast`]. See the\n-    [`~Qwen2_5OmniProcessor.__call__`] and [`~Qwen2_5OmniProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n-            The image processor.\n-        video_processor ([`Qwen2VLVideoProcessor`], *optional*):\n-            The video processor.\n-        feature_extractor ([`WhisperFeatureExtractor`], *optional*):\n-            The audio feature extractor.\n-        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n-            The text tokenizer.\n-        chat_template (`Optional[str]`, *optional*):\n-            The Jinja template to use for formatting the conversation. If not provided, the default chat template is used.\n-    \"\"\"\n-\n     def __init__(\n         self, image_processor=None, video_processor=None, feature_extractor=None, tokenizer=None, chat_template=None\n     ):\n@@ -100,6 +117,7 @@ def __init__(\n         self.audio_bos_token = self.tokenizer.audio_bos_token\n         self.audio_eos_token = self.tokenizer.audio_eos_token\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n@@ -108,30 +126,6 @@ def __call__(\n         audio: Optional[AudioInput] = None,\n         **kwargs: Unpack[Qwen2_5OmniProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`\n-        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the audio(s), this method forwards the `audio` and `kwargs` arguments to\n-        WhisperFeatureExtractor's [`~WhisperFeatureExtractor.__call__`] if `audio` is not `None`. To prepare the vision inputs,\n-        this method forwards the `vision_infos` and `kwargs` arguments to Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`]\n-        if `vision_infos` is not `None`. Please refer to the doctsring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            audio (`np.ndarray`, `list[np.ndarray]`):\n-                The audio or batch of audio to be prepared. Each audio can be a NumPy array.\n-        \"\"\"\n-\n         if text is None:\n             raise ValueError(\"You need to specify either a `text` input to process.\")\n "
        },
        {
            "sha": "bd0935db283f335e6f1a19d375348f08fab0414f",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 37,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -844,21 +844,6 @@ class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n \n \n class Qwen2_5_VLProcessor(Qwen2VLProcessor):\n-    r\"\"\"\n-    Constructs a Qwen2.5-VL processor which wraps a Qwen2.5-VL image processor and a Qwen2 tokenizer into a single processor.\n-    [`Qwen2_5_VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n-    [`~Qwen2_5_VLProcessor.__call__`] and [`~Qwen2_5_VLProcessor.decode`] for more information.\n-    Args:\n-        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`Qwen2_5_VLVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     @property\n     def model_input_names(self):\n         tokenizer_input_names = self.tokenizer.model_input_names\n@@ -876,28 +861,7 @@ def __call__(\n         videos: VideoInput | None = None,\n         **kwargs: Unpack[Qwen2_5_VLProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n-        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "7b94fe8bd1962731834564ad5b3b60f3350f0659",
            "filename": "src/transformers/models/qwen2_5_vl/processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 37,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fprocessing_qwen2_5_vl.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -29,6 +29,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n from ...video_utils import VideoInput\n \n \n@@ -42,22 +43,8 @@ class Qwen2_5_VLProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Qwen2_5_VLProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Qwen2.5-VL processor which wraps a Qwen2.5-VL image processor and a Qwen2 tokenizer into a single processor.\n-    [`Qwen2_5_VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n-    [`~Qwen2_5_VLProcessor.__call__`] and [`~Qwen2_5_VLProcessor.decode`] for more information.\n-    Args:\n-        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`Qwen2_5_VLVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n@@ -73,35 +60,15 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n         )\n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: ImageInput | None = None,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         videos: VideoInput | None = None,\n         **kwargs: Unpack[Qwen2_5_VLProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n-        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "6956a2d24be8b2cb2c8a9e9c7841600543840d19",
            "filename": "src/transformers/models/qwen2_audio/processing_qwen2_audio.py",
            "status": "modified",
            "additions": 11,
            "deletions": 37,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_audio%2Fprocessing_qwen2_audio.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -22,6 +22,7 @@\n from ...feature_extraction_utils import BatchFeature\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class Qwen2AudioProcessorKwargs(ProcessingKwargs, total=False):\n@@ -33,29 +34,8 @@ class Qwen2AudioProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Qwen2AudioProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Qwen2Audio processor which wraps a Qwen2Audio feature extractor and a Qwen2Audio tokenizer into a single processor.\n-\n-    [`Qwen2AudioProcessor`] offers all the functionalities of [`WhisperFeatureExtractor`] and [`Qwen2TokenizerFast`]. See the\n-    [`~Qwen2AudioProcessor.__call__`] and [`~Qwen2AudioProcessor.decode`] for more information.\n-\n-    Args:\n-        feature_extractor ([`WhisperFeatureExtractor`], *optional*):\n-            The feature extractor is a required input.\n-        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        chat_template (`Optional[str]`, *optional*):\n-                The Jinja template to use for formatting the conversation. If not provided, the default chat template\n-                is used.\n-        audio_token (`str`, *optional*, defaults to `\"<|AUDIO|>\"`):\n-            The token to use for audio tokens.\n-        audio_bos_token (`str`, *optional*, defaults to `\"<|audio_bos|>\"`):\n-            The token to use for audio bos tokens.\n-        audio_eos_token (`str`, *optional*, defaults to `\"<|audio_eos|>\"`):\n-            The token to use for audio eos tokens.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         feature_extractor=None,\n@@ -65,6 +45,14 @@ def __init__(\n         audio_bos_token=\"<|audio_bos|>\",\n         audio_eos_token=\"<|audio_eos|>\",\n     ):\n+        r\"\"\"\n+        audio_token (`str`, *optional*, defaults to `\"<|AUDIO|>\"`):\n+            The token to use for audio tokens.\n+        audio_bos_token (`str`, *optional*, defaults to `\"<|audio_bos|>\"`):\n+            The token to use for audio bos tokens.\n+        audio_eos_token (`str`, *optional*, defaults to `\"<|audio_eos|>\"`):\n+            The token to use for audio eos tokens.\n+        \"\"\"\n         if chat_template is None:\n             chat_template = self.default_chat_template\n         self.audio_token = tokenizer.audio_token if hasattr(tokenizer, \"audio_token\") else audio_token\n@@ -73,27 +61,13 @@ def __init__(\n         self.audio_eos_token = tokenizer.audio_eos_token if hasattr(tokenizer, \"audio_eos_token\") else audio_eos_token\n         super().__init__(feature_extractor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         audio: Union[np.ndarray, list[np.ndarray]] = None,\n         **kwargs: Unpack[Qwen2AudioProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`\n-        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the audio(s), this method forwards the `audios` and `kwargs` arguments to\n-        WhisperFeatureExtractor's [`~WhisperFeatureExtractor.__call__`] if `audios` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            audio (`np.ndarray`, `list[np.ndarray]`):\n-                The audio or batch of audios to be prepared. Each audio can be a NumPy array.\n-        \"\"\"\n         if text is None:\n             raise ValueError(\"You need to specify `text` input to process.\")\n         elif isinstance(text, str):"
        },
        {
            "sha": "5764693361ee838c879696dbbe8e9c7674b78a64",
            "filename": "src/transformers/models/qwen2_vl/processing_qwen2_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 38,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fprocessing_qwen2_vl.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -28,7 +28,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n from ...video_utils import VideoInput\n \n \n@@ -44,22 +44,8 @@ class Qwen2VLProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Qwen2VLProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Qwen2-VL processor which wraps a Qwen2-VL image processor and a Qwen2 tokenizer into a single processor.\n-    [`Qwen2VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n-    [`~Qwen2VLProcessor.__call__`] and [`~Qwen2VLProcessor.decode`] for more information.\n-    Args:\n-        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`Qwen2VLVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n@@ -75,35 +61,15 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n         )\n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[Qwen2VLProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n-        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "6a34d20cc93fdde72ea492966ee34f6253ae0067",
            "filename": "src/transformers/models/qwen3_omni_moe/modular_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fmodular_qwen3_omni_moe.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -2847,30 +2847,6 @@ def __call__(\n         audio: AudioInput | None = None,\n         **kwargs,\n     ):\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`\n-        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the audio(s), this method forwards the `audio` and `kwargs` arguments to\n-        WhisperFeatureExtractor's [`~WhisperFeatureExtractor.__call__`] if `audio` is not `None`. To prepare the vision inputs,\n-        this method forwards the `vision_infos` and `kwargs` arguments to Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`]\n-        if `vision_infos` is not `None`. Please refer to the doctsring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            audio (`np.ndarray`, `List[np.ndarray]`):\n-                The audio or batch of audio to be prepared. Each audio can be a NumPy array.\n-        \"\"\"\n-\n         if text is None:\n             raise ValueError(\"You need to specify either a `text` input to process.\")\n "
        },
        {
            "sha": "1f769b994dd20d761db0d26788ad58df1ca31728",
            "filename": "src/transformers/models/qwen3_omni_moe/processing_qwen3_omni_moe.py",
            "status": "modified",
            "additions": 36,
            "deletions": 42,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_omni_moe%2Fprocessing_qwen3_omni_moe.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -28,12 +28,46 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, VideosKwargs\n from ...tokenization_utils_base import TextInput\n+from ...utils import auto_docstring\n from ...video_utils import VideoInput, make_batched_videos\n \n \n # Redefine kwargs for videos because Qwen-Omni uses some kwargs for processing omni\n # and does not use them in video processor class\n class Qwen3OmniMoeVideosKwargs(VideosKwargs, total=False):\n+    \"\"\"\n+    min_pixels (`int`, *optional*):\n+        Minimum number of pixels (height Ã— width) for video frames after resizing. Frames smaller than this\n+        threshold will be upscaled to meet the minimum requirement.\n+    max_pixels (`int`, *optional*):\n+        Maximum number of pixels (height Ã— width) for video frames after resizing. Frames larger than this\n+        threshold will be downscaled to fit within the limit.\n+    patch_size (`int`, *optional*):\n+        The spatial patch size used by the vision encoder. Video frames are divided into patches of this size\n+        in both height and width dimensions.\n+    temporal_patch_size (`int`, *optional*):\n+        The temporal patch size used by the vision encoder. This determines how many consecutive frames are\n+        grouped together as a single temporal patch.\n+    merge_size (`int`, *optional*):\n+        The merge size used for combining spatial patches. Multiple patches are merged together to reduce the\n+        sequence length while maintaining spatial information.\n+    min_frames (`int`, *optional*):\n+        Minimum number of frames to extract from the video. Videos with fewer frames will be padded or repeated\n+        to meet this requirement.\n+    max_frames (`int`, *optional*):\n+        Maximum number of frames to extract from the video. Longer videos will be truncated or sampled to fit\n+        within this limit.\n+    use_audio_in_video (`bool`, *optional*, defaults to `False`):\n+        Whether to incorporate audio information when processing videos. When enabled, audio tokens are\n+        interleaved with video tokens based on temporal alignment, creating a unified multimodal representation.\n+    seconds_per_chunk (`float`, *optional*, defaults to `2.0`):\n+        The duration (in seconds) of each video chunk when splitting long videos. This parameter controls how\n+        videos are divided into temporal segments for processing.\n+    position_id_per_seconds (`int` or `float`, *optional*, defaults to `25`):\n+        The number of position IDs allocated per second of video. This parameter controls the temporal resolution\n+        of position embeddings and is used to align video tokens with audio tokens when `use_audio_in_video=True`.\n+    \"\"\"\n+\n     min_pixels: int\n     max_pixels: int\n     patch_size: int\n@@ -82,25 +116,8 @@ def _get_feat_extract_output_lengths(input_lengths):\n     return output_lengths\n \n \n+@auto_docstring\n class Qwen3OmniMoeProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Qwen2.5Omni processor.\n-    [`Qwen3OmniMoeProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`], [`WhisperFeatureExtractor`], and [`Qwen2TokenizerFast`]. See the\n-    [`~Qwen3OmniMoeProcessor.__call__`] and [`~Qwen3OmniMoeProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n-            The image processor.\n-        video_processor ([`Qwen2VLVideoProcessor`], *optional*):\n-            The video processor.\n-        feature_extractor ([`WhisperFeatureExtractor`], *optional*):\n-            The audio feature extractor.\n-        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n-            The text tokenizer.\n-        chat_template (`Optional[str]`, *optional*):\n-            The Jinja template to use for formatting the conversation. If not provided, the default chat template is used.\n-    \"\"\"\n-\n     def __init__(\n         self, image_processor=None, video_processor=None, feature_extractor=None, tokenizer=None, chat_template=None\n     ):\n@@ -113,6 +130,7 @@ def __init__(\n         self.audio_bos_token = self.tokenizer.audio_bos_token\n         self.audio_eos_token = self.tokenizer.audio_eos_token\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: TextInput = None,\n@@ -121,30 +139,6 @@ def __call__(\n         audio: AudioInput | None = None,\n         **kwargs,\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`\n-        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the audio(s), this method forwards the `audio` and `kwargs` arguments to\n-        WhisperFeatureExtractor's [`~WhisperFeatureExtractor.__call__`] if `audio` is not `None`. To prepare the vision inputs,\n-        this method forwards the `vision_infos` and `kwargs` arguments to Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`]\n-        if `vision_infos` is not `None`. Please refer to the doctsring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `List[str]`, `List[List[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `List[PIL.Image.Image]`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            videos (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            audio (`np.ndarray`, `List[np.ndarray]`):\n-                The audio or batch of audio to be prepared. Each audio can be a NumPy array.\n-        \"\"\"\n-\n         if text is None:\n             raise ValueError(\"You need to specify either a `text` input to process.\")\n "
        },
        {
            "sha": "118a80ebcdc04e17ed1375b91c06302e4bf37e66",
            "filename": "src/transformers/models/qwen3_vl/modular_qwen3_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 37,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fmodular_qwen3_vl.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -1361,21 +1361,6 @@ class Qwen3VLProcessorKwargs(ProcessingKwargs, total=False):\n \n \n class Qwen3VLProcessor(Qwen2VLProcessor):\n-    r\"\"\"\n-    Constructs a Qwen3VL processor which wraps a Qwen3VL image processor and a Qwen2 tokenizer into a single processor.\n-    [`Qwen3VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n-    [`~Qwen3VLProcessor.__call__`] and [`~Qwen3VLProcessor.decode`] for more information.\n-    Args:\n-        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`Qwen3VLVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         super().__init__(image_processor, tokenizer, video_processor, chat_template, **kwargs)\n         self.vision_start_token = (\n@@ -1402,28 +1387,7 @@ def __call__(\n         videos: VideoInput = None,\n         **kwargs: Unpack[Qwen3VLProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequence(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n-        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The video or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "4f8d733d3c5a313f2e4e64dd718585dc59f6f0cb",
            "filename": "src/transformers/models/qwen3_vl/processing_qwen3_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 38,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_vl%2Fprocessing_qwen3_vl.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -24,7 +24,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n from ...video_utils import VideoInput\n \n \n@@ -42,22 +42,8 @@ class Qwen3VLProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Qwen3VLProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Qwen3VL processor which wraps a Qwen3VL image processor and a Qwen2 tokenizer into a single processor.\n-    [`Qwen3VLProcessor`] offers all the functionalities of [`Qwen2VLImageProcessor`] and [`Qwen2TokenizerFast`]. See the\n-    [`~Qwen3VLProcessor.__call__`] and [`~Qwen3VLProcessor.decode`] for more information.\n-    Args:\n-        image_processor ([`Qwen2VLImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`Qwen2TokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`Qwen3VLVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n@@ -89,35 +75,15 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n             else tokenizer.convert_tokens_to_ids(self.vision_end_token)\n         )\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: ImageInput = None,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         videos: VideoInput = None,\n         **kwargs: Unpack[Qwen3VLProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequence(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n-        Qwen2VLImageProcessor's [`~Qwen2VLImageProcessor.__call__`] if `vision_infos` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The video or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "e5b242494efe94508bdcd1f80206363e3964e8b3",
            "filename": "src/transformers/models/sam/processing_sam.py",
            "status": "modified",
            "additions": 30,
            "deletions": 17,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -23,7 +23,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n-from ...utils import is_torch_available\n+from ...utils import auto_docstring, is_torch_available\n \n \n if is_torch_available():\n@@ -33,6 +33,33 @@\n \n \n class SamImagesKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    segmentation_maps (`ImageInput`, *optional*):\n+        Ground truth segmentation maps to process alongside the input images. These maps are used for training\n+        or evaluation purposes and are resized and normalized to match the processed image dimensions.\n+    input_points (`NestedList`, *optional*):\n+        Input points for prompt-based segmentation. Should be a nested list with structure\n+        `[image_level, object_level, point_level, [x, y]]` where each point is specified as `[x, y]` coordinates\n+        in the original image space. Points are normalized to the target image size before being passed to the model.\n+    input_labels (`NestedList`, *optional*):\n+        Labels for the input points, indicating whether each point is a foreground (1) or background (0) point.\n+        Should be a nested list with structure `[image_level, object_level, point_level]`. Must have the same\n+        structure as `input_points` (excluding the coordinate dimension).\n+    input_boxes (`NestedList`, *optional*):\n+        Bounding boxes for prompt-based segmentation. Should be a nested list with structure\n+        `[image_level, box_level, [x1, y1, x2, y2]]` where each box is specified as `[x1, y1, x2, y2]` coordinates\n+        in the original image space. Boxes are normalized to the target image size before being passed to the model.\n+    point_pad_value (`int`, *optional*, defaults to `-10`):\n+        The value used for padding input points when batching sequences of different lengths. This value marks\n+        padded positions and is preserved during coordinate normalization to distinguish real points from padding.\n+    mask_size (`dict[str, int]`, *optional*):\n+        Dictionary specifying the target mask size with keys `\"height\"` and `\"width\"`. This determines the\n+        resolution of the output segmentation masks generated by the model.\n+    mask_pad_size (`dict[str, int]`, *optional*):\n+        Dictionary specifying the padding size for masks with keys `\"height\"` and `\"width\"`. This is used when\n+        batching masks of different sizes to ensure consistent dimensions.\n+    \"\"\"\n+\n     segmentation_maps: Optional[ImageInput]\n     input_points: Optional[NestedList]\n     input_labels: Optional[NestedList]\n@@ -51,33 +78,19 @@ class SamProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class SamProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a SAM processor which wraps a SAM image processor and an 2D points & Bounding boxes processor into a\n-    single processor.\n-\n-    [`SamProcessor`] offers all the functionalities of [`SamImageProcessor`]. See the docstring of\n-    [`~SamImageProcessor.__call__`] for more information.\n-\n-    Args:\n-        image_processor (`SamImageProcessor`):\n-            An instance of [`SamImageProcessor`]. The image processor is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor):\n         super().__init__(image_processor)\n         self.target_size = self.image_processor.size[\"longest_edge\"]\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         **kwargs,\n     ) -> BatchEncoding:\n-        \"\"\"\n-        This method uses [`SamImageProcessor.__call__`] method to prepare image(s) for the model. It also prepares 2D\n-        points and bounding boxes for the model if they are provided.\n-        \"\"\"\n         output_kwargs = self._merge_kwargs(\n             SamProcessorKwargs,\n             tokenizer_init_kwargs={},"
        },
        {
            "sha": "163d318bf35d4120fc697d8d931cca588c8c5072",
            "filename": "src/transformers/models/sam2/processing_sam2.py",
            "status": "modified",
            "additions": 22,
            "deletions": 36,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fprocessing_sam2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -23,7 +23,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding\n-from ...utils import TensorType, is_torch_available, logging\n+from ...utils import TensorType, auto_docstring, is_torch_available, logging\n from ...utils.import_utils import requires\n \n \n@@ -34,28 +34,24 @@\n \n \n @requires(backends=(\"torch\",))\n+@auto_docstring\n class Sam2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a SAM2 processor which wraps a SAM2 image processor and an 2D points & Bounding boxes processor into a\n-    single processor.\n-\n-    [`Sam2Processor`] offers all the functionalities of [`Sam2ImageProcessorFast`] and [`Sam2VideoProcessor`]. See the docstring of\n-    [`~Sam2ImageProcessorFast.__call__`] and [`~Sam2VideoProcessor.__call__`] for more information.\n-\n-    Args:\n-        image_processor (`Sam2ImageProcessorFast`):\n-            An instance of [`Sam2ImageProcessorFast`].\n+    def __init__(self, image_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs):\n+        r\"\"\"\n         target_size (`int`, *optional*):\n-            The target size (target_size, target_size) to which the image will be resized.\n+            The target size (in pixels) for normalizing input points and bounding boxes. If not provided, defaults\n+            to the image processor's size configuration. All input coordinates (points and boxes) are normalized\n+            to this size before being passed to the model. This ensures consistent coordinate representation\n+            regardless of the original image dimensions.\n         point_pad_value (`int`, *optional*, defaults to -10):\n-            The value used for padding input points.\n-    \"\"\"\n-\n-    def __init__(self, image_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs):\n+            The value used for padding input points when batching sequences of different lengths. This value is\n+            used to mark padded positions and is preserved during coordinate normalization.\n+        \"\"\"\n         super().__init__(image_processor, **kwargs)\n         self.point_pad_value = point_pad_value\n         self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n@@ -68,26 +64,16 @@ def __call__(\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n-        This method uses [`Sam2ImageProcessorFast.__call__`] method to prepare image(s) for the model. It also prepares 2D\n-        points and bounding boxes for the model if they are provided.\n-\n-        Args:\n-            images (`ImageInput`, *optional*):\n-                The image(s) to process.\n-            segmentation_maps (`ImageInput`, *optional*):\n-                The segmentation maps to process.\n-            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n-                The points to add to the frame.\n-            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n-                The labels for the points.\n-            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n-                The bounding boxes to add to the frame.\n-            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n-                The original sizes of the images.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return.\n-            **kwargs:\n-                Additional keyword arguments to pass to the image processor.\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to process.\n+        input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+            The points to add to the frame.\n+        input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+            The labels for the points.\n+        input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+            The bounding boxes to add to the frame.\n+        original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+            The original sizes of the images.\n \n         Returns:\n             A [`BatchEncoding`] with the following fields:"
        },
        {
            "sha": "9edce89c157591a9b9422085c598344e5c1b015c",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -607,24 +607,6 @@ def reset_inference_session(self):\n \n \n class Sam2VideoProcessor(Sam2Processor):\n-    r\"\"\"\n-    Constructs a SAM2 processor which wraps a SAM2 image processor and an 2D points & Bounding boxes processor into a\n-    single processor.\n-\n-    [`Sam2VideoProcessor`] offers all the functionalities of [`Sam2ImageProcessorFast`] and [`Sam2VideoProcessor`]. See the docstring of\n-    [`~Sam2ImageProcessorFast.__call__`] and [`~Sam2VideoProcessor.__call__`] for more information.\n-\n-    Args:\n-        image_processor (`Sam2ImageProcessorFast`):\n-            An instance of [`Sam2ImageProcessorFast`].\n-        video_processor (`Sam2VideoVideoProcessor`):\n-            An instance of [`Sam2VideoVideoProcessor`].\n-        target_size (`int`, *optional*):\n-            The target size (target_size, target_size) to which the image will be resized.\n-        point_pad_value (`int`, *optional*, defaults to -10):\n-            The value used for padding input points.\n-    \"\"\"\n-\n     def __init__(\n         self, image_processor, video_processor, target_size: int | None = None, point_pad_value: int = -10, **kwargs\n     ):"
        },
        {
            "sha": "ffb8a46aaedde2ac0a3963791b6dc33da7c31d1a",
            "filename": "src/transformers/models/sam2_video/processing_sam2_video.py",
            "status": "modified",
            "additions": 23,
            "deletions": 39,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fprocessing_sam2_video.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -26,39 +26,33 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding\n-from ...utils import TensorType\n+from ...utils import TensorType, auto_docstring\n from ...utils.import_utils import requires\n from ...video_utils import VideoInput\n from .modeling_sam2_video import Sam2VideoInferenceSession\n \n \n @requires(backends=(\"torch\",))\n+@auto_docstring\n class Sam2VideoProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a SAM2 processor which wraps a SAM2 image processor and an 2D points & Bounding boxes processor into a\n-    single processor.\n-\n-    [`Sam2VideoProcessor`] offers all the functionalities of [`Sam2ImageProcessorFast`] and [`Sam2VideoProcessor`]. See the docstring of\n-    [`~Sam2ImageProcessorFast.__call__`] and [`~Sam2VideoProcessor.__call__`] for more information.\n-\n-    Args:\n-        image_processor (`Sam2ImageProcessorFast`):\n-            An instance of [`Sam2ImageProcessorFast`].\n-        video_processor (`Sam2VideoVideoProcessor`):\n-            An instance of [`Sam2VideoVideoProcessor`].\n-        target_size (`int`, *optional*):\n-            The target size (target_size, target_size) to which the image will be resized.\n-        point_pad_value (`int`, *optional*, defaults to -10):\n-            The value used for padding input points.\n-    \"\"\"\n-\n     def __init__(\n         self, image_processor, video_processor, target_size: int | None = None, point_pad_value: int = -10, **kwargs\n     ):\n+        r\"\"\"\n+        target_size (`int`, *optional*):\n+            The target size (in pixels) for normalizing input points and bounding boxes. If not provided, defaults\n+            to the image processor's size configuration. All input coordinates (points and boxes) are normalized\n+            to this size before being passed to the model. This ensures consistent coordinate representation\n+            regardless of the original image dimensions.\n+        point_pad_value (`int`, *optional*, defaults to -10):\n+            The value used for padding input points when batching sequences of different lengths. This value is\n+            used to mark padded positions and is preserved during coordinate normalization.\n+        \"\"\"\n         super().__init__(image_processor, video_processor, **kwargs)\n         self.point_pad_value = point_pad_value\n         self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n@@ -71,26 +65,16 @@ def __call__(\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n-        This method uses [`Sam2VideoImageProcessorFast.__call__`] method to prepare image(s) for the model. It also prepares 2D\n-        points and bounding boxes for the model if they are provided.\n-\n-        Args:\n-            images (`ImageInput`, *optional*):\n-                The image(s) to process.\n-            segmentation_maps (`ImageInput`, *optional*):\n-                The segmentation maps to process.\n-            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n-                The points to add to the frame.\n-            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n-                The labels for the points.\n-            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n-                The bounding boxes to add to the frame.\n-            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n-                The original sizes of the images.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return.\n-            **kwargs:\n-                Additional keyword arguments to pass to the image processor.\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to process.\n+        input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+            The points to add to the frame.\n+        input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+            The labels for the points.\n+        input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+            The bounding boxes to add to the frame.\n+        original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+            The original sizes of the images.\n \n         Returns:\n             A [`BatchEncoding`] with the following fields:"
        },
        {
            "sha": "d4bb06d8ea3cb96eb01754c2ca16bdf2d87366dc",
            "filename": "src/transformers/models/sam3/processing_sam3.py",
            "status": "modified",
            "additions": 20,
            "deletions": 37,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3%2Fprocessing_sam3.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -23,7 +23,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n-from ...utils import TensorType, is_torch_available, logging\n+from ...utils import TensorType, auto_docstring, is_torch_available, logging\n from ...utils.import_utils import requires\n \n \n@@ -84,32 +84,22 @@ def box_area(boxes):\n \n \n @requires(backends=(\"torch\",))\n+@auto_docstring\n class Sam3Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a SAM3 processor which wraps a SAM3 image processor and bounding boxes processing into a\n-    single processor.\n-\n-    [`Sam2Processor`] offers all the functionalities of [`Sam2ImageProcessorFast`] and [`Sam2VideoProcessor`]. See the docstring of\n-    [`~Sam2ImageProcessorFast.__call__`] and [`~Sam2VideoProcessor.__call__`] for more information.\n-\n-    Args:\n-        image_processor (`Sam2ImageProcessorFast`):\n-            An instance of [`Sam2ImageProcessorFast`].\n-        tokenizer ([`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]):\n-            An instance of [`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]. The tokenizer is a required input.\n+    def __init__(\n+        self, image_processor, tokenizer, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs\n+    ):\n+        r\"\"\"\n         target_size (`int`, *optional*):\n             The target size (target_size, target_size) to which the image will be resized.\n         point_pad_value (`int`, *optional*, defaults to -10):\n             The value used for padding input boxes.\n-    \"\"\"\n-\n-    def __init__(\n-        self, image_processor, tokenizer, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs\n-    ):\n+        \"\"\"\n         super().__init__(image_processor, tokenizer, **kwargs)\n         self.point_pad_value = point_pad_value\n         self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n@@ -122,25 +112,18 @@ def __call__(\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n-        This method uses [`Sam3ImageProcessorFast.__call__`] method to prepare image(s) for the model. It also prepares bounding boxes for the model if they are provided.\n-\n-        Args:\n-            images (`ImageInput`, *optional*):\n-                The image(s) to process.\n-            text (`str`, `list[str]`, `list[list[str]]`, *optional*):\n-                The text to process.\n-            segmentation_maps (`ImageInput`, *optional*):\n-                The segmentation maps to process.\n-            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n-                The bounding boxes to process.\n-            input_boxes_labels (`list[list[int]]`, `torch.Tensor`, *optional*):\n-                The labels for the bounding boxes.\n-            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n-                The original sizes of the images.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return.\n-            **kwargs:\n-                Additional keyword arguments to pass to the image processor.\n+        images (`ImageInput`, *optional*):\n+            The image(s) to process.\n+        text (`str`, `list[str]`, `list[list[str]]`, *optional*):\n+            The text to process.\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to process.\n+        input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+            The bounding boxes to process.\n+        input_boxes_labels (`list[list[int]]`, `torch.Tensor`, *optional*):\n+            The labels for the bounding boxes.\n+        original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+            The original sizes of the images.\n \n         Returns:\n             A [`BatchEncoding`] with the following fields:"
        },
        {
            "sha": "8be101a57515fc4f614ae69a29d37dee3a7f90b1",
            "filename": "src/transformers/models/sam3_tracker/processing_sam3_tracker.py",
            "status": "modified",
            "additions": 22,
            "deletions": 36,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fprocessing_sam3_tracker.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fprocessing_sam3_tracker.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker%2Fprocessing_sam3_tracker.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -28,33 +28,29 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding\n-from ...utils import TensorType\n+from ...utils import TensorType, auto_docstring\n from ...utils.import_utils import requires\n \n \n @requires(backends=(\"torch\",))\n+@auto_docstring\n class Sam3TrackerProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a SAM3_TRACKER processor which wraps a SAM3_TRACKER image processor and an 2D points & Bounding boxes processor into a\n-    single processor.\n-\n-    [`Sam3TrackerProcessor`] offers all the functionalities of [`Sam3TrackerImageProcessorFast`] and [`Sam3TrackerVideoProcessor`]. See the docstring of\n-    [`~Sam3TrackerImageProcessorFast.__call__`] and [`~Sam3TrackerVideoProcessor.__call__`] for more information.\n-\n-    Args:\n-        image_processor (`Sam3TrackerImageProcessorFast`):\n-            An instance of [`Sam3TrackerImageProcessorFast`].\n+    def __init__(self, image_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs):\n+        r\"\"\"\n         target_size (`int`, *optional*):\n-            The target size (target_size, target_size) to which the image will be resized.\n+            The target size (in pixels) for normalizing input points and bounding boxes. If not provided, defaults\n+            to the image processor's size configuration. All input coordinates (points and boxes) are normalized\n+            to this size before being passed to the model. This ensures consistent coordinate representation\n+            regardless of the original image dimensions.\n         point_pad_value (`int`, *optional*, defaults to -10):\n-            The value used for padding input points.\n-    \"\"\"\n-\n-    def __init__(self, image_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs):\n+            The value used for padding input points when batching sequences of different lengths. This value is\n+            used to mark padded positions and is preserved during coordinate normalization.\n+        \"\"\"\n         super().__init__(image_processor, **kwargs)\n         self.point_pad_value = point_pad_value\n         self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n@@ -67,26 +63,16 @@ def __call__(\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n-        This method uses [`Sam3TrackerImageProcessorFast.__call__`] method to prepare image(s) for the model. It also prepares 2D\n-        points and bounding boxes for the model if they are provided.\n-\n-        Args:\n-            images (`ImageInput`, *optional*):\n-                The image(s) to process.\n-            segmentation_maps (`ImageInput`, *optional*):\n-                The segmentation maps to process.\n-            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n-                The points to add to the frame.\n-            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n-                The labels for the points.\n-            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n-                The bounding boxes to add to the frame.\n-            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n-                The original sizes of the images.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return.\n-            **kwargs:\n-                Additional keyword arguments to pass to the image processor.\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to process.\n+        input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+            The points to add to the frame.\n+        input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+            The labels for the points.\n+        input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+            The bounding boxes to add to the frame.\n+        original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+            The original sizes of the images.\n \n         Returns:\n             A [`BatchEncoding`] with the following fields:"
        },
        {
            "sha": "d4f93de0c0ba3c096187af3fcefbacfd11493779",
            "filename": "src/transformers/models/sam3_tracker_video/processing_sam3_tracker_video.py",
            "status": "modified",
            "additions": 23,
            "deletions": 39,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fprocessing_sam3_tracker_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fprocessing_sam3_tracker_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_tracker_video%2Fprocessing_sam3_tracker_video.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -28,39 +28,33 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding\n-from ...utils import TensorType\n+from ...utils import TensorType, auto_docstring\n from ...utils.import_utils import requires\n from ...video_utils import VideoInput\n from .modeling_sam3_tracker_video import Sam3TrackerVideoInferenceSession\n \n \n @requires(backends=(\"torch\",))\n+@auto_docstring\n class Sam3TrackerVideoProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a SAM2 processor which wraps a SAM2 image processor and an 2D points & Bounding boxes processor into a\n-    single processor.\n-\n-    [`Sam3TrackerVideoProcessor`] offers all the functionalities of [`Sam2ImageProcessorFast`] and [`Sam3TrackerVideoProcessor`]. See the docstring of\n-    [`~Sam2ImageProcessorFast.__call__`] and [`~Sam3TrackerVideoProcessor.__call__`] for more information.\n-\n-    Args:\n-        image_processor (`Sam2ImageProcessorFast`):\n-            An instance of [`Sam2ImageProcessorFast`].\n-        video_processor (`Sam3TrackerVideoVideoProcessor`):\n-            An instance of [`Sam3TrackerVideoVideoProcessor`].\n-        target_size (`int`, *optional*):\n-            The target size (target_size, target_size) to which the image will be resized.\n-        point_pad_value (`int`, *optional*, defaults to -10):\n-            The value used for padding input points.\n-    \"\"\"\n-\n     def __init__(\n         self, image_processor, video_processor, target_size: int | None = None, point_pad_value: int = -10, **kwargs\n     ):\n+        r\"\"\"\n+        target_size (`int`, *optional*):\n+            The target size (in pixels) for normalizing input points and bounding boxes. If not provided, defaults\n+            to the image processor's size configuration. All input coordinates (points and boxes) are normalized\n+            to this size before being passed to the model. This ensures consistent coordinate representation\n+            regardless of the original image dimensions.\n+        point_pad_value (`int`, *optional*, defaults to -10):\n+            The value used for padding input points when batching sequences of different lengths. This value is\n+            used to mark padded positions and is preserved during coordinate normalization.\n+        \"\"\"\n         super().__init__(image_processor, video_processor, **kwargs)\n         self.point_pad_value = point_pad_value\n         self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n@@ -73,26 +67,16 @@ def __call__(\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n-        This method uses [`Sam3TrackerVideoImageProcessorFast.__call__`] method to prepare image(s) for the model. It also prepares 2D\n-        points and bounding boxes for the model if they are provided.\n-\n-        Args:\n-            images (`ImageInput`, *optional*):\n-                The image(s) to process.\n-            segmentation_maps (`ImageInput`, *optional*):\n-                The segmentation maps to process.\n-            input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n-                The points to add to the frame.\n-            input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n-                The labels for the points.\n-            input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n-                The bounding boxes to add to the frame.\n-            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n-                The original sizes of the images.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return.\n-            **kwargs:\n-                Additional keyword arguments to pass to the image processor.\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to process.\n+        input_points (`list[list[list[list[float]]]]`, `torch.Tensor`, *optional*):\n+            The points to add to the frame.\n+        input_labels (`list[list[list[int]]]`, `torch.Tensor`, *optional*):\n+            The labels for the points.\n+        input_boxes (`list[list[list[float]]]`, `torch.Tensor`, *optional*):\n+            The bounding boxes to add to the frame.\n+        original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+            The original sizes of the images.\n \n         Returns:\n             A [`BatchEncoding`] with the following fields:"
        },
        {
            "sha": "9d141237e709f7d896e6dac42fb4e05ed6f51714",
            "filename": "src/transformers/models/sam3_video/processing_sam3_video.py",
            "status": "modified",
            "additions": 13,
            "deletions": 32,
            "changes": 45,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam3_video%2Fprocessing_sam3_video.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -19,32 +19,15 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import BatchEncoding\n-from ...utils import TensorType\n+from ...utils import TensorType, auto_docstring\n from ...utils.import_utils import requires\n from ...video_utils import VideoInput\n from .modeling_sam3_video import Sam3VideoInferenceSession\n \n \n @requires(backends=(\"torch\",))\n+@auto_docstring\n class Sam3VideoProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a SAM3 processor which wraps a SAM3 image processor and an 2D points & Bounding boxes processor into a\n-    single processor.\n-\n-    [`Sam3Processor`] offers all the functionalities of [`Sam3ImageProcessor`] and [`Sam3VideoProcessor`]. See the docstring of\n-    [`~Sam3ImageProcessor.__call__`] and [`~Sam3VideoProcessor.__call__`] for more information.\n-\n-    Args:\n-        image_processor (`Sam3ImageProcessorFast`):\n-            An instance of [`Sam3ImageProcessorFast`].\n-        video_processor (`Sam2VideoVideoProcessor`):\n-            An instance of [`Sam2VideoVideoProcessor`].\n-        tokenizer ([`CLIPTokenizer`, `CLIPTokenizerFast`]):\n-            An instance of [`PreTrainedTokenizer`, `PreTrainedTokenizerFast`]. The tokenizer is a required input.\n-        target_size (`int`, *optional*):\n-            The target size (target_size, target_size) to which the image will be resized.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor,\n@@ -53,9 +36,14 @@ def __init__(\n         target_size: Optional[int] = None,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        target_size (`int`, *optional*):\n+            The target size (target_size, target_size) to which the image will be resized.\n+        \"\"\"\n         super().__init__(image_processor, video_processor, tokenizer, **kwargs)\n         self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n@@ -65,19 +53,12 @@ def __call__(\n         **kwargs,\n     ) -> BatchEncoding:\n         r\"\"\"\n-        This method uses [`Sam3VideoImageProcessorFast.__call__`] method to prepare image(s) for the model.\n-\n-        Args:\n-            images (`ImageInput`, *optional*):\n-                The image(s) to process.\n-            segmentation_maps (`ImageInput`, *optional*):\n-                The segmentation maps to process (optional, for image processor).\n-            original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n-                The original sizes of the images. Only used when images is not provided.\n-            return_tensors (`str` or `TensorType`, *optional*):\n-                The type of tensors to return.\n-            **kwargs:\n-                Additional keyword arguments to pass to the image processor.\n+        images (`ImageInput`, *optional*):\n+            The image(s) to process.\n+        segmentation_maps (`ImageInput`, *optional*):\n+            The segmentation maps to process (optional, for image processor).\n+        original_sizes (`list[list[float]]`, `torch.Tensor`, *optional*):\n+            The original sizes of the images. Only used when images is not provided.\n \n         Returns:\n             A [`BatchEncoding`] with the following fields:"
        },
        {
            "sha": "f56ba4c932c5fc6a011cb876a48964f2322ef1b7",
            "filename": "src/transformers/models/sam_hq/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam_hq%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam_hq%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2F__init__.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,7 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_sam_hq import *\n     from .modeling_sam_hq import *\n-    from .processing_samhq import *\n+    from .processing_sam_hq import *\n else:\n     import sys\n "
        },
        {
            "sha": "893d25e6844b44ea9da77ccf5692b7ba018d27b7",
            "filename": "src/transformers/models/sam_hq/processing_sam_hq.py",
            "status": "renamed",
            "additions": 32,
            "deletions": 19,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_sam_hq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_sam_hq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_sam_hq.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -22,8 +22,8 @@\n \n from ...image_utils import ImageInput\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n-from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n-from ...utils import is_torch_available\n+from ...tokenization_utils_base import BatchEncoding\n+from ...utils import auto_docstring, is_torch_available\n \n \n if is_torch_available():\n@@ -33,6 +33,34 @@\n \n \n class SamHQImagesKwargs(ImagesKwargs, total=False):\n+    \"\"\"\n+    segmentation_maps (`ImageInput`, *optional*):\n+        Ground truth segmentation maps to process alongside the input images. These maps are used for training\n+        or evaluation purposes and are resized and normalized to match the processed image dimensions.\n+    input_points (`NestedList`, *optional*):\n+        Input points for prompt-based segmentation. Should be a nested list with structure\n+        `[image_level, object_level, point_level, [x, y]]` where each point is specified as `[x, y]` coordinates\n+        in the original image space. Points are normalized to the target image size before being passed to the model.\n+    input_labels (`NestedList`, *optional*):\n+        Labels for the input points, indicating whether each point is a foreground (1) or background (0) point.\n+        Should be a nested list with structure `[image_level, object_level, point_level]`. Must have the same\n+        structure as `input_points` (excluding the coordinate dimension).\n+    input_boxes (`NestedList`, *optional*):\n+        Bounding boxes for prompt-based segmentation. Should be a nested list with structure\n+        `[image_level, box_level, [x1, y1, x2, y2]]` where each box is specified as `[x1, y1, x2, y2]` coordinates\n+        in the original image space. Boxes are normalized to the target image size before being passed to the model.\n+    point_pad_value (`int`, *optional*, defaults to `None`):\n+        The value used for padding input points when batching sequences of different lengths. This value marks\n+        padded positions and is preserved during coordinate normalization to distinguish real points from padding.\n+        If `None`, the default pad value from the processor configuration is used.\n+    mask_size (`dict[str, int]`, *optional*):\n+        Dictionary specifying the target mask size with keys `\"height\"` and `\"width\"`. This determines the\n+        resolution of the output segmentation masks generated by the model.\n+    mask_pad_size (`dict[str, int]`, *optional*):\n+        Dictionary specifying the padding size for masks with keys `\"height\"` and `\"width\"`. This is used when\n+        batching masks of different sizes to ensure consistent dimensions.\n+    \"\"\"\n+\n     segmentation_maps: Optional[ImageInput]\n     input_points: Optional[NestedList]\n     input_labels: Optional[NestedList]\n@@ -51,19 +79,8 @@ class SamHQProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class SamHQProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a SAM HQ processor which wraps a SAM  image processor and an 2D points & Bounding boxes processor into a\n-    single processor.\n-\n-    [`SamHQProcessor`] offers all the functionalities of [`SamImageProcessor`]. See the docstring of\n-    [`~SamImageProcessor.__call__`] for more information.\n-\n-    Args:\n-        image_processor (`SamImageProcessor`):\n-            An instance of [`SamImageProcessor`]. The image processor is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor):\n         super().__init__(image_processor)\n         # Ensure image_processor is properly initialized\n@@ -73,16 +90,12 @@ def __init__(self, image_processor):\n             raise ValueError(\"image_processor.size is not set\")\n         self.target_size = self.image_processor.size[\"longest_edge\"]\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n-        text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         **kwargs: Unpack[SamHQProcessorKwargs],\n     ) -> BatchEncoding:\n-        \"\"\"\n-        This method uses [`SamImageProcessor.__call__`] method to prepare image(s) for the model. It also prepares 2D\n-        points and bounding boxes for the model if they are provided.\n-        \"\"\"\n         output_kwargs = self._merge_kwargs(\n             SamHQProcessorKwargs,\n             tokenizer_init_kwargs={},",
            "previous_filename": "src/transformers/models/sam_hq/processing_samhq.py"
        },
        {
            "sha": "0e8c365bba8c6ea4806f9c30ca8c30b65ac1711b",
            "filename": "src/transformers/models/seamless_m4t/processing_seamless_m4t.py",
            "status": "modified",
            "additions": 14,
            "deletions": 32,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fprocessing_seamless_m4t.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,13 +20,23 @@\n from ...audio_utils import AudioInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n class SeamlessM4TTextKwargs(TextKwargs):\n+    \"\"\"\n+    src_lang (`str`, *optional*):\n+        The source language code for the input text (e.g., \"eng\" for English, \"fra\" for French). This is used\n+        to set the language token at the beginning of the input sequence, which helps the model understand the\n+        input language for translation or transcription tasks.\n+    tgt_lang (`str`, *optional*):\n+        The target language code for the output (e.g., \"eng\" for English, \"fra\" for French). This is used to\n+        specify the desired output language for translation tasks. The model will generate text in this language.\n+    \"\"\"\n+\n     src_lang: Optional[str]\n     tgt_lang: Optional[str]\n \n@@ -36,49 +46,21 @@ class SeamlessM4TProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {}\n \n \n+@auto_docstring\n class SeamlessM4TProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a SeamlessM4T processor which wraps a SeamlessM4T feature extractor and a SeamlessM4T tokenizer into a\n-    single processor.\n-\n-    [`SeamlessM4TProcessor`] offers all the functionalities of [`SeamlessM4TFeatureExtractor`] and\n-    [`SeamlessM4TTokenizerFast`]. See the [`~SeamlessM4TProcessor.__call__`] and [`~SeamlessM4TProcessor.decode`] for\n-    more information.\n-\n-    Args:\n-        feature_extractor ([`SeamlessM4TFeatureExtractor`]):\n-            The audio processor is a required input.\n-        tokenizer ([`SeamlessM4TTokenizerFast`]):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     valid_processor_kwargs = SeamlessM4TProcessorKwargs\n \n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         audio: Optional[AudioInput] = None,\n         **kwargs: Unpack[ProcessingKwargs],\n     ):\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `text`\n-        and `kwargs` arguments to SeamlessM4TTokenizerFast's [`~SeamlessM4TTokenizerFast.__call__`] if `text` is not\n-        `None` to encode the text. To prepare the audio(s), this method forwards the `audios` and `kwargs` arguments to\n-        SeamlessM4TFeatureExtractor's [`~SeamlessM4TFeatureExtractor.__call__`] if `audios` is not `None`. Please refer\n-        to the docstring of the above two methods for more information.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            audios (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The audio or batch of audios to be prepared. Each audio can be NumPy array or PyTorch tensor. In case\n-                of a NumPy array/PyTorch tensor, each audio should be of shape (C, T), where C is a number of channels,\n-                and T the sample length of the audio.\n+        r\"\"\"\n         Returns:\n             [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n "
        },
        {
            "sha": "0e554b22ce3241a056880faece92515460b63c81",
            "filename": "src/transformers/models/siglip/processing_siglip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fprocessing_siglip.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,22 +16,11 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class SiglipProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Siglip processor which wraps a Siglip image processor and a Siglip tokenizer into a single processor.\n-\n-    [`SiglipProcessor`] offers all the functionalities of [`SiglipImageProcessor`] and [`SiglipTokenizer`]. See the\n-    [`~SiglipProcessor.__call__`] and [`~SiglipProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`SiglipImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`SiglipTokenizer`]):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "2315eef2d016d7b233f032c341b0f6e6d80ff691",
            "filename": "src/transformers/models/siglip2/processing_siglip2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fprocessing_siglip2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,6 +16,7 @@\n \"\"\"\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+from ...utils import auto_docstring\n \n \n class Siglip2ProcessorKwargs(ProcessingKwargs, total=False):\n@@ -32,20 +33,8 @@ class Siglip2ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class Siglip2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Siglip2 processor which wraps a Siglip2 image processor and a Gemma tokenizer into a single processor.\n-\n-    [`Siglip2Processor`] offers all the functionalities of [`Siglip2ImageProcessor`] and [`GemmaTokenizerFast`]. See the\n-    [`~Siglip2Processor.__call__`] and [`~Siglip2Processor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`Siglip2ImageProcessor`]):\n-            The image processor is a required input.\n-        tokenizer ([`GemmaTokenizerFast`]):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     valid_processor_kwargs = Siglip2ProcessorKwargs\n \n     def __init__(self, image_processor, tokenizer):"
        },
        {
            "sha": "9bd2d8278a665779b80922580e6abd700e52f5bb",
            "filename": "src/transformers/models/smolvlm/processing_smolvlm.py",
            "status": "modified",
            "additions": 9,
            "deletions": 69,
            "changes": 78,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fprocessing_smolvlm.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -22,7 +22,7 @@\n from ...image_utils import ImageInput, make_nested_list_of_images\n from ...processing_utils import AllKwargsForChatTemplate, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, TextInput\n-from ...utils import is_num2words_available, is_vision_available, logging\n+from ...utils import auto_docstring, is_num2words_available, is_vision_available, logging\n from ...video_utils import VideoInput\n \n \n@@ -111,28 +111,8 @@ class SmolVLMProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class SmolVLMProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a SmolVLM processor which wraps a LLama tokenizer and SmolVLM image processor into a single processor.\n-\n-    [`SmolVLMProcessor`] offers all the functionalities of [`SmolVLMImageProcessor`] and [`SmolVLMTokenizerFast`]. See\n-    the docstring of [`~IdeficsProcessor.__call__`] and [`~IdeficsProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`SmolVLMImageProcessor`):\n-            An instance of [`SmolVLMImageProcessor`]. The image processor is a required input.\n-        tokenizer (`PreTrainedTokenizerBase`):\n-            An instance of [`PreTrainedTokenizerBase`]. This should correspond with the model's text model. The tokenizer is a required input.\n-        video_processor (`SmolVLMImageProcessor`):\n-            n instance of [`SmolVLMImageProcessor`]. The video processor is a required input.\n-        image_seq_len (`int`, *optional*, defaults to 169):\n-            The length of the image sequence i.e. the number of <image> tokens per image in the input.\n-            This parameter is used to build the string from the input prompt and image tokens and should match the\n-            value the model used. It is computed as: image_seq_len = int(((image_size // patch_size) ** 2) / (scale_factor**2))\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor,\n@@ -142,6 +122,12 @@ def __init__(\n         chat_template: Optional[str] = None,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        image_seq_len (`int`, *optional*, defaults to 169):\n+            The length of the image sequence i.e. the number of <image> tokens per image in the input.\n+            This parameter is used to build the string from the input prompt and image tokens and should match the\n+            value the model used. It is computed as: image_seq_len = int(((image_size // patch_size) ** 2) / (scale_factor**2))\n+        \"\"\"\n         self.fake_image_token = getattr(tokenizer, \"fake_image_token\", \"<fake_token_around_image>\")\n         self.image_token = getattr(tokenizer, \"image_token\", \"<image>\")\n         self.image_token_id = tokenizer.convert_tokens_to_ids(self.image_token)\n@@ -222,60 +208,14 @@ def expand_text_with_video_tokens(self, text, video_inputs):\n             prompt_strings.append(sample)\n         return prompt_strings\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Union[ImageInput, list[ImageInput], list[list[ImageInput]]] = None,\n         text: Union[TextInput, \"PreTokenizedInput\", list[TextInput], list[\"PreTokenizedInput\"]] = None,\n         videos: Optional[VideoInput] = None,\n         **kwargs: Unpack[SmolVLMProcessorKwargs],\n     ) -> BatchEncoding:\n-        \"\"\"\n-        Processes the input prompts and returns a BatchEncoding.\n-\n-        Example:\n-\n-        ```python\n-        >>> import requests\n-        >>> from transformers import SmolVLMProcessor\n-        >>> from transformers.image_utils import load_image\n-\n-        >>> processor = SmolVLMProcessor.from_pretrained(\"HuggingFaceM4/SmolVLM2-256M-Video-Instruct\")\n-        >>> processor.image_processor.do_image_splitting = False  # Force as False to simplify the example\n-\n-        >>> url1 = \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n-        >>> url2 = \"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\"\n-\n-        >>> image1, image2 = load_image(url1), load_image(url2)\n-        >>> images = [[image1], [image2]]\n-\n-        >>> text = [\n-        ...     \"<image>In this image, we see\",\n-        ...     \"bla bla bla<image>\",\n-        ... ]\n-        >>> outputs = processor(images=images, text=text, return_tensors=\"pt\", padding=True)\n-        >>> input_ids = outputs.input_ids\n-        >>> input_tokens = processor.tokenizer.batch_decode(input_ids)\n-        >>> print(input_tokens)\n-        ['<|begin_of_text|><fake_token_around_image><global-img>((<image>)*169)<fake_token_around_image> In this image, we see', '<|reserved_special_token_0|><|reserved_special_token_0|><|reserved_special_token_0|><|begin_of_text|>bla bla bla<fake_token_around_image><global-img>((<image>)*169)<fake_token_around_image>']\n-        ```\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`, *optional*):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. If is of type `list[ImageInput]`, it's assumed that this is for a single prompt i.e. of batch size 1.\n-            text (`Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]`, *optional*):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-                Wherever an image token, `<image>` is encountered it is expanded to\n-                `<fake_token_around_image>` + `<row_x_col_y>` + `<image>` * `image_seq_len` * <fake_token_around_image>`.\n-            videos (`list[PIL.Image.Image]`, `np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`, *optional*):\n-                The video or batch of videos to be prepared. Each video can be a list of PIL frames, NumPy array or PyTorch\n-                tensor. If is of type `list[VideoInput]`, it's assumed that this is for a single prompt i.e. of batch size 1.\n-            return_tensors (`Union[str, TensorType]`, *optional*):\n-                If set, will return tensors of a particular framework. See [`PreTrainedTokenizerFast.__call__`] for more\n-                information.\n-        \"\"\"\n         if text is None and images is None and videos is None:\n             raise ValueError(\"You must provide one of `text`, `images` or `videos'.\")\n "
        },
        {
            "sha": "87e03396d31aa2daa67e532cfbd604f263252286",
            "filename": "src/transformers/models/speech_to_text/processing_speech_to_text.py",
            "status": "modified",
            "additions": 3,
            "deletions": 22,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fprocessing_speech_to_text.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -18,35 +18,16 @@\n import warnings\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class Speech2TextProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Speech2Text processor which wraps a Speech2Text feature extractor and a Speech2Text tokenizer into a\n-    single processor.\n-\n-    [`Speech2TextProcessor`] offers all the functionalities of [`Speech2TextFeatureExtractor`] and\n-    [`Speech2TextTokenizer`]. See the [`~Speech2TextProcessor.__call__`] and [`~Speech2TextProcessor.decode`] for more\n-    information.\n-\n-    Args:\n-        feature_extractor (`Speech2TextFeatureExtractor`):\n-            An instance of [`Speech2TextFeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer (`Speech2TextTokenizer`):\n-            An instance of [`Speech2TextTokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n+    @auto_docstring\n     def __call__(self, *args, **kwargs):\n-        \"\"\"\n-        When used in normal mode, this method forwards all its arguments to Speech2TextFeatureExtractor's\n-        [`~Speech2TextFeatureExtractor.__call__`] and returns its output. If used in the context\n-        [`~Speech2TextProcessor.as_target_processor`] this method forwards all its arguments to Speech2TextTokenizer's\n-        [`~Speech2TextTokenizer.__call__`]. Please refer to the docstring of the above two methods for more\n-        information.\n-        \"\"\"\n         if \"raw_speech\" in kwargs:\n             warnings.warn(\"Using `raw_speech` as a keyword argument is deprecated. Use `audio` instead.\")\n             audio = kwargs.pop(\"raw_speech\")"
        },
        {
            "sha": "0e0ce789c5b2a3574c614c947e633fdb18892993",
            "filename": "src/transformers/models/speecht5/processing_speecht5.py",
            "status": "modified",
            "additions": 3,
            "deletions": 36,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fprocessing_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fprocessing_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fprocessing_speecht5.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -14,49 +14,16 @@\n \"\"\"Speech processor class for SpeechT5.\"\"\"\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class SpeechT5Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a SpeechT5 processor which wraps a feature extractor and a tokenizer into a single processor.\n-\n-    [`SpeechT5Processor`] offers all the functionalities of [`SpeechT5FeatureExtractor`] and [`SpeechT5Tokenizer`]. See\n-    the docstring of [`~SpeechT5Processor.__call__`] and [`~SpeechT5Processor.decode`] for more information.\n-\n-    Args:\n-        feature_extractor (`SpeechT5FeatureExtractor`):\n-            An instance of [`SpeechT5FeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer (`SpeechT5Tokenizer`):\n-            An instance of [`SpeechT5Tokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n+    @auto_docstring\n     def __call__(self, *args, **kwargs):\n-        \"\"\"\n-        Processes audio and text input, as well as audio and text targets.\n-\n-        You can process audio by using the argument `audio`, or process audio targets by using the argument\n-        `audio_target`. This forwards the arguments to SpeechT5FeatureExtractor's\n-        [`~SpeechT5FeatureExtractor.__call__`].\n-\n-        You can process text by using the argument `text`, or process text labels by using the argument `text_target`.\n-        This forwards the arguments to SpeechT5Tokenizer's [`~SpeechT5Tokenizer.__call__`].\n-\n-        Valid input combinations are:\n-\n-        - `text` only\n-        - `audio` only\n-        - `text_target` only\n-        - `audio_target` only\n-        - `text` and `audio_target`\n-        - `audio` and `audio_target`\n-        - `text` and `text_target`\n-        - `audio` and `text_target`\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n         audio = kwargs.pop(\"audio\", None)\n         text = kwargs.pop(\"text\", None)\n         text_target = kwargs.pop(\"text_target\", None)"
        },
        {
            "sha": "cbe2c51d26aeea7ef5b2f51de5701b96c17e26bc",
            "filename": "src/transformers/models/trocr/processing_trocr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 20,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fprocessing_trocr.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -21,42 +21,25 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class TrOCRProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {}\n \n \n+@auto_docstring\n class TrOCRProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a TrOCR processor which wraps a vision image processor and a TrOCR tokenizer into a single processor.\n-\n-    [`TrOCRProcessor`] offers all the functionalities of [`ViTImageProcessor`/`DeiTImageProcessor`] and\n-    [`RobertaTokenizer`/`XLMRobertaTokenizer`]. See the [`~TrOCRProcessor.__call__`] and [`~TrOCRProcessor.decode`] for\n-    more information.\n-\n-    Args:\n-        image_processor ([`ViTImageProcessor`/`DeiTImageProcessor`], *optional*):\n-            An instance of [`ViTImageProcessor`/`DeiTImageProcessor`]. The image processor is a required input.\n-        tokenizer ([`RobertaTokenizer`/`XLMRobertaTokenizer`], *optional*):\n-            An instance of [`RobertaTokenizer`/`XLMRobertaTokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         **kwargs: Unpack[TrOCRProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        When used in normal mode, this method forwards all its arguments to AutoImageProcessor's\n-        [`~AutoImageProcessor.__call__`] and returns its output. If used in the context\n-        [`~TrOCRProcessor.as_target_processor`] this method forwards all its arguments to TrOCRTokenizer's\n-        [`~TrOCRTokenizer.__call__`]. Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You need to specify either an `images` or `text` input to process.\")\n "
        },
        {
            "sha": "b72f6be48c02d5a0766437c6fc8745bede3e19f6",
            "filename": "src/transformers/models/tvp/processing_tvp.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fprocessing_tvp.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,6 +16,7 @@\n \"\"\"\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+from ...utils import auto_docstring\n \n \n class TvpProcessorKwargs(ProcessingKwargs, total=False):\n@@ -29,20 +30,8 @@ class TvpProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class TvpProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs an TVP processor which wraps a TVP image processor and a Bert tokenizer into a single processor.\n-\n-    [`TvpProcessor`] offers all the functionalities of [`TvpImageProcessor`] and [`BertTokenizerFast`]. See the\n-    [`~TvpProcessor.__call__`] and [`~TvpProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`TvpImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`BertTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n         self.video_processor = image_processor"
        },
        {
            "sha": "e4599ccde276fdb58840b97ab0702eb76f50619b",
            "filename": "src/transformers/models/udop/processing_udop.py",
            "status": "modified",
            "additions": 3,
            "deletions": 19,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -23,6 +23,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import ProcessingKwargs, ProcessorMixin, TextKwargs, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n logger = logging.get_logger(__name__)\n@@ -50,6 +51,7 @@ class UdopProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class UdopProcessor(ProcessorMixin):\n     r\"\"\"\n     Constructs a UDOP processor which combines a LayoutLMv3 image processor and a UDOP tokenizer into a single processor.\n@@ -64,36 +66,18 @@ class UdopProcessor(ProcessorMixin):\n \n     Additionally, it also supports passing `text_target` and `text_pair_target` to the tokenizer, which can be used to\n     prepare labels for language modeling tasks.\n-\n-    Args:\n-        image_processor (`LayoutLMv3ImageProcessor`):\n-            An instance of [`LayoutLMv3ImageProcessor`]. The image processor is a required input.\n-        tokenizer (`UdopTokenizer`):\n-            An instance of [`UdopTokenizer`]. The tokenizer is a required input.\n     \"\"\"\n \n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n         **kwargs: Unpack[UdopProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        This method first forwards the `images` argument to [`~UdopImageProcessor.__call__`]. In case\n-        [`UdopImageProcessor`] was initialized with `apply_ocr` set to `True`, it passes the obtained words and\n-        bounding boxes along with the additional arguments to [`~UdopTokenizer.__call__`] and returns the output,\n-        together with the prepared `pixel_values`. In case [`UdopImageProcessor`] was initialized with `apply_ocr` set\n-        to `False`, it passes the words (`text`/``text_pair`) and `boxes` specified by the user along with the\n-        additional arguments to [`~UdopTokenizer.__call__`] and returns the output, together with the prepared\n-        `pixel_values`.\n-\n-        Alternatively, one can pass `text_target` and `text_pair_target` to prepare the targets of UDOP.\n-\n-        Please refer to the docstring of the above two methods for more information.\n-        \"\"\"\n         # verify input\n         output_kwargs = self._merge_kwargs(\n             UdopProcessorKwargs,"
        },
        {
            "sha": "be47da465ba445aeb500bf6257f26ae68dc01b4b",
            "filename": "src/transformers/models/video_llama_3/modular_video_llama_3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fmodular_video_llama_3.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -1094,20 +1094,6 @@ class VideoLlama3ProcessorKwargs(Qwen2VLProcessorKwargs):\n \n \n class VideoLlama3Processor(Qwen2VLProcessor):\n-    r\"\"\"\n-    Constructs a VideoLLaMA3 processor which wraps a VideoLLaMA3 image processor and a Qwen2 tokenizer into a single processor.\n-    [`VideoLlama3Processor`] offers all the functionalities of [`VideoLlama3ImageProcessor`] and [`Qwen2Tokenizer`]. See the\n-    [`~VideoLlama3Processor.__call__`] and [`~VideoLlama3Processor.decode`] for more information.\n-    Args:\n-        image_processor ([`VideoLlama3ImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`Qwen2Tokenizer`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`VideoLlama3VideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-    \"\"\"\n-\n     def __call__(\n         self,\n         images: ImageInput = None,"
        },
        {
            "sha": "14ac00bf9c936301c5b5b400cd515017c2b0d947",
            "filename": "src/transformers/models/video_llama_3/processing_video_llama_3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 37,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fprocessing_video_llama_3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fprocessing_video_llama_3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llama_3%2Fprocessing_video_llama_3.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -24,7 +24,7 @@\n from ...image_utils import ImageInput\n from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n-from ...utils import logging\n+from ...utils import auto_docstring, logging\n from ...video_utils import VideoInput\n \n \n@@ -41,21 +41,8 @@ class VideoLlama3ProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class VideoLlama3Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a VideoLLaMA3 processor which wraps a VideoLLaMA3 image processor and a Qwen2 tokenizer into a single processor.\n-    [`VideoLlama3Processor`] offers all the functionalities of [`VideoLlama3ImageProcessor`] and [`Qwen2Tokenizer`]. See the\n-    [`~VideoLlama3Processor.__call__`] and [`~VideoLlama3Processor.decode`] for more information.\n-    Args:\n-        image_processor ([`VideoLlama3ImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`Qwen2Tokenizer`], *optional*):\n-            The tokenizer is a required input.\n-        video_processor ([`VideoLlama3VideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, video_processor=None, chat_template=None, **kwargs):\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n@@ -71,35 +58,15 @@ def __init__(self, image_processor=None, tokenizer=None, video_processor=None, c\n         )\n         super().__init__(image_processor, tokenizer, video_processor, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         images: ImageInput = None,\n         text: TextInput | PreTokenizedInput | list[TextInput] | list[PreTokenizedInput] = None,\n         videos: VideoInput = None,\n         **kwargs: Unpack[VideoLlama3ProcessorKwargs],\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to Qwen2TokenizerFast's [`~Qwen2TokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the vision inputs, this method forwards the `vision_infos` and `kwargs` arguments to\n-        VideoLlama3ImageProcessor's [`~VideoLlama3ImageProcessor.__call__`] if `vision_infos` is not `None`.\n-\n-        Args:\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. Both channels-first and channels-last formats are supported.\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of videos to be prepared. Each video can be a 4D NumPy array or PyTorch\n-                tensor, or a nested list of 3D frames. Both channels-first and channels-last formats are supported.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n-\n+        r\"\"\"\n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n "
        },
        {
            "sha": "6c40620bcff88e933e737e1a36eb23c6155094cc",
            "filename": "src/transformers/models/video_llava/processing_video_llava.py",
            "status": "modified",
            "additions": 31,
            "deletions": 68,
            "changes": 99,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fprocessing_video_llava.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -23,42 +23,14 @@\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n from ...processing_utils import ProcessorMixin\n from ...tokenization_utils_base import PaddingStrategy, PreTokenizedInput, TextInput, TruncationStrategy\n-from ...utils import TensorType, logging\n+from ...utils import TensorType, auto_docstring, logging\n \n \n logger = logging.get_logger(__name__)\n \n \n+@auto_docstring\n class VideoLlavaProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a VideoLlava processor which wraps a VideoLlava image processor and a Llava tokenizer into a single processor.\n-\n-    [`VideoLlavaProcessor`] offers all the functionalities of [`VideoLlavaImageProcessor`] and [`LlamaTokenizerFast`]. See the\n-    [`~VideoLlavaProcessor.__call__`] and [`~VideoLlavaProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`VideoLlavaImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        video_processor ([`VideoLlavaVideoProcessor`], *optional*):\n-            The video processor is a required input.\n-        tokenizer ([`LlamaTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-        patch_size (`int`, *optional*, defaults to 14):\n-            Patch size from the vision tower.\n-        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n-            The feature selection strategy used to select the vision feature from the vision backbone.\n-            Should be same as in model's config\n-        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n-            Special token used to denote image location.\n-        video_token (`str`, *optional*, defaults to `\"<video>\"`):\n-            Special token used to denote video location.\n-        chat_template (`str`, *optional*): A Jinja template which will be used to convert lists of messages\n-            in a chat into a tokenizable string.\n-        num_additional_image_tokens (`int`, *optional*, defaults to 1):\n-            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n-            extra tokens appended, no need to set this arg.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         image_processor=None,\n@@ -72,6 +44,20 @@ def __init__(\n         num_additional_image_tokens=1,\n         **kwargs,\n     ):\n+        r\"\"\"\n+        patch_size (`int`, *optional*, defaults to 14):\n+            Patch size from the vision tower.\n+        vision_feature_select_strategy (`str`, *optional*, defaults to `\"default\"`):\n+            The feature selection strategy used to select the vision feature from the vision backbone.\n+            Should be same as in model's config\n+        image_token (`str`, *optional*, defaults to `\"<image>\"`):\n+            Special token used to denote image location.\n+        video_token (`str`, *optional*, defaults to `\"<video>\"`):\n+            Special token used to denote video location.\n+        num_additional_image_tokens (`int`, *optional*, defaults to 1):\n+            Number of additional tokens added to the image embeddings, such as CLS (+1). If the backbone has no CLS or other\n+            extra tokens appended, no need to set this arg.\n+        \"\"\"\n         self.patch_size = patch_size\n         self.num_additional_image_tokens = num_additional_image_tokens\n         self.vision_feature_select_strategy = vision_feature_select_strategy\n@@ -81,6 +67,7 @@ def __init__(\n         self.video_token_id = tokenizer.convert_tokens_to_ids(self.video_token)\n         super().__init__(image_processor, video_processor, tokenizer, chat_template=chat_template)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n@@ -91,44 +78,20 @@ def __call__(\n         max_length: Optional[int] = None,\n         return_tensors: Optional[Union[str, TensorType]] = TensorType.PYTORCH,\n     ) -> BatchFeature:\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and image(s). This method forwards the `text`\n-        and `kwargs` arguments to LlamaTokenizerFast's [`~LlamaTokenizerFast.__call__`] if `text` is not `None` to encode\n-        the text. To prepare the image(s), this method forwards the `images` and `kwargs` arguments to\n-        VideoLlavaImageProcessor's [`~VideoLlavaImageProcessor.__call__`] if `images` is not `None`. Please refer to the docstring\n-        of the above two methods for more information.\n-\n-        Args:\n-            text (`TextInput`, `PreTokenizedInput`, `list[TextInput]`, `list[PreTokenizedInput]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            images (`PIL.Image.Image`, `np.ndarray`, `torch.Tensor`, `list[PIL.Image.Image]`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The image or batch of images to be prepared. Each image can be a PIL image, NumPy array or PyTorch\n-                tensor. In case of a NumPy array/PyTorch tensor, each image should be of shape (C, H, W), where C is a\n-                number of channels, H and W are image height and width.\n-            videos (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                Video frames to preprocess. Expects a single or batch of video frames in NumPy array or PyTorch\n-                tensor. Each video should be of shape (T, C, H, W), where T is number of frames, C is\n-                number of channels, H and W are image height and width.\n-            padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n-                Select a strategy to pad the returned sequences (according to the model's padding side and padding\n-                index) among:\n-                - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n-                  sequence if provided).\n-                - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n-                  acceptable input length for the model if that argument is not provided.\n-                - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n-                  lengths).\n-            max_length (`int`, *optional*):\n-                Maximum length of the returned list and optionally padding length (see above).\n-            truncation (`bool`, *optional*):\n-                Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-\n-                - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                - `'np'`: Return NumPy `np.ndarray` objects.\n+        r\"\"\"\n+        padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n+            Select a strategy to pad the returned sequences (according to the model's padding side and padding\n+            index) among:\n+            - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n+                sequence if provided).\n+            - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n+                acceptable input length for the model if that argument is not provided.\n+            - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n+                lengths).\n+        truncation (`bool`, *optional*):\n+            Activates truncation to cut input sequences longer than `max_length` to `max_length`.\n+        max_length (`int`, *optional*):\n+            Maximum length of the returned list and optionally padding length (see above).\n \n         Returns:\n             [`BatchFeature`]: A [`BatchFeature`] with the following fields:"
        },
        {
            "sha": "be47b2e6ee754c9b00660040bb7407f69b32483e",
            "filename": "src/transformers/models/vilt/processing_vilt.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvilt%2Fprocessing_vilt.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,6 +16,7 @@\n \"\"\"\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+from ...utils import auto_docstring\n \n \n class ViltProcessorKwargs(ProcessingKwargs, total=False):\n@@ -33,20 +34,8 @@ class ViltProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class ViltProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a ViLT processor which wraps a BERT tokenizer and ViLT image processor into a single processor.\n-\n-    [`ViltProcessor`] offers all the functionalities of [`ViltImageProcessor`] and [`BertTokenizerFast`]. See the\n-    docstring of [`~ViltProcessor.__call__`] and [`~ViltProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor (`ViltImageProcessor`, *optional*):\n-            An instance of [`ViltImageProcessor`]. The image processor is a required input.\n-        tokenizer (`BertTokenizerFast`, *optional*):\n-            An instance of ['BertTokenizerFast`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     valid_processor_kwargs = ViltProcessorKwargs\n \n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):"
        },
        {
            "sha": "1a90b073dee67b7adeb290f489577ee375a1ed9e",
            "filename": "src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 2,
            "deletions": 15,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,28 +16,15 @@\n \"\"\"\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin\n+from ...utils import auto_docstring\n \n \n class VisionTextDualEncoderProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {}\n \n \n+@auto_docstring\n class VisionTextDualEncoderProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a VisionTextDualEncoder processor which wraps an image processor and a tokenizer into a single\n-    processor.\n-\n-    [`VisionTextDualEncoderProcessor`] offers all the functionalities of [`AutoImageProcessor`] and [`AutoTokenizer`].\n-    See the [`~VisionTextDualEncoderProcessor.__call__`] and [`~VisionTextDualEncoderProcessor.decode`] for more\n-    information.\n-\n-    Args:\n-        image_processor ([`AutoImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`PreTrainedTokenizer`], *optional*):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n "
        },
        {
            "sha": "1cdc5eb399860e921dc41b91f8834dadedad43a2",
            "filename": "src/transformers/models/voxtral/processing_voxtral.py",
            "status": "modified",
            "additions": 18,
            "deletions": 39,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fprocessing_voxtral.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -15,7 +15,7 @@\n import io\n from typing import Optional, Union\n \n-from ...utils import is_mistral_common_available, is_soundfile_available, is_torch_available, logging\n+from ...utils import auto_docstring, is_mistral_common_available, is_soundfile_available, is_torch_available, logging\n \n \n if is_torch_available():\n@@ -37,10 +37,16 @@\n \n \n class VoxtralAudioKwargs(AudioKwargs, total=False):\n+    \"\"\"\n+    max_source_positions (`int`, *optional*, defaults to `3000`):\n+        Maximum number of positions per chunk when splitting mel spectrogram features along the time dimension.\n+    \"\"\"\n+\n     max_source_positions: Optional[int]\n \n \n class VoxtralProcessorKwargs(ProcessingKwargs, total=False):\n+    audio_kwargs: VoxtralAudioKwargs\n     _defaults = {\n         \"text_kwargs\": {\n             \"padding\": True,\n@@ -60,19 +66,8 @@ class VoxtralProcessorKwargs(ProcessingKwargs, total=False):\n     }\n \n \n+@auto_docstring\n class VoxtralProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Voxtral processor which wraps [`WhisperFeatureExtractor`] and\n-    [`MistralCommonBackend`] into a single processor that inherits both the audio feature extraction and\n-    tokenizer functionalities.\n-\n-    Args:\n-        feature_extractor ([`WhisperFeatureExtractor`]):\n-            The feature extractor is a required input.\n-        tokenizer ([`MistralCommonBackend`]):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         feature_extractor,\n@@ -225,37 +220,21 @@ def apply_chat_template(\n \n         return encoded_instruct_inputs\n \n+    @auto_docstring(\n+        custom_intro=r\"\"\"\n+    Method to prepare text to be fed as input to the model. This method forwards the `text`\n+    arguments to MistralCommonBackend's [`~MistralCommonBackend.__call__`] to encode\n+    the text. Please refer to the docstring of the above methods for more information.\n+    This method does not support audio. To prepare the audio, please use:\n+    1. `apply_chat_template` [`~VoxtralProcessor.apply_chat_template`] method.\n+    2. `apply_transcription_request` [`~VoxtralProcessor.apply_transcription_request`] method.\n+    \"\"\"\n+    )\n     def __call__(\n         self,\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]],\n         **kwargs: Unpack[VoxtralProcessorKwargs],\n     ):\n-        r\"\"\"\n-        Method to prepare text to be fed as input to the model. This method forwards the `text`\n-        arguments to MistralCommonBackend's [`~MistralCommonBackend.__call__`] to encode\n-        the text. Please refer to the docstring of the above methods for more information.\n-        This methods does not support audio. To prepare the audio, please use:\n-        1. `apply_chat_template` [`~VoxtralProcessor.apply_chat_template`] method.\n-        2. `apply_transcription_request` [`~VoxtralProcessor.apply_transcription_request`] method.\n-\n-        Args:\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n-            return_tensors (`str` or [`~utils.TensorType`], *optional*):\n-                If set, will return tensors of a particular framework. Acceptable values are:\n-                    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n-                    - `'np'`: Return NumPy `np.ndarray` objects.\n-        Returns:\n-            [`BatchFeature`]: A [`BatchFeature`] with the following fields:\n-\n-            - **input_ids** -- List of token ids to be fed to a model. Returned when `text` is not `None`.\n-            - **input_features** -- List of audio values to be fed to a model. Returned when `audio` is not `None`.\n-            - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n-              `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names` and if `text` is not\n-              `None`).\n-        \"\"\"\n         if isinstance(text, str):\n             text = [text]\n "
        },
        {
            "sha": "1997b7d9260901229c983c0785727f146e67608a",
            "filename": "src/transformers/models/wav2vec2/processing_wav2vec2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 25,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2%2Fprocessing_wav2vec2.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -20,47 +20,26 @@\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AudioInput, PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class Wav2Vec2ProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {}\n \n \n+@auto_docstring\n class Wav2Vec2Processor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor and a Wav2Vec2 CTC tokenizer into a single\n-    processor.\n-\n-    [`Wav2Vec2Processor`] offers all the functionalities of [`Wav2Vec2FeatureExtractor`] and [`PreTrainedTokenizer`].\n-    See the docstring of [`~Wav2Vec2Processor.__call__`] and [`~Wav2Vec2Processor.decode`] for more information.\n-\n-    Args:\n-        feature_extractor (`Wav2Vec2FeatureExtractor`):\n-            An instance of [`Wav2Vec2FeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer ([`PreTrainedTokenizer`]):\n-            An instance of [`PreTrainedTokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         audio: Optional[AudioInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         **kwargs: Unpack[Wav2Vec2ProcessorKwargs],\n     ):\n-        \"\"\"\n-        This method forwards all arguments to [`Wav2Vec2FeatureExtractor.__call__`] and/or\n-        [`PreTrainedTokenizer.__call__`] depending on the input modality and returns their outputs. If both modalities are passed, [`Wav2Vec2FeatureExtractor.__call__`] and [`PreTrainedTokenizer.__call__`] are called.\n-\n-        Args:\n-            audio (`np.ndarray`, `torch.Tensor`, `List[np.ndarray]`, `List[torch.Tensor]`, *optional*):\n-                An audio input is passed to [`Wav2Vec2FeatureExtractor.__call__`].\n-            text (`str`, `List[str]`, *optional*):\n-                A text input is passed to [`PreTrainedTokenizer.__call__`].\n-\n-\n+        r\"\"\"\n         Returns:\n             This method returns the results of each `call` method. If both are used, the output is a dictionary containing the results of both.\n         \"\"\""
        },
        {
            "sha": "6306e24f3d96b1f005c75455712c062dcb8c2469",
            "filename": "src/transformers/models/wav2vec2_bert/processing_wav2vec2_bert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 30,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_bert%2Fprocessing_wav2vec2_bert.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -19,52 +19,26 @@\n \n from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import AudioInput, PreTokenizedInput, TextInput\n+from ...utils import auto_docstring\n \n \n class Wav2Vec2BertProcessorKwargs(ProcessingKwargs, total=False):\n     _defaults = {}\n \n \n+@auto_docstring\n class Wav2Vec2BertProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Wav2Vec2-BERT processor which wraps a Wav2Vec2-BERT feature extractor and a Wav2Vec2 CTC tokenizer into a single\n-    processor.\n-\n-    [`Wav2Vec2Processor`] offers all the functionalities of [`SeamlessM4TFeatureExtractor`] and [`PreTrainedTokenizer`].\n-    See the docstring of [`~Wav2Vec2Processor.__call__`] and [`~Wav2Vec2Processor.decode`] for more information.\n-\n-    Args:\n-        feature_extractor (`SeamlessM4TFeatureExtractor`):\n-            An instance of [`SeamlessM4TFeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer ([`PreTrainedTokenizer`]):\n-            An instance of [`PreTrainedTokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n+    @auto_docstring\n     def __call__(\n         self,\n         audio: Optional[AudioInput] = None,\n         text: Optional[Union[str, list[str], TextInput, PreTokenizedInput]] = None,\n         **kwargs: Unpack[Wav2Vec2BertProcessorKwargs],\n     ):\n-        \"\"\"\n-        Main method to prepare for the model one or several sequences(s) and audio(s). This method forwards the `audio`\n-        and `kwargs` arguments to SeamlessM4TFeatureExtractor's [`~SeamlessM4TFeatureExtractor.__call__`] if `audio` is not\n-        `None` to pre-process the audio. To prepare the target sequences(s), this method forwards the `text` and `kwargs` arguments to\n-        PreTrainedTokenizer's [`~PreTrainedTokenizer.__call__`] if `text` is not `None`. Please refer to the docstring of the above two methods for more information.\n-\n-        Args:\n-            audio (`np.ndarray`, `torch.Tensor`, `list[np.ndarray]`, `list[torch.Tensor]`):\n-                The audio or batch of audios to be prepared. Each audio can be NumPy array or PyTorch tensor. In case\n-                of a NumPy array/PyTorch tensor, each audio should be of shape (C, T), where C is a number of channels,\n-                and T the sample length of the audio.\n-\n-            text (`str`, `list[str]`, `list[list[str]]`):\n-                The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n-                (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n-                `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n+        r\"\"\"\n         Returns:\n             [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n             - **input_features** -- Audio input features to be fed to a model. Returned when `audio` is not `None`."
        },
        {
            "sha": "dee6c97991b2e5e78a53e8d2eea98dab06f17447",
            "filename": "src/transformers/models/wav2vec2_with_lm/processing_wav2vec2_with_lm.py",
            "status": "modified",
            "additions": 7,
            "deletions": 21,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwav2vec2_with_lm%2Fprocessing_wav2vec2_with_lm.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -26,7 +26,7 @@\n import numpy as np\n \n from ...processing_utils import ProcessorMixin\n-from ...utils import ModelOutput, logging, requires_backends\n+from ...utils import ModelOutput, auto_docstring, logging, requires_backends\n \n \n logger = logging.get_logger(__name__)\n@@ -65,26 +65,18 @@ class Wav2Vec2DecoderWithLMOutput(ModelOutput):\n     word_offsets: Union[list[list[ListOfDict]], list[ListOfDict], ListOfDict] = None\n \n \n+@auto_docstring\n class Wav2Vec2ProcessorWithLM(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Wav2Vec2 processor which wraps a Wav2Vec2 feature extractor, a Wav2Vec2 CTC tokenizer and a decoder\n-    with language model support into a single processor for language model boosted speech recognition decoding.\n-\n-    Args:\n-        feature_extractor ([`Wav2Vec2FeatureExtractor`] or [`SeamlessM4TFeatureExtractor`]):\n-            An instance of [`Wav2Vec2FeatureExtractor`] or [`SeamlessM4TFeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer ([`Wav2Vec2CTCTokenizer`]):\n-            An instance of [`Wav2Vec2CTCTokenizer`]. The tokenizer is a required input.\n-        decoder (`pyctcdecode.BeamSearchDecoderCTC`):\n-            An instance of [`pyctcdecode.BeamSearchDecoderCTC`]. The decoder is a required input.\n-    \"\"\"\n-\n     def __init__(\n         self,\n         feature_extractor: \"FeatureExtractionMixin\",\n         tokenizer: \"PreTrainedTokenizerBase\",\n         decoder: \"BeamSearchDecoderCTC\",\n     ):\n+        r\"\"\"\n+        decoder (`pyctcdecode.BeamSearchDecoderCTC`):\n+            An instance of [`pyctcdecode.BeamSearchDecoderCTC`]. The decoder is a required input.\n+        \"\"\"\n         from pyctcdecode import BeamSearchDecoderCTC\n \n         super().__init__(feature_extractor, tokenizer)\n@@ -213,14 +205,8 @@ def get_missing_alphabet_tokens(decoder, tokenizer):\n \n         return missing_tokens\n \n+    @auto_docstring\n     def __call__(self, *args, **kwargs):\n-        \"\"\"\n-        When used in normal mode, this method forwards all its arguments to the feature extractor's\n-        [`~FeatureExtractionMixin.__call__`] and returns its output. If used in the context\n-        [`~Wav2Vec2ProcessorWithLM.as_target_processor`] this method forwards all its arguments to\n-        Wav2Vec2CTCTokenizer's [`~Wav2Vec2CTCTokenizer.__call__`]. Please refer to the docstring of the above two\n-        methods for more information.\n-        \"\"\"\n         if \"raw_speech\" in kwargs:\n             warnings.warn(\"Using `raw_speech` as a keyword argument is deprecated. Use `audio` instead.\")\n             audio = kwargs.pop(\"raw_speech\")"
        },
        {
            "sha": "1d1b33f3c1552db9dd9bdcf9dee463e879009116",
            "filename": "src/transformers/models/whisper/processing_whisper.py",
            "status": "modified",
            "additions": 3,
            "deletions": 19,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fprocessing_whisper.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,35 +16,19 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class WhisperProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs a Whisper processor which wraps a Whisper feature extractor and a Whisper tokenizer into a single\n-    processor.\n-\n-    [`WhisperProcessor`] offers all the functionalities of [`WhisperFeatureExtractor`] and [`WhisperTokenizer`]. See\n-    the [`~WhisperProcessor.__call__`] and [`~WhisperProcessor.decode`] for more information.\n-\n-    Args:\n-        feature_extractor (`WhisperFeatureExtractor`):\n-            An instance of [`WhisperFeatureExtractor`]. The feature extractor is a required input.\n-        tokenizer (`WhisperTokenizer`):\n-            An instance of [`WhisperTokenizer`]. The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, feature_extractor, tokenizer):\n         super().__init__(feature_extractor, tokenizer)\n \n     def get_decoder_prompt_ids(self, task=None, language=None, no_timestamps=True):\n         return self.tokenizer.get_decoder_prompt_ids(task=task, language=language, no_timestamps=no_timestamps)\n \n+    @auto_docstring\n     def __call__(self, *args, **kwargs):\n-        \"\"\"\n-        Forwards the `audio` argument to WhisperFeatureExtractor's [`~WhisperFeatureExtractor.__call__`] and the `text`\n-        argument to [`~WhisperTokenizer.__call__`]. Please refer to the docstring of the above two methods for more\n-        information.\n-        \"\"\"\n         audio = kwargs.pop(\"audio\", None)\n         sampling_rate = kwargs.pop(\"sampling_rate\", None)\n         text = kwargs.pop(\"text\", None)"
        },
        {
            "sha": "d6b9fcf327361e7c6e782f28657293e4c270f8fc",
            "filename": "src/transformers/models/x_clip/processing_x_clip.py",
            "status": "modified",
            "additions": 2,
            "deletions": 13,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fprocessing_x_clip.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -16,22 +16,11 @@\n \"\"\"\n \n from ...processing_utils import ProcessorMixin\n+from ...utils import auto_docstring\n \n \n+@auto_docstring\n class XCLIPProcessor(ProcessorMixin):\n-    r\"\"\"\n-    Constructs an X-CLIP processor which wraps a VideoMAE image processor and a CLIP tokenizer into a single processor.\n-\n-    [`XCLIPProcessor`] offers all the functionalities of [`CLIPImageProcessor`] and [`CLIPTokenizerFast`]. See the\n-    [`~XCLIPProcessor.__call__`] and [`~XCLIPProcessor.decode`] for more information.\n-\n-    Args:\n-        image_processor ([`CLIPImageProcessor`], *optional*):\n-            The image processor is a required input.\n-        tokenizer ([`CLIPTokenizerFast`], *optional*):\n-            The tokenizer is a required input.\n-    \"\"\"\n-\n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         super().__init__(image_processor, tokenizer)\n         self.video_processor = self.image_processor"
        },
        {
            "sha": "f84e92a207de86cdbf40aaa2e53cf19a5b790da4",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -373,10 +373,6 @@ def _init_weights(self, module):\n @auto_docstring\n class XGLMModel(XGLMPreTrainedModel):\n     def __init__(self, config: XGLMConfig):\n-        r\"\"\"\n-        embed_tokens (`nn.Embedding`, *optional*):\n-            output embeddings\n-        \"\"\"\n         super().__init__(config)\n         self.dropout = config.dropout\n         self.layerdrop = config.layerdrop"
        },
        {
            "sha": "c374ff76d9ac08306dc286eb9b983efa18d5e63b",
            "filename": "src/transformers/utils/auto_docstring.py",
            "status": "modified",
            "additions": 564,
            "deletions": 11,
            "changes": 575,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/src%2Ftransformers%2Futils%2Fauto_docstring.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fauto_docstring.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -45,6 +45,7 @@\n \n PLACEHOLDER_TO_AUTO_MODULE = {\n     \"image_processor_class\": (\"image_processing_auto\", \"IMAGE_PROCESSOR_MAPPING_NAMES\"),\n+    \"tokenizer_class\": (\"tokenization_auto\", \"TOKENIZER_MAPPING_NAMES\"),\n     \"video_processor_class\": (\"video_processing_auto\", \"VIDEO_PROCESSOR_MAPPING_NAMES\"),\n     \"feature_extractor_class\": (\"feature_extraction_auto\", \"FEATURE_EXTRACTOR_MAPPING_NAMES\"),\n     \"processor_class\": (\"processing_auto\", \"PROCESSOR_MAPPING_NAMES\"),\n@@ -53,10 +54,12 @@\n \n UNROLL_KWARGS_METHODS = {\n     \"preprocess\",\n+    \"__call__\",\n }\n \n UNROLL_KWARGS_CLASSES = {\n     \"ImageProcessorFast\",\n+    \"ProcessorMixin\",\n }\n \n HARDCODED_CONFIG_FOR_MODELS = {\n@@ -68,6 +71,7 @@\n     \"esmfold\": \"EsmConfig\",\n     \"parakeet\": \"ParakeetCTCConfig\",\n     \"lasr\": \"LasrCTCConfig\",\n+    \"wav2vec2-with-lm\": \"Wav2Vec2Config\",\n }\n \n _re_checkpoint = re.compile(r\"\\[(.+?)\\]\\((https://huggingface\\.co/.+?)\\)\")\n@@ -250,6 +254,270 @@ class ImageProcessorArgs:\n     }\n \n \n+class ProcessorArgs:\n+    # __init__ arguments\n+    image_processor = {\n+        \"description\": \"\"\"\n+    The image processor is a required input.\n+    \"\"\",\n+        \"type\": \"{image_processor_class}\",\n+    }\n+\n+    tokenizer = {\n+        \"description\": \"\"\"\n+    The tokenizer is a required input.\n+    \"\"\",\n+        \"type\": \"{tokenizer_class}\",\n+    }\n+\n+    video_processor = {\n+        \"description\": \"\"\"\n+    The video processor is a required input.\n+    \"\"\",\n+        \"type\": \"{video_processor_class}\",\n+    }\n+\n+    audio_processor = {\n+        \"description\": \"\"\"\n+    The audio processor is a required input.\n+    \"\"\",\n+        \"type\": \"{audio_processor_class}\",\n+    }\n+\n+    feature_extractor = {\n+        \"description\": \"\"\"\n+    The feature extractor is a required input.\n+    \"\"\",\n+        \"type\": \"{feature_extractor_class}\",\n+    }\n+\n+    chat_template = {\n+        \"description\": \"\"\"\n+    A Jinja template to convert lists of messages in a chat into a tokenizable string.\n+    \"\"\",\n+        \"type\": \"str\",\n+    }\n+\n+    # __call__ arguments\n+    text = {\n+        \"description\": \"\"\"\n+    The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n+    (pretokenized string). If you pass a pretokenized input, set `is_split_into_words=True` to avoid ambiguity with batched inputs.\n+    \"\"\",\n+    }\n+\n+    audio = {\n+        \"description\": \"\"\"\n+    The audio or batch of audios to be prepared. Each audio can be a NumPy array or PyTorch tensor.\n+    In case of a NumPy array/PyTorch tensor, each audio should be of shape (C, T), where C is a number of channels,\n+    and T is the sample length of the audio.\n+    \"\"\",\n+    }\n+\n+    audios = {\n+        \"description\": \"\"\"\n+    The audio or batch of audios to be prepared. Each audio can be a NumPy array or PyTorch tensor.\n+    In case of a NumPy array/PyTorch tensor, each audio should be of shape (C, T), where C is a number of channels,\n+    and T is the sample length of the audio.\n+    \"\"\",\n+    }\n+\n+    return_tensors = {\n+        \"description\": \"\"\"\n+    If set, will return tensors of a particular framework. Acceptable values are:\n+\n+    - `'pt'`: Return PyTorch `torch.Tensor` objects.\n+    - `'np'`: Return NumPy `np.ndarray` objects.\n+    \"\"\",\n+        \"shape\": None,\n+    }\n+\n+    # Standard tokenizer arguments\n+    add_special_tokens = {\n+        \"description\": \"\"\"\n+    Whether or not to add special tokens when encoding the sequences. This will use the underlying\n+    [`PretrainedTokenizerBase.build_inputs_with_special_tokens`] function, which defines which tokens are\n+    automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n+    automatically.\n+    \"\"\",\n+        \"type\": \"bool\",\n+    }\n+\n+    padding = {\n+        \"description\": \"\"\"\n+    Activates and controls padding. Accepts the following values:\n+\n+    - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n+      sequence is provided).\n+    - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n+      acceptable input length for the model if that argument is not provided.\n+    - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n+      lengths).\n+    \"\"\",\n+        \"type\": \"bool, str or [`~utils.PaddingStrategy`]\",\n+    }\n+\n+    truncation = {\n+        \"description\": \"\"\"\n+    Activates and controls truncation. Accepts the following values:\n+\n+    - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n+      to the maximum acceptable input length for the model if that argument is not provided. This will\n+      truncate token by token, removing a token from the longest sequence in the pair if a pair of\n+      sequences (or a batch of pairs) is provided.\n+    - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n+      maximum acceptable input length for the model if that argument is not provided. This will only\n+      truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n+    - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n+      maximum acceptable input length for the model if that argument is not provided. This will only\n+      truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n+    - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n+      greater than the model maximum admissible input size).\n+    \"\"\",\n+        \"type\": \"bool, str or [`~tokenization_utils_base.TruncationStrategy`]\",\n+    }\n+\n+    max_length = {\n+        \"description\": \"\"\"\n+    Controls the maximum length to use by one of the truncation/padding parameters.\n+\n+    If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n+    is required by one of the truncation/padding parameters. If the model has no specific maximum input\n+    length (like XLNet) truncation/padding to a maximum length will be deactivated.\n+    \"\"\",\n+        \"type\": \"int\",\n+    }\n+\n+    stride = {\n+        \"description\": \"\"\"\n+    If set to a number along with `max_length`, the overflowing tokens returned when\n+    `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n+    returned to provide some overlap between truncated and overflowing sequences. The value of this\n+    argument defines the number of overlapping tokens.\n+    \"\"\",\n+        \"type\": \"int\",\n+    }\n+\n+    pad_to_multiple_of = {\n+        \"description\": \"\"\"\n+    If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n+    This is especially useful to enable using Tensor Cores on NVIDIA hardware with compute capability\n+    `>= 7.5` (Volta).\n+    \"\"\",\n+        \"type\": \"int\",\n+    }\n+\n+    return_token_type_ids = {\n+        \"description\": \"\"\"\n+    Whether to return token type IDs. If left to the default, will return the token type IDs according to\n+    the specific tokenizer's default, defined by the `return_outputs` attribute.\n+\n+    [What are token type IDs?](../glossary#token-type-ids)\n+    \"\"\",\n+        \"type\": \"bool\",\n+    }\n+\n+    return_attention_mask = {\n+        \"description\": \"\"\"\n+    Whether to return the attention mask. If left to the default, will return the attention mask according\n+    to the specific tokenizer's default, defined by the `return_outputs` attribute.\n+\n+    [What are attention masks?](../glossary#attention-mask)\n+    \"\"\",\n+        \"type\": \"bool\",\n+    }\n+\n+    return_overflowing_tokens = {\n+        \"description\": \"\"\"\n+    Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n+    of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n+    of returning overflowing tokens.\n+    \"\"\",\n+        \"type\": \"bool\",\n+    }\n+\n+    return_special_tokens_mask = {\n+        \"description\": \"\"\"\n+    Whether or not to return special tokens mask information.\n+    \"\"\",\n+        \"type\": \"bool\",\n+    }\n+\n+    return_offsets_mapping = {\n+        \"description\": \"\"\"\n+    Whether or not to return `(char_start, char_end)` for each token.\n+\n+    This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n+    Python's tokenizer, this method will raise `NotImplementedError`.\n+    \"\"\",\n+        \"type\": \"bool\",\n+    }\n+\n+    return_length = {\n+        \"description\": \"\"\"\n+    Whether or not to return the lengths of the encoded inputs.\n+    \"\"\",\n+        \"type\": \"bool\",\n+    }\n+\n+    verbose = {\n+        \"description\": \"\"\"\n+    Whether or not to print more information and warnings.\n+    \"\"\",\n+        \"type\": \"bool\",\n+    }\n+\n+    text_pair = {\n+        \"description\": \"\"\"\n+    Optional second sequence to be encoded. This can be a string, a list of strings (tokenized string using\n+    the `tokenize` method) or a list of integers (tokenized string ids using the `convert_tokens_to_ids`\n+    method).\n+    \"\"\",\n+        \"type\": \"str, list[str] or list[int]\",\n+    }\n+\n+    text_target = {\n+        \"description\": \"\"\"\n+    The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n+    list of strings (pretokenized string). If you pass pretokenized input, set `is_split_into_words=True`\n+    to avoid ambiguity with batched inputs.\n+    \"\"\",\n+        \"type\": \"str, list[str] or list[list[str]]\",\n+    }\n+\n+    text_pair_target = {\n+        \"description\": \"\"\"\n+    The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n+    list of strings (pretokenized string). If you pass pretokenized input, set `is_split_into_words=True`\n+    to avoid ambiguity with batched inputs.\n+    \"\"\",\n+        \"type\": \"str, list[str] or list[list[str]]\",\n+    }\n+\n+    is_split_into_words = {\n+        \"description\": \"\"\"\n+    Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n+    tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n+    which it will tokenize. This is useful for NER or token classification.\n+    \"\"\",\n+        \"type\": \"bool\",\n+    }\n+\n+    boxes = {\n+        \"description\": \"\"\"\n+    Word-level bounding boxes. Each bounding box should be normalized to be on a 0-1000 scale.\n+    \"\"\",\n+        \"type\": \"list[list[int]] or list[list[list[int]]]\",\n+    }\n+\n+    word_labels = {\n+        \"description\": \"\"\"\n+    Word-level integer labels (for token classification tasks such as FUNSD, CORD).\n+    \"\"\",\n+        \"type\": \"list[int] or list[list[int]]\",\n+    }\n+\n+\n class ModelArgs:\n     labels = {\n         \"description\": \"\"\"\n@@ -1131,6 +1399,63 @@ def get_model_name(obj):\n     return \"model\"\n \n \n+def generate_processor_intro(cls) -> str:\n+    \"\"\"\n+    Generate the intro docstring for a processor class based on its attributes.\n+\n+    Args:\n+        cls: Processor class to generate intro for\n+\n+    Returns:\n+        str: Generated intro text\n+    \"\"\"\n+    class_name = cls.__name__\n+\n+    # Get attributes and their corresponding class names\n+    attributes = cls.get_attributes()\n+    if not attributes:\n+        return \"\"\n+\n+    # Build list of component names and their classes\n+    components = []\n+    component_classes = []\n+\n+    for attr in attributes:\n+        # Get the class name for this attribute\n+        class_attr = f\"{attr}_class\"\n+        # Format attribute name for display\n+        attr_display = attr.replace(\"_\", \" \")\n+        components.append(attr_display)\n+        component_classes.append(f\"[`{{{class_attr}}}`]\")\n+    if not components:\n+        return \"\"\n+\n+    # Generate the intro text\n+    if len(components) == 1:\n+        components_text = f\"a {components[0]}\"\n+        classes_text = component_classes[0]\n+        classes_text_short = component_classes[0].replace(\"[`\", \"[`~\")\n+    elif len(components) == 2:\n+        components_text = f\"a {components[0]} and a {components[1]}\"\n+        classes_text = f\"{component_classes[0]} and {component_classes[1]}\"\n+        classes_text_short = (\n+            f\"{component_classes[0].replace('[`', '[`~')} and {component_classes[1].replace('[`', '[`~')}\"\n+        )\n+    else:\n+        components_text = \", \".join(f\"a {c}\" for c in components[:-1]) + f\", and a {components[-1]}\"\n+        classes_text = \", \".join(component_classes[:-1]) + f\", and {component_classes[-1]}\"\n+        classes_short = [c.replace(\"[`\", \"[`~\") for c in component_classes]\n+        classes_text_short = \", \".join(classes_short[:-1]) + f\", and {classes_short[-1]}\"\n+\n+    intro = f\"\"\"Constructs a {class_name} which wraps {components_text} into a single processor.\n+\n+[`{class_name}`] offers all the functionalities of {classes_text}. See the\n+{classes_text_short} for more information.\n+\"\"\"\n+\n+    return intro\n+\n+\n def get_placeholders_dict(placeholders: list, model_name: str) -> dict:\n     \"\"\"\n     Get the dictionary of placeholders for the given model name.\n@@ -1152,7 +1477,9 @@ def get_placeholders_dict(placeholders: list, model_name: str) -> dict:\n                 place_holder_value = None\n             if place_holder_value is not None:\n                 if isinstance(place_holder_value, (list, tuple)):\n-                    place_holder_value = place_holder_value[0]\n+                    place_holder_value = (\n+                        place_holder_value[-1] if place_holder_value[-1] is not None else place_holder_value[0]\n+                    )\n                 placeholders_dict[placeholder] = place_holder_value if place_holder_value is not None else placeholder\n             else:\n                 placeholders_dict[placeholder] = placeholder\n@@ -1351,13 +1678,14 @@ def _get_parameter_info(param_name, documented_params, source_args_dict, param_t\n         ):\n             param_type = documented_params[param_name][\"type\"]\n         optional = documented_params[param_name][\"optional\"]\n-        shape = documented_params[param_name][\"shape\"]\n+        shape = documented_params[param_name].get(\"shape\", None)\n         shape_string = shape if shape else \"\"\n         additional_info = documented_params[param_name][\"additional_info\"] or \"\"\n         description = f\"{documented_params[param_name]['description']}\\n\"\n     elif param_name in source_args_dict:\n         # Parameter is documented in ModelArgs or ImageProcessorArgs\n-        shape = source_args_dict[param_name][\"shape\"]\n+        param_type = source_args_dict[param_name].get(\"type\", param_type)\n+        shape = source_args_dict[param_name].get(\"shape\", None)\n         shape_string = \" \" + shape if shape else \"\"\n         description = source_args_dict[param_name][\"description\"]\n         additional_info = source_args_dict[param_name].get(\"additional_info\", None)\n@@ -1385,9 +1713,16 @@ def _process_regular_parameters(\n         undocumented_parameters (`list`): List to append undocumented parameters to\n     \"\"\"\n     docstring = \"\"\n-    source_args_dict = (\n-        get_args_doc_from_source([ModelArgs, ImageProcessorArgs]) if source_args_dict is None else source_args_dict\n-    )\n+    # Check if this is a processor by inspecting class hierarchy\n+    is_processor = _is_processor_class(func, parent_class)\n+\n+    # Use appropriate args source based on whether it's a processor or not\n+    if source_args_dict is None:\n+        if is_processor:\n+            source_args_dict = get_args_doc_from_source([ModelArgs, ImageProcessorArgs, ProcessorArgs])\n+        else:\n+            source_args_dict = get_args_doc_from_source([ModelArgs, ImageProcessorArgs])\n+\n     missing_args = {}\n \n     for param_name, param in sig.parameters.items():\n@@ -1463,6 +1798,47 @@ def find_sig_line(lines, line_end):\n     return sig_line_end\n \n \n+def _is_processor_class(func, parent_class):\n+    \"\"\"\n+    Check if a function belongs to a ProcessorMixin class.\n+\n+    Uses two methods:\n+    1. Check parent_class inheritance (if provided)\n+    2. Check if the source file is named processing_*.py (multimodal processors)\n+       vs image_processing_*.py, video_processing_*.py, etc. (single-modality processors)\n+\n+    Args:\n+        func: The function to check\n+        parent_class: Optional parent class (if available)\n+\n+    Returns:\n+        bool: True if this is a multimodal processor (inherits from ProcessorMixin), False otherwise\n+    \"\"\"\n+    # First, check if parent_class is provided and use it\n+    if parent_class is not None:\n+        return \"ProcessorMixin\" in parent_class.__name__ or any(\n+            \"ProcessorMixin\" in base.__name__ for base in parent_class.__mro__\n+        )\n+\n+    # If parent_class is None, check the filename\n+    # Multimodal processors are in files named \"processing_*.py\"\n+    # Single-modality processors are in \"image_processing_*.py\", \"video_processing_*.py\", etc.\n+    try:\n+        source_file = inspect.getsourcefile(func)\n+        if source_file:\n+            filename = os.path.basename(source_file)\n+            # Check if it's a processing file (processing_*.py) but NOT a single-modality processor\n+            # Single-modality processors: image_processing_*.py, video_processing_*.py, feature_extraction_*.py\n+            if filename.startswith(\"processing_\") and filename.endswith(\".py\"):\n+                # This is a multimodal processor file\n+                return True\n+    except Exception:\n+        pass\n+\n+    # Default to False (conservative approach)\n+    return False\n+\n+\n def _process_kwargs_parameters(sig, func, parent_class, documented_kwargs, indent_level, undocumented_parameters):\n     \"\"\"\n     Process **kwargs parameters if needed.\n@@ -1476,7 +1852,15 @@ def _process_kwargs_parameters(sig, func, parent_class, documented_kwargs, inden\n         undocumented_parameters (`list`): List to append undocumented parameters to\n     \"\"\"\n     docstring = \"\"\n-    source_args_dict = get_args_doc_from_source(ImageProcessorArgs)\n+\n+    # Check if this is a processor by inspecting class hierarchy\n+    is_processor = _is_processor_class(func, parent_class)\n+\n+    # Use appropriate args source based on whether it's a processor or not\n+    if is_processor:\n+        source_args_dict = get_args_doc_from_source([ImageProcessorArgs, ProcessorArgs])\n+    else:\n+        source_args_dict = get_args_doc_from_source(ImageProcessorArgs)\n \n     # Check if we need to add typed kwargs description to the docstring\n     unroll_kwargs = func.__name__ in UNROLL_KWARGS_METHODS\n@@ -1485,7 +1869,6 @@ def _process_kwargs_parameters(sig, func, parent_class, documented_kwargs, inden\n         unroll_kwargs = any(\n             unroll_kwargs_class in parent_class.__name__ for unroll_kwargs_class in UNROLL_KWARGS_CLASSES\n         )\n-\n     if unroll_kwargs:\n         # get all unpackable \"kwargs\" parameters\n         kwargs_parameters = [\n@@ -1505,6 +1888,117 @@ def _process_kwargs_parameters(sig, func, parent_class, documented_kwargs, inden\n \n             # Process each kwarg parameter\n             for param_name, param_type_annotation in kwarg_param.annotation.__args__[0].__annotations__.items():\n+                # Handle nested kwargs structures for processors\n+                if is_processor and param_name.endswith(\"_kwargs\"):\n+                    # Check if this is a basic kwargs type that should be skipped\n+                    # Basic kwargs types are generic containers that shouldn't be documented as individual params\n+                    basic_kwargs_types = [\"TextKwargs\", \"ImagesKwargs\", \"VideosKwargs\", \"AudioKwargs\"]\n+\n+                    # Get the actual type (unwrap Optional if needed)\n+                    actual_type = param_type_annotation\n+                    type_name = getattr(param_type_annotation, \"__name__\", None)\n+                    if type_name is None and hasattr(param_type_annotation, \"__origin__\"):\n+                        # Handle Optional[Type] or Union cases\n+                        args = getattr(param_type_annotation, \"__args__\", ())\n+                        for arg in args:\n+                            if arg is not type(None):\n+                                actual_type = arg\n+                                type_name = getattr(arg, \"__name__\", None)\n+                                break\n+\n+                    # Skip only if it's one of the basic kwargs types\n+                    if type_name in basic_kwargs_types:\n+                        continue\n+\n+                    # Otherwise, unroll the custom typed kwargs\n+                    # Get the nested TypedDict's annotations\n+                    if hasattr(actual_type, \"__annotations__\"):\n+                        nested_kwargs_doc = getattr(actual_type, \"__doc__\", None)\n+                        documented_nested_kwargs = {}\n+                        if nested_kwargs_doc:\n+                            documented_nested_kwargs = parse_docstring(nested_kwargs_doc)[0]\n+\n+                        # Only process fields that are documented in the custom kwargs class's own docstring\n+                        # This prevents showing too many inherited parameters\n+                        if not documented_nested_kwargs:\n+                            # No documentation in the custom kwargs class, skip unrolling\n+                            continue\n+\n+                        # Process each field in the custom typed kwargs\n+                        for nested_param_name, nested_param_type in actual_type.__annotations__.items():\n+                            # Only document parameters that are explicitly documented in the TypedDict's docstring\n+                            if nested_param_name not in documented_nested_kwargs:\n+                                continue\n+                            nested_param_type_str = str(nested_param_type)\n+                            nested_optional = False\n+\n+                            # Process parameter type\n+                            if \"typing\" in nested_param_type_str:\n+                                nested_param_type_str = \"\".join(nested_param_type_str.split(\"typing.\")).replace(\n+                                    \"transformers.\", \"~\"\n+                                )\n+                            else:\n+                                nested_param_type_str = f\"{nested_param_type_str.replace('transformers.', '~').replace('builtins', '')}.{nested_param_name}\"\n+                            if \"ForwardRef\" in nested_param_type_str:\n+                                nested_param_type_str = re.sub(\n+                                    r\"ForwardRef\\('([\\w.]+)'\\)\", r\"\\1\", nested_param_type_str\n+                                )\n+                            if \"Optional\" in nested_param_type_str:\n+                                nested_param_type_str = re.sub(r\"Optional\\[(.*?)\\]\", r\"\\1\", nested_param_type_str)\n+                                nested_optional = True\n+\n+                            # Check for default value\n+                            nested_param_default = \"\"\n+                            if parent_class is not None:\n+                                nested_param_default = str(getattr(parent_class, nested_param_name, \"\"))\n+                                nested_param_default = (\n+                                    f\", defaults to `{nested_param_default}`\" if nested_param_default != \"\" else \"\"\n+                                )\n+\n+                            # Only use the TypedDict's own docstring, not source_args_dict\n+                            # This prevents pulling in too many inherited parameters\n+                            (\n+                                nested_param_type_str,\n+                                nested_optional_string,\n+                                nested_shape_string,\n+                                nested_additional_info,\n+                                nested_description,\n+                                nested_is_documented,\n+                            ) = _get_parameter_info(\n+                                nested_param_name,\n+                                documented_nested_kwargs,\n+                                {},  # Empty dict - only use TypedDict's own docstring\n+                                nested_param_type_str,\n+                                nested_optional,\n+                            )\n+\n+                            # nested_is_documented should always be True here since we filter for it above\n+                            # Check if type is missing\n+                            if nested_param_type_str == \"\":\n+                                print(\n+                                    f\"ðŸš¨ {nested_param_name} for {type_name} in file {func.__code__.co_filename} has no type\"\n+                                )\n+                            nested_param_type_str = (\n+                                nested_param_type_str if \"`\" in nested_param_type_str else f\"`{nested_param_type_str}`\"\n+                            )\n+                            # Format the parameter docstring\n+                            if nested_additional_info:\n+                                docstring += set_min_indent(\n+                                    f\"{nested_param_name} ({nested_param_type_str}{nested_additional_info}):{nested_description}\",\n+                                    indent_level + 8,\n+                                )\n+                            else:\n+                                docstring += set_min_indent(\n+                                    f\"{nested_param_name} ({nested_param_type_str}{nested_shape_string}{nested_optional_string}{nested_param_default}):{nested_description}\",\n+                                    indent_level + 8,\n+                                )\n+\n+                        # Skip processing the _kwargs parameter itself since we've processed its contents\n+                        continue\n+                    else:\n+                        # If we can't get annotations, skip this parameter\n+                        continue\n+\n                 param_type = str(param_type_annotation)\n                 optional = False\n \n@@ -1555,6 +2049,43 @@ def _process_kwargs_parameters(sig, func, parent_class, documented_kwargs, inden\n     return docstring\n \n \n+def _add_return_tensors_for_processor_call(func, parent_class, docstring, indent_level):\n+    \"\"\"\n+    Add return_tensors parameter documentation for processor __call__ methods if not already present.\n+\n+    Args:\n+        func (`function`): Function being processed\n+        parent_class (`class`): Parent class of the function\n+        docstring (`str`): Current docstring being built\n+        indent_level (`int`): Indentation level\n+\n+    Returns:\n+        str: Updated docstring with return_tensors if applicable\n+    \"\"\"\n+    # Check if this is a processor __call__ method\n+    is_processor_call = False\n+    if func.__name__ == \"__call__\":\n+        # Check if this is a processor by inspecting class hierarchy\n+        is_processor_call = _is_processor_class(func, parent_class)\n+\n+    # If it's a processor __call__ method and return_tensors is not already documented\n+    if is_processor_call and \"return_tensors\" not in docstring:\n+        # Get the return_tensors documentation from ImageProcessorArgs\n+        source_args_dict = get_args_doc_from_source(ProcessorArgs)\n+        return_tensors_info = source_args_dict[\"return_tensors\"]\n+        param_type = return_tensors_info.get(\"type\", \"`str` or [`~utils.TensorType`]\")\n+        description = return_tensors_info[\"description\"]\n+\n+        # Format the parameter type\n+        param_type = param_type if \"`\" in param_type else f\"`{param_type}`\"\n+\n+        # Format the parameter docstring\n+        param_docstring = f\"return_tensors ({param_type}, *optional*):{description}\"\n+        docstring += set_min_indent(param_docstring, indent_level + 8)\n+\n+    return docstring\n+\n+\n def _process_parameters_section(\n     func_documentation, sig, func, class_name, model_name_lowercase, parent_class, indent_level, source_args_dict\n ):\n@@ -1592,6 +2123,9 @@ def _process_parameters_section(\n     )\n     docstring += kwargs_docstring\n \n+    # Add return_tensors for processor __call__ methods if not already present\n+    docstring = _add_return_tensors_for_processor_call(func, parent_class, docstring, indent_level)\n+\n     # Report undocumented parameters\n     if len(undocumented_parameters) > 0:\n         print(\"\\n\".join(undocumented_parameters))\n@@ -1656,10 +2190,13 @@ def _process_example_section(\n     example_docstring = \"\"\n \n     # Use existing example section if available\n-\n     if func_documentation is not None and (match := re.search(r\"(?m)^([ \\t]*)(?=Example)\", func_documentation)):\n         example_docstring = func_documentation[match.start() :]\n         example_docstring = \"\\n\" + set_min_indent(example_docstring, indent_level + 4)\n+    # Skip examples for processors\n+    elif _is_processor_class(func, parent_class):\n+        # Processors don't get auto-generated examples\n+        return example_docstring\n     # No examples for __init__ methods or if the class is not a model\n     elif parent_class is None and model_name_lowercase is not None:\n         task = rf\"({'|'.join(PT_SAMPLE_DOCSTRINGS.keys())})\"\n@@ -1790,12 +2327,22 @@ def auto_class_docstring(cls, custom_intro=None, custom_args=None, checkpoint=No\n     from transformers.models import auto as auto_module\n \n     is_dataclass = False\n+    is_processor = False\n     docstring_init = \"\"\n     docstring_args = \"\"\n     if \"PreTrainedModel\" in (x.__name__ for x in cls.__mro__):\n         docstring_init = auto_method_docstring(\n             cls.__init__, parent_class=cls, custom_args=custom_args, checkpoint=checkpoint\n         ).__doc__.replace(\"Args:\", \"Parameters:\")\n+    elif \"ProcessorMixin\" in (x.__name__ for x in cls.__mro__):\n+        is_processor = True\n+        docstring_init = auto_method_docstring(\n+            cls.__init__,\n+            parent_class=cls,\n+            custom_args=custom_args,\n+            checkpoint=checkpoint,\n+            source_args_dict=get_args_doc_from_source([ModelArgs, ImageProcessorArgs, ProcessorArgs]),\n+        ).__doc__.replace(\"Args:\", \"Parameters:\")\n     elif \"ModelOutput\" in (x.__name__ for x in cls.__mro__):\n         # We have a data class\n         is_dataclass = True\n@@ -1819,17 +2366,23 @@ def auto_class_docstring(cls, custom_intro=None, custom_args=None, checkpoint=No\n         model_name_lowercase = model_name_lowercase.replace(\"_\", \"-\")\n \n     name = re.findall(rf\"({'|'.join(ClassDocstring.__dict__.keys())})$\", cls.__name__)\n-    if name == [] and custom_intro is None and not is_dataclass:\n+    if name == [] and custom_intro is None and not is_dataclass and not is_processor:\n         raise ValueError(\n             f\"`{cls.__name__}` is not registered in the auto doc. Here are the available classes: {ClassDocstring.__dict__.keys()}.\\n\"\n             \"Add a `custom_intro` to the decorator if you want to use `auto_docstring` on a class not registered in the auto doc.\"\n         )\n-    if name != [] or custom_intro is not None or is_dataclass:\n+    if name != [] or custom_intro is not None or is_dataclass or is_processor:\n         name = name[0] if name else None\n         if custom_intro is not None:\n             pre_block = equalize_indent(custom_intro, indent_level)\n             if not pre_block.endswith(\"\\n\"):\n                 pre_block += \"\\n\"\n+        elif is_processor:\n+            # Generate processor intro dynamically\n+            pre_block = generate_processor_intro(cls)\n+            if pre_block:\n+                pre_block = equalize_indent(pre_block, indent_level)\n+                pre_block = format_args_docstring(pre_block, model_name_lowercase)\n         elif model_name_title is None or name is None:\n             pre_block = \"\"\n         else:"
        },
        {
            "sha": "ed0dfd5a60138e2e83a7e0338109475f8fb04422",
            "filename": "tests/models/timm_backbone/test_modeling_timm_backbone.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftimm_backbone%2Ftest_modeling_timm_backbone.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -17,7 +17,7 @@\n import unittest\n \n from transformers import AutoBackbone\n-from transformers.testing_utils import require_timm, require_torch, torch_device\n+from transformers.testing_utils import is_flaky, require_timm, require_torch, torch_device\n from transformers.utils.import_utils import is_torch_available\n \n from ...test_backbone_common import BackboneTesterMixin\n@@ -101,6 +101,7 @@ def test_config(self):\n         self.config_tester.run_common_tests()\n \n     # `TimmBackbone` has no `_init_weights`. Timm's way of weight init. seems to give larger magnitude in the intermediate values during `forward`.\n+    @is_flaky(description=\"Large difference with A10. Still flaky after setting larger tolerance\")\n     def test_batching_equivalence(self, atol=1e-4, rtol=1e-4):\n         super().test_batching_equivalence(atol=atol, rtol=rtol)\n "
        },
        {
            "sha": "d63fa494e5827c8b02278f19e0d76140f82488a8",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 506,
            "deletions": 37,
            "changes": 543,
            "blob_url": "https://github.com/huggingface/transformers/blob/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c8bc4dea4c24037cafa38a10ffd0d583b35441a2/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=c8bc4dea4c24037cafa38a10ffd0d583b35441a2",
            "patch": "@@ -53,6 +53,7 @@\n     ImageProcessorArgs,\n     ModelArgs,\n     ModelOutputArgs,\n+    ProcessorArgs,\n     get_args_doc_from_source,\n     parse_docstring,\n     set_min_indent,\n@@ -76,6 +77,7 @@ class DecoratedItem:\n     has_init: bool = False  # Whether the class has an __init__ method\n     init_def_line: int | None = None  # 1-based line number of __init__ def (if has_init)\n     is_model_output: bool = False  # Whether the class inherits from ModelOutput\n+    is_processor: bool = False  # Whether the class inherits from ProcessorMixin\n \n \n PATH_TO_REPO = Path(__file__).parent.parent.resolve()\n@@ -536,6 +538,23 @@ class DecoratedItem:\n }\n \n \n+def has_auto_docstring_decorator(obj) -> bool:\n+    try:\n+        # Get the source lines for the object\n+        source_lines = inspect.getsourcelines(obj)[0]\n+\n+        # Check the lines before the definition for @auto_docstring decorator\n+        for line in source_lines[:10]:  # Check first 10 lines (decorators come before def/class)\n+            line = line.strip()\n+            if line.startswith(\"@auto_docstring\"):\n+                return True\n+    except (TypeError, OSError):\n+        # Some objects don't have source code available\n+        pass\n+\n+    return False\n+\n+\n def find_indent(line: str) -> int:\n     \"\"\"\n     Returns the number of spaces that start a line indent.\n@@ -983,6 +1002,8 @@ def find_matching_model_files(check_all: bool = False):\n     potential_files = glob.glob(modeling_glob_pattern)\n     image_processing_glob_pattern = os.path.join(PATH_TO_TRANSFORMERS, \"models/**/image_processing_*_fast.py\")\n     potential_files += glob.glob(image_processing_glob_pattern)\n+    processing_glob_pattern = os.path.join(PATH_TO_TRANSFORMERS, \"models/**/processing_*.py\")\n+    potential_files += glob.glob(processing_glob_pattern)\n     matching_files = []\n     for file_path in potential_files:\n         if os.path.isfile(file_path):\n@@ -1031,6 +1052,7 @@ def generate_new_docstring_for_signature(\n     output_docstring_indent=8,\n     custom_args_dict={},\n     source_args_doc=[ModelArgs, ImageProcessorArgs],\n+    is_model_output=False,\n ):\n     \"\"\"\n     Generalized docstring generator for a function or class signature.\n@@ -1040,6 +1062,7 @@ def generate_new_docstring_for_signature(\n         sig_end_line: Line index where the signature ends.\n         docstring_line: Line index where the docstring starts (or None if not present).\n         arg_indent: Indentation for missing argument doc entries.\n+        is_model_output: Whether this is a ModelOutput dataclass (inherited args should be kept)\n     Returns:\n         new_docstring, sig_end_line, docstring_end (last docstring line index)\n     \"\"\"\n@@ -1062,24 +1085,43 @@ def generate_new_docstring_for_signature(\n     # Remove pre-existing entries for *args and untyped **kwargs from the docstring\n     # (No longer needed since *args are excluded from args_in_signature)\n \n-    # Remove args that are the same as the ones in the source args doc\n+    # Remove args that are the same as the ones in the source args doc OR have placeholders\n     for arg in args_docstring_dict:\n         if arg in get_args_doc_from_source(source_args_doc) and arg not in ALWAYS_OVERRIDE:\n             source_arg_doc = get_args_doc_from_source(source_args_doc)[arg]\n-            if source_arg_doc[\"description\"].strip(\"\\n \") == args_docstring_dict[arg][\"description\"].strip(\"\\n \"):\n-                if source_arg_doc.get(\"shape\") is not None and args_docstring_dict[arg].get(\"shape\") is not None:\n-                    if source_arg_doc.get(\"shape\").strip(\"\\n \") == args_docstring_dict[arg].get(\"shape\").strip(\"\\n \"):\n+            arg_doc = args_docstring_dict[arg]\n+\n+            # Check if this arg has placeholders\n+            has_placeholder = \"<fill_type>\" in arg_doc.get(\"type\", \"\") or \"<fill_docstring>\" in arg_doc.get(\n+                \"description\", \"\"\n+            )\n+\n+            # Remove if has placeholder (source will provide the real doc)\n+            if has_placeholder:\n+                docstring_args_ro_remove.append(arg)\n+            # Or remove if description matches source exactly\n+            elif source_arg_doc[\"description\"].strip(\"\\n \") == arg_doc[\"description\"].strip(\"\\n \"):\n+                if source_arg_doc.get(\"shape\") is not None and arg_doc.get(\"shape\") is not None:\n+                    if source_arg_doc.get(\"shape\").strip(\"\\n \") == arg_doc.get(\"shape\").strip(\"\\n \"):\n                         docstring_args_ro_remove.append(arg)\n-                elif (\n-                    source_arg_doc.get(\"additional_info\") is not None\n-                    and args_docstring_dict[arg].get(\"additional_info\") is not None\n-                ):\n-                    if source_arg_doc.get(\"additional_info\").strip(\"\\n \") == args_docstring_dict[arg].get(\n-                        \"additional_info\"\n-                    ).strip(\"\\n \"):\n+                elif source_arg_doc.get(\"additional_info\") is not None and arg_doc.get(\"additional_info\") is not None:\n+                    if source_arg_doc.get(\"additional_info\").strip(\"\\n \") == arg_doc.get(\"additional_info\").strip(\n+                        \"\\n \"\n+                    ):\n                         docstring_args_ro_remove.append(arg)\n                 else:\n                     docstring_args_ro_remove.append(arg)\n+\n+    # For regular methods/functions (not ModelOutput), also remove args not in signature\n+    if not is_model_output:\n+        for arg in list(args_docstring_dict.keys()):\n+            if (\n+                arg not in args_in_signature\n+                and arg not in get_args_doc_from_source(source_args_doc)\n+                and arg not in custom_args_dict\n+            ):\n+                docstring_args_ro_remove.append(arg)\n+\n     args_docstring_dict = {\n         arg: args_docstring_dict[arg] for arg in args_docstring_dict if arg not in docstring_args_ro_remove\n     }\n@@ -1101,7 +1143,8 @@ def generate_new_docstring_for_signature(\n                 \"additional_info\": None,\n             }\n \n-    # Handle docstring of inherited args (for dataclasses)\n+    # Handle docstring of inherited args (for dataclasses like ModelOutput)\n+    # For regular methods, this will be empty since we removed args not in signature above\n     ordered_args_docstring_dict = OrderedDict(\n         (arg, args_docstring_dict[arg]) for arg in args_docstring_dict if arg not in args_in_signature\n     )\n@@ -1154,13 +1197,22 @@ def generate_new_docstring_for_function(\n     sig_end_line = item.body_start_line - 1  # Convert to 0-based\n     args_in_signature = item.args\n     docstring_start_line = sig_end_line if '\"\"\"' in lines[sig_end_line] else None\n+\n+    # Use ProcessorArgs for processor methods\n+    if item.is_processor:\n+        source_args_doc = [ModelArgs, ImageProcessorArgs, ProcessorArgs]\n+    else:\n+        source_args_doc = [ModelArgs, ImageProcessorArgs]\n+\n     return generate_new_docstring_for_signature(\n         lines,\n         args_in_signature,\n         sig_end_line,\n         docstring_start_line,\n         arg_indent=\"    \",\n         custom_args_dict=custom_args_dict,\n+        source_args_doc=source_args_doc,\n+        is_model_output=False,  # Functions are never ModelOutput\n     )\n \n \n@@ -1180,7 +1232,11 @@ def generate_new_docstring_for_class(\n         sig_end_line = item.body_start_line - 1  # Convert from body start to sig end (0-based)\n         args_in_signature = item.args\n         output_docstring_indent = 8\n-        source_args_doc = [ModelArgs, ImageProcessorArgs]\n+        # Add ProcessorArgs for Processor classes\n+        if item.is_processor:\n+            source_args_doc = [ModelArgs, ImageProcessorArgs, ProcessorArgs]\n+        else:\n+            source_args_doc = [ModelArgs, ImageProcessorArgs]\n     elif item.is_model_output:\n         # ModelOutput class - extract args from dataclass attributes\n         current_line_end = item.def_line - 1  # Convert to 0-based\n@@ -1211,6 +1267,7 @@ def generate_new_docstring_for_class(\n         custom_args_dict=custom_args_dict,\n         output_docstring_indent=output_docstring_indent,\n         source_args_doc=source_args_doc,\n+        is_model_output=item.is_model_output,\n     )\n \n \n@@ -1235,10 +1292,21 @@ def _build_ast_indexes(source: str) -> list[DecoratedItem]:\n             if isinstance(node.value.value, str) and isinstance(node.target, ast.Name):\n                 var_to_string[node.target.id] = node.value.value\n     # Second pass: find all @auto_docstring decorated functions/classes\n-    decorated_items: list[DecoratedItem] = []\n+    # First, identify processor classes to track method context\n+    processor_classes: set[str] = set()\n     for node in ast.walk(tree):\n+        if isinstance(node, ast.ClassDef):\n+            for base in node.bases:\n+                if isinstance(base, ast.Name) and (\"ProcessorMixin\" in base.id or \"Processor\" in base.id):\n+                    processor_classes.add(node.name)\n+                    break\n+\n+    decorated_items: list[DecoratedItem] = []\n+\n+    # Helper function to process decorated items\n+    def process_node(node, parent_class_name=None):\n         if not isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef, ast.ClassDef)):\n-            continue\n+            return\n         # Find @auto_docstring decorator and extract custom_args if present\n         decorator_line = None\n         custom_args_text = None\n@@ -1256,7 +1324,7 @@ def _build_ast_indexes(source: str) -> list[DecoratedItem]:\n                             custom_args_text = var_to_string.get(kw.value.id, \"\").strip()\n             break\n         if decorator_line is None:  # No @auto_docstring decorator found\n-            continue\n+            return\n         # Extract info for this decorated item\n         kind = \"class\" if isinstance(node, ast.ClassDef) else \"function\"\n         body_start_line = node.body[0].lineno if node.body else node.lineno + 1\n@@ -1265,16 +1333,23 @@ def _build_ast_indexes(source: str) -> list[DecoratedItem]:\n         has_init = False\n         init_def_line = None\n         is_model_output = False\n+        is_processor = False\n+\n         if isinstance(node, (ast.FunctionDef, ast.AsyncFunctionDef)):\n-            # For functions, extract args directly\n+            # For functions/methods, extract args directly\n             arg_names = _extract_function_args(node)\n+            # Check if this method is inside a processor class\n+            if parent_class_name and parent_class_name in processor_classes:\n+                is_processor = True\n         elif isinstance(node, ast.ClassDef):\n-            # For classes, look for __init__ method and check if it's a ModelOutput\n-            # Check if class inherits from ModelOutput\n+            # For classes, look for __init__ method and check if it's a ModelOutput or Processor\n+            # Check if class inherits from ModelOutput or ProcessorMixin\n             for base in node.bases:\n-                if isinstance(base, ast.Name) and \"ModelOutput\" in base.id:\n-                    is_model_output = True\n-                    break\n+                if isinstance(base, ast.Name):\n+                    if \"ModelOutput\" in base.id:\n+                        is_model_output = True\n+                    elif \"ProcessorMixin\" in base.id or \"Processor\" in base.id:\n+                        is_processor = True\n             # Look for __init__ method in the class body\n             for class_item in node.body:\n                 if isinstance(class_item, ast.FunctionDef) and class_item.name == \"__init__\":\n@@ -1296,12 +1371,358 @@ def _build_ast_indexes(source: str) -> list[DecoratedItem]:\n                 has_init=has_init,\n                 init_def_line=init_def_line,\n                 is_model_output=is_model_output,\n+                is_processor=is_processor,\n             )\n         )\n \n+    # Traverse tree with parent context\n+    for node in tree.body:\n+        if isinstance(node, ast.ClassDef):\n+            # Check class itself\n+            process_node(node)\n+            # Check methods within the class\n+            for class_item in node.body:\n+                process_node(class_item, parent_class_name=node.name)\n+        else:\n+            # Top-level functions\n+            process_node(node)\n+\n     return sorted(decorated_items, key=lambda x: x.decorator_line)\n \n \n+def _extract_type_name(annotation) -> str | None:\n+    \"\"\"\n+    Extract the type name from an AST annotation node.\n+    Handles: TypeName, Optional[TypeName], Union[TypeName, ...], list[TypeName], etc.\n+    Returns the base type name if found, or None.\n+    \"\"\"\n+    if isinstance(annotation, ast.Name):\n+        # Simple type: TypeName\n+        return annotation.id\n+    elif isinstance(annotation, ast.Subscript):\n+        # Generic type: Optional[TypeName], list[TypeName], etc.\n+        # Try to extract from the subscript value\n+        if isinstance(annotation.value, ast.Name):\n+            # If it's Optional, Union, list, etc., look at the slice\n+            if isinstance(annotation.slice, ast.Name):\n+                return annotation.slice.id\n+            elif isinstance(annotation.slice, ast.Tuple):\n+                # Union[TypeName, None] - take first element\n+                if annotation.slice.elts and isinstance(annotation.slice.elts[0], ast.Name):\n+                    return annotation.slice.elts[0].id\n+    return None\n+\n+\n+def _find_typed_dict_classes(source: str) -> list[dict]:\n+    \"\"\"\n+    Find all custom TypedDict kwargs classes in the source.\n+\n+    Returns:\n+        List of dicts with TypedDict info: name, line, fields, all_fields, field_types, docstring info\n+        - fields: fields that need custom documentation (not in standard args, not nested TypedDicts)\n+        - all_fields: all fields including those in standard args (for redundancy checking)\n+    \"\"\"\n+    tree = ast.parse(source)\n+\n+    # Get standard args that are already documented in source classes\n+    standard_args = set()\n+    try:\n+        standard_args.update(get_args_doc_from_source([ModelArgs, ImageProcessorArgs, ProcessorArgs]).keys())\n+    except Exception:\n+        pass\n+\n+    # Collect all TypedDict class names first (for excluding nested TypedDicts)\n+    typed_dict_names = set()\n+    for node in ast.walk(tree):\n+        if isinstance(node, ast.ClassDef):\n+            for base in node.bases:\n+                if isinstance(base, ast.Name) and (\"TypedDict\" in base.id or \"Kwargs\" in base.id):\n+                    typed_dict_names.add(node.name)\n+                    break\n+\n+    typed_dicts = []\n+\n+    # Check each TypedDict class\n+    for node in ast.walk(tree):\n+        if not isinstance(node, ast.ClassDef):\n+            continue\n+\n+        # Check if this is a TypedDict\n+        is_typed_dict = False\n+        for base in node.bases:\n+            if isinstance(base, ast.Name) and (\"TypedDict\" in base.id or \"Kwargs\" in base.id):\n+                is_typed_dict = True\n+                break\n+\n+        if not is_typed_dict:\n+            continue\n+\n+        # Skip standard kwargs classes\n+        if node.name in [\"TextKwargs\", \"ImagesKwargs\", \"VideosKwargs\", \"AudioKwargs\", \"ProcessingKwargs\"]:\n+            continue\n+\n+        # Extract fields and their types (in declaration order)\n+        fields = []  # Fields that need custom documentation\n+        all_fields = []  # All fields including those in standard args\n+        field_types = {}\n+        for class_item in node.body:\n+            if isinstance(class_item, ast.AnnAssign) and isinstance(class_item.target, ast.Name):\n+                field_name = class_item.target.id\n+                if not field_name.startswith(\"_\"):\n+                    # Extract type and check if it's a nested TypedDict\n+                    if class_item.annotation:\n+                        type_name = _extract_type_name(class_item.annotation)\n+                        if type_name:\n+                            field_types[field_name] = type_name\n+                            # Skip nested TypedDicts\n+                            if type_name in typed_dict_names or type_name.endswith(\"Kwargs\"):\n+                                continue\n+                    # Track all fields for redundancy checking\n+                    all_fields.append(field_name)\n+                    # Only add to fields if not in standard args (needs custom documentation)\n+                    if field_name not in standard_args:\n+                        fields.append(field_name)\n+\n+        # Skip if no fields at all (including standard args)\n+        if not all_fields:\n+            continue\n+\n+        # Extract docstring info\n+        docstring = None\n+        docstring_start_line = None\n+        docstring_end_line = None\n+        if (\n+            node.body\n+            and isinstance(node.body[0], ast.Expr)\n+            and isinstance(node.body[0].value, ast.Constant)\n+            and isinstance(node.body[0].value.value, str)\n+        ):\n+            docstring = node.body[0].value.value\n+            docstring_start_line = node.body[0].lineno\n+            docstring_end_line = node.body[0].end_lineno\n+\n+        typed_dicts.append(\n+            {\n+                \"name\": node.name,\n+                \"line\": node.lineno,\n+                \"fields\": fields,\n+                \"all_fields\": all_fields,\n+                \"field_types\": field_types,\n+                \"docstring\": docstring,\n+                \"docstring_start_line\": docstring_start_line,\n+                \"docstring_end_line\": docstring_end_line,\n+            }\n+        )\n+\n+    return typed_dicts\n+\n+\n+def _process_typed_dict_docstrings(\n+    candidate_file: str,\n+    overwrite: bool = False,\n+) -> tuple[list[str], list[str], list[str]]:\n+    \"\"\"\n+    Check and optionally fix TypedDict docstrings.\n+    Runs as a separate pass after @auto_docstring processing.\n+\n+    Args:\n+        candidate_file: Path to the file to process\n+        overwrite: Whether to fix issues by writing to the file\n+\n+    Returns:\n+        Tuple of (missing_warnings, fill_warnings, redundant_warnings)\n+    \"\"\"\n+    with open(candidate_file, \"r\", encoding=\"utf-8\") as f:\n+        content = f.read()\n+\n+    typed_dicts = _find_typed_dict_classes(content)\n+    if not typed_dicts:\n+        return [], [], []\n+\n+    # Get source args for comparison\n+    source_args_doc = get_args_doc_from_source([ModelArgs, ImageProcessorArgs, ProcessorArgs])\n+\n+    missing_warnings = []\n+    fill_warnings = []\n+    redundant_warnings = []\n+\n+    # Process each TypedDict\n+    for td in typed_dicts:\n+        # Parse existing docstring\n+        documented_fields = {}\n+        remaining_docstring = \"\"\n+        if td[\"docstring\"]:\n+            try:\n+                documented_fields, remaining_docstring = parse_docstring(td[\"docstring\"])\n+            except Exception:\n+                pass\n+\n+        # Find missing, fill, and redundant fields\n+        missing_fields = []\n+        fill_fields = []\n+        redundant_fields = []\n+\n+        # Check fields that need custom documentation (not in source args)\n+        for field in td[\"fields\"]:\n+            if field not in documented_fields:\n+                missing_fields.append(field)\n+            else:\n+                field_doc = documented_fields[field]\n+                desc = field_doc.get(\"description\", \"\")\n+                type_str = field_doc.get(\"type\", \"\")\n+                has_placeholder = \"<fill_type>\" in type_str or \"<fill_docstring>\" in desc\n+                if has_placeholder:\n+                    fill_fields.append(field)\n+\n+        # Check ALL documented fields (including those in source args) for redundancy\n+        for field in documented_fields:\n+            if field in source_args_doc:\n+                field_doc = documented_fields[field]\n+                desc = field_doc.get(\"description\", \"\")\n+                type_str = field_doc.get(\"type\", \"\")\n+                has_placeholder = \"<fill_type>\" in type_str or \"<fill_docstring>\" in desc\n+\n+                source_doc = source_args_doc[field]\n+                source_desc = source_doc.get(\"description\", \"\").strip(\"\\n \")\n+                field_desc = desc.strip(\"\\n \")\n+\n+                # Mark as redundant if has placeholder OR description matches source\n+                if has_placeholder or source_desc == field_desc:\n+                    redundant_fields.append(field)\n+\n+        if missing_fields:\n+            field_list = \", \".join(sorted(missing_fields))\n+            missing_warnings.append(f\"    - {td['name']} (line {td['line']}): undocumented fields: {field_list}\")\n+\n+        if fill_fields:\n+            field_list = \", \".join(sorted(fill_fields))\n+            fill_warnings.append(f\"    - {td['name']} (line {td['line']}): fields with placeholders: {field_list}\")\n+\n+        if redundant_fields:\n+            field_list = \", \".join(sorted(redundant_fields))\n+            redundant_warnings.append(\n+                f\"    - {td['name']} (line {td['line']}): redundant fields (in source): {field_list}\"\n+            )\n+\n+    # If overwrite mode, fix missing fields and remove redundant ones\n+    if overwrite and (missing_warnings or redundant_warnings):\n+        lines = content.split(\"\\n\")\n+\n+        # Process TypedDicts in reverse order to avoid line number shifts\n+        for td in sorted(typed_dicts, key=lambda x: x[\"line\"], reverse=True):\n+            # Parse existing docstring\n+            documented_fields = {}\n+            remaining_docstring = \"\"\n+            if td[\"docstring\"]:\n+                try:\n+                    documented_fields, remaining_docstring = parse_docstring(td[\"docstring\"])\n+                except Exception:\n+                    pass\n+\n+            # Determine which fields to remove (redundant with source)\n+            fields_to_remove = set()\n+            for field in documented_fields:\n+                if field in source_args_doc:\n+                    field_doc = documented_fields[field]\n+                    desc = field_doc.get(\"description\", \"\")\n+                    type_str = field_doc.get(\"type\", \"\")\n+                    has_placeholder = \"<fill_type>\" in type_str or \"<fill_docstring>\" in desc\n+\n+                    source_doc = source_args_doc[field]\n+                    source_desc = source_doc.get(\"description\", \"\").strip(\"\\n \")\n+                    field_desc = desc.strip(\"\\n \")\n+\n+                    # Remove if has placeholder OR description matches source\n+                    if has_placeholder or source_desc == field_desc:\n+                        fields_to_remove.add(field)\n+\n+            # Check if any fields are missing or need removal\n+            has_missing = any(f not in documented_fields for f in td[\"fields\"])\n+            has_changes = has_missing or len(fields_to_remove) > 0\n+\n+            if not has_changes:\n+                continue\n+\n+            # Build new docstring dict (preserving existing, removing redundant, adding missing)\n+            # We iterate over documented_fields first to preserve order, then add missing fields\n+            new_doc_dict = OrderedDict()\n+\n+            # First, add documented fields that should be kept (not redundant)\n+            for field in documented_fields:\n+                if field not in fields_to_remove:\n+                    # Only keep fields that are either:\n+                    # 1. In td[\"fields\"] (needs custom documentation)\n+                    # 2. Not in source_args_doc (might be inherited or custom)\n+                    if field in td[\"fields\"] or field not in source_args_doc:\n+                        new_doc_dict[field] = documented_fields[field]\n+\n+            # Then, add missing fields from td[\"fields\"]\n+            for field in td[\"fields\"]:\n+                if field not in documented_fields and field not in new_doc_dict:\n+                    # Add placeholder for missing field\n+                    new_doc_dict[field] = {\n+                        \"type\": \"`<fill_type>`\",\n+                        \"optional\": False,\n+                        \"shape\": None,\n+                        \"description\": \"\\n    <fill_docstring>\",\n+                        \"default\": None,\n+                        \"additional_info\": None,\n+                    }\n+\n+            # Build new docstring text\n+            class_line_idx = td[\"line\"] - 1\n+            class_line = lines[class_line_idx]\n+            indent = len(class_line) - len(class_line.lstrip())\n+\n+            # If all fields were removed, remove the docstring entirely\n+            if not new_doc_dict and not remaining_docstring:\n+                if td[\"docstring\"] is not None:\n+                    doc_start_idx = td[\"docstring_start_line\"] - 1\n+                    doc_end_idx = td[\"docstring_end_line\"]\n+                    lines = lines[:doc_start_idx] + lines[doc_end_idx:]\n+                continue\n+\n+            # Build docstring content (without indentation first)\n+            docstring_content = '\"\"\"\\n'\n+            for field_name, field_doc in new_doc_dict.items():\n+                additional_info = field_doc.get(\"additional_info\", \"\") or \"\"\n+                description = field_doc[\"description\"]\n+                if description.endswith('\"\"\"'):\n+                    description = \"\\n\".join(description.split(\"\\n\")[:-1])\n+                docstring_content += f\"{field_name} ({field_doc['type']}{additional_info}):{description}\\n\"\n+\n+            # Add remaining docstring content if any\n+            close_docstring = True\n+            if remaining_docstring:\n+                if remaining_docstring.endswith('\"\"\"'):\n+                    close_docstring = False\n+                end_str = \"\\n\" if close_docstring else \"\"\n+                docstring_content += f\"{set_min_indent(remaining_docstring, 0)}{end_str}\"\n+            if close_docstring:\n+                docstring_content += '\"\"\"'\n+\n+            # Apply proper indentation\n+            docstring_content = set_min_indent(docstring_content, indent + 4)\n+            docstring_lines = docstring_content.split(\"\\n\")\n+\n+            # Replace in lines\n+            if td[\"docstring\"] is None:\n+                # Insert new docstring after class definition\n+                insert_idx = class_line_idx + 1\n+                lines = lines[:insert_idx] + docstring_lines + lines[insert_idx:]\n+            else:\n+                # Replace existing docstring\n+                doc_start_idx = td[\"docstring_start_line\"] - 1\n+                doc_end_idx = td[\"docstring_end_line\"]  # end_lineno is 1-based, we want to include this line\n+                lines = lines[:doc_start_idx] + docstring_lines + lines[doc_end_idx:]\n+\n+        # Write updated content\n+        with open(candidate_file, \"w\", encoding=\"utf-8\") as f:\n+            f.write(\"\\n\".join(lines))\n+\n+    return missing_warnings, fill_warnings, redundant_warnings\n+\n+\n def update_file_with_new_docstrings(\n     candidate_file,\n     lines,\n@@ -1404,6 +1825,10 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n         return\n     # 2. Find files that contain the @auto_docstring decorator\n     auto_docstrings_files = find_files_with_auto_docstring(matching_files)\n+\n+    # Collect all errors before raising\n+    has_errors = False\n+\n     # 3. For each file, update docstrings for all candidates\n     for candidate_file in auto_docstrings_files:\n         with open(candidate_file, \"r\", encoding=\"utf-8\") as f:\n@@ -1413,20 +1838,56 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n         # Parse file once to find all @auto_docstring decorated items\n         decorated_items = _build_ast_indexes(content)\n \n-        if not decorated_items:\n-            continue\n-\n-        # Update docstrings for all decorated items\n-        missing_docstring_args_warnings, fill_docstring_args_warnings, docstring_args_ro_remove_warnings = (\n-            update_file_with_new_docstrings(\n-                candidate_file,\n-                lines,\n-                decorated_items,\n-                content,\n-                overwrite=overwrite,\n+        missing_docstring_args_warnings = []\n+        fill_docstring_args_warnings = []\n+        docstring_args_ro_remove_warnings = []\n+\n+        # Process @auto_docstring decorated items\n+        if decorated_items:\n+            missing_docstring_args_warnings, fill_docstring_args_warnings, docstring_args_ro_remove_warnings = (\n+                update_file_with_new_docstrings(\n+                    candidate_file,\n+                    lines,\n+                    decorated_items,\n+                    content,\n+                    overwrite=overwrite,\n+                )\n             )\n+\n+        # Process TypedDict kwargs (separate pass to avoid line number conflicts)\n+        # This runs AFTER @auto_docstring processing is complete\n+        typed_dict_missing_warnings, typed_dict_fill_warnings, typed_dict_redundant_warnings = (\n+            _process_typed_dict_docstrings(candidate_file, overwrite=overwrite)\n         )\n+\n+        # Report TypedDict errors\n+        if typed_dict_missing_warnings:\n+            has_errors = True\n+            if not overwrite:\n+                print(\n+                    \"Some TypedDict fields are undocumented. Run `make fix-copies` or \"\n+                    \"`python utils/check_docstrings.py --fix_and_overwrite` to generate placeholders.\"\n+                )\n+            print(f\"[ERROR] Undocumented fields in custom TypedDict kwargs in {candidate_file}:\")\n+            for warning in typed_dict_missing_warnings:\n+                print(warning)\n+        if typed_dict_redundant_warnings:\n+            has_errors = True\n+            if not overwrite:\n+                print(\n+                    \"Some TypedDict fields are redundant (same as source or have placeholders). \"\n+                    \"Run `make fix-copies` or `python utils/check_docstrings.py --fix_and_overwrite` to remove them.\"\n+                )\n+            print(f\"[ERROR] Redundant TypedDict docstrings in {candidate_file}:\")\n+            for warning in typed_dict_redundant_warnings:\n+                print(warning)\n+        if typed_dict_fill_warnings:\n+            has_errors = True\n+            print(f\"[ERROR] TypedDict docstrings need to be filled in {candidate_file}:\")\n+            for warning in typed_dict_fill_warnings:\n+                print(warning)\n         if missing_docstring_args_warnings:\n+            has_errors = True\n             if not overwrite:\n                 print(\n                     \"Some docstrings are missing. Run `make fix-copies` or `python utils/check_docstrings.py --fix_and_overwrite` to generate the docstring templates where needed.\"\n@@ -1435,6 +1896,7 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n             for warning in missing_docstring_args_warnings:\n                 print(warning)\n         if docstring_args_ro_remove_warnings:\n+            has_errors = True\n             if not overwrite:\n                 print(\n                     \"Some docstrings are redundant with the ones in `auto_docstring.py` and will be removed. Run `make fix-copies` or `python utils/check_docstrings.py --fix_and_overwrite` to remove the redundant docstrings.\"\n@@ -1443,13 +1905,16 @@ def check_auto_docstrings(overwrite: bool = False, check_all: bool = False):\n             for warning in docstring_args_ro_remove_warnings:\n                 print(warning)\n         if fill_docstring_args_warnings:\n+            has_errors = True\n             print(f\"[ERROR] Docstring needs to be filled for the following arguments in {candidate_file}:\")\n             for warning in fill_docstring_args_warnings:\n                 print(warning)\n-        if missing_docstring_args_warnings or docstring_args_ro_remove_warnings or fill_docstring_args_warnings:\n-            raise ValueError(\n-                \"There was at least one problem when checking docstrings of objects decorated with @auto_docstring.\"\n-            )\n+\n+    # Raise error after processing all files\n+    if has_errors:\n+        raise ValueError(\n+            \"There was at least one problem when checking docstrings of objects decorated with @auto_docstring.\"\n+        )\n \n \n def check_docstrings(overwrite: bool = False, check_all: bool = False):\n@@ -1495,6 +1960,10 @@ def check_docstrings(overwrite: bool = False, check_all: bool = False):\n         if not callable(obj) or not isinstance(obj, type) or getattr(obj, \"__doc__\", None) is None:\n             continue\n \n+        # Skip objects decorated with @auto_docstring - they have auto-generated documentation\n+        if has_auto_docstring_decorator(obj):\n+            continue\n+\n         # If we are checking against the diff, we skip objects that are not part of the diff.\n         if module_diff_files is not None:\n             object_file = find_source_file(getattr(transformers, name))"
        }
    ],
    "stats": {
        "total": 6302,
        "additions": 2463,
        "deletions": 3839
    }
}