{
    "author": "Cyrilvallez",
    "message": "Tie weights recursively on all submodels (#39996)\n\n* recursive call\n\n* add missing keys\n\n* remove bad keys",
    "sha": "a96cccd0dd139ab7ff29d955a21655379d19d0a0",
    "files": [
        {
            "sha": "4d4c3fcbbfd28daf5c04c8149363841bb8b7f9f9",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/a96cccd0dd139ab7ff29d955a21655379d19d0a0/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a96cccd0dd139ab7ff29d955a21655379d19d0a0/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=a96cccd0dd139ab7ff29d955a21655379d19d0a0",
            "patch": "@@ -2992,9 +2992,10 @@ def smart_apply(self, fn):\n         # Let the magic happen with this simple call\n         self.smart_apply(self._initialize_weights)\n \n-    def tie_weights(self):\n+    def tie_embeddings_and_encoder_decoder(self):\n         \"\"\"\n-        Tie the weights between the input embeddings and the output embeddings.\n+        If set in the config, tie the weights between the input embeddings and the output embeddings,\n+        and the encoder and decoder.\n \n         If the `torchscript` flag is set in the configuration, can't handle parameter sharing so we are cloning the\n         weights instead.\n@@ -3015,7 +3016,16 @@ def tie_weights(self):\n             # Leading to issues on subsequent calls by different tests or subsequent calls.\n             self._dynamic_tied_weights_keys = tied_weights\n \n+    def tie_weights(self):\n+        \"\"\"\n+        Recursively (for all submodels) tie all the weights of the model.\n+        \"\"\"\n+        # Note that `self` is included in `self.modules` so we also apply to current PreTrainedModel with this call\n         for module in self.modules():\n+            # If it's a PreTrainedModel, may need to tie the embeddings and/or encoder/decoder weights\n+            if isinstance(module, PreTrainedModel):\n+                module.tie_embeddings_and_encoder_decoder()\n+            # Additionally, if it has a custom `_tie_weights`, honor it\n             if hasattr(module, \"_tie_weights\"):\n                 module._tie_weights()\n "
        },
        {
            "sha": "55b274596ab54a30095b52d1f41e6c3e0fb007c5",
            "filename": "src/transformers/models/blip/modeling_blip_text.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a96cccd0dd139ab7ff29d955a21655379d19d0a0/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a96cccd0dd139ab7ff29d955a21655379d19d0a0/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fmodeling_blip_text.py?ref=a96cccd0dd139ab7ff29d955a21655379d19d0a0",
            "patch": "@@ -842,6 +842,8 @@ def forward(\n \n # Adapted from https://github.com/salesforce/BLIP/blob/main/models/med.py#L811\n class BlipTextLMHeadModel(BlipTextPreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"cls.predictions.decoder.weight\", \"cls.predictions.decoder.bias\"]\n+\n     def __init__(self, config):\n         super().__init__(config)\n "
        },
        {
            "sha": "2a9e23cb92fbada9225484eeb3a8e483fee9ae90",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a96cccd0dd139ab7ff29d955a21655379d19d0a0/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a96cccd0dd139ab7ff29d955a21655379d19d0a0/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=a96cccd0dd139ab7ff29d955a21655379d19d0a0",
            "patch": "@@ -1989,7 +1989,7 @@ class SeamlessM4TTextToUnitForConditionalGeneration(SeamlessM4TPreTrainedModel,\n         \"text_encoder\",\n         \"text_decoder\",\n     ]\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = [\"lm_head.weight\"]\n \n     def __init__(\n         self,"
        },
        {
            "sha": "775d8c1a68ed84fffba3a253b1c3c4a3bde88ca8",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a96cccd0dd139ab7ff29d955a21655379d19d0a0/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a96cccd0dd139ab7ff29d955a21655379d19d0a0/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=a96cccd0dd139ab7ff29d955a21655379d19d0a0",
            "patch": "@@ -2191,7 +2191,7 @@ class SeamlessM4Tv2TextToUnitForConditionalGeneration(SeamlessM4Tv2PreTrainedMod\n         \"text_encoder\",\n         \"text_decoder\",\n     ]\n-    _tied_weights_keys = [\"decoder.embed_tokens.weight\", \"lm_head.weight\"]\n+    _tied_weights_keys = [\"lm_head.weight\"]\n \n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TTextToUnitForConditionalGeneration.__init__ with SeamlessM4T->SeamlessM4Tv2\n     def __init__("
        },
        {
            "sha": "750c5c22324d2e0f9bdd7aa2c68ce1d4cda13403",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a96cccd0dd139ab7ff29d955a21655379d19d0a0/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a96cccd0dd139ab7ff29d955a21655379d19d0a0/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=a96cccd0dd139ab7ff29d955a21655379d19d0a0",
            "patch": "@@ -48,6 +48,7 @@\n     is_deepspeed_zero3_enabled,\n     unset_hf_deepspeed_config,\n )\n+from transformers.modeling_utils import _get_tied_weight_keys\n from transformers.models.auto import get_values\n from transformers.models.auto.modeling_auto import (\n     MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES,\n@@ -2572,7 +2573,7 @@ def test_tied_weights_keys(self):\n             copied_config.get_text_config().tie_word_embeddings = True\n             model_tied = model_class(copied_config)\n \n-            tied_weight_keys = model_tied._tied_weights_keys if model_tied._tied_weights_keys is not None else []\n+            tied_weight_keys = _get_tied_weight_keys(model_tied)\n             # If we don't find any tied weights keys, and by default we don't tie the embeddings, it's because the model\n             # does not tie them\n             if len(tied_weight_keys) == 0 and not original_config.tie_word_embeddings:"
        }
    ],
    "stats": {
        "total": 23,
        "additions": 18,
        "deletions": 5
    }
}