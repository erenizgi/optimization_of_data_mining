{
    "author": "qubvel",
    "message": "Fix OneFormer integration test (#38016)\n\n* Fix integration tests\n\n* format",
    "sha": "4220039b29f04f5ad4d8334b226a8d068dfb1afb",
    "files": [
        {
            "sha": "0ce791dd3c923c655f6960bd1712decd6a6c810d",
            "filename": "tests/models/oneformer/test_modeling_oneformer.py",
            "status": "modified",
            "additions": 20,
            "deletions": 30,
            "changes": 50,
            "blob_url": "https://github.com/huggingface/transformers/blob/4220039b29f04f5ad4d8334b226a8d068dfb1afb/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4220039b29f04f5ad4d8334b226a8d068dfb1afb/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Foneformer%2Ftest_modeling_oneformer.py?ref=4220039b29f04f5ad4d8334b226a8d068dfb1afb",
            "patch": "@@ -528,32 +528,22 @@ def test_inference_no_head(self):\n         with torch.no_grad():\n             outputs = model(**inputs)\n \n-        expected_slice_hidden_state = torch.tensor(\n-            [[0.2723, 0.8280, 0.6026], [1.2699, 1.1257, 1.1444], [1.1344, 0.6153, 0.4177]]\n-        ).to(torch_device)\n-        self.assertTrue(\n-            torch.allclose(\n-                outputs.encoder_hidden_states[-1][0, 0, :3, :3], expected_slice_hidden_state, atol=TOLERANCE\n-            )\n-        )\n-\n-        expected_slice_hidden_state = torch.tensor(\n-            [[1.0581, 1.2276, 1.2003], [1.1903, 1.2925, 1.2862], [1.158, 1.2559, 1.3216]]\n-        ).to(torch_device)\n-        self.assertTrue(\n-            torch.allclose(\n-                outputs.pixel_decoder_hidden_states[0][0, 0, :3, :3], expected_slice_hidden_state, atol=TOLERANCE\n-            )\n-        )\n-\n-        expected_slice_hidden_state = torch.tensor(\n-            [[3.0668, -1.1833, -5.1103], [3.344, -3.362, -5.1101], [2.6017, -4.3613, -4.1444]]\n-        ).to(torch_device)\n-        self.assertTrue(\n-            torch.allclose(\n-                outputs.transformer_decoder_class_predictions[0, :3, :3], expected_slice_hidden_state, atol=TOLERANCE\n-            )\n-        )\n+        expected_slice_hidden_state = [[0.2723, 0.8280, 0.6026], [1.2699, 1.1257, 1.1444], [1.1344, 0.6153, 0.4177]]\n+        expected_slice_hidden_state = torch.tensor(expected_slice_hidden_state).to(torch_device)\n+        slice_hidden_state = outputs.encoder_hidden_states[-1][0, 0, :3, :3]\n+        torch.testing.assert_close(slice_hidden_state, expected_slice_hidden_state, atol=TOLERANCE, rtol=TOLERANCE)\n+\n+        expected_slice_hidden_state = [[1.0581, 1.2276, 1.2003], [1.1903, 1.2925, 1.2862], [1.158, 1.2559, 1.3216]]\n+        expected_slice_hidden_state = torch.tensor(expected_slice_hidden_state).to(torch_device)\n+        slice_hidden_state = outputs.pixel_decoder_hidden_states[0][0, 0, :3, :3]\n+        torch.testing.assert_close(slice_hidden_state, expected_slice_hidden_state, atol=TOLERANCE, rtol=TOLERANCE)\n+\n+        # fmt: off\n+        expected_slice_hidden_state = [[3.0668, -1.1833, -5.1103], [3.344, -3.362, -5.1101], [2.6017, -4.3613, -4.1444]]\n+        expected_slice_hidden_state = torch.tensor(expected_slice_hidden_state).to(torch_device)\n+        slice_hidden_state = outputs.transformer_decoder_class_predictions[0, :3, :3]\n+        torch.testing.assert_close(slice_hidden_state, expected_slice_hidden_state, atol=TOLERANCE, rtol=TOLERANCE)\n+        # fmt: on\n \n     def test_inference_universal_segmentation_head(self):\n         model = OneFormerForUniversalSegmentation.from_pretrained(self.model_checkpoints).to(torch_device).eval()\n@@ -573,18 +563,18 @@ def test_inference_universal_segmentation_head(self):\n             masks_queries_logits.shape,\n             (1, model.config.num_queries, inputs_shape[-2] // 4, (inputs_shape[-1] + 2) // 4),\n         )\n-        expected_slice = [[[3.1848, 4.2141, 4.1993], [2.9000, 3.5721, 3.6603], [2.5358, 3.0883, 3.6168]]]\n+        expected_slice = [[3.1848, 4.2141, 4.1993], [2.9000, 3.5721, 3.6603], [2.5358, 3.0883, 3.6168]]\n         expected_slice = torch.tensor(expected_slice).to(torch_device)\n         torch.testing.assert_close(masks_queries_logits[0, 0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n+\n         # class_queries_logits\n         class_queries_logits = outputs.class_queries_logits\n         self.assertEqual(\n             class_queries_logits.shape,\n             (1, model.config.num_queries, model.config.num_labels + 1),\n         )\n-        expected_slice = torch.tensor(\n-            [[3.0668, -1.1833, -5.1103], [3.3440, -3.3620, -5.1101], [2.6017, -4.3613, -4.1444]]\n-        ).to(torch_device)\n+        expected_slice = [[3.0668, -1.1833, -5.1103], [3.3440, -3.3620, -5.1101], [2.6017, -4.3613, -4.1444]]\n+        expected_slice = torch.tensor(expected_slice).to(torch_device)\n         torch.testing.assert_close(class_queries_logits[0, :3, :3], expected_slice, rtol=TOLERANCE, atol=TOLERANCE)\n \n     @require_torch_accelerator"
        }
    ],
    "stats": {
        "total": 50,
        "additions": 20,
        "deletions": 30
    }
}