{
    "author": "xvyv99",
    "message": "Fix convert_internvl_weights_to_hf.py to support local paths (#38264)\n\nfix(internvl): add local path support to convert_internvl_weights_to_hf.py",
    "sha": "d0fccbf7ef4a856b10c15f5a112f91eb04aaf941",
    "files": [
        {
            "sha": "a14372666885a5c366fa2e0f5a53a58e0a0a0600",
            "filename": "src/transformers/models/internvl/convert_internvl_weights_to_hf.py",
            "status": "modified",
            "additions": 29,
            "deletions": 6,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0fccbf7ef4a856b10c15f5a112f91eb04aaf941/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0fccbf7ef4a856b10c15f5a112f91eb04aaf941/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fconvert_internvl_weights_to_hf.py?ref=d0fccbf7ef4a856b10c15f5a112f91eb04aaf941",
            "patch": "@@ -15,7 +15,7 @@\n import gc\n import os\n import re\n-from typing import Optional\n+from typing import Literal, Optional\n \n import torch\n from einops import rearrange\n@@ -124,6 +124,29 @@\n CONTEXT_LENGTH = 8192\n \n \n+def get_lm_type(path: str) -> Literal[\"qwen2\", \"llama\"]:\n+    \"\"\"\n+    Determine the type of language model (either 'qwen2' or 'llama') based on a given model path.\n+    \"\"\"\n+    if path not in LM_TYPE_CORRESPONDENCE.keys():\n+        base_config = AutoModel.from_pretrained(path, trust_remote_code=True).config\n+\n+        lm_arch = base_config.llm_config.architectures[0]\n+\n+        if lm_arch == \"InternLM2ForCausalLM\":\n+            lm_type = \"llama\"\n+        elif lm_arch == \"Qwen2ForCausalLM\":\n+            lm_type = \"qwen2\"\n+        else:\n+            raise ValueError(\n+                f\"Architecture '{lm_arch}' is not supported. Only 'Qwen2ForCausalLM' and 'InternLM2ForCausalLM' are recognized.\"\n+            )\n+    else:\n+        lm_type: Literal[\"qwen2\", \"llama\"] = LM_TYPE_CORRESPONDENCE[path]\n+\n+    return lm_type\n+\n+\n def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None, path: Optional[str] = None):\n     \"\"\"\n     This function should be applied only once, on the concatenated keys to efficiently rename using\n@@ -138,7 +161,7 @@ def convert_old_keys_to_new_keys(state_dict_keys: Optional[dict] = None, path: O\n         output_dict = dict(zip(old_text_vision.split(\"\\n\"), new_text.split(\"\\n\")))\n         old_text_language = \"\\n\".join([key for key in state_dict_keys if key.startswith(\"language_model\")])\n         new_text = old_text_language\n-        if LM_TYPE_CORRESPONDENCE[path] == \"llama\":\n+        if get_lm_type(path) == \"llama\":\n             for pattern, replacement in ORIGINAL_TO_CONVERTED_KEY_MAPPING_TEXT_LLAMA.items():\n                 new_text = re.sub(pattern, replacement, new_text)\n         elif LM_TYPE_CORRESPONDENCE[path] == \"qwen2\":\n@@ -177,7 +200,7 @@ def get_internvl_config(input_base_path):\n     llm_config = base_config.llm_config.to_dict()\n     vision_config = base_config.vision_config.to_dict()\n     vision_config[\"use_absolute_position_embeddings\"] = True\n-    if LM_TYPE_CORRESPONDENCE[input_base_path] == \"qwen2\":\n+    if get_lm_type(input_base_path) == \"qwen2\":\n         image_token_id = 151667\n         language_config_class = Qwen2Config\n     else:\n@@ -188,7 +211,7 @@ def get_internvl_config(input_base_path):\n     # Force use_cache to True\n     llm_config[\"use_cache\"] = True\n     # Force correct eos_token_id for InternVL3\n-    if \"InternVL3\" in input_base_path and LM_TYPE_CORRESPONDENCE[input_base_path] == \"qwen2\":\n+    if \"InternVL3\" in input_base_path and get_lm_type(input_base_path) == \"qwen2\":\n         llm_config[\"eos_token_id\"] = 151645\n \n     vision_config = {k: v for k, v in vision_config.items() if k not in UNNECESSARY_CONFIG_KEYS}\n@@ -299,7 +322,7 @@ def write_model(\n         processor.push_to_hub(hub_dir, use_temp_dir=True)\n \n     # generation config\n-    if LM_TYPE_CORRESPONDENCE[input_base_path] == \"llama\":\n+    if get_lm_type(input_base_path) == \"llama\":\n         print(\"Saving generation config...\")\n         # in the original model, eos_token is not the same in the text_config and the generation_config\n         # (\"</s>\" - 2 in the text_config and \"<|im_end|>\" - 92542 in the generation_config)\n@@ -323,7 +346,7 @@ def write_model(\n def write_tokenizer(\n     save_dir: str, push_to_hub: bool = False, path: Optional[str] = None, hub_dir: Optional[str] = None\n ):\n-    if LM_TYPE_CORRESPONDENCE[path] == \"qwen2\":\n+    if get_lm_type(path) == \"qwen2\":\n         tokenizer = AutoTokenizer.from_pretrained(\n             \"Qwen/Qwen2.5-VL-7B-Instruct\",\n             return_token_type_ids=False,"
        }
    ],
    "stats": {
        "total": 35,
        "additions": 29,
        "deletions": 6
    }
}