{
    "author": "remi-or",
    "message": "Switch to CB if cache_implementation == paged (#41655)\n\n* Add a switch to CB in case of paged cache\n\n* Added paged as a valid cache implem\n\n* Added a fallback on inputs_ids as a name\n\n* Rookie mistake\n\n* Removed paged from cache implems\n\n* Added warning about some  beam search args\n\n* Moved up CB warning",
    "sha": "0215846d98e25a39952530cbdf7cc42bd4515324",
    "files": [
        {
            "sha": "86d02aa2f8e20777da04f403b058986820742085",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 2,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0215846d98e25a39952530cbdf7cc42bd4515324/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0215846d98e25a39952530cbdf7cc42bd4515324/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=0215846d98e25a39952530cbdf7cc42bd4515324",
            "patch": "@@ -556,10 +556,13 @@ def validate(self, strict=False):\n                 \"`model.generation_config.pad_token_id=PAD_TOKEN_ID` to avoid errors in generation\"\n             )\n         # 1.2. Cache attributes\n-        if self.cache_implementation is not None and self.cache_implementation not in ALL_CACHE_IMPLEMENTATIONS:\n+        # \"paged\" re-routes to continuous batching and so it is a valid cache implementation. But we do not want to test\n+        # it with the `generate` as the other would be, so we we cannot add it to ALL_CACHE_IMPLEMENTATIONS\n+        valid_cache_implementations = ALL_CACHE_IMPLEMENTATIONS + (\"paged\",)\n+        if self.cache_implementation is not None and self.cache_implementation not in valid_cache_implementations:\n             raise ValueError(\n                 f\"Invalid `cache_implementation` ({self.cache_implementation}). Choose one of: \"\n-                f\"{ALL_CACHE_IMPLEMENTATIONS}\"\n+                f\"{valid_cache_implementations}\"\n             )\n         # 1.3. Performance attributes\n         if self.compile_config is not None and not isinstance(self.compile_config, CompileConfig):"
        },
        {
            "sha": "d44c22c7cc135bbf11d62bd5c2ab15bba1c81ac4",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 70,
            "deletions": 1,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/0215846d98e25a39952530cbdf7cc42bd4515324/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0215846d98e25a39952530cbdf7cc42bd4515324/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=0215846d98e25a39952530cbdf7cc42bd4515324",
            "patch": "@@ -2370,7 +2370,7 @@ def generate(\n                     - [`~generation.GenerateEncoderDecoderOutput`],\n                     - [`~generation.GenerateBeamEncoderDecoderOutput`]\n         \"\"\"\n-        # 0. If requested, load an arbitrary generation recipe from the Hub and run it instead\n+        # 0.a. If requested, load an arbitrary generation recipe from the Hub and run it instead\n         trust_remote_code = kwargs.pop(\"trust_remote_code\", None)\n \n         if custom_generate is not None and isinstance(custom_generate, str):\n@@ -2392,6 +2392,75 @@ def generate(\n             )\n             return custom_generate_function(model=self, **generate_arguments)\n \n+        # 0.b. If requested, switched to continuous batching generation\n+        if kwargs.get(\"cache_implementation\") == \"paged\":\n+            logger.warning(\n+                \"Detected cache_implementation=paged: switching to continuous batching. You should consider using \"\n+                \"generate_batch directly instead.\"\n+            )\n+\n+            # generate_batch expects a list of lists of ints, so we create it from the inputs or input_ids\n+            inputs = inputs if inputs is not None else kwargs.get(\"input_ids\")\n+            if inputs is None:\n+                raise ValueError(\"inputs or input_ids must be provided for CB generation.\")\n+\n+            if inputs.dim() == 1:\n+                inputs = inputs.unsqueeze(0).tolist()\n+            elif inputs.dim() == 2:\n+                inputs = inputs.tolist()\n+            else:\n+                raise ValueError(f\"inputs must be a 1D or 2D tensor, got {inputs.dim() = }\")\n+\n+            # some arguments are not supported for continuous batching\n+            if stopping_criteria is not None:\n+                raise NotImplementedError(\n+                    f\"stopping_criteria is not supported for continuous batching. Got {stopping_criteria = }\"\n+                )\n+            if prefix_allowed_tokens_fn is not None:\n+                raise NotImplementedError(\n+                    f\"prefix_allowed_tokens_fn is not supported for continuous batching. Got {prefix_allowed_tokens_fn = }\"\n+                )\n+            if assistant_model is not None:\n+                raise NotImplementedError(\n+                    f\"assistant_model is not supported for continuous batching. Got {assistant_model = }\"\n+                )\n+            if streamer is not None:  # TODO: actualy this could be supported\n+                raise NotImplementedError(f\"streaming is not supported for continuous batching. Got {streamer = }\")\n+            if negative_prompt_ids is not None:\n+                raise NotImplementedError(\n+                    f\"negative_prompt_ids is not supported for continuous batching. Got {negative_prompt_ids = }\"\n+                )\n+            if negative_prompt_attention_mask is not None:\n+                raise NotImplementedError(\n+                    f\"negative_prompt_attention_mask is not supported for continuous batching. Got {negative_prompt_attention_mask = }\"\n+                )\n+\n+            # others are ignored\n+            if synced_gpus is not None:\n+                logger.warning(f\"synced_gpus is not ignored for continuous batching. Got {synced_gpus = }\")\n+            num_return_sequences = kwargs.get(\"num_return_sequences\", 1)\n+            num_beams = kwargs.get(\"num_beams\", 1)\n+            if num_return_sequences > 1 or num_beams > 1:  # FIXME: remove this once CB supports it (which is planned)\n+                logger.warning(\n+                    f\"num_return_sequences and num_beams are not supported for continuous batching yet. \"\n+                    f\"Got {num_return_sequences = } and {num_beams = }. \"\n+                )\n+\n+            # switch to CB\n+            outputs = self.generate_batch(\n+                inputs=inputs,\n+                generation_config=self._prepare_generation_config(generation_config, use_model_defaults, **kwargs)[0],\n+                **kwargs,\n+            )\n+            sequences = [\n+                outputs[f\"req_{i}\"].prompt_ids + outputs[f\"req_{i}\"].generated_tokens for i in range(len(outputs))\n+            ]\n+\n+            # To use the same indexing (outputs[0]) as the regular generate method, we unsqueeze the tensor\n+            sequences_as_tensor = torch.tensor(sequences, dtype=torch.long, device=self.device)\n+            sequences_as_tensor = sequences_as_tensor.unsqueeze(0)\n+            return sequences_as_tensor\n+\n         # 1. Handle kwargs, `generation_config`, validate them and obtain generation mode\n         generation_mode_kwargs = self._extract_generation_mode_kwargs(\n             custom_generate,"
        }
    ],
    "stats": {
        "total": 78,
        "additions": 75,
        "deletions": 3
    }
}