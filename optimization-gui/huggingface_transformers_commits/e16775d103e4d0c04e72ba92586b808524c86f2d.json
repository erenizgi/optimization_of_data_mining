{
    "author": "rootonchair",
    "message": "Add Fast Image Processor for LayoutLMv2 (#37203)\n\n* add support layoutlmv2\n\n* make style\n\n* Apply suggestions from code review\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n* add warning and clean up\n\n* make style\n\n* Update src/transformers/models/layoutlmv2/image_processing_layoutlmv2_fast.py\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "e16775d103e4d0c04e72ba92586b808524c86f2d",
    "files": [
        {
            "sha": "af2068757192c30f65a79987ed70c4eebeb810e0",
            "filename": "docs/source/en/model_doc/layoutlmv2.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/e16775d103e4d0c04e72ba92586b808524c86f2d/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/e16775d103e4d0c04e72ba92586b808524c86f2d/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Flayoutlmv2.md?ref=e16775d103e4d0c04e72ba92586b808524c86f2d",
            "patch": "@@ -310,6 +310,11 @@ print(encoding.keys())\n [[autodoc]] LayoutLMv2ImageProcessor\n     - preprocess\n \n+## LayoutLMv2ImageProcessorFast\n+\n+[[autodoc]] LayoutLMv2ImageProcessorFast\n+    - preprocess\n+\n ## LayoutLMv2Tokenizer\n \n [[autodoc]] LayoutLMv2Tokenizer"
        },
        {
            "sha": "7e8ef1c750ff2a552f54ef97b73a2927f869d441",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/e16775d103e4d0c04e72ba92586b808524c86f2d/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e16775d103e4d0c04e72ba92586b808524c86f2d/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=e16775d103e4d0c04e72ba92586b808524c86f2d",
            "patch": "@@ -102,7 +102,7 @@\n             (\"instructblip\", (\"BlipImageProcessor\", \"BlipImageProcessorFast\")),\n             (\"instructblipvideo\", (\"InstructBlipVideoImageProcessor\",)),\n             (\"kosmos-2\", (\"CLIPImageProcessor\", \"CLIPImageProcessorFast\")),\n-            (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\",)),\n+            (\"layoutlmv2\", (\"LayoutLMv2ImageProcessor\", \"LayoutLMv2ImageProcessorFast\")),\n             (\"layoutlmv3\", (\"LayoutLMv3ImageProcessor\",)),\n             (\"levit\", (\"LevitImageProcessor\",)),\n             (\"llama4\", (\"Llama4ImageProcessor\", \"Llama4ImageProcessorFast\")),"
        },
        {
            "sha": "b68a523c0b0c362d9930f5bee492cea73f3937f0",
            "filename": "src/transformers/models/layoutlmv2/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/e16775d103e4d0c04e72ba92586b808524c86f2d/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e16775d103e4d0c04e72ba92586b808524c86f2d/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2F__init__.py?ref=e16775d103e4d0c04e72ba92586b808524c86f2d",
            "patch": "@@ -21,6 +21,7 @@\n     from .configuration_layoutlmv2 import *\n     from .feature_extraction_layoutlmv2 import *\n     from .image_processing_layoutlmv2 import *\n+    from .image_processing_layoutlmv2_fast import *\n     from .modeling_layoutlmv2 import *\n     from .processing_layoutlmv2 import *\n     from .tokenization_layoutlmv2 import *"
        },
        {
            "sha": "937ceed3a2cf30dae7ff22359714af6a41a64ad7",
            "filename": "src/transformers/models/layoutlmv2/image_processing_layoutlmv2_fast.py",
            "status": "added",
            "additions": 164,
            "deletions": 0,
            "changes": 164,
            "blob_url": "https://github.com/huggingface/transformers/blob/e16775d103e4d0c04e72ba92586b808524c86f2d/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e16775d103e4d0c04e72ba92586b808524c86f2d/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flayoutlmv2%2Fimage_processing_layoutlmv2_fast.py?ref=e16775d103e4d0c04e72ba92586b808524c86f2d",
            "patch": "@@ -0,0 +1,164 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for LayoutLMv2.\"\"\"\n+\n+from typing import Optional, Union\n+\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    BatchFeature,\n+    DefaultFastImageProcessorKwargs,\n+)\n+from ...image_transforms import ChannelDimension, group_images_by_shape, reorder_images\n+from ...image_utils import ImageInput, PILImageResampling, SizeDict\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+    logging,\n+    requires_backends,\n+)\n+from .image_processing_layoutlmv2 import apply_tesseract\n+\n+\n+logger = logging.get_logger(__name__)\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class LayoutLMv2FastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    apply_ocr: Optional[bool]\n+    ocr_lang: Optional[str]\n+    tesseract_config: Optional[str]\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast LayoutLMv2 image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        apply_ocr (`bool`, *optional*, defaults to `True`):\n+            Whether to apply the Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden by\n+            the `apply_ocr` parameter in the `preprocess` method.\n+        ocr_lang (`str`, *optional*):\n+            The language, specified by its ISO code, to be used by the Tesseract OCR engine. By default, English is\n+            used. Can be overridden by the `ocr_lang` parameter in the `preprocess` method.\n+        tesseract_config (`str`, *optional*):\n+            Any additional custom configuration flags that are forwarded to the `config` parameter when calling\n+            Tesseract. For example: '--psm 6'. Can be overridden by the `tesseract_config` parameter in the\n+            `preprocess` method.\n+    \"\"\",\n+)\n+class LayoutLMv2ImageProcessorFast(BaseImageProcessorFast):\n+    resample = PILImageResampling.BILINEAR\n+    size = {\"height\": 224, \"width\": 224}\n+    rescale_factor = None\n+    do_resize = True\n+    apply_ocr = True\n+    ocr_lang = None\n+    tesseract_config = \"\"\n+    valid_kwargs = LayoutLMv2FastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[LayoutLMv2FastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+            apply_ocr (`bool`, *optional*, defaults to `True`):\n+                Whether to apply the Tesseract OCR engine to get words + normalized bounding boxes. Can be overridden by\n+                the `apply_ocr` parameter in the `preprocess` method.\n+            ocr_lang (`str`, *optional*):\n+                The language, specified by its ISO code, to be used by the Tesseract OCR engine. By default, English is\n+                used. Can be overridden by the `ocr_lang` parameter in the `preprocess` method.\n+            tesseract_config (`str`, *optional*):\n+                Any additional custom configuration flags that are forwarded to the `config` parameter when calling\n+                Tesseract. For example: '--psm 6'. Can be overridden by the `tesseract_config` parameter in the\n+                `preprocess` method.\n+        \"\"\",\n+    )\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[LayoutLMv2FastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def _preprocess(\n+        self,\n+        images: list[\"torch.Tensor\"],\n+        do_resize: bool,\n+        size: SizeDict,\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        apply_ocr: bool,\n+        ocr_lang: Optional[str],\n+        tesseract_config: Optional[str],\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        # Tesseract OCR to get words + normalized bounding boxes\n+        if apply_ocr:\n+            requires_backends(self, \"pytesseract\")\n+            words_batch = []\n+            boxes_batch = []\n+            for image in images:\n+                if image.is_cuda:\n+                    logger.warning_once(\n+                        \"apply_ocr can only be performed on cpu. Tensors will be transferred to cpu before processing.\"\n+                    )\n+                words, boxes = apply_tesseract(\n+                    image.cpu(), ocr_lang, tesseract_config, input_data_format=ChannelDimension.FIRST\n+                )\n+                words_batch.append(words)\n+                boxes_batch.append(boxes)\n+\n+        # Group images by size for batched resizing\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        resized_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_resize:\n+                stacked_images = self.resize(image=stacked_images, size=size, interpolation=interpolation)\n+            resized_images_grouped[shape] = stacked_images\n+        resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n+\n+        # Group images by size for further processing\n+        # Needed in case do_resize is False, or resize returns images with different sizes\n+        grouped_images, grouped_images_index = group_images_by_shape(resized_images)\n+        processed_images_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            # flip color channels from RGB to BGR (as Detectron2 requires this)\n+            stacked_images = stacked_images.flip(1)\n+            processed_images_grouped[shape] = stacked_images\n+\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        data = BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+        if apply_ocr:\n+            data[\"words\"] = words_batch\n+            data[\"boxes\"] = boxes_batch\n+\n+        return data\n+\n+\n+__all__ = [\"LayoutLMv2ImageProcessorFast\"]"
        },
        {
            "sha": "01d0f45398ceb7589f2c7d3f536b90d70305366d",
            "filename": "tests/models/layoutlmv2/test_image_processing_layoutlmv2.py",
            "status": "modified",
            "additions": 104,
            "deletions": 29,
            "changes": 133,
            "blob_url": "https://github.com/huggingface/transformers/blob/e16775d103e4d0c04e72ba92586b808524c86f2d/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/e16775d103e4d0c04e72ba92586b808524c86f2d/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_image_processing_layoutlmv2.py?ref=e16775d103e4d0c04e72ba92586b808524c86f2d",
            "patch": "@@ -12,20 +12,27 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n import unittest\n \n-from transformers.testing_utils import require_pytesseract, require_torch\n-from transformers.utils import is_pytesseract_available\n+import requests\n+\n+from transformers.testing_utils import require_pytesseract, require_torch, require_vision\n+from transformers.utils import is_pytesseract_available, is_torch_available, is_torchvision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n \n+if is_torch_available():\n+    import torch\n+\n if is_pytesseract_available():\n     from PIL import Image\n \n     from transformers import LayoutLMv2ImageProcessor\n \n+    if is_torchvision_available():\n+        from transformers import LayoutLMv2ImageProcessorFast\n+\n \n class LayoutLMv2ImageProcessingTester:\n     def __init__(\n@@ -73,6 +80,9 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_pytesseract\n class LayoutLMv2ImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = LayoutLMv2ImageProcessor if is_pytesseract_available() else None\n+    fast_image_processing_class = (\n+        LayoutLMv2ImageProcessorFast if (is_torchvision_available() and is_pytesseract_available()) else None\n+    )\n \n     def setUp(self):\n         super().setUp()\n@@ -83,46 +93,111 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processing = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processing, \"do_resize\"))\n-        self.assertTrue(hasattr(image_processing, \"size\"))\n-        self.assertTrue(hasattr(image_processing, \"apply_ocr\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_resize\"))\n+            self.assertTrue(hasattr(image_processing, \"size\"))\n+            self.assertTrue(hasattr(image_processing, \"apply_ocr\"))\n \n     def test_image_processor_from_dict_with_kwargs(self):\n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict)\n-        self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n+        for image_processing_class in self.image_processor_list:\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict)\n+            self.assertEqual(image_processor.size, {\"height\": 18, \"width\": 18})\n \n-        image_processor = self.image_processing_class.from_dict(self.image_processor_dict, size=42)\n-        self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n+            image_processor = image_processing_class.from_dict(self.image_processor_dict, size=42)\n+            self.assertEqual(image_processor.size, {\"height\": 42, \"width\": 42})\n \n     @unittest.skip(reason=\"Tesseract version is not correct in ci. @Arthur FIXME\")\n     def test_layoutlmv2_integration_test(self):\n-        # with apply_OCR = True\n-        image_processing = LayoutLMv2ImageProcessor()\n-\n         from datasets import load_dataset\n \n         ds = load_dataset(\"hf-internal-testing/fixtures_docvqa\", split=\"test\", trust_remote_code=True)\n \n-        image = Image.open(ds[0][\"file\"]).convert(\"RGB\")\n+        for image_processing_class in self.image_processor_list:\n+            # with apply_OCR = True\n+            image_processing = image_processing_class()\n+\n+            image = Image.open(ds[0][\"file\"]).convert(\"RGB\")\n+\n+            encoding = image_processing(image, return_tensors=\"pt\")\n+\n+            self.assertEqual(encoding.pixel_values.shape, (1, 3, 224, 224))\n+            self.assertEqual(len(encoding.words), len(encoding.boxes))\n+\n+            # fmt: off\n+            # the words and boxes were obtained with Tesseract 5.3.0\n+            expected_words = [['11:14', 'to', '11:39', 'a.m', '11:39', 'to', '11:44', 'a.m.', '11:44', 'a.m.', 'to', '12:25', 'p.m.', '12:25', 'to', '12:58', 'p.m.', '12:58', 'to', '4:00', 'p.m.', '2:00', 'to', '5:00', 'p.m.', 'Coffee', 'Break', 'Coffee', 'will', 'be', 'served', 'for', 'men', 'and', 'women', 'in', 'the', 'lobby', 'adjacent', 'to', 'exhibit', 'area.', 'Please', 'move', 'into', 'exhibit', 'area.', '(Exhibits', 'Open)', 'TRRF', 'GENERAL', 'SESSION', '(PART', '|)', 'Presiding:', 'Lee', 'A.', 'Waller', 'TRRF', 'Vice', 'President', '“Introductory', 'Remarks”', 'Lee', 'A.', 'Waller,', 'TRRF', 'Vice', 'Presi-', 'dent', 'Individual', 'Interviews', 'with', 'TRRF', 'Public', 'Board', 'Members', 'and', 'Sci-', 'entific', 'Advisory', 'Council', 'Mem-', 'bers', 'Conducted', 'by', 'TRRF', 'Treasurer', 'Philip', 'G.', 'Kuehn', 'to', 'get', 'answers', 'which', 'the', 'public', 'refrigerated', 'warehousing', 'industry', 'is', 'looking', 'for.', 'Plus', 'questions', 'from', 'the', 'floor.', 'Dr.', 'Emil', 'M.', 'Mrak,', 'University', 'of', 'Cal-', 'ifornia,', 'Chairman,', 'TRRF', 'Board;', 'Sam', 'R.', 'Cecil,', 'University', 'of', 'Georgia', 'College', 'of', 'Agriculture;', 'Dr.', 'Stanley', 'Charm,', 'Tufts', 'University', 'School', 'of', 'Medicine;', 'Dr.', 'Robert', 'H.', 'Cotton,', 'ITT', 'Continental', 'Baking', 'Company;', 'Dr.', 'Owen', 'Fennema,', 'University', 'of', 'Wis-', 'consin;', 'Dr.', 'Robert', 'E.', 'Hardenburg,', 'USDA.', 'Questions', 'and', 'Answers', 'Exhibits', 'Open', 'Capt.', 'Jack', 'Stoney', 'Room', 'TRRF', 'Scientific', 'Advisory', 'Council', 'Meeting', 'Ballroom', 'Foyer']]  # noqa: E231\n+            expected_boxes = [[[141, 57, 210, 69], [228, 58, 252, 69], [141, 75, 216, 88], [230, 79, 280, 88], [142, 260, 218, 273], [230, 261, 255, 273], [143, 279, 218, 290], [231, 282, 290, 291], [143, 342, 218, 354], [231, 345, 289, 355], [202, 362, 227, 373], [143, 379, 220, 392], [231, 382, 291, 394], [144, 714, 220, 726], [231, 715, 256, 726], [144, 732, 220, 745], [232, 736, 291, 747], [144, 769, 218, 782], [231, 770, 256, 782], [141, 788, 202, 801], [215, 791, 274, 804], [143, 826, 204, 838], [215, 826, 240, 838], [142, 844, 202, 857], [215, 847, 274, 859], [334, 57, 427, 69], [440, 57, 522, 69], [369, 75, 461, 88], [469, 75, 516, 88], [528, 76, 562, 88], [570, 76, 667, 88], [675, 75, 711, 87], [721, 79, 778, 88], [789, 75, 840, 88], [369, 97, 470, 107], [484, 94, 507, 106], [518, 94, 562, 107], [576, 94, 655, 110], [668, 94, 792, 109], [804, 95, 829, 107], [369, 113, 465, 125], [477, 116, 547, 125], [562, 113, 658, 125], [671, 116, 748, 125], [761, 113, 811, 125], [369, 131, 465, 143], [477, 133, 548, 143], [563, 130, 698, 145], [710, 130, 802, 146], [336, 171, 412, 183], [423, 171, 572, 183], [582, 170, 716, 184], [728, 171, 817, 187], [829, 171, 844, 186], [338, 197, 482, 212], [507, 196, 557, 209], [569, 196, 595, 208], [610, 196, 702, 209], [505, 214, 583, 226], [595, 214, 656, 227], [670, 215, 807, 227], [335, 259, 543, 274], [556, 259, 708, 272], [372, 279, 422, 291], [435, 279, 460, 291], [474, 279, 574, 292], [587, 278, 664, 291], [676, 278, 738, 291], [751, 279, 834, 291], [372, 298, 434, 310], [335, 341, 483, 354], [497, 341, 655, 354], [667, 341, 728, 354], [740, 341, 825, 354], [335, 360, 430, 372], [442, 360, 534, 372], [545, 359, 687, 372], [697, 360, 754, 372], [765, 360, 823, 373], [334, 378, 428, 391], [440, 378, 577, 394], [590, 378, 705, 391], [720, 378, 801, 391], [334, 397, 400, 409], [370, 416, 529, 429], [544, 416, 576, 432], [587, 416, 665, 428], [677, 416, 814, 429], [372, 435, 452, 450], [465, 434, 495, 447], [511, 434, 600, 447], [611, 436, 637, 447], [649, 436, 694, 451], [705, 438, 824, 447], [369, 453, 452, 466], [464, 454, 509, 466], [522, 453, 611, 469], [625, 453, 792, 469], [370, 472, 556, 488], [570, 472, 684, 487], [697, 472, 718, 485], [732, 472, 835, 488], [369, 490, 411, 503], [425, 490, 484, 503], [496, 490, 635, 506], [645, 490, 707, 503], [718, 491, 761, 503], [771, 490, 840, 503], [336, 510, 374, 521], [388, 510, 447, 522], [460, 510, 489, 521], [503, 510, 580, 522], [592, 509, 736, 525], [745, 509, 770, 522], [781, 509, 840, 522], [338, 528, 434, 541], [448, 528, 596, 541], [609, 527, 687, 540], [700, 528, 792, 541], [336, 546, 397, 559], [407, 546, 431, 559], [443, 546, 525, 560], [537, 546, 680, 562], [695, 546, 714, 559], [722, 546, 837, 562], [336, 565, 449, 581], [461, 565, 485, 577], [497, 565, 665, 581], [681, 565, 718, 577], [732, 565, 837, 580], [337, 584, 438, 597], [452, 583, 521, 596], [535, 584, 677, 599], [690, 583, 787, 596], [801, 583, 825, 596], [338, 602, 478, 615], [492, 602, 530, 614], [543, 602, 638, 615], [650, 602, 676, 614], [688, 602, 788, 615], [802, 602, 843, 614], [337, 621, 502, 633], [516, 621, 615, 637], [629, 621, 774, 636], [789, 621, 827, 633], [337, 639, 418, 652], [432, 640, 571, 653], [587, 639, 731, 655], [743, 639, 769, 652], [780, 639, 841, 652], [338, 658, 440, 673], [455, 658, 491, 670], [508, 658, 602, 671], [616, 658, 638, 670], [654, 658, 835, 674], [337, 677, 429, 689], [337, 714, 482, 726], [495, 714, 548, 726], [561, 714, 683, 726], [338, 770, 461, 782], [474, 769, 554, 785], [489, 788, 562, 803], [576, 788, 643, 801], [656, 787, 751, 804], [764, 788, 844, 801], [334, 825, 421, 838], [430, 824, 574, 838], [584, 824, 723, 841], [335, 844, 450, 857], [464, 843, 583, 860], [628, 862, 755, 875], [769, 861, 848, 878]]]  # noqa: E231\n+            # fmt: on\n+\n+            self.assertListEqual(encoding.words, expected_words)\n+            self.assertListEqual(encoding.boxes, expected_boxes)\n \n-        encoding = image_processing(image, return_tensors=\"pt\")\n+            # with apply_OCR = False\n+            image_processing = image_processing_class(apply_ocr=False)\n \n-        self.assertEqual(encoding.pixel_values.shape, (1, 3, 224, 224))\n-        self.assertEqual(len(encoding.words), len(encoding.boxes))\n+            encoding = image_processing(image, return_tensors=\"pt\")\n \n-        # fmt: off\n-        # the words and boxes were obtained with Tesseract 5.3.0\n-        expected_words = [['11:14', 'to', '11:39', 'a.m', '11:39', 'to', '11:44', 'a.m.', '11:44', 'a.m.', 'to', '12:25', 'p.m.', '12:25', 'to', '12:58', 'p.m.', '12:58', 'to', '4:00', 'p.m.', '2:00', 'to', '5:00', 'p.m.', 'Coffee', 'Break', 'Coffee', 'will', 'be', 'served', 'for', 'men', 'and', 'women', 'in', 'the', 'lobby', 'adjacent', 'to', 'exhibit', 'area.', 'Please', 'move', 'into', 'exhibit', 'area.', '(Exhibits', 'Open)', 'TRRF', 'GENERAL', 'SESSION', '(PART', '|)', 'Presiding:', 'Lee', 'A.', 'Waller', 'TRRF', 'Vice', 'President', '“Introductory', 'Remarks”', 'Lee', 'A.', 'Waller,', 'TRRF', 'Vice', 'Presi-', 'dent', 'Individual', 'Interviews', 'with', 'TRRF', 'Public', 'Board', 'Members', 'and', 'Sci-', 'entific', 'Advisory', 'Council', 'Mem-', 'bers', 'Conducted', 'by', 'TRRF', 'Treasurer', 'Philip', 'G.', 'Kuehn', 'to', 'get', 'answers', 'which', 'the', 'public', 'refrigerated', 'warehousing', 'industry', 'is', 'looking', 'for.', 'Plus', 'questions', 'from', 'the', 'floor.', 'Dr.', 'Emil', 'M.', 'Mrak,', 'University', 'of', 'Cal-', 'ifornia,', 'Chairman,', 'TRRF', 'Board;', 'Sam', 'R.', 'Cecil,', 'University', 'of', 'Georgia', 'College', 'of', 'Agriculture;', 'Dr.', 'Stanley', 'Charm,', 'Tufts', 'University', 'School', 'of', 'Medicine;', 'Dr.', 'Robert', 'H.', 'Cotton,', 'ITT', 'Continental', 'Baking', 'Company;', 'Dr.', 'Owen', 'Fennema,', 'University', 'of', 'Wis-', 'consin;', 'Dr.', 'Robert', 'E.', 'Hardenburg,', 'USDA.', 'Questions', 'and', 'Answers', 'Exhibits', 'Open', 'Capt.', 'Jack', 'Stoney', 'Room', 'TRRF', 'Scientific', 'Advisory', 'Council', 'Meeting', 'Ballroom', 'Foyer']]  # noqa: E231\n-        expected_boxes = [[[141, 57, 210, 69], [228, 58, 252, 69], [141, 75, 216, 88], [230, 79, 280, 88], [142, 260, 218, 273], [230, 261, 255, 273], [143, 279, 218, 290], [231, 282, 290, 291], [143, 342, 218, 354], [231, 345, 289, 355], [202, 362, 227, 373], [143, 379, 220, 392], [231, 382, 291, 394], [144, 714, 220, 726], [231, 715, 256, 726], [144, 732, 220, 745], [232, 736, 291, 747], [144, 769, 218, 782], [231, 770, 256, 782], [141, 788, 202, 801], [215, 791, 274, 804], [143, 826, 204, 838], [215, 826, 240, 838], [142, 844, 202, 857], [215, 847, 274, 859], [334, 57, 427, 69], [440, 57, 522, 69], [369, 75, 461, 88], [469, 75, 516, 88], [528, 76, 562, 88], [570, 76, 667, 88], [675, 75, 711, 87], [721, 79, 778, 88], [789, 75, 840, 88], [369, 97, 470, 107], [484, 94, 507, 106], [518, 94, 562, 107], [576, 94, 655, 110], [668, 94, 792, 109], [804, 95, 829, 107], [369, 113, 465, 125], [477, 116, 547, 125], [562, 113, 658, 125], [671, 116, 748, 125], [761, 113, 811, 125], [369, 131, 465, 143], [477, 133, 548, 143], [563, 130, 698, 145], [710, 130, 802, 146], [336, 171, 412, 183], [423, 171, 572, 183], [582, 170, 716, 184], [728, 171, 817, 187], [829, 171, 844, 186], [338, 197, 482, 212], [507, 196, 557, 209], [569, 196, 595, 208], [610, 196, 702, 209], [505, 214, 583, 226], [595, 214, 656, 227], [670, 215, 807, 227], [335, 259, 543, 274], [556, 259, 708, 272], [372, 279, 422, 291], [435, 279, 460, 291], [474, 279, 574, 292], [587, 278, 664, 291], [676, 278, 738, 291], [751, 279, 834, 291], [372, 298, 434, 310], [335, 341, 483, 354], [497, 341, 655, 354], [667, 341, 728, 354], [740, 341, 825, 354], [335, 360, 430, 372], [442, 360, 534, 372], [545, 359, 687, 372], [697, 360, 754, 372], [765, 360, 823, 373], [334, 378, 428, 391], [440, 378, 577, 394], [590, 378, 705, 391], [720, 378, 801, 391], [334, 397, 400, 409], [370, 416, 529, 429], [544, 416, 576, 432], [587, 416, 665, 428], [677, 416, 814, 429], [372, 435, 452, 450], [465, 434, 495, 447], [511, 434, 600, 447], [611, 436, 637, 447], [649, 436, 694, 451], [705, 438, 824, 447], [369, 453, 452, 466], [464, 454, 509, 466], [522, 453, 611, 469], [625, 453, 792, 469], [370, 472, 556, 488], [570, 472, 684, 487], [697, 472, 718, 485], [732, 472, 835, 488], [369, 490, 411, 503], [425, 490, 484, 503], [496, 490, 635, 506], [645, 490, 707, 503], [718, 491, 761, 503], [771, 490, 840, 503], [336, 510, 374, 521], [388, 510, 447, 522], [460, 510, 489, 521], [503, 510, 580, 522], [592, 509, 736, 525], [745, 509, 770, 522], [781, 509, 840, 522], [338, 528, 434, 541], [448, 528, 596, 541], [609, 527, 687, 540], [700, 528, 792, 541], [336, 546, 397, 559], [407, 546, 431, 559], [443, 546, 525, 560], [537, 546, 680, 562], [695, 546, 714, 559], [722, 546, 837, 562], [336, 565, 449, 581], [461, 565, 485, 577], [497, 565, 665, 581], [681, 565, 718, 577], [732, 565, 837, 580], [337, 584, 438, 597], [452, 583, 521, 596], [535, 584, 677, 599], [690, 583, 787, 596], [801, 583, 825, 596], [338, 602, 478, 615], [492, 602, 530, 614], [543, 602, 638, 615], [650, 602, 676, 614], [688, 602, 788, 615], [802, 602, 843, 614], [337, 621, 502, 633], [516, 621, 615, 637], [629, 621, 774, 636], [789, 621, 827, 633], [337, 639, 418, 652], [432, 640, 571, 653], [587, 639, 731, 655], [743, 639, 769, 652], [780, 639, 841, 652], [338, 658, 440, 673], [455, 658, 491, 670], [508, 658, 602, 671], [616, 658, 638, 670], [654, 658, 835, 674], [337, 677, 429, 689], [337, 714, 482, 726], [495, 714, 548, 726], [561, 714, 683, 726], [338, 770, 461, 782], [474, 769, 554, 785], [489, 788, 562, 803], [576, 788, 643, 801], [656, 787, 751, 804], [764, 788, 844, 801], [334, 825, 421, 838], [430, 824, 574, 838], [584, 824, 723, 841], [335, 844, 450, 857], [464, 843, 583, 860], [628, 862, 755, 875], [769, 861, 848, 878]]]  # noqa: E231\n-        # fmt: on\n+            self.assertEqual(encoding.pixel_values.shape, (1, 3, 224, 224))\n \n-        self.assertListEqual(encoding.words, expected_words)\n-        self.assertListEqual(encoding.boxes, expected_boxes)\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n \n-        # with apply_OCR = False\n-        image_processing = LayoutLMv2ImageProcessor(apply_ocr=False)\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        encoding = image_processing(image, return_tensors=\"pt\")\n+        dummy_image = Image.open(\n+            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n+        )\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_image, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_image, return_tensors=\"pt\")\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_slow.pixel_values.float() / 255, encoding_fast.pixel_values.float() / 255, atol=1e-1\n+            )\n+        )\n+        self.assertLessEqual(\n+            torch.mean(\n+                torch.abs(encoding_slow.pixel_values.float() - encoding_fast.pixel_values.float()) / 255\n+            ).item(),\n+            1e-3,\n+        )\n+\n+    @require_vision\n+    @require_torch\n+    def test_slow_fast_equivalence_batched(self):\n+        if not self.test_slow_image_processor or not self.test_fast_image_processor:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test\")\n+\n+        if self.image_processing_class is None or self.fast_image_processing_class is None:\n+            self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        self.assertEqual(encoding.pixel_values.shape, (1, 3, 224, 224))\n+        if hasattr(self.image_processor_tester, \"do_center_crop\") and self.image_processor_tester.do_center_crop:\n+            self.skipTest(\n+                reason=\"Skipping as do_center_crop is True and center_crop functions are not equivalent for fast and slow processors\"\n+            )\n+\n+        dummy_images = self.image_processor_tester.prepare_image_inputs(equal_resolution=False, torchify=True)\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoding_slow = image_processor_slow(dummy_images, return_tensors=\"pt\")\n+        encoding_fast = image_processor_fast(dummy_images, return_tensors=\"pt\")\n+\n+        self.assertTrue(\n+            torch.allclose(\n+                encoding_slow.pixel_values.float() / 255, encoding_fast.pixel_values.float() / 255, atol=1e-1\n+            )\n+        )\n+        self.assertLessEqual(\n+            torch.mean(\n+                torch.abs(encoding_slow.pixel_values.float() - encoding_fast.pixel_values.float()) / 255\n+            ).item(),\n+            1e-3,\n+        )"
        }
    ],
    "stats": {
        "total": 305,
        "additions": 275,
        "deletions": 30
    }
}