{
    "author": "remi-or",
    "message": "Gemma3 hybrid fix (#42287)\n\n* Fix gemma3 on H100\n\n* Partial fixes for Mi325\n\n* First half of A10 fix\n\n* Final A10 fix",
    "sha": "3410ba9bab09ca6dadc130ab29fabfaba8baa131",
    "files": [
        {
            "sha": "17e9d9991bd493dca9e2bb77bbac3402fb1d1e15",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 16,
            "deletions": 30,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/3410ba9bab09ca6dadc130ab29fabfaba8baa131/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3410ba9bab09ca6dadc130ab29fabfaba8baa131/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=3410ba9bab09ca6dadc130ab29fabfaba8baa131",
            "patch": "@@ -134,7 +134,6 @@ def test_generation_beyond_sliding_window_tiny_model(self):\n                 max_new_tokens=1,\n                 do_sample=False,\n                 use_cache=True,\n-                cache_implementation=\"hybrid\",\n                 disable_compile=True,\n             )\n             # 2 generations are needed to trigger https://github.com/huggingface/transformers/issues/39711\n@@ -145,11 +144,9 @@ def test_generation_beyond_sliding_window_tiny_model(self):\n                 max_new_tokens=5,\n                 do_sample=False,\n                 use_cache=True,\n-                cache_implementation=\"hybrid\",\n                 disable_compile=True,\n             )\n         generated_sequences = output[:, input_len:].cpu()\n-        print(generated_sequences)\n         EXPECTED_OUTPUT = torch.tensor([[90109, 90109, 90109, 83191, 83191], [246901, 69832, 69832, 69832, 62288]])\n         torch.testing.assert_close(generated_sequences, EXPECTED_OUTPUT)\n \n@@ -493,16 +490,15 @@ def test_model_4b_bf16(self):\n             add_generation_prompt=True,\n         ).to(torch_device)\n \n-        # cache_implementation=\"hybrid\" an in the original transformers implementation\n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_TEXTS = Expectations(\n             {\n                 (\"xpu\", 3): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with turquoise water and a blue sky in the background. It looks like a'],\n                 (\"cuda\", (8, 0)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like'],\n                 (\"cuda\", (8, 6)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear blue water and a blue sky in the background. It looks like'],\n-                (\"rocm\", (9, 4)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with turquoise water and a blue sky in the background. It looks like a'],\n+                (\"rocm\", (9, 4)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear blue water and a blue sky in the background. It looks like'],\n                 (\"rocm\", (9, 5)): ['user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown and white cow standing on a sandy beach with turquoise water and a distant coastline in the background. It looks'],\n             }\n         )  # fmt: skip\n@@ -543,8 +539,7 @@ def test_model_4b_batch(self):\n             add_generation_prompt=True,\n         ).to(torch_device)\n \n-        # cache_implementation=\"hybrid\" an in the original transformers implementation\n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_TEXTS = Expectations(\n@@ -561,12 +556,12 @@ def test_model_4b_batch(self):\n                         ],\n                 (\"cuda\", (8,6)):\n                     [\n-                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear blue water and a blue sky in the background. It looks like',\n+                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like',\n                         \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n*   **Image 1:** Shows a brown\"\n                     ],\n                 (\"rocm\", (9, 4)):\n                     [\n-                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with clear turquoise water and a blue sky in the background. It looks like',\n+                        'user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nCertainly! \\n\\nThe image shows a brown cow standing on a sandy beach with turquoise water and a blue sky in the background. It looks like a',\n                         \"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nAre these images identical?\\nmodel\\nNo, these images are not identical. \\n\\nHere's a breakdown of the differences:\\n\\n*   **Image 1:** Shows a cow\"\n                     ],\n                 (\"rocm\", (9, 5)):\n@@ -603,24 +598,22 @@ def test_model_4b_crops(self):\n             **crop_config,\n         ).to(torch_device)\n \n-        # cache_implementation=\"hybrid\" an in the original transformers implementation\n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"static\")\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_NUM_IMAGES = 3  # one for the origin image and two crops of images\n         EXPECTED_TEXTS = Expectations(\n             {\n                 (\"xpu\", 3): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a bright blue sky with some white clouds in the\"],\n-                (\"cuda\", 7): [],\n-                (\"cuda\", (8, 6)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a clear blue sky with some white clouds above.\"],\n                 (\"cuda\", (8, 0)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a blue sky with some white clouds in the background\"],\n+                (\"cuda\", (8, 6)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a bright blue sky with some white clouds in the\"],\n+                (\"cuda\", (9, 0)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a bright blue sky with some white clouds in the\"],\n                 (\"rocm\", (9, 4)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a bright blue sky with some white clouds in the\"],\n                 (\"rocm\", (9, 5)): [\"user\\nYou are a helpful assistant.\\n\\nHere is the original image \\n\\n\\n\\n and here are some crops to help you see better \\n\\n\\n\\n \\n\\n\\n\\nWhat is shown in this image?\\nmodel\\nThe image shows a brown cow standing on a sandy beach next to a turquoise ocean. There's a blue sky with some white clouds in the background\"]\n             }\n         )  # fmt: skip\n         EXPECTED_TEXT = EXPECTED_TEXTS.get_expectation()\n         self.assertEqual(len(inputs[\"pixel_values\"]), EXPECTED_NUM_IMAGES)\n-        print(f\"Generated text: {output_text}\")\n         self.assertEqual(output_text, EXPECTED_TEXT)\n \n     @require_torch_large_accelerator\n@@ -665,8 +658,7 @@ def test_model_4b_batch_crops(self):\n             **crop_config,\n         ).to(torch_device)\n \n-        # cache_implementation=\"hybrid\" an in the original transformers implementation\n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"static\")\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n         EXPECTED_NUM_IMAGES = 9  # 3 * (one for the origin image and two crops of images) = 9\n         EXPECTED_TEXTS = Expectations(\n@@ -726,15 +718,14 @@ def test_model_4b_multiimage(self):\n             add_generation_prompt=True,\n         ).to(torch_device)\n \n-        # cache_implementation=\"hybrid\" an in the original transformers implementation\n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"static\")\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n         EXPECTED_TEXTS = Expectations(\n             {\n                 (\"xpu\", 3): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Overall Scene:**\\n\\nIt looks like a street scene in a city with\"],\n-                (\"cuda\", 7): [],\n                 (\"cuda\", (8, 0)): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Overall Scene:**\\n\\nIt looks like a street scene in a vibrant,\"],\n                 (\"cuda\", (8, 6)): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Overall Scene:**\\n\\nIt appears to be a street scene in a city\"],\n+                (\"cuda\", (9, 0)): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image!\\n\\nHere's a description of the scene:\\n\\n*   **Location:**\"],\n                 (\"rocm\", (9, 4)): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Overall Scene:**\\n\\nIt appears to be a street scene in a vibrant\"],\n                 (\"rocm\", (9, 5)): [\"user\\nYou are a helpful assistant.\\n\\n\\n\\n\\n\\nWhat do you see here?\\nmodel\\nOkay, let's break down what I see in this image:\\n\\n**Main Features:**\\n\\n*   **Chinese Archway:** The most prominent\"],\n             }\n@@ -750,8 +741,7 @@ def test_model_1b_text_only(self):\n         tokenizer = AutoTokenizer.from_pretrained(model_id, padding_side=\"left\")\n         inputs = tokenizer(\"Write a poem about Machine Learning.\", return_tensors=\"pt\").to(torch_device)\n \n-        # cache_implementation=\"hybrid\" an in the original transformers implementation\n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"static\")\n         output_text = tokenizer.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_TEXTS = Expectations(\n@@ -785,8 +775,7 @@ def test_model_4b_flash_attn(self):\n             add_generation_prompt=True,\n         ).to(torch_device)\n \n-        # cache_implementation=\"hybrid\" an in the original transformers implementation\n-        output = model.generate(**inputs, max_new_tokens=30, do_sample=False, cache_implementation=\"hybrid\")\n+        output = model.generate(**inputs, max_new_tokens=30, do_sample=False)\n         output_text = self.processor.batch_decode(output, skip_special_tokens=True)\n \n         EXPECTED_TEXTS = Expectations(\n@@ -827,9 +816,7 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         input_size = inputs.input_ids.shape[-1]\n         self.assertTrue(input_size > model.config.sliding_window)\n \n-        out = model.generate(**inputs, max_new_tokens=20, do_sample=False, cache_implementation=\"static\")[\n-            :, input_size:\n-        ]\n+        out = model.generate(**inputs, max_new_tokens=20, do_sample=False)[:, input_size:]\n         output_text = tokenizer.batch_decode(out)\n \n         EXPECTED_COMPLETIONS = [\n@@ -839,7 +826,7 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         self.assertEqual(output_text, EXPECTED_COMPLETIONS)\n \n     @pytest.mark.torch_export_test\n-    def test_export_text_only_with_hybrid_cache(self):\n+    def test_export_text_only(self):\n         if not is_torch_greater_or_equal(\"2.6.0\"):\n             self.skipTest(reason=\"This test requires torch >= 2.6 to run.\")\n \n@@ -849,7 +836,7 @@ def test_export_text_only_with_hybrid_cache(self):\n         model = AutoModelForCausalLM.from_pretrained(model_id)\n         self.assertEqual(model.config.cache_implementation, \"hybrid\")\n \n-        # Export + hybrid cache\n+        # Export\n         model.eval()\n         exportable_module = TorchExportableModuleForDecoderOnlyLM(model, batch_size=1, max_cache_len=1024)\n         exported_program = exportable_module.export(\n@@ -874,7 +861,6 @@ def test_export_text_only_with_hybrid_cache(self):\n                 **input_text,\n                 max_new_tokens=max_new_tokens_to_generate,\n                 do_sample=False,  # Use greedy decoding to match the exported model\n-                cache_implementation=\"hybrid\",\n             )\n \n         eager_generated_text = tokenizer.decode(eager_outputs[0], skip_special_tokens=True)"
        }
    ],
    "stats": {
        "total": 46,
        "additions": 16,
        "deletions": 30
    }
}