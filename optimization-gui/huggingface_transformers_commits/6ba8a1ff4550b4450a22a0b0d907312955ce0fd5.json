{
    "author": "kyle-cohere",
    "message": "Update documentation for Cohere2Vision models (#39817)\n\n* Update docs with pipeline example\n\n* Add Cohere2Vision to list of vision models\n\n* Sort models",
    "sha": "6ba8a1ff4550b4450a22a0b0d907312955ce0fd5",
    "files": [
        {
            "sha": "b0fcddc6d37a021ff99a304d0082e2738571bcfe",
            "filename": "docs/source/en/model_doc/cohere2_vision.md",
            "status": "modified",
            "additions": 32,
            "deletions": 1,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ba8a1ff4550b4450a22a0b0d907312955ce0fd5/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ba8a1ff4550b4450a22a0b0d907312955ce0fd5/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcohere2_vision.md?ref=6ba8a1ff4550b4450a22a0b0d907312955ce0fd5",
            "patch": "@@ -19,9 +19,12 @@ Command A Vision is built upon a robust architecture that leverages the latest a\n \n The model and image processor can be loaded as follows:\n \n-```python\n+<hfoptions id=\"usage\">\n+<hfoption id=\"AutoModel\">\n \n+```python\n import torch\n+\n from transformers import AutoProcessor, AutoModelForImageTextToText\n \n model_id = \"CohereLabs/command-a-vision-07-2025\"\n@@ -68,6 +71,34 @@ print(\n )\n ```\n \n+</hfoption>\n+<hfoption id=\"Pipeline\">\n+\n+```python\n+from transformers import pipeline\n+\n+pipe = pipeline(model=\"CohereLabs/command-a-vision-07-2025\", task=\"image-text-to-text\", device_map=\"auto\")\n+\n+messages = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\n+                \"type\": \"image\",\n+                \"url\": \"https://media.istockphoto.com/id/458012057/photo/istanbul-turkey.jpg?s=612x612&w=0&k=20&c=qogAOVvkpfUyqLUMr_XJQyq-HkACXyYUSZbKhBlPrxo=\",\n+            },\n+            {\"type\": \"text\", \"text\": \"Where was this taken ?\"},\n+        ],\n+    },\n+]\n+\n+outputs = pipe(text=messages, max_new_tokens=300, return_full_text=False)\n+\n+print(outputs)\n+```\n+</hfoption>\n+</hfoptions>\n+\n ## Cohere2VisionConfig\n \n [[autodoc]] Cohere2VisionConfig"
        },
        {
            "sha": "259a297bf68839f4766751db9a4e8f2f0c3aa2a1",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/6ba8a1ff4550b4450a22a0b0d907312955ce0fd5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6ba8a1ff4550b4450a22a0b0d907312955ce0fd5/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=6ba8a1ff4550b4450a22a0b0d907312955ce0fd5",
            "patch": "@@ -712,6 +712,7 @@\n         (\"aimv2_vision_model\", \"Aimv2VisionModel\"),\n         (\"beit\", \"BeitModel\"),\n         (\"bit\", \"BitModel\"),\n+        (\"cohere2_vision\", \"Cohere2VisionModel\"),\n         (\"conditional_detr\", \"ConditionalDetrModel\"),\n         (\"convnext\", \"ConvNextModel\"),\n         (\"convnextv2\", \"ConvNextV2Model\"),"
        }
    ],
    "stats": {
        "total": 34,
        "additions": 33,
        "deletions": 1
    }
}