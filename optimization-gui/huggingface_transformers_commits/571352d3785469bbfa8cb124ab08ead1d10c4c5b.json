{
    "author": "manueldeprada",
    "message": "ðŸ”´  Isolate prefill from generation loops (#40652)\n\n* isolate-prefill: squash\n\n* prefill inside decoding methods\n\n* simplify autocompile helpers",
    "sha": "571352d3785469bbfa8cb124ab08ead1d10c4c5b",
    "files": [
        {
            "sha": "2c407ecfd919899e5959fac9e912b862756c981a",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 71,
            "deletions": 74,
            "changes": 145,
            "blob_url": "https://github.com/huggingface/transformers/blob/571352d3785469bbfa8cb124ab08ead1d10c4c5b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/571352d3785469bbfa8cb124ab08ead1d10c4c5b/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=571352d3785469bbfa8cb124ab08ead1d10c4c5b",
            "patch": "@@ -2192,14 +2192,26 @@ def _valid_auto_compile_criteria(self, model_kwargs: dict[str, Any], generation_\n             has_disk_offload = \"disk\" in all_model_devices\n             can_compile &= not has_disk_offload\n \n-        # Finally: if the user has manually specified compilation options, but compilation is not possible, let's warn\n+        # If the user has manually specified compilation options, but compilation is not possible, let's warn\n         # them\n         if generation_config.compile_config is not None and not can_compile:\n             logger.warning_once(\n                 \"You have set `compile_config`, but we are unable to meet the criteria for compilation. Compilation \"\n                 \"will be skipped.\"\n             )\n \n+            # Finally: if we can compile, disable tokenizers parallelism and check for FA2 + static cache\n+            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n+            # If we use FA2 and a static cache, we cannot compile with fullgraph\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                # only raise warning if the user passed an explicit compile-config\n+                if generation_config.compile_config is not None and generation_config.compile_config.fullgraph:\n+                    logger.warning_once(\n+                        \"When using Flash Attention 2 and a static cache, you cannot use the option `CompileConfig(fullgraph=True)` as \"\n+                        \"FA2 introduces graph breaks. We overrode the option with `fullgraph=False`.\"\n+                    )\n+                    generation_config.compile_config.fullgraph = False\n+\n         return can_compile\n \n     def _get_deprecated_gen_repo(\n@@ -2636,7 +2648,7 @@ def generate(\n                 UserWarning,\n             )\n \n-        # 8. prepare logits processors and stopping criteria\n+        # 8. Prepare logits processors and stopping criteria\n         prepared_logits_processor = self._get_logits_processor(\n             generation_config=generation_config,\n             input_ids_seq_length=input_ids_length,\n@@ -2843,40 +2855,21 @@ def _sample(\n         batch_size, cur_len = input_ids.shape[:2]\n         this_peer_finished = False\n         unfinished_sequences = torch.ones(batch_size, dtype=torch.long, device=input_ids.device)\n-        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n \n-        model_forward = self.__call__\n-        compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n-        if compile_forward:\n-            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n-            # If we use FA2 and a static cache, we cannot compile with fullgraph\n-            if self.config._attn_implementation == \"flash_attention_2\":\n-                # only raise warning if the user passed an explicit compile-config\n-                if generation_config.compile_config is not None and generation_config.compile_config.fullgraph:\n-                    logger.warning_once(\n-                        \"When using Flash Attention 2 and a static cache, you cannot use the option `CompileConfig(fullgraph=True)` as \"\n-                        \"FA2 introduces graph breaks. We overrode the option with `fullgraph=False`.\"\n-                    )\n-                    generation_config.compile_config.fullgraph = False\n-            model_forward = self.get_compiled_call(generation_config.compile_config)\n+        model_forward = (\n+            self.get_compiled_call(generation_config.compile_config)\n+            if self._valid_auto_compile_criteria(model_kwargs, generation_config)\n+            else self.__call__\n+        )\n \n-        if generation_config.prefill_chunk_size is not None:\n-            model_kwargs = self._prefill_chunking(input_ids, generation_config, **model_kwargs)\n-            is_prefill = False\n-        else:\n-            is_prefill = True\n+        prefill_consumed = False\n+        outputs = self._prefill(input_ids, generation_config, model_kwargs)\n \n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n-            # prepare model inputs\n-            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n-\n-            if is_prefill:\n-                outputs = self(**model_inputs, return_dict=True)\n-                is_prefill = False\n-            else:\n+            if prefill_consumed:\n+                model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n                 outputs = model_forward(**model_inputs, return_dict=True)\n-\n-            # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n+            prefill_consumed = True\n             model_kwargs = self._update_model_kwargs_for_generation(\n                 outputs,\n                 model_kwargs,\n@@ -3246,7 +3239,6 @@ def _beam_search(\n             `return_dict_in_generate=True` or a [`~generation.GenerateBeamEncoderDecoderOutput`] if\n             `model.config.is_encoder_decoder=True`.\n         \"\"\"\n-\n         # 1. init beam_search values\n         pad_token_id = generation_config._pad_token_tensor\n         eos_token_id = generation_config._eos_token_tensor\n@@ -3287,8 +3279,6 @@ def _beam_search(\n             dim=0,\n         ).to(input_ids.device)\n \n-        model_kwargs = self._get_initial_cache_position(cur_len, input_ids.device, model_kwargs)\n-\n         # (joao) feature lost in the refactor. Probably won't implement, hurts readability with minimal gains (there\n         # are newer low-memory alternatives like the offloaded cache)\n         sequential = generation_config.low_memory\n@@ -3350,13 +3340,18 @@ def _beam_search(\n         )\n         beam_indices = running_beam_indices.detach().clone()\n \n+        prefill_consumed = False\n+        flat_running_sequences = input_ids\n+        model_outputs = self._prefill(input_ids, generation_config, model_kwargs)\n+\n         # 4. run the generation loop\n         while self._has_unfinished_sequences(this_peer_finished, synced_gpus, device=input_ids.device):\n-            # a. Forward current tokens, obtain the logits\n-            flat_running_sequences = self._flatten_beam_dim(running_sequences[:, :, :cur_len])\n-            model_inputs = self.prepare_inputs_for_generation(flat_running_sequences, **model_kwargs)\n-\n-            model_outputs = self(**model_inputs, return_dict=True)\n+            if prefill_consumed:\n+                # a. Forward current tokens, obtain the logits\n+                flat_running_sequences = self._flatten_beam_dim(running_sequences[:, :, :cur_len])\n+                model_inputs = self.prepare_inputs_for_generation(flat_running_sequences, **model_kwargs)\n+                model_outputs = self(**model_inputs, return_dict=True)\n+            prefill_consumed = True\n \n             # synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\n             model_kwargs = self._update_model_kwargs_for_generation(\n@@ -3839,49 +3834,51 @@ def _assisted_decoding(\n         else:\n             return input_ids\n \n-    def _prefill_chunking(self, input_ids: torch.LongTensor, generation_config: GenerationConfig, **model_kwargs):\n-        # Even if we are not compiling the forward, flex is always compiled when used. With chunk prefill, we may\n-        # end up needing just a bit more graphs than the default (which is 8). Doing this avoids very cryptic warnings\n-        torch._dynamo.config.cache_size_limit = 64\n-\n-        chunk_size = generation_config.prefill_chunk_size\n-        # Only chunk up the token just before last, so that decoding is completely performed outside this function\n-        # (here we simply prefill the cache)\n-        input_chunks = torch.split(input_ids[:, :-1], chunk_size, dim=-1)\n-\n-        if \"past_key_values\" not in model_kwargs:\n-            raise ValueError(\"Cannot use prefill chunking without a cache\")\n-\n-        model_forward = self.forward\n+    # TODO: v5.1: make public once API stabilized\n+    def _prefill(self, input_ids: torch.LongTensor, generation_config: GenerationConfig, model_kwargs):\n+        if generation_config.prefill_chunk_size is None:\n+            model_kwargs = self._get_initial_cache_position(input_ids.shape[1], input_ids.device, model_kwargs)\n+            model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n+            return self(**model_inputs, return_dict=True)\n+        else:  # Chunked prefill\n+            # Even if we are not compiling the forward, flex is always compiled when used. With chunked prefill, we may\n+            # end up needing just a bit more graphs than the default (which is 8). Doing this avoids very cryptic warnings\n+            torch._dynamo.config.cache_size_limit = 64\n \n-        compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n-        if compile_forward:\n-            model_forward = self.get_compiled_call(generation_config.compile_config)\n+            chunk_size = generation_config.prefill_chunk_size\n+            input_chunks = torch.split(input_ids, chunk_size, dim=-1)\n \n-        attention_mask = model_kwargs.pop(\"attention_mask\", None)\n+            if \"past_key_values\" not in model_kwargs:\n+                raise ValueError(\"Cannot use prefill chunking without a cache\")\n \n-        past_length = 0\n-        for input_chunk in input_chunks:\n-            current_length = past_length + input_chunk.shape[-1]\n-            # Prepare inputs\n-            if attention_mask is not None:\n-                model_kwargs[\"attention_mask\"] = attention_mask[:, :current_length]\n-            model_kwargs[\"cache_position\"] = torch.arange(\n-                past_length, current_length, dtype=torch.long, device=input_chunk.device\n+            model_forward = (\n+                self.get_compiled_call(generation_config.compile_config)\n+                if self._valid_auto_compile_criteria(model_kwargs, generation_config)\n+                else self.__call__\n             )\n-            model_kwargs[\"position_ids\"] = model_kwargs[\"cache_position\"].unsqueeze(0)\n-            model_inputs = self.prepare_inputs_for_generation(input_chunk, **model_kwargs)\n \n-            outputs = model_forward(**model_inputs, return_dict=True)\n+            attention_mask = model_kwargs.pop(\"attention_mask\", None)\n+            past_length = 0\n+            for input_chunk in input_chunks:\n+                current_length = past_length + input_chunk.shape[-1]\n+                if attention_mask is not None:\n+                    model_kwargs[\"attention_mask\"] = attention_mask[:, :current_length]\n+                model_kwargs[\"cache_position\"] = torch.arange(\n+                    past_length, current_length, dtype=torch.long, device=input_chunk.device\n+                )\n+                model_kwargs[\"position_ids\"] = model_kwargs[\"cache_position\"].unsqueeze(0)\n+                model_inputs = self.prepare_inputs_for_generation(input_chunk, **model_kwargs)\n \n-            model_kwargs[\"past_key_values\"] = outputs.past_key_values\n-            past_length = current_length\n+                outputs = model_forward(**model_inputs, return_dict=True)\n \n-        model_kwargs[\"attention_mask\"] = attention_mask\n-        model_kwargs[\"cache_position\"] = model_kwargs[\"cache_position\"][-1:] + 1\n-        _ = model_kwargs.pop(\"position_ids\", None)\n+                model_kwargs[\"past_key_values\"] = outputs.past_key_values\n+                past_length = current_length\n \n-        return model_kwargs\n+            model_kwargs[\"attention_mask\"] = attention_mask\n+            model_kwargs[\"cache_position\"] = model_kwargs[\"cache_position\"][-1:] + 1\n+            _ = model_kwargs.pop(\"position_ids\", None)\n+            # Latest outputs contain next token logits\n+            return outputs\n \n \n def _speculative_sampling("
        },
        {
            "sha": "b2a946940ee2307c4be2ba10ad794dff318a8dcb",
            "filename": "src/transformers/models/csm/generation_csm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/571352d3785469bbfa8cb124ab08ead1d10c4c5b/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/571352d3785469bbfa8cb124ab08ead1d10c4c5b/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py?ref=571352d3785469bbfa8cb124ab08ead1d10c4c5b",
            "patch": "@@ -13,7 +13,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-import os\n from dataclasses import dataclass\n from typing import TYPE_CHECKING, Any, Optional, Union\n \n@@ -204,11 +203,11 @@ def _sample(\n                     criterion.max_length -= cur_len\n         # ============================================\n \n-        model_forward = self.__call__\n-        compile_forward = self._valid_auto_compile_criteria(model_kwargs, generation_config)\n-        if compile_forward:\n-            os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n-            model_forward = self.get_compiled_call(generation_config.compile_config)\n+        model_forward = (\n+            self.get_compiled_call(generation_config.compile_config)\n+            if self._valid_auto_compile_criteria(model_kwargs, generation_config)\n+            else self.__call__\n+        )\n \n         is_prefill = True\n         while self._has_unfinished_sequences("
        },
        {
            "sha": "afef565370cbc3ddf0a0e8d2536291baf9c2cef1",
            "filename": "src/transformers/models/dia/generation_dia.py",
            "status": "modified",
            "additions": 29,
            "deletions": 20,
            "changes": 49,
            "blob_url": "https://github.com/huggingface/transformers/blob/571352d3785469bbfa8cb124ab08ead1d10c4c5b/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/571352d3785469bbfa8cb124ab08ead1d10c4c5b/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py?ref=571352d3785469bbfa8cb124ab08ead1d10c4c5b",
            "patch": "@@ -278,6 +278,12 @@ def _main_generate_loop(\n         )\n         generation_mode = generation_config.get_generation_mode(assistant_model)\n \n+        if generation_mode not in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n+            raise ValueError(\n+                \"Got incompatible mode for generation, should be one of greedy or sampling. \"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1`.\"\n+            )\n+\n         self._validate_model_kwargs(model_kwargs.copy())\n         self._validate_generation_mode(generation_mode, generation_config, generation_mode_kwargs)\n \n@@ -382,26 +388,29 @@ def _main_generate_loop(\n         # Prepare inner 2D logic in generation loop\n         input_ids = input_ids.reshape(-1, input_ids.shape[-1])\n \n-        # 10. go into different generation modes\n-        if generation_mode in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n-            # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n-            if generation_config.num_return_sequences > 1:\n-                raise ValueError(\"`num_return_sequences>1` is incompatible with Dia.\")\n-\n-            # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n-            return self._sample(\n-                input_ids,\n-                logits_processor=prepared_logits_processor,\n-                stopping_criteria=prepared_stopping_criteria,\n-                generation_config=generation_config,\n-                **generation_mode_kwargs,\n-                **model_kwargs,\n-            )\n-        else:\n-            raise ValueError(\n-                \"Got incompatible mode for generation, should be one of greedy or sampling. \"\n-                \"Ensure that beam search is de-activated by setting `num_beams=1`.\"\n-            )\n+        model_kwargs = self._get_initial_cache_position(input_ids.shape[1], input_ids.device, model_kwargs)\n+        # prepare model inputs\n+        model_inputs = self.prepare_inputs_for_generation(input_ids, **model_kwargs)\n+\n+        # 10. Prefill\n+        model_inputs.update({\"output_attentions\": generation_config.output_attentions})\n+        model_inputs.update({\"output_hidden_states\": generation_config.output_hidden_states})\n+        outputs = self(**model_inputs, return_dict=True)\n+\n+        # 11. expand input_ids with `num_return_sequences` additional sequences per batch\n+        if generation_config.num_return_sequences > 1:\n+            raise ValueError(\"`num_return_sequences>1` is incompatible with Dia.\")\n+\n+        # 12. run sample (it degenerates to greedy search when `generation_config.do_sample=False`)\n+        return self._sample(\n+            input_ids,\n+            logits_processor=prepared_logits_processor,\n+            stopping_criteria=prepared_stopping_criteria,\n+            generation_config=generation_config,\n+            prefill_outputs=outputs,\n+            **generation_mode_kwargs,\n+            **model_kwargs,\n+        )\n \n     @torch.no_grad()\n     def generate("
        },
        {
            "sha": "61b5f2948e3f90efcb121888e509608fd6b45a04",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 35,
            "deletions": 28,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/571352d3785469bbfa8cb124ab08ead1d10c4c5b/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/571352d3785469bbfa8cb124ab08ead1d10c4c5b/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=571352d3785469bbfa8cb124ab08ead1d10c4c5b",
            "patch": "@@ -2109,6 +2109,7 @@ def generate(\n         stopping_criteria: Optional[StoppingCriteriaList] = None,\n         synced_gpus: Optional[bool] = None,\n         streamer: Optional[\"BaseStreamer\"] = None,\n+        use_model_defaults: Optional[bool] = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -2153,6 +2154,11 @@ def generate(\n             streamer (`BaseStreamer`, *optional*):\n                 Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                 through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n+            use_model_defaults (`bool`, *optional*):\n+                When it is `True`, unset parameters in `generation_config` will be set to the model-specific default\n+                generation configuration (`model.generation_config`), as opposed to the global defaults\n+                (`GenerationConfig()`). If unset, models saved starting from `v4.50` will consider this flag to be\n+                `True`.\n             kwargs (`dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n@@ -2175,13 +2181,19 @@ def generate(\n                     - [`~generation.GenerateBeamEncoderDecoderOutput`]\n         \"\"\"\n         # 1. Handle `generation_config` and kwargs that might update it, and validate the resulting objects\n-        if generation_config is None:\n-            generation_config = self.generation_config\n+        generation_mode_kwargs = self._extract_generation_mode_kwargs(None, kwargs, False, None, None)\n+        generation_config, model_kwargs = self._prepare_generation_config(\n+            generation_config, use_model_defaults, **kwargs\n+        )\n+        generation_mode = generation_config.get_generation_mode()\n+        if generation_mode not in [GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH]:\n+            raise ValueError(\n+                \"Got incompatible mode for generation, should be one of greedy or sampling. \"\n+                \"Ensure that beam search is de-activated by setting `num_beams=1`.\"\n+            )\n \n-        generation_config = copy.deepcopy(generation_config)\n-        model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs\n-        generation_config.validate()\n         self._validate_model_kwargs(model_kwargs.copy())\n+        self._validate_generation_mode(generation_mode, generation_config, generation_mode_kwargs)\n \n         if model_kwargs.get(\"encoder_outputs\") is not None and type(model_kwargs[\"encoder_outputs\"]) is tuple:\n             # wrap the unconditional outputs as a BaseModelOutput for compatibility with the rest of generate\n@@ -2281,31 +2293,26 @@ def generate(\n             generation_config=generation_config, stopping_criteria=stopping_criteria\n         )\n \n-        if generation_mode in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n-            # expand input_ids with `num_return_sequences` additional sequences per batch\n-            input_ids, model_kwargs = self._expand_inputs_for_generation(\n-                input_ids=input_ids,\n-                expand_size=generation_config.num_return_sequences,\n-                is_encoder_decoder=self.config.is_encoder_decoder,\n-                **model_kwargs,\n-            )\n+        # expand input_ids with `num_return_sequences` additional sequences per batch\n+        input_ids, model_kwargs = self._expand_inputs_for_generation(\n+            input_ids=input_ids,\n+            expand_size=generation_config.num_return_sequences,\n+            is_encoder_decoder=self.config.is_encoder_decoder,\n+            **model_kwargs,\n+        )\n \n-            # 11. run sample\n-            outputs = self._sample(\n-                input_ids,\n-                logits_processor=logits_processor,\n-                stopping_criteria=stopping_criteria,\n-                generation_config=generation_config,\n-                synced_gpus=synced_gpus,\n-                streamer=streamer,\n-                **model_kwargs,\n-            )\n+        # 10b. prepare prefill outputs\n+        generation_mode_kwargs[\"prefill_outputs\"] = self._prefill(input_ids, generation_config, model_kwargs)\n \n-        else:\n-            raise ValueError(\n-                \"Got incompatible mode for generation, should be one of greedy or sampling. \"\n-                \"Ensure that beam search is de-activated by setting `num_beams=1`.\"\n-            )\n+        # 11. run sample\n+        outputs = self._sample(\n+            input_ids,\n+            logits_processor=logits_processor,\n+            stopping_criteria=stopping_criteria,\n+            generation_config=generation_config,\n+            **generation_mode_kwargs,\n+            **model_kwargs,\n+        )\n \n         if generation_config.return_dict_in_generate:\n             output_ids = outputs.sequences"
        },
        {
            "sha": "e3fc26f71dbfd0a94ec5a74431fdf423363dc9a2",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 38,
            "deletions": 37,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/571352d3785469bbfa8cb124ab08ead1d10c4c5b/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/571352d3785469bbfa8cb124ab08ead1d10c4c5b/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=571352d3785469bbfa8cb124ab08ead1d10c4c5b",
            "patch": "@@ -14,7 +14,6 @@\n # limitations under the License.\n \"\"\"RAG model implementation.\"\"\"\n \n-import copy\n from collections.abc import Callable\n from dataclasses import dataclass\n from typing import Optional, Union\n@@ -24,7 +23,8 @@\n \n from ...cache_utils import Cache, EncoderDecoderCache\n from ...configuration_utils import PreTrainedConfig\n-from ...generation import GenerationConfig, GenerationMixin, LogitsProcessorList, StoppingCriteriaList\n+from ...generation import GenerationConfig, GenerationMixin, GenerationMode, LogitsProcessorList, StoppingCriteriaList\n+from ...generation.utils import GENERATION_MODES_MAPPING\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n@@ -1403,6 +1403,7 @@ def generate(\n         prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n         logits_processor: Optional[LogitsProcessorList] = LogitsProcessorList(),\n         stopping_criteria: Optional[StoppingCriteriaList] = StoppingCriteriaList(),\n+        use_model_defaults: Optional[bool] = None,\n         **kwargs,\n     ) -> torch.LongTensor:\n         \"\"\"\n@@ -1461,6 +1462,11 @@ def generate(\n                 Custom stopping criteria that complement the default stopping criteria built from arguments and a\n                 model's config. If a stopping criteria is passed that is already created with the arguments or a\n                 model's config an error is thrown.\n+            use_model_defaults (`bool`, *optional*):\n+                When it is `True`, unset parameters in `generation_config` will be set to the model-specific default\n+                generation configuration (`model.generation_config`), as opposed to the global defaults\n+                (`GenerationConfig()`). If unset, models saved starting from `v4.50` will consider this flag to be\n+                `True`.\n             kwargs (`dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model.\n@@ -1471,10 +1477,24 @@ def generate(\n             finished early due to the `eos_token_id`.\n         \"\"\"\n         # Handle `generation_config` and kwargs that might update it\n-        if generation_config is None:\n-            generation_config = self.generation_config\n-        generation_config = copy.deepcopy(generation_config)\n-        model_kwargs = generation_config.update(**kwargs)  # All unused kwargs must be model kwargs\n+        generation_mode_kwargs = self._extract_generation_mode_kwargs(None, kwargs, False, None, None)\n+        generation_config, model_kwargs = self._prepare_generation_config(\n+            generation_config, use_model_defaults, **kwargs\n+        )\n+        generation_mode = generation_config.get_generation_mode()\n+        if generation_mode not in [\n+            GenerationMode.SAMPLE,\n+            GenerationMode.GREEDY_SEARCH,\n+            GenerationMode.BEAM_SEARCH,\n+            GenerationMode.BEAM_SAMPLE,\n+        ]:\n+            raise ValueError(\n+                f\"RAG model is not compatible with {generation_mode} generation. Please check your generation parameters.\"\n+            )\n+        # type() required to access the unbound class-level method\n+        decoding_method = getattr(type(self), GENERATION_MODES_MAPPING[generation_mode])\n+        self._validate_model_kwargs(model_kwargs.copy())\n+        self._validate_generation_mode(generation_mode, generation_config, generation_mode_kwargs)\n \n         kwargs_has_attention_mask = model_kwargs.get(\"attention_mask\", None) is not None\n         self._prepare_special_tokens(generation_config, kwargs_has_attention_mask)\n@@ -1550,7 +1570,7 @@ def extend_enc_output(tensor, num_beams=None):\n         model_kwargs[\"attention_mask\"] = context_attention_mask\n         model_kwargs[\"n_docs\"] = n_docs\n \n-        pre_processor = self._get_logits_processor(\n+        prepared_logits_processor = self._get_logits_processor(\n             generation_config=generation_config,\n             input_ids_seq_length=input_ids_seq_length,\n             encoder_input_ids=context_input_ids,\n@@ -1571,37 +1591,18 @@ def extend_enc_output(tensor, num_beams=None):\n             max_cache_length=generation_config.max_length - 1,\n         )\n \n-        if generation_config.num_beams == 1:\n-            if generation_config.num_return_sequences > 1:\n-                raise ValueError(\n-                    f\"num_return_sequences has to be 1, but is {generation_config.num_return_sequences} when doing\"\n-                    \" greedy search.\"\n-                )\n-            return self._sample(\n-                input_ids,\n-                logits_processor=pre_processor,\n-                stopping_criteria=prepared_stopping_criteria,\n-                generation_config=generation_config,\n-                synced_gpus=False,\n-                streamer=None,\n-                **model_kwargs,\n-            )\n-        elif generation_config.num_beams > 1:\n-            if generation_config.num_return_sequences > generation_config.num_beams:\n-                raise ValueError(\"`num_return_sequences` has to be smaller or equal to `num_beams`.\")\n+        # Prefill pass\n+        generation_mode_kwargs[\"prefill_outputs\"] = self._prefill(input_ids, generation_config, model_kwargs)\n \n-            return self._beam_search(\n-                input_ids,\n-                logits_processor=pre_processor,\n-                stopping_criteria=prepared_stopping_criteria,\n-                generation_config=generation_config,\n-                synced_gpus=False,\n-                **model_kwargs,\n-            )\n-        else:\n-            raise ValueError(\n-                f\"`num_beams` has to be an integer strictly superior to 0 (â‰¥ 1), but is {generation_config.num_beams}\"\n-            )\n+        return decoding_method(\n+            self,\n+            input_ids,\n+            logits_processor=prepared_logits_processor,\n+            stopping_criteria=prepared_stopping_criteria,\n+            generation_config=generation_config,\n+            **generation_mode_kwargs,\n+            **model_kwargs,\n+        )\n \n     # Auxiliary functions for beam search\n     def _temporary_reorder_cache(self, past_key_values, beam_idx):"
        }
    ],
    "stats": {
        "total": 343,
        "additions": 178,
        "deletions": 165
    }
}