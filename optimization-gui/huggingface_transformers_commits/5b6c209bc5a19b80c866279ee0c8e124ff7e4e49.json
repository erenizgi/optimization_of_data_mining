{
    "author": "MekkCyber",
    "message": "[kernels] change import time in KernelConfig (#42004)\n\n* change import time\n\n* style",
    "sha": "5b6c209bc5a19b80c866279ee0c8e124ff7e4e49",
    "files": [
        {
            "sha": "fe9f368ac8e77a707541501159f1bfa36d3ceaf3",
            "filename": "src/transformers/utils/kernel_config.py",
            "status": "modified",
            "additions": 7,
            "deletions": 4,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/5b6c209bc5a19b80c866279ee0c8e124ff7e4e49/src%2Ftransformers%2Futils%2Fkernel_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5b6c209bc5a19b80c866279ee0c8e124ff7e4e49/src%2Ftransformers%2Futils%2Fkernel_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fkernel_config.py?ref=5b6c209bc5a19b80c866279ee0c8e124ff7e4e49",
            "patch": "@@ -12,12 +12,9 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-from ..utils import PushToHubMixin, is_kernels_available, is_torch_available\n+from ..utils import PushToHubMixin, is_torch_available\n \n \n-if is_kernels_available():\n-    from kernels import LayerRepository, Mode\n-\n if is_torch_available():\n     import torch\n \n@@ -58,6 +55,8 @@ def infer_device(model):\n \n \n def add_to_mapping(layer_name, device, repo_name, mode, compatible_mapping):\n+    from kernels import LayerRepository\n+\n     if device not in [\"cuda\", \"rocm\", \"xpu\"]:\n         raise ValueError(f\"Only cuda, rocm, and xpu devices supported, got: {device}\")\n     repo_layer_name = repo_name.split(\":\")[1]\n@@ -82,6 +81,8 @@ def __init__(self, kernel_mapping={}):\n         self.registered_layer_names = {}\n \n     def update_kernel(self, repo_id, registered_name, layer_name, device, mode, revision=None):\n+        from kernels import LayerRepository\n+\n         self.kernel_mapping[registered_name] = {\n             device: {\n                 mode: LayerRepository(\n@@ -204,6 +205,8 @@ def create_compatible_mapping(self, model, compile=False):\n         The device is inferred from the model's parameters if not provided.\n         The Mode is inferred from the model's training state.\n         \"\"\"\n+        from kernels import Mode\n+\n         compatible_mapping = {}\n         for layer_name, kernel in self.kernel_mapping.items():\n             # Infer Mode: use Mode.TRAINING if model is training, else use Mode.INFERENCE"
        }
    ],
    "stats": {
        "total": 11,
        "additions": 7,
        "deletions": 4
    }
}