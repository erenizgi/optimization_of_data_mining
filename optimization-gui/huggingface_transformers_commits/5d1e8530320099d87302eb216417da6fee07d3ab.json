{
    "author": "SunMarc",
    "message": "[Trainer] deprecate `num_train_tokens` (#41165)\n\n* dep\n\n* fix\n\n* fix",
    "sha": "5d1e8530320099d87302eb216417da6fee07d3ab",
    "files": [
        {
            "sha": "27adca9c836e8c8c43e5d57197e04188bfb1637d",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d1e8530320099d87302eb216417da6fee07d3ab/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d1e8530320099d87302eb216417da6fee07d3ab/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=5d1e8530320099d87302eb216417da6fee07d3ab",
            "patch": "@@ -2296,16 +2296,6 @@ def _inner_training_loop(\n             max_steps,\n         ) = self.set_initial_training_values(args, train_dataloader, total_train_batch_size)\n \n-        num_train_tokens = None\n-        if self.args.include_tokens_per_second:\n-            num_train_tokens = self.num_tokens(train_dataloader, None if epoch_based else max_steps)\n-            # If going by epochs, multiply tokens linearly\n-            if len_dataloader is not None and epoch_based:\n-                num_train_tokens *= args.num_train_epochs\n-            # Otherwise since its steps, we just multiply by grad accum\n-            else:\n-                num_train_tokens *= args.gradient_accumulation_steps\n-\n         if DebugOption.UNDERFLOW_OVERFLOW in self.args.debug:\n             if self.args.n_gpu > 1:\n                 # nn.DataParallel(model) replicates the model, creating new variables and module\n@@ -2527,7 +2517,7 @@ def _inner_training_loop(\n                     # Since we perform prefetching, we need to manually set sync_gradients\n                     self.accelerator.gradient_state._set_sync_gradients(do_sync_step)\n \n-                    if self.args.include_num_input_tokens_seen not in [\"no\", False]:\n+                    if self.args.include_num_input_tokens_seen != \"no\":\n                         main_input_name = getattr(self.model, \"main_input_name\", \"input_ids\")\n                         if main_input_name not in inputs:\n                             logger.warning(\n@@ -2715,7 +2705,6 @@ def _inner_training_loop(\n             start_time,\n             num_samples=num_train_samples,\n             num_steps=self.state.max_steps,\n-            num_tokens=num_train_tokens,\n         )\n         self.store_flos()\n         metrics[\"total_flos\"] = self.state.total_flos"
        },
        {
            "sha": "2abf0d5c883d9b60f58a4bde2eb91bf7c9c7021b",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 17,
            "deletions": 19,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/5d1e8530320099d87302eb216417da6fee07d3ab/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5d1e8530320099d87302eb216417da6fee07d3ab/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=5d1e8530320099d87302eb216417da6fee07d3ab",
            "patch": "@@ -737,15 +737,8 @@ class TrainingArguments:\n             Refer to the PyTorch doc for possible values and note that they may change across PyTorch versions.\n \n             This flag is experimental and subject to change in future releases.\n-        include_tokens_per_second (`bool`, *optional*, defaults to `False`):\n-            Whether or not to compute the number of tokens per second per device for training speed metrics.\n-\n-            This will iterate over the entire training dataloader once beforehand,\n-            and will slow down the entire process.\n-\n-        include_num_input_tokens_seen (`bool`, *optional*):\n-            Whether or not to track the number of input tokens seen throughout training.\n-\n+        include_num_input_tokens_seen (`Optional[Union[str, bool]]`, *optional*, defaults to \"no\"):\n+            Whether to track the number of input tokens seen. Must be one of [\"all\", \"non_padding\", \"no\"] or a boolean value which map to \"all\" or \"no\".\n             May be slower in distributed training as gather operations must be called.\n \n         neftune_noise_alpha (`Optional[float]`):\n@@ -1368,18 +1361,19 @@ class TrainingArguments:\n         },\n     )\n \n-    include_tokens_per_second: bool = field(\n-        default=False,\n-        metadata={\"help\": \"If set to `True`, the speed metrics will include `tgs` (tokens per second per device).\"},\n+    include_tokens_per_second: Optional[bool] = field(\n+        default=None,\n+        metadata={\n+            \"help\": \"This arg is deprecated and will be removed in v5 , use `include_num_input_tokens_seen` instead.\"\n+        },\n     )\n \n     include_num_input_tokens_seen: Union[str, bool] = field(\n-        default=False,\n+        default=\"no\",\n         metadata={\n             \"help\": (\n                 \"Whether to track the number of input tokens seen. \"\n-                \"Can be `'all'` to count all tokens, `'non_padding'` to count only non-padding tokens, \"\n-                \"or a boolean (`True` maps to `'all'`, `False` to `'no'`).\"\n+                \"Must be one of [`all`, `non_padding`, `no`] or a boolean value which map to `all` or `no`\"\n             )\n         },\n     )\n@@ -1890,10 +1884,14 @@ def __post_init__(self):\n                     \"This is not supported and we recommend you to update your version.\"\n                 )\n \n-        if self.include_num_input_tokens_seen is True:\n-            self.include_num_input_tokens_seen = \"all\"\n-        elif self.include_num_input_tokens_seen is False:\n-            self.include_num_input_tokens_seen = \"no\"\n+        if self.include_tokens_per_second is not None:\n+            logger.warning(\n+                \"include_tokens_per_second is deprecated and will be removed in v5. Use `include_num_input_tokens_seen` instead. \"\n+            )\n+            self.include_num_input_tokens_seen = self.include_tokens_per_second\n+\n+        if isinstance(self.include_num_input_tokens_seen, bool):\n+            self.include_num_input_tokens_seen = \"all\" if self.include_num_input_tokens_seen else \"no\"\n \n     def __str__(self):\n         self_as_dict = asdict(self)"
        }
    ],
    "stats": {
        "total": 49,
        "additions": 18,
        "deletions": 31
    }
}