{
    "author": "cyyever",
    "message": "Fix Latex typesetting in documentation (#41177)\n\nFix Latex typsetting in documentation\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>",
    "sha": "7164924a7e83f223a2bf2e104bef98eabe545091",
    "files": [
        {
            "sha": "1e86c6692f19ab3548381c7c5707c1e3ee86fa47",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=7164924a7e83f223a2bf2e104bef98eabe545091",
            "patch": "@@ -41,13 +41,13 @@ $$\n \n The query (`Q`), key (`K`), and value (`V`) matrices are projections from the input embeddings of shape `(b, h, T, d_head)`.\n \n-For causal attention, the mask prevents the model from attending to future tokens. Once a token is processed, its representation never changes with respect to future tokens, which means \\\\( K_{\\text{past}} \\\\) and \\\\( V_{\\text{past}} \\\\) can be cached and reused to compute the last token's representation.\n+For causal attention, the mask prevents the model from attending to future tokens. Once a token is processed, its representation never changes with respect to future tokens, which means $ K_{\\text{past}} $ and $ V_{\\text{past}} $ can be cached and reused to compute the last token's representation.\n \n $$\n \\text{Attention}(q_t, [\\underbrace{k_1, k_2, \\dots, k_{t-1}}_{\\text{cached}}, k_{t}], [\\underbrace{v_1, v_2, \\dots, v_{t-1}}_{\\text{cached}}, v_{t}])\n $$\n \n-At inference time, you only need the last token's query to compute the representation \\\\( x_t \\\\) that predicts the next token \\\\( t+1 \\\\). At each step, the new key and value vectors are **stored** in the cache and **appended** to the past keys and values.\n+At inference time, you only need the last token's query to compute the representation $ x_t $ that predicts the next token $ t+1 $. At each step, the new key and value vectors are **stored** in the cache and **appended** to the past keys and values.\n \n $$\n K_{\\text{cache}} \\leftarrow \\text{concat}(K_{\\text{past}}, k_t), \\quad V_{\\text{cache}} \\leftarrow \\text{concat}(V_{\\text{past}}, v_t)\n@@ -59,7 +59,7 @@ Refer to the table below to compare how caching improves efficiency.\n \n | without caching | with caching |\n |---|---|\n-| for each step, recompute all previous `K` and `V`  | for each step, only compute current `K` and `V`\n+| for each step, recompute all previous `K` and `V`  | for each step, only compute current `K` and `V` |\n | attention cost per step is **quadratic** with sequence length | attention cost per step is **linear** with sequence length (memory grows linearly, but compute/token remains low) |\n \n ## Cache class"
        },
        {
            "sha": "35948c4f918eacb7922dfb38f0d6d0a985bb966d",
            "filename": "docs/source/en/model_doc/reformer.md",
            "status": "modified",
            "additions": 20,
            "deletions": 20,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Freformer.md?ref=7164924a7e83f223a2bf2e104bef98eabe545091",
            "patch": "@@ -50,14 +50,14 @@ found [here](https://github.com/google/trax/tree/master/trax/models/reformer).\n \n Axial Positional Encodings were first implemented in Google's [trax library](https://github.com/google/trax/blob/4d99ad4965bab1deba227539758d59f0df0fef48/trax/layers/research/position_encodings.py#L29)\n and developed by the authors of this model's paper. In models that are treating very long input sequences, the\n-conventional position id encodings store an embeddings vector of size \\\\(d\\\\) being the `config.hidden_size` for\n-every position \\\\(i, \\ldots, n_s\\\\), with \\\\(n_s\\\\) being `config.max_embedding_size`. This means that having\n-a sequence length of \\\\(n_s = 2^{19} \\approx 0.5M\\\\) and a `config.hidden_size` of \\\\(d = 2^{10} \\approx 1000\\\\)\n+conventional position id encodings store an embeddings vector of size $d$ being the `config.hidden_size` for\n+every position $i, \\ldots, n_s$, with $n_s$ being `config.max_embedding_size`. This means that having\n+a sequence length of $n_s = 2^{19} \\approx 0.5M$ and a `config.hidden_size` of $d = 2^{10} \\approx 1000$\n would result in a position encoding matrix:\n \n $$X_{i,j}, \\text{ with } i \\in \\left[1,\\ldots, d\\right] \\text{ and } j \\in \\left[1,\\ldots, n_s\\right]$$\n \n-which alone has over 500M parameters to store. Axial positional encodings factorize \\\\(X_{i,j}\\\\) into two matrices:\n+which alone has over 500M parameters to store. Axial positional encodings factorize $X_{i,j}$ into two matrices:\n \n $$X^{1}_{i,j}, \\text{ with } i \\in \\left[1,\\ldots, d^1\\right] \\text{ and } j \\in \\left[1,\\ldots, n_s^1\\right]$$\n \n@@ -76,16 +76,16 @@ X^{1}_{i, k}, & \\text{if }\\ i < d^1 \\text{ with } k = j \\mod n_s^1 \\\\\n X^{2}_{i - d^1, l}, & \\text{if } i \\ge d^1 \\text{ with } l = \\lfloor\\frac{j}{n_s^1}\\rfloor\n \\end{cases}$$\n \n-Intuitively, this means that a position embedding vector \\\\(x_j \\in \\mathbb{R}^{d}\\\\) is now the composition of two\n-factorized embedding vectors: \\\\(x^1_{k, l} + x^2_{l, k}\\\\), where as the `config.max_embedding_size` dimension\n-\\\\(j\\\\) is factorized into \\\\(k \\text{ and } l\\\\). This design ensures that each position embedding vector\n-\\\\(x_j\\\\) is unique.\n+Intuitively, this means that a position embedding vector $x_j \\in \\mathbb{R}^{d}$ is now the composition of two\n+factorized embedding vectors: $x^1_{k, l} + x^2_{l, k}$, where as the `config.max_embedding_size` dimension\n+$j$ is factorized into $k \\text{ and } l$. This design ensures that each position embedding vector\n+$x_j$ is unique.\n \n-Using the above example again, axial position encoding with \\\\(d^1 = 2^9, d^2 = 2^9, n_s^1 = 2^9, n_s^2 = 2^{10}\\\\)\n-can drastically reduced the number of parameters from 500 000 000 to \\\\(2^{18} + 2^{19} \\approx 780 000\\\\) parameters, this means 85% less memory usage.\n+Using the above example again, axial position encoding with $d^1 = 2^9, d^2 = 2^9, n_s^1 = 2^9, n_s^2 = 2^{10}$\n+can drastically reduced the number of parameters from 500 000 000 to $2^{18} + 2^{19} \\approx 780 000$ parameters, this means 85% less memory usage.\n \n-In practice, the parameter `config.axial_pos_embds_dim` is set to a tuple \\\\((d^1, d^2)\\\\) which sum has to be\n-equal to `config.hidden_size` and `config.axial_pos_shape` is set to a tuple \\\\((n_s^1, n_s^2)\\\\) which\n+In practice, the parameter `config.axial_pos_embds_dim` is set to a tuple $(d^1, d^2)$ which sum has to be\n+equal to `config.hidden_size` and `config.axial_pos_shape` is set to a tuple $(n_s^1, n_s^2)$ which\n product has to be equal to `config.max_embedding_size`, which during training has to be equal to the *sequence\n length* of the `input_ids`.\n \n@@ -107,19 +107,19 @@ neighboring chunks and `config.lsh_num_chunks_after` following neighboring chunk\n \n For more information, see the [original Paper](https://huggingface.co/papers/2001.04451) or this great [blog post](https://www.pragmatic.ml/reformer-deep-dive/).\n \n-Note that `config.num_buckets` can also be factorized into a list \\\\((n_{\\text{buckets}}^1,\n-n_{\\text{buckets}}^2)\\\\). This way instead of assigning the query key embedding vectors to one of \\\\((1,\\ldots,\n-n_{\\text{buckets}})\\\\) they are assigned to one of \\\\((1-1,\\ldots, n_{\\text{buckets}}^1-1, \\ldots,\n-1-n_{\\text{buckets}}^2, \\ldots, n_{\\text{buckets}}^1-n_{\\text{buckets}}^2)\\\\). This is crucial for very long sequences to\n+Note that `config.num_buckets` can also be factorized into a list $(n_{\\text{buckets}}^1,\n+n_{\\text{buckets}}^2)$. This way instead of assigning the query key embedding vectors to one of $(1,\\ldots,\n+n_{\\text{buckets}})$ they are assigned to one of $(1-1,\\ldots, n_{\\text{buckets}}^1-1, \\ldots,\n+1-n_{\\text{buckets}}^2, \\ldots, n_{\\text{buckets}}^1-n_{\\text{buckets}}^2)$. This is crucial for very long sequences to\n save memory.\n \n When training a model from scratch, it is recommended to leave `config.num_buckets=None`, so that depending on the\n sequence length a good value for `num_buckets` is calculated on the fly. This value will then automatically be\n saved in the config and should be reused for inference.\n \n Using LSH self attention, the memory and time complexity of the query-key matmul operation can be reduced from\n-\\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to \\\\(\\mathcal{O}(n_s \\times \\log(n_s))\\\\), which usually represents the memory\n-and time bottleneck in a transformer model, with \\\\(n_s\\\\) being the sequence length.\n+$\\mathcal{O}(n_s \\times n_s)$ to $\\mathcal{O}(n_s \\times \\log(n_s))$, which usually represents the memory\n+and time bottleneck in a transformer model, with $n_s$ being the sequence length.\n \n ### Local Self Attention\n \n@@ -129,8 +129,8 @@ the key embedding vectors in its chunk and to the key embedding vectors of `conf\n previous neighboring chunks and `config.local_num_chunks_after` following neighboring chunks.\n \n Using Local self attention, the memory and time complexity of the query-key matmul operation can be reduced from\n-\\\\(\\mathcal{O}(n_s \\times n_s)\\\\) to \\\\(\\mathcal{O}(n_s \\times \\log(n_s))\\\\), which usually represents the memory\n-and time bottleneck in a transformer model, with \\\\(n_s\\\\) being the sequence length.\n+$\\mathcal{O}(n_s \\times n_s)$ to $\\mathcal{O}(n_s \\times \\log(n_s))$, which usually represents the memory\n+and time bottleneck in a transformer model, with $n_s$ being the sequence length.\n \n ### Training\n "
        },
        {
            "sha": "f3c1ae7ea7361bc0e8fb8fca4376f2cb632ca956",
            "filename": "docs/source/en/model_doc/rwkv.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Frwkv.md?ref=7164924a7e83f223a2bf2e104bef98eabe545091",
            "patch": "@@ -94,43 +94,43 @@ In a traditional auto-regressive Transformer, attention is written as\n \n $$O = \\hbox{softmax}(QK^{T} / \\sqrt{d}) V$$\n \n-with \\\\(Q\\\\), \\\\(K\\\\) and \\\\(V\\\\) are matrices of shape `seq_len x hidden_size` named query, key and value (they are actually bigger matrices with a batch dimension and an attention head dimension but we're only interested in the last two, which is where the matrix product is taken, so for the sake of simplicity we only consider those two). The product \\\\(QK^{T}\\\\) then has shape `seq_len x seq_len` and we can take the matrix product with \\\\(V\\\\) to get the output \\\\(O\\\\) of the same shape as the others.  \n+with $Q$, $K$ and $V$ are matrices of shape `seq_len x hidden_size` named query, key and value (they are actually bigger matrices with a batch dimension and an attention head dimension but we're only interested in the last two, which is where the matrix product is taken, so for the sake of simplicity we only consider those two). The product $QK^{T}$ then has shape `seq_len x seq_len` and we can take the matrix product with $V$ to get the output $O$ of the same shape as the others.  \n \n Replacing the softmax by its value gives:\n \n $$O_{i} = \\frac{\\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} / \\sqrt{d}} V_{j}}{\\sum_{j=1}^{i} e^{Q_{i} K_{j}^{T} / \\sqrt{d}}}$$\n \n-Note that the entries in \\\\(QK^{T}\\\\) corresponding to \\\\(j > i\\\\) are masked (the sum stops at j) because the attention is not allowed to look at future tokens (only past ones).\n+Note that the entries in $QK^{T}$ corresponding to $j > i$ are masked (the sum stops at j) because the attention is not allowed to look at future tokens (only past ones).\n \n In comparison, the RWKV attention is given by\n \n $$O_{i} = \\sigma(R_{i}) \\frac{\\sum_{j=1}^{i} e^{W_{i-j} + K_{j}} V_{j}}{\\sum_{j=1}^{i} e^{W_{i-j} + K_{j}}}$$\n \n-where \\\\(R\\\\) is a new matrix called receptance by the author, \\\\(K\\\\) and \\\\(V\\\\) are still the key and value (\\\\(\\sigma\\\\) here is the sigmoid function). \\\\(W\\\\) is a new vector that represents the position of the token and is given by\n+where $R$ is a new matrix called receptance by the author, $K$ and $V$ are still the key and value ($\\sigma$ here is the sigmoid function). $W$ is a new vector that represents the position of the token and is given by\n \n $$W_{0} = u \\hbox{  and  } W_{k} = (k-1)w \\hbox{ for } k \\geq 1$$\n \n-with \\\\(u\\\\) and \\\\(w\\\\) learnable parameters called in the code `time_first` and `time_decay` respectively. The numerator and denominator can both be expressed recursively. Naming them \\\\(N_{i}\\\\) and \\\\(D_{i}\\\\) we have:\n+with $u$ and $w$ learnable parameters called in the code `time_first` and `time_decay` respectively. The numerator and denominator can both be expressed recursively. Naming them $N_{i}$ and $D_{i}$ we have:\n \n $$N_{i} = e^{u + K_{i}} V_{i} + \\hat{N}_{i} \\hbox{  where  } \\hat{N}_{i} = e^{K_{i-1}} V_{i-1} + e^{w + K_{i-2}} V_{i-2} \\cdots + e^{(i-2)w + K_{1}} V_{1}$$\n \n-so \\\\(\\hat{N}_{i}\\\\) (called `numerator_state` in the code) satisfies\n+so $\\hat{N}_{i}$ (called `numerator_state` in the code) satisfies\n \n $$\\hat{N}_{0} = 0 \\hbox{  and  } \\hat{N}_{j+1} = e^{K_{j}} V_{j} + e^{w} \\hat{N}_{j}$$\n \n and\n \n $$D_{i} = e^{u + K_{i}} + \\hat{D}_{i} \\hbox{  where  } \\hat{D}_{i} = e^{K_{i-1}} + e^{w + K_{i-2}} \\cdots + e^{(i-2)w + K_{1}}$$\n \n-so \\\\(\\hat{D}_{i}\\\\) (called `denominator_state` in the code) satisfies\n+so $\\hat{D}_{i}$ (called `denominator_state` in the code) satisfies\n \n $$\\hat{D}_{0} = 0 \\hbox{  and  } \\hat{D}_{j+1} = e^{K_{j}} + e^{w} \\hat{D}_{j}$$\n \n The actual recurrent formula used are a tiny bit more complex, as for numerical stability we don't want to compute exponentials of big numbers. Usually the softmax is not computed as is, but the exponential of the maximum term is divided of the numerator and denominator:\n \n $$\\frac{e^{x_{i}}}{\\sum_{j=1}^{n} e^{x_{j}}} = \\frac{e^{x_{i} - M}}{\\sum_{j=1}^{n} e^{x_{j} - M}}$$\n \n-with \\\\(M\\\\) the maximum of all \\\\(x_{j}\\\\). So here on top of saving the numerator state (\\\\(\\hat{N}\\\\)) and the denominator state (\\\\(\\hat{D}\\\\)) we also keep track of the maximum of all terms encountered in the exponentials. So we actually use\n+with $M$ the maximum of all $x_{j}$. So here on top of saving the numerator state ($\\hat{N}$) and the denominator state ($\\hat{D}$) we also keep track of the maximum of all terms encountered in the exponentials. So we actually use\n \n $$\\tilde{N}_{i} = e^{-M_{i}} \\hat{N}_{i} \\hbox{  and  } \\tilde{D}_{i} = e^{-M_{i}} \\hat{D}_{i}$$\n \n@@ -142,7 +142,7 @@ and\n \n $$\\tilde{D}_{0} = 0 \\hbox{  and  } \\tilde{D}_{j+1} = e^{K_{j} - q} + e^{w + M_{j} - q} \\tilde{D}_{j} \\hbox{  where  } q = \\max(K_{j}, w + M_{j})$$\n \n-and \\\\(M_{j+1} = q\\\\). With those, we can then compute\n+and $M_{j+1} = q$. With those, we can then compute\n \n $$N_{i} = e^{u + K_{i} - q} V_{i} + e^{M_{i}} \\tilde{N}_{i} \\hbox{  where  } q = \\max(u + K_{i}, M_{i})$$\n "
        },
        {
            "sha": "7c16b952c4a2fc0ab16e824f32224cf7d848946e",
            "filename": "docs/source/en/perplexity.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Fperplexity.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Fperplexity.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperplexity.md?ref=7164924a7e83f223a2bf2e104bef98eabe545091",
            "patch": "@@ -23,11 +23,11 @@ that the metric applies specifically to classical language models (sometimes cal\n models) and is not well defined for masked language models like BERT (see [summary of the models](model_summary)).\n \n Perplexity is defined as the exponentiated average negative log-likelihood of a sequence. If we have a tokenized\n-sequence \\\\(X = (x_0, x_1, \\dots, x_t)\\\\), then the perplexity of \\\\(X\\\\) is,\n+sequence $X = (x_0, x_1, \\dots, x_t)$, then the perplexity of $X$ is,\n \n-$$\\text{PPL}(X) = \\exp \\left\\{ {-\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{<i}) } \\right\\}$$\n+$$ \\text{PPL}(X) = \\exp\\left\\{ -\\frac{1}{t}\\sum_i^t \\log p_\\theta (x_i|x_{<i})  \\right\\} $$\n \n-where \\\\(\\log p_\\theta (x_i|x_{<i})\\\\) is the log-likelihood of the ith token conditioned on the preceding tokens \\\\(x_{<i}\\\\) according to our model. Intuitively, it can be thought of as an evaluation of the model's ability to predict uniformly among the set of specified tokens in a corpus. Importantly, this means that the tokenization procedure has a direct impact on a model's perplexity which should always be taken into consideration when comparing different models.\n+where $\\log p_\\theta (x_i|x_{<i})$ is the log-likelihood of the ith token conditioned on the preceding tokens $x_{<i}$ according to our model. Intuitively, it can be thought of as an evaluation of the model's ability to predict uniformly among the set of specified tokens in a corpus. Importantly, this means that the tokenization procedure has a direct impact on a model's perplexity which should always be taken into consideration when comparing different models.\n \n This is also equivalent to the exponentiation of the cross-entropy between the data and model predictions. For more\n intuition about perplexity and its relationship to Bits Per Character (BPC) and data compression, check out this\n@@ -42,11 +42,11 @@ factorizing a sequence and conditioning on the entire preceding subsequence at e\n \n When working with approximate models, however, we typically have a constraint on the number of tokens the model can\n process. The largest version of [GPT-2](model_doc/gpt2), for example, has a fixed length of 1024 tokens, so we\n-cannot calculate \\\\(p_\\theta(x_t|x_{<t})\\\\) directly when \\\\(t\\\\) is greater than 1024.\n+cannot calculate $p_\\theta(x_t|x_{<t})$ directly when $t$ is greater than 1024.\n \n Instead, the sequence is typically broken into subsequences equal to the model's maximum input size. If a model's max\n-input size is \\\\(k\\\\), we then approximate the likelihood of a token \\\\(x_t\\\\) by conditioning only on the\n-\\\\(k-1\\\\) tokens that precede it rather than the entire context. When evaluating the model's perplexity of a\n+input size is $k$, we then approximate the likelihood of a token $x_t$ by conditioning only on the\n+$k-1$ tokens that precede it rather than the entire context. When evaluating the model's perplexity of a\n sequence, a tempting but suboptimal approach is to break the sequence into disjoint chunks and add up the decomposed\n log-likelihoods of each segment independently.\n "
        },
        {
            "sha": "b97ac99ffde50eb03c2894815a562a9e19637ed4",
            "filename": "docs/source/en/quantization/concept_guide.md",
            "status": "modified",
            "additions": 13,
            "deletions": 13,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Fquantization%2Fconcept_guide.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Fquantization%2Fconcept_guide.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fconcept_guide.md?ref=7164924a7e83f223a2bf2e104bef98eabe545091",
            "patch": "@@ -30,7 +30,7 @@ The sections below cover quantization schemes, granularity, and techniques.\n \n ## Quantization schemes\n \n-The core idea is to map the range of values found in the original float32 weights and activations to the much smaller range represented by int8 (typically \\\\([-128, 127]\\\\)).\n+The core idea is to map the range of values found in the original float32 weights and activations to the much smaller range represented by int8 (typically $[-128, 127]$).\n \n This section covers how some quantization techniques work.\n \n@@ -40,20 +40,20 @@ This section covers how some quantization techniques work.\n \n ### Affine quantization\n \n-The most common method is *affine quantization*. For a given float32 tensor (like a layer's weights), it finds the minimum \\\\(val_{min}\\\\) and maximum \\\\(val_{max}\\\\) values. This range \\\\([val_{min}, val_{max}]\\\\) is mapped to the int8 range \\\\([q_{min}, q_{max}]\\\\), which is typically \\\\([-128, 127]\\\\).\n+The most common method is *affine quantization*. For a given float32 tensor (like a layer's weights), it finds the minimum $val_{min}$ and maximum $val_{max}$ values. This range $[val_{min}, val_{max}]$ is mapped to the int8 range $[q_{min}, q_{max}]$, which is typically $[-128, 127]$.\n \n There are two main ways to perform this mapping, *symmetric* and *asymmetric*. The choice between symmetric and asymmetric quantization determines how the float32 range is mapped to the int8 range.\n \n-- Symmetric: This method assumes the original float32 range is symmetric around zero ( \\\\([ -a, a ]\\\\) ). This range is mapped symmetrically to the int8 range, for example, \\\\([-127, 127]\\\\). A key characteristic is that the float32 value \\\\(0.0\\\\) maps directly to the int8 value \\\\(0\\\\). This only requires one parameter, the **scale ( \\\\(S\\\\) )**, to define the mapping. It can simplify computations, but it might be less accurate if the original data distribution isn't naturally centered around zero.\n-- Asymmetric (Affine): This method does not assume the data is centered around zero. It maps the exact range \\\\([val_{min}, val_{max}]\\\\) from float32 to the full int8 range, like \\\\([-128, 127]\\\\). This requires two parameters, a **scale ( \\\\(S\\\\) )** and a **zero-point ( \\\\(Z\\\\) )**.\n+- Symmetric: This method assumes the original float32 range is symmetric around zero ( $[ -a, a ]$ ). This range is mapped symmetrically to the int8 range, for example, $[-127, 127]$. A key characteristic is that the float32 value $0.0$ maps directly to the int8 value $0$. This only requires one parameter, the **scale ( $S$ )**, to define the mapping. It can simplify computations, but it might be less accurate if the original data distribution isn't naturally centered around zero.\n+- Asymmetric (Affine): This method does not assume the data is centered around zero. It maps the exact range $[val_{min}, val_{max}]$ from float32 to the full int8 range, like $[-128, 127]$. This requires two parameters, a **scale ( $S$ )** and a **zero-point ( $Z$ )**.\n \n-    scale ( \\\\(S\\\\) ): A positive float32 number representing the ratio between the float32 and the int8 range.\n+    scale ( $S$ ): A positive float32 number representing the ratio between the float32 and the int8 range.\n \n $$\n S = \\frac{val_{max} - val_{min}}{q_{max} - q_{min}}\n $$\n \n-zero-Point ( \\\\(Z\\\\) ): An int8 value that corresponds to the float32 value \\\\(0.0\\\\).\n+zero-point ( $Z$ ): An int8 value that corresponds to the float32 value $0.0$.\n \n $$\n Z = q_{min} - round\\left(\\frac{val_{min}}{S}\\right)\n@@ -62,13 +62,13 @@ $$\n > [!TIP]\n > In symmetric quantization, Z would typically be fixed at 0.\n \n-With these parameters, a float32 value, \\\\(x\\\\). can be quantized to int8 ( \\\\(q\\\\) ) with the formula below.\n+With these parameters, a float32 value, $x$. can be quantized to int8 ( $q$ ) with the formula below.\n \n $$\n q = round\\left(\\frac{x}{S} + Z\\right)\n $$\n \n-The int8 value, \\\\(q\\\\), can be dequantized back to approximate float32 with the formula below.\n+The int8 value, $q$, can be dequantized back to approximate float32 with the formula below.\n \n $$\n x \\approx S \\cdot (q - Z)\n@@ -78,15 +78,15 @@ $$\n     <img width=\"606\" alt=\"dequant\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/dequant.png\" />\n </div>\n \n-During inference, computations like matrix multiplication are performed using the int8 values ( \\\\(q\\\\) ), and the result is dequantized back to float32 (often using a higher-precision accumulation type like int32 internally) before it is passed to the next layer.\n+During inference, computations like matrix multiplication are performed using the int8 values ( $q$ ), and the result is dequantized back to float32 (often using a higher-precision accumulation type like int32 internally) before it is passed to the next layer.\n \n ### int4 and weight packing\n \n <div class=\"flex justify-center\">\n     <img width=\"606\" alt=\"weight packing\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/weight_packing.png\" />\n </div>\n \n-int4 quantization further reduces the model size and memory usage (halving it compared to int8). The same affine or symmetric quantization principles apply, mapping the float32 range to the 16 possible values representable by int4 ( \\\\([-8, 7]\\\\) for signed int4).\n+int4 quantization further reduces the model size and memory usage (halving it compared to int8). The same affine or symmetric quantization principles apply, mapping the float32 range to the 16 possible values representable by int4 ( $[-8, 7]$ for signed int4).\n \n A key aspect of int4 quantization is **weight packing**. Since most hardware can't natively handle 4-bit data types in memory, two int4 values are typically packed together into a single int8 byte for storage and transfer. For example, the first value might occupy the lower 4 bits and the second value the upper 4 bits of the byte (`packed_byte = (val1 & 0x0F) | (val2 << 4)`).\n \n@@ -114,10 +114,10 @@ Transformers supports FP8 through specific backends like [FBGEMM](./fbgemm_fp8),\n \n ## Granularity\n \n-Quantization parameters ( \\\\(S\\\\) and \\\\(Z\\\\)) can be calculated in one of two ways.\n+Quantization parameters ( $S$ and $Z$) can be calculated in one of two ways.\n \n-- Per-Tensor: One set of \\\\(S\\\\) and \\\\(Z\\\\) for the entire tensor. Simpler, but less accurate if data values vary greatly within the tensor.\n-- Per-Channel (or Per-Group/Block): Separate \\\\(S\\\\) and \\\\(Z\\\\) for each channel or group. More accurate and better performance at the cost of slightly more complexity and memory.\n+- Per-Tensor: One set of $S$ and $Z$ for the entire tensor. Simpler, but less accurate if data values vary greatly within the tensor.\n+- Per-Channel (or Per-Group/Block): Separate $S$ and $Z$ for each channel or group. More accurate and better performance at the cost of slightly more complexity and memory.\n \n <div class=\"flex justify-center\">\n     <img width=\"625\" alt=\"Granularities\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Granularities.png\" />"
        },
        {
            "sha": "ba9fedceaf967e9bf98f21a74ec321a6b40a92dc",
            "filename": "docs/source/en/tokenizer_summary.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Ftokenizer_summary.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/7164924a7e83f223a2bf2e104bef98eabe545091/docs%2Fsource%2Fen%2Ftokenizer_summary.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftokenizer_summary.md?ref=7164924a7e83f223a2bf2e104bef98eabe545091",
            "patch": "@@ -257,8 +257,8 @@ likely tokenization in practice, but also offers the possibility to sample a pos\n probabilities.\n \n Those probabilities are defined by the loss the tokenizer is trained on. Assuming that the training data consists of\n-the words \\\\(x_{1}, \\dots, x_{N}\\\\) and that the set of all possible tokenizations for a word \\\\(x_{i}\\\\) is\n-defined as \\\\(S(x_{i})\\\\), then the overall loss is defined as\n+the words $x_{1}, \\dots, x_{N}$ and that the set of all possible tokenizations for a word $x_{i}$ is\n+defined as $S(x_{i})$, then the overall loss is defined as\n \n $$\\mathcal{L} = -\\sum_{i=1}^{N} \\log \\left ( \\sum_{x \\in S(x_{i})} p(x) \\right )$$\n "
        }
    ],
    "stats": {
        "total": 104,
        "additions": 52,
        "deletions": 52
    }
}