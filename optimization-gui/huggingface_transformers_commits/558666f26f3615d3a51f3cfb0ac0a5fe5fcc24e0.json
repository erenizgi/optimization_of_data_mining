{
    "author": "ankke",
    "message": "Preprocessing fixes and more tests for LFM2-VL (#42784)\n\n* fix processing bugs + add more test cases\n\n* add more image processor tests\n\n* refactor expand_text_with_placeholders\n\n* CI fix",
    "sha": "558666f26f3615d3a51f3cfb0ac0a5fe5fcc24e0",
    "files": [
        {
            "sha": "0567323160fa0791c36212aeff8832b373ea5f58",
            "filename": "src/transformers/models/lfm2_vl/processing_lfm2_vl.py",
            "status": "modified",
            "additions": 82,
            "deletions": 42,
            "changes": 124,
            "blob_url": "https://github.com/huggingface/transformers/blob/558666f26f3615d3a51f3cfb0ac0a5fe5fcc24e0/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/558666f26f3615d3a51f3cfb0ac0a5fe5fcc24e0/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2_vl%2Fprocessing_lfm2_vl.py?ref=558666f26f3615d3a51f3cfb0ac0a5fe5fcc24e0",
            "patch": "@@ -165,63 +165,103 @@ def expand_text_with_placeholders(\n         image_sizes: list[list[int]],\n         use_image_special_tokens: bool,\n         **images_kwargs,\n-    ):\n-        prompt_strings = []\n+    ) -> list[str]:\n+        use_thumbnail = images_kwargs.get(\"use_thumbnail\", self.image_processor.use_thumbnail)\n+        image_data = iter(zip(image_rows, image_cols, image_sizes))\n \n-        image_data = iter(zip(*[image_rows, image_cols, image_sizes]))\n+        prompt_strings = []\n         for sample_text, sample_images in zip(text, images):\n-            split_sample = sample_text.split(self.image_token)\n-            sample_text_with_image_tokens = \"\"\n-            for i, image in enumerate(sample_images):\n-                sample_text_with_image_tokens += split_sample[i]\n-                if use_image_special_tokens:\n-                    sample_text_with_image_tokens += self.image_start_token\n+            text_parts = sample_text.split(self.image_token)\n+            result_parts = []\n+\n+            for i, _ in enumerate(sample_images):\n+                result_parts.append(text_parts[i])\n \n                 rows, cols, image_size = next(image_data)\n-                num_thumbnail_tokens, num_tokens_per_tile = self._get_image_num_tokens(image_size, **images_kwargs)\n-\n-                if rows > 1 or cols > 1:\n-                    for row in range(rows):\n-                        for col in range(cols):\n-                            if use_image_special_tokens:\n-                                sample_text_with_image_tokens += f\"<|img_row_{row + 1}_col_{col + 1}|>\"\n-                            sample_text_with_image_tokens += self.image_token * num_tokens_per_tile\n-\n-                    if num_thumbnail_tokens > 0:\n-                        if use_image_special_tokens:\n-                            sample_text_with_image_tokens += self.image_thumbnail_token\n-                        sample_text_with_image_tokens += self.image_token * num_thumbnail_tokens\n-                else:\n-                    sample_text_with_image_tokens += self.image_token * num_thumbnail_tokens\n+                tokens_per_tile, tokens_for_image = self._get_image_num_tokens(image_size, **images_kwargs)\n+                image_tokens = self._build_image_tokens(\n+                    rows,\n+                    cols,\n+                    tokens_per_tile,\n+                    tokens_for_image,\n+                    use_thumbnail,\n+                    use_image_special_tokens,\n+                )\n+                result_parts.append(image_tokens)\n \n-                if use_image_special_tokens:\n-                    sample_text_with_image_tokens += self.image_end_token\n+            # Add remaining text after the last image\n+            if len(sample_images) < len(text_parts):\n+                result_parts.append(text_parts[-1])\n \n-                sample_text_with_image_tokens += split_sample[i + 1]\n-            prompt_strings.append(sample_text_with_image_tokens)\n+            prompt_strings.append(\"\".join(result_parts))\n \n         return prompt_strings\n \n+    def _build_image_tokens(\n+        self,\n+        rows: int,\n+        cols: int,\n+        tokens_per_tile: int,\n+        tokens_for_image: int,\n+        use_thumbnail: bool,\n+        use_image_special_tokens: bool,\n+    ) -> str:\n+        \"\"\"Build the expanded token string for a single image.\"\"\"\n+        parts = []\n+\n+        if use_image_special_tokens:\n+            parts.append(self.image_start_token)\n+\n+        is_multi_tile = rows > 1 or cols > 1\n+        if is_multi_tile:\n+            for row in range(rows):\n+                for col in range(cols):\n+                    if use_image_special_tokens:\n+                        parts.append(f\"<|img_row_{row + 1}_col_{col + 1}|>\")\n+                    parts.append(self.image_token * tokens_per_tile)\n+\n+            if use_thumbnail:\n+                if use_image_special_tokens:\n+                    parts.append(self.image_thumbnail_token)\n+                parts.append(self.image_token * tokens_for_image)\n+        else:\n+            parts.append(self.image_token * tokens_for_image)\n+\n+        if use_image_special_tokens:\n+            parts.append(self.image_end_token)\n+\n+        return \"\".join(parts)\n+\n+    def _compute_tokens_per_tile(self, tile_size: int, encoder_patch_size: int, downsample_factor: int) -> int:\n+        \"\"\"Compute the number of tokens for a single tile.\"\"\"\n+        num_patches = tile_size // encoder_patch_size\n+        downsampled_patches = math.ceil(num_patches / downsample_factor)\n+        return downsampled_patches * downsampled_patches\n+\n+    def _compute_tokens_for_image(self, image_size: list[int], encoder_patch_size: int, downsample_factor: int) -> int:\n+        \"\"\"Compute the number of tokens for a resized image (used for single-tile or thumbnail).\"\"\"\n+        image_height, image_width = image_size\n+        patches_h = math.ceil((image_height // encoder_patch_size) / downsample_factor)\n+        patches_w = math.ceil((image_width // encoder_patch_size) / downsample_factor)\n+        return patches_h * patches_w\n+\n     def _get_image_num_tokens(self, image_size: list[int], **images_kwargs) -> tuple[int, int]:\n+        \"\"\"\n+        Compute token counts for image processing.\n+\n+        Returns:\n+            tuple[int, int]: (tokens_per_tile, tokens_for_image)\n+                - tokens_per_tile: tokens for each tile in multi-tile mode\n+                - tokens_for_image: tokens for the resized image (single-tile) or thumbnail (multi-tile)\n+        \"\"\"\n         tile_size = images_kwargs.get(\"tile_size\", self.image_processor.tile_size)\n         downsample_factor = images_kwargs.get(\"downsample_factor\", self.image_processor.downsample_factor)\n         encoder_patch_size = images_kwargs.get(\"encoder_patch_size\", self.image_processor.encoder_patch_size)\n-        use_thumbnail = images_kwargs.get(\"use_thumbnail\", self.image_processor.use_thumbnail)\n-\n-        thumbnail_tokens = 0\n-        if use_thumbnail:\n-            image_height, image_width = image_size\n-            num_patches_height = image_height // encoder_patch_size\n-            num_patches_width = image_width // encoder_patch_size\n-            dwn_num_patches_height = math.ceil(num_patches_height / downsample_factor)\n-            dwn_num_patches_width = math.ceil(num_patches_width / downsample_factor)\n-            thumbnail_tokens = dwn_num_patches_height * dwn_num_patches_width\n \n-        num_patches_tile = tile_size // encoder_patch_size\n-        dwn_num_patches_tile = math.ceil(num_patches_tile / downsample_factor)\n-        tile_tokens = dwn_num_patches_tile * dwn_num_patches_tile\n+        tokens_per_tile = self._compute_tokens_per_tile(tile_size, encoder_patch_size, downsample_factor)\n+        tokens_for_image = self._compute_tokens_for_image(image_size, encoder_patch_size, downsample_factor)\n \n-        return thumbnail_tokens, tile_tokens\n+        return tokens_per_tile, tokens_for_image\n \n     def batch_decode(self, *args, **kwargs):\n         \"\"\""
        },
        {
            "sha": "f642ac5cf4e5a6c47c10689a7d4fba39581d0af9",
            "filename": "tests/models/lfm2_vl/test_image_processing_lfm2_vl.py",
            "status": "modified",
            "additions": 546,
            "deletions": 1,
            "changes": 547,
            "blob_url": "https://github.com/huggingface/transformers/blob/558666f26f3615d3a51f3cfb0ac0a5fe5fcc24e0/tests%2Fmodels%2Flfm2_vl%2Ftest_image_processing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/558666f26f3615d3a51f3cfb0ac0a5fe5fcc24e0/tests%2Fmodels%2Flfm2_vl%2Ftest_image_processing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2Ftest_image_processing_lfm2_vl.py?ref=558666f26f3615d3a51f3cfb0ac0a5fe5fcc24e0",
            "patch": "@@ -33,7 +33,10 @@\n \n     if is_torchvision_available():\n         from transformers import Lfm2VlImageProcessorFast\n-        from transformers.models.lfm2_vl.image_processing_lfm2_vl_fast import find_closest_aspect_ratio\n+        from transformers.models.lfm2_vl.image_processing_lfm2_vl_fast import (\n+            find_closest_aspect_ratio,\n+            round_by_factor,\n+        )\n \n \n class Lfm2VlImageProcessingTester:\n@@ -287,3 +290,545 @@ def test_call_pytorch(self):\n                 3 * image_processing.encoder_patch_size**2,\n             ),\n         )\n+\n+    def test_small_image_no_tiling_no_thumbnail(self):\n+        \"\"\"Small image with tiling disabled should use smart resize, no thumbnail.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=False,\n+            use_thumbnail=True,  # even if enabled, should not be used for small/non-tiled images\n+        )\n+        # Create a small image (256x256)\n+        small_image = Image.new(\"RGB\", (256, 256), color=\"red\")\n+        result = image_processing([[small_image]], return_tensors=\"pt\", return_row_col_info=True)\n+\n+        # With tiling disabled, should be 1 tile (no thumbnail)\n+        self.assertEqual(result.image_rows[0].item(), 1)\n+        self.assertEqual(result.image_cols[0].item(), 1)\n+        # Should have exactly 1 image in batch (no thumbnail)\n+        self.assertEqual(result.pixel_values.shape[0], 1)\n+\n+    def test_small_image_tiling_enabled_no_thumbnail(self):\n+        \"\"\"Small image with tiling enabled should not be tiled (too small), no thumbnail.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=True,\n+            use_thumbnail=True,\n+            min_tiles=2,\n+            max_tiles=10,\n+        )\n+        # Create a small image that won't exceed the max_image_tokens threshold\n+        small_image = Image.new(\"RGB\", (256, 256), color=\"blue\")\n+        result = image_processing([[small_image]], return_tensors=\"pt\", return_row_col_info=True)\n+\n+        # Small image should not be tiled (1x1 grid), no thumbnail added\n+        self.assertEqual(result.image_rows[0].item(), 1)\n+        self.assertEqual(result.image_cols[0].item(), 1)\n+        # Should have exactly 1 image in batch (no thumbnail)\n+        self.assertEqual(result.pixel_values.shape[0], 1)\n+\n+    def test_large_image_no_tiling_smart_resize(self):\n+        \"\"\"Large image with tiling disabled should use smart resize, no thumbnail.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=False,\n+            use_thumbnail=True,  # even if enabled, should not be used\n+        )\n+        # Create a large image (2048x2048)\n+        large_image = Image.new(\"RGB\", (2048, 2048), color=\"green\")\n+        result = image_processing([[large_image]], return_tensors=\"pt\", return_row_col_info=True)\n+\n+        # With tiling disabled, should be 1 tile even for large images\n+        self.assertEqual(result.image_rows[0].item(), 1)\n+        self.assertEqual(result.image_cols[0].item(), 1)\n+        # Should have exactly 1 image in batch (no thumbnail, smart resize only)\n+        self.assertEqual(result.pixel_values.shape[0], 1)\n+\n+    def test_large_image_tiling_enabled_thumbnail_disabled(self):\n+        \"\"\"Large image with tiling enabled but thumbnail disabled should tile without thumbnail.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=True,\n+            use_thumbnail=False,\n+            min_tiles=2,\n+            max_tiles=10,\n+            tile_size=512,\n+        )\n+        # Create a large image that will require tiling\n+        large_image = Image.new(\"RGB\", (2048, 2048), color=\"yellow\")\n+        result = image_processing([[large_image]], return_tensors=\"pt\", return_row_col_info=True)\n+\n+        # Large image should be tiled into multiple tiles\n+        num_rows = result.image_rows[0].item()\n+        num_cols = result.image_cols[0].item()\n+        num_tiles = num_rows * num_cols\n+        self.assertGreater(num_tiles, 1, \"Large image should be tiled into multiple tiles\")\n+\n+        # Count actual patches - with thumbnail disabled, should equal number of tiles\n+        num_images_in_batch = result.pixel_values.shape[0]\n+        self.assertEqual(\n+            num_images_in_batch, num_tiles, \"Number of patches should equal number of tiles (no thumbnail)\"\n+        )\n+\n+    def test_large_image_tiling_enabled_thumbnail_enabled(self):\n+        \"\"\"Large image with tiling and thumbnail enabled should tile AND add thumbnail.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=True,\n+            use_thumbnail=True,\n+            min_tiles=2,\n+            max_tiles=10,\n+            tile_size=512,\n+        )\n+        # Create a large image that will require tiling\n+        large_image = Image.new(\"RGB\", (2048, 2048), color=\"purple\")\n+        result = image_processing([[large_image]], return_tensors=\"pt\", return_row_col_info=True)\n+\n+        # Large image should be tiled into multiple tiles\n+        num_rows = result.image_rows[0].item()\n+        num_cols = result.image_cols[0].item()\n+        num_tiles = num_rows * num_cols\n+        self.assertGreater(num_tiles, 1, \"Large image should be tiled into multiple tiles\")\n+\n+        # With thumbnail enabled, we should have tiles + 1 thumbnail\n+        num_images_in_batch = result.pixel_values.shape[0]\n+        self.assertEqual(num_images_in_batch, num_tiles + 1, \"Number of patches should equal tiles + 1 (thumbnail)\")\n+\n+    # ==================== Non-Square Aspect Ratio Tests ====================\n+\n+    def test_landscape_image_aspect_ratio(self):\n+        \"\"\"Test that landscape images (wider than tall) are processed correctly.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=True,\n+            use_thumbnail=True,\n+            min_tiles=2,\n+            max_tiles=10,\n+            tile_size=512,\n+        )\n+        # Create a landscape image (1920x1080, ~16:9 aspect ratio)\n+        landscape_image = Image.new(\"RGB\", (1920, 1080), color=\"blue\")\n+        result = image_processing([[landscape_image]], return_tensors=\"pt\", return_row_col_info=True)\n+\n+        num_rows = result.image_rows[0].item()\n+        num_cols = result.image_cols[0].item()\n+\n+        # Landscape image should have more columns than rows\n+        self.assertGreaterEqual(num_cols, num_rows, \"Landscape image should have cols >= rows\")\n+\n+    def test_extreme_aspect_ratio_wide(self):\n+        \"\"\"Test extremely wide image (panorama-like).\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=True,\n+            use_thumbnail=True,\n+            min_tiles=2,\n+            max_tiles=10,\n+            tile_size=512,\n+        )\n+        # Create an extremely wide image (3000x500)\n+        wide_image = Image.new(\"RGB\", (3000, 500), color=\"red\")\n+        result = image_processing([[wide_image]], return_tensors=\"pt\", return_row_col_info=True)\n+\n+        num_rows = result.image_rows[0].item()\n+        num_cols = result.image_cols[0].item()\n+\n+        # Very wide image should have significantly more cols than rows\n+        self.assertGreater(num_cols, num_rows, \"Very wide image should have cols > rows\")\n+\n+    def test_extreme_aspect_ratio_tall(self):\n+        \"\"\"Test extremely tall image.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=True,\n+            use_thumbnail=True,\n+            min_tiles=2,\n+            max_tiles=10,\n+            tile_size=512,\n+        )\n+        # Create an extremely tall image (500x3000)\n+        tall_image = Image.new(\"RGB\", (500, 3000), color=\"yellow\")\n+        result = image_processing([[tall_image]], return_tensors=\"pt\", return_row_col_info=True)\n+\n+        num_rows = result.image_rows[0].item()\n+        num_cols = result.image_cols[0].item()\n+\n+        # Very tall image should have significantly more rows than cols\n+        self.assertGreater(num_rows, num_cols, \"Very tall image should have rows > cols\")\n+\n+    # ==================== Output Validation Tests ====================\n+\n+    def test_image_sizes_returned_with_row_col_info(self):\n+        \"\"\"Test that image_sizes is returned when return_row_col_info=True.\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+        image = Image.new(\"RGB\", (512, 256), color=\"green\")\n+        result = image_processing([[image]], return_tensors=\"pt\", return_row_col_info=True)\n+\n+        # Check all row/col info is returned\n+        self.assertIn(\"image_rows\", result)\n+        self.assertIn(\"image_cols\", result)\n+        self.assertIn(\"image_sizes\", result)\n+\n+        # image_sizes should contain [height, width] for the resized image\n+        image_sizes = result.image_sizes\n+        self.assertIsInstance(image_sizes, torch.Tensor)\n+        self.assertEqual(image_sizes.shape[0], 1)  # one sample\n+        self.assertEqual(image_sizes.shape[1], 2)  # [height, width]\n+\n+    def test_output_consistency_across_formats(self):\n+        \"\"\"Test that outputs are consistent regardless of input format (PIL, numpy, torch).\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        # Create same image in different formats\n+        pil_image = Image.new(\"RGB\", (256, 256), color=\"white\")\n+        np_image = np.array(pil_image)\n+        torch_image = torch.from_numpy(np_image).permute(2, 0, 1)\n+\n+        result_pil = image_processing([[pil_image]], return_tensors=\"pt\")\n+        result_np = image_processing([[np_image]], return_tensors=\"pt\")\n+        result_torch = image_processing([[torch_image]], return_tensors=\"pt\")\n+\n+        # All should produce same shapes\n+        self.assertEqual(result_pil.pixel_values.shape, result_np.pixel_values.shape)\n+        self.assertEqual(result_pil.pixel_values.shape, result_torch.pixel_values.shape)\n+        self.assertEqual(result_pil.spatial_shapes.tolist(), result_np.spatial_shapes.tolist())\n+        self.assertEqual(result_pil.spatial_shapes.tolist(), result_torch.spatial_shapes.tolist())\n+\n+    # ==================== Multiple Images Per Sample Tests ====================\n+\n+    def test_multiple_images_per_sample(self):\n+        \"\"\"Test processing multiple images in a single sample: [[img1, img2, img3]].\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        img1 = Image.new(\"RGB\", (256, 256), color=\"red\")\n+        img2 = Image.new(\"RGB\", (256, 256), color=\"green\")\n+        img3 = Image.new(\"RGB\", (256, 256), color=\"blue\")\n+\n+        result = image_processing([[img1, img2, img3]], return_tensors=\"pt\")\n+\n+        # Should have 3 images processed\n+        self.assertEqual(result.pixel_values.shape[0], 3)\n+        self.assertEqual(result.spatial_shapes.shape[0], 3)\n+        self.assertEqual(result.pixel_attention_mask.shape[0], 3)\n+\n+    def test_mixed_image_counts_across_batch(self):\n+        \"\"\"Test batch with different number of images per sample: [[img1], [img2, img3]].\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        img1 = Image.new(\"RGB\", (256, 256), color=\"red\")\n+        img2 = Image.new(\"RGB\", (256, 256), color=\"green\")\n+        img3 = Image.new(\"RGB\", (256, 256), color=\"blue\")\n+\n+        # First sample has 1 image, second sample has 2 images\n+        result = image_processing([[img1], [img2, img3]], return_tensors=\"pt\")\n+\n+        # Total should be 3 images (1 + 2)\n+        self.assertEqual(result.pixel_values.shape[0], 3)\n+        self.assertEqual(result.spatial_shapes.shape[0], 3)\n+\n+    def test_multiple_images_different_sizes(self):\n+        \"\"\"Test multiple images per sample with different sizes.\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        img_small = Image.new(\"RGB\", (256, 256), color=\"red\")\n+        img_medium = Image.new(\"RGB\", (512, 512), color=\"green\")\n+        img_large = Image.new(\"RGB\", (768, 768), color=\"blue\")\n+\n+        result = image_processing([[img_small, img_medium, img_large]], return_tensors=\"pt\")\n+\n+        # Should have 3 images processed\n+        self.assertEqual(result.pixel_values.shape[0], 3)\n+        # All should have same max_num_patches due to padding\n+        self.assertEqual(result.pixel_values.shape[1], image_processing.max_num_patches)\n+\n+    # ==================== Parameter Variations Tests ====================\n+\n+    def test_forced_grid_config_min_equals_max(self):\n+        \"\"\"Test forcing a specific grid configuration with min_tiles == max_tiles.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=True,\n+            min_tiles=4,\n+            max_tiles=4,  # Force exactly 4 tiles\n+            tile_size=512,\n+            use_thumbnail=False,\n+        )\n+        # Large image that would normally get more tiles\n+        wide_image = Image.new(\"RGB\", (3000, 500), color=\"red\")\n+        result = image_processing([[wide_image]], return_tensors=\"pt\", return_row_col_info=True)\n+\n+        num_rows = result.image_rows[0].item()\n+        num_cols = result.image_cols[0].item()\n+        num_tiles = num_rows * num_cols\n+\n+        # Should be exactly 4 tiles\n+        self.assertEqual(num_tiles, 4, \"Should have exactly 4 tiles when min_tiles == max_tiles == 4\")\n+\n+    # ==================== Input Validation Tests ====================\n+\n+    def test_min_tiles_greater_than_max_tiles_raises_error(self):\n+        \"\"\"Test that min_tiles > max_tiles raises ValueError.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=True,\n+            min_tiles=10,\n+            max_tiles=2,  # Invalid: min > max\n+        )\n+        image = Image.new(\"RGB\", (1024, 1024), color=\"red\")\n+\n+        with self.assertRaises(ValueError) as context:\n+            image_processing([[image]], return_tensors=\"pt\")\n+\n+        self.assertIn(\"min_tiles\", str(context.exception).lower())\n+\n+    # ==================== Edge Case Images Tests ====================\n+\n+    def test_very_small_image(self):\n+        \"\"\"Test image smaller than encoder_patch_size.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=False,\n+            encoder_patch_size=16,\n+        )\n+        # Image smaller than patch size\n+        tiny_image = Image.new(\"RGB\", (8, 8), color=\"red\")\n+        result = image_processing([[tiny_image]], return_tensors=\"pt\")\n+\n+        # Should still process without error\n+        self.assertIn(\"pixel_values\", result)\n+        self.assertEqual(result.pixel_values.dim(), 3)\n+\n+    def test_grayscale_image(self):\n+        \"\"\"Test that grayscale (1-channel) images are converted to RGB.\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        # Create grayscale image\n+        grayscale_image = Image.new(\"L\", (256, 256), color=128)\n+        result = image_processing([[grayscale_image]], return_tensors=\"pt\")\n+\n+        # Should process and output 3 channels (converted to RGB)\n+        self.assertIn(\"pixel_values\", result)\n+        # pixel_values shape is (batch, num_patches, patch_size^2 * 3)\n+        expected_patch_dim = 3 * image_processing.encoder_patch_size**2\n+        self.assertEqual(result.pixel_values.shape[2], expected_patch_dim)\n+\n+    def test_rgba_4_channel_image(self):\n+        \"\"\"Test that RGBA (4-channel) images are converted to RGB.\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        # Create RGBA image with alpha channel\n+        rgba_image = Image.new(\"RGBA\", (256, 256), color=(255, 0, 0, 128))\n+        result = image_processing([[rgba_image]], return_tensors=\"pt\", do_convert_rgb=True)\n+\n+        # Should process and output 3 channels (alpha dropped)\n+        self.assertIn(\"pixel_values\", result)\n+        expected_patch_dim = 3 * image_processing.encoder_patch_size**2\n+        self.assertEqual(result.pixel_values.shape[2], expected_patch_dim)\n+\n+    def test_numpy_4_channel_rgba(self):\n+        \"\"\"Test actual 4-channel numpy array input - convert to PIL for RGB conversion.\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        # Create 4-channel numpy array (RGBA) and convert to PIL Image for RGB conversion\n+        rgba_np = np.random.randint(0, 255, (256, 256, 4), dtype=np.uint8)\n+        rgba_pil = Image.fromarray(rgba_np, mode=\"RGBA\")\n+        result = image_processing([[rgba_pil]], return_tensors=\"pt\", do_convert_rgb=True)\n+\n+        # Should convert to 3 channels\n+        self.assertIn(\"pixel_values\", result)\n+        expected_patch_dim = 3 * image_processing.encoder_patch_size**2\n+        self.assertEqual(result.pixel_values.shape[2], expected_patch_dim)\n+\n+    def test_single_pixel_image(self):\n+        \"\"\"Test 1x1 pixel image (extreme edge case).\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        single_pixel = Image.new(\"RGB\", (1, 1), color=\"blue\")\n+        result = image_processing([[single_pixel]], return_tensors=\"pt\")\n+\n+        # Should process without error\n+        self.assertIn(\"pixel_values\", result)\n+\n+    # ==================== Helper Function Unit Tests ====================\n+\n+    def test_round_by_factor(self):\n+        \"\"\"Test round_by_factor function.\"\"\"\n+        # Exact multiples should return themselves\n+        self.assertEqual(round_by_factor(32, 16), 32)\n+        self.assertEqual(round_by_factor(64, 16), 64)\n+\n+        # Values should round to nearest multiple\n+        self.assertEqual(round_by_factor(30, 16), 32)  # 30 -> 32 (closer to 32 than 16)\n+        self.assertEqual(round_by_factor(20, 16), 16)  # 20 -> 16 (closer to 16 than 32)\n+        self.assertEqual(round_by_factor(24, 16), 32)  # 24 -> 32 (equidistant, rounds up)\n+\n+        # Test with different factors\n+        self.assertEqual(round_by_factor(100, 32), 96)  # 100 -> 96\n+        self.assertEqual(round_by_factor(50, 32), 64)  # 50 -> 64\n+\n+        # Test with factor of 1\n+        self.assertEqual(round_by_factor(17, 1), 17)\n+\n+    def test_is_image_too_large_small_image(self):\n+        \"\"\"Test _is_image_too_large with small image.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            max_image_tokens=256,\n+            encoder_patch_size=16,\n+            downsample_factor=2,\n+            max_pixels_tolerance=2.0,\n+        )\n+\n+        is_large = image_processing._is_image_too_large(\n+            height=512,\n+            width=512,\n+            max_image_tokens=256,\n+            encoder_patch_size=16,\n+            downsample_factor=2,\n+            max_pixels_tolerance=2.0,\n+        )\n+        self.assertFalse(is_large)\n+\n+    def test_is_image_too_large_large_image(self):\n+        \"\"\"Test _is_image_too_large with large image.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            max_image_tokens=256,\n+            encoder_patch_size=16,\n+            downsample_factor=2,\n+            max_pixels_tolerance=1.0,\n+        )\n+\n+        is_large = image_processing._is_image_too_large(\n+            height=565,\n+            width=565,\n+            max_image_tokens=256,\n+            encoder_patch_size=16,\n+            downsample_factor=2,\n+            max_pixels_tolerance=1.0,\n+        )\n+        self.assertTrue(is_large)\n+\n+    # ==================== Batch Processing Tests ====================\n+\n+    def test_batch_mixed_image_sizes(self):\n+        \"\"\"Test batch processing with different image sizes requiring different processing paths.\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        # Create images with significantly different sizes\n+        small_image = Image.new(\"RGB\", (256, 256), color=\"red\")\n+        medium_image = Image.new(\"RGB\", (512, 512), color=\"green\")\n+        large_image = Image.new(\"RGB\", (1024, 1024), color=\"blue\")\n+\n+        # Process as batch\n+        result = image_processing([[small_image], [medium_image], [large_image]], return_tensors=\"pt\")\n+\n+        # All should be processed and padded to same size\n+        self.assertEqual(result.pixel_values.shape[0], 3)\n+        # All should have same max_num_patches\n+        self.assertEqual(result.pixel_values.shape[1], image_processing.max_num_patches)\n+        # Patch dimension should be patch_size^2 * 3 channels\n+        expected_patch_dim = 3 * image_processing.encoder_patch_size**2\n+        self.assertEqual(result.pixel_values.shape[2], expected_patch_dim)\n+\n+        # Spatial shapes should all be square (equal height and width)\n+        shapes = result.spatial_shapes.tolist()\n+        for shape in shapes:\n+            self.assertEqual(shape[0], shape[1], \"Square images should have equal height and width\")\n+\n+        # pixel_attention_mask should have correct shape\n+        self.assertEqual(result.pixel_attention_mask.shape[0], 3)\n+        self.assertEqual(result.pixel_attention_mask.shape[1], image_processing.max_num_patches)\n+\n+    def test_batch_mixed_aspect_ratios(self):\n+        \"\"\"Test batch with mixed aspect ratios.\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        square = Image.new(\"RGB\", (512, 512), color=\"red\")\n+        landscape = Image.new(\"RGB\", (1024, 512), color=\"green\")\n+        portrait = Image.new(\"RGB\", (512, 1024), color=\"blue\")\n+\n+        result = image_processing([[square], [landscape], [portrait]], return_tensors=\"pt\")\n+\n+        # All should be processed\n+        self.assertEqual(result.pixel_values.shape[0], 3)\n+        self.assertEqual(result.spatial_shapes.shape[0], 3)\n+\n+        # Spatial shapes should reflect aspect ratios: [height, width]\n+        shapes = result.spatial_shapes.tolist()\n+        square_shape, landscape_shape, portrait_shape = shapes\n+\n+        # Square: height == width\n+        self.assertEqual(square_shape[0], square_shape[1], \"Square image should have equal spatial dimensions\")\n+\n+        # Landscape: width > height\n+        self.assertGreater(landscape_shape[1], landscape_shape[0], \"Landscape image should have width > height\")\n+\n+        # Portrait: height > width\n+        self.assertGreater(portrait_shape[0], portrait_shape[1], \"Portrait image should have height > width\")\n+\n+        # pixel_attention_mask should match batch size and max_num_patches\n+        self.assertEqual(result.pixel_attention_mask.shape[0], 3)\n+        self.assertEqual(result.pixel_attention_mask.shape[1], image_processing.max_num_patches)\n+\n+    def test_disable_grouping_single_image(self):\n+        \"\"\"Test disable_grouping parameter with single image.\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        image = Image.new(\"RGB\", (512, 512), color=\"purple\")\n+\n+        # Process with and without disable_grouping\n+        result_grouped = image_processing([[image]], return_tensors=\"pt\", disable_grouping=False)\n+        result_ungrouped = image_processing([[image]], return_tensors=\"pt\", disable_grouping=True)\n+\n+        # Both should produce all expected output keys\n+        for result in [result_grouped, result_ungrouped]:\n+            self.assertIn(\"pixel_values\", result)\n+            self.assertIn(\"spatial_shapes\", result)\n+            self.assertIn(\"pixel_attention_mask\", result)\n+\n+        # Both should have same output shapes for single image\n+        self.assertEqual(result_grouped.pixel_values.shape, result_ungrouped.pixel_values.shape)\n+        self.assertEqual(result_grouped.spatial_shapes.shape, result_ungrouped.spatial_shapes.shape)\n+        self.assertEqual(result_grouped.pixel_attention_mask.shape, result_ungrouped.pixel_attention_mask.shape)\n+\n+        # Verify specific shapes\n+        self.assertEqual(result_ungrouped.pixel_values.shape[0], 1)\n+        self.assertEqual(result_ungrouped.pixel_values.shape[1], image_processing.max_num_patches)\n+        expected_patch_dim = 3 * image_processing.encoder_patch_size**2\n+        self.assertEqual(result_ungrouped.pixel_values.shape[2], expected_patch_dim)\n+\n+    def test_disable_grouping_batch(self):\n+        \"\"\"Test disable_grouping parameter with batch of images.\"\"\"\n+        image_processing = self.fast_image_processing_class(do_image_splitting=False)\n+\n+        # Images of same size - normally would be grouped\n+        img1 = Image.new(\"RGB\", (256, 256), color=\"red\")\n+        img2 = Image.new(\"RGB\", (256, 256), color=\"green\")\n+        img3 = Image.new(\"RGB\", (256, 256), color=\"blue\")\n+\n+        # Process with disable_grouping=True\n+        result = image_processing([[img1], [img2], [img3]], return_tensors=\"pt\", disable_grouping=True)\n+\n+        # Should produce valid output for all images\n+        self.assertEqual(result.pixel_values.shape[0], 3)\n+\n+    def test_batch_with_tiling(self):\n+        \"\"\"Test batch processing when some images need tiling.\"\"\"\n+        image_processing = self.fast_image_processing_class(\n+            do_image_splitting=True,\n+            use_thumbnail=True,\n+            min_tiles=2,\n+            max_tiles=4,\n+            tile_size=512,\n+        )\n+\n+        # Small image (no tiling needed) and large image (will be tiled)\n+        small = Image.new(\"RGB\", (256, 256), color=\"red\")\n+        large = Image.new(\"RGB\", (1024, 1024), color=\"blue\")  # 2x2 tiles at 512\n+\n+        result = image_processing([[small], [large]], return_tensors=\"pt\", return_row_col_info=True)\n+\n+        # Calculate tiles for each image\n+        small_tiles = result.image_rows[0].item() * result.image_cols[0].item()\n+        large_tiles = result.image_rows[1].item() * result.image_cols[1].item()\n+\n+        # Small image: single tile (no splitting needed)\n+        self.assertEqual(small_tiles, 1, \"Small 256x256 image should have 1 tile (no splitting)\")\n+\n+        # Large image: 2x2 = 4 tiles for 1024x1024 with tile_size=512\n+        self.assertEqual(large_tiles, 4, \"Large 1536x1536 image should have 4 tiles (2x2)\")\n+\n+        # Total images: small (1) + large tiles (4) + thumbnail for large (1) = 6\n+        # Thumbnail is only added when there's more than 1 tile\n+        expected_total = 1 + 4 + 1  # small + large_tiles + large_thumbnail\n+        self.assertEqual(result.pixel_values.shape[0], expected_total)\n+        self.assertEqual(result.spatial_shapes.shape[0], expected_total)\n+        self.assertEqual(result.pixel_attention_mask.shape[0], expected_total)"
        },
        {
            "sha": "1e892de6b2f1edb9668decca3a1f3d5739057494",
            "filename": "tests/models/lfm2_vl/test_processing_lfm2_vl.py",
            "status": "modified",
            "additions": 141,
            "deletions": 0,
            "changes": 141,
            "blob_url": "https://github.com/huggingface/transformers/blob/558666f26f3615d3a51f3cfb0ac0a5fe5fcc24e0/tests%2Fmodels%2Flfm2_vl%2Ftest_processing_lfm2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/558666f26f3615d3a51f3cfb0ac0a5fe5fcc24e0/tests%2Fmodels%2Flfm2_vl%2Ftest_processing_lfm2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flfm2_vl%2Ftest_processing_lfm2_vl.py?ref=558666f26f3615d3a51f3cfb0ac0a5fe5fcc24e0",
            "patch": "@@ -456,3 +456,144 @@ def test_missing_images_error(self):\n         with self.assertRaises(ValueError) as context:\n             processor(text=texts, images=None)\n         self.assertTrue(\"We detected 2 tokens in the text but no images were passed\" in str(context.exception))\n+\n+    def test_single_tile_image_with_thumbnail_disabled(self):\n+        \"\"\"Test that single-tile images work correctly when use_thumbnail=False.\"\"\"\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding_side=\"left\")\n+        processor_components[\"image_processor\"] = self.get_component(\"image_processor\", do_image_splitting=False)\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+\n+        image_str = \"<image>\"\n+        text_str = \"Describe this image.\"\n+        text = image_str + text_str\n+\n+        # Test with use_thumbnail=False - this should still generate correct tokens\n+        inputs = processor(text=text, images=self.small_image, use_thumbnail=False)\n+\n+        # Count image tokens in input_ids\n+        num_image_tokens = sum(1 for token_id in inputs[\"input_ids\"][0] if token_id == self.image_token_id)\n+\n+        # Verify we have image tokens (the bug caused 0 tokens)\n+        self.assertGreater(num_image_tokens, 0, \"Single-tile image with use_thumbnail=False should have image tokens\")\n+\n+        # Verify the number of image tokens matches expected based on spatial_shapes\n+        spatial_shape = inputs[\"spatial_shapes\"][0].tolist()\n+        expected_tokens = math.ceil(spatial_shape[0] / processor.image_processor.downsample_factor) * math.ceil(\n+            spatial_shape[1] / processor.image_processor.downsample_factor\n+        )\n+        self.assertEqual(\n+            num_image_tokens,\n+            expected_tokens,\n+            f\"Image tokens ({num_image_tokens}) should match expected ({expected_tokens}) based on spatial shapes\",\n+        )\n+\n+        # Verify pixel_values shape is correct\n+        encoder_feature_dims = (\n+            3 * processor.image_processor.encoder_patch_size * processor.image_processor.encoder_patch_size\n+        )\n+        self.assertEqual(\n+            np.array(inputs[\"pixel_values\"]).shape,\n+            (1, processor.image_processor.max_num_patches, encoder_feature_dims),\n+        )\n+\n+    def test_multi_image(self):\n+        \"\"\"Test that text is correctly processed when multiple images are present.\"\"\"\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding_side=\"left\")\n+        processor_components[\"image_processor\"] = self.get_component(\"image_processor\", do_image_splitting=False)\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+\n+        # Text with multiple images and text segments between them\n+        text_1 = \"First: \"\n+        text_2 = \" Middle: \"\n+        text_3 = \" End.\"\n+        text = text_1 + \"<image>\" + text_2 + \"<image>\" + text_3\n+        images = [[self.small_image, self.small_image]]\n+\n+        inputs = processor(text=text, images=images)\n+\n+        # Construct expected input_ids\n+        tokenized_1 = processor.tokenizer(text_1, add_special_tokens=False)[\"input_ids\"]\n+        tokenized_2 = processor.tokenizer(text_2, add_special_tokens=False)[\"input_ids\"]\n+        tokenized_3 = processor.tokenizer(text_3, add_special_tokens=False)[\"input_ids\"]\n+        image_tokens = [self.image_start_token_id] + [self.image_token_id] * 9 + [self.image_end_token_id]\n+\n+        expected_input_ids = tokenized_1 + image_tokens + tokenized_2 + image_tokens + tokenized_3\n+        self.assertEqual(inputs[\"input_ids\"], [expected_input_ids])\n+        self.assertEqual(inputs[\"attention_mask\"], [[1] * len(expected_input_ids)])\n+\n+    def test_multi_turn_multi_image(self):\n+        \"\"\"Test that text is correctly processed when multiple images are present in a multi-turn conversation.\"\"\"\n+        processor_components = self.prepare_components()\n+        processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding_side=\"left\")\n+        processor_components[\"image_processor\"] = self.get_component(\"image_processor\", do_image_splitting=False)\n+        processor_kwargs = self.prepare_processor_dict()\n+\n+        processor = self.processor_class(**processor_components, **processor_kwargs)\n+\n+        # Simulate a multi-turn conversation with images\n+        messages = [\n+            {\n+                \"role\": \"user\",\n+                \"content\": [\n+                    {\"type\": \"text\", \"text\": \"What is in image A?\"},\n+                    {\"type\": \"image\"},\n+                    {\"type\": \"text\", \"text\": \"And image B?\"},\n+                    {\"type\": \"image\"},\n+                ],\n+            },\n+            {\n+                \"role\": \"assistant\",\n+                \"content\": [{\"type\": \"text\", \"text\": \"Image A shows X. Image B shows Y.\"}],\n+            },\n+            {\n+                \"role\": \"user\",\n+                \"content\": [{\"type\": \"text\", \"text\": \"Tell me more about image A.\"}],\n+            },\n+        ]\n+\n+        text = processor.apply_chat_template(messages, add_generation_prompt=True)\n+        images = [[self.small_image, self.small_image]]\n+\n+        inputs = processor(text=text, images=images, do_image_splitting=False)\n+\n+        # Construct expected input_ids based on the chat template structure\n+        image_tokens = [self.image_start_token_id] + [self.image_token_id] * 9 + [self.image_end_token_id]\n+\n+        # Build expected sequence from chat template parts\n+        bos = processor.tokenizer(self.bos_token, add_special_tokens=False)[\"input_ids\"]\n+        user_start = processor.tokenizer(\"<|im_start|>user\\n\", add_special_tokens=False)[\"input_ids\"]\n+        assistant_start = processor.tokenizer(\"<|im_start|>assistant\\n\", add_special_tokens=False)[\"input_ids\"]\n+        im_end = processor.tokenizer(\"<|im_end|>\\n\", add_special_tokens=False)[\"input_ids\"]\n+\n+        text_a = processor.tokenizer(\"What is in image A?\", add_special_tokens=False)[\"input_ids\"]\n+        text_b = processor.tokenizer(\"And image B?\", add_special_tokens=False)[\"input_ids\"]\n+        assistant_response = processor.tokenizer(\"Image A shows X. Image B shows Y.\", add_special_tokens=False)[\n+            \"input_ids\"\n+        ]\n+        followup = processor.tokenizer(\"Tell me more about image A.\", add_special_tokens=False)[\"input_ids\"]\n+\n+        expected_input_ids = (\n+            bos\n+            + user_start\n+            + text_a\n+            + image_tokens\n+            + text_b\n+            + image_tokens\n+            + im_end\n+            + assistant_start\n+            + assistant_response\n+            + im_end\n+            + user_start\n+            + followup\n+            + im_end\n+            + assistant_start\n+        )\n+\n+        self.assertEqual(inputs[\"input_ids\"], [expected_input_ids])\n+        self.assertEqual(inputs[\"attention_mask\"], [[1] * len(expected_input_ids)])"
        }
    ],
    "stats": {
        "total": 812,
        "additions": 769,
        "deletions": 43
    }
}