{
    "author": "winglian",
    "message": "make sure the FSDP plugin args are appropriately cast to bools (#42566)\n\n* make sure the FSDP plugin args are appropriately cast to bools\n\n* handle fsdp_version properly\n\n* include reshard_after_forward and handle correctly for fsdp2 vs fsdp2\n\n* lint\n\n* chore: lint",
    "sha": "377a8ee73f210476c4efb15170d0c32ad3b2c653",
    "files": [
        {
            "sha": "0eeeedea551a6818aa47f5831006f69f4d858dcc",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 34,
            "deletions": 6,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/377a8ee73f210476c4efb15170d0c32ad3b2c653/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/377a8ee73f210476c4efb15170d0c32ad3b2c653/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=377a8ee73f210476c4efb15170d0c32ad3b2c653",
            "patch": "@@ -2742,10 +2742,24 @@ def _process_fsdp_args(self):\n                         fsdp_plugin_args[\"transformer_cls_names_to_wrap\"] = \",\".join(\n                             self.fsdp_config[\"transformer_layer_cls_to_wrap\"]\n                         )\n-            fsdp_plugin_args[\"fsdp_version\"] = self.fsdp_config.get(\"fsdp_version\", 1)\n+            fsdp_version = int(self.fsdp_config.get(\"version\", 1))\n+            fsdp_plugin_args[\"fsdp_version\"] = fsdp_version\n             prefetch_policy = self.fsdp_config.get(\"backward_prefetch\", \"NO_PREFETCH\")\n-            fsdp_plugin_args[\"backward_prefetch\"] = prefetch_policy.upper()\n-            fsdp_plugin_args[\"forward_prefetch\"] = str(self.fsdp_config.get(\"forward_prefetch\", \"false\")).lower()\n+            if fsdp_version == 2:\n+                fsdp_plugin_args[\"reshard_after_forward\"] = str_to_bool(\n+                    str(self.fsdp_config.get(\"reshard_after_forward\", \"false\")).lower()\n+                )\n+            else:\n+                fsdp_plugin_args[\"forward_prefetch\"] = str_to_bool(\n+                    str(self.fsdp_config.get(\"forward_prefetch\", \"false\")).lower()\n+                )\n+                fsdp_plugin_args[\"backward_prefetch\"] = prefetch_policy.upper()\n+                fsdp_plugin_args[\"reshard_after_forward\"] = str(\n+                    self.fsdp_config.get(\"reshard_after_forward\", \"false\")\n+                ).lower()\n+                fsdp_plugin_args[\"use_orig_params\"] = str_to_bool(\n+                    str(self.fsdp_config.get(\"use_orig_params\", \"true\")).lower()\n+                )\n \n             sync_module_states = str(self.fsdp_config.get(\"sync_module_states\", \"true\")).lower()\n             cpu_ram_efficient_loading = str(self.fsdp_config.get(\"cpu_ram_efficient_loading\", \"false\")).lower()\n@@ -2755,11 +2769,10 @@ def _process_fsdp_args(self):\n                 raise ValueError('`sync_module_states` must be `\"True\"` if `cpu_ram_efficient_loading` is `\"True\"`')\n \n             # we need to set the env here as otherwise we get a warning in accelerate + we need to set it for transformers\n-            fsdp_plugin_args[\"cpu_ram_efficient_loading\"] = cpu_ram_efficient_loading\n+            fsdp_plugin_args[\"cpu_ram_efficient_loading\"] = str_to_bool(cpu_ram_efficient_loading)\n             os.environ[\"FSDP_CPU_RAM_EFFICIENT_LOADING\"] = cpu_ram_efficient_loading\n \n-            fsdp_plugin_args[\"sync_module_states\"] = sync_module_states\n-            fsdp_plugin_args[\"use_orig_params\"] = str(self.fsdp_config.get(\"use_orig_params\", \"true\")).lower()\n+            fsdp_plugin_args[\"sync_module_states\"] = str_to_bool(sync_module_states)\n \n         return fsdp_plugin_args\n \n@@ -2771,3 +2784,18 @@ class ParallelMode(Enum):\n     SAGEMAKER_MODEL_PARALLEL = \"sagemaker_model_parallel\"\n     SAGEMAKER_DATA_PARALLEL = \"sagemaker_data_parallel\"\n     TPU = \"tpu\"\n+\n+\n+def str_to_bool(value, to_bool: bool = True) -> int | bool:\n+    \"\"\"\n+    Converts a string representation of truth to `True` (1) or `False` (0).\n+\n+    True values are `y`, `yes`, `t`, `true`, `on`, and `1`; False value are `n`, `no`, `f`, `false`, `off`, and `0`;\n+    \"\"\"\n+    value = value.lower()\n+    if value in (\"y\", \"yes\", \"t\", \"true\", \"on\", \"1\"):\n+        return 1 if not to_bool else True\n+    elif value in (\"n\", \"no\", \"f\", \"false\", \"off\", \"0\"):\n+        return 0 if not to_bool else False\n+    else:\n+        raise ValueError(f\"invalid truth value {value}\")"
        }
    ],
    "stats": {
        "total": 40,
        "additions": 34,
        "deletions": 6
    }
}