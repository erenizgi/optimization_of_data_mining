{
    "author": "zucchini-nlp",
    "message": "Delete deprecated stuff (#38838)\n\n* delete deprecated stuff\n\n* fix copies\n\n* remove unused tests\n\n* fix modernbert and fuyu\n\n* Update src/transformers/cache_utils.py\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>\n\n* bye bye `seen_tokens`\n\n* address comments\n\n* update typings\n\n* ecnoder decoder models follow same pattern as whisper\n\n* fix copies\n\n* why is it set to False?\n\n* fix switch transformers\n\n* fix encoder decoder models shared weight\n\n* fix copies and RAG\n\n* remove `next_cache`\n\n* fix gptj/git\n\n* fix copies\n\n* fix copies\n\n* style...\n\n* another forgotten docsrting\n\n---------\n\nCo-authored-by: Joao Gante <joaofranciscocardosogante@gmail.com>",
    "sha": "bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
    "files": [
        {
            "sha": "6c31035234bbe2816c7d29451ef89a44c244186e",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 0,
            "deletions": 2,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -99,8 +99,6 @@ self.value_cache[layer_idx] = torch.cat([self.value_cache[layer_idx], value_stat\n \n 2. The cache grows dynamically as more tokens are processed. The sequence length dimension (`seq_len`) increases with each new token.\n \n-3. The cache maintains a count of seen tokens through `self._seen_tokens`. This is updated when the first layer processes a new token.\n-\n The example below demonstrates how to create a generation loop with [`DynamicCache`]. As discussed, the attention mask is a concatenation of past and current token values and `1` is added to the cache position for the next token.\n \n ```py"
        },
        {
            "sha": "0b7052c634851b2be4e03b8c5f62c97ec001d243",
            "filename": "src/transformers/cache_utils.py",
            "status": "modified",
            "additions": 24,
            "deletions": 44,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fcache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fcache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fcache_utils.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -185,17 +185,6 @@ def reorder_cache(self, beam_idx: torch.LongTensor):\n                 device = self.value_cache[layer_idx].device\n                 self.value_cache[layer_idx] = self.value_cache[layer_idx].index_select(0, beam_idx.to(device))\n \n-    @property\n-    def seen_tokens(self):\n-        logger.warning_once(\n-            \"The `seen_tokens` attribute is deprecated and will be removed in v4.41. Use the `cache_position` \"\n-            \"model input instead.\"\n-        )\n-        if hasattr(self, \"_seen_tokens\"):\n-            return self._seen_tokens\n-        else:\n-            return None\n-\n     def get_mask_sizes(self, cache_position: torch.Tensor, layer_idx: int) -> tuple[int, int]:\n         \"\"\"\n         Return a tuple (kv_length, kv_offset) corresponding to the length and offset that will be returned for\n@@ -472,7 +461,6 @@ class DynamicCache(Cache):\n \n     def __init__(self, _distributed_cache_data: Optional[Iterable] = None) -> None:\n         super().__init__()\n-        self._seen_tokens = 0  # Used in `generate` to keep tally of how many tokens the cache has seen\n         self.key_cache: list[torch.Tensor] = []\n         self.value_cache: list[torch.Tensor] = []\n \n@@ -535,10 +523,6 @@ def update(\n         Return:\n             A tuple containing the updated key and value states.\n         \"\"\"\n-        # Update the number of seen tokens\n-        if layer_idx == 0:\n-            self._seen_tokens += key_states.shape[-2]\n-\n         # Update the cache\n         if key_states is not None:\n             if len(self.key_cache) <= layer_idx:\n@@ -605,7 +589,6 @@ def crop(self, max_length: int):\n         if self.get_seq_length() <= max_length:\n             return\n \n-        self._seen_tokens = max_length\n         for idx in range(len(self.key_cache)):\n             if self.key_cache[idx].numel():\n                 self.key_cache[idx] = self.key_cache[idx][..., :max_length, :]\n@@ -617,7 +600,6 @@ def batch_split(self, full_batch_size: int, split_size: int) -> list[\"DynamicCac\n         out = []\n         for i in range(0, full_batch_size, split_size):\n             current_split = DynamicCache()\n-            current_split._seen_tokens = self._seen_tokens\n             current_split.key_cache = [tensor[i : i + split_size] for tensor in self.key_cache]\n             current_split.value_cache = [tensor[i : i + split_size] for tensor in self.value_cache]\n             out.append(current_split)\n@@ -815,10 +797,6 @@ def update(\n         Return:\n             A tuple containing the updated key and value states.\n         \"\"\"\n-        # Update the number of seen tokens\n-        if layer_idx == 0:\n-            self._seen_tokens += key_states.shape[-2]\n-\n         # Update the cache\n         if len(self.key_cache) < layer_idx:\n             raise ValueError(\"OffloadedCache does not support model usage where layers are skipped. Use DynamicCache.\")\n@@ -857,6 +835,9 @@ class QuantizedCache(DynamicCache):\n \n     def __init__(self, cache_config: QuantizedCacheConfig) -> None:\n         super().__init__()\n+\n+        # Used only for QuantCache where the seq-length can't be inferred easily from cache contents\n+        self._seen_tokens = 0\n         self._quantized_key_cache: list[torch.Tensor] = []\n         self._quantized_value_cache: list[torch.Tensor] = []\n \n@@ -1412,6 +1393,19 @@ def __init__(self, self_attention_cache: Cache, cross_attention_cache: Cache):\n         for layer_idx in range(len(cross_attention_cache.key_cache)):\n             self.is_updated[layer_idx] = bool(cross_attention_cache.get_seq_length(layer_idx) > 0)\n \n+    def __iter__(self):\n+        \"\"\"\n+        Support for backwards-compatible `past_key_value` iteration, e.g. `for x in past_key_value:` to iterate over\n+        keys and values\n+        \"\"\"\n+        for layer_idx in range(len(self)):\n+            yield (\n+                self.self_attention_cache.key_cache[layer_idx],\n+                self.self_attention_cache.value_cache[layer_idx],\n+                self.cross_attention_cache.key_cache[layer_idx],\n+                self.cross_attention_cache.value_cache[layer_idx],\n+            )\n+\n     def __getitem__(self, layer_idx: int) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Support for backwards-compatible `past_key_value` indexing, e.g. `past_key_value[0][0].shape[2]` to get the\n@@ -2313,10 +2307,6 @@ def __init__(\n             self._device_key_cache.append(key_cache)\n             self._device_value_cache.append(value_cache)\n \n-        # For backwards compatibility.\n-        # TODO(gante): Remove this.\n-        self._seen_tokens = 0\n-\n         # Create new CUDA stream for parallel prefetching.\n         self._prefetch_stream = torch.cuda.Stream() if self.device.type == \"cuda\" else None\n \n@@ -2350,10 +2340,6 @@ def update(\n         value_states = value_states.to(self.value_cache[layer_idx].dtype)\n \n         if layer_idx == 0:\n-            # Update seen tokens.\n-            # TODO(gante): Remove this.\n-            self._seen_tokens += key_states.shape[-2]\n-\n             # Always there.\n             k_out = self.key_cache[0]\n             v_out = self.value_cache[0]\n@@ -2407,10 +2393,14 @@ def update(\n         return k_out, v_out\n \n     def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n-        \"\"\"Returns the sequence length of the cached states that were seen by the model.\"\"\"\n-\n-        # TODO(gante): Remove this.\n-        return self._seen_tokens\n+        \"\"\"Returns the sequence length of the cached states. A layer index can be optionally passed.\"\"\"\n+        is_empty_layer = (\n+            len(self.key_cache) == 0  # no cache in any layer\n+            or len(self.key_cache) <= layer_idx  # hasn't run a layer with cache after it\n+            or not self.key_cache[layer_idx].numel()  # the layer has no cache\n+        )\n+        layer_seq_length = self.key_cache[layer_idx].shape[-2] if not is_empty_layer else 0\n+        return layer_seq_length\n \n     def get_max_cache_shape(self) -> Optional[int]:\n         \"\"\"Returns the maximum sequence length of the cached states.\"\"\"\n@@ -2420,22 +2410,12 @@ def get_max_cache_shape(self) -> Optional[int]:\n     def reset(self) -> None:\n         \"\"\"Resets the cache values while preserving the objects.\"\"\"\n \n-        # For backwards compatibility.\n-        # TODO(gante): Remove this.\n-        self._seen_tokens = 0\n-\n         # Zero out cache.\n         for layer_idx in range(len(self.key_cache)):\n             # In-place ops prevent breaking the static address.\n             self.key_cache[layer_idx].zero_()\n             self.value_cache[layer_idx].zero_()\n \n-    @property\n-    def seen_tokens(self) -> int:\n-        # For backwards compatibility.\n-        # TODO(gante): Remove this.\n-        return self._seen_tokens\n-\n     def _create_key_value_cache_tensors(\n         self, shape: tuple[int, ...], device: torch.device\n     ) -> tuple[torch.Tensor, torch.Tensor]:"
        },
        {
            "sha": "41a30c1374aaca2b8abb41bb7d3bc7d11c6fc0da",
            "filename": "src/transformers/integrations/executorch.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fintegrations%2Fexecutorch.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fexecutorch.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -15,7 +15,7 @@\n \n import torch\n \n-from ..cache_utils import DynamicCache, HybridCache, StaticCache\n+from ..cache_utils import DynamicCache, EncoderDecoderCache, HybridCache, StaticCache\n from ..generation.configuration_utils import GenerationConfig\n from ..masking_utils import (\n     ALL_MASK_ATTENTION_FUNCTIONS,\n@@ -548,14 +548,15 @@ def __init__(self, model, max_static_cache_length, batch_size):\n         self.lm_head = model.lm_head\n         self.config = model.config\n \n-        # Initialize static cache\n+        # Initialize static cache for decoder and DynamicCache for encoder\n         self.static_cache = StaticCache(\n             config=self.config,\n             max_batch_size=batch_size,\n             max_cache_len=max_static_cache_length,\n             device=\"cpu\",\n             dtype=torch.float32,\n         )\n+        self.cache = EncoderDecoderCache(self.static_cache, DynamicCache())\n \n         # Register cache buffers to make them exportable\n         for i in range(len(self.static_cache.key_cache)):\n@@ -567,7 +568,7 @@ def forward(self, decoder_input_ids, encoder_hidden_states, cache_position):\n         outputs = self.decoder(\n             input_ids=decoder_input_ids,\n             encoder_hidden_states=encoder_hidden_states,\n-            past_key_values=self.static_cache,\n+            past_key_values=self.cache,\n             use_cache=True,\n             cache_position=cache_position,\n         )"
        },
        {
            "sha": "59989aa5927cc9bb52792db840fe5da352833fa9",
            "filename": "src/transformers/modeling_rope_utils.py",
            "status": "modified",
            "additions": 16,
            "deletions": 71,
            "changes": 87,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_rope_utils.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -93,7 +93,6 @@ def _compute_default_rope_parameters(\n     config: Optional[PretrainedConfig] = None,\n     device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n-    **rope_kwargs,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies according to the original RoPE implementation\n@@ -104,25 +103,14 @@ def _compute_default_rope_parameters(\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n             The current sequence length. Unused for this type of RoPE.\n-        rope_kwargs (`Dict`, *optional*):\n-            BC compatibility with the previous RoPE class instantiation, will be removed in v4.45.\n     Returns:\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n     \"\"\"\n-    if config is not None and len(rope_kwargs) > 0:\n-        raise ValueError(\n-            \"Unexpected arguments: `**rope_kwargs` and `config` are mutually exclusive in \"\n-            f\"`_compute_default_rope_parameters`, got `rope_kwargs`={rope_kwargs} and `config`={config}\"\n-        )\n-    if len(rope_kwargs) > 0:\n-        base = rope_kwargs[\"base\"]\n-        dim = rope_kwargs[\"dim\"]\n-    elif config is not None:\n-        base = config.rope_theta\n-        partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n-        head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n-        dim = int(head_dim * partial_rotary_factor)\n+    base = config.rope_theta\n+    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n+    head_dim = getattr(config, \"head_dim\", None) or config.hidden_size // config.num_attention_heads\n+    dim = int(head_dim * partial_rotary_factor)\n \n     attention_factor = 1.0  # Unused in this type of RoPE\n \n@@ -135,7 +123,6 @@ def _compute_linear_scaling_rope_parameters(\n     config: Optional[PretrainedConfig] = None,\n     device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n-    **rope_kwargs,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with linear scaling. Credits to the Reddit user /u/kaiokendev\n@@ -146,24 +133,14 @@ def _compute_linear_scaling_rope_parameters(\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n             The current sequence length. Unused for this type of RoPE.\n-        rope_kwargs (`Dict`, *optional*):\n-            BC compatibility with the previous RoPE class instantiation, will be removed in v4.45.\n     Returns:\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n     \"\"\"\n-    if config is not None and len(rope_kwargs) > 0:\n-        raise ValueError(\n-            \"Unexpected arguments: `**rope_kwargs` and `config` are mutually exclusive in \"\n-            f\"`_compute_linear_scaling_rope_parameters`, got `rope_kwargs`={rope_kwargs} and `config`={config}\"\n-        )\n-    if len(rope_kwargs) > 0:\n-        factor = rope_kwargs[\"factor\"]\n-    elif config is not None:\n-        factor = config.rope_scaling[\"factor\"]\n+    factor = config.rope_scaling[\"factor\"]\n \n     # Gets the default RoPE parameters\n-    inv_freq, attention_factor = _compute_default_rope_parameters(config, device, seq_len, **rope_kwargs)\n+    inv_freq, attention_factor = _compute_default_rope_parameters(config, device, seq_len)\n \n     # Then applies linear scaling to the frequencies.\n     # NOTE: originally, scaling was applied to the position_ids. However, we get `embs = inv_freq @ position_ids`, so\n@@ -176,7 +153,6 @@ def _compute_dynamic_ntk_parameters(\n     config: Optional[PretrainedConfig] = None,\n     device: Optional[\"torch.device\"] = None,\n     seq_len: Optional[int] = None,\n-    **rope_kwargs,\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with NTK scaling. Credits to the Reddit users /u/bloc97 and /u/emozilla\n@@ -187,30 +163,17 @@ def _compute_dynamic_ntk_parameters(\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n             The current sequence length, used to update the dynamic RoPE at inference time.\n-        rope_kwargs (`Dict`, *optional*):\n-            BC compatibility with the previous RoPE class instantiation, will be removed in v4.45.\n     Returns:\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin (unused in this type of RoPE).\n     \"\"\"\n     # TODO (joao): use the new `original_max_position_embeddings` from rope_scaling\n-    if config is not None and len(rope_kwargs) > 0:\n-        raise ValueError(\n-            \"Unexpected arguments: `**rope_kwargs` and `config` are mutually exclusive in \"\n-            f\"`_compute_dynamic_ntk_parameters`, got `rope_kwargs`={rope_kwargs} and `config`={config}\"\n-        )\n-    if len(rope_kwargs) > 0:\n-        base = rope_kwargs[\"base\"]\n-        dim = rope_kwargs[\"dim\"]\n-        max_position_embeddings = rope_kwargs[\"max_position_embeddings\"]\n-        factor = rope_kwargs[\"factor\"]\n-    elif config is not None:\n-        base = config.rope_theta\n-        partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n-        head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n-        dim = int(head_dim * partial_rotary_factor)\n-        max_position_embeddings = config.max_position_embeddings\n-        factor = config.rope_scaling[\"factor\"]\n+    base = config.rope_theta\n+    partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n+    head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+    dim = int(head_dim * partial_rotary_factor)\n+    max_position_embeddings = config.max_position_embeddings\n+    factor = config.rope_scaling[\"factor\"]\n \n     attention_factor = 1.0  # Unused in this type of RoPE\n \n@@ -232,7 +195,7 @@ def _compute_dynamic_ntk_parameters(\n \n \n def _compute_yarn_parameters(\n-    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None, **rope_kwargs\n+    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with NTK scaling. Please refer to the\n@@ -244,17 +207,10 @@ def _compute_yarn_parameters(\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n             The current sequence length. Unused for this type of RoPE.\n-        rope_kwargs (`Dict`, *optional*):\n-            BC compatibility with the previous RoPE class instantiation, will be removed in v4.45.\n     Returns:\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin.\n     \"\"\"\n-    # No need to keep BC with yarn, unreleased when this new pattern was created.\n-    if len(rope_kwargs) > 0:\n-        raise ValueError(\n-            f\"Unexpected arguments: `**rope_kwargs` should be unset in `_compute_yarn_parameters`, got {rope_kwargs}\"\n-        )\n \n     base = config.rope_theta\n     partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n@@ -328,7 +284,7 @@ def linear_ramp_factor(min, max, dim):\n \n \n def _compute_longrope_parameters(\n-    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None, **rope_kwargs\n+    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies with LongRoPE scaling. Please refer to the\n@@ -340,20 +296,11 @@ def _compute_longrope_parameters(\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n             The current sequence length.\n-        rope_kwargs (`Dict`, *optional*):\n-            BC compatibility with the previous RoPE class instantiation, will be removed in v4.45.\n     Returns:\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin.\n     \"\"\"\n     # TODO (joao): use the new `original_max_position_embeddings` from rope_scaling\n-    # No need to keep BC with longrope, unreleased when this new pattern was created.\n-    if len(rope_kwargs) > 0:\n-        raise ValueError(\n-            \"Unexpected arguments: `**rope_kwargs` should be unset in `_compute_longrope_parameters`, got \"\n-            f\"{rope_kwargs}\"\n-        )\n-\n     base = config.rope_theta\n     partial_rotary_factor = config.partial_rotary_factor if hasattr(config, \"partial_rotary_factor\") else 1.0\n     head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n@@ -391,7 +338,7 @@ def _compute_longrope_parameters(\n \n \n def _compute_llama3_parameters(\n-    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None, **rope_kwargs\n+    config: PretrainedConfig, device: \"torch.device\", seq_len: Optional[int] = None\n ) -> tuple[\"torch.Tensor\", float]:\n     \"\"\"\n     Computes the inverse frequencies for llama 3.1.\n@@ -403,14 +350,12 @@ def _compute_llama3_parameters(\n             The device to use for initialization of the inverse frequencies.\n         seq_len (`int`, *optional*):\n             The current sequence length. Unused for this type of RoPE.\n-        rope_kwargs (`Dict`, *optional*):\n-            BC compatibility with the previous RoPE class instantiation, will be removed in v4.45.\n     Returns:\n         Tuple of (`torch.Tensor`, `float`), containing the inverse frequencies for the RoPE embeddings and the\n         post-processing scaling factor applied to the computed cos/sin.\n     \"\"\"\n     # Gets the default RoPE parameters\n-    inv_freq, attention_factor = _compute_default_rope_parameters(config, device, seq_len, **rope_kwargs)\n+    inv_freq, attention_factor = _compute_default_rope_parameters(config, device, seq_len)\n \n     factor = config.rope_scaling[\"factor\"]  # `8` in the original implementation\n     low_freq_factor = config.rope_scaling[\"low_freq_factor\"]  # `1` in the original implementation"
        },
        {
            "sha": "dfc98c4405b45472453e742b0b29e6b0c8bfb532",
            "filename": "src/transformers/models/align/processing_align.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fprocessing_align.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -19,7 +19,7 @@\n from typing import Union\n \n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n \n \n@@ -110,8 +110,6 @@ def __call__(\n         \"\"\"\n         if text is None and images is None:\n             raise ValueError(\"You must specify either text or images.\")\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n \n         output_kwargs = self._merge_kwargs(\n             AlignProcessorKwargs,"
        },
        {
            "sha": "386346ae2a63001dc32b79f06f87b9763f651114",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -920,7 +920,7 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -947,7 +947,7 @@ class AriaCausalLMOutputWithPast(ModelOutput):\n )\n class AriaModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -1034,7 +1034,7 @@ def forward(\n         pixel_mask: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1192,7 +1192,7 @@ def forward(\n         pixel_mask: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "aeebd1dd09d1dcc8695135a3fe084744752d6fb1",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -18,6 +18,7 @@\n import numpy as np\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...configuration_utils import PretrainedConfig\n from ...image_processing_utils import BaseImageProcessor, BatchFeature, get_patch_output_size, select_best_resolution\n from ...image_transforms import PaddingMode, convert_to_rgb, pad, resize, to_channel_dimension_format\n@@ -1431,7 +1432,7 @@ def forward(\n         pixel_mask: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1528,7 +1529,7 @@ def forward(\n         pixel_mask: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "599bc63b115624063cbccec7acb9113c07551077",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -26,6 +26,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n@@ -129,7 +130,7 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -156,7 +157,7 @@ class AyaVisionCausalLMOutputWithPast(ModelOutput):\n )\n class AyaVisionModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -261,7 +262,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n@@ -413,7 +414,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "a533ca32daa431b14eb08fc7d2e1eb98fbac796d",
            "filename": "src/transformers/models/aya_vision/modular_aya_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodular_aya_vision.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -29,6 +29,7 @@\n )\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n@@ -181,7 +182,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n@@ -267,7 +268,7 @@ def forward(\n         pixel_values: Optional[torch.FloatTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "7d768a73482740b398a401b4d2fd3987a8c09671",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -949,7 +949,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -998,7 +998,7 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n@@ -1230,7 +1230,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1402,7 +1402,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1901,7 +1901,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "1e126fbcaff88e85e05e694f8db67a008457a1cb",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -2107,7 +2107,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -2156,7 +2156,7 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n@@ -2381,7 +2381,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -2543,7 +2543,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "659b856a77c11e834c592ff676d35a53a260989e",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -953,7 +953,7 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n@@ -1186,7 +1186,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1361,7 +1361,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1551,7 +1551,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "f82a0d322283a37fa917e4fbf7ff346d8241adf8",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -934,7 +934,7 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n@@ -1153,7 +1153,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1315,7 +1315,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple, BaseModelOutput]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1505,7 +1505,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "6a1866ffcbb3e4fa1b38a69a95b00f084a2f133f",
            "filename": "src/transformers/models/blip_2/modeling_blip_2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip_2%2Fmodeling_blip_2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -24,6 +24,7 @@\n from torch.nn import CrossEntropyLoss\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import (\n@@ -774,7 +775,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.attention(\n@@ -1146,7 +1147,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,"
        },
        {
            "sha": "0360bdf6f72422b6e2c8727b0259262bf880a4dc",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 12,
            "deletions": 41,
            "changes": 53,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -328,12 +328,7 @@ def forward(\n             output_tensor = self.dense(context_layer)\n \n         output_tensor = dropout_add(output_tensor, residual, self.hidden_dropout, self.training)\n-\n-        outputs = (output_tensor, layer_past)\n-        if output_attentions:\n-            outputs += (attention_probs,)\n-\n-        return outputs\n+        return output_tensor, attention_probs\n \n \n class BloomMLP(nn.Module):\n@@ -405,7 +400,7 @@ def forward(\n             residual = hidden_states\n \n         # Self attention.\n-        attn_outputs = self.self_attention(\n+        attention_output, attn_weights = self.self_attention(\n             layernorm_output,\n             residual,\n             layer_past=layer_past,\n@@ -417,10 +412,6 @@ def forward(\n             cache_position=cache_position,\n         )\n \n-        attention_output = attn_outputs[0]\n-\n-        outputs = attn_outputs[1:]\n-\n         layernorm_output = self.post_attention_layernorm(attention_output)\n \n         # Get residual\n@@ -432,12 +423,7 @@ def forward(\n         # MLP.\n         output = self.mlp(layernorm_output, residual)\n \n-        if use_cache:\n-            outputs = (output,) + outputs\n-        else:\n-            outputs = (output,) + outputs[1:]\n-\n-        return outputs  # hidden_states, past_kv, attentions\n+        return output, attn_weights  # hidden_states, attentions\n \n \n @auto_docstring\n@@ -560,19 +546,12 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         batch_size, seq_length, _ = inputs_embeds.shape\n         past_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -587,7 +566,6 @@ def forward(\n         head_mask = self.get_head_mask(head_mask, self.config.n_layer)\n         hidden_states = self.word_embeddings_layernorm(inputs_embeds)\n \n-        next_decoder_cache = None\n         all_self_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n \n@@ -618,30 +596,23 @@ def forward(\n             )\n \n             hidden_states = outputs[0]\n-            if use_cache:\n-                next_decoder_cache = outputs[1]\n-\n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n \n         # Add last hidden state\n         hidden_states = self.ln_f(hidden_states)\n \n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n-                v for v in [hidden_states, next_cache, all_hidden_states, all_self_attentions] if v is not None\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attentions] if v is not None\n             )\n \n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n         )"
        },
        {
            "sha": "a7864e28bad8bfeb222d7ea87ef4059e4931a5ba",
            "filename": "src/transformers/models/chameleon/processing_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fprocessing_chameleon.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -28,7 +28,6 @@\n     ProcessorMixin,\n     TextKwargs,\n     Unpack,\n-    _validate_images_text_input_order,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n \n@@ -129,8 +128,7 @@ def __call__(\n               `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n+\n         if isinstance(text, str):\n             text = [text]\n         elif not isinstance(text, list) and not isinstance(text[0], str):"
        },
        {
            "sha": "f39364180171901f053c57fce1362428c90239f8",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 13,
            "deletions": 41,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -217,12 +217,7 @@ def forward(\n         attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_dim)\n         attn_output = self.out_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n-\n-        outputs = (attn_output, layer_past)\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs  # a, present, (attentions)\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.gptj.modeling_gptj.GPTJMLP with GPTJ->CodeGen\n@@ -268,7 +263,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], Optional[tuple[torch.Tensor, tuple[torch.FloatTensor, ...]]]]:\n         residual = hidden_states\n         hidden_states = self.ln_1(hidden_states)\n-        attn_outputs = self.attn(\n+        attn_outputs, attn_weights = self.attn(\n             hidden_states=hidden_states,\n             layer_past=layer_past,\n             attention_mask=attention_mask,\n@@ -278,18 +273,10 @@ def forward(\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n-        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n-        outputs = attn_outputs[1:]\n-\n         feed_forward_hidden_states = self.mlp(hidden_states)\n-        hidden_states = attn_output + feed_forward_hidden_states + residual\n-\n-        if use_cache:\n-            outputs = (hidden_states,) + outputs\n-        else:\n-            outputs = (hidden_states,) + outputs[1:]\n+        hidden_states = attn_outputs + feed_forward_hidden_states + residual\n \n-        return outputs  # hidden_states, present, (attentions)\n+        return hidden_states, attn_weights\n \n \n @auto_docstring\n@@ -390,19 +377,12 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         seq_length = inputs_embeds.shape[1]\n         if cache_position is None:\n@@ -431,7 +411,6 @@ def forward(\n         hidden_states = self.drop(hidden_states)\n         output_shape = (-1, seq_length, hidden_states.size(-1))\n \n-        next_decoder_cache = None\n         all_self_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n         for i, block in enumerate(self.h):\n@@ -450,11 +429,8 @@ def forward(\n             )\n \n             hidden_states = outputs[0]\n-            if use_cache is True:\n-                next_decoder_cache = outputs[1]\n-\n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n \n         hidden_states = self.ln_f(hidden_states)\n \n@@ -463,18 +439,14 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n-                v for v in [hidden_states, next_cache, all_hidden_states, all_self_attentions] if v is not None\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attentions] if v is not None\n             )\n \n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n         )"
        },
        {
            "sha": "0ac554ad2eae8d88315f0bf8344f889c22be1775",
            "filename": "src/transformers/models/colpali/modeling_colpali.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolpali%2Fmodeling_colpali.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -63,7 +63,7 @@ class ColPaliForRetrievalOutput(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n         The embeddings of the model.\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n "
        },
        {
            "sha": "65bfa7d9214f8ff8e279827d71e76a19067089d1",
            "filename": "src/transformers/models/colqwen2/modeling_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodeling_colqwen2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -75,7 +75,7 @@ class ColQwen2ForRetrievalOutput(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n         The embeddings of the model.\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -130,7 +130,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "08b79e247e685d1d88f2f694fb2fdb455eb2cf23",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -243,7 +243,7 @@ class ColQwen2ForRetrievalOutput(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     embeddings (`torch.FloatTensor` of shape `(batch_size, sequence_length, hidden_size)`):\n         The embeddings of the model.\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -280,7 +280,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "7c18216eeb347661d9991dbf2b8e97e60f4d8a37",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -58,7 +58,7 @@ class CsmOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n "
        },
        {
            "sha": "f6098993c387100d59125ad0b9ca94ca14689d91",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -58,7 +58,7 @@ class CsmOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n "
        },
        {
            "sha": "84c72d9ac2c4ae208e1002260ce19cc2bab82972",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 14,
            "deletions": 32,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -305,7 +305,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class DbrxFlashAttention2(DbrxAttention):\n@@ -430,7 +430,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class DbrxSdpaAttention(DbrxAttention):\n@@ -525,7 +525,7 @@ def forward(\n \n         attn_output = self.out_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None\n \n \n DBRX_ATTENTION_CLASSES = {\n@@ -561,7 +561,7 @@ def forward(\n         residual_states = hidden_states\n         hidden_states = self.norm_1(hidden_states).to(hidden_states.dtype)\n \n-        hidden_states, attn_weights, past_key_value = self.attn(\n+        hidden_states, attn_weights = self.attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -578,7 +578,7 @@ def forward(\n         residual_states = hidden_states\n         hidden_states = self.norm_2(hidden_states).to(hidden_states.dtype)\n \n-        return residual_states, hidden_states, attn_weights, past_key_value\n+        return residual_states, hidden_states, attn_weights\n \n \n class DbrxRouter(nn.Module):\n@@ -775,7 +775,7 @@ def forward(\n         \"\"\"\n \n         # Norm + Attention + Norm\n-        resid_states, hidden_states, self_attn_weights, present_key_value = self.norm_attn_norm(\n+        resid_states, hidden_states, self_attn_weights = self.norm_attn_norm(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -796,9 +796,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         if output_router_logits:\n             outputs += (router_logits,)\n \n@@ -909,19 +906,12 @@ def forward(\n \n         inputs_embeds = nn.functional.dropout(inputs_embeds, p=self.emb_pdrop, training=self.training)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -943,7 +933,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_logits = () if output_router_logits else None\n-        next_decoder_cache = None\n \n         for block in self.blocks:\n             if output_hidden_states:\n@@ -962,9 +951,6 @@ def forward(\n \n             hidden_states = block_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = block_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (block_outputs[1],)\n \n@@ -977,19 +963,15 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v\n-                for v in [hidden_states, next_cache, all_hidden_states, all_self_attns, all_router_logits]\n+                for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns, all_router_logits]\n                 if v is not None\n             )\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,"
        },
        {
            "sha": "ebcdf42086a3efe4c68d7a83c392b71e0b70a280",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 15,
            "deletions": 48,
            "changes": 63,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -226,10 +226,6 @@ def __init__(self, config: FalconConfig, layer_idx=None):\n         self.attention_dropout = nn.Dropout(config.attention_dropout)\n         self.num_kv_heads = config.num_kv_heads if (self.new_decoder_architecture or not self.multi_query) else 1\n \n-        # TODO (raushan): remove in v4.46 (RoPE is computed in the model, not in the decoder layers)\n-        if config.rotary:\n-            self.rotary_emb = FalconRotaryEmbedding(config=self.config)\n-\n     def _split_heads(self, fused_qkv: torch.Tensor) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor]:\n         \"\"\"\n         Split the last dimension into (num_heads, head_dim), results share same memory storage as `fused_qkv`\n@@ -362,10 +358,7 @@ def forward(\n \n             attn_output = self.dense(attn_output)\n \n-            if output_attentions:\n-                return attn_output, layer_past, attention_scores\n-            else:\n-                return attn_output, layer_past\n+            return attn_output, attention_scores\n \n         else:\n             if self._use_sdpa and not output_attentions and head_mask is None:\n@@ -380,6 +373,7 @@ def forward(\n                     dropout_p=self.attention_dropout.p if self.training else 0.0,\n                     is_causal=is_causal,\n                 )\n+                attention_probs = None\n                 attn_output = attn_output.transpose(1, 2)\n                 attn_output = attn_output.reshape(batch_size, query_length, self.num_heads * self.head_dim)\n \n@@ -416,10 +410,7 @@ def forward(\n \n                 attn_output = self.dense(attn_output)\n \n-            if output_attentions:\n-                return attn_output, layer_past, attention_probs\n-            else:\n-                return attn_output, layer_past\n+            return attn_output, attention_probs\n \n \n class FalconFlashAttention2(FalconAttention):\n@@ -528,7 +519,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, layer_past, attn_weights\n+        return attn_output, attn_weights\n \n \n class FalconMLP(nn.Module):\n@@ -603,7 +594,7 @@ def forward(\n             attention_layernorm_out = self.input_layernorm(hidden_states)\n \n         # Self attention.\n-        attn_outputs = self.self_attention(\n+        attention_output, attn_weights = self.self_attention(\n             attention_layernorm_out,\n             layer_past=layer_past,\n             attention_mask=attention_mask,\n@@ -616,8 +607,6 @@ def forward(\n             position_embeddings=position_embeddings,\n         )\n \n-        attention_output = attn_outputs[0]\n-\n         if not self.config.new_decoder_architecture:\n             if self.config.parallel_attn:\n                 mlp_layernorm_out = attention_layernorm_out\n@@ -634,8 +623,6 @@ def forward(\n         ):\n             mlp_layernorm_out = attention_layernorm_out\n \n-        outputs = attn_outputs[1:]\n-\n         # MLP.\n         mlp_output = self.mlp(mlp_layernorm_out)\n \n@@ -644,12 +631,7 @@ def forward(\n \n         output = dropout_add(mlp_output, residual, self.config.hidden_dropout, training=self.training)\n \n-        if use_cache:\n-            outputs = (output,) + outputs\n-        else:\n-            outputs = (output,) + outputs[1:]\n-\n-        return outputs  # hidden_states, past_kv, attentions\n+        return output, attn_weights\n \n \n @auto_docstring\n@@ -777,19 +759,12 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.word_embeddings(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         # Compute alibi tensor: check build_alibi_tensor documentation\n         alibi = None\n@@ -827,7 +802,6 @@ def forward(\n         # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-        next_decoder_cache = None\n         all_self_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n \n@@ -849,30 +823,23 @@ def forward(\n             )\n \n             hidden_states = outputs[0]\n-            if use_cache is True:\n-                next_decoder_cache = outputs[1]\n-\n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n \n         # Add last hidden state\n         hidden_states = self.ln_f(hidden_states)\n \n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n-                v for v in [hidden_states, next_cache, all_hidden_states, all_self_attentions] if v is not None\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attentions] if v is not None\n             )\n \n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n         )"
        },
        {
            "sha": "c75644036b5c002eac63e82d4728493b72c3a06c",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -91,8 +91,6 @@ def __init__(\n         self.has_previous_state = False\n         self.conv_kernel_size = config.mamba_d_conv\n \n-        self._seen_tokens = 0\n-\n         self.intermediate_size = (\n             config.mamba_d_ssm if config.mamba_d_ssm is not None else int(config.mamba_expand * config.hidden_size)\n         )\n@@ -149,10 +147,6 @@ def update(\n         Return:\n             A tuple containing the updated key and value states.\n         \"\"\"\n-        # Update the number of seen tokens\n-        if layer_idx == 0:\n-            self._seen_tokens += key_states.shape[-2]\n-\n         # Update the cache\n         if len(self.key_cache) <= layer_idx:\n             # There may be skipped layers, fill them with empty lists"
        },
        {
            "sha": "13146e7bd1f9dbce30a9af1755b70bddd825b8e3",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -99,8 +99,6 @@ def __init__(\n         self.has_previous_state = False\n         self.conv_kernel_size = config.mamba_d_conv\n \n-        self._seen_tokens = 0\n-\n         self.intermediate_size = (\n             config.mamba_d_ssm if config.mamba_d_ssm is not None else int(config.mamba_expand * config.hidden_size)\n         )\n@@ -157,10 +155,6 @@ def update(\n         Return:\n             A tuple containing the updated key and value states.\n         \"\"\"\n-        # Update the number of seen tokens\n-        if layer_idx == 0:\n-            self._seen_tokens += key_states.shape[-2]\n-\n         # Update the cache\n         if len(self.key_cache) <= layer_idx:\n             # There may be skipped layers, fill them with empty lists"
        },
        {
            "sha": "589620ff804cb38947fffb5ee802f59036ba709b",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 5,
            "deletions": 9,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -20,6 +20,7 @@\n import torch.utils.checkpoint\n from torch import nn\n \n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_outputs import CausalLMOutputWithPast\n from ...modeling_utils import PreTrainedModel\n@@ -41,6 +42,7 @@ class FuyuPreTrainedModel(PreTrainedModel):\n     _supports_flash_attn_3 = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n+    _supports_cache_class = True\n     _no_split_modules = []\n     _skip_keys_device_placement = \"past_key_values\"\n \n@@ -155,7 +157,7 @@ def forward(\n         image_patches_indices: torch.Tensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -187,15 +189,9 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_is or inputs_embeds\")\n \n-        seq_length_with_past = seq_length\n-        past_key_values_length = 0\n-\n-        if past_key_values is not None:\n-            past_key_values_length = past_key_values[0][0].shape[2]\n-            seq_length_with_past = seq_length_with_past + past_key_values_length\n-\n         if position_ids is None:\n             device = input_ids.device if input_ids is not None else inputs_embeds.device\n+            past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n             position_ids = torch.arange(\n                 past_key_values_length, seq_length + past_key_values_length, dtype=torch.long, device=device\n             )\n@@ -281,7 +277,7 @@ def forward(\n         image_patches_indices: torch.Tensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         labels: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "f13869a146dbdf330db25b444870b6d7470fa220",
            "filename": "src/transformers/models/fuyu/processing_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fprocessing_fuyu.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -27,7 +27,6 @@\n     ProcessingKwargs,\n     ProcessorMixin,\n     Unpack,\n-    _validate_images_text_input_order,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_torch_available, logging, requires_backends\n@@ -522,8 +521,6 @@ def __call__(\n         # --- Check input validity ---\n         if text is None and images is None:\n             raise ValueError(\"You have to specify either text or images. Both cannot be None.\")\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n \n         output_kwargs = self._merge_kwargs(\n             FuyuProcessorKwargs,"
        },
        {
            "sha": "d65aed200c748a363fdea2f00ca32da9ce9e7483",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -63,7 +63,7 @@\n )\n class Gemma3ModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -89,7 +89,7 @@ class Gemma3CausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n "
        },
        {
            "sha": "ad52c63d3d384ec2d3539477660cea88497163cb",
            "filename": "src/transformers/models/git/modeling_git.py",
            "status": "modified",
            "additions": 14,
            "deletions": 42,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fmodeling_git.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -227,10 +227,7 @@ def forward(\n         new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n         context_layer = context_layer.view(new_context_layer_shape)\n \n-        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)\n-\n-        outputs = outputs + (past_key_value,)\n-        return outputs\n+        return context_layer, attention_probs\n \n \n # Copied from transformers.models.bert.modeling_bert.BertSelfOutput\n@@ -290,17 +287,16 @@ def forward(\n         output_attentions: Optional[bool] = False,\n         pixel_values_present: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n-        self_outputs = self.self(\n+        attn_output, self_attn_weights = self.self(\n             hidden_states,\n             attention_mask,\n             head_mask,\n             past_key_value,\n             output_attentions,\n             pixel_values_present,\n         )\n-        attention_output = self.output(self_outputs[0], hidden_states)\n-        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them\n-        return outputs\n+        attention_output = self.output(attn_output, hidden_states)\n+        return attention_output, self_attn_weights\n \n \n # Copied from transformers.models.bert.modeling_bert.BertIntermediate\n@@ -353,29 +349,19 @@ def forward(\n         pixel_values_present: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         # decoder uni-directional self-attention cached key/values tuple is at positions 1,2\n-        self_attention_outputs = self.attention(\n+        attention_output, self_attention_weights = self.attention(\n             hidden_states,\n             attention_mask,\n             head_mask,\n             output_attentions=output_attentions,\n             past_key_value=past_key_value,\n             pixel_values_present=pixel_values_present,\n         )\n-        attention_output = self_attention_outputs[0]\n-\n-        # if decoder, the last output is tuple of self-attn cache\n-        outputs = self_attention_outputs[1:-1]\n-        present_key_value = self_attention_outputs[-1]\n \n         layer_output = apply_chunking_to_forward(\n             self.feed_forward_chunk, self.chunk_size_feed_forward, self.seq_len_dim, attention_output\n         )\n-        outputs = (layer_output,) + outputs\n-\n-        # if decoder, return the attn key/values as the last output\n-        outputs = outputs + (present_key_value,)\n-\n-        return outputs\n+        return layer_output, self_attention_weights\n \n     def feed_forward_chunk(self, attention_output):\n         intermediate_output = self.intermediate(attention_output)\n@@ -409,23 +395,15 @@ def forward(\n                 )\n                 use_cache = False\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         all_hidden_states = () if output_hidden_states else None\n         all_self_attentions = () if output_attentions else None\n-        next_decoder_cache = None\n         for i, layer_module in enumerate(self.layer):\n             if output_hidden_states:\n                 all_hidden_states = all_hidden_states + (hidden_states,)\n@@ -442,32 +420,26 @@ def forward(\n             )\n \n             hidden_states = layer_outputs[0]\n-            if use_cache:\n-                next_decoder_cache = layer_outputs[-1]\n             if output_attentions:\n                 all_self_attentions = all_self_attentions + (layer_outputs[1],)\n \n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_self_attentions,\n                 ]\n                 if v is not None\n             )\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n         )"
        },
        {
            "sha": "0980b81b55ad08975cbc10b7a7d3e5933e1987a9",
            "filename": "src/transformers/models/git/processing_git.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgit%2Fprocessing_git.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -20,7 +20,7 @@\n \n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n \n@@ -98,9 +98,6 @@ def __call__(\n         if text is None and images is None:\n             raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n \n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n-\n         output_kwargs = self._merge_kwargs(\n             GitProcessorKwargs,\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,"
        },
        {
            "sha": "86bf2b56ffea9344f4634bfd4598f1ae77bd6e2c",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -801,7 +801,7 @@ def forward(\n )\n class Glm4vModelOutputWithPast(ModelOutput):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -1364,7 +1364,7 @@ class Glm4vCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n "
        },
        {
            "sha": "2aec3cf284a847e52c7b59307e96133ddde363ca",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -31,6 +31,7 @@\n from transformers.utils.generic import check_model_inputs\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -498,7 +499,7 @@ class GotOcr2CausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -525,7 +526,7 @@ class GotOcr2CausalLMOutputWithPast(ModelOutput):\n )\n class GotOcr2ModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -590,7 +591,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -727,7 +728,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "1ae6880753cf36f278b9dfc38451ecc11424bc82",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -35,6 +35,7 @@\n     SamVisionLayer,\n )\n \n+from ...cache_utils import Cache\n from ...configuration_utils import PretrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n@@ -330,7 +331,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -403,7 +404,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "9d606297e829ee234107012681c3a8615f867568",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 14,
            "deletions": 43,
            "changes": 57,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -258,11 +258,7 @@ def forward(\n         attn_output = self.out_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n \n-        outputs = (attn_output, layer_past)\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs  # a, past_kv, (attentions)\n+        return attn_output, attn_weights\n \n \n class GPTNeoFlashAttention2(GPTNeoSelfAttention):\n@@ -364,11 +360,7 @@ def forward(\n         attn_output = self.out_proj(attn_weights_reshaped)\n         attn_output = self.resid_dropout(attn_output)\n \n-        outputs = (attn_output, layer_past)\n-        if output_attentions:\n-            outputs += (attn_weights_reshaped,)\n-\n-        return outputs\n+        return attn_output, attn_weights_reshaped\n \n \n GPT_NEO_ATTENTION_CLASSES = {\n@@ -454,7 +446,7 @@ def forward(\n     ):\n         residual = hidden_states\n         hidden_states = self.ln_1(hidden_states)\n-        attn_outputs = self.attn(\n+        attn_output, attn_weights = self.attn(\n             hidden_states,\n             layer_past=layer_past,\n             attention_mask=attention_mask,\n@@ -463,8 +455,7 @@ def forward(\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n-        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n-        outputs = attn_outputs[1:]\n+\n         # residual connection\n         hidden_states = attn_output + residual\n \n@@ -474,12 +465,7 @@ def forward(\n         # residual connection\n         hidden_states = residual + feed_forward_hidden_states\n \n-        if use_cache:\n-            outputs = (hidden_states,) + outputs\n-        else:\n-            outputs = (hidden_states,) + outputs[1:]\n-\n-        return outputs  # hidden_states, past_kv, attentions\n+        return hidden_states, attn_weights\n \n \n @auto_docstring\n@@ -588,19 +574,12 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         seq_length = inputs_embeds.shape[1]\n         if cache_position is None:\n@@ -630,7 +609,6 @@ def forward(\n         hidden_states = self.drop(hidden_states)\n         output_shape = (-1, seq_length, hidden_states.size(-1))\n \n-        next_decoder_cache = None\n         all_self_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n         for i, block in enumerate(self.h):\n@@ -648,11 +626,8 @@ def forward(\n             )\n \n             hidden_states = outputs[0]\n-            if use_cache:\n-                next_decoder_cache = outputs[1]\n-\n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n \n         hidden_states = self.ln_f(hidden_states)\n \n@@ -661,18 +636,14 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n-                v for v in [hidden_states, next_cache, all_hidden_states, all_self_attentions] if v is not None\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attentions] if v is not None\n             )\n \n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n         )"
        },
        {
            "sha": "599e7ee76f1c966e23aed125a7dda77fae2ca714",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -446,6 +446,10 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_in(input_ids)\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache()\n "
        },
        {
            "sha": "704b69aa5dcf239647c3ff349d8e6fc720b40a13",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -320,6 +320,10 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_in(input_ids)\n \n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n         if use_cache and past_key_values is None:\n             past_key_values = DynamicCache()\n "
        },
        {
            "sha": "30a2ce2fbc5f3fa2e5e60308dc3d4e29a1814e13",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 14,
            "deletions": 37,
            "changes": 51,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -150,11 +150,7 @@ def forward(\n         attn_output = self._merge_heads(attn_output, self.num_attention_heads, self.head_size)\n         attn_output = self.dense(attn_output)\n \n-        outputs = (attn_output, layer_past)\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs, self.dense_bias\n+        return attn_output, attn_weights, self.dense_bias\n \n     @classmethod\n     def _split_heads(cls, tensor, num_attention_heads, attn_head_size):\n@@ -357,7 +353,7 @@ def forward(\n     ):\n         residual = hidden_states\n         ln_out = self.input_layernorm(hidden_states)\n-        attention_layer_outputs, attn_bias = self.attention(\n+        attn_output, attn_weights, attn_bias = self.attention(\n             ln_out,\n             attention_mask=attention_mask,\n             layer_past=layer_past,\n@@ -368,8 +364,6 @@ def forward(\n             cache_position=cache_position,\n             position_embeddings=position_embeddings,\n         )\n-        attn_output = attention_layer_outputs[0]  # output_attn: a, present, (attentions)\n-        outputs = attention_layer_outputs[1:]\n \n         # attn_output = (atten_output + bias) + residual\n         attn_output = bias_dropout_add(\n@@ -386,12 +380,7 @@ def forward(\n             mlp_output, bias=None, residual=attn_output, prob=self.hidden_dropout, training=self.training\n         )\n \n-        if use_cache:\n-            outputs = (attn_output,) + outputs\n-        else:\n-            outputs = (attn_output,) + outputs[1:]\n-\n-        return outputs  # hidden_states, present, (attentions)\n+        return attn_output, attn_weights\n \n \n @auto_docstring\n@@ -460,19 +449,12 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_in(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         seq_length = inputs_embeds.shape[1]\n         if cache_position is None:\n@@ -497,7 +479,6 @@ def forward(\n         # create position embeddings to be shared across the decoder layers\n         position_embeddings = self.rotary_emb(hidden_states, position_ids)\n \n-        next_decoder_cache = None\n         all_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n         for i, layer in enumerate(self.layers):\n@@ -516,26 +497,22 @@ def forward(\n                 position_embeddings=position_embeddings,\n             )\n             hidden_states = outputs[0]\n-            if use_cache is True:\n-                next_decoder_cache = outputs[1]\n             if output_attentions:\n-                all_attentions = all_attentions + (outputs[2 if use_cache else 1],)\n+                all_attentions = all_attentions + (outputs[1],)\n \n         hidden_states = self.final_layer_norm(hidden_states)\n         # Add last hidden state\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_attentions] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_attentions] if v is not None\n+            )\n \n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n         )"
        },
        {
            "sha": "0aa03b559b95820e2718e49d2a34e1b7d386a57c",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 14,
            "deletions": 46,
            "changes": 60,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -254,11 +254,7 @@ def forward(\n         attn_output = self.out_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n \n-        outputs = (attn_output, layer_past)\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs  # a, present, (attentions)\n+        return attn_output, attn_weights\n \n \n class GPTJFlashAttention2(GPTJAttention):\n@@ -402,12 +398,7 @@ def forward(\n         )\n         attn_output = self.out_proj(attn_output)\n         attn_output = self.resid_dropout(attn_output)\n-\n-        outputs = (attn_output, layer_past)\n-        if output_attentions:\n-            outputs += (attn_weights,)\n-\n-        return outputs\n+        return attn_output, attn_weights\n \n \n GPTJ_ATTENTION_CLASSES = {\n@@ -456,7 +447,7 @@ def forward(\n     ) -> Union[tuple[torch.Tensor], Optional[tuple[torch.Tensor, tuple[torch.FloatTensor, ...]]]]:\n         residual = hidden_states\n         hidden_states = self.ln_1(hidden_states)\n-        attn_outputs = self.attn(\n+        attn_outputs, attn_weights = self.attn(\n             hidden_states=hidden_states,\n             layer_past=layer_past,\n             attention_mask=attention_mask,\n@@ -466,18 +457,10 @@ def forward(\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n-        attn_output = attn_outputs[0]  # output_attn: a, present, (attentions)\n-        outputs = attn_outputs[1:]\n-\n         feed_forward_hidden_states = self.mlp(hidden_states)\n-        hidden_states = attn_output + feed_forward_hidden_states + residual\n-\n-        if use_cache:\n-            outputs = (hidden_states,) + outputs\n-        else:\n-            outputs = (hidden_states,) + outputs[1:]\n+        hidden_states = attn_outputs + feed_forward_hidden_states + residual\n \n-        return outputs  # hidden_states, present, (attentions)\n+        return hidden_states, attn_weights\n \n \n @auto_docstring\n@@ -676,19 +659,12 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.wte(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         seq_length = inputs_embeds.shape[1]\n         if cache_position is None:\n@@ -719,7 +695,6 @@ def forward(\n         hidden_states = self.drop(hidden_states)\n         output_shape = (-1, seq_length, hidden_states.size(-1))\n \n-        next_decoder_cache = None\n         all_self_attentions = () if output_attentions else None\n         all_hidden_states = () if output_hidden_states else None\n         for i, block in enumerate(self.h):\n@@ -752,11 +727,8 @@ def forward(\n             )\n \n             hidden_states = outputs[0]\n-            if use_cache is True:\n-                next_decoder_cache = outputs[1]\n-\n             if output_attentions:\n-                all_self_attentions = all_self_attentions + (outputs[2 if use_cache else 1],)\n+                all_self_attentions = all_self_attentions + (outputs[1],)\n \n             # Model Parallel: If it's the last layer for that device, put things on the next device\n             if self.model_parallel:\n@@ -771,18 +743,14 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n-                v for v in [hidden_states, next_cache, all_hidden_states, all_self_attentions] if v is not None\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attentions] if v is not None\n             )\n \n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attentions,\n         )"
        },
        {
            "sha": "2b0efc519e664526f08ee759dba91f6a6476a08e",
            "filename": "src/transformers/models/granite_speech/modeling_granite_speech.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite_speech%2Fmodeling_granite_speech.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -21,6 +21,7 @@\n import torch.nn.functional as F\n from torch import nn\n \n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_outputs import ModelOutput\n from ...modeling_utils import PreTrainedModel\n@@ -44,7 +45,7 @@ class GraniteSpeechCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -361,7 +362,7 @@ def forward(\n         input_features_mask: Optional[torch.Tensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "065dfca74b6f1196b117f552b246448a11f737c4",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 23,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -464,7 +464,7 @@ def forward(\n         attn_output = attn_output.view(bsz, q_len, -1)\n         attn_output = self.o_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n def eager_attention_forward(\n@@ -550,7 +550,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -576,9 +576,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         if output_router_logits:\n             outputs += (router_logits,)\n \n@@ -683,14 +680,12 @@ def forward(\n \n         inputs_embeds = inputs_embeds * self.embedding_multiplier\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):  # kept for BC (non `Cache` `past_key_values` inputs)\n-            return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -716,7 +711,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_logits = () if output_router_logits else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -736,9 +730,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -751,15 +742,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,"
        },
        {
            "sha": "8139103d210cd5903d20bc0f5847ed7dc9094817",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 4,
            "deletions": 13,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -219,7 +219,7 @@ def forward(\n         attn_output = attn_output.view(bsz, q_len, -1)\n         attn_output = self.o_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # Adapted from transformers.models.jamba.modeling_jamba.HybridMambaAttentionDynamicCache for the v2 mixer\n@@ -1118,7 +1118,7 @@ def forward(\n             # No attention weights for state space layers\n             self_attn_weights = None\n         else:\n-            hidden_states, self_attn_weights, _ = self.self_attn(\n+            hidden_states, self_attn_weights = self.self_attn(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 past_key_value=past_key_value,\n@@ -1149,9 +1149,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         if output_router_logits:\n             outputs += (router_logits,)\n \n@@ -1335,7 +1332,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_logits = () if output_router_logits else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             # Depending on the layer type we opt for 2D base attention mask (Mamba) or 4D causal mask (Attention)\n@@ -1357,9 +1353,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 if layer_outputs[1] is not None:\n                     # append attentions only of attention layers. Mamba layers return `None` as the attention weights\n@@ -1379,11 +1372,9 @@ def forward(\n         if past_key_values and not past_key_values.has_previous_state:\n             past_key_values.has_previous_state = True\n \n-        next_cache = next_decoder_cache if use_cache else None\n-\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,\n@@ -1786,7 +1777,7 @@ def prepare_inputs_for_generation(\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]\n-        else:\n+        elif use_cache:\n             past_key_values = HybridMambaAttentionDynamicCache(\n                 self.config, input_ids.shape[0], self.dtype, device=self.device\n             )"
        },
        {
            "sha": "f894d1a8f8a754a955b00698ae04882418f12f9f",
            "filename": "src/transformers/models/granitemoehybrid/modular_granitemoehybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 12,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodular_granitemoehybrid.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -124,7 +124,7 @@ def forward(\n             # No attention weights for state space layers\n             self_attn_weights = None\n         else:\n-            hidden_states, self_attn_weights, _ = self.self_attn(\n+            hidden_states, self_attn_weights = self.self_attn(\n                 hidden_states=hidden_states,\n                 attention_mask=attention_mask,\n                 past_key_value=past_key_value,\n@@ -155,9 +155,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         if output_router_logits:\n             outputs += (router_logits,)\n \n@@ -260,7 +257,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_logits = () if output_router_logits else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             # Depending on the layer type we opt for 2D base attention mask (Mamba) or 4D causal mask (Attention)\n@@ -282,9 +278,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 if layer_outputs[1] is not None:\n                     # append attentions only of attention layers. Mamba layers return `None` as the attention weights\n@@ -304,11 +297,9 @@ def forward(\n         if past_key_values and not past_key_values.has_previous_state:\n             past_key_values.has_previous_state = True\n \n-        next_cache = next_decoder_cache if use_cache else None\n-\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,\n@@ -363,7 +354,7 @@ def prepare_inputs_for_generation(\n                 input_ids = input_ids[:, -cache_position.shape[0] :]\n             elif input_ids.shape[1] != cache_position.shape[0]:  # Default case (the \"else\", a no op, is Exception 2)\n                 input_ids = input_ids[:, cache_position]\n-        else:\n+        elif use_cache:\n             past_key_values = HybridMambaAttentionDynamicCache(\n                 self.config, input_ids.shape[0], self.dtype, device=self.device\n             )"
        },
        {
            "sha": "475e97dc84d7b429c70ea0e59a257a596a484a1d",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 12,
            "deletions": 23,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -403,7 +403,7 @@ def forward(\n         attn_output = attn_output.view(bsz, q_len, -1)\n         attn_output = self.o_proj(attn_output)\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class GraniteMoeSharedDecoderLayer(GradientCheckpointingLayer):\n@@ -463,7 +463,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -494,9 +494,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         if output_router_logits:\n             outputs += (router_logits,)\n \n@@ -635,14 +632,12 @@ def forward(\n \n         inputs_embeds = inputs_embeds * self.embedding_multiplier\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):  # kept for BC (non `Cache` `past_key_values` inputs)\n-            return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/v4.41.3/en/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -668,7 +663,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_logits = () if output_router_logits else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -688,9 +682,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -703,15 +694,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,"
        },
        {
            "sha": "ee8fc48d38e2d21385c1a196a5524236dd14812d",
            "filename": "src/transformers/models/granitemoeshared/modular_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodular_granitemoeshared.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -107,7 +107,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -140,9 +140,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         if output_router_logits:\n             outputs += (router_logits,)\n "
        },
        {
            "sha": "dd05709e0805dbf3d33f6f86a3e286d2f527ce87",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 14,
            "deletions": 34,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -64,7 +64,7 @@ class IdeficsBaseModelOutputWithPast(ModelOutput):\n \n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n         `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n@@ -99,7 +99,7 @@ class IdeficsCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -647,7 +647,7 @@ def forward(\n         if output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # this was adapted from LlamaDecoderLayer\n@@ -701,7 +701,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -726,9 +726,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -847,7 +844,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.cross_attn(\n+        hidden_states, self_attn_weights = self.cross_attn(\n             hidden_states=hidden_states,\n             key_value_states=image_hidden_states,\n             attention_mask=image_attention_mask,\n@@ -871,9 +868,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -1011,7 +1005,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         image_encoder_embeddings: Optional[torch.FloatTensor] = None,\n@@ -1055,19 +1049,12 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         batch_size, seq_length, _ = inputs_embeds.shape\n         past_key_values_length = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -1160,7 +1147,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for idx, decoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n@@ -1194,9 +1180,6 @@ def forward(\n             )\n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1206,14 +1189,11 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n         image_hidden_states = image_hidden_states.view(batch_size, num_images, image_seq_len, image_hidden_size)\n \n         return IdeficsBaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             image_hidden_states=image_hidden_states,\n@@ -1409,7 +1389,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         image_encoder_embeddings: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "3c59105c2360dec3aaa5c135ad9722370f1a84fd",
            "filename": "src/transformers/models/idefics/processing_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fprocessing_idefics.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -27,7 +27,6 @@\n     ProcessorMixin,\n     TextKwargs,\n     Unpack,\n-    _validate_images_text_input_order,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_tf_available, is_torch_available\n@@ -340,8 +339,6 @@ def __call__(\n         \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You need to specify either `text` or `images` and `text`.\")\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n \n         if images is None:\n             # assuming the user wants to use the old behavior with prompts as the only argument"
        },
        {
            "sha": "06fccf9614c16cfdd4d6e1e00e5516ede9db67bc",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -50,7 +50,7 @@ class Idefics2BaseModelOutputWithPast(ModelOutput):\n         Sequence of hidden-states at the output of the last layer of the model.\n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n         `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n@@ -84,7 +84,7 @@ class Idefics2CausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n@@ -639,7 +639,7 @@ def forward(\n         context: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: bool = False,\n         use_cache: bool = False,\n     ) -> tuple[torch.Tensor, Optional[torch.Tensor], Optional[tuple[torch.Tensor]]]:\n@@ -728,7 +728,7 @@ def forward(\n         context: torch.Tensor,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_value: Optional[tuple[torch.Tensor]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n         use_cache: Optional[bool] = False,\n         **kwargs,\n@@ -1012,7 +1012,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_attention_mask: Optional[torch.BoolTensor] = None,\n@@ -1051,7 +1051,11 @@ def forward(\n         else:\n             raise ValueError(\"You have to specify either input_ids or inputs_embeds\")\n \n-        if use_cache and not isinstance(past_key_values, Cache):\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n             past_key_values = DynamicCache()\n \n         if inputs_embeds is None:\n@@ -1155,7 +1159,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_attention_mask: Optional[torch.BoolTensor] = None,"
        },
        {
            "sha": "fedc24eb0e6139674a0b0a7994031301d4f04662",
            "filename": "src/transformers/models/idefics2/processing_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fprocessing_idefics2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -26,7 +26,6 @@\n     ProcessingKwargs,\n     ProcessorMixin,\n     Unpack,\n-    _validate_images_text_input_order,\n )\n from ...tokenization_utils_base import AddedToken, TextInput\n from ...utils import logging\n@@ -181,8 +180,6 @@ def __call__(\n         \"\"\"\n         if text is None and images is None:\n             raise ValueError(\"You must provide either `text` or `images`.\")\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n \n         output_kwargs = self._merge_kwargs(\n             Idefics2ProcessorKwargs,"
        },
        {
            "sha": "c018963943dac248cae426f0976b9d8d50b21ab5",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -22,7 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import DynamicCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -50,7 +50,7 @@ class Idefics3BaseModelOutputWithPast(ModelOutput):\n         Sequence of hidden-states at the output of the last layer of the model.\n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n         `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n@@ -83,7 +83,7 @@ class Idefics3CausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n@@ -739,7 +739,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_attention_mask: Optional[torch.BoolTensor] = None,\n@@ -889,7 +889,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_attention_mask: Optional[torch.BoolTensor] = None,"
        },
        {
            "sha": "d0372118b958fda438cbec522c9d704d60589244",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 40,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -16,7 +16,6 @@\n \n import math\n import os\n-import warnings\n from typing import Any, Optional, Union\n \n import torch\n@@ -597,19 +596,6 @@ def forward(\n         >>> last_hidden_states = outputs.last_hidden_state\n         ```\"\"\"\n \n-        if \"pixel_values\" in kwargs:\n-            warnings.warn(\n-                \"The `pixel_values` argument is deprecated and will be removed in v4.47, use `input_ids` instead.\",\n-                FutureWarning,\n-            )\n-\n-            if input_ids is not None:\n-                raise ValueError(\n-                    \"You cannot pass both `pixel_values` and `input_ids`. Please make sure to only pass `input_ids`.\"\n-                )\n-\n-            input_ids = kwargs.pop(\"pixel_values\")\n-\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n             output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n@@ -865,19 +851,6 @@ def forward(\n         ...     ax.imshow(img)\n         ```\"\"\"\n \n-        if \"pixel_values\" in kwargs:\n-            warnings.warn(\n-                \"The `pixel_values` argument is deprecated and will be removed in v4.47, use `input_ids` instead.\",\n-                FutureWarning,\n-            )\n-\n-            if input_ids is not None:\n-                raise ValueError(\n-                    \"You cannot pass both `pixel_values` and `input_ids`. Please make sure to only pass `input_ids`.\"\n-                )\n-\n-            input_ids = kwargs.pop(\"pixel_values\")\n-\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         transformer_outputs = self.transformer(\n@@ -1002,19 +975,6 @@ def forward(\n         >>> logits = outputs.logits\n         ```\"\"\"\n \n-        if \"pixel_values\" in kwargs:\n-            warnings.warn(\n-                \"The `pixel_values` argument is deprecated and will be removed in v4.47, use `input_ids` instead.\",\n-                FutureWarning,\n-            )\n-\n-            if input_ids is not None:\n-                raise ValueError(\n-                    \"You cannot pass both `pixel_values` and `input_ids`. Please make sure to only pass `input_ids`.\"\n-                )\n-\n-            input_ids = kwargs.pop(\"pixel_values\")\n-\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n         transformer_outputs = self.transformer("
        },
        {
            "sha": "67500d82eff003fcf665c3a7e9cdd6f52e124574",
            "filename": "src/transformers/models/instructblip/modeling_instructblip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblip%2Fmodeling_instructblip.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -693,7 +694,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.attention(\n@@ -1069,7 +1070,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1084,7 +1085,7 @@ def forward(\n             the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of:\n+        past_key_values (`Cache` of length `config.n_layers` with each tuple having 4 tensors of:\n             shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and\n             value hidden states of the attention blocks. Can be used to speed up decoding. If `past_key_values` are\n             used, the user can optionally input only the last `decoder_input_ids` (those that don't have their past key"
        },
        {
            "sha": "cdd9824690da126f8cc7a4ae2695798d2adca634",
            "filename": "src/transformers/models/instructblipvideo/modeling_instructblipvideo.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodeling_instructblipvideo.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -27,6 +27,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -554,7 +555,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_value: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_value: Optional[Cache] = None,\n         output_attentions: Optional[bool] = False,\n     ) -> tuple[torch.Tensor]:\n         self_outputs = self.attention(\n@@ -1030,7 +1031,7 @@ def forward(\n         head_mask: Optional[torch.FloatTensor] = None,\n         encoder_hidden_states: Optional[torch.FloatTensor] = None,\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1045,7 +1046,7 @@ def forward(\n             the cross-attention if the model is configured as a decoder. Mask values selected in `[0, 1]`:\n             - 1 for tokens that are **not masked**,\n             - 0 for tokens that are **masked**.\n-        past_key_values (`tuple(tuple(torch.FloatTensor))` of length `config.n_layers` with each tuple having 4 tensors of:\n+        past_key_values (`Cache` of length `config.n_layers` with each tuple having 4 tensors of:\n             shape `(batch_size, num_heads, sequence_length - 1, embed_size_per_head)`): Contains precomputed key and\n             value hidden states of the attention blocks. Can be used to speed up decoding. If `past_key_values` are\n             used, the user can optionally input only the last `decoder_input_ids` (those that don't have their past key\n@@ -1603,7 +1604,7 @@ def forward(\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n                 \"Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.54.\"\n             )\n             inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n             attention_mask = torch.cat(\n@@ -1732,7 +1733,7 @@ def generate(\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n                 \"Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.54.\"\n             )\n             inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n             attention_mask = torch.cat("
        },
        {
            "sha": "e6f32896084edd20033e18e448c89a954d5cd08d",
            "filename": "src/transformers/models/instructblipvideo/modular_instructblipvideo.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fmodular_instructblipvideo.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -492,7 +492,7 @@ def forward(\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n                 \"Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.54.\"\n             )\n             inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n             attention_mask = torch.cat(\n@@ -621,7 +621,7 @@ def generate(\n             logger.warning_once(\n                 \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n                 \"Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. \"\n-                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                \"Using processors without these attributes in the config is deprecated and will throw an error in v4.54.\"\n             )\n             inputs_embeds = torch.cat([language_model_inputs, inputs_embeds.to(language_model_inputs.device)], dim=1)\n             attention_mask = torch.cat("
        },
        {
            "sha": "e2174f248aaf10019b25a1f5e278523e91f6967b",
            "filename": "src/transformers/models/instructblipvideo/processing_instructblipvideo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finstructblipvideo%2Fprocessing_instructblipvideo.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -147,7 +147,7 @@ def __call__(\n                     logger.warning_once(\n                         \"Expanding inputs for video tokens in InstructBLIPVideo should be done in processing. \"\n                         \"Please follow instruction here (https://gist.github.com/zucchini-nlp/65f22892b054dc0d68228af56fbeaac2) to update your InstructBLIPVideo model. \"\n-                        \"Using processors without these attributes in the config is deprecated and will throw an error in v4.47.\"\n+                        \"Using processors without these attributes in the config is deprecated and will throw an error in v4.54.\"\n                     )\n \n             # cast to desired return tensors type after concatenating"
        },
        {
            "sha": "68074964c415ed838b1f3e8cb541a7980359b849",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -28,6 +28,7 @@\n import torch.nn as nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -567,7 +568,7 @@ def forward(self, image_features):\n )\n class InternVLModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -670,7 +671,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n@@ -796,7 +797,7 @@ class InternVLCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -888,7 +889,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "1c29907bb16e0f8d715b4e955f83f4c4d7abc170",
            "filename": "src/transformers/models/internvl/modular_internvl.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodular_internvl.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -23,6 +23,7 @@\n import torch.utils.checkpoint\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling\n@@ -600,7 +601,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "9c31d8530d2e6330bcbfae94702b2895fea2303e",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -118,7 +118,7 @@ class JanusBaseModelOutputWithPast(ModelOutput):\n \n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n         `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n@@ -153,7 +153,7 @@ class JanusCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n "
        },
        {
            "sha": "f9bd85898ce36ab77d9464ccf071b1a0bcad485e",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 12,
            "deletions": 30,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -548,7 +548,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value, router_logits\n+        return attn_output, attn_weights, router_logits\n \n \n class JetMoeSdpaAttention(JetMoeAttention):\n@@ -636,7 +636,7 @@ def forward(\n         attn_output = self.experts.reduce(attn_output, topo_info)\n         attn_output = attn_output.view(bsz, q_len, -1)\n \n-        return attn_output, None, past_key_value, router_logits\n+        return attn_output, None, router_logits\n \n \n class JetMoeFlashAttention2(JetMoeAttention):\n@@ -756,7 +756,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value, router_logits\n+        return attn_output, attn_weights, router_logits\n \n \n JETMOE_ATTENTION_CLASSES = {\n@@ -794,7 +794,7 @@ def forward(\n         cache_position: Optional[torch.LongTensor] = None,\n     ) -> Union[tuple[torch.Tensor], Optional[tuple[torch.Tensor, tuple[torch.FloatTensor, ...]]]]:\n         # Self Attention\n-        attn_output, self_attn_weights, present_key_value, attn_router_logits = self.self_attention(\n+        attn_output, self_attn_weights, attn_router_logits = self.self_attention(\n             hidden_states=self.input_layernorm(hidden_states),\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -813,9 +813,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         if output_router_logits:\n             outputs += attn_router_logits, mlp_router_logits\n \n@@ -924,19 +921,12 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -965,7 +955,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_logits = () if output_router_logits else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -983,9 +972,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -998,13 +984,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,\n@@ -1181,7 +1163,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "a067580cf08dc461667c8c0a0fe0af681493a146",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 14,
            "deletions": 33,
            "changes": 47,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -490,7 +490,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaFlashAttention2 with Gemma->KyutaiSpeechToText\n@@ -612,7 +612,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaSdpaAttention with Gemma->KyutaiSpeechToText\n@@ -707,7 +707,7 @@ def forward(\n \n         attn_output = self.o_proj(attn_output, cache_position)  # Ignore copy\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None\n \n \n KYUTAI_SPEECH_TO_TEXT_ATTENTION_CLASSES = {\n@@ -769,7 +769,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -794,9 +794,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -855,13 +852,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        return_legacy_cache = False  # noqa: F841\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n-            return_legacy_cache = True  # noqa: F841\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n@@ -880,20 +870,16 @@ def forward(\n         # embed positions\n         hidden_states = inputs_embeds\n \n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n-            return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -911,9 +897,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -923,15 +906,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )"
        },
        {
            "sha": "653006d631dc0f5dc6ef88b154adc7d90fd776e0",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -728,7 +728,7 @@ class Llama4CausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -1280,7 +1280,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "b7b67a8a776a586dd460813e3d27213385b09f4c",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n@@ -43,7 +44,7 @@\n )\n class LlavaModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -69,7 +70,7 @@ class LlavaCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -240,7 +241,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n@@ -395,7 +396,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "3d1cfc61e6ff9e3f21e3ae86bb47132cad24c055",
            "filename": "src/transformers/models/llava/processing_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fprocessing_llava.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -27,7 +27,6 @@\n     ProcessingKwargs,\n     ProcessorMixin,\n     Unpack,\n-    _validate_images_text_input_order,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n@@ -133,9 +132,6 @@ def __call__(\n         if images is None and text is None:\n             raise ValueError(\"You have to specify at least one of `images` or `text`.\")\n \n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n-\n         output_kwargs = self._merge_kwargs(\n             LlavaProcessorKwargs,\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,"
        },
        {
            "sha": "1c33fb8a047afab05c3c8d374780fd28a47fa533",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -23,6 +23,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -152,7 +153,7 @@ def unpad_image(tensor, original_size):\n )\n class LlavaNextModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -178,7 +179,7 @@ class LlavaNextCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -435,7 +436,7 @@ def forward(\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n@@ -605,7 +606,7 @@ def forward(\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "05efc60fcd2e34dddf5dcd32a6313150aa8a2577",
            "filename": "src/transformers/models/llava_next/processing_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fprocessing_llava_next.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -28,7 +28,6 @@\n     ProcessingKwargs,\n     ProcessorMixin,\n     Unpack,\n-    _validate_images_text_input_order,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n@@ -136,8 +135,6 @@ def __call__(\n         \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You have to specify at least images or text.\")\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n \n         output_kwargs = self._merge_kwargs(\n             LlavaNextProcessorKwargs,"
        },
        {
            "sha": "32fa77f97c06c9a822eb45bb28616aaf9bec8b5d",
            "filename": "src/transformers/models/llava_next_video/configuration_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fconfiguration_llava_next_video.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -19,7 +19,6 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n-\n from ...configuration_utils import PretrainedConfig\n from ..auto import CONFIG_MAPPING, AutoConfig\n "
        },
        {
            "sha": "cf6435e1c7c9d1211136b0784fb36fa912740408",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -28,6 +28,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -50,7 +51,7 @@\n )\n class LlavaNextVideoModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -81,7 +82,7 @@ class LlavaNextVideoCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -486,7 +487,7 @@ def forward(\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n@@ -743,7 +744,7 @@ def forward(\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "713b7f979bb2a19ff12549978f6a7a3d431b218a",
            "filename": "src/transformers/models/llava_next_video/modular_llava_next_video.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodular_llava_next_video.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -29,6 +29,7 @@\n     image_size_to_num_patches,\n )\n \n+from ...cache_utils import Cache\n from ...configuration_utils import PretrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n@@ -183,7 +184,7 @@ def __init__(\n \n class LlavaNextVideoModelOutputWithPast(LlavaNextModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -206,7 +207,7 @@ class LlavaNextVideoCausalLMOutputWithPast(LlavaNextCausalLMOutputWithPast):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -406,7 +407,7 @@ def forward(\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n@@ -545,7 +546,7 @@ def forward(\n         image_sizes: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "dd039ff024834c1f0590044dfe32dc4e70255b31",
            "filename": "src/transformers/models/llava_next_video/processing_llava_next_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fprocessing_llava_next_video.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -23,7 +23,7 @@\n from ...feature_extraction_utils import BatchFeature\n from ...image_processing_utils import select_best_resolution\n from ...image_utils import ImageInput, get_image_size, to_numpy_array\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import logging\n from ...video_utils import VideoInput\n@@ -157,8 +157,6 @@ def __call__(\n               `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n \n         output_kwargs = self._merge_kwargs(\n             LlavaNextVideoProcessorKwargs,"
        },
        {
            "sha": "6552f840a7b68ed2f6b19ec1762b4741c5f11798",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -28,6 +28,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...image_processing_utils import select_best_resolution\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -56,7 +57,7 @@\n )\n class LlavaOnevisionModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -87,7 +88,7 @@ class LlavaOnevisionCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -502,7 +503,7 @@ def forward(\n         image_sizes_videos: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n@@ -785,7 +786,7 @@ def forward(\n         image_sizes_videos: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "196cbf8e103e76f99385e061523b387a04ce588d",
            "filename": "src/transformers/models/llava_onevision/modular_llava_onevision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodular_llava_onevision.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -32,6 +32,7 @@\n     unpad_image,\n )\n \n+from ...cache_utils import Cache\n from ...image_processing_utils import BatchFeature\n from ...image_processing_utils_fast import DefaultFastImageProcessorKwargs, group_images_by_shape, reorder_images\n from ...image_utils import (\n@@ -485,7 +486,7 @@ def forward(\n         image_sizes_videos: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n@@ -635,7 +636,7 @@ def forward(\n         image_sizes_videos: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "537b816b79f8f83d6fbe116f3260bda834a547db",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 41,
            "deletions": 59,
            "changes": 100,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -467,13 +467,16 @@ def forward(\n         query_states = self.q(hidden_states)\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n+        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n                 curr_past_key_value = past_key_value.cross_attention_cache\n             else:\n                 curr_past_key_value = past_key_value.self_attention_cache\n+        else:\n+            curr_past_key_value = past_key_value\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n@@ -542,7 +545,7 @@ def forward(\n         attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        outputs = (attn_output, past_key_value, position_bias)\n+        outputs = (attn_output, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -733,8 +736,10 @@ def unshape(states):\n         attn_output = attn_output[:, :seq_length, :]\n         attn_output = self.o(attn_output)\n \n-        present_key_value_state = None\n-        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n+        outputs = (\n+            attn_output,\n+            position_bias,\n+        )\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -996,8 +1001,7 @@ def unshape(states):\n         attn_output = attn_output[:, :seq_length, :]\n         attn_output = self.o(attn_output)\n \n-        present_key_value_state = None\n-        outputs = (attn_output,) + (present_key_value_state,) + (position_bias,)\n+        outputs = (attn_output, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -1194,8 +1198,8 @@ def forward(\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n-        hidden_states, past_key_value = self_attention_outputs[:2]\n-        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n+        hidden_states = self_attention_outputs[0]\n+        attention_outputs = self_attention_outputs[1:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 inference - check https://github.com/huggingface/transformers/pull/19229/\n         if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n@@ -1216,15 +1220,15 @@ def forward(\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n-            hidden_states, past_key_value = cross_attention_outputs[:2]\n+            hidden_states = cross_attention_outputs[0]\n \n             # clamp inf values to enable fp16 inference - check https://github.com/huggingface/transformers/pull/19229/\n             if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n                 clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n             # Keep cross-attention outputs and relative position weights\n-            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n+            attention_outputs = attention_outputs + cross_attention_outputs[1:]\n \n         # Apply Feed Forward layer\n         hidden_states = self.layer[-1](hidden_states)\n@@ -1234,14 +1238,9 @@ def forward(\n             clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-        outputs = (hidden_states,)\n-\n-        if use_cache:\n-            outputs = outputs + (past_key_value,) + attention_outputs\n-        else:\n-            outputs = outputs + attention_outputs\n-\n-        return outputs  # hidden-states, present_key_value_states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n+        return (\n+            (hidden_states,) + attention_outputs\n+        )  # hidden-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n \n \n @auto_docstring\n@@ -1426,23 +1425,12 @@ def forward(\n \n         batch_size, seq_length = input_shape\n \n-        # initialize past_key_values\n-        return_legacy_cache = False\n-        return_self_attention_cache = False\n-        if self.is_decoder and (use_cache or past_key_values is not None):\n-            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n-                return_self_attention_cache = True\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n-            elif not isinstance(past_key_values, EncoderDecoderCache):\n-                return_legacy_cache = True\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n-                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-            elif past_key_values is None:\n-                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if self.is_decoder:\n+            if use_cache and past_key_values is None:\n+                if self.config.is_encoder_decoder:\n+                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                else:\n+                    past_key_values = DynamicCache()\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place\n@@ -1464,7 +1452,9 @@ def forward(\n                 attention_mask,\n                 inputs_embeds,\n                 cache_position,\n-                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                past_key_values.self_attention_cache\n+                if isinstance(past_key_values, EncoderDecoderCache)\n+                else past_key_values,\n                 output_attentions,\n             )\n         # We use local attention in encoder self-attention, otherwise standard self & cross attentions are used\n@@ -1519,23 +1509,21 @@ def forward(\n             )\n \n             # layer_outputs is a tuple with:\n-            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n-            if use_cache is False:\n-                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n+            # hidden-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n \n-            hidden_states, next_decoder_cache = layer_outputs[:2]\n+            hidden_states = layer_outputs[0]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n-            position_bias = layer_outputs[2]\n+            position_bias = layer_outputs[1]\n             if self.is_decoder and encoder_hidden_states is not None:\n-                encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n+                encoder_decoder_position_bias = layer_outputs[3 if output_attentions else 2]\n \n             if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[3],)\n+                all_attentions = all_attentions + (layer_outputs[2],)\n                 if self.is_decoder:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[4],)\n \n         hidden_states = self.final_layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n@@ -1544,18 +1532,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_self_attention_cache:\n-            next_cache = past_key_values.self_attention_cache\n-        if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1564,7 +1546,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1719,12 +1701,12 @@ def __init__(self, config: LongT5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = LongT5Stack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = LongT5Stack(decoder_config, self.shared)\n \n@@ -1769,7 +1751,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1920,12 +1902,12 @@ def __init__(self, config: LongT5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = LongT5Stack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = LongT5Stack(decoder_config, self.shared)\n \n@@ -1970,7 +1952,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.Tensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2167,7 +2149,7 @@ def __init__(self, config: LongT5Config):\n \n         encoder_config = copy.deepcopy(config)\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = LongT5Stack(encoder_config, self.shared)\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "df2f6cdaa96cf26a3a4f0f1aab194eda43492b29",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -956,7 +956,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1005,7 +1005,7 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n@@ -1234,7 +1234,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1362,7 +1362,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "c4393a3948bdda564c67be556bfea9131fe4ce47",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -900,7 +900,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -949,7 +949,7 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n@@ -1222,7 +1222,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple[torch.Tensor], BaseModelOutput]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1479,7 +1479,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[Union[tuple[torch.Tensor], BaseModelOutput]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1660,7 +1660,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "861eeaf68ec0e43f50a3325c652020d286cecca9",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -944,7 +944,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -993,7 +993,7 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n@@ -1218,7 +1218,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1374,7 +1374,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1863,7 +1863,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "23208f3006edf77111d800cffc83f8e1bf9db119",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 14,
            "deletions": 25,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -697,7 +697,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaFlashAttention2 with Gemma->Mimi\n@@ -814,7 +814,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaSdpaAttention with Gemma->Mimi\n@@ -904,7 +904,7 @@ def forward(\n \n         attn_output = self.o_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None\n \n \n MIMI_ATTENTION_CLASSES = {\n@@ -962,7 +962,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -985,9 +985,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -1094,16 +1091,12 @@ def forward(\n             )\n             use_cache = False\n \n-        if use_cache and not isinstance(past_key_values, Cache):\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -1126,7 +1119,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -1144,24 +1136,21 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n         # add hidden states from the last decoder layer\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n \n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )"
        },
        {
            "sha": "2157315238aedd86e4c76cb658b2bea201fe457c",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -872,7 +872,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "d319e973ac003fc4492d77b333530ab93564b990",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -26,6 +26,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...integrations import use_kernel_forward_from_hub\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -134,7 +135,7 @@ class Mistral3CausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -161,7 +162,7 @@ class Mistral3CausalLMOutputWithPast(ModelOutput):\n )\n class Mistral3ModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -286,7 +287,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         use_cache: Optional[bool] = None,\n@@ -434,7 +435,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "5b5f27579cc9bff38469be75b2c8cc1d6dfe2025",
            "filename": "src/transformers/models/mistral3/modular_mistral3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodular_mistral3.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -19,6 +19,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n from ...utils import is_torchdynamo_compiling, logging\n@@ -181,7 +182,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         use_cache: Optional[bool] = None,\n@@ -277,7 +278,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "32f8f9a84b4bb185458867e62cf1f141dfd70e9a",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -627,7 +627,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "de02a2a833bc9ec4b0c56f69de118a5b20528122",
            "filename": "src/transformers/models/mixtral/modular_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodular_mixtral.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -364,7 +364,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "0f68c2d03d7ef35c676c006c17b0a55c771dc603",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 9,
            "deletions": 19,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -532,7 +532,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.llama.modeling_llama.rotate_half\n@@ -647,7 +647,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # Copied from transformers.models.gemma2.modeling_gemma2.Gemma2MLP with Gemma2->MllamaText\n@@ -724,7 +724,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -748,9 +748,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -787,7 +784,7 @@ def forward(\n         residual = hidden_states\n         hidden_states = self.input_layernorm(hidden_states)\n \n-        hidden_states, attn_weights, past_key_value = self.cross_attn(\n+        hidden_states, attn_weights = self.cross_attn(\n             hidden_states=hidden_states,\n             attention_mask=cross_attention_mask,\n             cross_attention_states=cross_attention_states,\n@@ -810,9 +807,6 @@ def forward(\n         if output_attentions:\n             outputs += (attn_weights,)\n \n-        if use_cache:\n-            outputs += (past_key_value,)\n-\n         return outputs\n \n \n@@ -1386,7 +1380,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for idx, decoder_layer in enumerate(self.layers):\n             if output_hidden_states:\n@@ -1436,9 +1429,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1448,13 +1438,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n@@ -1657,7 +1647,7 @@ def forward(\n         cross_attention_mask: Optional[torch.Tensor] = None,\n         cross_attention_states: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,"
        },
        {
            "sha": "3b0da20ad203cb02375f40e14ea818b642c26c91",
            "filename": "src/transformers/models/modernbert/configuration_modernbert.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fconfiguration_modernbert.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -129,6 +129,7 @@ class ModernBertConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"modernbert\"\n+    attribute_map = {\"rope_theta\": \"global_rope_theta\"}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     def __init__("
        },
        {
            "sha": "f4fd7cf37b41cf2a8403d90ddfcf9f884f42a6ef",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -241,7 +241,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class ModernBertRotaryEmbedding(nn.Module):\n-    def __init__(self, config: ModernBertConfig, dim: int, base: float, device: Optional[torch.device] = None):\n+    def __init__(self, config: ModernBertConfig, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n@@ -253,7 +253,8 @@ def __init__(self, config: ModernBertConfig, dim: int, base: float, device: Opti\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-        inv_freq, self.attention_scaling = self.rope_init_fn(None, device, dim=dim, base=base)\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n \n@@ -461,19 +462,17 @@ def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n         else:\n             self.local_attention = (-1, -1)\n \n-        rope_theta = config.global_rope_theta\n         max_position_embeddings = config.max_position_embeddings\n         if self.local_attention != (-1, -1):\n-            if config.local_rope_theta is not None:\n-                rope_theta = config.local_rope_theta\n+            rope_theta = config.global_rope_theta if config.local_rope_theta is None else config.local_rope_theta\n             max_position_embeddings = config.local_attention\n \n         if config._attn_implementation == \"flash_attention_2\":\n             self.rotary_emb = ModernBertUnpaddedRotaryEmbedding(\n                 dim=self.head_dim, max_seqlen=max_position_embeddings, base=rope_theta\n             )\n         else:\n-            self.rotary_emb = ModernBertRotaryEmbedding(config=config, dim=self.head_dim, base=rope_theta)\n+            self.rotary_emb = ModernBertRotaryEmbedding(config=config)\n \n         self.Wo = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)\n         self.out_drop = nn.Dropout(config.attention_dropout) if config.attention_dropout > 0.0 else nn.Identity()"
        },
        {
            "sha": "94e45fcc5a6de321e616bd0e03261d95d08c01d2",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -156,6 +156,7 @@ class ModernBertConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"modernbert\"\n+    attribute_map = {\"rope_theta\": \"global_rope_theta\"}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     def __init__(\n@@ -504,9 +505,7 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class ModernBertRotaryEmbedding(GemmaRotaryEmbedding):\n-    def __init__(self, config: ModernBertConfig, dim: int, base: float, device: Optional[torch.device] = None):\n-        super().__init__(self, config=config, device=device)\n-        inv_freq, self.attention_scaling = self.rope_init_fn(None, device, dim=dim, base=base)\n+    pass\n \n \n def eager_attention_forward(\n@@ -663,19 +662,17 @@ def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n         else:\n             self.local_attention = (-1, -1)\n \n-        rope_theta = config.global_rope_theta\n         max_position_embeddings = config.max_position_embeddings\n         if self.local_attention != (-1, -1):\n-            if config.local_rope_theta is not None:\n-                rope_theta = config.local_rope_theta\n+            rope_theta = config.global_rope_theta if config.local_rope_theta is None else config.local_rope_theta\n             max_position_embeddings = config.local_attention\n \n         if config._attn_implementation == \"flash_attention_2\":\n             self.rotary_emb = ModernBertUnpaddedRotaryEmbedding(\n                 dim=self.head_dim, max_seqlen=max_position_embeddings, base=rope_theta\n             )\n         else:\n-            self.rotary_emb = ModernBertRotaryEmbedding(config=config, dim=self.head_dim, base=rope_theta)\n+            self.rotary_emb = ModernBertRotaryEmbedding(config=config)\n \n         self.Wo = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)\n         self.out_drop = nn.Dropout(config.attention_dropout) if config.attention_dropout > 0.0 else nn.Identity()"
        },
        {
            "sha": "9b55fa8c961b7a296c701e420de5360485ab7936",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -414,7 +414,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         encoder_position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:"
        },
        {
            "sha": "7180d35e8e6ef8f870f2fa9f7e1467bb35d91598",
            "filename": "src/transformers/models/moonshine/modular_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodular_moonshine.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -449,7 +449,7 @@ def forward(\n         past_key_value: Optional[Cache] = None,\n         use_cache: Optional[bool] = False,\n         cache_position: Optional[torch.LongTensor] = None,\n-        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # will become mandatory in v4.46\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         encoder_position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,\n         **kwargs: Unpack[TransformersKwargs],\n     ) -> tuple[torch.FloatTensor, Optional[tuple[torch.FloatTensor, torch.FloatTensor]]]:"
        },
        {
            "sha": "45bea58cdf526ecf55855e60f04f43b57c191d4f",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 24,
            "deletions": 50,
            "changes": 74,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -32,7 +32,7 @@\n from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast, ModelOutput, Seq2SeqLMOutput\n from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n from ...modeling_utils import PreTrainedModel\n-from ...utils import auto_docstring, is_torch_flex_attn_available, is_torchdynamo_compiling, logging\n+from ...utils import auto_docstring, is_torch_flex_attn_available, logging\n from ..auto.modeling_auto import AutoModel\n from .configuration_moshi import MoshiConfig, MoshiDepthConfig\n \n@@ -113,7 +113,7 @@ class MoshiCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -141,7 +141,7 @@ class MoshiConditionalGenerationOutputWithPast(ModelOutput):\n         Text language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the text language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -492,7 +492,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaFlashAttention2 with Gemma->Moshi\n@@ -614,7 +614,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # NO LONGER EXIST Copied from transformers.models.gemma.modeling_gemma.GemmaSdpaAttention with Gemma->Moshi\n@@ -709,7 +709,7 @@ def forward(\n \n         attn_output = self.o_proj(attn_output, cache_position)  # Ignore copy\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None\n \n \n MOSHI_ATTENTION_CLASSES = {\n@@ -771,7 +771,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -796,9 +796,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -870,7 +867,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         last_hidden_state: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.BoolTensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1002,7 +999,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n         hidden_states = inputs_embeds\n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -1020,17 +1016,13 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n         # add hidden states from the last decoder layer\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n         logits = self.lm_heads(hidden_states, cache_position)\n \n         loss = None\n@@ -1045,13 +1037,15 @@ def forward(\n             loss = loss_fct(logits.reshape(-1, self.config.audio_vocab_size), labels)\n \n         if not return_dict:\n-            return tuple(v for v in [loss, logits, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [loss, logits, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n \n         return CausalLMOutputWithPast(\n             loss=loss,\n             logits=logits,\n-            past_key_values=next_cache,\n-            hidden_states=all_hidden_states,\n+            past_key_values=past_key_values,\n+            hidden_states=past_key_values,\n             attentions=all_self_attns,\n         )\n \n@@ -1269,13 +1263,6 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        return_legacy_cache = False  # noqa: F841\n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n-            return_legacy_cache = True  # noqa: F841\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-\n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n             cache_position = torch.arange(\n@@ -1294,20 +1281,16 @@ def forward(\n         # embed positions\n         hidden_states = inputs_embeds\n \n-        if (\n-            use_cache and not isinstance(past_key_values, Cache) and not self.training\n-        ):  # kept for BC (non `Cache` `past_key_values` inputs)\n-            return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            logger.warning_once(\n-                \"We detected that you are passing `past_key_values` as a tuple and this is deprecated and will be removed in v4.43. \"\n-                \"Please use an appropriate `Cache` class (https://huggingface.co/docs/transformers/internal/generation_utils#transformers.Cache)\"\n-            )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -1325,9 +1308,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1337,15 +1317,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n@@ -1604,10 +1582,6 @@ def forward(\n         )\n \n         hidden_states = outputs[0]\n-        if labels is None and not is_torchdynamo_compiling():\n-            logger.warning_once(\n-                \"Starting from v4.46, the `logits` model output will have the same type as the model (except at train time, where it will always be FP32)\"\n-            )\n         # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n         slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n         logits = self.lm_head(hidden_states[:, slice_indices, :])\n@@ -1694,7 +1668,7 @@ def forward(\n         user_audio_codes: Optional[torch.Tensor] = None,\n         moshi_input_values: Optional[torch.FloatTensor] = None,\n         moshi_audio_codes: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         text_labels: Optional[torch.LongTensor] = None,\n         audio_labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "0341c3afcfdec1ecef7b1a39b64ff9a028793e07",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 36,
            "deletions": 56,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -365,13 +365,16 @@ def forward(\n         query_states = self.q(hidden_states)\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n+        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n                 curr_past_key_value = past_key_value.cross_attention_cache\n             else:\n                 curr_past_key_value = past_key_value.self_attention_cache\n+        else:\n+            curr_past_key_value = past_key_value\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n@@ -440,7 +443,7 @@ def forward(\n         attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        outputs = (attn_output, past_key_value, position_bias)\n+        outputs = (attn_output, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -563,8 +566,8 @@ def forward(\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n-        hidden_states, past_key_value = self_attention_outputs[:2]\n-        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n+        hidden_states = self_attention_outputs[0]\n+        attention_outputs = self_attention_outputs[1:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n         if hidden_states.dtype == torch.float16:\n@@ -588,7 +591,7 @@ def forward(\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n             )\n-            hidden_states, past_key_value = cross_attention_outputs[:2]\n+            hidden_states = cross_attention_outputs[0]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16:\n@@ -600,7 +603,7 @@ def forward(\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n             # Keep cross-attention outputs and relative position weights\n-            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n+            attention_outputs = attention_outputs + cross_attention_outputs[1:]\n \n         # Apply Feed Forward layer\n         hidden_states = self.layer[-1](hidden_states)\n@@ -616,12 +619,9 @@ def forward(\n \n         outputs = (hidden_states,)\n \n-        if use_cache:\n-            outputs = outputs + (past_key_value,) + attention_outputs\n-        else:\n-            outputs = outputs + attention_outputs\n-\n-        return outputs  # hidden-states, past_key_value, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n+        return (\n+            outputs + attention_outputs\n+        )  # hidden-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n \n \n def load_tf_weights_in_mt5(model, config, tf_checkpoint_path):\n@@ -993,23 +993,12 @@ def forward(\n             if not self.is_decoder:\n                 raise ValueError(f\"`use_cache` can only be set to `True` if {self} is used as a decoder\")\n \n-        # initialize past_key_values\n-        return_legacy_cache = False\n-        return_self_attention_cache = False\n-        if self.is_decoder and (use_cache or past_key_values is not None):\n-            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n-                return_self_attention_cache = True\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n-            elif not isinstance(past_key_values, EncoderDecoderCache):\n-                return_legacy_cache = True\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n-                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-            elif past_key_values is None:\n-                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if self.is_decoder:\n+            if use_cache and past_key_values is None:\n+                if self.config.is_encoder_decoder:\n+                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                else:\n+                    past_key_values = DynamicCache()\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place\n@@ -1031,7 +1020,9 @@ def forward(\n                 attention_mask,\n                 inputs_embeds,\n                 cache_position,\n-                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                past_key_values.self_attention_cache\n+                if isinstance(past_key_values, EncoderDecoderCache)\n+                else past_key_values,\n                 output_attentions,\n             )\n         elif attention_mask is not None:\n@@ -1105,24 +1096,19 @@ def forward(\n                 cache_position=cache_position,\n             )\n \n-            # layer_outputs is a tuple with:\n-            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n-            if use_cache is False:\n-                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n-\n-            hidden_states, next_decoder_cache = layer_outputs[:2]\n+            hidden_states = layer_outputs[0]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n-            position_bias = layer_outputs[2]\n+            position_bias = layer_outputs[1]\n             if self.is_decoder and encoder_hidden_states is not None:\n-                encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n+                encoder_decoder_position_bias = layer_outputs[3 if output_attentions else 2]\n \n             if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[3],)\n+                all_attentions = all_attentions + (layer_outputs[2],)\n                 if self.is_decoder:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[4],)\n \n             # Model Parallel: If it's the last layer for that device, put things on the next device\n             if self.model_parallel:\n@@ -1137,18 +1123,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_self_attention_cache:\n-            next_cache = past_key_values.self_attention_cache\n-        if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1157,7 +1137,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1330,12 +1310,12 @@ def __init__(self, config: MT5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = MT5Stack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = MT5Stack(decoder_config, self.shared)\n \n@@ -1420,7 +1400,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1599,12 +1579,12 @@ def __init__(self, config: MT5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = MT5Stack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = MT5Stack(decoder_config, self.shared)\n \n@@ -1692,7 +1672,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.Tensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2311,12 +2291,12 @@ def __init__(self, config: MT5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = MT5Stack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = MT5Stack(decoder_config, self.shared)\n "
        },
        {
            "sha": "30b3faa144282a846fd3a2124da9bc2df4ab8fec",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 5,
            "deletions": 14,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -275,7 +275,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # NO LONGER EXIST Copied from transformers.models.llama.modeling_llama.LlamaFlashAttention2 with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n@@ -396,7 +396,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # NO LONGER EXIST Copied from transformers.models.llama.modeling_llama.LlamaSdpaAttention with LLAMA->NEMOTRON,Llama->Nemotron,llama->nemotron\n@@ -489,7 +489,7 @@ def forward(\n \n         attn_output = self.o_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None\n \n \n NEMOTRON_ATTENTION_CLASSES = {\n@@ -552,7 +552,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -576,9 +576,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -701,7 +698,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -720,9 +716,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -732,11 +725,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )"
        },
        {
            "sha": "0fa4d1961abe7c5ace6ee960e44f8482ffc1ca22",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 15,
            "deletions": 31,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -346,7 +346,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class OlmoeFlashAttention2(OlmoeAttention):\n@@ -459,7 +459,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class OlmoeSdpaAttention(OlmoeAttention):\n@@ -554,7 +554,7 @@ def forward(\n \n         attn_output = self.o_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None\n \n \n OLMOE_ATTENTION_CLASSES = {\n@@ -666,7 +666,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -690,9 +690,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         if output_router_logits:\n             outputs += (router_logits,)\n \n@@ -788,19 +785,12 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if cache_position is None:\n             past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n@@ -824,7 +814,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_logits = () if output_router_logits else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n             if output_hidden_states:\n@@ -844,9 +833,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -859,15 +845,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,\n@@ -1034,7 +1018,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "de52e725340052bfeac9e5cde96b5aed0cd4d92d",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 26,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -198,7 +198,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class OPTDecoderLayer(GradientCheckpointingLayer):\n@@ -256,7 +256,7 @@ def forward(\n             hidden_states = self.self_attn_layer_norm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             past_key_value=past_key_value,\n             position_ids=position_ids,\n@@ -299,9 +299,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -517,7 +514,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -550,7 +547,7 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n \n@@ -606,16 +603,8 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-            if past_key_values is None:\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.53.0. \"\n-                    \"You should pass an instance of `DynamicCache` instead, e.g. \"\n-                    \"`past_key_values=DynamicCache.from_legacy_cache(past_key_values)`.\"\n-                )\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n         if cache_position is None:\n@@ -649,7 +638,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         # check if head_mask has a correct number of layers specified if desired\n         for attn_mask, mask_name in zip([head_mask], [\"head_mask\"]):\n@@ -684,9 +672,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -700,13 +685,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )"
        },
        {
            "sha": "de8f23d3d6b8af1b562515e6a8bc97dd0c12d9a2",
            "filename": "src/transformers/models/owlv2/processing_owlv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fprocessing_owlv2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -28,7 +28,6 @@\n     ProcessingKwargs,\n     ProcessorMixin,\n     Unpack,\n-    _validate_images_text_input_order,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import TensorType, is_flax_available, is_tf_available, is_torch_available\n@@ -71,8 +70,6 @@ class Owlv2Processor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"Owlv2ImageProcessor\"\n     tokenizer_class = (\"CLIPTokenizer\", \"CLIPTokenizerFast\")\n-    # For backward compatibility. See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details.\n-    optional_call_args = [\"query_images\"]\n \n     def __init__(self, image_processor, tokenizer, **kwargs):\n         super().__init__(image_processor, tokenizer)\n@@ -82,12 +79,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        # The following is to capture `query_images` argument that may be passed as a positional argument.\n-        # See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details,\n-        # or this conversation for more context: https://github.com/huggingface/transformers/pull/32544#discussion_r1720208116\n-        # This behavior is only needed for backward compatibility and will be removed in future versions.\n-        #\n-        *args,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[Owlv2ProcessorKwargs],\n@@ -132,7 +123,6 @@ def __call__(\n             Owlv2ProcessorKwargs,\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n-            **self.prepare_and_validate_optional_call_args(*args),\n         )\n         query_images = output_kwargs[\"images_kwargs\"].pop(\"query_images\", None)\n         return_tensors = output_kwargs[\"common_kwargs\"][\"return_tensors\"]\n@@ -141,8 +131,6 @@ def __call__(\n             raise ValueError(\n                 \"You have to specify at least one text or query image or image. All three cannot be none.\"\n             )\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n \n         data = {}\n         if text is not None:"
        },
        {
            "sha": "375402bb0a544c0b08f9f7e5b44e4a8b69467be5",
            "filename": "src/transformers/models/owlvit/processing_owlvit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlvit%2Fprocessing_owlvit.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -28,7 +28,6 @@\n     ProcessingKwargs,\n     ProcessorMixin,\n     Unpack,\n-    _validate_images_text_input_order,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import TensorType, is_flax_available, is_tf_available, is_torch_available\n@@ -71,8 +70,6 @@ class OwlViTProcessor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"OwlViTImageProcessor\"\n     tokenizer_class = (\"CLIPTokenizer\", \"CLIPTokenizerFast\")\n-    # For backward compatibility. See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details.\n-    optional_call_args = [\"query_images\"]\n \n     def __init__(self, image_processor=None, tokenizer=None, **kwargs):\n         feature_extractor = None\n@@ -96,12 +93,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        # The following is to capture `query_images` argument that may be passed as a positional argument.\n-        # See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details,\n-        # or this conversation for more context: https://github.com/huggingface/transformers/pull/32544#discussion_r1720208116\n-        # This behavior is only needed for backward compatibility and will be removed in future versions.\n-        #\n-        *args,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[OwlViTProcessorKwargs],\n@@ -146,7 +137,6 @@ def __call__(\n             OwlViTProcessorKwargs,\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n-            **self.prepare_and_validate_optional_call_args(*args),\n         )\n         query_images = output_kwargs[\"images_kwargs\"].pop(\"query_images\", None)\n         return_tensors = output_kwargs[\"common_kwargs\"][\"return_tensors\"]\n@@ -155,8 +145,6 @@ def __call__(\n             raise ValueError(\n                 \"You have to specify at least one text or query image or image. All three cannot be none.\"\n             )\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n \n         data = {}\n         if text is not None:"
        },
        {
            "sha": "9ee315b8bebad21cd6aba59894f663f02ccb91ce",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -50,7 +50,7 @@\n )\n class PaligemmaModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -76,7 +76,7 @@ class PaliGemmaCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.text_config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n "
        },
        {
            "sha": "b4c8e555b52dbec5dc4718bcadb01d7bdedbd44b",
            "filename": "src/transformers/models/paligemma/processing_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fprocessing_paligemma.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -29,7 +29,6 @@\n     ProcessorMixin,\n     TextKwargs,\n     Unpack,\n-    _validate_images_text_input_order,\n )\n from ...tokenization_utils_base import AddedToken, PreTokenizedInput, TextInput\n from ...utils import logging\n@@ -216,8 +215,6 @@ def __call__(\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n             - **labels** -- Labels compatible with training if `suffix` is not None\n         \"\"\"\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n \n         output_kwargs = self._merge_kwargs(\n             PaliGemmaProcessorKwargs,"
        },
        {
            "sha": "d8f8a511bc0c265713a6e233f439d45b00ef0630",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -998,7 +998,7 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n@@ -1630,7 +1630,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "500150bba4d3bfcab0c64c016a87ade3595c2f6f",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -1254,7 +1254,7 @@ def forward(\n \n                 [What are attention masks?](../glossary#attention-mask)\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`."
        },
        {
            "sha": "5e62375845bf6d2f50aaf4263ff7a774c30b7cc1",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 11,
            "deletions": 29,
            "changes": 40,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -298,7 +298,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class PersimmonDecoderLayer(GradientCheckpointingLayer):\n@@ -352,7 +352,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -378,9 +378,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -453,7 +450,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -477,19 +474,12 @@ def forward(\n                 )\n                 use_cache = False\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -514,7 +504,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -534,9 +523,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -546,13 +532,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n@@ -727,7 +709,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "f3ad3bafb8229896c56090082bca5fcd33725c11",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -1657,7 +1657,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         image_pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n@@ -1786,7 +1786,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         image_pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "4ccba03cce4301dbc2ea1879e143da3b248c254c",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -21,7 +21,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import DynamicCache\n+from ...cache_utils import Cache, DynamicCache\n from ...configuration_utils import PretrainedConfig\n from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n@@ -1504,7 +1504,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         image_pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,\n@@ -1610,7 +1610,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         image_pixel_values: Optional[torch.FloatTensor] = None,\n         image_sizes: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "e8f9e1060026cc4d95aa10f3c20c3efdc6c93ec3",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 13,
            "deletions": 31,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -321,7 +321,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class PhimoeFlashAttention2(PhimoeAttention):\n@@ -419,7 +419,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class PhimoeSdpaAttention(PhimoeAttention):\n@@ -507,7 +507,7 @@ def forward(\n \n         attn_output = self.o_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None\n \n \n PHIMOE_ATTENTION_CLASSES = {\n@@ -851,7 +851,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -874,9 +874,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         if output_router_logits:\n             outputs += (router_logits,)\n \n@@ -950,7 +947,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -979,19 +976,12 @@ def forward(\n                 )\n                 use_cache = False\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -1016,7 +1006,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_logits = () if output_router_logits else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -1036,9 +1025,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1051,13 +1037,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,\n@@ -1264,7 +1246,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "93c281dc5bab858c70c63eee43e03fe77ae4abf4",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 26,
            "deletions": 49,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -759,13 +759,16 @@ def forward(\n         query_states = self.query(hidden_states)\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n+        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n                 curr_past_key_value = past_key_value.cross_attention_cache\n             else:\n                 curr_past_key_value = past_key_value.self_attention_cache\n+        else:\n+            curr_past_key_value = past_key_value\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value and is_updated:\n@@ -834,7 +837,7 @@ def forward(\n         attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.output(attn_output)\n \n-        outputs = (attn_output, past_key_value, position_bias)\n+        outputs = (attn_output, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -960,8 +963,8 @@ def forward(\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n-        hidden_states, past_key_value = self_attention_outputs[:2]\n-        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n+        hidden_states = self_attention_outputs[0]\n+        attention_outputs = self_attention_outputs[1:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n         if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n@@ -981,15 +984,15 @@ def forward(\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n             )\n-            hidden_states, past_key_value = cross_attention_outputs[:2]\n+            hidden_states = cross_attention_outputs[0]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n                 clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n             # Keep cross-attention outputs and relative position weights\n-            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n+            attention_outputs = attention_outputs + cross_attention_outputs[1:]\n \n         # Apply Feed Forward layer\n         hidden_states = self.mlp(hidden_states)\n@@ -1001,12 +1004,7 @@ def forward(\n \n         outputs = (hidden_states,)\n \n-        if use_cache:\n-            outputs = outputs + (past_key_value,) + attention_outputs\n-        else:\n-            outputs = outputs + attention_outputs\n-\n-        return outputs\n+        return outputs + attention_outputs\n \n \n @auto_docstring(\n@@ -1092,7 +1090,7 @@ def forward(\n         inputs_embeds: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n@@ -1162,23 +1160,11 @@ def forward(\n \n         batch_size, seq_length = input_shape\n \n-        # initialize past_key_values\n-        return_legacy_cache = False\n-        return_self_attention_cache = False\n-        if use_cache or past_key_values is not None:\n-            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n-                return_self_attention_cache = True\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n-            elif not isinstance(past_key_values, EncoderDecoderCache):\n-                return_legacy_cache = True\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n-                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-            elif past_key_values is None:\n+        if use_cache and past_key_values is None:\n+            if self.config.is_encoder_decoder:\n                 past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            else:\n+                past_key_values = DynamicCache()\n \n         past_key_values_length = 0\n         if cache_position is not None:\n@@ -1203,7 +1189,9 @@ def forward(\n                 attention_mask,\n                 inputs_embeds,\n                 cache_position,\n-                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                past_key_values.self_attention_cache\n+                if isinstance(past_key_values, EncoderDecoderCache)\n+                else past_key_values,\n                 output_attentions,\n             )\n         else:\n@@ -1254,24 +1242,19 @@ def forward(\n                 cache_position=cache_position,\n             )\n \n-            # layer_outputs is a tuple with:\n-            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n-            if use_cache is False:\n-                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n-\n-            hidden_states, next_decoder_cache = layer_outputs[:2]\n+            hidden_states = layer_outputs[0]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n-            position_bias = layer_outputs[2]\n+            position_bias = layer_outputs[1]\n             if encoder_hidden_states is not None:\n-                encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n+                encoder_decoder_position_bias = layer_outputs[3 if output_attentions else 2]\n \n             if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[3],)\n+                all_attentions = all_attentions + (layer_outputs[2],)\n                 if encoder_hidden_states is not None:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[4],)\n \n         hidden_states = self.final_layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n@@ -1290,19 +1273,13 @@ def forward(\n \n             loss = loss_fct(logits.contiguous().view(-1, logits.size(-1)), labels.contiguous().view(-1))\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_self_attention_cache:\n-            next_cache = past_key_values.self_attention_cache\n-        if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     loss,\n                     logits,\n-                    next_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1312,7 +1289,7 @@ def forward(\n         return CausalLMOutputWithCrossAttentions(\n             loss=loss,\n             logits=logits,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1494,7 +1471,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         labels: Optional[torch.LongTensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "16ebaa0d55247ecba8d3d5580f561fe4326b5610",
            "filename": "src/transformers/models/pixtral/processing_pixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpixtral%2Fprocessing_pixtral.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -27,7 +27,6 @@\n     ProcessingKwargs,\n     ProcessorMixin,\n     Unpack,\n-    _validate_images_text_input_order,\n )\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import is_vision_available, logging\n@@ -157,8 +156,6 @@ def __call__(\n             `None`).\n             - **pixel_values** -- Pixel values to be fed to a model. Returned when `images` is not `None`.\n         \"\"\"\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n \n         output_kwargs = self._merge_kwargs(\n             PixtralProcessorKwargs,"
        },
        {
            "sha": "3b071a1fe3d9371880a011304165c4909b28b0fb",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -876,7 +876,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.LongTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -925,7 +925,7 @@ def forward(\n                 - 1 indicates the head is **not masked**,\n                 - 0 indicates the head is **masked**.\n \n-            past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+            past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n                 Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of\n                 shape `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and 2 additional tensors of\n                 shape `(batch_size, num_heads, encoder_sequence_length, embed_size_per_head)`.\n@@ -1169,7 +1169,7 @@ def forward(\n         decoder_head_mask: Optional[torch.LongTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1326,7 +1326,7 @@ def forward(\n         decoder_head_mask: Optional[torch.LongTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.Tensor] = None,\n@@ -1689,7 +1689,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "22200c0bbf65bcb45fa561c7558a9e640aae9ae2",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -325,7 +325,7 @@ def forward(\n         decoder_head_mask: Optional[torch.LongTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -482,7 +482,7 @@ def forward(\n         decoder_head_mask: Optional[torch.LongTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[list[torch.FloatTensor]] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.Tensor] = None,"
        },
        {
            "sha": "dca3191be9e0be23190544d5780a65fb6e34a803",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 31,
            "deletions": 51,
            "changes": 82,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -309,13 +309,16 @@ def forward(\n         query_states = self.q(hidden_states)\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n+        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n                 curr_past_key_value = past_key_value.cross_attention_cache\n             else:\n                 curr_past_key_value = past_key_value.self_attention_cache\n+        else:\n+            curr_past_key_value = past_key_value\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n@@ -384,7 +387,7 @@ def forward(\n         attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        outputs = (attn_output, past_key_value, position_bias)\n+        outputs = (attn_output, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -509,8 +512,8 @@ def forward(\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n-        hidden_states, past_key_value = self_attention_outputs[:2]\n-        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n+        hidden_states = self_attention_outputs[0]\n+        attention_outputs = self_attention_outputs[1:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n         if hidden_states.dtype == torch.float16:\n@@ -534,7 +537,7 @@ def forward(\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n             )\n-            hidden_states, past_key_value = cross_attention_outputs[:2]\n+            hidden_states = cross_attention_outputs[0]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16:\n@@ -546,7 +549,7 @@ def forward(\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n             # Keep cross-attention outputs and relative position weights\n-            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n+            attention_outputs = attention_outputs + cross_attention_outputs[1:]\n \n         # Apply Feed Forward layer\n         hidden_states = self.layer[-1](hidden_states)\n@@ -562,12 +565,9 @@ def forward(\n \n         outputs = (hidden_states,)\n \n-        if use_cache:\n-            outputs = outputs + (past_key_value,) + attention_outputs\n-        else:\n-            outputs = outputs + attention_outputs\n-\n-        return outputs  # hidden-states, past_key_value, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n+        return (\n+            outputs + attention_outputs\n+        )  # hidden-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n \n \n @auto_docstring\n@@ -741,23 +741,12 @@ def forward(\n             if not self.is_decoder:\n                 raise ValueError(f\"`use_cache` can only be set to `True` if {self} is used as a decoder\")\n \n-        # initialize past_key_values\n-        return_legacy_cache = False\n-        return_self_attention_cache = False\n-        if self.is_decoder and (use_cache or past_key_values is not None):\n-            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n-                return_self_attention_cache = True\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n-            elif not isinstance(past_key_values, EncoderDecoderCache):\n-                return_legacy_cache = True\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n-                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-            elif past_key_values is None:\n-                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if self.is_decoder:\n+            if use_cache and past_key_values is None:\n+                if self.config.is_encoder_decoder:\n+                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                else:\n+                    past_key_values = DynamicCache()\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place\n@@ -779,7 +768,9 @@ def forward(\n                 attention_mask,\n                 inputs_embeds,\n                 cache_position,\n-                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                past_key_values.self_attention_cache\n+                if isinstance(past_key_values, EncoderDecoderCache)\n+                else past_key_values,\n                 output_attentions,\n             )\n         else:\n@@ -830,24 +821,19 @@ def forward(\n                 cache_position=cache_position,\n             )\n \n-            # layer_outputs is a tuple with:\n-            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n-            if use_cache is False:\n-                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n-\n-            hidden_states, next_decoder_cache = layer_outputs[:2]\n+            hidden_states = layer_outputs[0]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n-            position_bias = layer_outputs[2]\n+            position_bias = layer_outputs[1]\n             if self.is_decoder and encoder_hidden_states is not None:\n-                encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n+                encoder_decoder_position_bias = layer_outputs[3 if output_attentions else 2]\n \n             if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[3],)\n+                all_attentions = all_attentions + (layer_outputs[2],)\n                 if self.is_decoder:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[4],)\n \n         hidden_states = self.final_layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n@@ -856,18 +842,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_self_attention_cache:\n-            next_cache = past_key_values.self_attention_cache\n-        if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -876,7 +856,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1042,13 +1022,13 @@ def __init__(self, config: Pop2PianoConfig):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n \n         self.encoder = Pop2PianoStack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = Pop2PianoStack(decoder_config, self.shared)\n \n@@ -1137,7 +1117,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.Tensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         input_features: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,"
        },
        {
            "sha": "99596434270fd1e5cf8a467833ed02bd8b967c6a",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -1467,13 +1467,13 @@ def __init__(self, config: ProphetNetConfig):\n         self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=config.pad_token_id)\n \n         encoder_config = copy.deepcopy(config)\n-        encoder_config.is_encoder_decoder = False\n         encoder_config.use_cache = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = ProphetNetEncoder(encoder_config, self.word_embeddings)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         self.decoder = ProphetNetDecoder(decoder_config, self.word_embeddings)\n \n         # Initialize weights and apply final processing"
        },
        {
            "sha": "552249f0ed2b60f28c360639a8d0a011158a09a4",
            "filename": "src/transformers/models/qwen2_5_omni/configuration_qwen2_5_omni.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fconfiguration_qwen2_5_omni.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -19,7 +19,6 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n-\n from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...modeling_rope_utils import rope_config_validation\n from ...utils import logging"
        },
        {
            "sha": "2ac680b7fec8e49c785c31c54a6a8d8b27e594e7",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 16,
            "deletions": 27,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -531,7 +531,7 @@ class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -1438,7 +1438,7 @@ def forward(\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class Qwen2MLP(nn.Module):\n@@ -1511,7 +1511,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -1535,9 +1535,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -1576,7 +1573,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1649,7 +1646,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -1669,9 +1665,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -1681,13 +1674,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n@@ -1817,7 +1810,7 @@ def forward(\n         feature_attention_mask: Optional[torch.Tensor] = None,\n         audio_feature_lengths: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         rope_deltas: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2082,7 +2075,7 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -2138,7 +2131,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -2211,7 +2204,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -2231,9 +2223,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -2243,13 +2232,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n@@ -2294,7 +2283,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         thinker_reply_part: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         rope_deltas: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "bf7f9930f64adc6d04020c91c7c3096ae5fbfac2",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -42,6 +42,7 @@\n from transformers.models.qwen2_audio.modeling_qwen2_audio import Qwen2AudioEncoderLayer\n from transformers.models.qwen2_vl.modeling_qwen2_vl import Qwen2VLRotaryEmbedding\n \n+from ...cache_utils import Cache\n from ...configuration_utils import PretrainedConfig, layer_type_validation\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import is_flash_attn_available\n@@ -1572,7 +1573,7 @@ class Qwen2_5OmniThinkerCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`, *optional*):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -2262,7 +2263,7 @@ def forward(\n         feature_attention_mask: Optional[torch.Tensor] = None,\n         audio_feature_lengths: Optional[torch.LongTensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         rope_deltas: Optional[torch.LongTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2527,7 +2528,7 @@ class Qwen2_5OmniTalkerCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -2597,7 +2598,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         thinker_reply_part: Optional[torch.FloatTensor] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         rope_deltas: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "d6efd976452c3a9e805ade0472497b622ead7f89",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 18,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -513,7 +513,7 @@ def forward(self, hidden_states: torch.Tensor, grid_thw: torch.Tensor, **kwargs)\n )\n class Qwen2_5_VLModelOutputWithPast(ModelOutput):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -716,7 +716,7 @@ def forward(\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class Qwen2_5_VLDecoderLayer(GradientCheckpointingLayer):\n@@ -775,7 +775,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -799,9 +799,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -839,7 +836,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -912,7 +909,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -932,9 +928,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -944,13 +937,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n@@ -1206,7 +1199,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1371,7 +1364,7 @@ class Qwen2_5_VLCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -1445,7 +1438,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "343cdc2620b2a6b3a6f6531f0a4b24d277213310",
            "filename": "src/transformers/models/qwen2_5_vl/modular_qwen2_5_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodular_qwen2_5_vl.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -44,6 +44,7 @@\n from transformers.models.qwen2_vl.processing_qwen2_vl import Qwen2VLImagesKwargs, Qwen2VLProcessor\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...configuration_utils import PretrainedConfig\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput\n@@ -548,7 +549,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -711,7 +712,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "c3cce2f9b61c8f3d8380a8fca3adaae6a993262c",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 13,
            "deletions": 31,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -362,7 +362,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # NO LONGER EXIST Copied from transformers.models.qwen2.modeling_qwen2.Qwen2FlashAttention2 with Qwen2->Qwen2Moe\n@@ -478,7 +478,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n # NO LONGER EXIST Copied from transformers.models.qwen2.modeling_qwen2.Qwen2SdpaAttention with Qwen2->Qwen2Moe\n@@ -569,7 +569,7 @@ def forward(\n \n         attn_output = self.o_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None\n \n \n QWEN2MOE_ATTENTION_CLASSES = {\n@@ -702,7 +702,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -731,9 +731,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         if output_router_logits:\n             outputs += (router_logits,)\n \n@@ -798,7 +795,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -825,19 +822,12 @@ def forward(\n                 )\n                 use_cache = False\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -863,7 +853,6 @@ def forward(\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n         all_router_logits = () if output_router_logits else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -883,9 +872,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -898,13 +884,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         return MoeModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n             router_logits=all_router_logits,\n@@ -1110,7 +1092,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "062561bb523afe88d172ad67b8a555bed1407213",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 11,
            "deletions": 18,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -59,7 +59,7 @@\n )\n class Qwen2VLModelOutputWithPast(ModelOutput):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -88,7 +88,7 @@ class Qwen2VLCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -563,7 +563,7 @@ def forward(\n \n         attn_output = attn_output.reshape(bsz, q_len, -1).contiguous()\n         attn_output = self.o_proj(attn_output)\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class Qwen2VLDecoderLayer(GradientCheckpointingLayer):\n@@ -622,7 +622,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        hidden_states, self_attn_weights, present_key_value = self.self_attn(\n+        hidden_states, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -646,9 +646,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -814,7 +811,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -887,7 +884,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -907,9 +903,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -919,13 +912,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-\n         if not return_dict:\n-            return tuple(v for v in [hidden_states, next_cache, all_hidden_states, all_self_attns] if v is not None)\n+            return tuple(\n+                v for v in [hidden_states, past_key_values, all_hidden_states, all_self_attns] if v is not None\n+            )\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n@@ -1146,7 +1139,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -1346,7 +1339,7 @@ def forward(\n         input_ids: torch.LongTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "75d0ea87424eb4212c9d6b5144b66261acafdb17",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -650,7 +650,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "c71c1cae2c2f9e1191178b12c41827c5c5e51ba4",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...processing_utils import Unpack\n@@ -191,7 +192,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "299c90e2cfe2c6503f47ab121d64d2a245822043",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -21,6 +21,7 @@\n import torch\n from torch import nn\n \n+from ...cache_utils import EncoderDecoderCache\n from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationConfig, GenerationMixin, LogitsProcessorList, StoppingCriteriaList\n from ...modeling_outputs import ModelOutput\n@@ -1201,6 +1202,8 @@ def _reorder_stacked(hidden_states, new_order):\n             reordered_past += (\n                 tuple(_reorder_stacked(past_state, beam_idx.to(past_state.device)) for past_state in layer_past),\n             )\n+        if isinstance(past_key_values, EncoderDecoderCache):\n+            reordered_past = EncoderDecoderCache.from_legacy_cache(reordered_past)\n \n         return reordered_past\n \n@@ -1592,6 +1595,14 @@ def extend_enc_output(tensor, num_beams=None):\n                 f\"`num_beams` has to be an integer strictly superior to 0 ( 1), but is {generation_config.num_beams}\"\n             )\n \n+    # Auxiliary functions for beam search\n+    def _temporary_reorder_cache(self, past_key_values, beam_idx):\n+        # RAG should always use the legacy path even though the LM backbone (T5) uses new cache format\n+        # because RAG expands input for doc-size internally. TODO: raushan, remove me when all models support\n+        # new cache format\n+        past_key_values = self._reorder_cache(past_key_values, beam_idx)\n+        return past_key_values\n+\n     def get_input_embeddings(self):\n         return self.rag.generator.get_input_embeddings()\n "
        },
        {
            "sha": "c1c7f22cc7770e80199347697c9a12cfeb5c32c0",
            "filename": "src/transformers/models/sam/processing_sam.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam%2Fprocessing_sam.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -67,13 +67,6 @@ class SamProcessor(ProcessorMixin):\n \n     attributes = [\"image_processor\"]\n     image_processor_class = \"SamImageProcessor\"\n-    # For backward compatibility. See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details.\n-    optional_call_args = [\n-        \"segmentation_maps\",\n-        \"input_points\",\n-        \"input_labels\",\n-        \"input_boxes\",\n-    ]\n \n     def __init__(self, image_processor):\n         super().__init__(image_processor)\n@@ -82,13 +75,6 @@ def __init__(self, image_processor):\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n-        # The following is to capture `segmentation_maps`, `input_points`, `input_labels` and `input_boxes`\n-        # arguments that may be passed as a positional argument.\n-        # See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details,\n-        # or this conversation for more context:\n-        # https://github.com/huggingface/transformers/pull/32544#discussion_r1720208116\n-        # This behavior is only needed for backward compatibility and will be removed in future versions.\n-        *args,  # to be deprecated\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         audio: Optional[AudioInput] = None,\n         video: Optional[VideoInput] = None,\n@@ -102,7 +88,6 @@ def __call__(\n             SamProcessorKwargs,\n             tokenizer_init_kwargs={},\n             **kwargs,\n-            **self.prepare_and_validate_optional_call_args(*args),\n         )\n         input_points = output_kwargs[\"images_kwargs\"].pop(\"input_points\", None)\n         input_labels = output_kwargs[\"images_kwargs\"].pop(\"input_labels\", None)"
        },
        {
            "sha": "97dbcdfab638dd47d70caff2462835f9960febe2",
            "filename": "src/transformers/models/sam_hq/processing_samhq.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam_hq%2Fprocessing_samhq.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -65,13 +65,6 @@ class SamHQProcessor(ProcessorMixin):\n     attributes = [\"image_processor\"]\n     image_processor_class = \"SamImageProcessor\"\n \n-    optional_call_args = [\n-        \"segmentation_maps\",\n-        \"input_points\",\n-        \"input_labels\",\n-        \"input_boxes\",\n-    ]\n-\n     def __init__(self, image_processor):\n         super().__init__(image_processor)\n         # Ensure image_processor is properly initialized\n@@ -84,13 +77,6 @@ def __init__(self, image_processor):\n     def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n-        # The following is to capture `segmentation_maps`, `input_points`, `input_labels` and `input_boxes`\n-        # arguments that may be passed as a positional argument.\n-        # See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details,\n-        # or this conversation for more context:\n-        # https://github.com/huggingface/transformers/pull/32544#discussion_r1720208116\n-        # This behavior is only needed for backward compatibility and will be removed in future versions.\n-        *args,  # to be deprecated\n         text: Optional[Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]]] = None,\n         audio: Optional[AudioInput] = None,\n         video: Optional[VideoInput] = None,\n@@ -104,7 +90,6 @@ def __call__(\n             SamHQProcessorKwargs,\n             tokenizer_init_kwargs={},\n             **kwargs,\n-            **self.prepare_and_validate_optional_call_args(*args),\n         )\n \n         input_points = output_kwargs[\"images_kwargs\"].pop(\"input_points\", None)"
        },
        {
            "sha": "befec29d9dede66a44c923734c58d57875000e4f",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -26,7 +26,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...cache_utils import DynamicCache\n+from ...cache_utils import Cache, DynamicCache\n from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n@@ -463,7 +463,7 @@ class SmolVLMBaseModelOutputWithPast(ModelOutput):\n         Sequence of hidden-states at the output of the last layer of the model.\n         If `past_key_values` is used only the last hidden-state of the sequences of shape `(batch_size, 1,\n         hidden_size)` is output.\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n         `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n@@ -687,7 +687,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_attention_mask: Optional[torch.BoolTensor] = None,\n@@ -784,7 +784,7 @@ class SmolVLMCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n         Contains pre-computed hidden-states (key and values in the self-attention blocks) that can be used (see\n@@ -861,7 +861,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_attention_mask: Optional[torch.BoolTensor] = None,"
        },
        {
            "sha": "dfda67472ca9fb12d0d9b8b89addea3e0dd27e36",
            "filename": "src/transformers/models/smolvlm/modular_smolvlm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodular_smolvlm.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -19,7 +19,7 @@\n import torch.utils.checkpoint\n from torch import nn\n \n-from ...cache_utils import DynamicCache\n+from ...cache_utils import Cache, DynamicCache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...processing_utils import Unpack\n from ...utils import auto_docstring, can_return_tuple, logging\n@@ -270,7 +270,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         pixel_values: Optional[torch.FloatTensor] = None,\n         pixel_attention_mask: Optional[torch.BoolTensor] = None,"
        },
        {
            "sha": "0aef9d3ab7d6e5559a4362fc137ebfde307e87aa",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 13,
            "deletions": 31,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -301,7 +301,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class StableLmSdpaAttention(StableLmAttention):\n@@ -409,7 +409,7 @@ def forward(\n \n         attn_output = self.o_proj(attn_output)\n \n-        return attn_output, None, past_key_value\n+        return attn_output, None\n \n \n class StableLmFlashAttention2(StableLmAttention):\n@@ -512,7 +512,7 @@ def forward(\n         if not output_attentions:\n             attn_weights = None\n \n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n ATTENTION_CLASSES = {\n@@ -576,7 +576,7 @@ def forward(\n         hidden_states = self.input_layernorm(hidden_states)\n \n         # Self Attention\n-        self_attn_output, self_attn_weights, present_key_value = self.self_attn(\n+        self_attn_output, self_attn_weights = self.self_attn(\n             hidden_states=hidden_states,\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n@@ -608,9 +608,6 @@ def forward(\n         if output_attentions:\n             outputs += (self_attn_weights,)\n \n-        if use_cache:\n-            outputs += (present_key_value,)\n-\n         return outputs\n \n \n@@ -682,7 +679,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = None,\n         output_attentions: Optional[bool] = None,\n@@ -705,19 +702,12 @@ def forward(\n                 )\n                 use_cache = False\n \n-        # kept for BC (non `Cache` `past_key_values` inputs)\n-        return_legacy_cache = False\n-        if use_cache and not isinstance(past_key_values, Cache):\n-            return_legacy_cache = True\n-            if past_key_values is None:\n-                past_key_values = DynamicCache()\n-            else:\n-                past_key_values = DynamicCache.from_legacy_cache(past_key_values)\n-                logger.warning_once(\n-                    \"We detected that you are passing `past_key_values` as a tuple of tuples. This is deprecated and \"\n-                    \"will be removed in v4.47. Please convert your cache or use an appropriate `Cache` class \"\n-                    \"(https://huggingface.co/docs/transformers/kv_cache#legacy-cache-format)\"\n-                )\n+        # TODO (joao): remove this exception in v4.56 -- it exists for users that try to pass a legacy cache\n+        if not isinstance(past_key_values, (type(None), Cache)):\n+            raise ValueError(\"The `past_key_values` should be either a `Cache` object or `None`.\")\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache()\n \n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n@@ -742,7 +732,6 @@ def forward(\n         # decoder layers\n         all_hidden_states = () if output_hidden_states else None\n         all_self_attns = () if output_attentions else None\n-        next_decoder_cache = None\n \n         for decoder_layer in self.layers:\n             if output_hidden_states:\n@@ -761,9 +750,6 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[2 if output_attentions else 1]\n-\n             if output_attentions:\n                 all_self_attns += (layer_outputs[1],)\n \n@@ -773,13 +759,9 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states += (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_legacy_cache:\n-            next_cache = next_cache.to_legacy_cache()\n-\n         return BaseModelOutputWithPast(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_self_attns,\n         )\n@@ -956,7 +938,7 @@ def forward(\n         input_ids: Optional[torch.LongTensor] = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "63f1bfadeebe7bbb244881da7bca6100900d58e4",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 34,
            "deletions": 54,
            "changes": 88,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -502,13 +502,16 @@ def forward(\n         query_states = self.q(hidden_states)\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n+        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n                 curr_past_key_value = past_key_value.cross_attention_cache\n             else:\n                 curr_past_key_value = past_key_value.self_attention_cache\n+        else:\n+            curr_past_key_value = past_key_value\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n@@ -577,7 +580,7 @@ def forward(\n         attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        outputs = (attn_output, past_key_value, position_bias)\n+        outputs = (attn_output, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -705,8 +708,8 @@ def forward(\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n-        hidden_states, past_key_value = self_attention_outputs[:2]\n-        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n+        hidden_states = self_attention_outputs[0]\n+        attention_outputs = self_attention_outputs[1:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n         if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n@@ -727,15 +730,15 @@ def forward(\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n-            hidden_states, past_key_value = cross_attention_outputs[:2]\n+            hidden_states = cross_attention_outputs[0]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16 and torch.isinf(hidden_states).any():\n                 clamp_value = torch.finfo(hidden_states.dtype).max - 1000\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n             # Keep cross-attention outputs and relative position weights\n-            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n+            attention_outputs = attention_outputs + cross_attention_outputs[1:]\n \n         # Apply Feed Forward layer\n         hidden_states = self.layer[-1](hidden_states, output_router_logits)\n@@ -752,12 +755,9 @@ def forward(\n \n         outputs = (hidden_states,)\n \n-        if use_cache:\n-            outputs = outputs + (past_key_value,) + attention_outputs + (router_tuple,)\n-        else:\n-            outputs = outputs + attention_outputs + (router_tuple,)\n-\n-        return outputs  # hidden-states, past_key_value, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights), (router_tuple)\n+        return (\n+            outputs + attention_outputs + (router_tuple,)\n+        )  # hidden-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights), (router_tuple)\n \n \n @auto_docstring\n@@ -949,23 +949,12 @@ def forward(\n             if not self.is_decoder:\n                 raise ValueError(f\"`use_cache` can only be set to `True` if {self} is used as a decoder\")\n \n-        # initialize past_key_values\n-        return_legacy_cache = False\n-        return_self_attention_cache = False\n-        if self.is_decoder and (use_cache or past_key_values is not None):\n-            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n-                return_self_attention_cache = True\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n-            elif not isinstance(past_key_values, EncoderDecoderCache):\n-                return_legacy_cache = True\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n-                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-            elif past_key_values is None:\n-                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if self.is_decoder:\n+            if use_cache and past_key_values is None:\n+                if self.config.is_encoder_decoder:\n+                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                else:\n+                    past_key_values = DynamicCache()\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place\n@@ -987,7 +976,9 @@ def forward(\n                 attention_mask,\n                 inputs_embeds,\n                 cache_position,\n-                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                past_key_values.self_attention_cache\n+                if isinstance(past_key_values, EncoderDecoderCache)\n+                else past_key_values,\n                 output_attentions,\n             )\n         else:\n@@ -1045,24 +1036,19 @@ def forward(\n             router_probs = layer_outputs[-1]\n             layer_outputs = layer_outputs[:-1]\n \n-            # layer_outputs is a tuple with:\n-            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n-            if use_cache is False:\n-                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n-\n-            hidden_states, next_decoder_cache = layer_outputs[:2]\n+            hidden_states = layer_outputs[0]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n-            position_bias = layer_outputs[2]\n+            position_bias = layer_outputs[1]\n             if self.is_decoder and encoder_hidden_states is not None:\n-                encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n+                encoder_decoder_position_bias = layer_outputs[3 if output_attentions else 2]\n \n             if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[3],)\n+                all_attentions = all_attentions + (layer_outputs[2],)\n                 if self.is_decoder:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[4],)\n \n             if output_router_logits:\n                 all_router_probs = all_router_probs + (router_probs,)\n@@ -1074,18 +1060,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_self_attention_cache:\n-            next_cache = past_key_values.self_attention_cache\n-        if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1095,7 +1075,7 @@ def forward(\n             )\n         return MoEModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1248,12 +1228,12 @@ def __init__(self, config: SwitchTransformersConfig):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = SwitchTransformersStack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         self.decoder = SwitchTransformersStack(decoder_config, self.shared)\n \n         # Initialize weights and apply final processing\n@@ -1300,7 +1280,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1465,12 +1445,12 @@ def __init__(self, config: SwitchTransformersConfig):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = SwitchTransformersStack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = SwitchTransformersStack(decoder_config, self.shared)\n \n@@ -1521,7 +1501,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.Tensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "216ea793fd870722986b4cba37bcd151ca8d6c73",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 36,
            "deletions": 56,
            "changes": 92,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -490,13 +490,16 @@ def forward(\n         query_states = self.q(hidden_states)\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n+        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n                 curr_past_key_value = past_key_value.cross_attention_cache\n             else:\n                 curr_past_key_value = past_key_value.self_attention_cache\n+        else:\n+            curr_past_key_value = past_key_value\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n@@ -565,7 +568,7 @@ def forward(\n         attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        outputs = (attn_output, past_key_value, position_bias)\n+        outputs = (attn_output, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -685,8 +688,8 @@ def forward(\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n-        hidden_states, past_key_value = self_attention_outputs[:2]\n-        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n+        hidden_states = self_attention_outputs[0]\n+        attention_outputs = self_attention_outputs[1:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n         if hidden_states.dtype == torch.float16:\n@@ -710,7 +713,7 @@ def forward(\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n             )\n-            hidden_states, past_key_value = cross_attention_outputs[:2]\n+            hidden_states = cross_attention_outputs[0]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16:\n@@ -722,7 +725,7 @@ def forward(\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n             # Keep cross-attention outputs and relative position weights\n-            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n+            attention_outputs = attention_outputs + cross_attention_outputs[1:]\n \n         # Apply Feed Forward layer\n         hidden_states = self.layer[-1](hidden_states)\n@@ -738,12 +741,9 @@ def forward(\n \n         outputs = (hidden_states,)\n \n-        if use_cache:\n-            outputs = outputs + (past_key_value,) + attention_outputs\n-        else:\n-            outputs = outputs + attention_outputs\n-\n-        return outputs  # hidden-states, past_key_value, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n+        return (\n+            outputs + attention_outputs\n+        )  # hidden-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n \n \n class T5ClassificationHead(nn.Module):\n@@ -1006,23 +1006,12 @@ def forward(\n             if not self.is_decoder:\n                 raise ValueError(f\"`use_cache` can only be set to `True` if {self} is used as a decoder\")\n \n-        # initialize past_key_values\n-        return_legacy_cache = False\n-        return_self_attention_cache = False\n-        if self.is_decoder and (use_cache or past_key_values is not None):\n-            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n-                return_self_attention_cache = True\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n-            elif not isinstance(past_key_values, EncoderDecoderCache):\n-                return_legacy_cache = True\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n-                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-            elif past_key_values is None:\n-                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if self.is_decoder:\n+            if use_cache and past_key_values is None:\n+                if self.config.is_encoder_decoder:\n+                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                else:\n+                    past_key_values = DynamicCache()\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place\n@@ -1044,7 +1033,9 @@ def forward(\n                 attention_mask,\n                 inputs_embeds,\n                 cache_position,\n-                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                past_key_values.self_attention_cache\n+                if isinstance(past_key_values, EncoderDecoderCache)\n+                else past_key_values,\n                 output_attentions,\n             )\n         elif attention_mask is not None:\n@@ -1118,24 +1109,19 @@ def forward(\n                 cache_position=cache_position,\n             )\n \n-            # layer_outputs is a tuple with:\n-            # hidden-states, key-value-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n-            if use_cache is False:\n-                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n-\n-            hidden_states, next_decoder_cache = layer_outputs[:2]\n+            hidden_states = layer_outputs[0]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention position bias), (self-attention weights),\n             # (cross-attention position bias), (cross-attention weights)\n-            position_bias = layer_outputs[2]\n+            position_bias = layer_outputs[1]\n             if self.is_decoder and encoder_hidden_states is not None:\n-                encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n+                encoder_decoder_position_bias = layer_outputs[3 if output_attentions else 2]\n \n             if output_attentions:\n-                all_attentions = all_attentions + (layer_outputs[3],)\n+                all_attentions = all_attentions + (layer_outputs[2],)\n                 if self.is_decoder:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[4],)\n \n             # Model Parallel: If it's the last layer for that device, put things on the next device\n             if self.model_parallel:\n@@ -1150,18 +1136,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_self_attention_cache:\n-            next_cache = past_key_values.self_attention_cache\n-        if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1170,7 +1150,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1325,12 +1305,12 @@ def __init__(self, config: T5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = T5Stack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = T5Stack(decoder_config, self.shared)\n \n@@ -1412,7 +1392,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1574,12 +1554,12 @@ def __init__(self, config: T5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = T5Stack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = T5Stack(decoder_config, self.shared)\n \n@@ -1663,7 +1643,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.Tensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -2256,12 +2236,12 @@ def __init__(self, config: T5Config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = T5Stack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = T5Stack(decoder_config, self.shared)\n "
        },
        {
            "sha": "23af62e4a1dc830ac064f93da2442269bb42ee5a",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 36,
            "deletions": 54,
            "changes": 90,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -75,7 +75,7 @@ class BaseModelOutputWithAttentionMask(ModelOutput):\n         Mask values selected in `[0, 1]`:\n         - 1 for tokens that are **not masked**,\n         - 0 for tokens that are **masked**.\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`) and optionally if\n         `config.is_encoder_decoder=True` 2 additional tensors of shape `(batch_size, num_heads,\n@@ -98,7 +98,7 @@ class BaseModelOutputWithAttentionMask(ModelOutput):\n \n     last_hidden_state: Optional[torch.FloatTensor] = None\n     attention_mask: Optional[torch.FloatTensor] = None\n-    past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None\n+    past_key_values: Optional[Cache] = None\n     hidden_states: Optional[tuple[torch.FloatTensor]] = None\n     attentions: Optional[tuple[torch.FloatTensor]] = None\n     cross_attentions: Optional[tuple[torch.FloatTensor]] = None\n@@ -588,13 +588,16 @@ def forward(\n         query_states = self.q(hidden_states)\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n+        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n                 curr_past_key_value = past_key_value.cross_attention_cache\n             else:\n                 curr_past_key_value = past_key_value.self_attention_cache\n+        else:\n+            curr_past_key_value = past_key_value\n \n         current_states = key_value_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n@@ -663,7 +666,7 @@ def forward(\n         attn_output = attn_output.view(batch_size, -1, self.inner_dim)\n         attn_output = self.o(attn_output)\n \n-        outputs = (attn_output, past_key_value, position_bias)\n+        outputs = (attn_output, position_bias)\n \n         if output_attentions:\n             outputs = outputs + (attn_weights,)\n@@ -788,8 +791,8 @@ def forward(\n             output_attentions=output_attentions,\n             cache_position=cache_position,\n         )\n-        hidden_states, past_key_value = self_attention_outputs[:2]\n-        attention_outputs = self_attention_outputs[2:]  # Keep self-attention outputs and relative position weights\n+        hidden_states = self_attention_outputs[0]\n+        attention_outputs = self_attention_outputs[1:]  # Keep self-attention outputs and relative position weights\n \n         # clamp inf values to enable fp16 training\n         if hidden_states.dtype == torch.float16:\n@@ -813,7 +816,7 @@ def forward(\n                 use_cache=use_cache,\n                 output_attentions=output_attentions,\n             )\n-            hidden_states, past_key_value = cross_attention_outputs[:2]\n+            hidden_states = cross_attention_outputs[0]\n \n             # clamp inf values to enable fp16 training\n             if hidden_states.dtype == torch.float16:\n@@ -825,7 +828,7 @@ def forward(\n                 hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n             # Keep cross-attention outputs and relative position weights\n-            attention_outputs = attention_outputs + cross_attention_outputs[2:]\n+            attention_outputs = attention_outputs + cross_attention_outputs[1:]\n \n         # Apply Feed Forward layer\n         hidden_states = self.layer[-1](hidden_states)\n@@ -841,12 +844,9 @@ def forward(\n \n         outputs = (hidden_states,)\n \n-        if use_cache:\n-            outputs = outputs + (past_key_value,) + attention_outputs\n-        else:\n-            outputs = outputs + attention_outputs\n-\n-        return outputs  # hidden-states, past_key_value, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n+        return (\n+            outputs + attention_outputs\n+        )  # hidden-states, (self-attention position bias), (self-attention weights), (cross-attention position bias), (cross-attention weights)\n \n \n class UdopCellEmbeddings(nn.Module):\n@@ -1225,23 +1225,12 @@ def forward(\n         if use_cache is True:\n             assert self.is_decoder, f\"`use_cache` can only be set to `True` if {self} is used as a decoder\"\n \n-        # initialize past_key_values\n-        return_legacy_cache = False\n-        return_self_attention_cache = False\n-        if self.is_decoder and (use_cache or past_key_values is not None):\n-            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n-                return_self_attention_cache = True\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n-            elif not isinstance(past_key_values, EncoderDecoderCache):\n-                return_legacy_cache = True\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n-                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-            elif past_key_values is None:\n-                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if self.is_decoder:\n+            if use_cache and past_key_values is None:\n+                if self.config.is_encoder_decoder:\n+                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                else:\n+                    past_key_values = DynamicCache()\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place\n@@ -1263,7 +1252,9 @@ def forward(\n                 attention_mask,\n                 inputs_embeds,\n                 cache_position,\n-                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                past_key_values.self_attention_cache\n+                if isinstance(past_key_values, EncoderDecoderCache)\n+                else past_key_values,\n                 output_attentions,\n             )\n         else:\n@@ -1310,24 +1301,21 @@ def forward(\n                 output_attentions=output_attentions,\n                 cache_position=cache_position,\n             )\n-            # layer_outputs is a tuple with:\n-            # hidden-states, key-value-states, (self-attention weights), (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n-            if use_cache is False:  # MP fixes\n-                layer_outputs = layer_outputs[:1] + (None,) + layer_outputs[1:]\n-            hidden_states, next_decoder_cache = layer_outputs[:2]\n+\n+            hidden_states = layer_outputs[0]\n \n             # We share the position biases between the layers - the first layer store them\n             # layer_outputs = hidden-states, key-value-states (self-attention weights),\n             # (self-attention position bias), (cross-attention weights), (cross-attention position bias)\n \n-            position_bias = layer_outputs[2]\n+            position_bias = layer_outputs[1]\n             if self.is_decoder and encoder_hidden_states is not None:\n-                encoder_decoder_position_bias = layer_outputs[4 if output_attentions else 3]\n+                encoder_decoder_position_bias = layer_outputs[3 if output_attentions else 2]\n \n             if output_attentions:\n                 all_attentions = all_attentions + (layer_outputs[2],)  # We keep only self-attention weights for now\n                 if self.is_decoder:\n-                    all_cross_attentions = all_cross_attentions + (layer_outputs[5],)\n+                    all_cross_attentions = all_cross_attentions + (layer_outputs[4],)\n \n         hidden_states = self.final_layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n@@ -1336,19 +1324,13 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_self_attention_cache:\n-            next_cache = past_key_values.self_attention_cache\n-        if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n                     attention_mask,\n-                    next_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -1359,7 +1341,7 @@ def forward(\n         return BaseModelOutputWithAttentionMask(\n             last_hidden_state=hidden_states,\n             attention_mask=attention_mask,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -1512,12 +1494,12 @@ def __init__(self, config):\n         encoder_config = deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = UdopStack(encoder_config, self.shared, self.patch_embed)\n \n         decoder_config = deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = UdopStack(decoder_config, self.shared)\n \n@@ -1550,7 +1532,7 @@ def forward(\n         decoder_attention_mask: Optional[Tensor] = None,\n         inputs_embeds: Optional[Tensor] = None,\n         encoder_outputs: Optional[Tensor] = None,\n-        past_key_values: Optional[Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n         head_mask: Optional[Tensor] = None,\n         decoder_inputs_embeds: Optional[Tensor] = None,\n         decoder_head_mask: Optional[Tensor] = None,\n@@ -1708,12 +1690,12 @@ def __init__(self, config):\n         encoder_config = deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = UdopStack(encoder_config, self.shared, self.patch_embed)\n \n         decoder_config = deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = UdopStack(decoder_config, self.shared)\n \n@@ -1755,7 +1737,7 @@ def forward(\n         decoder_attention_mask: Optional[Tensor] = None,\n         inputs_embeds: Optional[Tensor] = None,\n         encoder_outputs: Optional[Tensor] = None,\n-        past_key_values: Optional[Tensor] = None,\n+        past_key_values: Optional[Cache] = None,\n         head_mask: Optional[Tensor] = None,\n         decoder_inputs_embeds: Optional[Tensor] = None,\n         decoder_head_mask: Optional[Tensor] = None,"
        },
        {
            "sha": "1c9b7ee2971d35905d7b378d52fe7e0f78b5c71e",
            "filename": "src/transformers/models/udop/processing_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fprocessing_udop.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -77,8 +77,6 @@ class UdopProcessor(ProcessorMixin):\n     attributes = [\"image_processor\", \"tokenizer\"]\n     image_processor_class = \"LayoutLMv3ImageProcessor\"\n     tokenizer_class = (\"UdopTokenizer\", \"UdopTokenizerFast\")\n-    # For backward compatibility. See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details.\n-    optional_call_args = [\"text_pair\"]\n \n     def __init__(self, image_processor, tokenizer):\n         super().__init__(image_processor, tokenizer)\n@@ -87,12 +85,6 @@ def __call__(\n         self,\n         images: Optional[ImageInput] = None,\n         text: Union[TextInput, PreTokenizedInput, list[TextInput], list[PreTokenizedInput]] = None,\n-        # The following is to capture `text_pair` argument that may be passed as a positional argument.\n-        # See transformers.processing_utils.ProcessorMixin.prepare_and_validate_optional_call_args for more details,\n-        # or this conversation for more context: https://github.com/huggingface/transformers/pull/32544#discussion_r1720208116\n-        # This behavior is only needed for backward compatibility and will be removed in future versions.\n-        #\n-        *args,\n         audio=None,\n         videos=None,\n         **kwargs: Unpack[UdopProcessorKwargs],\n@@ -115,7 +107,6 @@ def __call__(\n             UdopProcessorKwargs,\n             tokenizer_init_kwargs=self.tokenizer.init_kwargs,\n             **kwargs,\n-            **self.prepare_and_validate_optional_call_args(*args),\n         )\n \n         boxes = output_kwargs[\"text_kwargs\"].pop(\"boxes\", None)"
        },
        {
            "sha": "4b9d96db2f21caacee68bfbaf8fa19f8d19b47be",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 29,
            "deletions": 46,
            "changes": 75,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -274,13 +274,16 @@ def forward(\n         query_states = self.q(hidden_states)\n         query_states = query_states.view(batch_size, -1, self.n_heads, self.key_value_proj_dim).transpose(1, 2)\n \n-        if past_key_value is not None:\n+        # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n+        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n                 curr_past_key_value = past_key_value.cross_attention_cache\n             else:\n                 curr_past_key_value = past_key_value.self_attention_cache\n+        else:\n+            curr_past_key_value = past_key_value\n \n         current_states = encoder_hidden_states if is_cross_attention else hidden_states\n         if is_cross_attention and past_key_value is not None and is_updated:\n@@ -346,7 +349,7 @@ def forward(\n         attn_output = attn_output.view(batch_size, seq_length, -1)\n \n         attn_output = self.o(attn_output)\n-        return attn_output, attn_weights, past_key_value\n+        return attn_output, attn_weights\n \n \n class UMT5LayerSelfAttention(nn.Module):\n@@ -431,7 +434,7 @@ def forward(\n         output_attentions=False,\n         cache_position=None,\n     ):\n-        hidden_states, self_attn_weights, past_key_value = self.layer[0](\n+        hidden_states, self_attn_weights = self.layer[0](\n             hidden_states,\n             attention_mask=attention_mask,\n             layer_head_mask=layer_head_mask,\n@@ -449,7 +452,7 @@ def forward(\n         cross_attn_weights = None\n         do_cross_attention = self.is_decoder and encoder_hidden_states is not None\n         if do_cross_attention:\n-            hidden_states, cross_attn_weights, past_key_value = self.layer[1](\n+            hidden_states, cross_attn_weights = self.layer[1](\n                 hidden_states,\n                 encoder_hidden_states=encoder_hidden_states,\n                 attention_mask=encoder_attention_mask,\n@@ -472,10 +475,7 @@ def forward(\n             clamp_value = torch.where(torch.isinf(hidden_states).any(), max_dtype - 1000, max_dtype)\n             hidden_states = torch.clamp(hidden_states, min=-clamp_value, max=clamp_value)\n \n-        outputs = (\n-            hidden_states,\n-            past_key_value,\n-        )\n+        outputs = (hidden_states,)\n \n         if output_attentions:\n             outputs += (self_attn_weights, cross_attn_weights)\n@@ -692,22 +692,12 @@ def forward(\n                 raise ValueError(f\"`use_cache` can only be set to `True` if {self} is used as a decoder\")\n \n         # initialize past_key_values\n-        return_legacy_cache = False\n-        return_self_attention_cache = False\n-        if self.is_decoder and (use_cache or past_key_values is not None):\n-            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n-                return_self_attention_cache = True\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n-            elif not isinstance(past_key_values, EncoderDecoderCache):\n-                return_legacy_cache = True\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. \"\n-                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n-            elif past_key_values is None:\n-                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+        if self.is_decoder:\n+            if use_cache and past_key_values is None:\n+                if self.config.is_encoder_decoder:\n+                    past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+                else:\n+                    past_key_values = DynamicCache()\n         elif not self.is_decoder:\n             # do not pass cache object down the line for encoder stack\n             # it messes indexing later in decoder-stack because cache object is modified in-place\n@@ -729,7 +719,9 @@ def forward(\n                 attention_mask,\n                 inputs_embeds,\n                 cache_position,\n-                past_key_values.self_attention_cache if past_key_values is not None else None,\n+                past_key_values.self_attention_cache\n+                if isinstance(past_key_values, EncoderDecoderCache)\n+                else past_key_values,\n                 output_attentions,\n             )\n         elif attention_mask is not None:\n@@ -781,13 +773,10 @@ def forward(\n \n             hidden_states = layer_outputs[0]\n \n-            if use_cache:\n-                next_decoder_cache = layer_outputs[1]\n-\n             if output_attentions:\n-                all_attentions += (layer_outputs[2],)\n+                all_attentions += (layer_outputs[1],)\n                 if self.is_decoder:\n-                    all_cross_attentions += (layer_outputs[3],)\n+                    all_cross_attentions += (layer_outputs[2],)\n \n         hidden_states = self.final_layer_norm(hidden_states)\n         hidden_states = self.dropout(hidden_states)\n@@ -796,18 +785,12 @@ def forward(\n         if output_hidden_states:\n             all_hidden_states = all_hidden_states + (hidden_states,)\n \n-        next_cache = next_decoder_cache if use_cache else None\n-        if return_self_attention_cache:\n-            next_cache = past_key_values.self_attention_cache\n-        if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n-\n         if not return_dict:\n             return tuple(\n                 v\n                 for v in [\n                     hidden_states,\n-                    next_cache,\n+                    past_key_values,\n                     all_hidden_states,\n                     all_attentions,\n                     all_cross_attentions,\n@@ -816,7 +799,7 @@ def forward(\n             )\n         return BaseModelOutputWithPastAndCrossAttentions(\n             last_hidden_state=hidden_states,\n-            past_key_values=next_cache,\n+            past_key_values=past_key_values,\n             hidden_states=all_hidden_states,\n             attentions=all_attentions,\n             cross_attentions=all_cross_attentions,\n@@ -978,12 +961,12 @@ def __init__(self, config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = UMT5Stack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = UMT5Stack(decoder_config, self.shared)\n \n@@ -1034,7 +1017,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.Tensor] = None,\n         decoder_inputs_embeds: Optional[torch.Tensor] = None,\n         use_cache: Optional[bool] = None,\n@@ -1193,12 +1176,12 @@ def __init__(self, config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = UMT5Stack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = UMT5Stack(decoder_config, self.shared)\n \n@@ -1250,7 +1233,7 @@ def forward(\n         decoder_head_mask: Optional[torch.FloatTensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.Tensor]]] = None,\n-        past_key_values: Optional[tuple[tuple[torch.Tensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         decoder_inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1783,12 +1766,12 @@ def __init__(self, config):\n         encoder_config = copy.deepcopy(config)\n         encoder_config.is_decoder = False\n         encoder_config.use_cache = False\n-        encoder_config.is_encoder_decoder = False\n+        encoder_config.tie_encoder_decoder = False\n         self.encoder = UMT5Stack(encoder_config, self.shared)\n \n         decoder_config = copy.deepcopy(config)\n         decoder_config.is_decoder = True\n-        decoder_config.is_encoder_decoder = False\n+        decoder_config.tie_encoder_decoder = False\n         decoder_config.num_layers = config.num_decoder_layers\n         self.decoder = UMT5Stack(decoder_config, self.shared)\n "
        },
        {
            "sha": "4fa05080ad426165ace8d1e81a7e2ce7eeb0ee87",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -22,6 +22,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_outputs import ModelOutput\n@@ -43,7 +44,7 @@\n )\n class VideoLlavaModelOutputWithPast(ModelOutput):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -77,7 +78,7 @@ class VideoLlavaCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -291,7 +292,7 @@ def forward(\n         pixel_values_videos: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,\n@@ -481,7 +482,7 @@ def forward(\n         pixel_values_videos: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layer: Optional[Union[int, list[int]]] = None,\n         vision_feature_select_strategy: Optional[str] = None,"
        },
        {
            "sha": "2826e7449ea217785e64d04d01d8418c008e3157",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -26,6 +26,7 @@\n from torch import nn\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...generation import GenerationMixin\n from ...modeling_outputs import BaseModelOutputWithPast, ModelOutput\n from ...modeling_utils import PreTrainedModel\n@@ -42,7 +43,7 @@\n )\n class VipLlavaModelOutputWithPast(BaseModelOutputWithPast):\n     r\"\"\"\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -68,7 +69,7 @@ class VipLlavaCausalLMOutputWithPast(ModelOutput):\n         Language modeling loss (for next-token prediction).\n     logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, config.vocab_size)`):\n         Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n-    past_key_values (`tuple(tuple(torch.FloatTensor))`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n+    past_key_values (`Cache`, *optional*, returned when `use_cache=True` is passed or when `config.use_cache=True`):\n         Tuple of `tuple(torch.FloatTensor)` of length `config.n_layers`, with each tuple having 2 tensors of shape\n         `(batch_size, num_heads, sequence_length, embed_size_per_head)`)\n \n@@ -207,7 +208,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layers: Optional[Union[int, list[int]]] = None,\n         use_cache: Optional[bool] = None,\n@@ -348,7 +349,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layers: Optional[Union[int, list[int]]] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "74cefba7a998860774d91f5f4f1d09f72e349a41",
            "filename": "src/transformers/models/vipllava/modular_vipllava.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodular_vipllava.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -27,6 +27,7 @@\n )\n \n from ...activations import ACT2FN\n+from ...cache_utils import Cache\n from ...utils import auto_docstring, is_torchdynamo_compiling, logging\n from .configuration_vipllava import VipLlavaConfig\n \n@@ -109,7 +110,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layers: Optional[Union[int, list[int]]] = None,\n         use_cache: Optional[bool] = None,\n@@ -198,7 +199,7 @@ def forward(\n         pixel_values: torch.FloatTensor = None,\n         attention_mask: Optional[torch.Tensor] = None,\n         position_ids: Optional[torch.LongTensor] = None,\n-        past_key_values: Optional[list[torch.FloatTensor]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         vision_feature_layers: Optional[Union[int, list[int]]] = None,\n         labels: Optional[torch.LongTensor] = None,"
        },
        {
            "sha": "8a5320708f1613fde253fa6b278caefeab509d0e",
            "filename": "src/transformers/models/vision_text_dual_encoder/processing_vision_text_dual_encoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvision_text_dual_encoder%2Fprocessing_vision_text_dual_encoder.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -20,7 +20,7 @@\n from typing import Optional, Union\n \n from ...image_utils import ImageInput\n-from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack, _validate_images_text_input_order\n+from ...processing_utils import ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n \n \n@@ -111,8 +111,6 @@ def __call__(\n \n         if text is None and images is None:\n             raise ValueError(\"You have to specify either text or images. Both cannot be none.\")\n-        # check if images and text inputs are reversed for BC\n-        images, text = _validate_images_text_input_order(images, text)\n \n         output_kwargs = self._merge_kwargs(\n             VisionTextDualEncoderProcessorKwargs,"
        },
        {
            "sha": "f8473cda9fe0764f56e2ddfe7c704341404205b5",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 10,
            "deletions": 22,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -316,7 +316,8 @@ def forward(\n         query_states = query_states.view(*q_input_shape)\n         query_states = query_states.transpose(1, 2).contiguous()\n \n-        if past_key_value is not None:\n+        # Check is encoder-decoder model is being used. Otherwise we'll get `DynamicCache`\n+        if past_key_value is not None and isinstance(past_key_value, EncoderDecoderCache):\n             is_updated = past_key_value.is_updated.get(self.layer_idx)\n             if is_cross_attention:\n                 # after the first generated id, we can subsequently re-use all key/value_states from cache\n@@ -881,20 +882,11 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.embed_tokens(input_ids)\n \n-        return_legacy_cache = False\n-        return_self_attention_cache = False\n-        if use_cache or past_key_values is not None:\n-            if isinstance(past_key_values, Cache) and not isinstance(past_key_values, EncoderDecoderCache):\n-                return_self_attention_cache = True\n-                past_key_values = EncoderDecoderCache(past_key_values, DynamicCache())\n-            elif not isinstance(past_key_values, EncoderDecoderCache):\n-                return_legacy_cache = True\n-                logger.warning_once(\n-                    \"Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. \"\n-                    \"You should pass an instance of `EncoderDecoderCache` instead, e.g. \"\n-                    \"`past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\"\n-                )\n-                past_key_values = EncoderDecoderCache.from_legacy_cache(past_key_values)\n+        if use_cache and past_key_values is None:\n+            if self.config.is_encoder_decoder:\n+                past_key_values = EncoderDecoderCache(DynamicCache(), DynamicCache())\n+            else:\n+                past_key_values = DynamicCache()\n \n         past_key_values_length = 0\n         if cache_position is not None:\n@@ -984,10 +976,6 @@ def forward(\n             all_hidden_states += (hidden_states,)\n \n         next_cache = past_key_values if use_cache else None\n-        if return_self_attention_cache:\n-            next_cache = past_key_values.self_attention_cache\n-        if return_legacy_cache:\n-            next_cache = past_key_values.to_legacy_cache()\n         if not return_dict:\n             return tuple(\n                 v\n@@ -1086,7 +1074,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[Union[EncoderDecoderCache, tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache]] = None,\n         decoder_inputs_embeds: Optional[tuple[torch.FloatTensor]] = None,\n         decoder_position_ids: Optional[tuple[torch.LongTensor]] = None,\n         use_cache: Optional[bool] = None,\n@@ -1256,7 +1244,7 @@ def forward(\n         decoder_head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n         encoder_outputs: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n-        past_key_values: Optional[Union[EncoderDecoderCache, tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Union[Cache]] = None,\n         decoder_inputs_embeds: Optional[tuple[torch.FloatTensor]] = None,\n         decoder_position_ids: Optional[tuple[torch.LongTensor]] = None,\n         labels: Optional[torch.LongTensor] = None,\n@@ -1453,7 +1441,7 @@ def forward(\n         encoder_outputs: Optional[tuple[torch.FloatTensor]] = None,\n         head_mask: Optional[torch.Tensor] = None,\n         cross_attn_head_mask: Optional[torch.Tensor] = None,\n-        past_key_values: Optional[tuple[tuple[torch.FloatTensor]]] = None,\n+        past_key_values: Optional[Cache] = None,\n         inputs_embeds: Optional[torch.FloatTensor] = None,\n         labels: Optional[torch.LongTensor] = None,\n         use_cache: Optional[bool] = None,"
        },
        {
            "sha": "2789e508f6fa7e4c79de9abefd0da150ec1aeb2e",
            "filename": "src/transformers/models/zamba2/configuration_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fconfiguration_zamba2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -125,6 +125,7 @@ class Zamba2Config(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"zamba2\"\n+    attribute_map = {\"head_dim\": \"attention_head_dim\"}\n     keys_to_ignore_at_inference = [\"past_key_values\"]\n \n     def __init__("
        },
        {
            "sha": "5731d56643b03fed171c57febd5ae7337dc6fbc7",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -206,11 +206,7 @@ def reset(self):\n \n \n class Zamba2RotaryEmbedding(nn.Module):\n-    def __init__(\n-        self,\n-        config: Zamba2Config,\n-        device=None,\n-    ):\n+    def __init__(self, config: Zamba2Config, device=None):\n         super().__init__()\n         # BC: \"rope_type\" was originally \"type\"\n         if hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n@@ -222,10 +218,8 @@ def __init__(\n \n         self.config = config\n         self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n-        # we cannot use the config here to parameterize because of a factor 2 for the head_dim\n-        inv_freq, self.attention_scaling = self.rope_init_fn(\n-            device=device, base=config.rope_theta, dim=config.attention_head_dim\n-        )\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n         self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n         self.original_inv_freq = self.inv_freq\n "
        },
        {
            "sha": "a980d35828d0ea21618db4542362c3c742da1bad",
            "filename": "src/transformers/models/zamba2/modular_zamba2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 10,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodular_zamba2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -167,16 +167,7 @@ def get_seq_length(self, layer_idx: Optional[int] = 0) -> int:\n \n \n class Zamba2RotaryEmbedding(LlamaRotaryEmbedding):\n-    def __init__(\n-        self,\n-        config: Zamba2Config,\n-        device=None,\n-    ):\n-        super().__init__(config, device)\n-        # we cannot use the config here to parameterize because of a factor 2 for the head_dim\n-        inv_freq, self.attention_scaling = self.rope_init_fn(\n-            device=device, base=config.rope_theta, dim=config.attention_head_dim\n-        )\n+    pass\n \n \n class Zamba2Attention(ZambaAttention):"
        },
        {
            "sha": "cb58e5585b75886f02c43fe559886867e4406ef4",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 117,
            "changes": 118,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -33,7 +33,7 @@\n from .audio_utils import load_audio\n from .dynamic_module_utils import custom_object_save\n from .feature_extraction_utils import BatchFeature\n-from .image_utils import ChannelDimension, is_valid_image, is_vision_available, load_image\n+from .image_utils import ChannelDimension, is_vision_available, load_image\n from .utils.chat_template_utils import render_jinja_template\n from .video_utils import VideoMetadata, load_video\n \n@@ -1413,64 +1413,6 @@ def validate_init_kwargs(processor_config, valid_kwargs):\n \n         return unused_kwargs, valid_kwargs\n \n-    def prepare_and_validate_optional_call_args(self, *args):\n-        \"\"\"\n-        Matches optional positional arguments to their corresponding names in `optional_call_args`\n-        in the processor class in the order they are passed to the processor call.\n-\n-        Note that this should only be used in the `__call__` method of the processors with special\n-        arguments. Special arguments are arguments that aren't `text`, `images`, `audio`, nor `videos`\n-        but also aren't passed to the tokenizer, image processor, etc. Examples of such processors are:\n-            - `CLIPSegProcessor`\n-            - `LayoutLMv2Processor`\n-            - `OwlViTProcessor`\n-\n-        Also note that passing by position to the processor call is now deprecated and will be disallowed\n-        in future versions. We only have this for backward compatibility.\n-\n-        Example:\n-            Suppose that the processor class has `optional_call_args = [\"arg_name_1\", \"arg_name_2\"]`.\n-            And we define the call method as:\n-            ```python\n-            def __call__(\n-                self,\n-                text: str,\n-                images: Optional[ImageInput] = None,\n-                *arg,\n-                audio=None,\n-                videos=None,\n-            )\n-            ```\n-\n-            Then, if we call the processor as:\n-            ```python\n-            images = [...]\n-            processor(\"What is common in these images?\", images, arg_value_1, arg_value_2)\n-            ```\n-\n-            Then, this method will return:\n-            ```python\n-            {\n-                \"arg_name_1\": arg_value_1,\n-                \"arg_name_2\": arg_value_2,\n-            }\n-            ```\n-            which we could then pass as kwargs to `self._merge_kwargs`\n-        \"\"\"\n-        if len(args):\n-            warnings.warn(\n-                \"Passing positional arguments to the processor call is now deprecated and will be disallowed in v4.47. \"\n-                \"Please pass all arguments as keyword arguments.\"\n-            )\n-        if len(args) > len(self.optional_call_args):\n-            raise ValueError(\n-                f\"Expected *at most* {len(self.optional_call_args)} optional positional arguments in processor call\"\n-                f\"which will be matched with {' '.join(self.optional_call_args)} in the order they are passed.\"\n-                f\"However, got {len(args)} positional arguments instead.\"\n-                \"Please pass all arguments as keyword arguments instead (e.g. `processor(arg_name_1=..., arg_name_2=...))`.\"\n-            )\n-        return {arg_name: arg_value for arg_value, arg_name in zip(args, self.optional_call_args)}\n-\n     @deprecate_kwarg(\"video_fps\", version=\"4.58\", new_name=\"fps\")\n     def apply_chat_template(\n         self,\n@@ -1721,64 +1663,6 @@ def _check_special_mm_tokens(self, text: list[str], text_inputs: \"BatchFeature\",\n                 )\n \n \n-def _validate_images_text_input_order(images, text):\n-    \"\"\"\n-    For backward compatibility: reverse the order of `images` and `text` inputs if they are swapped.\n-    This method should only be called for processors where `images` and `text` have been swapped for uniformization purposes.\n-    Note that this method assumes that two `None` inputs are valid inputs. If this is not the case, it should be handled\n-    in the processor's `__call__` method before calling this method.\n-    \"\"\"\n-\n-    def is_url(val) -> bool:\n-        return isinstance(val, str) and val.startswith(\"http\")\n-\n-    def _is_valid_images_input_for_processor(imgs):\n-        # If we have an list of images, make sure every image is valid\n-        if isinstance(imgs, (list, tuple)):\n-            for img in imgs:\n-                if not _is_valid_images_input_for_processor(img):\n-                    return False\n-        # If not a list or tuple, we have been given a single image or batched tensor of images\n-        elif not (is_valid_image(imgs) or is_url(imgs)):\n-            return False\n-        return True\n-\n-    def _is_valid_text_input_for_processor(t):\n-        if isinstance(t, str):\n-            # Strings are fine\n-            return True\n-        elif isinstance(t, (list, tuple)):\n-            # List are fine as long as they are...\n-            if len(t) == 0:\n-                # ... not empty\n-                return False\n-            for t_s in t:\n-                return _is_valid_text_input_for_processor(t_s)\n-        return False\n-\n-    def _is_valid(input, validator):\n-        return validator(input) or input is None\n-\n-    images_is_valid = _is_valid(images, _is_valid_images_input_for_processor)\n-    images_is_text = _is_valid_text_input_for_processor(images)\n-\n-    text_is_valid = _is_valid(text, _is_valid_text_input_for_processor)\n-    text_is_images = _is_valid_images_input_for_processor(text)\n-    # Handle cases where both inputs are valid\n-    if images_is_valid and text_is_valid:\n-        return images, text\n-\n-    # Handle cases where inputs need to and can be swapped\n-    if (images is None and text_is_images) or (text is None and images_is_text) or (images_is_text and text_is_images):\n-        logger.warning_once(\n-            \"You may have used the wrong order for inputs. `images` should be passed before `text`. \"\n-            \"The `images` and `text` inputs will be swapped. This behavior will be deprecated in transformers v4.47.\"\n-        )\n-        return text, images\n-\n-    raise ValueError(\"Invalid input type. Check that `images` and/or `text` are valid inputs.\")\n-\n-\n ProcessorMixin.push_to_hub = copy_func(ProcessorMixin.push_to_hub)\n if ProcessorMixin.push_to_hub.__doc__ is not None:\n     ProcessorMixin.push_to_hub.__doc__ = ProcessorMixin.push_to_hub.__doc__.format("
        },
        {
            "sha": "55dbe51e2a5c9e0887e69c34e81c50473947d4ea",
            "filename": "tests/models/owlv2/test_processor_owlv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/tests%2Fmodels%2Fowlv2%2Ftest_processor_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/tests%2Fmodels%2Fowlv2%2Ftest_processor_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_processor_owlv2.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -2,8 +2,6 @@\n import tempfile\n import unittest\n \n-import pytest\n-\n from transformers import Owlv2Processor\n from transformers.testing_utils import require_scipy\n \n@@ -23,18 +21,3 @@ def setUpClass(cls):\n     @classmethod\n     def tearDownClass(cls):\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n-\n-    def test_processor_query_images_positional(self):\n-        processor_components = self.prepare_components()\n-        processor = Owlv2Processor(**processor_components)\n-\n-        image_input = self.prepare_image_inputs()\n-        query_images = self.prepare_image_inputs()\n-\n-        inputs = processor(None, image_input, query_images)\n-\n-        self.assertListEqual(list(inputs.keys()), [\"query_pixel_values\", \"pixel_values\"])\n-\n-        # test if it raises when no input is passed\n-        with pytest.raises(ValueError):\n-            processor()"
        },
        {
            "sha": "f31dbaf9fbccab36b897f9d6cc86015b94822e08",
            "filename": "tests/models/owlvit/test_processor_owlvit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 15,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/tests%2Fmodels%2Fowlvit%2Ftest_processor_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/tests%2Fmodels%2Fowlvit%2Ftest_processor_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_processor_owlvit.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -232,21 +232,6 @@ def test_processor_case2(self):\n         with pytest.raises(ValueError):\n             processor()\n \n-    def test_processor_query_images_positional(self):\n-        processor_components = self.prepare_components()\n-        processor = OwlViTProcessor(**processor_components)\n-\n-        image_input = self.prepare_image_inputs()\n-        query_images = self.prepare_image_inputs()\n-\n-        inputs = processor(None, image_input, query_images)\n-\n-        self.assertListEqual(list(inputs.keys()), [\"query_pixel_values\", \"pixel_values\"])\n-\n-        # test if it raises when no input is passed\n-        with pytest.raises(ValueError):\n-            processor()\n-\n     def test_tokenizer_decode(self):\n         image_processor = self.get_image_processor()\n         tokenizer = self.get_tokenizer()"
        },
        {
            "sha": "9427b3771b4a2a7e881e819fd6c0216c603435ea",
            "filename": "tests/utils/test_modeling_rope_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 52,
            "changes": 52,
            "blob_url": "https://github.com/huggingface/transformers/blob/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2/tests%2Futils%2Ftest_modeling_rope_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_rope_utils.py?ref=bc161d5d06ba3b40eaca99ce42734b4f1fecfaa2",
            "patch": "@@ -77,58 +77,6 @@ def test_rope_validation(self):\n                     self.assertEqual(len(logs.output), 1)\n                     self.assertIn(model_specific_kwarg, logs.output[0])\n \n-    def test_default_rope_function_bc(self):\n-        config = LlamaConfig()\n-        device = torch_device\n-\n-        rope_kwargs = {\n-            \"rope_type\": \"default\",\n-            \"dim\": config.hidden_size // config.num_attention_heads,\n-            \"max_position_embeddings\": config.max_position_embeddings,\n-            \"base\": config.rope_theta,\n-        }\n-\n-        rope_fn = ROPE_INIT_FUNCTIONS[\"default\"]\n-        config_freqs = rope_fn(config=config, device=device)[0]\n-        kwargs_freqs = rope_fn(**rope_kwargs, device=device)[0]\n-        torch.testing.assert_close(config_freqs, kwargs_freqs)\n-\n-    def test_linear_rope_function_bc(self):\n-        config = LlamaConfig()\n-        config.rope_scaling = {\"rope_type\": \"linear\", \"factor\": 10.0}\n-        device = torch_device\n-\n-        rope_kwargs = {\n-            \"rope_type\": \"linear\",\n-            \"dim\": config.hidden_size // config.num_attention_heads,\n-            \"max_position_embeddings\": config.max_position_embeddings,\n-            \"base\": config.rope_theta,\n-            \"factor\": 10.0,\n-        }\n-\n-        rope_fn = ROPE_INIT_FUNCTIONS[\"linear\"]\n-        config_freqs = rope_fn(config=config, device=device)[0]\n-        kwargs_freqs = rope_fn(**rope_kwargs, device=device)[0]\n-        torch.testing.assert_close(config_freqs, kwargs_freqs)\n-\n-    def test_dynamic_rope_function_bc(self):\n-        config = LlamaConfig()\n-        config.rope_scaling = {\"rope_type\": \"dynamic\", \"factor\": 10.0}\n-        device = torch_device\n-\n-        rope_kwargs = {\n-            \"rope_type\": \"dynamic\",\n-            \"dim\": config.hidden_size // config.num_attention_heads,\n-            \"max_position_embeddings\": config.max_position_embeddings,\n-            \"base\": config.rope_theta,\n-            \"factor\": 10.0,\n-        }\n-\n-        rope_fn = ROPE_INIT_FUNCTIONS[\"dynamic\"]\n-        config_freqs = rope_fn(config=config, device=device)[0]\n-        kwargs_freqs = rope_fn(**rope_kwargs, device=device)[0]\n-        torch.testing.assert_close(config_freqs, kwargs_freqs)\n-\n     def test_default_rope_numerically(self):\n         # Note: some RoPE scaling methods start off by calling the default RoPE frequencies. If this test fails, then\n         # multiple RoPE strategies will fail."
        },
        {
            "sha": "7b32b534a70e3d125451cf8c9f5b838f634c8cf5",
            "filename": "tests/utils/test_processing_utils.py",
            "status": "removed",
            "additions": 0,
            "deletions": 175,
            "changes": 175,
            "blob_url": "https://github.com/huggingface/transformers/blob/c6ee0b1da8ff57102548430e18480fa78a106022/tests%2Futils%2Ftest_processing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c6ee0b1da8ff57102548430e18480fa78a106022/tests%2Futils%2Ftest_processing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_processing_utils.py?ref=c6ee0b1da8ff57102548430e18480fa78a106022",
            "patch": "@@ -1,175 +0,0 @@\n-# Copyright 2024 HuggingFace Inc.\n-#\n-# Licensed under the Apache License, Version 2.0 (the \"License\");\n-# you may not use this file except in compliance with the License.\n-# You may obtain a copy of the License at\n-#\n-#     http://www.apache.org/licenses/LICENSE-2.0\n-#\n-# Unless required by applicable law or agreed to in writing, software\n-# distributed under the License is distributed on an \"AS IS\" BASIS,\n-# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n-# See the License for the specific language governing permissions and\n-# limitations under the License.\n-\n-import unittest\n-\n-import numpy as np\n-\n-from transformers import is_torch_available, is_vision_available\n-from transformers.processing_utils import _validate_images_text_input_order\n-from transformers.testing_utils import require_torch, require_vision\n-\n-\n-if is_vision_available():\n-    import PIL\n-\n-if is_torch_available():\n-    import torch\n-\n-\n-@require_vision\n-class ProcessingUtilTester(unittest.TestCase):\n-    def test_validate_images_text_input_order(self):\n-        # text string and PIL images inputs\n-        images = PIL.Image.new(\"RGB\", (224, 224))\n-        text = \"text\"\n-        # test correct text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n-        self.assertEqual(valid_images, images)\n-        self.assertEqual(valid_text, text)\n-        # test incorrect text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n-        self.assertEqual(valid_images, images)\n-        self.assertEqual(valid_text, text)\n-\n-        # text list of string and numpy images inputs\n-        images = np.random.rand(224, 224, 3)\n-        text = [\"text1\", \"text2\"]\n-        # test correct text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n-        self.assertTrue(np.array_equal(valid_images, images))\n-        self.assertEqual(valid_text, text)\n-        # test incorrect text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n-        self.assertTrue(np.array_equal(valid_images, images))\n-        self.assertEqual(valid_text, text)\n-\n-        # text nested list of string and list of pil images inputs\n-        images = [PIL.Image.new(\"RGB\", (224, 224)), PIL.Image.new(\"RGB\", (224, 224))]\n-        text = [[\"text1\", \"text2, text3\"], [\"text3\", \"text4\"]]\n-        # test correct text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n-        self.assertEqual(valid_images, images)\n-        self.assertEqual(valid_text, text)\n-        # test incorrect text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n-        self.assertEqual(valid_images, images)\n-        self.assertEqual(valid_text, text)\n-\n-        # list of strings and list of numpy images inputs\n-        images = [np.random.rand(224, 224, 3), np.random.rand(224, 224, 3)]\n-        text = [\"text1\", \"text2\"]\n-        # test correct text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n-        self.assertTrue(np.array_equal(valid_images[0], images[0]))\n-        self.assertEqual(valid_text, text)\n-        # test incorrect text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n-        self.assertTrue(np.array_equal(valid_images[0], images[0]))\n-        self.assertEqual(valid_text, text)\n-\n-        # list of strings and list of url images inputs\n-        images = [\"https://url1\", \"https://url2\"]\n-        text = [\"text1\", \"text2\"]\n-        # test correct text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n-        self.assertEqual(valid_images, images)\n-        self.assertEqual(valid_text, text)\n-        # test incorrect text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n-        self.assertEqual(valid_images, images)\n-        self.assertEqual(valid_text, text)\n-\n-        # list of strings and nested list of numpy images inputs\n-        images = [[np.random.rand(224, 224, 3), np.random.rand(224, 224, 3)], [np.random.rand(224, 224, 3)]]\n-        text = [\"text1\", \"text2\"]\n-        # test correct text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n-        self.assertTrue(np.array_equal(valid_images[0][0], images[0][0]))\n-        self.assertEqual(valid_text, text)\n-        # test incorrect text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n-        self.assertTrue(np.array_equal(valid_images[0][0], images[0][0]))\n-        self.assertEqual(valid_text, text)\n-\n-        # nested list of strings and nested list of PIL images inputs\n-        images = [\n-            [PIL.Image.new(\"RGB\", (224, 224)), PIL.Image.new(\"RGB\", (224, 224))],\n-            [PIL.Image.new(\"RGB\", (224, 224))],\n-        ]\n-        text = [[\"text1\", \"text2, text3\"], [\"text3\", \"text4\"]]\n-        # test correct text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n-        self.assertEqual(valid_images, images)\n-        self.assertEqual(valid_text, text)\n-        # test incorrect text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n-        self.assertEqual(valid_images, images)\n-        self.assertEqual(valid_text, text)\n-\n-        # None images\n-        images = None\n-        text = \"text\"\n-        # test correct text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n-        self.assertEqual(images, None)\n-        self.assertEqual(text, text)\n-        # test incorrect text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n-        self.assertEqual(images, None)\n-        self.assertEqual(text, text)\n-\n-        # None text\n-        images = PIL.Image.new(\"RGB\", (224, 224))\n-        text = None\n-        # test correct text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n-        self.assertEqual(images, images)\n-        self.assertEqual(text, None)\n-        # test incorrect text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n-        self.assertEqual(images, images)\n-        self.assertEqual(text, None)\n-\n-        # incorrect inputs\n-        images = \"text\"\n-        text = \"text\"\n-        with self.assertRaises(ValueError):\n-            _validate_images_text_input_order(images=images, text=text)\n-\n-    @require_torch\n-    def test_validate_images_text_input_order_torch(self):\n-        # text string and torch images inputs\n-        images = torch.rand(224, 224, 3)\n-        text = \"text\"\n-        # test correct text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n-        self.assertTrue(torch.equal(valid_images, images))\n-        self.assertEqual(valid_text, text)\n-        # test incorrect text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n-        self.assertTrue(torch.equal(valid_images, images))\n-        self.assertEqual(valid_text, text)\n-\n-        # text list of string and list of torch images inputs\n-        images = [torch.rand(224, 224, 3), torch.rand(224, 224, 3)]\n-        text = [\"text1\", \"text2\"]\n-        # test correct text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=images, text=text)\n-        self.assertTrue(torch.equal(valid_images[0], images[0]))\n-        self.assertEqual(valid_text, text)\n-        # test incorrect text and images order\n-        valid_images, valid_text = _validate_images_text_input_order(images=text, text=images)\n-        self.assertTrue(torch.equal(valid_images[0], images[0]))\n-        self.assertEqual(valid_text, text)"
        }
    ],
    "stats": {
        "total": 3078,
        "additions": 914,
        "deletions": 2164
    }
}