{
    "author": "h3110Fr13nd",
    "message": "[docs] update input documentation for MAMBA2 and MISTRAL models to include cache_position and attention_mask details (#34322)\n\n* [docs] update input documentation for MAMBA2 and MISTRAL models to include cache_position and attention_mask details\r\n\r\n* [docs] correct input documentation for MISTRAL model to reference `input_ids` instead of `decoder_input_ids`\r\n\r\n* [docs] clarify cache_position description in MISTRAL model documentation",
    "sha": "fc1ae7f30f1d16c7652c28dd8d91c5d8a8ed2f15",
    "files": [
        {
            "sha": "c312b9b94351d2a8d7ee4789fd94f7d3921e980c",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 0,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/fc1ae7f30f1d16c7652c28dd8d91c5d8a8ed2f15/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fc1ae7f30f1d16c7652c28dd8d91c5d8a8ed2f15/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=fc1ae7f30f1d16c7652c28dd8d91c5d8a8ed2f15",
            "patch": "@@ -805,6 +805,16 @@ class Mamba2CausalLMOutput(ModelOutput):\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n+            The position of the current input in the cache. This is used to ensure that the cache is correctly updated.\n+            If `cache_params` is passed, `cache_position` should also be passed.\n+        attention_mask (`torch.FloatTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Mask to avoid performing attention on padding token indices. Mask values selected in `[0, 1]`:\n+\n+            - 1 for tokens that are **not masked**,\n+            - 0 for tokens that are **masked**.\n+\n+            [What are attention masks?](../glossary#attention-mask)\n \"\"\"\n \n "
        },
        {
            "sha": "3b0fb75a4cb3ba5aab2c7de82bd883fb9b4c61e0",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/fc1ae7f30f1d16c7652c28dd8d91c5d8a8ed2f15/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fc1ae7f30f1d16c7652c28dd8d91c5d8a8ed2f15/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=fc1ae7f30f1d16c7652c28dd8d91c5d8a8ed2f15",
            "patch": "@@ -619,7 +619,7 @@ def _init_weights(self, module):\n             Indices can be obtained using [`AutoTokenizer`]. See [`PreTrainedTokenizer.encode`] and\n             [`PreTrainedTokenizer.__call__`] for details.\n \n-            If `past_key_values` is used, optionally only the last `decoder_input_ids` have to be input (see\n+            If `past_key_values` is used, optionally only the last `input_ids` have to be input (see\n             `past_key_values`).\n \n             If you want to change padding behavior, you should read [`modeling_opt._prepare_decoder_attention_mask`]\n@@ -666,6 +666,10 @@ def _init_weights(self, module):\n             more detail.\n         return_dict (`bool`, *optional*):\n             Whether or not to return a [`~utils.ModelOutput`] instead of a plain tuple.\n+        cache_position (`torch.LongTensor` of shape `(sequence_length)`, *optional*):\n+            Indices indicating the position of the input sequence tokens in the sequence. Unlike `position_ids`,\n+            this tensor is not affected by padding. It is used to update the cache in the correct position and to infer\n+            the complete sequence length.\n \"\"\"\n \n "
        }
    ],
    "stats": {
        "total": 16,
        "additions": 15,
        "deletions": 1
    }
}