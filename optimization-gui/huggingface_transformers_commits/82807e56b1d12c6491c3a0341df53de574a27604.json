{
    "author": "yonigozlan",
    "message": "[Fast image processor] refactor fast image processor glm4v (#39490)\n\nrefactor fast image processor glm4v",
    "sha": "82807e56b1d12c6491c3a0341df53de574a27604",
    "files": [
        {
            "sha": "a509be55a84ce1270bd398290af23b883edc5a12",
            "filename": "src/transformers/models/glm4v/image_processing_glm4v_fast.py",
            "status": "modified",
            "additions": 49,
            "deletions": 219,
            "changes": 268,
            "blob_url": "https://github.com/huggingface/transformers/blob/82807e56b1d12c6491c3a0341df53de574a27604/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82807e56b1d12c6491c3a0341df53de574a27604/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fimage_processing_glm4v_fast.py?ref=82807e56b1d12c6491c3a0341df53de574a27604",
            "patch": "@@ -28,13 +28,9 @@\n from ...image_utils import (\n     OPENAI_CLIP_MEAN,\n     OPENAI_CLIP_STD,\n-    ChannelDimension,\n     ImageInput,\n     PILImageResampling,\n     SizeDict,\n-    get_image_size,\n-    make_flat_list_of_images,\n-    valid_images,\n )\n from ...processing_utils import Unpack\n from ...utils import (\n@@ -45,7 +41,6 @@\n     is_torchvision_v2_available,\n     logging,\n )\n-from ...video_utils import VideoInput\n from .image_processing_glm4v import smart_resize\n \n \n@@ -54,8 +49,6 @@\n \n \n if is_torchvision_available():\n-    from ...image_utils import pil_torch_interpolation_mapping\n-\n     if is_torchvision_v2_available():\n         from torchvision.transforms.v2 import functional as F\n     else:\n@@ -96,19 +89,12 @@ class Glm4vImageProcessorFast(BaseImageProcessorFast):\n     model_input_names = [\"pixel_values\", \"image_grid_thw\"]\n \n     def __init__(self, **kwargs: Unpack[Glm4vFastImageProcessorKwargs]):\n-        size = kwargs.pop(\"size\", None)\n-        if size is not None and (\"shortest_edge\" not in size or \"longest_edge\" not in size):\n-            raise ValueError(\"size must contain 'shortest_edge' and 'longest_edge' keys.\")\n-        else:\n-            size = self.size\n-\n-        super().__init__(size=size, **kwargs)\n+        super().__init__(**kwargs)\n \n     def _preprocess(\n         self,\n         images: list[\"torch.Tensor\"],\n         do_resize: bool,\n-        size: SizeDict,\n         interpolation: Optional[\"F.InterpolationMode\"],\n         do_rescale: bool,\n         rescale_factor: float,\n@@ -118,65 +104,19 @@ def _preprocess(\n         patch_size: int,\n         temporal_patch_size: int,\n         merge_size: int,\n-        do_convert_rgb: bool,\n-        input_data_format: Optional[Union[str, ChannelDimension]],\n-        device: Optional[Union[str, torch.device]],\n         disable_grouping: Optional[bool],\n-    ):\n+        return_tensors: Optional[Union[str, TensorType]],\n+        **kwargs,\n+    ) -> BatchFeature:\n         \"\"\"\n         Preprocess an image or batch of images. Copy of the `preprocess` method from `CLIPImageProcessor`.\n-\n-        Args:\n-            images (`ImageInput`):\n-                Image or batch of images to preprocess. Expects pixel values ranging from 0 to 255. If pixel values range from 0 to 1, set `do_rescale=False`.\n-            vision_info (`List[Dict]`, *optional*):\n-                Optional list of dictionaries containing additional information about vision inputs.\n-            do_resize (`bool`, *optional*, defaults to `self.do_resize`):\n-                Whether to resize the image.\n-            size (`Dict[str, int]`, *optional*, defaults to `self.size`):\n-                Size of the image after resizing. `shortest_edge` and `longest_edge` keys must be present.\n-            interpolation (`InterpolationMode`):\n-                Resampling filter to use if resizing the image.\n-            do_rescale (`bool`, *optional*, defaults to `self.do_rescale`):\n-                Whether to rescale the image.\n-            rescale_factor (`float`, *optional*, defaults to `self.rescale_factor`):\n-                Scale factor to use if rescaling the image.\n-            do_normalize (`bool`, *optional*, defaults to `self.do_normalize`):\n-                Whether to normalize the image.\n-            image_mean (`float` or `List[float]`, *optional*, defaults to `self.image_mean`):\n-                Mean to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n-            image_std (`float` or `List[float]`, *optional*, defaults to `self.image_std`):\n-                Standard deviation to use if normalizing the image. Can be a float or a list of floats corresponding to the number of channels in the image.\n-            patch_size (`int`, *optional*, defaults to `self.patch_size`):\n-                The spatial patch size of the vision encoder.\n-            temporal_patch_size (`int`, *optional*, defaults to `self.temporal_patch_size`):\n-                The temporal patch size of the vision encoder.\n-            merge_size (`int`, *optional*, defaults to `self.merge_size`):\n-                The merge size of the vision encoder to llm encoder.\n-            do_convert_rgb (`bool`, *optional*, defaults to `self.do_convert_rgb`):\n-                Whether to convert the image to RGB.\n-            input_data_format (`ChannelDimension` or `str`, *optional*):\n-                The channel dimension format for the input image. Can be one of:\n-                - `\"channels_first\"` or `ChannelDimension.FIRST`: image in (num_channels, height, width) format.\n-                - `\"channels_last\"` or `ChannelDimension.LAST`: image in (height, width, num_channels) format.\n-                - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.   - `\"none\"` or `ChannelDimension.NONE`: image in (height, width) format.\n-            device (`torch.device`, *optional*):\n-                The device to process the images on. If unset, the device is inferred from the input images.\n         \"\"\"\n-        images = self._prepare_input_images(\n-            images=images,\n-            do_convert_rgb=do_convert_rgb,\n-            input_data_format=input_data_format,\n-            device=device,\n-        )\n-\n-        height, width = get_image_size(images[0], channel_dim=ChannelDimension.FIRST)\n-        resized_height, resized_width = height, width\n \n         # Group images by size for batched resizing\n         grouped_images, grouped_images_index = group_images_by_shape(images, disable_grouping=disable_grouping)\n         resized_images_grouped = {}\n         for shape, stacked_images in grouped_images.items():\n+            height, width = stacked_images.shape[-2:]\n             if do_resize:\n                 resized_height, resized_width = smart_resize(\n                     num_frames=temporal_patch_size,\n@@ -185,183 +125,73 @@ def _preprocess(\n                     temporal_factor=temporal_patch_size,\n                     factor=patch_size * merge_size,\n                 )\n-                stacked_images = F.resize(\n-                    stacked_images, size=(resized_height, resized_width), interpolation=interpolation\n+                stacked_images = self.resize(\n+                    stacked_images,\n+                    size=SizeDict(height=resized_height, width=resized_width),\n+                    interpolation=interpolation,\n                 )\n             resized_images_grouped[shape] = stacked_images\n         resized_images = reorder_images(resized_images_grouped, grouped_images_index)\n         # Group images by size for further processing\n         # Needed in case do_resize is False, or resize returns images with different sizes\n         grouped_images, grouped_images_index = group_images_by_shape(resized_images, disable_grouping=disable_grouping)\n         processed_images_grouped = {}\n+        processed_grids = {}\n         for shape, stacked_images in grouped_images.items():\n             # Fused rescale and normalize\n             stacked_images = self.rescale_and_normalize(\n                 stacked_images, do_rescale, rescale_factor, do_normalize, image_mean, image_std\n             )\n-            processed_images_grouped[shape] = stacked_images\n+            # add a temporal dimension\n+            patches = stacked_images.unsqueeze(1)\n+            if patches.shape[1] % temporal_patch_size != 0:\n+                repeats = patches[:, -1:].repeat(1, temporal_patch_size - 1, 1, 1, 1)\n+                patches = torch.cat([patches, repeats], dim=1)\n+            batch_size, grid_t, channel = patches.shape[:3]\n+            grid_t = grid_t // temporal_patch_size\n+            grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+\n+            patches = patches.view(\n+                batch_size,\n+                grid_t,\n+                temporal_patch_size,\n+                channel,\n+                grid_h // merge_size,\n+                merge_size,\n+                patch_size,\n+                grid_w // merge_size,\n+                merge_size,\n+                patch_size,\n+            )\n+            patches = patches.permute(0, 1, 4, 7, 5, 8, 3, 2, 6, 9)\n+            flatten_patches = patches.reshape(\n+                batch_size,\n+                grid_t * grid_h * grid_w,\n+                channel * temporal_patch_size * patch_size * patch_size,\n+            )\n \n-        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n-        patches = torch.stack(processed_images, dim=0)\n-        if patches.shape[0] % temporal_patch_size != 0:\n-            repeats = patches[-1].unsqueeze(0).repeat(temporal_patch_size - 1, 1, 1, 1)\n-            patches = torch.cat([patches, repeats], dim=0)\n+            processed_images_grouped[shape] = flatten_patches\n+            processed_grids[shape] = [[grid_t, grid_h, grid_w]] * batch_size\n \n-        channel = patches.shape[1]\n-        grid_t = patches.shape[0] // temporal_patch_size\n-        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n+        processed_images = reorder_images(processed_images_grouped, grouped_images_index)\n+        processed_grids = reorder_images(processed_grids, grouped_images_index)\n+        pixel_values = torch.stack(processed_images, dim=0)\n+        image_grid_thw = torch.tensor(processed_grids)\n \n-        patches = patches.view(\n-            grid_t,\n-            temporal_patch_size,\n-            channel,\n-            grid_h // merge_size,\n-            merge_size,\n-            patch_size,\n-            grid_w // merge_size,\n-            merge_size,\n-            patch_size,\n-        )\n-        patches = patches.permute(0, 3, 6, 4, 7, 2, 1, 5, 8)\n-        flatten_patches = patches.reshape(\n-            grid_t * grid_h * grid_w, channel * temporal_patch_size * patch_size * patch_size\n+        return BatchFeature(\n+            data={\"pixel_values\": pixel_values, \"image_grid_thw\": image_grid_thw}, tensor_type=return_tensors\n         )\n \n-        return flatten_patches, (grid_t, grid_h, grid_w)\n-\n     @auto_docstring\n     def preprocess(\n         self,\n         images: ImageInput,\n-        videos: VideoInput = None,\n-        do_resize: Optional[bool] = None,\n-        size: Optional[dict[str, int]] = None,\n-        resample: Optional[Union[\"PILImageResampling\", \"F.InterpolationMode\"]] = None,\n-        do_rescale: Optional[bool] = None,\n-        rescale_factor: Optional[float] = None,\n-        do_normalize: Optional[bool] = None,\n-        image_mean: Optional[Union[float, list[float]]] = None,\n-        image_std: Optional[Union[float, list[float]]] = None,\n-        patch_size: Optional[int] = None,\n-        temporal_patch_size: Optional[int] = None,\n-        merge_size: Optional[int] = None,\n-        do_convert_rgb: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        data_format: Optional[ChannelDimension] = ChannelDimension.FIRST,\n-        input_data_format: Optional[Union[str, ChannelDimension]] = None,\n-        device: Optional[\"torch.device\"] = None,\n-        disable_grouping: Optional[bool] = False,\n-        **kwargs,\n-    ):\n-        r\"\"\"\n-        patch_size (`int`, *optional*, defaults to 14):\n-            The spatial patch size of the vision encoder.\n-        temporal_patch_size (`int`, *optional*, defaults to 2):\n-            The temporal patch size of the vision encoder.\n-        merge_size (`int`, *optional*, defaults to 2):\n-            The merge size of the vision encoder to llm encoder.\n+        **kwargs: Unpack[Glm4vFastImageProcessorKwargs],\n+    ) -> BatchFeature:\n         \"\"\"\n-\n-        do_resize = do_resize if do_resize is not None else self.do_resize\n-        size = size if size is not None else self.size\n-        resample = resample if resample is not None else self.resample\n-        do_rescale = do_rescale if do_rescale is not None else self.do_rescale\n-        rescale_factor = rescale_factor if rescale_factor is not None else self.rescale_factor\n-        do_normalize = do_normalize if do_normalize is not None else self.do_normalize\n-        image_mean = image_mean if image_mean is not None else self.image_mean\n-        image_std = image_std if image_std is not None else self.image_std\n-        patch_size = patch_size if patch_size is not None else self.patch_size\n-        temporal_patch_size = temporal_patch_size if temporal_patch_size is not None else self.temporal_patch_size\n-        merge_size = merge_size if merge_size is not None else self.merge_size\n-        do_convert_rgb = do_convert_rgb if do_convert_rgb is not None else self.do_convert_rgb\n-\n-        # Make hashable for cache\n-        size = SizeDict(**size) if size is not None else None\n-        image_mean = tuple(image_mean) if image_mean is not None else None\n-        image_std = tuple(image_std) if image_std is not None else None\n-\n-        self._validate_preprocess_kwargs(\n-            do_rescale=do_rescale,\n-            rescale_factor=rescale_factor,\n-            do_normalize=do_normalize,\n-            image_mean=image_mean,\n-            image_std=image_std,\n-            do_resize=do_resize,\n-            size=size,\n-            resample=resample,\n-            return_tensors=return_tensors,\n-            data_format=data_format,\n-        )\n-        interpolation = (\n-            pil_torch_interpolation_mapping[resample] if isinstance(resample, (PILImageResampling, int)) else resample\n-        )\n-\n-        if images is not None:\n-            images = make_flat_list_of_images(images)\n-\n-        if images is not None and not valid_images(images):\n-            raise ValueError(\n-                \"Invalid image type. Must be of type PIL.Image.Image, numpy.ndarray, \"\n-                \"torch.Tensor, tf.Tensor or jax.ndarray.\"\n-            )\n-\n-        data = {}\n-        if images is not None:\n-            pixel_values, vision_grid_thws = [], []\n-            for image in images:\n-                patches, image_grid_thw = self._preprocess(\n-                    image,\n-                    do_resize=do_resize,\n-                    size=size,\n-                    interpolation=interpolation,\n-                    do_rescale=do_rescale,\n-                    rescale_factor=rescale_factor,\n-                    do_normalize=do_normalize,\n-                    image_mean=image_mean,\n-                    image_std=image_std,\n-                    patch_size=patch_size,\n-                    temporal_patch_size=temporal_patch_size,\n-                    merge_size=merge_size,\n-                    do_convert_rgb=do_convert_rgb,\n-                    input_data_format=input_data_format,\n-                    device=device,\n-                    disable_grouping=disable_grouping,\n-                )\n-                pixel_values.extend(patches)\n-                vision_grid_thws.append(image_grid_thw)\n-            pixel_values = torch.stack(pixel_values)\n-            vision_grid_thws = torch.tensor(vision_grid_thws)\n-            data.update({\"pixel_values\": pixel_values, \"image_grid_thw\": vision_grid_thws})\n-\n-        return BatchFeature(data=data, tensor_type=return_tensors)\n-\n-    def get_number_of_image_patches(self, height: int, width: int, images_kwargs=None):\n+        Preprocess an image or batch of images.\n         \"\"\"\n-        A utility that returns number of image patches for a given image size.\n-\n-        Args:\n-            height (`int`):\n-                Height of the input image.\n-            width (`int`):\n-                Width of the input image.\n-            images_kwargs (`dict`, *optional*)\n-                Any kwargs to override defaults of the image processor.\n-        Returns:\n-            `int`: Number of image patches per image.\n-        \"\"\"\n-        patch_size = images_kwargs.get(\"patch_size\", None) or self.patch_size\n-        merge_size = images_kwargs.get(\"merge_size\", None) or self.merge_size\n-\n-        factor = patch_size * merge_size\n-        resized_height, resized_width = smart_resize(\n-            num_frames=self.temporal_patch_size,\n-            height=height,\n-            width=width,\n-            temporal_factor=self.temporal_patch_size,\n-            factor=factor,\n-        )\n-        grid_h, grid_w = resized_height // patch_size, resized_width // patch_size\n-        return grid_h * grid_w\n+        return super().preprocess(images, **kwargs)\n \n \n __all__ = [\"Glm4vImageProcessorFast\"]"
        },
        {
            "sha": "74d56ac76fbd970ae6d3fccf6f0cc7c591087d75",
            "filename": "src/transformers/models/glm4v/video_processing_glm4v.py",
            "status": "modified",
            "additions": 5,
            "deletions": 6,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/82807e56b1d12c6491c3a0341df53de574a27604/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/82807e56b1d12c6491c3a0341df53de574a27604/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fvideo_processing_glm4v.py?ref=82807e56b1d12c6491c3a0341df53de574a27604",
            "patch": "@@ -53,8 +53,6 @@\n if is_vision_available():\n     from ...image_utils import PILImageResampling\n \n-import torch.nn.functional as F\n-\n \n class Glm4vVideoProcessorInitKwargs(VideosKwargs):\n     max_image_size: dict[str, int] = None\n@@ -145,9 +143,8 @@ def _preprocess(\n         self,\n         videos: list[torch.Tensor],\n         video_metadata: Optional[Union[list[VideoMetadata], list[dict]]] = None,\n-        do_convert_rgb: bool = True,\n         do_resize: bool = True,\n-        size: SizeDict = None,\n+        interpolation: PILImageResampling = PILImageResampling.BICUBIC,\n         do_rescale: bool = True,\n         rescale_factor: float = 1 / 255.0,\n         do_normalize: bool = True,\n@@ -194,8 +191,10 @@ def _preprocess(\n                     max_pixels=self.max_image_size[\"longest_edge\"],\n                 )\n                 stacked_videos = stacked_videos.view(B * T, C, H, W)\n-                stacked_videos = F.interpolate(\n-                    stacked_videos, size=(resized_height, resized_width), mode=\"bicubic\", align_corners=False\n+                stacked_videos = self.resize(\n+                    stacked_videos,\n+                    size=SizeDict(height=resized_height, width=resized_width),\n+                    interpolation=interpolation,\n                 )\n                 stacked_videos = stacked_videos.view(B, T, C, resized_height, resized_width)\n             resized_videos_grouped[shape] = stacked_videos"
        }
    ],
    "stats": {
        "total": 279,
        "additions": 54,
        "deletions": 225
    }
}