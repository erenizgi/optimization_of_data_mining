{
    "author": "MekkCyber",
    "message": "[Quantization] FBgemm FP8 for XPU (#42773)\n\n* enable xpu in fp8_gemm\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\n\n* refine the code\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\n\n* updated\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\n\n* fix\n\n* style\n\n* small fix\n\n---------\n\nSigned-off-by: Wang, Yi <yi.a.wang@intel.com>\nCo-authored-by: Wang, Yi <yi.a.wang@intel.com>",
    "sha": "86644be4790613d9bb21372fd9d10f48fd8d892c",
    "files": [
        {
            "sha": "538c32f5dcce08d3dd6886cb960156a3a96c86a0",
            "filename": "src/transformers/integrations/fbgemm_fp8.py",
            "status": "modified",
            "additions": 88,
            "deletions": 38,
            "changes": 126,
            "blob_url": "https://github.com/huggingface/transformers/blob/86644be4790613d9bb21372fd9d10f48fd8d892c/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/86644be4790613d9bb21372fd9d10f48fd8d892c/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffbgemm_fp8.py?ref=86644be4790613d9bb21372fd9d10f48fd8d892c",
            "patch": "@@ -12,12 +12,19 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n \n+from functools import lru_cache\n from typing import Optional\n \n from ..activations import ACT2FN\n from ..core_model_loading import ConversionOps\n from ..quantizers.quantizers_utils import get_module_from_name, should_convert_module\n-from ..utils import is_accelerate_available, is_fbgemm_gpu_available, is_torch_available, logging\n+from ..utils import (\n+    is_accelerate_available,\n+    is_fbgemm_gpu_available,\n+    is_torch_available,\n+    is_torch_xpu_available,\n+    logging,\n+)\n \n \n if is_torch_available():\n@@ -27,7 +34,9 @@\n if is_accelerate_available():\n     from accelerate import init_empty_weights\n \n-if is_fbgemm_gpu_available():\n+_is_torch_xpu_available = is_torch_xpu_available()\n+\n+if is_fbgemm_gpu_available() and not _is_torch_xpu_available:\n     import fbgemm_gpu.experimental.gen_ai  # noqa: F401\n \n logger = logging.get_logger(__name__)\n@@ -61,7 +70,7 @@ def convert(\n                 flattened_param = transposed_param.reshape(-1, original_shape[-1])\n \n                 # Quantize using per row instead of per column\n-                new_value_flat, weight_scale_flat = torch.ops.fbgemm.quantize_fp8_per_row(flattened_param)\n+                new_value_flat, weight_scale_flat = quantize_fp8_per_row(flattened_param)\n \n                 # Reshape back to original dimensions\n                 new_value = new_value_flat.reshape(original_shape)\n@@ -77,14 +86,14 @@ def convert(\n                 flattened_param = transposed_param.reshape(-1, original_shape[-1])\n \n                 # Quantize using per column\n-                new_value_flat, weight_scale_flat = torch.ops.fbgemm.quantize_fp8_per_row(flattened_param)\n+                new_value_flat, weight_scale_flat = quantize_fp8_per_row(flattened_param)\n \n                 # Reshape back to original dimensions\n                 new_value = new_value_flat.reshape(original_shape)\n                 new_value = new_value.transpose(1, 2)\n                 weight_scale = weight_scale_flat.reshape(original_shape[0], original_shape[1], 1)\n         else:\n-            new_value, weight_scale = torch.ops.fbgemm.quantize_fp8_per_row(value)\n+            new_value, weight_scale = quantize_fp8_per_row(value)\n             weight_scale = torch.nn.Parameter(weight_scale.view(weight_scale.shape[0], 1))\n \n         return {target_key: torch.nn.Parameter(new_value), f\"{target_key}_scale\": weight_scale}\n@@ -110,18 +119,26 @@ def forward(self, x):\n         output_shape = (*x.shape[:-1], -1)\n         # x_quantized and x_scale are not necessarily on the same device as x, this is an issue.\n         # https://github.com/pytorch/FBGEMM/blob/e08af8539c391437f447173863df0f3f6f6f1855/fbgemm_gpu/experimental/gen_ai/src/quantize/quantize.cu#L1237C3-L1237C45\n-        x_quantized, x_scale = torch.ops.fbgemm.quantize_fp8_per_row(\n-            x.view(-1, x.shape[-1]).contiguous(), scale_ub=self.input_scale_ub\n-        )\n+        x_quantized, x_scale = quantize_fp8_per_row(x.view(-1, x.shape[-1]).contiguous(), scale_ub=self.input_scale_ub)\n         # moving x_quantized, x_scale here creates glibberish output ... However, if we move the output, it works\n         # x_quantized, x_scale = x_quantized.to(x.device), x_scale.to(x.device)\n \n         # The computation still happens on the device where self.weight is even if x_quantized is not on the same device as self.weight\n         weight_scale_float32 = self.weight_scale.to(torch.float32)\n-        output = torch.ops.fbgemm.f8f8bf16_rowwise(\n-            x_quantized, self.weight, x_scale, weight_scale_float32, use_fast_accum=True\n-        )\n-        output = output + self.bias if self.bias is not None else output\n+        if _is_torch_xpu_available:\n+            output = torch._scaled_mm(\n+                x_quantized,\n+                self.weight.t(),\n+                scale_a=x_scale.unsqueeze(-1),\n+                scale_b=weight_scale_float32.t(),\n+                out_dtype=x.dtype,\n+                bias=self.bias,\n+            )\n+        else:\n+            output = torch.ops.fbgemm.f8f8bf16_rowwise(\n+                x_quantized, self.weight, x_scale, weight_scale_float32, use_fast_accum=True\n+            )\n+            output = output + self.bias if self.bias is not None else output\n         # Hacky for now, we have the output to the device of x\n         output = output.to(x.device)\n         output = output.reshape(output_shape)\n@@ -173,48 +190,79 @@ def forward(self, hidden_states):\n             expert_hidden = hidden_states[i]\n             expert_hidden_reshaped = expert_hidden.reshape(-1, self.hidden_size)\n             # Quantize for this expert\n-            expert_quantized, expert_scale = torch.ops.fbgemm.quantize_fp8_per_row(\n+            expert_quantized, expert_scale = quantize_fp8_per_row(\n                 expert_hidden_reshaped, num_tokens, self.input_scale_ub\n             )\n             sharded_expert_dim = self.gate_up_proj.shape[-1] // 2\n             gate_up_proj_scale_float32 = self.gate_up_proj_scale.to(torch.float32)\n+            if _is_torch_xpu_available:\n+                gate = torch._scaled_mm(\n+                    expert_quantized,\n+                    self.gate_up_proj[i].transpose(0, 1)[:sharded_expert_dim].contiguous().t(),\n+                    scale_a=expert_scale.unsqueeze(-1),\n+                    scale_b=gate_up_proj_scale_float32[i][0][:sharded_expert_dim].view(-1, 1).contiguous().t(),\n+                    out_dtype=hidden_states.dtype,\n+                )\n+                up = torch._scaled_mm(\n+                    expert_quantized,\n+                    self.gate_up_proj[i].transpose(0, 1)[sharded_expert_dim:].contiguous().t(),\n+                    scale_a=expert_scale.unsqueeze(-1),\n+                    scale_b=gate_up_proj_scale_float32[i][0][sharded_expert_dim:].view(-1, 1).contiguous().t(),\n+                    out_dtype=hidden_states.dtype,\n+                )\n+            else:\n+                gate = torch.ops.fbgemm.f8f8bf16_rowwise(\n+                    expert_quantized,\n+                    self.gate_up_proj[i].transpose(0, 1)[:sharded_expert_dim].contiguous(),\n+                    expert_scale,\n+                    gate_up_proj_scale_float32[i][0][:sharded_expert_dim].view(-1, 1).contiguous(),\n+                    use_fast_accum=True,\n+                )\n \n-            gate = torch.ops.fbgemm.f8f8bf16_rowwise(\n-                expert_quantized,\n-                self.gate_up_proj[i].transpose(0, 1)[:sharded_expert_dim].contiguous(),\n-                expert_scale,\n-                gate_up_proj_scale_float32[i][0][:sharded_expert_dim].view(-1, 1).contiguous(),\n-                use_fast_accum=True,\n-            )\n-\n-            up = torch.ops.fbgemm.f8f8bf16_rowwise(\n-                expert_quantized,\n-                self.gate_up_proj[i].transpose(0, 1)[sharded_expert_dim:].contiguous(),\n-                expert_scale,\n-                gate_up_proj_scale_float32[i][0][sharded_expert_dim:].view(-1, 1).contiguous(),\n-                use_fast_accum=True,\n-            )\n+                up = torch.ops.fbgemm.f8f8bf16_rowwise(\n+                    expert_quantized,\n+                    self.gate_up_proj[i].transpose(0, 1)[sharded_expert_dim:].contiguous(),\n+                    expert_scale,\n+                    gate_up_proj_scale_float32[i][0][sharded_expert_dim:].view(-1, 1).contiguous(),\n+                    use_fast_accum=True,\n+                )\n \n             activated = up * self.act_fn(gate)\n \n-            activated_quantized, activated_scale = torch.ops.fbgemm.quantize_fp8_per_row(\n-                activated, num_tokens, self.input_scale_ub\n-            )\n+            activated_quantized, activated_scale = quantize_fp8_per_row(activated, num_tokens, self.input_scale_ub)\n \n             down_proj_scale_float32 = self.down_proj_scale.to(torch.float32)\n-            expert_output = torch.ops.fbgemm.f8f8bf16_rowwise(\n-                activated_quantized,\n-                self.down_proj[i].transpose(0, 1).contiguous(),\n-                activated_scale,\n-                down_proj_scale_float32[i].view(-1, 1).contiguous(),\n-                use_fast_accum=True,\n-            )\n+            if _is_torch_xpu_available:\n+                expert_output = torch._scaled_mm(\n+                    activated_quantized,\n+                    self.down_proj[i].transpose(0, 1).contiguous(),\n+                    scale_a=activated_scale.unsqueeze(-1),\n+                    scale_b=down_proj_scale_float32[i].view(-1, 1).contiguous().t(),\n+                    out_dtype=hidden_states.dtype,\n+                )\n+            else:\n+                expert_output = torch.ops.fbgemm.f8f8bf16_rowwise(\n+                    activated_quantized,\n+                    self.down_proj[i].transpose(0, 1).contiguous(),\n+                    activated_scale,\n+                    down_proj_scale_float32[i].view(-1, 1).contiguous(),\n+                    use_fast_accum=True,\n+                )\n \n             next_states[i] = expert_output\n         next_states = next_states.to(hidden_states.device)\n         return next_states.view(-1, self.hidden_size)\n \n \n+@lru_cache(maxsize=1)\n+def get_quantize_fp8_per_row():\n+    if _is_torch_xpu_available:\n+        from kernels import get_kernel\n+\n+        return get_kernel(\"kernels-community/fp8-fbgemm\").quantize_fp8_per_row\n+    return torch.ops.fbgemm.quantize_fp8_per_row\n+\n+\n def replace_with_fbgemm_fp8_linear(\n     model, modules_to_not_convert: list[str] | None = None, quantization_config=None, pre_quantized=False, tp_plan=None\n ):\n@@ -232,6 +280,8 @@ def replace_with_fbgemm_fp8_linear(\n         pre_quantized (`book`, defaults to `False`):\n             Whether the model is pre-quantized or not\n     \"\"\"\n+    global quantize_fp8_per_row\n+    quantize_fp8_per_row = get_quantize_fp8_per_row()\n \n     has_been_replaced = False\n     module_kwargs = {} if pre_quantized else {\"dtype\": None}"
        },
        {
            "sha": "2fd3957af42c20e3771c8d52463392d143efcf46",
            "filename": "src/transformers/quantizers/quantizer_fbgemm_fp8.py",
            "status": "modified",
            "additions": 24,
            "deletions": 13,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/86644be4790613d9bb21372fd9d10f48fd8d892c/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/86644be4790613d9bb21372fd9d10f48fd8d892c/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_fbgemm_fp8.py?ref=86644be4790613d9bb21372fd9d10f48fd8d892c",
            "patch": "@@ -19,14 +19,21 @@\n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n \n-from ..utils import is_accelerate_available, is_fbgemm_gpu_available, is_torch_available, logging\n+from ..utils import (\n+    is_accelerate_available,\n+    is_fbgemm_gpu_available,\n+    is_kernels_available,\n+    is_torch_available,\n+    is_torch_cuda_available,\n+    is_torch_xpu_available,\n+    logging,\n+)\n from .quantizers_utils import get_module_from_name\n \n \n if is_torch_available():\n     import torch\n \n-\n logger = logging.get_logger(__name__)\n \n \n@@ -41,27 +48,32 @@ def __init__(self, quantization_config, **kwargs):\n         super().__init__(quantization_config, **kwargs)\n \n     def validate_environment(self, *args, **kwargs):\n-        if not is_fbgemm_gpu_available():\n+        if not is_torch_cuda_available() and not is_torch_xpu_available():\n+            raise ImportError(\"Using fbgemm fp8 quantization requires a GPU or XPU\")\n+        if is_torch_xpu_available() and not is_kernels_available():\n+            raise ImportError(\"Using FP8 fbgemm on XPU requires kernels (`pip install kernels`)\")\n+        if is_torch_cuda_available() and not is_fbgemm_gpu_available():\n             raise ImportError(\n-                \"Using fbgemm fp8 quantization requires fbgemm-gpu library\"\n+                \"Loading an FP8 fbgemm quantized model on CUDA requires fbgemm-gpu library\"\n                 \"Please install the latest version of fbgemm-gpu library by following : https://pytorch.org/FBGEMM/fbgemm_gpu-development/InstallationInstructions.html#fbgemm-gpu-install-libraries\"\n             )\n         if not is_accelerate_available():\n             raise ImportError(\n                 \"Loading an FP8 quantized model requires accelerate (`pip install --upgrade accelerate`)\"\n             )\n-        compute_capability = torch.cuda.get_device_capability()\n-        major, _ = compute_capability\n-        if major < 9:\n-            raise ValueError(\n-                \"FP8 quantized models is only supported on GPUs with compute capability >= 9.0 (e.g H100)\"\n-            )\n+        if is_torch_cuda_available():\n+            compute_capability = torch.cuda.get_device_capability()\n+            major, _ = compute_capability\n+            if major < 9:\n+                raise ValueError(\n+                    \"FP8 quantized models is only supported on GPUs with compute capability >= 9.0 (e.g H100)\"\n+                )\n \n         device_map = kwargs.get(\"device_map\")\n         if device_map is None:\n             logger.warning_once(\n-                \"You have loaded an FP8 model on CPU and have a CUDA device available, make sure to set \"\n-                \"your model on a GPU device in order to run your model. To remove this warning, pass device_map = 'cuda'. \"\n+                \"You have loaded an FP8 model on CPU and have a CUDA/XPU device available, make sure to set \"\n+                \"your model on a GPU/XPU device in order to run your model. To remove this warning, pass device_map = 'cuda' or 'xpu' or 'auto'. \"\n             )\n         elif isinstance(device_map, dict):\n             if not self.pre_quantized and (\"cpu\" in device_map.values() or \"disk\" in device_map.values()):\n@@ -121,7 +133,6 @@ def _process_model_before_weight_loading(\n             modules_to_not_convert=self.modules_to_not_convert,\n             quantization_config=self.quantization_config,\n             pre_quantized=self.pre_quantized,\n-            config=model.config,\n             tp_plan=model._tp_plan,\n         )\n "
        },
        {
            "sha": "cd4926af2459feb210bab36bb83f37d223613888",
            "filename": "tests/quantization/fbgemm_fp8/test_fbgemm_fp8.py",
            "status": "modified",
            "additions": 23,
            "deletions": 12,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/86644be4790613d9bb21372fd9d10f48fd8d892c/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/86644be4790613d9bb21372fd9d10f48fd8d892c/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Ffbgemm_fp8%2Ftest_fbgemm_fp8.py?ref=86644be4790613d9bb21372fd9d10f48fd8d892c",
            "patch": "@@ -21,14 +21,19 @@\n from transformers.testing_utils import (\n     backend_empty_cache,\n     require_accelerate,\n-    require_fbgemm_gpu,\n+    require_deterministic_for_xpu,\n     require_read_token,\n-    require_torch_gpu,\n-    require_torch_multi_gpu,\n+    require_torch_accelerator,\n+    require_torch_multi_accelerator,\n     slow,\n     torch_device,\n )\n-from transformers.utils import is_accelerate_available, is_torch_available\n+from transformers.utils import (\n+    is_accelerate_available,\n+    is_fbgemm_gpu_available,\n+    is_torch_available,\n+    is_torch_xpu_available,\n+)\n \n \n if is_torch_available():\n@@ -38,7 +43,7 @@\n     from accelerate import init_empty_weights\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n class FbgemmFp8ConfigTest(unittest.TestCase):\n     def test_to_dict(self):\n         \"\"\"\n@@ -62,8 +67,8 @@ def test_from_dict(self):\n \n \n @slow\n-@require_torch_gpu\n-@require_fbgemm_gpu\n+@require_torch_accelerator\n+@unittest.skipIf(not is_torch_xpu_available() and not is_fbgemm_gpu_available(), \"test requires fbgemm-gpu or xpu\")\n @require_accelerate\n @require_read_token\n class FbgemmFp8Test(unittest.TestCase):\n@@ -76,10 +81,11 @@ class FbgemmFp8Test(unittest.TestCase):\n         [\n             \"What are we having for dinner?\\nI'm having a steak and a salad\",\n             \"What are we having for dinner? I don’t know. What are we having\",\n+            \"What are we having for dinner? I don’t know, what are you having\",\n         ]\n     )\n \n-    device_map = \"cuda\"\n+    device_map = \"xpu\" if is_torch_xpu_available() else \"cuda\"\n \n     offload_device_map = {\n         \"model.embed_tokens\": 0,\n@@ -176,6 +182,7 @@ def test_quantized_model_conversion(self):\n \n         self.assertEqual(nb_linears - 24, nb_fbgemm_linear)\n \n+    @require_deterministic_for_xpu\n     def test_quantized_model(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly\n@@ -185,6 +192,7 @@ def test_quantized_model(self):\n         output = self.quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n         self.assertTrue(self.tokenizer.decode(output[0], skip_special_tokens=True) in self.EXPECTED_OUTPUT)\n \n+    @require_deterministic_for_xpu\n     def test_save_pretrained(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly after being saved and loaded\n@@ -219,7 +227,8 @@ def test_change_loading_attributes(self):\n             output = model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n             self.assertTrue(self.tokenizer.decode(output[0], skip_special_tokens=True) in self.EXPECTED_OUTPUT)\n \n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n+    @require_deterministic_for_xpu\n     def test_quantized_model_multi_gpu(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly with multiple GPUs\n@@ -248,6 +257,7 @@ def test_quantized_model_offload(self):\n                 self.model_name, device_map=self.offload_device_map, quantization_config=quantization_config\n             )\n \n+    @require_deterministic_for_xpu\n     def test_save_pretrained_offload(self):\n         \"\"\"\n         Simple test that checks if the saved quantized model is working properly cpu/disk offload\n@@ -261,7 +271,8 @@ def test_save_pretrained_offload(self):\n             output = quantized_model.generate(**input_ids, max_new_tokens=self.max_new_tokens, do_sample=False)\n             self.assertTrue(self.tokenizer.decode(output[0], skip_special_tokens=True) in self.EXPECTED_OUTPUT)\n \n-    @require_torch_multi_gpu\n+    @require_torch_multi_accelerator\n+    @require_deterministic_for_xpu\n     def test_save_pretrained_multi_gpu(self):\n         \"\"\"\n         Simple test that checks if the quantized model is working properly after being saved and loaded\n@@ -278,9 +289,9 @@ def test_save_pretrained_multi_gpu(self):\n             self.assertTrue(self.tokenizer.decode(output[0], skip_special_tokens=True) in self.EXPECTED_OUTPUT)\n \n \n-@require_torch_gpu\n+@require_torch_accelerator\n @require_accelerate\n-@require_fbgemm_gpu\n+@unittest.skipIf(not is_torch_xpu_available() and not is_fbgemm_gpu_available(), \"test requires fbgemm-gpu or xpu\")\n class FbgemmFp8LinearTest(unittest.TestCase):\n     def test_linear_preserves_shape(self):\n         \"\"\""
        }
    ],
    "stats": {
        "total": 198,
        "additions": 135,
        "deletions": 63
    }
}