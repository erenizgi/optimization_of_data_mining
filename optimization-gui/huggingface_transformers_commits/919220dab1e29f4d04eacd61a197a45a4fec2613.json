{
    "author": "jla524",
    "message": "Update translated docs for `sdpa_kernel` (#35461)\n\n* docs: update sdpa_kernel for translation\n\n* fix: nn.attention\n\n* update: infer many",
    "sha": "919220dab1e29f4d04eacd61a197a45a4fec2613",
    "files": [
        {
            "sha": "18a19c849eb2c7536d13303d640d3f1c238c0757",
            "filename": "docs/source/ja/perf_infer_gpu_many.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/919220dab1e29f4d04eacd61a197a45a4fec2613/docs%2Fsource%2Fja%2Fperf_infer_gpu_many.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/919220dab1e29f4d04eacd61a197a45a4fec2613/docs%2Fsource%2Fja%2Fperf_infer_gpu_many.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_infer_gpu_many.md?ref=919220dab1e29f4d04eacd61a197a45a4fec2613",
            "patch": "@@ -34,7 +34,7 @@ BetterTransformerは、テキスト、画像、音声モデルの単一GPUおよ\n <Tip>\n \n Flash Attentionは、fp16またはbf16 dtypeを使用しているモデルにのみ使用できます。BetterTransformerを使用する前に、モデルを適切なdtypeにキャストしてください。\n-  \n+\n </Tip>\n \n ### Decoder models\n@@ -53,11 +53,12 @@ model.to_bettertransformer()\n # Use it for training or inference\n ```\n \n-SDPAは、ハードウェアや問題のサイズなどの特定の設定で[Flash Attention](https://arxiv.org/abs/2205.14135)カーネルを呼び出すこともできます。Flash Attentionを有効にするか、特定の設定（ハードウェア、問題のサイズ）で利用可能かを確認するには、[`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel)をコンテキストマネージャとして使用します。\n+SDPAは、ハードウェアや問題のサイズなどの特定の設定で[Flash Attention](https://arxiv.org/abs/2205.14135)カーネルを呼び出すこともできます。Flash Attentionを有効にするか、特定の設定（ハードウェア、問題のサイズ）で利用可能かを確認するには、[`torch.nn.kernel.sdpa_kernel`](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html)をコンテキストマネージャとして使用します。\n \n \n ```diff\n import torch\n++ from torch.nn.attention import SDPBackend, sdpa_kernel\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n@@ -68,7 +69,7 @@ model.to_bettertransformer()\n input_text = \"Hello my dog is cute and\"\n inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n-+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n++ with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n     outputs = model.generate(**inputs)\n \n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n@@ -105,6 +106,7 @@ BetterTransformerのパフォーマンスの詳細については、この[ブ\n \n ```py\n import torch\n+from torch.nn.attention import SDPBackend, sdpa_kernel\n from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n \n quantization_config = BitsAndBytesConfig(\n@@ -118,7 +120,7 @@ model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_c\n input_text = \"Hello my dog is cute and\"\n inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n-with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n+with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n     outputs = model.generate(**inputs)\n \n print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
        },
        {
            "sha": "6a3dc5fa64a8527c14dcf1a238b2feb331923d3b",
            "filename": "docs/source/ja/perf_infer_gpu_one.md",
            "status": "modified",
            "additions": 11,
            "deletions": 9,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/919220dab1e29f4d04eacd61a197a45a4fec2613/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/919220dab1e29f4d04eacd61a197a45a4fec2613/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_infer_gpu_one.md?ref=919220dab1e29f4d04eacd61a197a45a4fec2613",
            "patch": "@@ -55,8 +55,8 @@ model_id = \"tiiuae/falcon-7b\"\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n model = AutoModelForCausalLM.from_pretrained(\n-    model_id, \n-    torch_dtype=torch.bfloat16, \n+    model_id,\n+    torch_dtype=torch.bfloat16,\n     attn_implementation=\"flash_attention_2\",\n )\n ```\n@@ -112,7 +112,7 @@ model_id = \"tiiuae/falcon-7b\"\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n model = AutoModelForCausalLM.from_pretrained(\n-    model_id, \n+    model_id,\n     load_in_8bit=True,\n     attn_implementation=\"flash_attention_2\",\n )\n@@ -130,7 +130,7 @@ model_id = \"tiiuae/falcon-7b\"\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n model = AutoModelForCausalLM.from_pretrained(\n-    model_id, \n+    model_id,\n     load_in_4bit=True,\n     attn_implementation=\"flash_attention_2\",\n )\n@@ -149,7 +149,7 @@ model_id = \"tiiuae/falcon-7b\"\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n model = AutoModelForCausalLM.from_pretrained(\n-    model_id, \n+    model_id,\n     load_in_4bit=True,\n     attn_implementation=\"flash_attention_2\",\n )\n@@ -173,7 +173,7 @@ BetterTransformerは、テキスト、画像、およびオーディオモデル\n <Tip>\n \n Flash Attentionは、fp16またはbf16のdtypeを使用するモデルにのみ使用できます。BetterTransformerを使用する前に、モデルを適切なdtypeにキャストしてください。\n-  \n+\n </Tip>\n \n ### Encoder models\n@@ -214,11 +214,12 @@ model.to_bettertransformer()\n # Use it for training or inference\n ```\n \n-SDPAは、ハードウェアや問題のサイズに応じて[Flash Attention](https://arxiv.org/abs/2205.14135)カーネルを使用することもできます。Flash Attentionを有効にするか、特定の設定（ハードウェア、問題サイズ）で使用可能かどうかを確認するには、[`torch.backends.cuda.sdp_kernel`](https://pytorch.org/docs/master/backends.html#torch.backends.cuda.sdp_kernel)をコンテキストマネージャとして使用します。\n+SDPAは、ハードウェアや問題のサイズに応じて[Flash Attention](https://arxiv.org/abs/2205.14135)カーネルを使用することもできます。Flash Attentionを有効にするか、特定の設定（ハードウェア、問題サイズ）で使用可能かどうかを確認するには、[`torch.nn.attention.sdpa_kernel`](https://pytorch.org/docs/stable/generated/torch.nn.attention.sdpa_kernel.html)をコンテキストマネージャとして使用します。\n \n \n ```diff\n import torch\n++ from torch.nn.attention import SDPBackend, sdpa_kernel\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-350m\")\n@@ -229,7 +230,7 @@ model.to_bettertransformer()\n input_text = \"Hello my dog is cute and\"\n inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n-+ with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n++ with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n     outputs = model.generate(**inputs)\n \n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n@@ -421,6 +422,7 @@ In this example, the first GPU will use 1GB of memory and the second 2GB.\n \n ```py\n import torch\n+from torch.nn.attention import SDPBackend, sdpa_kernel\n from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n \n quantization_config = BitsAndBytesConfig(\n@@ -434,7 +436,7 @@ model = AutoModelForCausalLM.from_pretrained(\"facebook/opt-350m\", quantization_c\n input_text = \"Hello my dog is cute and\"\n inputs = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n \n-with torch.backends.cuda.sdp_kernel(enable_flash=True, enable_math=False, enable_mem_efficient=False):\n+with sdpa_kernel(SDPBackend.FLASH_ATTENTION):\n     outputs = model.generate(**inputs)\n \n print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
        }
    ],
    "stats": {
        "total": 30,
        "additions": 17,
        "deletions": 13
    }
}