{
    "author": "sniper35",
    "message": "[Bug] qwen2_5_omni: cap generation length to be less than the max_position_embedding in DiT (#43068)\n\n* qwen2_5_omni: make max_mel_frames an inference-time knob\n\n* not fail with raising ValueError, instead make it continue to run by choosing a target_duration that's capped and aligned\n\n* added unit tests for Token2WavShape shape mismatch\n\nSigned-off-by: Dong Wang <dongw2019@gmail.com>\n\n* make fixup\n\n* remove unit test which takes too much GPU memory\n\nSigned-off-by: Dong Wang <dongw2019@gmail.com>\n\n* reduce gpu memory usage from the unit test\n\n* addressed comments\n\nSigned-off-by: Dong Wang <dongw2019@gmail.com>\n\n---------\n\nSigned-off-by: Dong Wang <dongw2019@gmail.com>",
    "sha": "0d8f1870555614693166cfceefeb0f4231c1d4be",
    "files": [
        {
            "sha": "d5d41620c09160a0c8323a9616bae49145808e92",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d8f1870555614693166cfceefeb0f4231c1d4be/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d8f1870555614693166cfceefeb0f4231c1d4be/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=0d8f1870555614693166cfceefeb0f4231c1d4be",
            "patch": "@@ -3676,15 +3676,24 @@ def sample(\n         guidance_scale=0.5,\n         sway_coefficient=-1.0,\n     ):\n-        noise_initialization = torch.randn([1, 30000, self.mel_dim], dtype=reference_mel_spectrogram.dtype)\n         maximum_duration = quantized_code.shape[1] * self.repeats\n-        initial_state = noise_initialization[:, :maximum_duration].to(quantized_code.device)\n         batch_size = reference_mel_spectrogram.shape[0]\n-        conditioning_vector = conditioning_vector.unsqueeze(1).repeat(1, maximum_duration, 1)\n-\n         if batch_size != 1:\n             raise ValueError(\"Only batch size = 1 is currently supported\")\n \n+        if maximum_duration > self.config.max_position_embeddings:\n+            raise ValueError(\n+                f\"Requested mel length ({maximum_duration}) exceeds `dit_config.max_position_embeddings` \"\n+                f\"({self.config.max_position_embeddings}). Provide shorter `quantized_code`.\"\n+            )\n+\n+        initial_state = torch.randn(\n+            [batch_size, maximum_duration, self.mel_dim],\n+            dtype=reference_mel_spectrogram.dtype,\n+            device=quantized_code.device,\n+        )\n+        conditioning_vector = conditioning_vector.unsqueeze(1).repeat(1, maximum_duration, 1)\n+\n         def ode_function(time_step, hidden_states):\n             if guidance_scale < 1e-5:\n                 prediction = self(\n@@ -3695,6 +3704,7 @@ def ode_function(time_step, hidden_states):\n                     time_step=time_step,\n                     drop_audio_conditioning=False,\n                     drop_code=False,\n+                    apply_cfg=False,\n                 )\n                 return prediction\n "
        },
        {
            "sha": "310d1915de5bf9b4642b7ed7e5832a8f78865cc3",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 14,
            "deletions": 4,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d8f1870555614693166cfceefeb0f4231c1d4be/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d8f1870555614693166cfceefeb0f4231c1d4be/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=0d8f1870555614693166cfceefeb0f4231c1d4be",
            "patch": "@@ -3835,15 +3835,24 @@ def sample(\n         guidance_scale=0.5,\n         sway_coefficient=-1.0,\n     ):\n-        noise_initialization = torch.randn([1, 30000, self.mel_dim], dtype=reference_mel_spectrogram.dtype)\n         maximum_duration = quantized_code.shape[1] * self.repeats\n-        initial_state = noise_initialization[:, :maximum_duration].to(quantized_code.device)\n         batch_size = reference_mel_spectrogram.shape[0]\n-        conditioning_vector = conditioning_vector.unsqueeze(1).repeat(1, maximum_duration, 1)\n-\n         if batch_size != 1:\n             raise ValueError(\"Only batch size = 1 is currently supported\")\n \n+        if maximum_duration > self.config.max_position_embeddings:\n+            raise ValueError(\n+                f\"Requested mel length ({maximum_duration}) exceeds `dit_config.max_position_embeddings` \"\n+                f\"({self.config.max_position_embeddings}). Provide shorter `quantized_code`.\"\n+            )\n+\n+        initial_state = torch.randn(\n+            [batch_size, maximum_duration, self.mel_dim],\n+            dtype=reference_mel_spectrogram.dtype,\n+            device=quantized_code.device,\n+        )\n+        conditioning_vector = conditioning_vector.unsqueeze(1).repeat(1, maximum_duration, 1)\n+\n         def ode_function(time_step, hidden_states):\n             if guidance_scale < 1e-5:\n                 prediction = self(\n@@ -3854,6 +3863,7 @@ def ode_function(time_step, hidden_states):\n                     time_step=time_step,\n                     drop_audio_conditioning=False,\n                     drop_code=False,\n+                    apply_cfg=False,\n                 )\n                 return prediction\n "
        },
        {
            "sha": "e3f910e8168a3fb1eb0741a770456bafbaa9f66c",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 97,
            "deletions": 0,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/0d8f1870555614693166cfceefeb0f4231c1d4be/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0d8f1870555614693166cfceefeb0f4231c1d4be/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=0d8f1870555614693166cfceefeb0f4231c1d4be",
            "patch": "@@ -859,3 +859,100 @@ def test_small_model_integration_test_batch_flashatt2(self):\n         decoded_texts = self.processor.batch_decode(output, skip_special_tokens=True)\n         self.assertEqual(decoded_texts[0], EXPECTED_DECODED_TEXT)\n         self.assertEqual(decoded_texts[1], EXPECTED_DECODED_TEXT)\n+\n+\n+@require_torch\n+class Qwen2_5OmniToken2WavMaxPositionEmbeddingsTest(unittest.TestCase):\n+    \"\"\"\n+    Tests to verify that ValueError is raised when input length exceeds max_position_embeddings.\n+    \"\"\"\n+\n+    @classmethod\n+    def setUpClass(cls):\n+        \"\"\"Create minimal DiT model config for testing - shared across all tests.\"\"\"\n+        from transformers.models.qwen2_5_omni.configuration_qwen2_5_omni import Qwen2_5OmniDiTConfig\n+\n+        # Use minimal dimensions to reduce memory usage\n+        # Note: enc_channels needs at least 3 elements for the ECAPA-TDNN encoder architecture\n+        cls.config = Qwen2_5OmniDiTConfig(\n+            hidden_size=32,\n+            num_hidden_layers=1,\n+            num_attention_heads=2,\n+            head_dim=16,\n+            ff_mult=1,\n+            emb_dim=16,\n+            mel_dim=16,\n+            enc_emb_dim=16,\n+            enc_dim=16,\n+            enc_channels=[16, 16, 16],\n+            enc_kernel_sizes=[3, 3, 1],\n+            enc_dilations=[1, 1, 1],\n+            enc_attention_channels=8,\n+            enc_res2net_scale=2,\n+            enc_se_channels=8,\n+            num_embeds=100,\n+            look_ahead_layers=[],\n+            look_backward_layers=[0],\n+            max_position_embeddings=100,  # Small for testing\n+            block_size=24,\n+            repeats=2,\n+        )\n+\n+    def setUp(self):\n+        \"\"\"Create model instance for each test.\"\"\"\n+        from transformers.models.qwen2_5_omni.modeling_qwen2_5_omni import Qwen2_5OmniToken2WavDiTModel\n+\n+        self.model = Qwen2_5OmniToken2WavDiTModel(self.config).to(torch_device)\n+        self.model.eval()\n+\n+    def tearDown(self):\n+        \"\"\"Clean up model to free memory.\"\"\"\n+        del self.model\n+        if torch.cuda.is_available():\n+            torch.cuda.empty_cache()\n+\n+    def test_error_when_exceeding_max_position_embeddings(self):\n+        \"\"\"Verify ValueError is raised when maximum_duration > max_position_embeddings.\"\"\"\n+        batch_size = 1\n+        # With repeats=2 and max_position_embeddings=100, we need > 50 tokens to exceed\n+        num_speech_tokens = 60  # Will result in 120 mel frames, exceeds max_position_embeddings=100\n+\n+        conditioning_vector = torch.randn(batch_size, self.config.enc_emb_dim, device=torch_device)\n+        reference_mel = torch.randn(batch_size, 200, self.config.mel_dim, device=torch_device)\n+        quantized_code = torch.randint(0, self.config.num_embeds, (batch_size, num_speech_tokens), device=torch_device)\n+\n+        with self.assertRaises(ValueError) as context:\n+            self.model.sample(\n+                conditioning_vector=conditioning_vector,\n+                reference_mel_spectrogram=reference_mel,\n+                quantized_code=quantized_code,\n+                num_steps=2,\n+            )\n+\n+        self.assertIn(\"exceeds `dit_config.max_position_embeddings`\", str(context.exception))\n+        self.assertIn(\"120\", str(context.exception))  # Requested mel length\n+        self.assertIn(\"100\", str(context.exception))  # max_position_embeddings\n+\n+    def test_no_error_when_within_limits(self):\n+        \"\"\"Verify no error when maximum_duration <= max_position_embeddings.\"\"\"\n+        batch_size = 1\n+        # With repeats=2 and max_position_embeddings=100, 50 tokens = 100 mel frames (exactly at limit)\n+        num_speech_tokens = 50\n+\n+        conditioning_vector = torch.randn(batch_size, self.config.enc_emb_dim, device=torch_device)\n+        reference_mel = torch.randn(batch_size, 200, self.config.mel_dim, device=torch_device)\n+        quantized_code = torch.randint(0, self.config.num_embeds, (batch_size, num_speech_tokens), device=torch_device)\n+\n+        # Should complete without error\n+        output = self.model.sample(\n+            conditioning_vector=conditioning_vector,\n+            reference_mel_spectrogram=reference_mel,\n+            quantized_code=quantized_code,\n+            num_steps=2,\n+        )\n+\n+        # Check output shape is valid\n+        self.assertEqual(len(output.shape), 3)\n+        self.assertEqual(output.shape[0], batch_size)\n+        self.assertEqual(output.shape[1], self.config.mel_dim)\n+        self.assertEqual(output.shape[2], 100)  # 50 tokens * 2 repeats"
        }
    ],
    "stats": {
        "total": 133,
        "additions": 125,
        "deletions": 8
    }
}