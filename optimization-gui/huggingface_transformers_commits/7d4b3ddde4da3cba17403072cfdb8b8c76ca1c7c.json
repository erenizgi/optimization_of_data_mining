{
    "author": "dvrogozh",
    "message": "ci: fix xpu skip condition for test_model_parallel_beam_search (#35742)\n\n`return unittest.skip()` used in the `test_model_parallel_beam_search` in\r\nskip condition for xpu did not actually mark test to be skipped running\r\nunder pytest:\r\n* 148 passed, 1 skipped\r\n\r\nOther tests use `self.skipTest()`. Reusing this approach and moving the\r\ncondition outside the loop (since it does not depend on it) allows to skip\r\nfor xpu correctly:\r\n* 148 skipped\r\n\r\nSecondly, `device_map=\"auto\"` is now implemented for XPU for IPEX>=2.5 and\r\ntorch>=2.6, so we can now enable these tests for XPU for new IPEX/torch\r\nversions.\r\n\r\nFixes: 1ea3ad1ae (\"[tests] use `torch_device` instead of `auto` for model testing (#29531)\")\r\n\r\nSigned-off-by: Dmitry Rogozhkin <dmitry.v.rogozhkin@intel.com>",
    "sha": "7d4b3ddde4da3cba17403072cfdb8b8c76ca1c7c",
    "files": [
        {
            "sha": "ac07281b3d33f3c1c4afff9fcafb36d1a972411c",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d4b3ddde4da3cba17403072cfdb8b8c76ca1c7c/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d4b3ddde4da3cba17403072cfdb8b8c76ca1c7c/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=7d4b3ddde4da3cba17403072cfdb8b8c76ca1c7c",
            "patch": "@@ -865,7 +865,7 @@ def is_ninja_available():\n         return True\n \n \n-def is_ipex_available():\n+def is_ipex_available(min_version: str = \"\"):\n     def get_major_and_minor_from_version(full_version):\n         return str(version.parse(full_version).major) + \".\" + str(version.parse(full_version).minor)\n \n@@ -880,6 +880,8 @@ def get_major_and_minor_from_version(full_version):\n             f\" but PyTorch {_torch_version} is found. Please switch to the matching version and run again.\"\n         )\n         return False\n+    if min_version:\n+        return version.parse(_ipex_version) >= version.parse(min_version)\n     return True\n \n "
        },
        {
            "sha": "ba61d4b43677dae31288b90f5a40b1c91c04772e",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d4b3ddde4da3cba17403072cfdb8b8c76ca1c7c/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d4b3ddde4da3cba17403072cfdb8b8c76ca1c7c/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=7d4b3ddde4da3cba17403072cfdb8b8c76ca1c7c",
            "patch": "@@ -24,6 +24,7 @@\n \n import numpy as np\n import pytest\n+from packaging import version\n from parameterized import parameterized\n \n from transformers import AutoConfig, is_torch_available, pipeline\n@@ -44,6 +45,7 @@\n     slow,\n     torch_device,\n )\n+from transformers.utils import is_ipex_available\n \n from ..test_modeling_common import floats_tensor, ids_tensor\n from .test_framework_agnostic import GenerationIntegrationTestsMixin\n@@ -675,10 +677,11 @@ def test_beam_search_generate_dict_outputs_use_cache(self):\n     @require_torch_multi_accelerator\n     @pytest.mark.generate\n     def test_model_parallel_beam_search(self):\n-        for model_class in self.all_generative_model_classes:\n-            if \"xpu\" in torch_device:\n-                return unittest.skip(reason=\"device_map='auto' does not work with XPU devices\")\n+        if \"xpu\" in torch_device:\n+            if not (is_ipex_available(\"2.5\") or version.parse(torch.__version__) >= version.parse(\"2.6\")):\n+                self.skipTest(reason=\"device_map='auto' does not work with XPU devices\")\n \n+        for model_class in self.all_generative_model_classes:\n             if model_class._no_split_modules is None:\n                 continue\n "
        }
    ],
    "stats": {
        "total": 13,
        "additions": 9,
        "deletions": 4
    }
}