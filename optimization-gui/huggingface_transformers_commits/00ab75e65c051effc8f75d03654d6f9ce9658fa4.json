{
    "author": "engmohamedsalah",
    "message": "fix(benchmarks): correct sdpa_backend inconsistency and attn_implementation for continuous batching (#42339)\n\nThis commit fixes two bugs in BenchmarkConfig reported in issue #42211:\n\n1. **sdpa_backend inconsistency (line 105)**: The warning message states\n   \"sdpa_backend must be None\" but the code was setting it to \"math\".\n   Changed to None to match the warning message. This allows PyTorch to\n   auto-select the appropriate SDPA backend rather than forcing one globally,\n   which is correct for continuous batching with custom attention masks.\n\n2. **Invalid attn_implementation (line 243)**: Changed from \"paged|sdpa\" to\n   \"sdpa\". Using \"paged|sdpa\" directly bypassed the validation logic at\n   lines 91-105 since it only checks for exactly \"sdpa\". The \"paged|\" prefix\n   is automatically added by init_continuous_batching() in continuous_api.py,\n   so the config should use plain \"sdpa\" for consistency with other configs.\n\nBoth bugs were introduced in commit 069684ef87 (PR #41916).\n\nFixes #42211",
    "sha": "00ab75e65c051effc8f75d03654d6f9ce9658fa4",
    "files": [
        {
            "sha": "a29748c7049f837a7bf26c8fdcfedc573023d198",
            "filename": "benchmark_v2/framework/benchmark_config.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/00ab75e65c051effc8f75d03654d6f9ce9658fa4/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/00ab75e65c051effc8f75d03654d6f9ce9658fa4/benchmark_v2%2Fframework%2Fbenchmark_config.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/benchmark_v2%2Fframework%2Fbenchmark_config.py?ref=00ab75e65c051effc8f75d03654d6f9ce9658fa4",
            "patch": "@@ -102,7 +102,7 @@ def check_validity(self, skip_validity_check: bool = False) -> None:\n                 logger.warning(\n                     \"when continuous batching is enabled, sdpa_backend must be None because of the attention mask, setting it to None\"\n                 )\n-                self.sdpa_backend = \"math\"\n+                self.sdpa_backend = None\n \n     @property\n     def hash(self) -> str:\n@@ -240,5 +240,5 @@ def get_config_by_level(level: int) -> list[BenchmarkConfig]:\n         configs.append(BenchmarkConfig(attn_implementation=\"sdpa\", compile_mode=\"default\"))\n         configs.append(BenchmarkConfig(attn_implementation=\"flex_attention\", compile_mode=\"default\", kernelize=True))\n         configs.append(BenchmarkConfig(attn_implementation=\"flash_attention_2\", kernelize=True))\n-        configs.append(BenchmarkConfig(attn_implementation=\"paged|sdpa\", continuous_batching=True))\n+        configs.append(BenchmarkConfig(attn_implementation=\"sdpa\", continuous_batching=True))\n     return configs"
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}