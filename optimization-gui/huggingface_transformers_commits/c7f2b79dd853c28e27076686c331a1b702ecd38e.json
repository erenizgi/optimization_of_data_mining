{
    "author": "SunMarc",
    "message": "protect dtensor import  (#38496)\n\nprotect",
    "sha": "c7f2b79dd853c28e27076686c331a1b702ecd38e",
    "files": [
        {
            "sha": "9639ff7ce060810c79f30228b2ac6176b1066169",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 1,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7f2b79dd853c28e27076686c331a1b702ecd38e/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7f2b79dd853c28e27076686c331a1b702ecd38e/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=c7f2b79dd853c28e27076686c331a1b702ecd38e",
            "patch": "@@ -37,7 +37,6 @@\n from zipfile import is_zipfile\n \n import torch\n-import torch.distributed.tensor\n from huggingface_hub import split_torch_state_dict_into_shards\n from packaging import version\n from torch import Tensor, nn"
        },
        {
            "sha": "9bb02bff963990c56aefaffb67aaedbe8d30a322",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/c7f2b79dd853c28e27076686c331a1b702ecd38e/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/c7f2b79dd853c28e27076686c331a1b702ecd38e/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=c7f2b79dd853c28e27076686c331a1b702ecd38e",
            "patch": "@@ -42,9 +42,6 @@\n # Cache this result has it's a C FFI call which can be pretty time-consuming\n _torch_distributed_available = torch.distributed.is_available()\n \n-if is_torch_greater_or_equal(\"2.5\") and _torch_distributed_available:\n-    pass\n-\n \n def softmax_backward_data(parent, grad_output, output, dim, self):\n     \"\"\"\n@@ -296,7 +293,7 @@ def id_tensor_storage(tensor: torch.Tensor) -> tuple[torch.device, int, int]:\n     guaranteed to be unique and constant for this tensor's storage during its lifetime. Two tensor storages with\n     non-overlapping lifetimes may have the same id.\n     \"\"\"\n-    if is_torch_greater_or_equal_than_2_0:\n+    if _torch_distributed_available and is_torch_greater_or_equal(\"2.5\"):\n         from torch.distributed.tensor import DTensor\n \n         if isinstance(tensor, DTensor):"
        }
    ],
    "stats": {
        "total": 6,
        "additions": 1,
        "deletions": 5
    }
}