{
    "author": "zucchini-nlp",
    "message": "[configuration] allow to overwrite kwargs from subconfigs (#40241)\n\nallow to overwrite kwargs from subconfigs",
    "sha": "7db228a92ad5fcb9d5089cac91d7c7c70e5578d6",
    "files": [
        {
            "sha": "a290fcfc733b04d516d9285cb53b4302247896e1",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/7db228a92ad5fcb9d5089cac91d7c7c70e5578d6/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7db228a92ad5fcb9d5089cac91d7c7c70e5578d6/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=7db228a92ad5fcb9d5089cac91d7c7c70e5578d6",
            "patch": "@@ -825,8 +825,11 @@ def from_dict(\n             if hasattr(config, key):\n                 current_attr = getattr(config, key)\n                 # To authorize passing a custom subconfig as kwarg in models that have nested configs.\n+                # We need to update only custom kwarg values instead and keep other attributes in subconfig.\n                 if isinstance(current_attr, PretrainedConfig) and isinstance(value, dict):\n-                    value = current_attr.__class__(**value)\n+                    current_attr_updated = current_attr.to_dict()\n+                    current_attr_updated.update(value)\n+                    value = current_attr.__class__(**current_attr_updated)\n                 setattr(config, key, value)\n                 if key != \"dtype\":\n                     to_remove.append(key)"
        },
        {
            "sha": "f7836dca6db3e31f56517241c1731a4ae474514a",
            "filename": "tests/test_configuration_common.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/7db228a92ad5fcb9d5089cac91d7c7c70e5578d6/tests%2Ftest_configuration_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/7db228a92ad5fcb9d5089cac91d7c7c70e5578d6/tests%2Ftest_configuration_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_configuration_common.py?ref=7db228a92ad5fcb9d5089cac91d7c7c70e5578d6",
            "patch": "@@ -154,6 +154,41 @@ def create_and_test_config_from_and_save_pretrained_composite(self):\n                     sub_config_loaded_2 = sub_class.from_pretrained(tmpdirname2)\n                     self.parent.assertEqual(sub_config_loaded.to_dict(), sub_config_loaded_2.to_dict())\n \n+    def create_and_test_config_from_pretrained_custom_kwargs(self):\n+        \"\"\"\n+        Tests that passing custom kwargs to the `from_pretrained` will overwrite model's saved config values.\n+        for composite configs. We should overwrite only the requested keys, keeping all values of the\n+        subconfig that are loaded from the checkpoint.\n+        \"\"\"\n+        # Check only composite configs. We can't know which attributes each type fo config has so check\n+        # only text config because we are sure that all text configs have a `vocab_size`\n+        config = self.config_class(**self.inputs_dict)\n+        if config.get_text_config() is config or not hasattr(self.parent.model_tester, \"get_config\"):\n+            return\n+\n+        # First create a config with non-default values and save it. The reload it back with a new\n+        # `vocab_size` and check that all values are loaded from checkpoint and not init from defaults\n+        non_default_inputs = self.parent.model_tester.get_config().to_dict()\n+        config = self.config_class(**non_default_inputs)\n+        original_text_config = config.get_text_config()\n+        text_config_key = [key for key in config if getattr(config, key) is original_text_config]\n+\n+        # The heuristic is a bit brittle so let's just skip the test\n+        if len(text_config_key) != 1:\n+            return\n+\n+        text_config_key = text_config_key[0]\n+        with tempfile.TemporaryDirectory() as tmpdirname:\n+            config.save_pretrained(tmpdirname)\n+\n+            # Set vocab size to 20 tokens and reload from checkpoint and check if all keys/values are identical except for `vocab_size`\n+            config_reloaded = self.config_class.from_pretrained(tmpdirname, **{text_config_key: {\"vocab_size\": 20}})\n+            original_text_config_dict = original_text_config.to_dict()\n+            original_text_config_dict[\"vocab_size\"] = 20\n+\n+            text_config_reloaded_dict = config_reloaded.get_text_config().to_dict()\n+            self.parent.assertDictEqual(text_config_reloaded_dict, original_text_config_dict)\n+\n     def create_and_test_config_with_num_labels(self):\n         config = self.config_class(**self.inputs_dict, num_labels=5)\n         self.parent.assertEqual(len(config.id2label), 5)\n@@ -204,3 +239,4 @@ def run_common_tests(self):\n         self.create_and_test_config_with_num_labels()\n         self.check_config_can_be_init_without_params()\n         self.check_config_arguments_init()\n+        self.create_and_test_config_from_pretrained_custom_kwargs()"
        }
    ],
    "stats": {
        "total": 41,
        "additions": 40,
        "deletions": 1
    }
}