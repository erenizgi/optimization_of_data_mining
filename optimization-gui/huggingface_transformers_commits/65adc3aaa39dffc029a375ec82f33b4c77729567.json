{
    "author": "molbap",
    "message": "Fix getter  regression (#40824)\n\n* test things\n\n* style\n\n* move tests to a sane place",
    "sha": "65adc3aaa39dffc029a375ec82f33b4c77729567",
    "files": [
        {
            "sha": "a11f7743ed8efa5ef113fc881def5a3c3161bb7f",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/65adc3aaa39dffc029a375ec82f33b4c77729567/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65adc3aaa39dffc029a375ec82f33b4c77729567/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=65adc3aaa39dffc029a375ec82f33b4c77729567",
            "patch": "@@ -3004,11 +3004,14 @@ def get_decoder(self):\n \n         if hasattr(self, \"model\"):\n             inner = self.model\n-            if hasattr(inner, \"get_decoder\"):\n+            # See: https://github.com/huggingface/transformers/issues/40815\n+            if hasattr(inner, \"get_decoder\") and type(inner) is not type(self):\n                 return inner.get_decoder()\n             return inner\n \n-        return None  # raise AttributeError(f\"{self.__class__.__name__} has no decoder; override `get_decoder()` if needed.\")\n+        # If this is a base transformer model (no decoder/model attributes), return self\n+        # This handles cases like MistralModel which is itself the decoder\n+        return self\n \n     def set_decoder(self, decoder):\n         \"\"\"\n@@ -3027,7 +3030,7 @@ def set_decoder(self, decoder):\n                 self.model = decoder\n             return\n \n-        return  # raise AttributeError(f\"{self.__class__.__name__} cannot accept a decoder; override `set_decoder()`.\")\n+        return\n \n     def _init_weights(self, module):\n         \"\"\""
        },
        {
            "sha": "be55cc563300f4ca871e35d3be5341176c586af4",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 178,
            "deletions": 0,
            "changes": 178,
            "blob_url": "https://github.com/huggingface/transformers/blob/65adc3aaa39dffc029a375ec82f33b4c77729567/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/65adc3aaa39dffc029a375ec82f33b4c77729567/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=65adc3aaa39dffc029a375ec82f33b4c77729567",
            "patch": "@@ -39,16 +39,27 @@\n     AutoModel,\n     AutoModelForImageClassification,\n     AutoModelForSequenceClassification,\n+    BartConfig,\n+    BartForConditionalGeneration,\n     CLIPTextModelWithProjection,\n     DynamicCache,\n+    GPT2Config,\n+    GPT2LMHeadModel,\n+    LlavaConfig,\n     LlavaForConditionalGeneration,\n+    MistralConfig,\n     MistralForCausalLM,\n+    OPTConfig,\n+    OPTForCausalLM,\n     OwlViTForObjectDetection,\n     PretrainedConfig,\n+    T5Config,\n+    T5ForConditionalGeneration,\n     is_torch_available,\n     logging,\n )\n from transformers.modeling_flash_attention_utils import is_flash_attn_available\n+from transformers.models.mistral.modeling_mistral import MistralModel\n from transformers.testing_utils import (\n     TOKEN,\n     CaptureLogger,\n@@ -2871,3 +2882,170 @@ def forward(self, hidden_states, attention_mask):\n             model.save_pretrained(tmpdirname)\n             model = MyModel.from_pretrained(tmpdirname)\n             self.assertEqual(model.my_layer.some_counter, 42)\n+\n+\n+class TestGetDecoder(unittest.TestCase):\n+    def test_causal_lm_get_decoder_returns_underlying_model(self):\n+        cfg = MistralConfig(\n+            vocab_size=128,\n+            hidden_size=32,\n+            intermediate_size=64,\n+            num_hidden_layers=2,\n+            num_attention_heads=4,\n+        )\n+        model = MistralForCausalLM(cfg)\n+        dec = model.get_decoder()\n+\n+        assert dec is model.model, f\"Expected get_decoder() to return model.model, got {type(dec)}\"\n+\n+    def test_seq2seq_get_decoder_still_returns_decoder_module(self):\n+        cfg = BartConfig(\n+            vocab_size=128,\n+            d_model=32,\n+            encoder_layers=2,\n+            decoder_layers=2,\n+            encoder_attention_heads=4,\n+            decoder_attention_heads=4,\n+            encoder_ffn_dim=64,\n+            decoder_ffn_dim=64,\n+        )\n+        model = BartForConditionalGeneration(cfg)\n+        dec = model.get_decoder()\n+\n+        assert dec is model.model.decoder, \"Seq2seq get_decoder() should return the decoder submodule\"\n+\n+    def test_base_model_returns_self(self):\n+        \"\"\"Test that base transformer models (no decoder/model attributes) return self.\"\"\"\n+        cfg = MistralConfig(\n+            vocab_size=128,\n+            hidden_size=32,\n+            intermediate_size=64,\n+            num_hidden_layers=2,\n+            num_attention_heads=4,\n+        )\n+        base_model = MistralModel(cfg)\n+        dec = base_model.get_decoder()\n+\n+        assert dec is base_model, f\"Base model get_decoder() should return self, got {type(dec)}\"\n+\n+    def test_explicit_decoder_attribute_opt(self):\n+        \"\"\"Test models with explicit decoder attribute (OPT style).\"\"\"\n+        cfg = OPTConfig(\n+            vocab_size=128,\n+            hidden_size=32,\n+            ffn_dim=64,\n+            num_hidden_layers=2,\n+            num_attention_heads=4,\n+            max_position_embeddings=512,\n+        )\n+        model = OPTForCausalLM(cfg)\n+        dec = model.get_decoder()\n+\n+        assert dec is model.model.decoder, f\"OPT get_decoder() should return model.decoder, got {type(dec)}\"\n+\n+    def test_explicit_decoder_attribute_t5(self):\n+        \"\"\"Test encoder-decoder models with explicit decoder attribute.\"\"\"\n+        cfg = T5Config(\n+            vocab_size=128,\n+            d_model=32,\n+            d_ff=64,\n+            num_layers=2,\n+            num_heads=4,\n+        )\n+        model = T5ForConditionalGeneration(cfg)\n+        dec = model.get_decoder()\n+\n+        assert dec is model.decoder, f\"T5 get_decoder() should return decoder attribute, got {type(dec)}\"\n+\n+    def test_same_type_recursion_prevention(self):\n+        \"\"\"Test that same-type recursion is prevented (see issue #40815).\"\"\"\n+        cfg = MistralConfig(\n+            vocab_size=128,\n+            hidden_size=32,\n+            intermediate_size=64,\n+            num_hidden_layers=2,\n+            num_attention_heads=4,\n+        )\n+        model = MistralForCausalLM(cfg)\n+\n+        assert type(model) is not type(model.model), \"Types should be different to prevent recursion\"\n+\n+        dec = model.get_decoder()\n+        assert dec is model.model, f\"Should return model.model without infinite recursion, got {type(dec)}\"\n+\n+        inner_dec = model.model.get_decoder()\n+        assert inner_dec is model.model, f\"Inner model should return itself, got {type(inner_dec)}\"\n+\n+    def test_nested_wrapper_recursion(self):\n+        \"\"\"Test models that don't have model/decoder attributes return self.\"\"\"\n+        cfg = GPT2Config(\n+            vocab_size=128,\n+            n_embd=32,\n+            n_layer=2,\n+            n_head=4,\n+            n_positions=512,\n+        )\n+        model = GPT2LMHeadModel(cfg)\n+        dec = model.get_decoder()\n+\n+        assert dec is model, f\"GPT2 get_decoder() should return self (fallback), got {type(dec)}\"\n+\n+    def test_model_without_get_decoder(self):\n+        \"\"\"Test edge case where model has model attribute but no get_decoder method.\"\"\"\n+\n+        class MockInnerModel:\n+            \"\"\"Mock model without get_decoder method.\"\"\"\n+\n+            pass\n+\n+        class MockWrapperModel:\n+            \"\"\"Mock wrapper with model attribute but inner has no get_decoder.\"\"\"\n+\n+            def __init__(self):\n+                self.model = MockInnerModel()\n+\n+            def get_decoder(self):\n+                if hasattr(self, \"decoder\"):\n+                    return self.decoder\n+                if hasattr(self, \"model\"):\n+                    inner = self.model\n+                    if hasattr(inner, \"get_decoder\") and type(inner) is not type(self):\n+                        return inner.get_decoder()\n+                    return inner\n+                return self\n+\n+        wrapper = MockWrapperModel()\n+        dec = wrapper.get_decoder()\n+\n+        assert dec is wrapper.model, f\"Should return inner model when no get_decoder, got {type(dec)}\"\n+\n+    def test_vision_language_model(self):\n+        \"\"\"Test vision-language models like LLaVA that delegate to language_model.\"\"\"\n+        text_config = MistralConfig(\n+            vocab_size=128,\n+            hidden_size=32,\n+            intermediate_size=64,\n+            num_hidden_layers=2,\n+            num_attention_heads=4,\n+        )\n+\n+        vision_config = {\n+            \"hidden_size\": 32,\n+            \"intermediate_size\": 64,\n+            \"num_hidden_layers\": 2,\n+            \"num_attention_heads\": 4,\n+            \"num_channels\": 3,\n+            \"image_size\": 224,\n+            \"patch_size\": 16,\n+        }\n+\n+        cfg = LlavaConfig(\n+            text_config=text_config.to_dict(),\n+            vision_config=vision_config,\n+            vocab_size=128,\n+        )\n+\n+        model = LlavaForConditionalGeneration(cfg)\n+        dec = model.get_decoder()\n+\n+        assert dec is model.language_model, f\"LLaVA get_decoder() should return language_model, got {type(dec)}\""
        }
    ],
    "stats": {
        "total": 187,
        "additions": 184,
        "deletions": 3
    }
}