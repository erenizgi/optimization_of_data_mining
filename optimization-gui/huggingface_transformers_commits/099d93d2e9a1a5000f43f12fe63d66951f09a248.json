{
    "author": "qubvel",
    "message": "Grounding DINO Processor standardization (#34853)\n\n* Add input ids to model output\r\n\r\n* Add text preprocessing for processor\r\n\r\n* Fix snippet\r\n\r\n* Add test for equivalence\r\n\r\n* Add type checking guard\r\n\r\n* Fixing typehint\r\n\r\n* Fix test for added `input_ids` in output\r\n\r\n* Add deprecations and \"text_labels\" to output\r\n\r\n* Adjust tests\r\n\r\n* Fix test\r\n\r\n* Update code examples\r\n\r\n* Minor docs and code improvement\r\n\r\n* Remove one-liner functions and rename class to CamelCase\r\n\r\n* Update docstring\r\n\r\n* Fixup",
    "sha": "099d93d2e9a1a5000f43f12fe63d66951f09a248",
    "files": [
        {
            "sha": "d024ff6ba736fe8117ae69478b6750aa54dbe0da",
            "filename": "docs/source/en/model_doc/grounding-dino.md",
            "status": "modified",
            "additions": 12,
            "deletions": 11,
            "changes": 23,
            "blob_url": "https://github.com/huggingface/transformers/blob/099d93d2e9a1a5000f43f12fe63d66951f09a248/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/099d93d2e9a1a5000f43f12fe63d66951f09a248/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgrounding-dino.md?ref=099d93d2e9a1a5000f43f12fe63d66951f09a248",
            "patch": "@@ -56,25 +56,26 @@ Here's how to use the model for zero-shot object detection:\n >>> image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n >>> image = Image.open(requests.get(image_url, stream=True).raw)\n >>> # Check for cats and remote controls\n->>> text = \"a cat. a remote control.\"\n+>>> text_labels = [[\"a cat\", \"a remote control\"]]\n \n->>> inputs = processor(images=image, text=text, return_tensors=\"pt\").to(device)\n+>>> inputs = processor(images=image, text=text_labels, return_tensors=\"pt\").to(device)\n >>> with torch.no_grad():\n ...     outputs = model(**inputs)\n \n >>> results = processor.post_process_grounded_object_detection(\n ...     outputs,\n-...     inputs.input_ids,\n-...     box_threshold=0.4,\n+...     threshold=0.4,\n ...     text_threshold=0.3,\n-...     target_sizes=[image.size[::-1]]\n+...     target_sizes=[(image.height, image.width)]\n ... )\n->>> print(results)\n-[{'boxes': tensor([[344.6959,  23.1090, 637.1833, 374.2751],\n-        [ 12.2666,  51.9145, 316.8582, 472.4392],\n-        [ 38.5742,  70.0015, 176.7838, 118.1806]], device='cuda:0'),\n-  'labels': ['a cat', 'a cat', 'a remote control'],\n-  'scores': tensor([0.4785, 0.4381, 0.4776], device='cuda:0')}]\n+>>> # Retrieve the first image result\n+>>> result = results[0]\n+>>> for box, score, text_label in zip(result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]):\n+...     box = [round(x, 2) for x in box.tolist()]\n+...     print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n+Detected a cat with confidence 0.479 at location [344.7, 23.11, 637.18, 374.28]\n+Detected a cat with confidence 0.438 at location [12.27, 51.91, 316.86, 472.44]\n+Detected a remote control with confidence 0.478 at location [38.57, 70.0, 176.78, 118.18]\n ```\n \n ## Grounded SAM"
        },
        {
            "sha": "695ef41e2e023c9c8446e051ae23a54794c18db5",
            "filename": "src/transformers/models/grounding_dino/modeling_grounding_dino.py",
            "status": "modified",
            "additions": 42,
            "deletions": 30,
            "changes": 72,
            "blob_url": "https://github.com/huggingface/transformers/blob/099d93d2e9a1a5000f43f12fe63d66951f09a248/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/099d93d2e9a1a5000f43f12fe63d66951f09a248/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fmodeling_grounding_dino.py?ref=099d93d2e9a1a5000f43f12fe63d66951f09a248",
            "patch": "@@ -286,7 +286,7 @@ class GroundingDinoObjectDetectionOutput(ModelOutput):\n         pred_boxes (`torch.FloatTensor` of shape `(batch_size, num_queries, 4)`):\n             Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height). These\n             values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding\n-            possible padding). You can use [`~GroundingDinoProcessor.post_process_object_detection`] to retrieve the\n+            possible padding). You can use [`~GroundingDinoProcessor.post_process_grounded_object_detection`] to retrieve the\n             unnormalized bounding boxes.\n         auxiliary_outputs (`List[Dict]`, *optional*):\n             Optional, only returned when auxilary losses are activated (i.e. `config.auxiliary_loss` is set to `True`)\n@@ -331,6 +331,8 @@ class GroundingDinoObjectDetectionOutput(ModelOutput):\n             background).\n         enc_outputs_coord_logits (`torch.FloatTensor` of shape `(batch_size, sequence_length, 4)`, *optional*, returned when `config.two_stage=True`):\n             Logits of predicted bounding boxes coordinates in the first stage.\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+            Encoded candidate labels sequence. Used in processor to post process object detection result.\n     \"\"\"\n \n     loss: Optional[torch.FloatTensor] = None\n@@ -351,6 +353,7 @@ class GroundingDinoObjectDetectionOutput(ModelOutput):\n     encoder_attentions: Optional[Tuple[Tuple[torch.FloatTensor]]] = None\n     enc_outputs_class: Optional[torch.FloatTensor] = None\n     enc_outputs_coord_logits: Optional[torch.FloatTensor] = None\n+    input_ids: Optional[torch.LongTensor] = None\n \n \n # Copied from transformers.models.detr.modeling_detr.DetrFrozenBatchNorm2d with Detr->GroundingDino\n@@ -2546,30 +2549,41 @@ def forward(\n         Examples:\n \n         ```python\n-        >>> from transformers import AutoProcessor, GroundingDinoForObjectDetection\n-        >>> from PIL import Image\n         >>> import requests\n \n-        >>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n-        >>> image = Image.open(requests.get(url, stream=True).raw)\n-        >>> text = \"a cat.\"\n-\n-        >>> processor = AutoProcessor.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n-        >>> model = GroundingDinoForObjectDetection.from_pretrained(\"IDEA-Research/grounding-dino-tiny\")\n-\n-        >>> inputs = processor(images=image, text=text, return_tensors=\"pt\")\n-        >>> outputs = model(**inputs)\n-\n-        >>> # convert outputs (bounding boxes and class logits) to COCO API\n-        >>> target_sizes = torch.tensor([image.size[::-1]])\n-        >>> results = processor.image_processor.post_process_object_detection(\n-        ...     outputs, threshold=0.35, target_sizes=target_sizes\n-        ... )[0]\n-        >>> for score, label, box in zip(results[\"scores\"], results[\"labels\"], results[\"boxes\"]):\n-        ...     box = [round(i, 1) for i in box.tolist()]\n-        ...     print(f\"Detected {label.item()} with confidence \" f\"{round(score.item(), 2)} at location {box}\")\n-        Detected 1 with confidence 0.45 at location [344.8, 23.2, 637.4, 373.8]\n-        Detected 1 with confidence 0.41 at location [11.9, 51.6, 316.6, 472.9]\n+        >>> import torch\n+        >>> from PIL import Image\n+        >>> from transformers import AutoProcessor, AutoModelForZeroShotObjectDetection\n+\n+        >>> model_id = \"IDEA-Research/grounding-dino-tiny\"\n+        >>> device = \"cuda\"\n+\n+        >>> processor = AutoProcessor.from_pretrained(model_id)\n+        >>> model = AutoModelForZeroShotObjectDetection.from_pretrained(model_id).to(device)\n+\n+        >>> image_url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+        >>> image = Image.open(requests.get(image_url, stream=True).raw)\n+        >>> # Check for cats and remote controls\n+        >>> text_labels = [[\"a cat\", \"a remote control\"]]\n+\n+        >>> inputs = processor(images=image, text=text_labels, return_tensors=\"pt\").to(device)\n+        >>> with torch.no_grad():\n+        ...     outputs = model(**inputs)\n+\n+        >>> results = processor.post_process_grounded_object_detection(\n+        ...     outputs,\n+        ...     threshold=0.4,\n+        ...     text_threshold=0.3,\n+        ...     target_sizes=[(image.height, image.width)]\n+        ... )\n+        >>> # Retrieve the first image result\n+        >>> result = results[0]\n+        >>> for box, score, text_label in zip(result[\"boxes\"], result[\"scores\"], result[\"text_labels\"]):\n+        ...     box = [round(x, 2) for x in box.tolist()]\n+        ...     print(f\"Detected {text_label} with confidence {round(score.item(), 3)} at location {box}\")\n+        Detected a cat with confidence 0.479 at location [344.7, 23.11, 637.18, 374.28]\n+        Detected a cat with confidence 0.438 at location [12.27, 51.91, 316.86, 472.44]\n+        Detected a remote control with confidence 0.478 at location [38.57, 70.0, 176.78, 118.18]\n         ```\"\"\"\n         return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n \n@@ -2639,13 +2653,10 @@ def forward(\n             )\n \n         if not return_dict:\n-            if auxiliary_outputs is not None:\n-                output = (logits, pred_boxes) + auxiliary_outputs + outputs\n-            else:\n-                output = (logits, pred_boxes) + outputs\n-            tuple_outputs = ((loss, loss_dict) + output) if loss is not None else output\n-\n-            return tuple_outputs\n+            auxiliary_outputs = auxiliary_outputs if auxiliary_outputs is not None else []\n+            output = [loss, loss_dict, logits, pred_boxes, *auxiliary_outputs, *outputs, input_ids]\n+            output = tuple(out for out in output if out is not None)\n+            return output\n \n         dict_outputs = GroundingDinoObjectDetectionOutput(\n             loss=loss,\n@@ -2666,6 +2677,7 @@ def forward(\n             init_reference_points=outputs.init_reference_points,\n             enc_outputs_class=outputs.enc_outputs_class,\n             enc_outputs_coord_logits=outputs.enc_outputs_coord_logits,\n+            input_ids=input_ids,\n         )\n \n         return dict_outputs"
        },
        {
            "sha": "f21846ab18944f068b4c58855e1257142cbc48de",
            "filename": "src/transformers/models/grounding_dino/processing_grounding_dino.py",
            "status": "modified",
            "additions": 117,
            "deletions": 29,
            "changes": 146,
            "blob_url": "https://github.com/huggingface/transformers/blob/099d93d2e9a1a5000f43f12fe63d66951f09a248/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/099d93d2e9a1a5000f43f12fe63d66951f09a248/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fprocessing_grounding_dino.py?ref=099d93d2e9a1a5000f43f12fe63d66951f09a248",
            "patch": "@@ -17,19 +17,24 @@\n \"\"\"\n \n import pathlib\n-from typing import Dict, List, Optional, Tuple, Union\n+import warnings\n+from typing import TYPE_CHECKING, Dict, List, Optional, Tuple, Union\n \n from ...image_processing_utils import BatchFeature\n from ...image_transforms import center_to_corners_format\n from ...image_utils import AnnotationFormat, ImageInput\n from ...processing_utils import ImagesKwargs, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import BatchEncoding, PreTokenizedInput, TextInput\n from ...utils import TensorType, is_torch_available\n+from ...utils.deprecation import deprecate_kwarg\n \n \n if is_torch_available():\n     import torch\n \n+if TYPE_CHECKING:\n+    from .modeling_grounding_dino import GroundingDinoObjectDetectionOutput\n+\n \n AnnotationType = Dict[str, Union[int, str, List[Dict]]]\n \n@@ -60,6 +65,42 @@ def get_phrases_from_posmap(posmaps, input_ids):\n     return token_ids\n \n \n+def _is_list_of_candidate_labels(text) -> bool:\n+    \"\"\"Check that text is list/tuple of strings and each string is a candidate label and not merged candidate labels text.\n+    Merged candidate labels text is a string with candidate labels separated by a dot.\n+    \"\"\"\n+    if isinstance(text, (list, tuple)):\n+        return all(isinstance(t, str) and \".\" not in t for t in text)\n+    return False\n+\n+\n+def _merge_candidate_labels_text(text: List[str]) -> str:\n+    \"\"\"\n+    Merge candidate labels text into a single string. Ensure all labels are lowercase.\n+    For example, [\"A cat\", \"a dog\"] -> \"a cat. a dog.\"\n+    \"\"\"\n+    labels = [t.strip().lower() for t in text]  # ensure lowercase\n+    merged_labels_str = \". \".join(labels) + \".\"  # join with dot and add a dot at the end\n+    return merged_labels_str\n+\n+\n+class DictWithDeprecationWarning(dict):\n+    message = (\n+        \"The key `labels` is will return integer ids in `GroundingDinoProcessor.post_process_grounded_object_detection` \"\n+        \"output since v4.51.0. Use `text_labels` instead to retrieve string object names.\"\n+    )\n+\n+    def __getitem__(self, key):\n+        if key == \"labels\":\n+            warnings.warn(self.message, FutureWarning)\n+        return super().__getitem__(key)\n+\n+    def get(self, key, *args, **kwargs):\n+        if key == \"labels\":\n+            warnings.warn(self.message, FutureWarning)\n+        return super().get(key, *args, **kwargs)\n+\n+\n class GroundingDinoImagesKwargs(ImagesKwargs, total=False):\n     annotations: Optional[Union[AnnotationType, List[AnnotationType]]]\n     return_segmentation_masks: Optional[bool]\n@@ -120,7 +161,15 @@ def __call__(\n         This method uses [`GroundingDinoImageProcessor.__call__`] method to prepare image(s) for the model, and\n         [`BertTokenizerFast.__call__`] to prepare text for the model.\n \n-        Please refer to the docstring of the above two methods for more information.\n+        Args:\n+            images (`ImageInput`, `List[ImageInput]`, *optional*):\n+                The image or batch of images to be processed. The image might be either PIL image, numpy array or a torch tensor.\n+            text (`TextInput`, `PreTokenizedInput`, `List[TextInput]`, `List[PreTokenizedInput]`, *optional*):\n+                Candidate labels to be detected on the image. The text might be one of the following:\n+                - A list of candidate labels (strings) to be detected on the image (e.g. [\"a cat\", \"a dog\"]).\n+                - A batch of candidate labels to be detected on the batch of images (e.g. [[\"a cat\", \"a dog\"], [\"a car\", \"a person\"]]).\n+                - A merged candidate labels string to be detected on the image, separated by \".\" (e.g. \"a cat. a dog.\").\n+                - A batch of merged candidate labels text to be detected on the batch of images (e.g. [\"a cat. a dog.\", \"a car. a person.\"]).\n         \"\"\"\n         if images is None and text is None:\n             raise ValueError(\"You must specify either text or images.\")\n@@ -138,6 +187,7 @@ def __call__(\n             encoding_image_processor = BatchFeature()\n \n         if text is not None:\n+            text = self._preprocess_input_text(text)\n             text_encoding = self.tokenizer(\n                 text=text,\n                 **output_kwargs[\"text_kwargs\"],\n@@ -149,6 +199,23 @@ def __call__(\n \n         return text_encoding\n \n+    def _preprocess_input_text(self, text):\n+        \"\"\"\n+        Preprocess input text to ensure that labels are in the correct format for the model.\n+        If the text is a list of candidate labels, merge the candidate labels into a single string,\n+        for example, [\"a cat\", \"a dog\"] -> \"a cat. a dog.\". In case candidate labels are already in a form of\n+        \"a cat. a dog.\", the text is returned as is.\n+        \"\"\"\n+\n+        if _is_list_of_candidate_labels(text):\n+            text = _merge_candidate_labels_text(text)\n+\n+        # for batched input\n+        elif isinstance(text, (list, tuple)) and all(_is_list_of_candidate_labels(t) for t in text):\n+            text = [_merge_candidate_labels_text(sample) for sample in text]\n+\n+        return text\n+\n     # Copied from transformers.models.blip.processing_blip.BlipProcessor.batch_decode with BertTokenizerFast->PreTrainedTokenizer\n     def batch_decode(self, *args, **kwargs):\n         \"\"\"\n@@ -172,13 +239,15 @@ def model_input_names(self):\n         image_processor_input_names = self.image_processor.model_input_names\n         return list(dict.fromkeys(tokenizer_input_names + image_processor_input_names))\n \n+    @deprecate_kwarg(\"box_threshold\", new_name=\"threshold\", version=\"4.51.0\")\n     def post_process_grounded_object_detection(\n         self,\n-        outputs,\n-        input_ids,\n-        box_threshold: float = 0.25,\n+        outputs: \"GroundingDinoObjectDetectionOutput\",\n+        input_ids: Optional[TensorType] = None,\n+        threshold: float = 0.25,\n         text_threshold: float = 0.25,\n-        target_sizes: Union[TensorType, List[Tuple]] = None,\n+        target_sizes: Optional[Union[TensorType, List[Tuple]]] = None,\n+        text_labels: Optional[List[List[str]]] = None,\n     ):\n         \"\"\"\n         Converts the raw output of [`GroundingDinoForObjectDetection`] into final bounding boxes in (top_left_x, top_left_y,\n@@ -187,32 +256,38 @@ def post_process_grounded_object_detection(\n         Args:\n             outputs ([`GroundingDinoObjectDetectionOutput`]):\n                 Raw outputs of the model.\n-            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n-                The token ids of the input text.\n-            box_threshold (`float`, *optional*, defaults to 0.25):\n-                Score threshold to keep object detection predictions.\n+            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n+                The token ids of the input text. If not provided will be taken from the model output.\n+            threshold (`float`, *optional*, defaults to 0.25):\n+                Threshold to keep object detection predictions based on confidence score.\n             text_threshold (`float`, *optional*, defaults to 0.25):\n                 Score threshold to keep text detection predictions.\n             target_sizes (`torch.Tensor` or `List[Tuple[int, int]]`, *optional*):\n                 Tensor of shape `(batch_size, 2)` or list of tuples (`Tuple[int, int]`) containing the target size\n                 `(height, width)` of each image in the batch. If unset, predictions will not be resized.\n+            text_labels (`List[List[str]]`, *optional*):\n+                List of candidate labels to be detected on each image. At the moment it's *NOT used*, but required\n+                to be in signature for the zero-shot object detection pipeline. Text labels are instead extracted\n+                from the `input_ids` tensor provided in `outputs`.\n+\n         Returns:\n-            `List[Dict]`: A list of dictionaries, each dictionary containing the scores, labels and boxes for an image\n-            in the batch as predicted by the model.\n+            `List[Dict]`: A list of dictionaries, each dictionary containing the\n+                - **scores**: tensor of confidence scores for detected objects\n+                - **boxes**: tensor of bounding boxes in [x0, y0, x1, y1] format\n+                - **labels**: list of text labels for each detected object (will be replaced with integer ids in v4.51.0)\n+                - **text_labels**: list of text labels for detected objects\n         \"\"\"\n-        logits, boxes = outputs.logits, outputs.pred_boxes\n+        batch_logits, batch_boxes = outputs.logits, outputs.pred_boxes\n+        input_ids = input_ids if input_ids is not None else outputs.input_ids\n \n-        if target_sizes is not None:\n-            if len(logits) != len(target_sizes):\n-                raise ValueError(\n-                    \"Make sure that you pass in as many target sizes as the batch dimension of the logits\"\n-                )\n+        if target_sizes is not None and len(target_sizes) != len(batch_logits):\n+            raise ValueError(\"Make sure that you pass in as many target sizes as the batch dimension of the logits\")\n \n-        probs = torch.sigmoid(logits)  # (batch_size, num_queries, 256)\n-        scores = torch.max(probs, dim=-1)[0]  # (batch_size, num_queries)\n+        batch_probs = torch.sigmoid(batch_logits)  # (batch_size, num_queries, 256)\n+        batch_scores = torch.max(batch_probs, dim=-1)[0]  # (batch_size, num_queries)\n \n         # Convert to [x0, y0, x1, y1] format\n-        boxes = center_to_corners_format(boxes)\n+        batch_boxes = center_to_corners_format(batch_boxes)\n \n         # Convert from relative [0, 1] to absolute [0, height] coordinates\n         if target_sizes is not None:\n@@ -222,17 +297,30 @@ def post_process_grounded_object_detection(\n             else:\n                 img_h, img_w = target_sizes.unbind(1)\n \n-            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(boxes.device)\n-            boxes = boxes * scale_fct[:, None, :]\n+            scale_fct = torch.stack([img_w, img_h, img_w, img_h], dim=1).to(batch_boxes.device)\n+            batch_boxes = batch_boxes * scale_fct[:, None, :]\n \n         results = []\n-        for idx, (s, b, p) in enumerate(zip(scores, boxes, probs)):\n-            score = s[s > box_threshold]\n-            box = b[s > box_threshold]\n-            prob = p[s > box_threshold]\n+        for idx, (scores, boxes, probs) in enumerate(zip(batch_scores, batch_boxes, batch_probs)):\n+            keep = scores > threshold\n+            scores = scores[keep]\n+            boxes = boxes[keep]\n+\n+            # extract text labels\n+            prob = probs[keep]\n             label_ids = get_phrases_from_posmap(prob > text_threshold, input_ids[idx])\n-            label = self.batch_decode(label_ids)\n-            results.append({\"scores\": score, \"labels\": label, \"boxes\": box})\n+            objects_text_labels = self.batch_decode(label_ids)\n+\n+            result = DictWithDeprecationWarning(\n+                {\n+                    \"scores\": scores,\n+                    \"boxes\": boxes,\n+                    \"text_labels\": objects_text_labels,\n+                    # TODO: @pavel, set labels to None since v4.51.0 or find a way to extract ids\n+                    \"labels\": objects_text_labels,\n+                }\n+            )\n+            results.append(result)\n \n         return results\n "
        },
        {
            "sha": "30a8d44c8e90ad6ceccb2827624d8208d8d1e8ee",
            "filename": "tests/models/grounding_dino/test_modeling_grounding_dino.py",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/099d93d2e9a1a5000f43f12fe63d66951f09a248/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/099d93d2e9a1a5000f43f12fe63d66951f09a248/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_modeling_grounding_dino.py?ref=099d93d2e9a1a5000f43f12fe63d66951f09a248",
            "patch": "@@ -322,9 +322,9 @@ def test_attention_outputs(self):\n             # loss is at first position\n             if \"labels\" in inputs_dict:\n                 correct_outlen += 1  # loss is added to beginning\n-            # Object Detection model returns pred_logits and pred_boxes\n+            # Object Detection model returns pred_logits and pred_boxes and input_ids\n             if model_class.__name__ == \"GroundingDinoForObjectDetection\":\n-                correct_outlen += 2\n+                correct_outlen += 3\n \n             self.assertEqual(out_len, correct_outlen)\n \n@@ -653,7 +653,7 @@ def test_inference_object_detection_head(self):\n \n         # verify postprocessing\n         results = processor.image_processor.post_process_object_detection(\n-            outputs, threshold=0.35, target_sizes=[image.size[::-1]]\n+            outputs, threshold=0.35, target_sizes=[(image.height, image.width)]\n         )[0]\n         expected_scores = torch.tensor([0.4526, 0.4082]).to(torch_device)\n         expected_slice_boxes = torch.tensor([344.8143, 23.1796, 637.4004, 373.8295]).to(torch_device)\n@@ -667,14 +667,14 @@ def test_inference_object_detection_head(self):\n         results = processor.post_process_grounded_object_detection(\n             outputs=outputs,\n             input_ids=encoding.input_ids,\n-            box_threshold=0.35,\n+            threshold=0.35,\n             text_threshold=0.3,\n-            target_sizes=[image.size[::-1]],\n+            target_sizes=[(image.height, image.width)],\n         )[0]\n \n         self.assertTrue(torch.allclose(results[\"scores\"], expected_scores, atol=1e-3))\n         self.assertTrue(torch.allclose(results[\"boxes\"][0, :], expected_slice_boxes, atol=1e-2))\n-        self.assertListEqual(results[\"labels\"], expected_labels)\n+        self.assertListEqual(results[\"text_labels\"], expected_labels)\n \n     @require_torch_accelerator\n     def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n@@ -706,11 +706,11 @@ def test_inference_object_detection_head_equivalence_cpu_gpu(self):\n \n         # assert postprocessing\n         results_cpu = processor.image_processor.post_process_object_detection(\n-            cpu_outputs, threshold=0.35, target_sizes=[image.size[::-1]]\n+            cpu_outputs, threshold=0.35, target_sizes=[(image.height, image.width)]\n         )[0]\n \n         result_gpu = processor.image_processor.post_process_object_detection(\n-            gpu_outputs, threshold=0.35, target_sizes=[image.size[::-1]]\n+            gpu_outputs, threshold=0.35, target_sizes=[(image.height, image.width)]\n         )[0]\n \n         self.assertTrue(torch.allclose(results_cpu[\"scores\"], result_gpu[\"scores\"].cpu(), atol=1e-3))"
        },
        {
            "sha": "8f9ced4b0c48a3d6a7728484a0be87296ab21d0d",
            "filename": "tests/models/grounding_dino/test_processor_grounding_dino.py",
            "status": "modified",
            "additions": 41,
            "deletions": 5,
            "changes": 46,
            "blob_url": "https://github.com/huggingface/transformers/blob/099d93d2e9a1a5000f43f12fe63d66951f09a248/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/099d93d2e9a1a5000f43f12fe63d66951f09a248/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgrounding_dino%2Ftest_processor_grounding_dino.py?ref=099d93d2e9a1a5000f43f12fe63d66951f09a248",
            "patch": "@@ -17,6 +17,7 @@\n import shutil\n import tempfile\n import unittest\n+from typing import Optional\n \n import pytest\n \n@@ -77,6 +78,20 @@ def setUp(self):\n         self.embed_dim = 5\n         self.seq_length = 5\n \n+    def prepare_text_inputs(self, batch_size: Optional[int] = None):\n+        labels = [\"a cat\", \"remote control\"]\n+        labels_longer = [\"a person\", \"a car\", \"a dog\", \"a cat\"]\n+\n+        if batch_size is None:\n+            return labels\n+\n+        if batch_size < 1:\n+            raise ValueError(\"batch_size must be greater than 0\")\n+\n+        if batch_size == 1:\n+            return [labels]\n+        return [labels, labels_longer] + [labels] * (batch_size - 2)\n+\n     # Copied from tests.models.clip.test_processor_clip.CLIPProcessorTest.get_tokenizer with CLIP->Bert\n     def get_tokenizer(self, **kwargs):\n         return BertTokenizer.from_pretrained(self.tmpdirname, **kwargs)\n@@ -98,6 +113,7 @@ def get_fake_grounding_dino_output(self):\n         return GroundingDinoObjectDetectionOutput(\n             pred_boxes=torch.rand(self.batch_size, self.num_queries, 4),\n             logits=torch.rand(self.batch_size, self.num_queries, self.embed_dim),\n+            input_ids=self.get_fake_grounding_dino_input_ids(),\n         )\n \n     def get_fake_grounding_dino_input_ids(self):\n@@ -111,14 +127,11 @@ def test_post_process_grounded_object_detection(self):\n         processor = GroundingDinoProcessor(tokenizer=tokenizer, image_processor=image_processor)\n \n         grounding_dino_output = self.get_fake_grounding_dino_output()\n-        grounding_dino_input_ids = self.get_fake_grounding_dino_input_ids()\n \n-        post_processed = processor.post_process_grounded_object_detection(\n-            grounding_dino_output, grounding_dino_input_ids\n-        )\n+        post_processed = processor.post_process_grounded_object_detection(grounding_dino_output)\n \n         self.assertEqual(len(post_processed), self.batch_size)\n-        self.assertEqual(list(post_processed[0].keys()), [\"scores\", \"labels\", \"boxes\"])\n+        self.assertEqual(list(post_processed[0].keys()), [\"scores\", \"boxes\", \"text_labels\", \"labels\"])\n         self.assertEqual(post_processed[0][\"boxes\"].shape, (self.num_queries, 4))\n         self.assertEqual(post_processed[0][\"scores\"].shape, (self.num_queries,))\n \n@@ -248,3 +261,26 @@ def test_model_input_names(self):\n         inputs = processor(text=input_str, images=image_input)\n \n         self.assertListEqual(list(inputs.keys()), processor.model_input_names)\n+\n+    def test_text_preprocessing_equivalence(self):\n+        processor = GroundingDinoProcessor.from_pretrained(self.tmpdirname)\n+\n+        # check for single input\n+        formatted_labels = \"a cat. a remote control.\"\n+        labels = [\"a cat\", \"a remote control\"]\n+        inputs1 = processor(text=formatted_labels, return_tensors=\"pt\")\n+        inputs2 = processor(text=labels, return_tensors=\"pt\")\n+        self.assertTrue(\n+            torch.allclose(inputs1[\"input_ids\"], inputs2[\"input_ids\"]),\n+            f\"Input ids are not equal for single input: {inputs1['input_ids']} != {inputs2['input_ids']}\",\n+        )\n+\n+        # check for batched input\n+        formatted_labels = [\"a cat. a remote control.\", \"a car. a person.\"]\n+        labels = [[\"a cat\", \"a remote control\"], [\"a car\", \"a person\"]]\n+        inputs1 = processor(text=formatted_labels, return_tensors=\"pt\", padding=True)\n+        inputs2 = processor(text=labels, return_tensors=\"pt\", padding=True)\n+        self.assertTrue(\n+            torch.allclose(inputs1[\"input_ids\"], inputs2[\"input_ids\"]),\n+            f\"Input ids are not equal for batched input: {inputs1['input_ids']} != {inputs2['input_ids']}\",\n+        )"
        }
    ],
    "stats": {
        "total": 303,
        "additions": 220,
        "deletions": 83
    }
}