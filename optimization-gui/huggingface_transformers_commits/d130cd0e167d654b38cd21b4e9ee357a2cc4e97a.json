{
    "author": "itazap",
    "message": "update error msg (#37207)",
    "sha": "d130cd0e167d654b38cd21b4e9ee357a2cc4e97a",
    "files": [
        {
            "sha": "c8cc1cdbe97ba150f62230eea53b67ab80dd1edc",
            "filename": "src/transformers/convert_slow_tokenizer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d130cd0e167d654b38cd21b4e9ee357a2cc4e97a/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d130cd0e167d654b38cd21b4e9ee357a2cc4e97a/src%2Ftransformers%2Fconvert_slow_tokenizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconvert_slow_tokenizer.py?ref=d130cd0e167d654b38cd21b4e9ee357a2cc4e97a",
            "patch": "@@ -1730,7 +1730,7 @@ def convert_slow_tokenizer(transformer_tokenizer, from_tiktoken=False) -> Tokeni\n             ).converted()\n         except Exception:\n             raise ValueError(\n-                f\"Converting from Tiktoken failed, if a converter for SentencePiece is available, provide a model path \"\n+                f\"Converting from SentencePiece and Tiktoken failed, if a converter for SentencePiece is available, provide a model path \"\n                 f\"with a SentencePiece tokenizer.model file.\"\n                 f\"Currently available slow->fast converters: {list(SLOW_TO_FAST_CONVERTERS.keys())}\"\n             )"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}