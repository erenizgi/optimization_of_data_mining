{
    "author": "ydshieh",
    "message": "[testing] fix `cwm` (#42261)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "f0d026f033388829a552eec57f5ad8dd8e105068",
    "files": [
        {
            "sha": "1381454a2b6a09627df44f3bcfedb20a7ee215a8",
            "filename": "tests/models/cwm/test_modeling_cwm.py",
            "status": "modified",
            "additions": 21,
            "deletions": 76,
            "changes": 97,
            "blob_url": "https://github.com/huggingface/transformers/blob/f0d026f033388829a552eec57f5ad8dd8e105068/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f0d026f033388829a552eec57f5ad8dd8e105068/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcwm%2Ftest_modeling_cwm.py?ref=f0d026f033388829a552eec57f5ad8dd8e105068",
            "patch": "@@ -17,6 +17,7 @@\n from transformers import is_torch_available\n from transformers.testing_utils import (\n     cleanup,\n+    require_read_token,\n     require_torch,\n     require_torch_accelerator,\n     slow,\n@@ -85,6 +86,7 @@ class CwmModelTest(CausalLMModelTest, unittest.TestCase):\n \n @require_torch_accelerator\n @slow\n+@require_read_token\n class CwmIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         cleanup(torch_device, gc_collect=True)\n@@ -116,45 +118,14 @@ def test_cwm_integration(self):\n         with torch.no_grad():\n             out = model(**inputs)\n \n+        # fmt: off\n         expected_logits = torch.tensor(\n-            [\n-                0.5625,\n-                2.9531,\n-                9.1875,\n-                0.4746,\n-                -0.3613,\n-                2.2031,\n-                2.9844,\n-                1.5312,\n-                0.5859,\n-                1.5391,\n-                2.7500,\n-                3.4375,\n-                2.0156,\n-                2.1719,\n-                1.5469,\n-                2.5469,\n-                2.8438,\n-                1.8203,\n-                1.7188,\n-                1.3984,\n-                1.0469,\n-                0.1748,\n-                0.4453,\n-                0.1533,\n-                -0.1157,\n-                0.8516,\n-                2.2344,\n-                5.2188,\n-                1.2891,\n-                1.5234,\n-                0.8555,\n-                0.6992,\n-            ],\n+            [0.5625, 2.9531, 9.1875, 0.5039, -0.3262, 2.2344, 3.0312, 1.5312, 0.5664, 1.5625, 2.7656, 3.4219, 2.0312, 2.1719, 1.5391, 2.5469, 2.8281, 1.8125, 1.7109, 1.3906, 1.0391, 0.1621, 0.4277, 0.1455, -0.1230, 0.8477, 2.2344, 5.2188, 1.2969, 1.5547, 0.8516, 0.7148],\n             dtype=torch.bfloat16,\n         ).to(model.device)\n+        # fmt: on\n \n-        self.assertTrue(torch.allclose(out.logits[0, -1, :32], expected_logits, atol=1e-2, rtol=1e-2))\n+        torch.testing.assert_close(out.logits[0, -1, :32], expected_logits, atol=1e-2, rtol=1e-2)\n \n         self.assertEqual(out.logits.shape[1], inputs.input_ids.shape[1])\n         self.assertEqual(out.logits.shape[2], model.config.vocab_size)\n@@ -166,10 +137,13 @@ def test_cwm_sliding_window_long_sequence(self):\n         from transformers import AutoTokenizer\n \n         tokenizer = AutoTokenizer.from_pretrained(\"facebook/cwm\")\n-        model = CwmForCausalLM.from_pretrained(\"facebook/cwm\", device_map=\"auto\", dtype=torch.bfloat16)\n+        # original `sliding_window` is `8192`, but it causes GPU OOM on A10\n+        model = CwmForCausalLM.from_pretrained(\n+            \"facebook/cwm\", device_map=\"auto\", dtype=torch.bfloat16, sliding_window=4096\n+        )\n \n         sliding_window = model.config.sliding_window\n-        long_text = \"for i in range(1000):\\n    print(f'iteration {i}')\\n\" * 600\n+        long_text = \"for i in range(1000):\\n    print(f'iteration {i}')\\n\" * 270\n \n         inputs = tokenizer(long_text, return_tensors=\"pt\").to(model.device)\n         seq_len = inputs.input_ids.shape[1]\n@@ -182,50 +156,21 @@ def test_cwm_sliding_window_long_sequence(self):\n         with torch.no_grad():\n             out = model(**inputs)\n \n+        # fmt: off\n         expected_logits = torch.tensor(\n-            [\n-                4.7812,\n-                6.1875,\n-                13.1875,\n-                4.4062,\n-                5.0312,\n-                3.9844,\n-                6.6875,\n-                4.8438,\n-                2.3125,\n-                6.5000,\n-                4.4688,\n-                0.5195,\n-                5.6562,\n-                3.3125,\n-                2.7500,\n-                4.9062,\n-                5.5938,\n-                4.1562,\n-                3.9531,\n-                2.4062,\n-                3.2812,\n-                2.8594,\n-                3.4688,\n-                2.9688,\n-                2.6875,\n-                3.4531,\n-                2.7344,\n-                7.2812,\n-                4.5000,\n-                5.7500,\n-                2.3438,\n-                5.9688,\n-            ],\n+            [5.2812, 6.4688, 12.8125, 4.6875, 5.2500, 4.2500, 6.9688, 4.9375, 2.7656, 6.5938, 4.9688, 1.1016, 5.9375, 3.7500, 3.1094, 5.5312, 6.1250, 4.7500, 4.5312, 2.8281, 4.0625, 3.3125, 3.9219, 3.3906, 3.1406, 3.6719, 3.2031, 7.0938, 4.8750, 6.0000, 2.7188, 6.2500],\n             dtype=torch.bfloat16,\n         ).to(model.device)\n+        # fmt: on\n \n-        self.assertTrue(torch.allclose(out.logits[0, -1, :32], expected_logits, atol=1e-2, rtol=1e-2))\n+        torch.testing.assert_close(out.logits[0, -1, :32], expected_logits, atol=1e-2, rtol=1e-2)\n \n-        self.assertEqual(out.logits.shape[1], seq_len)\n-        self.assertEqual(out.logits.shape[2], model.config.vocab_size)\n-        self.assertFalse(torch.isnan(out.logits).any())\n-        self.assertFalse(torch.isinf(out.logits).any())\n+        logits = out.logits.to(\"cpu\")\n+\n+        self.assertEqual(logits.shape[1], seq_len)\n+        self.assertEqual(logits.shape[2], model.config.vocab_size)\n+        self.assertFalse(torch.isnan(logits).any())\n+        self.assertFalse(torch.isinf(logits).any())\n \n         for i, layer in enumerate(model.model.layers):\n             if model.config.layer_types[i] == \"sliding_attention\":"
        }
    ],
    "stats": {
        "total": 97,
        "additions": 21,
        "deletions": 76
    }
}