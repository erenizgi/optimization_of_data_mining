{
    "author": "sanchit-gandhi",
    "message": "[whisper] alternative fix for long-form timestamps (#32131)\n\n* [whisper] alternative fix for long-form timestamps\r\n\r\n* update test",
    "sha": "51d15eb1c149f2d31b08766e7a21d5cebaa55b17",
    "files": [
        {
            "sha": "5cfd2300346e3423bf28b77dec41875f73692630",
            "filename": "src/transformers/models/whisper/tokenization_whisper.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/51d15eb1c149f2d31b08766e7a21d5cebaa55b17/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51d15eb1c149f2d31b08766e7a21d5cebaa55b17/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper.py?ref=51d15eb1c149f2d31b08766e7a21d5cebaa55b17",
            "patch": "@@ -587,11 +587,20 @@ def _compute_offsets(self, token_ids, time_precision=0.02):\n             consecutive = np.append(consecutive, np.where(timestamp_tokens)[0][-1] + 1)\n \n         last_slice = np.where(timestamp_tokens)[0][0]\n+        cur_max_timestamp = 0\n+        prev_segments_len = 0\n         for current_slice in consecutive:\n             sliced_tokens = token_ids[last_slice:current_slice]\n             if len(sliced_tokens) > 1:\n                 start_timestamp_position = sliced_tokens[0].item() - timestamp_begin\n                 end_timestamp_position = sliced_tokens[-1].item() - timestamp_begin\n+\n+                if start_timestamp_position < cur_max_timestamp:\n+                    # next segment has started\n+                    prev_segments_len += cur_max_timestamp\n+\n+                cur_max_timestamp = end_timestamp_position\n+\n                 # strip timestamp tokens from the text output\n                 sliced_tokens = self._preprocess_token_ids(sliced_tokens)\n                 text = self._decode(sliced_tokens)\n@@ -600,8 +609,8 @@ def _compute_offsets(self, token_ids, time_precision=0.02):\n                     {\n                         \"text\": text,\n                         \"timestamp\": (\n-                            start_timestamp_position * time_precision,\n-                            end_timestamp_position * time_precision,\n+                            (start_timestamp_position + prev_segments_len) * time_precision,\n+                            (end_timestamp_position + prev_segments_len) * time_precision,\n                         ),\n                     }\n                 )"
        },
        {
            "sha": "6b6fb3a199003a916192bde6263c16dc43ddf960",
            "filename": "src/transformers/models/whisper/tokenization_whisper_fast.py",
            "status": "modified",
            "additions": 11,
            "deletions": 2,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/51d15eb1c149f2d31b08766e7a21d5cebaa55b17/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51d15eb1c149f2d31b08766e7a21d5cebaa55b17/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Ftokenization_whisper_fast.py?ref=51d15eb1c149f2d31b08766e7a21d5cebaa55b17",
            "patch": "@@ -229,11 +229,20 @@ def _compute_offsets(self, token_ids, time_precision=0.02):\n             consecutive = np.append(consecutive, np.where(timestamp_tokens)[0][-1] + 1)\n \n         last_slice = np.where(timestamp_tokens)[0][0]\n+        cur_max_timestamp = 0\n+        prev_segments_len = 0\n         for current_slice in consecutive:\n             sliced_tokens = token_ids[last_slice:current_slice]\n             if len(sliced_tokens) > 1:\n                 start_timestamp_position = sliced_tokens[0].item() - timestamp_begin\n                 end_timestamp_position = sliced_tokens[-1].item() - timestamp_begin\n+\n+                if start_timestamp_position < cur_max_timestamp:\n+                    # next segment has started\n+                    prev_segments_len += cur_max_timestamp\n+\n+                cur_max_timestamp = end_timestamp_position\n+\n                 # strip timestamp tokens from the text output\n                 sliced_tokens = self._preprocess_token_ids(sliced_tokens)\n                 text = self._decode(sliced_tokens)\n@@ -242,8 +251,8 @@ def _compute_offsets(self, token_ids, time_precision=0.02):\n                     {\n                         \"text\": text,\n                         \"timestamp\": (\n-                            start_timestamp_position * time_precision,\n-                            end_timestamp_position * time_precision,\n+                            (start_timestamp_position + prev_segments_len) * time_precision,\n+                            (end_timestamp_position + prev_segments_len) * time_precision,\n                         ),\n                     }\n                 )"
        },
        {
            "sha": "66a930499f73d16c3c086b3c05d5a44652f44919",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 59,
            "deletions": 0,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/51d15eb1c149f2d31b08766e7a21d5cebaa55b17/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/51d15eb1c149f2d31b08766e7a21d5cebaa55b17/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=51d15eb1c149f2d31b08766e7a21d5cebaa55b17",
            "patch": "@@ -2099,6 +2099,65 @@ def test_tiny_timestamp_generation(self):\n         transcript = processor.batch_decode(generated_ids, skip_special_tokens=True, output_offsets=True)\n         self.assertEqual(transcript, EXPECTED_TRANSCRIPT)\n \n+    @slow\n+    def test_tiny_longform_timestamps_generation(self):\n+        set_seed(0)\n+        processor = WhisperProcessor.from_pretrained(\"openai/whisper-tiny\")\n+        model = WhisperForConditionalGeneration.from_pretrained(\"openai/whisper-tiny\")\n+        model.to(torch_device)\n+\n+        dataset = load_dataset(\"distil-whisper/librispeech_long\", \"clean\", split=\"validation\")\n+        sample = dataset[0][\"audio\"]\n+\n+        input_features = processor(\n+            sample[\"array\"], return_tensors=\"pt\", truncation=False, sampling_rate=sample[\"sampling_rate\"]\n+        )\n+        input_features = input_features.to(torch_device)\n+\n+        generated_ids = model.generate(**input_features, return_timestamps=True, return_segments=True)\n+\n+        EXPECTED_TRANSCRIPT = [\n+            {\n+                \"text\": \" Mr. Quilter is the apostle of the middle classes, and we are glad to welcome his gospel.\",\n+                \"timestamp\": (0.0, 6.5600000000000005),\n+            },\n+            {\n+                \"text\": \" Nor is Mr. Quilter's manner less interesting than his matter.\",\n+                \"timestamp\": (6.5600000000000005, 11.24),\n+            },\n+            {\n+                \"text\": \" He tells us that at this festive season of the year, with Christmas and roast beef looming\",\n+                \"timestamp\": (11.24, 16.88),\n+            },\n+            {\n+                \"text\": \" before us, similarly drawn from eating and its results occur most readily to the mind.\",\n+                \"timestamp\": (16.88, 23.76),\n+            },\n+            {\n+                \"text\": \" He has grave doubts whether Sir Frederick Latins' work is really Greek after all, and\",\n+                \"timestamp\": (23.76, 29.44),\n+            },\n+            {\"text\": \" can discover in it but little of rocky ithaka.\", \"timestamp\": (29.44, 33.72)},\n+            {\n+                \"text\": \" Lennils, pictures, are a sort of upguards and atom paintings, and Mason's exquisite itals\",\n+                \"timestamp\": (33.72, 40.32),\n+            },\n+            {\"text\": \" are as national as a jingo poem.\", \"timestamp\": (40.32, 44.72)},\n+            {\n+                \"text\": \" Mr. Birkut Foster's landscapes smile at one much in the same way that Mr. Carker used\",\n+                \"timestamp\": (44.72, 50.4),\n+            },\n+            {\"text\": \" to flash his teeth.\", \"timestamp\": (50.4, 52.96)},\n+            {\n+                \"text\": \" And Mr. John Collier gives his sitter a cheerful slap on the back before he says, like\",\n+                \"timestamp\": (52.96, 58.68),\n+            },\n+            {\"text\": \" a shampoo and a Turkish bath next man.\", \"timestamp\": (58.68, 61.96)},\n+        ]\n+\n+        transcript = processor.batch_decode(generated_ids[\"sequences\"], skip_special_tokens=True, output_offsets=True)\n+        self.assertEqual(transcript[0][\"offsets\"], EXPECTED_TRANSCRIPT)\n+\n     @slow\n     def test_large_timestamp_generation(self):\n         set_seed(0)"
        }
    ],
    "stats": {
        "total": 85,
        "additions": 81,
        "deletions": 4
    }
}