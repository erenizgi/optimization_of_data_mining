{
    "author": "zucchini-nlp",
    "message": "ðŸš¨ Generation config defaults are now `None` (#42702)\n\n* this way betetr maybe?\n\n* delete legacy from bart and mvp\n\n* import not found\n\n* fix some tests\n\n* fix more tests\n\n* revert smth to run tests again\n\n* i though I fixed it already, but there were more models\n\n* commit and check tests, clean-up later\n\n* assisted deocding shoudl work now\n\n* docs and whisper\n\n* fix a few more tests\n\n* no circular import errors pls\n\n* wording\n\n* add a test for defaults following TRL example\n\n* nit\n\n* Update src/transformers/configuration_utils.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update src/transformers/generation/candidate_generator.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* Update src/transformers/generation/utils.py\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>\n\n* comments\n\n* final fix tests\n\n* more comments\n\n---------\n\nCo-authored-by: Anton Vlasjuk <73884904+vasqu@users.noreply.github.com>",
    "sha": "a81e04a92371a6b1ba80018f012b2467bab5d61f",
    "files": [
        {
            "sha": "ac395a455032087350e6428afb437a79d44453e4",
            "filename": "examples/pytorch/continuous_batching.py",
            "status": "modified",
            "additions": 1,
            "deletions": 3,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/examples%2Fpytorch%2Fcontinuous_batching.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/examples%2Fpytorch%2Fcontinuous_batching.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fpytorch%2Fcontinuous_batching.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -45,9 +45,7 @@ def generate_without_cb(\n         key = \" \".join(map(str, input_ids))  # This will be used to identify the output after batched generation\n         input_ids = torch.tensor([input_ids]).to(\"cuda\")\n         attention_mask = torch.ones_like(input_ids)\n-        outputs = model.generate(\n-            input_ids, attention_mask=attention_mask, generation_config=generation_config, use_model_defaults=False\n-        )\n+        outputs = model.generate(input_ids, attention_mask=attention_mask, generation_config=generation_config)\n         generated_tokens = outputs[0][input_ids.shape[1] :]\n         decoded_outputs[key] = tokenizer.decode(generated_tokens, skip_special_tokens=False)\n     return decoded_outputs"
        },
        {
            "sha": "2e745f4689183550f6aefc16a889a0f34ab7a005",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 14,
            "deletions": 56,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -25,6 +25,7 @@\n \n from . import __version__\n from .dynamic_module_utils import custom_object_save\n+from .generation.configuration_utils import GenerationConfig\n from .modeling_gguf_pytorch_utils import load_gguf_checkpoint\n from .modeling_rope_utils import RotaryEmbeddingConfigMixin\n from .utils import (\n@@ -310,7 +311,7 @@ def __init__(\n         self.decoder_start_token_id = decoder_start_token_id\n \n         # Parameters for sequence generation saved in the config are popped instead of loading them.\n-        for parameter_name in self._get_global_generation_defaults().keys():\n+        for parameter_name in GenerationConfig._get_default_generation_params().keys():\n             kwargs.pop(parameter_name, None)\n \n         # Name or path to the pretrained checkpoint\n@@ -449,13 +450,11 @@ def save_pretrained(self, save_directory: str | os.PathLike, push_to_hub: bool =\n         if os.path.isfile(save_directory):\n             raise AssertionError(f\"Provided path ({save_directory}) should be a directory, not a file\")\n \n-        non_default_generation_parameters = self._get_non_default_generation_parameters()\n-        if len(non_default_generation_parameters) > 0:\n+        generation_parameters = self._get_generation_parameters()\n+        if len(generation_parameters) > 0:\n             raise ValueError(\n-                \"Some non-default generation parameters are set in the model config. These should go into either a) \"\n-                \"`model.generation_config` (as opposed to `model.config`); OR b) a GenerationConfig file \"\n-                \"(https://huggingface.co/docs/transformers/generation_strategies#save-a-custom-decoding-strategy-with-your-model).\"\n-                f\"\\nNon-default generation parameters: {str(non_default_generation_parameters)}\",\n+                \"Some generation parameters are set in the model config. These should go into `model.generation_config`\"\n+                f\"as opposed to `model.config`. \\nGeneration parameters found: {str(generation_parameters)}\",\n             )\n \n         os.makedirs(save_directory, exist_ok=True)\n@@ -1059,58 +1058,17 @@ def register_for_auto_class(cls, auto_class=\"AutoConfig\"):\n \n         cls._auto_class = auto_class\n \n-    @staticmethod\n-    def _get_global_generation_defaults() -> dict[str, Any]:\n-        return {\n-            \"max_length\": 20,\n-            \"min_length\": 0,\n-            \"do_sample\": False,\n-            \"early_stopping\": False,\n-            \"num_beams\": 1,\n-            \"temperature\": 1.0,\n-            \"top_k\": 50,\n-            \"top_p\": 1.0,\n-            \"typical_p\": 1.0,\n-            \"repetition_penalty\": 1.0,\n-            \"length_penalty\": 1.0,\n-            \"no_repeat_ngram_size\": 0,\n-            \"encoder_no_repeat_ngram_size\": 0,\n-            \"bad_words_ids\": None,\n-            \"num_return_sequences\": 1,\n-            \"output_scores\": False,\n-            \"return_dict_in_generate\": False,\n-            \"forced_bos_token_id\": None,\n-            \"forced_eos_token_id\": None,\n-            \"remove_invalid_values\": False,\n-            \"exponential_decay_length_penalty\": None,\n-            \"suppress_tokens\": None,\n-            \"begin_suppress_tokens\": None,\n-            # Deprecated arguments (moved to the Hub). TODO joao, manuel: remove in v4.62.0\n-            \"num_beam_groups\": 1,\n-            \"diversity_penalty\": 0.0,\n-        }\n-\n-    def _get_non_default_generation_parameters(self) -> dict[str, Any]:\n+    def _get_generation_parameters(self) -> dict[str, Any]:\n         \"\"\"\n         Gets the non-default generation parameters on the PreTrainedConfig instance\n         \"\"\"\n-        non_default_generation_parameters = {}\n-        decoder_attribute_name = None\n-\n-        # If it is a composite model, we want to check the subconfig that will be used for generation\n-        self_decoder_config = self if decoder_attribute_name is None else getattr(self, decoder_attribute_name)\n-\n-        for parameter_name, default_global_value in self._get_global_generation_defaults().items():\n-            if hasattr(self_decoder_config, parameter_name):\n-                parameter_value = getattr(self_decoder_config, parameter_name, None)\n-                # Two cases in which is okay for the model config to hold generation config parameters:\n-                # 1. The parameter is set to `None`, effectively delegating its value to the generation config\n-                # 2. The parameter is set the global generation defaults\n-                if parameter_value is None or parameter_value == default_global_value:\n-                    continue\n-                non_default_generation_parameters[parameter_name] = getattr(self_decoder_config, parameter_name)\n-\n-        return non_default_generation_parameters\n+        generation_params = {}\n+        default_config = self.__class__().to_dict() if not self.has_no_defaults_at_init else {}\n+        for key in GenerationConfig._get_default_generation_params().keys():\n+            if hasattr(self, key) and getattr(self, key) is not None and key not in default_config:\n+                generation_params[key] = getattr(self, key)\n+\n+        return generation_params\n \n     def get_text_config(self, decoder=None, encoder=None) -> \"PreTrainedConfig\":\n         \"\"\""
        },
        {
            "sha": "8395f1f6b36bb94d4c3703cbca22220c97abf483",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 17,
            "deletions": 12,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -117,11 +117,16 @@ def __init__(\n \n         # Prepare the assistant and the starting number of candidate tokens\n         self.assistant_model = assistant_model\n-        self.num_assistant_tokens = assistant_model.generation_config.num_assistant_tokens\n-        self.assistant_confidence_threshold = assistant_model.generation_config.assistant_confidence_threshold\n+\n+        # Prepare the generation config by updating with default values if not already set by users\n+        self.assistant_generation_config = copy.deepcopy(assistant_model.generation_config)\n+        global_defaults = self.assistant_generation_config._get_default_generation_params()\n+        self.assistant_generation_config.update(**global_defaults, defaults_only=True)\n+        self.num_assistant_tokens = self.assistant_generation_config.num_assistant_tokens\n+        self.assistant_confidence_threshold = self.assistant_generation_config.assistant_confidence_threshold\n \n         # Set eos in assistant same as in target model\n-        self.assistant_model.generation_config.eos_token_id = generation_config.eos_token_id\n+        self.assistant_generation_config.eos_token_id = generation_config.eos_token_id\n \n         # Prepare the kwargs for the assistant model\n         assistant_kwargs = {}\n@@ -138,10 +143,10 @@ def __init__(\n         # If the assistant is an encoder-decoder model, assume the encoder is different on the assistant.\n         if assistant_model.config.is_encoder_decoder:\n             inputs_tensor, model_input_name, assistant_kwargs = assistant_model._prepare_model_inputs(\n-                inputs_tensor, assistant_model.generation_config.bos_token_id, assistant_kwargs\n+                inputs_tensor, self.assistant_generation_config.bos_token_id, assistant_kwargs\n             )\n             assistant_kwargs = assistant_model._prepare_encoder_decoder_kwargs_for_generation(\n-                inputs_tensor, assistant_kwargs, model_input_name, assistant_model.generation_config\n+                inputs_tensor, assistant_kwargs, model_input_name, self.assistant_generation_config\n             )\n         elif \"encoder_outputs\" in model_kwargs:\n             assistant_kwargs[\"encoder_outputs\"] = model_kwargs[\"encoder_outputs\"]\n@@ -189,7 +194,7 @@ def __init__(\n \n         if (\n             is_sklearn_available()\n-            and self.assistant_model.generation_config.assistant_confidence_threshold\n+            and self.assistant_generation_config.assistant_confidence_threshold\n             and type(self) is AssistedCandidateGenerator\n         ):\n             self.probs = []\n@@ -236,7 +241,7 @@ def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.F\n         # Adjust the max number of assistant tokens to use in the next iteration. This is a simple heuristic,\n         # probably can be improved -- we want to balance the benefits of getting assistant tokens correct with the\n         # cost of forecasting incorrect assistant tokens.\n-        if self.assistant_model.generation_config.num_assistant_tokens_schedule in {\n+        if self.assistant_generation_config.num_assistant_tokens_schedule in {\n             \"heuristic\",\n             \"heuristic_transient\",\n         }:\n@@ -250,7 +255,7 @@ def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.F\n         # This adaptation is not compatible with UAG, as it relies on the number of matched tokens based on the draft vocabulary, which is unavailable in UAG.\n         if (\n             is_sklearn_available()\n-            and self.assistant_model.generation_config.assistant_confidence_threshold\n+            and self.assistant_generation_config.assistant_confidence_threshold\n             and type(self) is AssistedCandidateGenerator\n         ):\n             # update self.matches\n@@ -276,7 +281,7 @@ def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.F\n                 optimal_threshold_index = np.argmin(costs)\n                 best_threshold = thresholds[optimal_threshold_index]\n \n-                self.assistant_model.generation_config.assistant_confidence_threshold = best_threshold\n+                self.assistant_generation_config.assistant_confidence_threshold = best_threshold\n \n     def _calculate_new_tokens(self, input_ids: torch.LongTensor) -> tuple[int, int]:\n         \"\"\"Calculate the minimum and maximum number of new tokens to generate.\"\"\"\n@@ -320,7 +325,7 @@ def _generate_candidates(self, generation_args: dict) -> tuple[torch.LongTensor,\n         self.assistant_kwargs[\"past_key_values\"] = assistant_output.past_key_values\n         if (\n             is_sklearn_available()\n-            and self.assistant_model.generation_config.assistant_confidence_threshold\n+            and self.assistant_generation_config.assistant_confidence_threshold\n             and type(self) is AssistedCandidateGenerator\n         ):\n             scores_tensor = torch.cat(assistant_output.scores, dim=0)\n@@ -383,8 +388,8 @@ def __init__(\n         self.assistant_tokenizer = assistant_tokenizer\n         self.prev_target_ids_len: int | None = None\n         self.prev_assistant_ids = None\n-        self.target_lookbehind = assistant_model.generation_config.target_lookbehind\n-        self.assistant_lookbehind = assistant_model.generation_config.assistant_lookbehind\n+        self.target_lookbehind = self.assistant_generation_config.target_lookbehind\n+        self.assistant_lookbehind = self.assistant_generation_config.assistant_lookbehind\n \n     @staticmethod\n     def _get_longest_diag_dict(input_matrix, nonzero_idx):"
        },
        {
            "sha": "21de4e972276149aadc1e36f6eadca35f8187481",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 147,
            "deletions": 112,
            "changes": 259,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -20,12 +20,11 @@\n from abc import ABC, abstractmethod\n from collections.abc import Callable\n from dataclasses import dataclass, is_dataclass\n-from typing import TYPE_CHECKING, Any, Optional\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n from huggingface_hub import create_repo\n \n from .. import __version__\n-from ..configuration_utils import PreTrainedConfig\n from ..utils import (\n     GENERATION_CONFIG_NAME,\n     ExplicitEnum,\n@@ -38,6 +37,7 @@\n \n \n if TYPE_CHECKING:\n+    from ..configuration_utils import PreTrainedConfig\n     from ..modeling_utils import PreTrainedModel\n \n \n@@ -104,18 +104,18 @@ class GenerationConfig(PushToHubMixin):\n     Arg:\n         > Parameters that control the length of the output\n \n-        max_length (`int`, *optional*, defaults to 20):\n+        max_length (`int`, *optional*):\n             `max_new_tokens` is recommended for controlling how many tokens the model generates.\n             `max_length` remains for backward compatibility.\n \n         max_new_tokens (`int`, *optional*):\n             The maximum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n-        min_length (`int`, *optional*, defaults to 0):\n+        min_length (`int`, *optional*):\n             The minimum length of the sequence to be generated. Corresponds to the length of the input prompt +\n             `min_new_tokens`. Its effect is overridden by `min_new_tokens`, if also set.\n         min_new_tokens (`int`, *optional*):\n             The minimum numbers of tokens to generate, ignoring the number of tokens in the prompt.\n-        early_stopping (`bool` or `str`, *optional*, defaults to `False`):\n+        early_stopping (`bool` or `str`, *optional*):\n             Controls the stopping condition for beam-based methods, like beam-search. It accepts the following values:\n             `True`, where the generation stops as soon as there are `num_beams` complete candidates; `False`, where an\n             heuristic is applied and the generation stops when is it very unlikely to find better candidates;\n@@ -129,17 +129,17 @@ class GenerationConfig(PushToHubMixin):\n \n         > Parameters that control the generation strategy used\n \n-        do_sample (`bool`, *optional*, defaults to `False`):\n+        do_sample (`bool`, defaults to `False`):\n             Whether or not to use sampling ; use greedy decoding otherwise.\n-        num_beams (`int`, *optional*, defaults to 1):\n+        num_beams (`int`, *optional*):\n             Number of beams for beam search. 1 means no beam search.\n \n         > Parameters that control the cache\n \n-        use_cache (`bool`, *optional*, defaults to `True`):\n+        use_cache (`bool`, defaults to `True`):\n             Whether or not the model should use the past last key/values attentions (if applicable to the model) to\n             speed up decoding.\n-        cache_implementation (`str`, *optional*, default to `None`):\n+        cache_implementation (`str`, *optional*):\n             Name of the cache class that will be instantiated in `generate`, for faster decoding. Possible values are:\n \n             - `\"dynamic\"`: [`DynamicCache`]\n@@ -155,11 +155,11 @@ class GenerationConfig(PushToHubMixin):\n \n         > Parameters for manipulation of the model output logits\n \n-        temperature (`float`, *optional*, defaults to 1.0):\n+        temperature (`float`, *optional*):\n             The value used to module the next token probabilities. This value is set in a model's `generation_config.json` file. If it isn't set, the default value is 1.0\n-        top_k (`int`, *optional*, defaults to 50):\n+        top_k (`int`, *optional*):\n             The number of highest probability vocabulary tokens to keep for top-k-filtering. This value is set in a model's `generation_config.json` file. If it isn't set, the default value is 50.\n-        top_p (`float`, *optional*, defaults to 1.0):\n+        top_p (`float`, *optional*):\n             If set to float < 1, only the smallest set of most probable tokens with probabilities that add up to\n             `top_p` or higher are kept for generation. This value is set in a model's `generation_config.json` file. If it isn't set, the default value is 1.0\n         min_p (`float`, *optional*):\n@@ -172,41 +172,41 @@ class GenerationConfig(PushToHubMixin):\n             is kept whose *renormalized* entropy is less than or equal to `top_h` times the entropy of the full distribution.\n             Smaller values (e.g., 0.2â€“0.5) lead to more focused, deterministic outputs, while values closer to 1.0 allow more\n             randomness and diversity. Typical values are in the 0.3â€“0.6 range.\n-        typical_p (`float`, *optional*, defaults to 1.0):\n+        typical_p (`float`, *optional*):\n             Local typicality measures how similar the conditional probability of predicting a target token next is to\n             the expected conditional probability of predicting a random token next, given the partial text already\n             generated. If set to float < 1, the smallest set of the most locally typical tokens with probabilities that\n             add up to `typical_p` or higher are kept for generation. See [this\n             paper](https://huggingface.co/papers/2202.00666) for more details.\n-        epsilon_cutoff (`float`, *optional*, defaults to 0.0):\n+        epsilon_cutoff (`float`, *optional*):\n             If set to float strictly between 0 and 1, only tokens with a conditional probability greater than\n             `epsilon_cutoff` will be sampled. In the paper, suggested values range from 3e-4 to 9e-4, depending on the\n             size of the model. See [Truncation Sampling as Language Model\n             Desmoothing](https://huggingface.co/papers/2210.15191) for more details.\n-        eta_cutoff (`float`, *optional*, defaults to 0.0):\n+        eta_cutoff (`float`, *optional*):\n             Eta sampling is a hybrid of locally typical sampling and epsilon sampling. If set to float strictly between\n             0 and 1, a token is only considered if it is greater than either `eta_cutoff` or `sqrt(eta_cutoff) *\n             exp(-entropy(softmax(next_token_logits)))`. The latter term is intuitively the expected next token\n             probability, scaled by `sqrt(eta_cutoff)`. In the paper, suggested values range from 3e-4 to 2e-3,\n             depending on the size of the model. See [Truncation Sampling as Language Model\n             Desmoothing](https://huggingface.co/papers/2210.15191) for more details.\n-        repetition_penalty (`float`, *optional*, defaults to 1.0):\n+        repetition_penalty (`float`, *optional*):\n             The parameter for repetition penalty. 1.0 means no penalty. See [this\n             paper](https://huggingface.co/papers/1909.05858) for more details.\n-        encoder_repetition_penalty (`float`, *optional*, defaults to 1.0):\n+        encoder_repetition_penalty (`float`, *optional*):\n             The parameter for encoder_repetition_penalty. An exponential penalty on sequences that are not in the\n             original input. 1.0 means no penalty.\n-        length_penalty (`float`, *optional*, defaults to 1.0):\n+        length_penalty (`float`, *optional*):\n             Exponential penalty to the length that is used with beam-based generation. It is applied as an exponent to\n             the sequence length, which in turn is used to divide the score of the sequence. Since the score is the log\n             likelihood of the sequence (i.e. negative), `length_penalty` > 0.0 promotes longer sequences, while\n             `length_penalty` < 0.0 encourages shorter sequences.\n-        no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n+        no_repeat_ngram_size (`int`, *optional*):\n             If set to int > 0, all ngrams of that size can only occur once.\n         bad_words_ids (`list[list[int]]`, *optional*):\n             List of list of token ids that are not allowed to be generated. Check\n             [`~generation.NoBadWordsLogitsProcessor`] for further documentation and examples.\n-        renormalize_logits (`bool`, *optional*, defaults to `False`):\n+        renormalize_logits (`bool`, defaults to `False`):\n             Whether to renormalize the logits after applying all the logits processors (including the custom\n             ones). It's highly recommended to set this flag to `True` as the search algorithms suppose the score logits\n             are normalized but some logit processors break the normalization.\n@@ -217,7 +217,7 @@ class GenerationConfig(PushToHubMixin):\n         forced_eos_token_id (`int` or list[int]`, *optional*, defaults to `model.config.forced_eos_token_id`):\n             The id of the token to force as the last generated token when `max_length` is reached. Optionally, use a\n             list to set multiple *end-of-sequence* tokens.\n-        remove_invalid_values (`bool`, *optional*, defaults to `model.config.remove_invalid_values`):\n+        remove_invalid_values (`bool`, defaults to `model.config.remove_invalid_values`):\n             Whether to remove possible *nan* and *inf* outputs of the model to prevent the generation method to crash.\n             Note that using `remove_invalid_values` can slow down generation.\n         exponential_decay_length_penalty (`tuple(int, float)`, *optional*):\n@@ -234,7 +234,7 @@ class GenerationConfig(PushToHubMixin):\n             Dictionary that maps a sequence of tokens to its bias term. Positive biases increase the odds of the\n             sequence being selected, while negative biases do the opposite. Check\n             [`~generation.SequenceBiasLogitsProcessor`] for further documentation and examples.\n-        token_healing (`bool`, *optional*, defaults to `False`):\n+        token_healing (`bool`, defaults to `False`):\n             Heal tail tokens of prompts by replacing them with their appropriate extensions.\n             This enhances the quality of completions for prompts affected by greedy tokenization bias.\n         guidance_scale (`float`, *optional*):\n@@ -250,18 +250,18 @@ class GenerationConfig(PushToHubMixin):\n \n         num_return_sequences (`int`, *optional*, defaults to 1):\n             The number of independently computed returned sequences for each element in the batch.\n-        output_attentions (`bool`, *optional*, defaults to `False`):\n+        output_attentions (`bool`, defaults to `False`):\n             Whether or not to return the attentions tensors of all attention layers. See `attentions` under returned\n             tensors for more details.\n-        output_hidden_states (`bool`, *optional*, defaults to `False`):\n+        output_hidden_states (`bool`, defaults to `False`):\n             Whether or not to return the hidden states of all layers. See `hidden_states` under returned tensors for\n             more details.\n-        output_scores (`bool`, *optional*, defaults to `False`):\n+        output_scores (`bool`, defaults to `False`):\n             Whether or not to return the prediction scores. See `scores` under returned tensors for more details.\n-        output_logits (`bool`, *optional*):\n+        output_logits (`bool`, defaults to `False`):\n             Whether or not to return the unprocessed prediction logit scores. See `logits` under returned tensors for\n             more details.\n-        return_dict_in_generate (`bool`, *optional*, defaults to `False`):\n+        return_dict_in_generate (`bool`, defaults to `False`):\n             Whether or not to return a [`~utils.ModelOutput`], as opposed to returning exclusively the generated\n             sequence. This flag must be set to `True` to return the generation cache (when `use_cache` is `True`)\n             or optional outputs (see flags starting with `output_`)\n@@ -277,7 +277,7 @@ class GenerationConfig(PushToHubMixin):\n \n         > Generation parameters exclusive to encoder-decoder models\n \n-        encoder_no_repeat_ngram_size (`int`, *optional*, defaults to 0):\n+        encoder_no_repeat_ngram_size (`int`, *optional*):\n             If set to int > 0, all ngrams of that size that occur in the `encoder_input_ids` cannot occur in the\n             `decoder_input_ids`.\n         decoder_start_token_id (`int` or `list[int]`, *optional*):\n@@ -286,20 +286,20 @@ class GenerationConfig(PushToHubMixin):\n             (e.g. multilingual models with different target languages in one batch)\n \n         > Generation parameters exclusive to assistant generation\n-        is_assistant (`bool`, *optional*, defaults to `False`):\n+        is_assistant (`bool`, defaults to `False`):\n             Whether the model is an assistant (draft) model.\n-        num_assistant_tokens (`int`, *optional*, defaults to 20):\n+        num_assistant_tokens (`int`, *optional*):\n             Defines the number of _speculative tokens_ that shall be generated by the assistant model before being\n             checked by the target model at each iteration. Higher values for `num_assistant_tokens` make the generation\n             more _speculative_ : If the assistant model is performant larger speed-ups can be reached, if the assistant\n             model requires lots of corrections, lower speed-ups are reached.\n-        num_assistant_tokens_schedule (`str`, *optional*, defaults to `\"constant\"`):\n+        num_assistant_tokens_schedule (`str`, *optional*):\n             Defines the schedule at which max assistant tokens shall be changed during inference.\n             - `\"heuristic\"`: When all speculative tokens are correct, increase `num_assistant_tokens` by 2 else\n               reduce by 1. `num_assistant_tokens` value is persistent over multiple generation calls with the same assistant model.\n             - `\"heuristic_transient\"`: Same as `\"heuristic\"` but `num_assistant_tokens` is reset to its initial value after each generation call.\n             - `\"constant\"`: `num_assistant_tokens` stays unchanged during generation\n-        assistant_confidence_threshold (`float`, *optional*, defaults to 0.4):\n+        assistant_confidence_threshold (`float`, *optional*):\n             The confidence threshold for the assistant model. If the assistant model's confidence in its prediction for the current token is lower\n             than this threshold, the assistant model stops the current token generation iteration, even if the number of _speculative tokens_\n             (defined by `num_assistant_tokens`) is not yet reached. The assistant's confidence threshold is adjusted throughout the speculative iterations to reduce the number of unnecessary draft and target forward passes, biased towards avoiding false negatives.\n@@ -313,11 +313,11 @@ class GenerationConfig(PushToHubMixin):\n         assistant_early_exit(`int`, *optional*):\n             If set to a positive integer, early exit of the model will be used as an assistant. Can only be used with\n             models that support early exit (i.e. models where logits from intermediate layers can be interpreted by the LM head).\n-        assistant_lookbehind(`int`, *optional*, defaults to 10):\n+        assistant_lookbehind(`int`, *optional*):\n             If set to a positive integer, the re-encodeing process will additionally consider the last `assistant_lookbehind` assistant tokens\n             to correctly align tokens. Can only be used with different tokenizers in speculative decoding.\n             See this [blog](https://huggingface.co/blog/universal_assisted_generation) for more details.\n-        target_lookbehind(`int`, *optional*, defaults to 10):\n+        target_lookbehind(`int`, *optional*):\n             If set to a positive integer, the re-encodeing process will additionally consider the last `target_lookbehind` target tokens\n             to correctly align tokens. Can only be used with different tokenizers in speculative decoding.\n             See this [blog](https://huggingface.co/blog/universal_assisted_generation) for more details.\n@@ -327,7 +327,7 @@ class GenerationConfig(PushToHubMixin):\n         compile_config (CompileConfig, *optional*):\n             If using a compilable cache, this controls how `generate` will `compile` the forward pass for faster\n             inference.\n-        disable_compile (`bool`, *optional*):\n+        disable_compile (`bool`, defaults to `False`):\n             Whether to disable the automatic compilation of the forward pass. Automatic compilation happens when\n             specific criteria are met, including using a compilable cache. Please open an issue if you find the\n             need to use this flag.\n@@ -337,38 +337,36 @@ class GenerationConfig(PushToHubMixin):\n \n     def __init__(self, **kwargs):\n         # Parameters that control the length of the output\n-        self.max_length = kwargs.pop(\"max_length\", 20)\n+        self.max_length = kwargs.pop(\"max_length\", None)\n         self.max_new_tokens = kwargs.pop(\"max_new_tokens\", None)\n-        self.min_length = kwargs.pop(\"min_length\", 0)\n+        self.min_length = kwargs.pop(\"min_length\", None)\n         self.min_new_tokens = kwargs.pop(\"min_new_tokens\", None)\n-        self.early_stopping = kwargs.pop(\"early_stopping\", False)\n+        self.early_stopping = kwargs.pop(\"early_stopping\", None)\n         self.max_time = kwargs.pop(\"max_time\", None)\n         self.stop_strings = kwargs.pop(\"stop_strings\", None)\n \n         # Parameters that control the generation strategy used\n         self.do_sample = kwargs.pop(\"do_sample\", False)\n-        self.num_beams = kwargs.pop(\"num_beams\", 1)\n+        self.num_beams = kwargs.pop(\"num_beams\", None)\n \n         # Parameters that control the cache\n         self.use_cache = kwargs.pop(\"use_cache\", True)\n         self.cache_implementation = kwargs.pop(\"cache_implementation\", None)\n         self.cache_config = kwargs.pop(\"cache_config\", None)\n \n-        self.prefill_chunk_size = kwargs.pop(\"prefill_chunk_size\", None)\n-\n         # Parameters for manipulation of the model output logits\n-        self.temperature = kwargs.pop(\"temperature\", 1.0)\n-        self.top_k = kwargs.pop(\"top_k\", 50)\n-        self.top_p = kwargs.pop(\"top_p\", 1.0)\n+        self.temperature = kwargs.pop(\"temperature\", None)\n+        self.top_k = kwargs.pop(\"top_k\", None)\n+        self.top_p = kwargs.pop(\"top_p\", None)\n         self.min_p = kwargs.pop(\"min_p\", None)\n         self.top_h = kwargs.pop(\"top_h\", None)\n-        self.typical_p = kwargs.pop(\"typical_p\", 1.0)\n-        self.epsilon_cutoff = kwargs.pop(\"epsilon_cutoff\", 0.0)\n-        self.eta_cutoff = kwargs.pop(\"eta_cutoff\", 0.0)\n-        self.repetition_penalty = kwargs.pop(\"repetition_penalty\", 1.0)\n-        self.encoder_repetition_penalty = kwargs.pop(\"encoder_repetition_penalty\", 1.0)\n-        self.length_penalty = kwargs.pop(\"length_penalty\", 1.0)\n-        self.no_repeat_ngram_size = kwargs.pop(\"no_repeat_ngram_size\", 0)\n+        self.typical_p = kwargs.pop(\"typical_p\", None)\n+        self.epsilon_cutoff = kwargs.pop(\"epsilon_cutoff\", None)\n+        self.eta_cutoff = kwargs.pop(\"eta_cutoff\", None)\n+        self.repetition_penalty = kwargs.pop(\"repetition_penalty\", None)\n+        self.encoder_repetition_penalty = kwargs.pop(\"encoder_repetition_penalty\", None)\n+        self.length_penalty = kwargs.pop(\"length_penalty\", None)\n+        self.no_repeat_ngram_size = kwargs.pop(\"no_repeat_ngram_size\", None)\n         self.bad_words_ids = kwargs.pop(\"bad_words_ids\", None)\n         self.renormalize_logits = kwargs.pop(\"renormalize_logits\", False)\n         self.forced_bos_token_id = kwargs.pop(\"forced_bos_token_id\", None)\n@@ -381,20 +379,16 @@ def __init__(self, **kwargs):\n         self.token_healing = kwargs.pop(\"token_healing\", False)\n         self.guidance_scale = kwargs.pop(\"guidance_scale\", None)\n \n-        watermarking_config = kwargs.pop(\"watermarking_config\", None)\n-        if watermarking_config is None:\n-            self.watermarking_config = None\n-        elif isinstance(watermarking_config, BaseWatermarkingConfig):\n-            self.watermarking_config = watermarking_config\n-        else:\n-            self.watermarking_config = WatermarkingConfig.from_dict(watermarking_config)\n+        self.watermarking_config = kwargs.pop(\"watermarking_config\", None)\n+        if isinstance(self.watermarking_config, dict):\n+            self.watermarking_config = WatermarkingConfig.from_dict(self.watermarking_config)\n \n         # Parameters that define the output variables of `generate`\n         self.num_return_sequences = kwargs.pop(\"num_return_sequences\", 1)\n         self.output_attentions = kwargs.pop(\"output_attentions\", False)\n         self.output_hidden_states = kwargs.pop(\"output_hidden_states\", False)\n         self.output_scores = kwargs.pop(\"output_scores\", False)\n-        self.output_logits = kwargs.pop(\"output_logits\", None)\n+        self.output_logits = kwargs.pop(\"output_logits\", False)\n         self.return_dict_in_generate = kwargs.pop(\"return_dict_in_generate\", False)\n \n         # Special tokens that can be used at generation time\n@@ -403,57 +397,57 @@ def __init__(self, **kwargs):\n         self.eos_token_id = kwargs.pop(\"eos_token_id\", None)\n \n         # Generation parameters exclusive to encoder-decoder models\n-        self.encoder_no_repeat_ngram_size = kwargs.pop(\"encoder_no_repeat_ngram_size\", 0)\n+        self.encoder_no_repeat_ngram_size = kwargs.pop(\"encoder_no_repeat_ngram_size\", None)\n         self.decoder_start_token_id = kwargs.pop(\"decoder_start_token_id\", None)\n \n         # Assistant generation\n-        self.is_assistant = False\n-        self.num_assistant_tokens = kwargs.pop(\"num_assistant_tokens\", 20)\n-        self.num_assistant_tokens_schedule = kwargs.pop(\"num_assistant_tokens_schedule\", \"constant\")\n-        self.assistant_confidence_threshold = kwargs.pop(\"assistant_confidence_threshold\", 0.4)\n+        self.is_assistant = kwargs.pop(\"is_assistant\", False)\n+        self.num_assistant_tokens = kwargs.pop(\"num_assistant_tokens\", None)\n+        self.num_assistant_tokens_schedule = kwargs.pop(\"num_assistant_tokens_schedule\", None)\n+        self.assistant_confidence_threshold = kwargs.pop(\"assistant_confidence_threshold\", None)\n         self.prompt_lookup_num_tokens = kwargs.pop(\"prompt_lookup_num_tokens\", None)\n         self.max_matching_ngram_size = kwargs.pop(\"max_matching_ngram_size\", None)\n         self.assistant_early_exit = kwargs.pop(\"assistant_early_exit\", None)\n-        ## assistant generation for different tokenizers, the windows size for assistant/target model\n-        self.assistant_lookbehind = kwargs.pop(\"assistant_lookbehind\", 10)\n-        self.target_lookbehind = kwargs.pop(\"target_lookbehind\", 10)\n+        self.assistant_lookbehind = kwargs.pop(\"assistant_lookbehind\", None)\n+        self.target_lookbehind = kwargs.pop(\"target_lookbehind\", None)\n \n         # Performance\n         self.compile_config = kwargs.pop(\"compile_config\", None)\n         self.disable_compile = kwargs.pop(\"disable_compile\", False)\n \n-        # Deprecated (moved to the Hub). TODO joao, manuel: remove in v4.62.0\n+        # Deprecated (moved to the Hub). TODO remove for v5\n         self.low_memory = kwargs.pop(\"low_memory\", None)\n         self.penalty_alpha = kwargs.pop(\"penalty_alpha\", None)\n         self.dola_layers = kwargs.pop(\"dola_layers\", None)\n-        self.diversity_penalty = kwargs.pop(\"diversity_penalty\", 0.0)\n-        self.num_beam_groups = kwargs.pop(\"num_beam_groups\", 1)\n+        self.diversity_penalty = kwargs.pop(\"diversity_penalty\", None)\n+        self.num_beam_groups = kwargs.pop(\"num_beam_groups\", None)\n         self.constraints = kwargs.pop(\"constraints\", None)\n         self.force_words_ids = kwargs.pop(\"force_words_ids\", None)\n \n-        # The remaining attributes do not parametrize `.generate()`, but are informative and/or used by the hub\n-        # interface.\n-        self._from_model_config = kwargs.pop(\"_from_model_config\", False)\n-        self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n-        self.transformers_version = kwargs.pop(\"transformers_version\", __version__)\n+        self.prefill_chunk_size = kwargs.pop(\"prefill_chunk_size\", None)\n \n-        # Ensure backward compatibility for models that use `forced_bos_token_id` within their config\n-        if self._from_model_config and kwargs.get(\"force_bos_token_to_be_generated\", False):\n-            self.forced_bos_token_id = self.bos_token_id\n-            logger.warning_once(\n-                f\"Please make sure the generation config includes `forced_bos_token_id={self.bos_token_id}`. \"\n-            )\n+        # Common attributes\n+        self._commit_hash = kwargs.pop(\"_commit_hash\", None)\n+        self._from_model_config = kwargs.pop(\"_from_model_config\", None)\n+        self.transformers_version = kwargs.pop(\"transformers_version\", None)\n \n         # Additional attributes without default values\n         if not self._from_model_config:\n-            # we don't want to copy values from the model config if we're initializing a `GenerationConfig` from a\n-            # model's default configuration file\n+            # we don't want to copy values from the model config if we're initializing\n+            # a `GenerationConfig` from a model's default configuration file\n             for key, value in kwargs.items():\n                 try:\n                     setattr(self, key, value)\n                 except AttributeError as err:\n                     logger.error(f\"Can't set {key} with value {value} for {self}\")\n                     raise err\n+        else:\n+            # Ensure backward compatibility for models that use `forced_bos_token_id` within their config\n+            if kwargs.get(\"force_bos_token_to_be_generated\", False):\n+                self.forced_bos_token_id = self.bos_token_id\n+                logger.warning_once(\n+                    f\"Please make sure the generation config includes `forced_bos_token_id={self.bos_token_id}`. \"\n+                )\n \n         # Validate the values of the attributes\n         self.validate()\n@@ -488,8 +482,8 @@ def get_generation_mode(self, assistant_model: Optional[\"PreTrainedModel\"] = Non\n         # property and part of the `__repr__`\n         if self.constraints is not None or self.force_words_ids is not None:\n             generation_mode = GenerationMode.CONSTRAINED_BEAM_SEARCH\n-        elif self.num_beams == 1:\n-            if self.do_sample is False:\n+        elif self.num_beams is None or self.num_beams == 1:\n+            if not self.do_sample:\n                 if (\n                     self.top_k is not None\n                     and self.top_k > 1\n@@ -502,9 +496,9 @@ def get_generation_mode(self, assistant_model: Optional[\"PreTrainedModel\"] = Non\n             else:\n                 generation_mode = GenerationMode.SAMPLE\n         else:\n-            if self.num_beam_groups > 1:\n+            if self.num_beam_groups is not None and self.num_beam_groups > 1:\n                 generation_mode = GenerationMode.GROUP_BEAM_SEARCH\n-            elif self.do_sample is True:\n+            elif self.do_sample:\n                 generation_mode = GenerationMode.BEAM_SAMPLE\n             else:\n                 generation_mode = GenerationMode.BEAM_SEARCH\n@@ -537,6 +531,45 @@ def get_generation_mode(self, assistant_model: Optional[\"PreTrainedModel\"] = Non\n                 )\n         return generation_mode\n \n+    @staticmethod\n+    def _get_default_generation_params() -> dict[str, Any]:\n+        return {\n+            \"max_length\": 20,\n+            \"min_length\": 0,\n+            \"do_sample\": False,\n+            \"early_stopping\": False,\n+            \"num_beams\": 1,\n+            \"temperature\": 1.0,\n+            \"top_k\": 50,\n+            \"top_p\": 1.0,\n+            \"typical_p\": 1.0,\n+            \"repetition_penalty\": 1.0,\n+            \"length_penalty\": 1.0,\n+            \"no_repeat_ngram_size\": 0,\n+            \"encoder_no_repeat_ngram_size\": 0,\n+            \"bad_words_ids\": None,\n+            \"num_return_sequences\": 1,\n+            \"output_scores\": False,\n+            \"return_dict_in_generate\": False,\n+            \"forced_bos_token_id\": None,\n+            \"forced_eos_token_id\": None,\n+            \"remove_invalid_values\": False,\n+            \"exponential_decay_length_penalty\": None,\n+            \"suppress_tokens\": None,\n+            \"begin_suppress_tokens\": None,\n+            \"epsilon_cutoff\": 0.0,\n+            \"eta_cutoff\": 0.0,\n+            \"encoder_repetition_penalty\": 1.0,\n+            \"num_assistant_tokens\": 20,\n+            \"num_assistant_tokens_schedule\": \"constant\",\n+            \"assistant_confidence_threshold\": 0.4,\n+            \"assistant_lookbehind\": 10,\n+            \"target_lookbehind\": 10,\n+            # Deprecated arguments (moved to the Hub). TODO joao, manuel: remove in v4.62.0\n+            \"num_beam_groups\": 1,\n+            \"diversity_penalty\": 0.0,\n+        }\n+\n     def validate(self, strict=False):\n         \"\"\"\n         Validates the values of the attributes of the [`GenerationConfig`] instance. Raises exceptions in the presence\n@@ -552,7 +585,7 @@ def validate(self, strict=False):\n \n         # 1. Validation of individual attributes\n         # 1.1. Decoding attributes\n-        if self.early_stopping not in {True, False, \"never\"}:\n+        if self.early_stopping not in {None, True, False, \"never\"}:\n             raise ValueError(f\"`early_stopping` must be a boolean or 'never', but is {self.early_stopping}.\")\n         if self.max_new_tokens is not None and self.max_new_tokens <= 0:\n             raise ValueError(f\"`max_new_tokens` must be greater than 0, but is {self.max_new_tokens}.\")\n@@ -583,9 +616,9 @@ def validate(self, strict=False):\n \n         # 2. Validation of attribute combinations\n         # 2.1. detect sampling-only parameterization when not in sampling mode\n-        if self.do_sample is False:\n+        if not self.do_sample:\n             greedy_wrong_parameter_msg = (\n-                \"`do_sample` is set to `False`. However, `{flag_name}` is set to `{flag_value}` -- this flag is only \"\n+                \"`do_sample` is set not to set `True`. However, `{flag_name}` is set to `{flag_value}` -- this flag is only \"\n                 \"used in sample-based generation modes. You should set `do_sample=True` or unset `{flag_name}`.\"\n             )\n             if self.temperature is not None and self.temperature != 1.0:\n@@ -614,42 +647,42 @@ def validate(self, strict=False):\n                 )\n \n         # 2.2. detect beam-only parameterization when not in beam mode\n-        if self.num_beams == 1:\n+        if self.num_beams is None or self.num_beams == 1:\n             single_beam_wrong_parameter_msg = (\n-                \"`num_beams` is set to 1. However, `{flag_name}` is set to `{flag_value}` -- this flag is only used \"\n+                \"`num_beams` is set to {num_beams}. However, `{flag_name}` is set to `{flag_value}` -- this flag is only used \"\n                 \"in beam-based generation modes. You should set `num_beams>1` or unset `{flag_name}`.\"\n             )\n-            if self.early_stopping is not False:\n+            if self.early_stopping is not None and self.early_stopping is not False:\n                 minor_issues[\"early_stopping\"] = single_beam_wrong_parameter_msg.format(\n-                    flag_name=\"early_stopping\", flag_value=self.early_stopping\n+                    num_beams=self.num_beams, flag_name=\"early_stopping\", flag_value=self.early_stopping\n                 )\n             if self.length_penalty is not None and self.length_penalty != 1.0:\n                 minor_issues[\"length_penalty\"] = single_beam_wrong_parameter_msg.format(\n-                    flag_name=\"length_penalty\", flag_value=self.length_penalty\n+                    num_beams=self.num_beams, flag_name=\"length_penalty\", flag_value=self.length_penalty\n                 )\n \n         # 2.4. check `num_return_sequences`\n-        if self.num_return_sequences != 1:\n-            if self.num_beams == 1:\n-                if self.do_sample is False:\n+        if self.num_return_sequences > 1:\n+            if self.num_beams is None or self.num_beams == 1:\n+                if not self.do_sample:\n                     raise ValueError(\n-                        \"Greedy methods without beam search do not support `num_return_sequences` different than 1 \"\n-                        f\"(got {self.num_return_sequences}).\"\n+                        \"Greedy methods (do_sample != True) without beam search do not support \"\n+                        f\"`num_return_sequences` different than 1 (got {self.num_return_sequences}).\"\n                     )\n-            elif self.num_return_sequences > self.num_beams:\n+            elif self.num_beams is not None and self.num_return_sequences > self.num_beams:\n                 raise ValueError(\n                     f\"`num_return_sequences` ({self.num_return_sequences}) has to be smaller or equal to `num_beams` \"\n                     f\"({self.num_beams}).\"\n                 )\n \n         # 2.5. check cache-related arguments\n-        if self.use_cache is False:\n+        if not self.use_cache:\n             # In this case, all cache-related arguments should be unset. However, since `use_cache=False` is often used\n             # passed to `generate` directly to hot-fix cache issues, let's raise a warning instead of an error\n             # (otherwise a user might need to overwrite several parameters).\n             no_cache_warning = (\n-                \"You have set `use_cache` to `False`, but {cache_arg} is set to {cache_arg_value}. {cache_arg} will \"\n-                \"have no effect.\"\n+                \"You have not set `use_cache` to `True`, but {cache_arg} is set to {cache_arg_value}.\"\n+                \"{cache_arg} will have no effect.\"\n             )\n             for arg_name in (\"cache_implementation\", \"cache_config\"):\n                 if getattr(self, arg_name) is not None:\n@@ -658,9 +691,9 @@ def validate(self, strict=False):\n                     )\n \n         # 2.6. other incorrect combinations\n-        if self.return_dict_in_generate is not True:\n+        if not self.return_dict_in_generate:\n             for extra_output_flag in self.extra_output_flags:\n-                if getattr(self, extra_output_flag) is True:\n+                if getattr(self, extra_output_flag):\n                     minor_issues[extra_output_flag] = (\n                         f\"`return_dict_in_generate` is NOT set to `True`, but `{extra_output_flag}` is. When \"\n                         f\"`return_dict_in_generate` is not `True`, `{extra_output_flag}` is ignored.\"\n@@ -676,7 +709,6 @@ def validate(self, strict=False):\n             \"streamer\",\n             \"negative_prompt_ids\",\n             \"negative_prompt_attention_mask\",\n-            \"use_model_defaults\",\n         )\n         for arg in generate_arguments:\n             if hasattr(self, arg):\n@@ -1101,7 +1133,7 @@ def to_json_file(\n             writer.write(self.to_json_string(use_diff=use_diff, keys_to_pop=keys_to_pop))\n \n     @classmethod\n-    def from_model_config(cls, model_config: PreTrainedConfig | dict) -> \"GenerationConfig\":\n+    def from_model_config(cls, model_config: Union[\"PreTrainedConfig\", dict]) -> \"GenerationConfig\":\n         \"\"\"\n         Instantiates a [`GenerationConfig`] from a [`PreTrainedConfig`]. This function is useful to convert legacy\n         [`PreTrainedConfig`] objects, which may contain generation parameters, into a stand-alone [`GenerationConfig`].\n@@ -1134,7 +1166,7 @@ def from_model_config(cls, model_config: PreTrainedConfig | dict) -> \"Generation\n                         setattr(generation_config, attr, decoder_config_dict[attr])\n \n         # If any `output_...` flag is set to `True`, we ensure `return_dict_in_generate` is set to `True`.\n-        if generation_config.return_dict_in_generate is False:\n+        if not generation_config.return_dict_in_generate:\n             if any(\n                 getattr(generation_config, extra_output_flag, False)\n                 for extra_output_flag in generation_config.extra_output_flags\n@@ -1145,12 +1177,14 @@ def from_model_config(cls, model_config: PreTrainedConfig | dict) -> \"Generation\n         generation_config._original_object_hash = hash(generation_config)\n         return generation_config\n \n-    def update(self, **kwargs):\n+    def update(self, defaults_only=False, **kwargs):\n         \"\"\"\n         Updates attributes of this class instance with attributes from `kwargs` if they match existing attributes,\n         returning all the unused kwargs.\n \n         Args:\n+            defaults_only (`bool`, *optional*, defaults to `False`):\n+                Whether to update all keys in config with `kwargs` or only those that are set to `None` (i.e. default value).\n             kwargs (`dict[str, Any]`):\n                 Dictionary of attributes to tentatively update this class.\n \n@@ -1160,8 +1194,9 @@ def update(self, **kwargs):\n         to_remove = []\n         for key, value in kwargs.items():\n             if hasattr(self, key):\n-                setattr(self, key, value)\n-                to_remove.append(key)\n+                if not defaults_only or getattr(self, key) is None:\n+                    setattr(self, key, value)\n+                    to_remove.append(key)\n \n         # Confirm that the updated instance is still valid\n         self.validate()"
        },
        {
            "sha": "3f65839a1ac072f42df19277a52153dbb4eb292e",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 43,
            "deletions": 99,
            "changes": 142,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -24,7 +24,6 @@\n \n import torch\n import torch.distributed as dist\n-from packaging import version\n from torch import nn\n \n from ..cache_utils import (\n@@ -1722,20 +1721,19 @@ def _prepare_generated_length(\n                 )\n             generation_config.max_length = generation_config.max_new_tokens + input_ids_length\n \n-        # if both `inputs_embeds` and `input_ids` are passed, we do not correct the length\n-        # otherwise we need total length [inputs-embeds-len + new-tokens-len] to not go beyond indicated `max_length``\n+        # If both `inputs_embeds` and `input_ids` are passed, we correct length with `inputs_tensor.shape`\n+        # We need to get max_length = inputs_embeds_len + max_new_tokens\n         elif (\n             model_input_name == \"inputs_embeds\"\n             and input_ids_length != inputs_tensor.shape[1]\n             and not self.config.is_encoder_decoder\n         ):\n             generation_config.max_length -= inputs_tensor.shape[1]\n         elif has_default_max_length:  # by default let's always generate 20 new tokens\n-            if generation_config.max_length == GenerationConfig().max_length:\n-                generation_config.max_length = generation_config.max_length + input_ids_length\n-                max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)\n-                if max_position_embeddings is not None:\n-                    generation_config.max_length = min(generation_config.max_length, max_position_embeddings)\n+            generation_config.max_length = generation_config.max_length + input_ids_length\n+            max_position_embeddings = getattr(self.config, \"max_position_embeddings\", None)\n+            if max_position_embeddings is not None:\n+                generation_config.max_length = min(generation_config.max_length, max_position_embeddings)\n \n         # same for min length\n         if generation_config.min_new_tokens is not None:\n@@ -1760,101 +1758,50 @@ def _prepare_generated_length(\n     def _prepare_generation_config(\n         self,\n         generation_config: GenerationConfig | None,\n-        use_model_defaults: bool | None = None,\n         **kwargs: Any,\n     ) -> tuple[GenerationConfig, dict]:\n         \"\"\"\n         Prepares the base generation config, then applies any generation configuration options from kwargs. This\n         function handles retrocompatibility with respect to configuration files.\n         \"\"\"\n         # parameterization priority:\n-        # kwargs > non-global default values in `generation_config` > `model.generation_config` > GenerationConfig()\n+        # user-defined kwargs or `generation_config` > `self.generation_config` > global default values\n+        # TODO: (raushan) doesn't make sense to allow kwargs and `generation_config`. Should be mutually exclusive!\n         # TODO (joao): per-model generation config classes.\n \n-        using_model_generation_config = False\n         if generation_config is None:\n-            # legacy: users may modify the model configuration to control generation. To trigger this legacy behavior,\n-            # the following conditions must be met\n-            # 1) the generation config must have been created from the model config (`_from_model_config` field);\n-            # 2) the generation config must have seen no modification since its creation (the hash is the same);\n-            # 3) there are non-default generation parameters in the model config.\n-            # 4) the user must have set new generation parameters in the model config.\n-            if (\n-                self.generation_config._from_model_config  # 1)\n-                and self.generation_config._original_object_hash == hash(self.generation_config)  # 2)\n-                and len(self.config._get_non_default_generation_parameters()) > 0  # 3)\n-            ):\n-                new_generation_config = GenerationConfig.from_model_config(self.config)\n-                if new_generation_config != self.generation_config:  # 4)\n-                    raise ValueError(\n-                        \"You have modified the pretrained model configuration to control generation.\"\n-                        \" This strategy to control generation is not supported anymore. \"\n-                        \" Please use and modify the model generation configuration (see\"\n-                        \" https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\",\n-                    )\n-\n-            generation_config = self.generation_config\n-            using_model_generation_config = True\n-\n-            # Related to #40039: prior to this PR, models with sliding window attention were forced to have\n-            # `cache_implementation=\"hybrid\"` (the static sliding window cache). For these models, we now want to use\n-            # the dynamic sliding window cache by default, so we UNSET `cache_implementation` if it is a default value.\n-            # (if we're inside this branch, then it is because we're using default values from the Hub)\n-            if generation_config.cache_implementation == \"hybrid\":\n-                generation_config.cache_implementation = None\n+            # Users may modify `model.config` to control generation. This is a legacy behavior and is not supported anymore\n+            if len(self.config._get_generation_parameters()) > 0:\n+                raise ValueError(\n+                    \"You have modified the pretrained model configuration to control generation \"\n+                    f\"We detected the following values set - {self.config._get_generation_parameters()}. \"\n+                    \"This strategy to control generation is not supported anymore. Please use and modify `model.generation_config` \"\n+                    \"(see https://huggingface.co/docs/transformers/generation_strategies#default-text-generation-configuration )\",\n+                )\n+            generation_config = GenerationConfig()\n \n         # `torch.export.export` usually raises an exception if it is called\n         # with ``strict=True``. deepcopy can only be processed if ``strict=False``.\n         generation_config = copy.deepcopy(generation_config)\n \n-        if not using_model_generation_config:\n-            # If `generation_config` is provided:\n-            # - `use_model_defaults`: let's fallback ALL default values to the model's generation config\n-            # - otherwise: legacy behavior, let's just make sure we have the tokens defined\n-            model_base_version = version.parse(version.parse(self.generation_config.transformers_version).base_version)\n-            if use_model_defaults is True or (\n-                use_model_defaults is None and model_base_version >= version.parse(\"4.50.0\")\n-            ):\n-                modified_values = {}\n-                global_default_generation_config = GenerationConfig()\n-                model_generation_config = self.generation_config\n-                # we iterate over the model's generation config: it may hold custom keys, which we'll want to copy\n-                for key, model_gen_config_value in model_generation_config.__dict__.items():\n-                    if key.startswith(\"_\") or key == \"transformers_version\":  # metadata\n-                        continue\n-                    # Don't set `cache_implementation = 'hybrid'` from the model defaults, see #40135\n-                    if key == \"cache_implementation\" and model_generation_config.cache_implementation == \"hybrid\":\n-                        continue\n-                    global_default_value = getattr(global_default_generation_config, key, None)\n-                    custom_gen_config_value = getattr(generation_config, key, None)\n-                    if (\n-                        custom_gen_config_value == global_default_value\n-                        and model_gen_config_value != global_default_value\n-                    ):\n-                        modified_values[key] = model_gen_config_value\n-                        setattr(generation_config, key, model_gen_config_value)\n-                # edge case: we may set `temperature=0.0` and `do_sample=False`, but the model defaults to\n-                # `do_sample=True`\n-                if generation_config.temperature == 0.0:\n-                    generation_config.do_sample = False\n-                if use_model_defaults is None and len(modified_values) > 0:\n-                    logger.warning_once(\n-                        f\"`generation_config` default values have been modified to match model-specific defaults: \"\n-                        f\"{modified_values}. If this is not desired, please set these values explicitly.\"\n-                    )\n-            else:\n-                if generation_config.bos_token_id is None:\n-                    generation_config.bos_token_id = self.generation_config.bos_token_id\n-                if generation_config.eos_token_id is None:\n-                    generation_config.eos_token_id = self.generation_config.eos_token_id\n-                if generation_config.pad_token_id is None:\n-                    generation_config.pad_token_id = self.generation_config.pad_token_id\n-                if generation_config.decoder_start_token_id is None:\n-                    generation_config.decoder_start_token_id = self.generation_config.decoder_start_token_id\n-\n-        # Finally, apply any passed kwargs\n+        # First set values from the loaded `self.generation_config`, then set default values (BC)\n+        # Do not update any values that aren't `None`, i.e. if set by users explicitly and passed\n+        # to `generate()`. Thus the `defaults_only=True` is used\n+        global_defaults = self.generation_config._get_default_generation_params()\n+        generation_config.update(**self.generation_config.to_dict(), defaults_only=True)\n+        generation_config.update(**global_defaults, defaults_only=True)\n+\n+        # Finally, if there are any kwargs, update config with it -> highest priority at the end\n         model_kwargs = generation_config.update(**kwargs)\n-        # And keep in model_kwargs variable output controls\n+\n+        # Related to #40039: prior to this PR, models with sliding window attention were forced to have\n+        # `cache_implementation=\"hybrid\"` (the static sliding window cache). For these models, we now want to use\n+        # the dynamic sliding window cache by default, so we UNSET `cache_implementation` if it is a default value.\n+        # (if we're inside this branch, then it is because we're using default values from the Hub)\n+        if generation_config.cache_implementation == \"hybrid\":\n+            generation_config.cache_implementation = None\n+\n+        # Finally keep output_xxx args in `model_kwargs` so it can be passed to `forward`\n         output_attentions = generation_config.output_attentions\n         output_hidden_states = generation_config.output_hidden_states\n         model_kwargs.update({\"output_attentions\": output_attentions} if output_attentions else {})\n@@ -2294,7 +2241,6 @@ def generate(\n         streamer: Optional[\"BaseStreamer\"] = None,\n         negative_prompt_ids: torch.Tensor | None = None,\n         negative_prompt_attention_mask: torch.Tensor | None = None,\n-        use_model_defaults: bool | None = None,\n         custom_generate: str | Callable | None = None,\n         **kwargs,\n     ) -> GenerateOutput | torch.LongTensor:\n@@ -2360,11 +2306,6 @@ def generate(\n                 size. This is an experimental feature, subject to breaking API changes in future versions.\n             negative_prompt_attention_mask (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\n                 Attention_mask for `negative_prompt_ids`.\n-            use_model_defaults (`bool`, *optional*):\n-                When it is `True`, unset parameters in `generation_config` will be set to the model-specific default\n-                generation configuration (`model.generation_config`), as opposed to the global defaults\n-                (`GenerationConfig()`). If unset, models saved starting from `v4.50` will consider this flag to be\n-                `True`.\n             custom_generate (`str` or `Callable`, *optional*):\n                 One of the following:\n                 - `str` (Hugging Face Hub repository name): runs the custom `generate` function defined at\n@@ -2474,7 +2415,7 @@ def generate(\n             # switch to CB\n             outputs = self.generate_batch(\n                 inputs=inputs,\n-                generation_config=self._prepare_generation_config(generation_config, use_model_defaults, **kwargs)[0],\n+                generation_config=self._prepare_generation_config(generation_config, **kwargs)[0],\n                 **kwargs,\n             )\n             sequences = [\n@@ -2495,9 +2436,15 @@ def generate(\n             streamer,\n         )\n \n-        generation_config, model_kwargs = self._prepare_generation_config(\n-            generation_config, use_model_defaults, **kwargs\n+        # Check length values before updating the config with defaults. We'll use it later to define the final min/max length (# 6)\n+        has_default_max_length = kwargs.get(\"max_length\") is None and (\n+            generation_config is None or generation_config.max_length is None\n         )\n+        has_default_min_length = kwargs.get(\"min_length\") is None and (\n+            generation_config is None or generation_config.min_length is None\n+        )\n+        generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n+\n         generation_mode = generation_config.get_generation_mode(assistant_model)\n         if isinstance(custom_generate, Callable):\n             decoding_method = custom_generate\n@@ -2523,7 +2470,6 @@ def generate(\n                 assistant_model=assistant_model,\n                 negative_prompt_ids=negative_prompt_ids,\n                 negative_prompt_attention_mask=negative_prompt_attention_mask,\n-                use_model_defaults=use_model_defaults,\n                 custom_generate=deprecated_mode_repo,\n                 trust_remote_code=trust_remote_code,\n                 **generation_mode_kwargs,\n@@ -2614,8 +2560,6 @@ def generate(\n \n         # 6. Prepare `max_length` depending on other stopping criteria.\n         input_ids_length = input_ids.shape[1]\n-        has_default_max_length = kwargs.get(\"max_length\") is None and generation_config.max_length is not None\n-        has_default_min_length = kwargs.get(\"min_length\") is None and generation_config.min_length is not None\n         generation_config = self._prepare_generated_length(\n             generation_config=generation_config,\n             has_default_max_length=has_default_max_length,"
        },
        {
            "sha": "32f7dfc0da8b18fdbe7a205cc43d28fa8da4a815",
            "filename": "src/transformers/generation/watermarking.py",
            "status": "modified",
            "additions": 8,
            "deletions": 5,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fgeneration%2Fwatermarking.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fwatermarking.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -16,20 +16,23 @@\n import collections\n from dataclasses import dataclass\n from functools import lru_cache\n-from typing import Any\n+from typing import TYPE_CHECKING, Any, Optional, Union\n \n import numpy as np\n import torch\n from torch import nn\n from torch.nn import BCELoss\n \n from .. import initialization as init\n+from ..configuration_utils import PreTrainedConfig\n from ..modeling_utils import PreTrainedModel\n from ..utils import ModelOutput, logging\n-from .configuration_utils import PreTrainedConfig, WatermarkingConfig\n from .logits_process import SynthIDTextWatermarkLogitsProcessor, WatermarkLogitsProcessor\n \n \n+if TYPE_CHECKING:\n+    from .configuration_utils import WatermarkingConfig\n+\n logger = logging.get_logger(__name__)\n \n \n@@ -120,13 +123,13 @@ class WatermarkDetector:\n \n     def __init__(\n         self,\n-        model_config: PreTrainedConfig,\n+        model_config: \"PreTrainedConfig\",\n         device: str,\n-        watermarking_config: WatermarkingConfig | dict,\n+        watermarking_config: Optional[Union[\"WatermarkingConfig\", dict]],\n         ignore_repeated_ngrams: bool = False,\n         max_cache_size: int = 128,\n     ):\n-        if isinstance(watermarking_config, WatermarkingConfig):\n+        if not isinstance(watermarking_config, dict):\n             watermarking_config = watermarking_config.to_dict()\n \n         self.bos_token_id = ("
        },
        {
            "sha": "4eedf70282194634e0c3bad09e4cd7ba8306d341",
            "filename": "src/transformers/models/csm/generation_csm.py",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fgeneration_csm.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -89,7 +89,7 @@ def _get_stopping_criteria(\n         return kept_criteria\n \n     def _prepare_generation_config(\n-        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: Any\n+        self, generation_config: Optional[GenerationConfig], **kwargs: Any\n     ) -> tuple[GenerationConfig, dict]:\n         \"\"\"\n         This method overrides [~generation.utils.GenerationMixin._prepare_generation_config].\n@@ -104,9 +104,7 @@ def _prepare_generation_config(\n         kwargs = {k: v for k, v in kwargs.items() if not k.startswith(\"depth_decoder_\")}\n \n         # initialize the generation config\n-        generation_config, model_kwargs = super()._prepare_generation_config(\n-            generation_config, use_model_defaults, **kwargs\n-        )\n+        generation_config, model_kwargs = super()._prepare_generation_config(generation_config, **kwargs)\n         self.depth_decoder.generation_config.update(**depth_decoder_kwargs)\n \n         # ensure the depth decoder generation config is valid"
        },
        {
            "sha": "6658a5e2cf6a51b3c5c028bdb765992f4ae461e0",
            "filename": "src/transformers/models/dia/generation_dia.py",
            "status": "modified",
            "additions": 3,
            "deletions": 10,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdia%2Fgeneration_dia.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -110,11 +110,9 @@ def _get_logits_processor(\n         return merged_processors\n \n     def _prepare_generation_config(\n-        self, generation_config: Optional[GenerationConfig], use_model_defaults: Optional[bool] = None, **kwargs: Any\n+        self, generation_config: Optional[GenerationConfig], **kwargs: Any\n     ) -> tuple[GenerationConfig, dict]:\n-        generation_config, model_kwargs = super()._prepare_generation_config(\n-            generation_config, use_model_defaults, **kwargs\n-        )\n+        generation_config, model_kwargs = super()._prepare_generation_config(generation_config, **kwargs)\n \n         # We allow generation up to max length + max delay pattern\n         # (will revert back to max length after generation)\n@@ -260,7 +258,6 @@ def _main_generate_loop(\n         streamer: Optional[\"BaseStreamer\"] = None,\n         negative_prompt_ids: Optional[torch.Tensor] = None,\n         negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n-        use_model_defaults: Optional[bool] = None,\n         custom_generate: Optional[str] = None,\n         **kwargs,\n     ):\n@@ -273,9 +270,7 @@ def _main_generate_loop(\n             assistant_model,\n             streamer,\n         )\n-        generation_config, model_kwargs = self._prepare_generation_config(\n-            generation_config, use_model_defaults, **kwargs\n-        )\n+        generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n         generation_mode = generation_config.get_generation_mode(assistant_model)\n \n         if generation_mode not in (GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH):\n@@ -425,7 +420,6 @@ def generate(\n         streamer: Optional[\"BaseStreamer\"] = None,\n         negative_prompt_ids: Optional[torch.Tensor] = None,\n         negative_prompt_attention_mask: Optional[torch.Tensor] = None,\n-        use_model_defaults: Optional[bool] = None,\n         custom_generate: Optional[str] = None,\n         **kwargs,\n     ) -> Union[GenerateOutput, torch.LongTensor]:\n@@ -445,7 +439,6 @@ def generate(\n             streamer=streamer,\n             negative_prompt_ids=negative_prompt_ids,\n             negative_prompt_attention_mask=negative_prompt_attention_mask,\n-            use_model_defaults=use_model_defaults,\n             custom_generate=custom_generate,\n             **kwargs,\n         )"
        },
        {
            "sha": "3884b538bbf5d75a8d6d5b9936d189fab45e8ab4",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -2082,7 +2082,6 @@ def generate(\n         stopping_criteria: Optional[StoppingCriteriaList] = None,\n         synced_gpus: Optional[bool] = None,\n         streamer: Optional[\"BaseStreamer\"] = None,\n-        use_model_defaults: Optional[bool] = None,\n         **kwargs,\n     ):\n         \"\"\"\n@@ -2127,11 +2126,6 @@ def generate(\n             streamer (`BaseStreamer`, *optional*):\n                 Streamer object that will be used to stream the generated sequences. Generated tokens are passed\n                 through `streamer.put(token_ids)` and the streamer is responsible for any further processing.\n-            use_model_defaults (`bool`, *optional*):\n-                When it is `True`, unset parameters in `generation_config` will be set to the model-specific default\n-                generation configuration (`model.generation_config`), as opposed to the global defaults\n-                (`GenerationConfig()`). If unset, models saved starting from `v4.50` will consider this flag to be\n-                `True`.\n             kwargs (`dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model. If the model is an encoder-decoder model, encoder\n@@ -2155,9 +2149,7 @@ def generate(\n         \"\"\"\n         # 1. Handle `generation_config` and kwargs that might update it, and validate the resulting objects\n         generation_mode_kwargs = self._extract_generation_mode_kwargs(None, kwargs, False, None, None)\n-        generation_config, model_kwargs = self._prepare_generation_config(\n-            generation_config, use_model_defaults, **kwargs\n-        )\n+        generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n         generation_mode = generation_config.get_generation_mode()\n         if generation_mode not in [GenerationMode.SAMPLE, GenerationMode.GREEDY_SEARCH]:\n             raise ValueError("
        },
        {
            "sha": "e591ba0444a17a9fc94333f2dffd7fa428ce3a36",
            "filename": "src/transformers/models/rag/configuration_rag.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fconfiguration_rag.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -70,9 +70,6 @@\n             `context_attention_mask` are returned. See returned tensors for more detail.\n         use_cache (`bool`, *optional*, defaults to `True`):\n             Whether or not the model should return the last key/values attentions (not used by all models).\n-        forced_eos_token_id (`int`, *optional*):\n-            The id of the token to force as the last generated token when `max_length` is reached. Usually set to\n-            `eos_token_id`.\n \"\"\"\n \n \n@@ -109,7 +106,6 @@ def __init__(\n         do_marginalize=False,\n         output_retrieved=False,\n         use_cache=True,\n-        forced_eos_token_id=None,\n         dataset_revision=None,\n         **kwargs,\n     ):\n@@ -118,7 +114,6 @@ def __init__(\n             pad_token_id=pad_token_id,\n             eos_token_id=eos_token_id,\n             decoder_start_token_id=decoder_start_token_id,\n-            forced_eos_token_id=forced_eos_token_id,\n             is_encoder_decoder=is_encoder_decoder,\n             prefix=prefix,\n             vocab_size=vocab_size,\n@@ -166,9 +161,6 @@ def __init__(\n \n         self.use_cache = use_cache\n \n-        if forced_eos_token_id is None:\n-            self.forced_eos_token_id = getattr(self.generator, \"forced_eos_token_id\", None)\n-\n     @classmethod\n     def from_question_encoder_generator_configs(\n         cls, question_encoder_config: PreTrainedConfig, generator_config: PreTrainedConfig, **kwargs"
        },
        {
            "sha": "d6d83459af5012d636b281325186599f52666062",
            "filename": "src/transformers/models/rag/modeling_rag.py",
            "status": "modified",
            "additions": 1,
            "deletions": 9,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frag%2Fmodeling_rag.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -1410,7 +1410,6 @@ def generate(\n         prefix_allowed_tokens_fn: Optional[Callable[[int, torch.Tensor], list[int]]] = None,\n         logits_processor: Optional[LogitsProcessorList] = LogitsProcessorList(),\n         stopping_criteria: Optional[StoppingCriteriaList] = StoppingCriteriaList(),\n-        use_model_defaults: Optional[bool] = None,\n         **kwargs,\n     ) -> torch.LongTensor:\n         \"\"\"\n@@ -1469,11 +1468,6 @@ def generate(\n                 Custom stopping criteria that complement the default stopping criteria built from arguments and a\n                 model's config. If a stopping criteria is passed that is already created with the arguments or a\n                 model's config an error is thrown.\n-            use_model_defaults (`bool`, *optional*):\n-                When it is `True`, unset parameters in `generation_config` will be set to the model-specific default\n-                generation configuration (`model.generation_config`), as opposed to the global defaults\n-                (`GenerationConfig()`). If unset, models saved starting from `v4.50` will consider this flag to be\n-                `True`.\n             kwargs (`dict[str, Any]`, *optional*):\n                 Ad hoc parametrization of `generate_config` and/or additional model-specific kwargs that will be\n                 forwarded to the `forward` function of the model.\n@@ -1485,9 +1479,7 @@ def generate(\n         \"\"\"\n         # Handle `generation_config` and kwargs that might update it\n         generation_mode_kwargs = self._extract_generation_mode_kwargs(None, kwargs, False, None, None)\n-        generation_config, model_kwargs = self._prepare_generation_config(\n-            generation_config, use_model_defaults, **kwargs\n-        )\n+        generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n         generation_mode = generation_config.get_generation_mode()\n         if generation_mode not in [\n             GenerationMode.SAMPLE,"
        },
        {
            "sha": "45e7fbc45bf14113c7d80a55162b47cca86efc73",
            "filename": "src/transformers/models/whisper/generation_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fgeneration_whisper.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -658,6 +658,7 @@ def generate(\n             )\n \n         # 1. prepare generation config\n+        generation_config = self.generation_config if generation_config is None else generation_config\n         generation_config, kwargs = self._prepare_generation_config(generation_config, **kwargs)\n \n         # 2. set global generate variables"
        },
        {
            "sha": "51164a464e345c1e193c603e8d80c58651648568",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -884,7 +884,7 @@ def __init__(\n                 # NOTE: _prepare_generation_config creates a deep copy of the generation config before updating it,\n                 # and returns all kwargs that were not used to update the generation config\n                 prepared_generation_config, kwargs = self.model._prepare_generation_config(\n-                    generation_config=default_pipeline_generation_config, use_model_defaults=True, **kwargs\n+                    generation_config=default_pipeline_generation_config, **kwargs\n                 )\n                 self.generation_config = prepared_generation_config\n                 # if the `max_new_tokens` is set to the pipeline default, but `max_length` is set to a non-default"
        },
        {
            "sha": "1951b6f1ce56e47c95ebfb7a35db240bf63193b7",
            "filename": "src/transformers/trainer_seq2seq.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_seq2seq.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -333,7 +333,11 @@ def prediction_step(\n             self.model.generation_config._from_model_config = False\n \n         # Retrieves GenerationConfig from model.generation_config\n+        # Update with defaults because earlier the generation config used ot be init\n+        # with default values. Now we init it with `None` and keep defaults for BC\n         gen_config = self.model.generation_config\n+        default_gen_config = gen_config._get_default_generation_params()\n+        gen_config.update(**default_gen_config, defaults_only=True)\n         # in case the batch is shorter than max length, the output should be padded\n         if generated_tokens.shape[-1] < gen_config.max_length:\n             generated_tokens = self._pad_tensors_to_max_len(generated_tokens, gen_config.max_length)"
        },
        {
            "sha": "28adc3ad70a494e1e54d64a9f3ef12ec12f5a45b",
            "filename": "tests/generation/test_candidate_generator.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fgeneration%2Ftest_candidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fgeneration%2Ftest_candidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_candidate_generator.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -237,8 +237,7 @@ def setUp(self):\n         self.target_config = AutoConfig.from_pretrained(self.target_name)\n         self.assistant_model = AutoModelForCausalLM.from_pretrained(self.assistant_name).to(torch_device)\n         self.assistant_tokenizer = AutoTokenizer.from_pretrained(self.assistant_name)\n-\n-        self.generation_config = GenerationConfig()\n+        self.generation_config = GenerationConfig(max_length=20, min_length=0)\n \n         # Ensure required tokens exist\n         if self.target_tokenizer.pad_token_id is None:"
        },
        {
            "sha": "9e1bbcd661e574efb0fab52197d7344504eafadf",
            "filename": "tests/generation/test_configuration_utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fgeneration%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_configuration_utils.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -86,8 +86,8 @@ def test_save_load_config(self, config_name):\n         self.assertEqual(loaded_config.bad_words_ids, [[1, 2, 3], [4, 5]])\n \n         # Checks parameters that were not specified (defaults)\n-        self.assertEqual(loaded_config.top_k, 50)\n-        self.assertEqual(loaded_config.max_length, 20)\n+        self.assertEqual(loaded_config.top_k, None)\n+        self.assertEqual(loaded_config.max_length, None)\n         self.assertEqual(loaded_config.max_time, None)\n \n     def test_from_model_config(self):\n@@ -123,9 +123,9 @@ def test_update(self):\n     def test_kwarg_init(self):\n         \"\"\"Tests that we can overwrite attributes at `from_pretrained` time.\"\"\"\n         default_config = GenerationConfig()\n-        self.assertEqual(default_config.temperature, 1.0)\n+        self.assertEqual(default_config.temperature, None)\n         self.assertEqual(default_config.do_sample, False)\n-        self.assertEqual(default_config.num_beams, 1)\n+        self.assertEqual(default_config.num_beams, None)\n \n         config = GenerationConfig(\n             do_sample=True,\n@@ -135,15 +135,15 @@ def test_kwarg_init(self):\n         )\n         self.assertEqual(config.temperature, 0.7)\n         self.assertEqual(config.do_sample, True)\n-        self.assertEqual(config.num_beams, 1)\n+        self.assertEqual(config.num_beams, None)\n \n         with tempfile.TemporaryDirectory() as tmp_dir:\n             config.save_pretrained(tmp_dir)\n             loaded_config = GenerationConfig.from_pretrained(tmp_dir, temperature=1.0)\n \n         self.assertEqual(loaded_config.temperature, 1.0)\n         self.assertEqual(loaded_config.do_sample, True)\n-        self.assertEqual(loaded_config.num_beams, 1)  # default value\n+        self.assertEqual(loaded_config.num_beams, None)  # default value\n \n     def test_validate(self):\n         \"\"\"\n@@ -253,7 +253,7 @@ def test_refuse_to_save(self):\n                 config.save_pretrained(tmp_dir)\n             self.assertTrue(\"Fix these issues to save the configuration.\" in str(exc.exception))\n             self.assertTrue(\n-                \"Greedy methods without beam search do not support `num_return_sequences` different than 1\"\n+                \"Greedy methods (do_sample != True) without beam search do not support `num_return_sequences` different than 1\"\n                 in str(exc.exception)\n             )\n             self.assertTrue(len(os.listdir(tmp_dir)) == 0)"
        },
        {
            "sha": "b148fcbcfe83da82130f535358243b6f3079e441",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 34,
            "deletions": 7,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -2674,6 +2674,32 @@ def floats_tensor(shape, scale=1.0, rng=None, name=None):\n @pytest.mark.generate\n @require_torch\n class GenerationIntegrationTests(unittest.TestCase):\n+    def test_generation_config_defaults(self):\n+        \"Tests that we can set config value to a global default. See https://github.com/huggingface/transformers/issues/42762\"\n+        model = AutoModelForCausalLM.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\").to(torch_device)\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-gpt2\")\n+        input_ids = tokenizer(\"Hello\", return_tensors=\"pt\").input_ids.to(torch_device)\n+\n+        # Set the min/max_length to non-default value\n+        model.generation_config.min_length = 50\n+        model.generation_config.max_length = 50\n+\n+        # Explicit generate with min/max_length set to global default values\n+        generation_config = GenerationConfig(\n+            temperature=1.0,\n+            max_length=20,\n+            min_length=0,\n+            do_sample=False,\n+            eos_token_id=-1,  # don't stop before max length reached\n+        )\n+\n+        runtime_generation_config, _ = model._prepare_generation_config(generation_config=generation_config)\n+        self.assertEqual(runtime_generation_config.max_length, 20)\n+        self.assertEqual(runtime_generation_config.min_length, 0)\n+\n+        out = model.generate(input_ids, generation_config=generation_config)\n+        self.assertTrue(len(out[0]) == 20)  # generated max_length=20 tokens, not 50!\n+\n     # TODO joao, manuel: remove in v4.62.0\n     @slow\n     def test_diverse_beam_search(self):\n@@ -4873,7 +4899,8 @@ def assert_no_sklearn(self):\n             self.assertEqual(self.candidate_generator.matches, self.original_matches)\n             self.assertEqual(self.candidate_generator.probs, self.original_probs)\n             self.assertEqual(\n-                self.assistant_model.generation_config.assistant_confidence_threshold, self.original_threshold\n+                self.candidate_generator.assistant_generation_config.assistant_confidence_threshold,\n+                self.original_threshold,\n             )\n \n     @parameterized.expand([(is_sklearn_available(),), (False,)])\n@@ -4886,7 +4913,7 @@ def test_update_candidate_strategy_no_matches_short(self, sklearn_available):\n             self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n             self.assertEqual(self.candidate_generator.matches, [0])\n             self.assertEqual(self.candidate_generator.probs, [0.9])\n-            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.4)\n+            self.assertEqual(self.candidate_generator.assistant_generation_config.assistant_confidence_threshold, 0.4)\n         else:\n             self.assert_no_sklearn()\n \n@@ -4899,7 +4926,7 @@ def test_update_candidate_strategy_with_mix_matches_3(self, sklearn_available):\n             self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n             self.assertEqual(self.candidate_generator.matches, [1, 0, 1, 0, 1, 1, 1, 1, 0])\n             self.assertEqual(self.candidate_generator.probs, [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])\n-            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.2)\n+            self.assertEqual(self.candidate_generator.assistant_generation_config.assistant_confidence_threshold, 0.2)\n         else:\n             self.assert_no_sklearn()\n \n@@ -4912,7 +4939,7 @@ def test_update_candidate_strategy_with_matches_4(self, sklearn_available):\n             self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n             self.assertEqual(self.candidate_generator.matches, [1, 1, 1, 1, 1, 1, 1, 1, 1])\n             self.assertEqual(self.candidate_generator.probs, [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])\n-            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.4)\n+            self.assertEqual(self.candidate_generator.assistant_generation_config.assistant_confidence_threshold, 0.4)\n         else:\n             self.assert_no_sklearn()\n \n@@ -4925,7 +4952,7 @@ def test_update_candidate_strategy_with_matches_3(self, sklearn_available):\n             self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n             self.assertEqual(self.candidate_generator.matches, [1, 1, 1, 1, 1, 1, 1, 1, 0])\n             self.assertEqual(self.candidate_generator.probs, [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2, 0.1])\n-            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.2)\n+            self.assertEqual(self.candidate_generator.assistant_generation_config.assistant_confidence_threshold, 0.2)\n         else:\n             self.assert_no_sklearn()\n \n@@ -4938,7 +4965,7 @@ def test_update_candidate_strategy_with_matches_2(self, sklearn_available):\n             self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n             self.assertEqual(self.candidate_generator.matches, [1, 1, 1, 1, 1, 1, 1, 0])\n             self.assertEqual(self.candidate_generator.probs, [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3, 0.2])\n-            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.3)\n+            self.assertEqual(self.candidate_generator.assistant_generation_config.assistant_confidence_threshold, 0.3)\n         else:\n             self.assert_no_sklearn()\n \n@@ -4951,7 +4978,7 @@ def test_update_candidate_strategy_with_matches_1(self, sklearn_available):\n             self.candidate_generator.update_candidate_strategy(self.input_ids, None, self.num_matches)\n             self.assertEqual(self.candidate_generator.matches, [1, 1, 1, 1, 1, 1, 0])\n             self.assertEqual(self.candidate_generator.probs, [0.9, 0.8, 0.7, 0.6, 0.5, 0.4, 0.3])\n-            self.assertEqual(self.assistant_model.generation_config.assistant_confidence_threshold, 0.4)\n+            self.assertEqual(self.candidate_generator.assistant_generation_config.assistant_confidence_threshold, 0.4)\n         else:\n             self.assert_no_sklearn()\n "
        },
        {
            "sha": "5f7f199a52943a5b5eadfe4d28a4dc05c3148ede",
            "filename": "tests/models/bart/test_modeling_bart.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbart%2Ftest_modeling_bart.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -987,7 +987,7 @@ def test_xsum_summarization_same_as_fairseq(self):\n \n     def test_xsum_config_generation_params(self):\n         model = BartForConditionalGeneration.from_pretrained(\"facebook/bart-large-xsum\")\n-        expected_params = {\"num_beams\": 6, \"do_sample\": False, \"early_stopping\": True, \"length_penalty\": 1.0}\n+        expected_params = {\"num_beams\": 6, \"do_sample\": False, \"early_stopping\": True, \"length_penalty\": None}\n         config_params = {k: getattr(model.generation_config, k, \"MISSING\") for k, v in expected_params.items()}\n         self.assertDictEqual(expected_params, config_params)\n "
        },
        {
            "sha": "d8341fefa82b63127bfd59a9d4b586d9a3f817a7",
            "filename": "tests/models/encoder_decoder/test_modeling_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fencoder_decoder%2Ftest_modeling_encoder_decoder.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -499,6 +499,7 @@ def check_encoder_decoder_model_generate(self, input_ids, config, decoder_config\n         if hasattr(enc_dec_model.generation_config, \"eos_token_id\"):\n             enc_dec_model.generation_config.eos_token_id = None\n         enc_dec_model.to(torch_device)\n+        enc_dec_model.generation_config.max_length = 20\n \n         # Bert does not have a bos token id, so use pad_token_id instead\n         generated_output = enc_dec_model.generate("
        },
        {
            "sha": "49243ce9befa46095b005182e0d8673101d7eed0",
            "filename": "tests/models/reformer/test_modeling_reformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Freformer%2Ftest_modeling_reformer.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -408,11 +408,11 @@ def create_and_check_reformer_model_fp16_forward(self, config, input_ids, input_\n     def create_and_check_reformer_model_generate(self, config, input_ids, input_mask, choice_labels):\n         config.is_decoder = True\n         config.lsh_num_chunks_after = 0\n-        config.bos_token_id = 0\n-        config.eos_token_id = None\n-        config.max_length = 20\n \n         model = ReformerModelWithLMHead(config=config)\n+        model.generation_config.bos_token_id = 0\n+        model.generation_config.eos_token_id = None\n+        model.generation_config.max_length = 20\n         model.to(torch_device)\n         model.eval()\n         output = model.generate()"
        },
        {
            "sha": "ce8d0062a422ae5d2f7b7e5fcee43ef28129ed4e",
            "filename": "tests/models/speech_encoder_decoder/test_modeling_speech_encoder_decoder.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeech_encoder_decoder%2Ftest_modeling_speech_encoder_decoder.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -364,6 +364,7 @@ def check_encoder_decoder_model_generate(\n             enc_dec_model.generation_config.eos_token_id = None\n \n         inputs = input_values if input_features is None else input_features\n+        enc_dec_model.generation_config.max_length = 20\n \n         # Bert does not have a bos token id, so use pad_token_id instead\n         generated_output = enc_dec_model.generate("
        },
        {
            "sha": "466817b5afb1777fbb0a2b0052432df75accef0d",
            "filename": "tests/models/vision_encoder_decoder/test_modeling_vision_encoder_decoder.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvision_encoder_decoder%2Ftest_modeling_vision_encoder_decoder.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -308,6 +308,7 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n         enc_dec_model.to(torch_device)\n \n         inputs = pixel_values\n+        enc_dec_model.generation_config.max_length = 20\n \n         # Bert does not have a bos token id, so use pad_token_id instead\n         generated_output = enc_dec_model.generate(\n@@ -879,6 +880,7 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n         if hasattr(enc_dec_model.generation_config, \"eos_token_id\"):\n             enc_dec_model.generation_config.eos_token_id = None\n         enc_dec_model.to(torch_device)\n+        enc_dec_model.generation_config.max_length = 20\n \n         generated_output = enc_dec_model.generate(\n             pixel_values=pixel_values,\n@@ -992,6 +994,7 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n         if hasattr(enc_dec_model.generation_config, \"eos_token_id\"):\n             enc_dec_model.generation_config.eos_token_id = None\n         enc_dec_model.to(torch_device)\n+        enc_dec_model.generation_config.max_length = 20\n \n         generated_output = enc_dec_model.generate(\n             pixel_values=pixel_values,\n@@ -1105,6 +1108,7 @@ def check_encoder_decoder_model_generate(self, config, decoder_config, pixel_val\n         if hasattr(enc_dec_model.generation_config, \"eos_token_id\"):\n             enc_dec_model.generation_config.eos_token_id = None\n         enc_dec_model.to(torch_device)\n+        enc_dec_model.generation_config.max_length = 20\n \n         generated_output = enc_dec_model.generate(\n             pixel_values=pixel_values,"
        },
        {
            "sha": "f7cabdefafb758e75b9038d93b0fce559a1e802a",
            "filename": "tests/utils/test_cache_utils.py",
            "status": "modified",
            "additions": 10,
            "deletions": 5,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Futils%2Ftest_cache_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Futils%2Ftest_cache_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_cache_utils.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -59,11 +59,13 @@\n     from transformers.integrations.executorch import export_with_dynamic_cache\n \n \n+# FIXME: offloaded cache is skipped becase it needs `offload_only_non_sliding=False`\n+# but we can't configure cache through `generate()`\n TEST_CACHE_IMPLEMENTATIONS = [\n     cache_name\n     for cache_name in ALL_CACHE_IMPLEMENTATIONS\n     # TODO (joao): offloaded_hybrid == offloaded_hybrid_chunked, deprecate one of them\n-    if cache_name != \"offloaded_hybrid\"\n+    if cache_name not in [\"offloaded_hybrid\", \"offloaded_static\", \"offloaded_hybrid_chunked\"]\n ]\n \n \n@@ -467,10 +469,13 @@ def test_data_parallel_dynamic_cache(self):\n \n         # Check that the caches are the same\n         for layer_idx in range(len(no_parallelism_cache)):\n-            for kv_idx in range(2):  # 0 = key, 1 = value\n-                torch.testing.assert_close(\n-                    actual=parallelism_cache[layer_idx][kv_idx], expected=no_parallelism_cache[layer_idx][kv_idx]\n-                )\n+            torch.testing.assert_close(\n+                actual=parallelism_cache.layers[layer_idx].keys, expected=no_parallelism_cache.layers[layer_idx].keys\n+            )\n+            torch.testing.assert_close(\n+                actual=parallelism_cache.layers[layer_idx].values,\n+                expected=no_parallelism_cache.layers[layer_idx].values,\n+            )\n \n     @require_torch_gpu\n     def test_static_cache_no_cuda_graph_skips(self):"
        },
        {
            "sha": "a5a3254516bfabbb8f011099fab3d91fae343e6e",
            "filename": "tests/utils/test_configuration_utils.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Futils%2Ftest_configuration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Futils%2Ftest_configuration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_configuration_utils.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -257,13 +257,13 @@ def test_saving_config_with_custom_generation_kwargs_raises_error(self):\n             with self.assertRaises(ValueError):\n                 config.save_pretrained(tmp_dir)\n \n-    def test_get_non_default_generation_parameters(self):\n+    def test_get_generation_parameters(self):\n         config = BertConfig()\n-        self.assertFalse(len(config._get_non_default_generation_parameters()) > 0)\n+        self.assertFalse(len(config._get_generation_parameters()) > 0)\n         config.min_length = 3\n-        self.assertTrue(len(config._get_non_default_generation_parameters()) > 0)\n-        config.min_length = 0  # `min_length = 0` is a default generation kwarg\n-        self.assertFalse(len(config._get_non_default_generation_parameters()) > 0)\n+        self.assertTrue(len(config._get_generation_parameters()) > 0)\n+        config.min_length = 0\n+        self.assertTrue(len(config._get_generation_parameters()) > 0)\n \n     def test_loading_config_do_not_raise_future_warnings(self):\n         \"\"\"Regression test for https://github.com/huggingface/transformers/issues/31002.\"\"\""
        },
        {
            "sha": "68e4b50070dc47c665e4004a2187d2a8365e8be8",
            "filename": "tests/utils/test_modeling_utils.py",
            "status": "modified",
            "additions": 4,
            "deletions": 7,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Futils%2Ftest_modeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a81e04a92371a6b1ba80018f012b2467bab5d61f/tests%2Futils%2Ftest_modeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_modeling_utils.py?ref=a81e04a92371a6b1ba80018f012b2467bab5d61f",
            "patch": "@@ -1682,7 +1682,7 @@ def test_saving_model_config_with_generation_params(self):\n         Calling `model.save_pretrained` with generation parameters should raise a `ValueError`\n         \"\"\"\n         model = AutoModelForCausalLM.from_pretrained(\"openai-community/gpt2\")\n-        self.assertTrue(model.generation_config.repetition_penalty == 1.0)\n+        self.assertTrue(model.generation_config.repetition_penalty is None)\n         self.assertFalse(hasattr(model.config, \"repetition_penalty\"))\n \n         # If the user attempts to save a custom generation parameter, we raise an Error\n@@ -1791,11 +1791,8 @@ def test_save_and_load_config_with_custom_generation(self):\n         \"\"\"\n         model = T5ForConditionalGeneration.from_pretrained(TINY_T5)\n \n-        # The default for `num_beams` is 1 and `early_stopping` is False\n-        # NOTE: accessible only from generation config, EVEN IF they are saved\n-        # in `config.json` file in the hub\n-        self.assertTrue(model.generation_config.num_beams == 1)\n-        self.assertTrue(model.generation_config.early_stopping is False)\n+        self.assertTrue(model.generation_config.num_beams is None)\n+        self.assertTrue(model.generation_config.early_stopping is None)\n         self.assertFalse(hasattr(model.config, \"num_beams\"))\n         self.assertFalse(hasattr(model.config, \"early_stopping\"))\n \n@@ -1809,7 +1806,7 @@ def test_save_and_load_config_with_custom_generation(self):\n         # we will throw an error, nudging user to save attributes in the generation_config\n         model.config.num_beams = 5\n         model.config.early_stopping = True\n-        self.assertTrue(model.generation_config.num_beams == 1)  # unmodified generation config\n+        self.assertTrue(model.generation_config.num_beams is None)  # default value\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             with self.assertRaises(ValueError):\n                 model.save_pretrained(tmp_dir)"
        }
    ],
    "stats": {
        "total": 679,
        "additions": 314,
        "deletions": 365
    }
}