{
    "author": "NielsRogge",
    "message": "[Docs] Improve VLM docs (#33393)\n\n* Improve docs\r\n\r\n* Update docs/source/en/model_doc/llava.md\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Update docs/source/en/model_doc/llava.md\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>\r\n\r\n* Address comment\r\n\r\n* Address comment\r\n\r\n* Improve pixtral docs\r\n\r\n---------\r\n\r\nCo-authored-by: amyeroberts <22614925+amyeroberts@users.noreply.github.com>",
    "sha": "f5aeb7c1a54465ac39859bd23420b03ae633e5f7",
    "files": [
        {
            "sha": "3c9ada777e67e80cfb47d3f62ce0c44435e3d1f3",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5aeb7c1a54465ac39859bd23420b03ae633e5f7/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5aeb7c1a54465ac39859bd23420b03ae633e5f7/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=f5aeb7c1a54465ac39859bd23420b03ae633e5f7",
            "patch": "@@ -536,12 +536,8 @@\n         title: QDQBert\n       - local: model_doc/qwen2\n         title: Qwen2\n-      - local: model_doc/qwen2_audio\n-        title: Qwen2Audio\n       - local: model_doc/qwen2_moe\n         title: Qwen2MoE\n-      - local: model_doc/qwen2_vl\n-        title: Qwen2VL\n       - local: model_doc/rag\n         title: RAG\n       - local: model_doc/realm\n@@ -888,6 +884,10 @@\n         title: Pix2Struct\n       - local: model_doc/pixtral\n         title: Pixtral\n+      - local: model_doc/qwen2_audio\n+        title: Qwen2Audio\n+      - local: model_doc/qwen2_vl\n+        title: Qwen2VL\n       - local: model_doc/sam\n         title: Segment Anything\n       - local: model_doc/siglip"
        },
        {
            "sha": "99950a2ffd8e9394a4248b2b8b13264c7021aeaa",
            "filename": "docs/source/en/model_doc/llava.md",
            "status": "modified",
            "additions": 57,
            "deletions": 2,
            "changes": 59,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5aeb7c1a54465ac39859bd23420b03ae633e5f7/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5aeb7c1a54465ac39859bd23420b03ae633e5f7/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava.md?ref=f5aeb7c1a54465ac39859bd23420b03ae633e5f7",
            "patch": "@@ -40,7 +40,9 @@ The original code can be found [here](https://github.com/haotian-liu/LLaVA/tree/\n \n - Note the model has not been explicitly trained to process multiple images in the same prompt, although this is technically possible, you may experience inaccurate results.\n \n-- For better results, we recommend users to use the processor's `apply_chat_template()` method to format your prompt correctly. For that you need to construct a conversation history, passing in a plain string will not format your prompt. Each message in the conversation history for chat templates is a dictionary with keys \"role\" and \"content\". The \"content\" should be a list of dictionaries, for \"text\" and \"image\" modalities, as follows:\n+### Single image inference\n+\n+For best results, we recommend users to use the processor's `apply_chat_template()` method to format your prompt correctly. For that you need to construct a conversation history, passing in a plain string will not format your prompt. Each message in the conversation history for chat templates is a dictionary with keys \"role\" and \"content\". The \"content\" should be a list of dictionaries, for \"text\" and \"image\" modalities, as follows:\n \n ```python\n from transformers import AutoProcessor\n@@ -75,6 +77,60 @@ print(text_prompt)\n >>> \"USER: <image>\\n<Whatâ€™s shown in this image? ASSISTANT: This image shows a red stop sign.</s>USER: Describe the image in more details. ASSISTANT:\"\n ```\n \n+### Batched inference\n+\n+LLaVa also supports batched inference. Here is how you can do it:\n+\n+```python\n+import requests\n+from PIL import Image\n+import torch\n+from transformers import AutoProcessor, LLavaForConditionalGeneration\n+\n+# Load the model in half-precision\n+model = LLavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\", torch_dtype=torch.float16, device_map=\"auto\")\n+processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n+\n+# Get two different images\n+url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+image_stop = Image.open(requests.get(url, stream=True).raw)\n+\n+url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n+image_cats = Image.open(requests.get(url, stream=True).raw)\n+\n+# Prepare a batch of two prompts\n+conversation_1 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ],\n+    },\n+]\n+\n+conversation_2 = [\n+    {\n+        \"role\": \"user\",\n+        \"content\": [\n+            {\"type\": \"image\"},\n+            {\"type\": \"text\", \"text\": \"What is shown in this image?\"},\n+        ],\n+    },\n+]\n+\n+prompt_1 = processor.apply_chat_template(conversation_1, add_generation_prompt=True)\n+prompt_2 = processor.apply_chat_template(conversation_2, add_generation_prompt=True)\n+prompts = [prompt_1, prompt_2]\n+\n+# We can simply feed images in the order they have to be used in the text prompt\n+inputs = processor(images=[image_stop, image_cats, image_snowman], text=prompts, padding=True, return_tensors=\"pt\").to(model.device, torch.float16)\n+\n+# Generate\n+generate_ids = model.generate(**inputs, max_new_tokens=30)\n+processor.batch_decode(generate_ids, skip_special_tokens=True)\n+```\n+\n - If you want to construct a chat prompt yourself, below is a list of prompt formats accepted by each llava checkpoint:\n \n [llava-interleave models](https://huggingface.co/collections/llava-hf/llava-interleave-668e19a97da0036aad4a2f19) requires the following format:\n@@ -99,7 +155,6 @@ For multiple turns conversation:\n \"USER: <image>\\n<prompt1> ASSISTANT: <answer1></s>USER: <prompt2> ASSISTANT: <answer2></s>USER: <prompt3> ASSISTANT:\"\n ```\n \n-\n ### Using Flash Attention 2\n \n Flash Attention 2 is an even faster, optimized version of the previous optimization, please refer to the [Flash Attention 2 section of performance docs](https://huggingface.co/docs/transformers/perf_infer_gpu_one)."
        },
        {
            "sha": "b6b0a2bfa1d123f968a8725b72efdb988937024f",
            "filename": "docs/source/en/model_doc/llava_onevision.md",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5aeb7c1a54465ac39859bd23420b03ae633e5f7/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5aeb7c1a54465ac39859bd23420b03ae633e5f7/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fllava_onevision.md?ref=f5aeb7c1a54465ac39859bd23420b03ae633e5f7",
            "patch": "@@ -14,13 +14,13 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# LLaVA-Onevision\n+# LLaVA-OneVision\n \n ## Overview\n \n-The LLaVA-Onevision model was proposed in [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326) by <Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li\n+The LLaVA-OneVision model was proposed in [LLaVA-OneVision: Easy Visual Task Transfer](https://arxiv.org/abs/2408.03326) by <Bo Li, Yuanhan Zhang, Dong Guo, Renrui Zhang, Feng Li, Hao Zhang, Kaichen Zhang, Yanwei Li, Ziwei Liu, Chunyuan Li\n \n-LLaVA-Onevision is a Vision-Language Model that can generate text conditioned on one or several images/videos. The model consists of SigLIP vision encoder and a Qwen2 language backbone. The images are processed with anyres-9 technique where the image is split into 9 patches to better process high resolution images and capture as much details as possible. However, videos are pooled to a total sequence length of 196 tokens each frame for more memory efficient computation. LLaVA-Onevision is available in three sizes: 0.5B, 7B and 72B and achieves remarkable performance on benchmark evaluations.\n+LLaVA-OneVision is a Vision-Language Model that can generate text conditioned on one or several images/videos. The model consists of SigLIP vision encoder and a Qwen2 language backbone. The images are processed with anyres-9 technique where the image is split into 9 patches to better process high resolution images and capture as much details as possible. However, videos are pooled to a total sequence length of 196 tokens each frame for more memory efficient computation. LLaVA-OneVision is available in three sizes: 0.5B, 7B and 72B and achieves remarkable performance on benchmark evaluations.\n \n The abstract from the paper is the following:\n \n@@ -32,19 +32,18 @@ yielding new emerging capabilities. In particular, strong video understanding an\n cross-scenario capabilities are demonstrated through task transfer from images to\n videos.*\n \n-\n <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/llava-ov-acrhitecture.png\"\n alt=\"drawing\" width=\"600\"/>\n \n-<small> LLaVA=Onevision architecture. Taken from the <a href=\"https://arxiv.org/abs/2408.03326\">original paper.</a> </small>\n+<small> LLaVA-OneVision architecture. Taken from the <a href=\"https://arxiv.org/abs/2408.03326\">original paper.</a> </small>\n \n Tips:\n \n - We advise users to use `padding_side=\"left\"` when computing batched generation as it leads to more accurate results. Simply make sure to call `processor.tokenizer.padding_side = \"left\"` before generating.\n \n <Tip warning={true}>\n \n-- Llava-Onevision uses different number of patches for images and thus has to pad the inputs inside modeling code, aside from the padding done when processing the inputs. The default setting is \"left-padding\" if model is in `eval()` mode, otherwise \"right-padding\".\n+- Llava-OneVision uses different number of patches for images and thus has to pad the inputs inside modeling code, aside from the padding done when processing the inputs. The default setting is \"left-padding\" if model is in `eval()` mode, otherwise \"right-padding\".\n \n </Tip>\n \n@@ -129,7 +128,7 @@ print(processor.decode(output[0], skip_special_tokens=True))\n \n ### Multi image inference\n \n-LLaVa-Onevision can perform inference with multiple images as input, where images either belong to the same prompt or different prompts (in batched inference). For that you have to use checkpoints with an \"ov\" suffix. Here is how you can do it:\n+LLaVa-OneVision can perform inference with multiple images as input, where images either belong to the same prompt or different prompts (in batched inference). For that you have to use checkpoints with an \"ov\" suffix. Here is how you can do it:\n \n ```python\n import requests\n@@ -200,7 +199,7 @@ processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokeniza\n \n ### Video inference\n \n-LLaVa-Onevision also can perform inference with videos as input, where video frames are treated as multiple images. Here is how you can do it:\n+LLaVa-OneVision also can perform inference with videos as input, where video frames are treated as multiple images. Here is how you can do it:\n \n ```python\n import av"
        },
        {
            "sha": "ab604e4521fc732c0fb8c27e5bce32544439b331",
            "filename": "docs/source/en/model_doc/pixtral.md",
            "status": "modified",
            "additions": 32,
            "deletions": 39,
            "changes": 71,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5aeb7c1a54465ac39859bd23420b03ae633e5f7/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5aeb7c1a54465ac39859bd23420b03ae633e5f7/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fpixtral.md?ref=f5aeb7c1a54465ac39859bd23420b03ae633e5f7",
            "patch": "@@ -18,69 +18,62 @@ rendered properly in your Markdown viewer.\n \n ## Overview\n \n-The Pixtral model was released by the Mistral AI team on [vLLM](https://github.com/vllm-project/vllm/pull/8377), where a version of the code can be found!\n+The Pixtral model was released by the Mistral AI team in a [blog post](https://mistral.ai/news/pixtral-12b/). Pixtral is a multimodal version of [Mistral](mistral), incorporating a 400 million parameter vision encoder trained from scratch.\n+\n+The intro from the blog says the following:\n+\n+*Pixtral is trained to understand both natural images and documents, achieving 52.5% on the MMMU reasoning benchmark, surpassing a number of larger models. The model shows strong abilities in tasks such as chart and figure understanding, document question answering, multimodal reasoning and instruction following. Pixtral is able to ingest images at their natural resolution and aspect ratio, giving the user flexibility on the number of tokens used to process an image. Pixtral is also able to process any number of images in its long context window of 128K tokens. Unlike previous open-source models, Pixtral does not compromise on text benchmark performance to excel in multimodal tasks.*\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/pixtral_architecture.webp\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> Pixtral architecture. Taken from the <a href=\"https://mistral.ai/news/pixtral-12b/\">blog post.</a> </small>\n \n Tips:\n \n - Pixtral is a multimodal model, taking images and text as input, and producing text as output.\n-- This model follows the [Llava](llava) family, meaning image embeddings are placed instead of the `[IMG]` token placeholders. The model uses [`PixtralVisionModel`] for its vision encoder, and [`MistralForCausalLM`] for its language decoder.\n-- The main contribution is the 2d ROPE (rotary postiion embeddings) on the images, and support for arbitrary image sizes (the images are not padded together nor are they resized).\n-- The format for one or mulitple prompts is the following:\n+- This model follows the [Llava](llava) architecture. The model uses [`PixtralVisionModel`] for its vision encoder, and [`MistralForCausalLM`] for its language decoder.\n+- The main contribution is the 2d ROPE (rotary position embeddings) on the images, and support for arbitrary image sizes (the images are not padded together nor are they resized).\n+- Similar to [Llava](llava), the model internally replaces the `[IMG]` token placeholders by image embeddings from the vision encoder. The format for one or multiple prompts is the following:\n ```\n \"<s>[INST][IMG]\\nWhat are the things I should be cautious about when I visit this place?[/INST]\"\n ```\n-Then, the processor will replace each `[IMG]` token with  a number of `[IMG]` token that depends on the height and the width of the image. Each *row* of the image is separated by a `[IMG_BREAK]` token, and each image is separated by a  `[IMG_END]` token.\n+Then, the processor will replace each `[IMG]` token with a number of `[IMG]` tokens that depend on the height and the width of each image. Each *row* of the image is separated by an `[IMG_BREAK]` token, and each image is separated by an `[IMG_END]` token. It's advised to use the `apply_chat_template` method of the processor, which takes care of all of this. See the [usage section](#usage) for more info.\n \n This model was contributed by [amyeroberts](https://huggingface.co/amyeroberts) and [ArthurZ](https://huggingface.co/ArthurZ). The original code can be found [here](https://github.com/vllm-project/vllm/pull/8377).\n \n ## Usage\n \n-Here is an example of how to run it:\n+At inference time, it's advised to use the processor's `apply_chat_template` method, which correctly formats the prompt for the model:\n \n ```python\n-from transformers import LlavaForConditionalGeneration, AutoProcessor\n+from transformers import AutoProcessor, LlavaForConditionalGeneration\n from PIL import Image\n \n model_id = \"mistral-community/pixtral-12b\"\n-model = LlavaForConditionalGeneration.from_pretrained(model_id).to(\"cuda\")\n processor = AutoProcessor.from_pretrained(model_id)\n+model = LlavaForConditionalGeneration.from_pretrained(model_id).to(\"cuda\")\n \n-IMG_URLS = [\n-    \"https://picsum.photos/id/237/400/300\",\n-    \"https://picsum.photos/id/231/200/300\",\n-    \"https://picsum.photos/id/27/500/500\",\n-    \"https://picsum.photos/id/17/150/600\",\n+url_dog = \"https://picsum.photos/id/237/200/300\"\n+url_mountain = \"https://picsum.photos/seed/picsum/200/300\"\n+\n+chat = [\n+    {\n+      \"role\": \"user\", \"content\": [\n+        {\"type\": \"text\", \"content\": \"Can this animal\"}, \n+        {\"type\": \"image\"}, \n+        {\"type\": \"text\", \"content\": \"live here?\"}, \n+        {\"type\": \"image\"}\n+      ]\n+    }\n ]\n-PROMPT = \"<s>[INST]Describe the images.\\n[IMG][IMG][IMG][IMG][/INST]\"\n \n-inputs = processor(images=IMG_URLS, text=PROMPT, return_tensors=\"pt\").to(\"cuda\")\n+prompt = processor.apply_chat_template(chat)\n+inputs = processor(text=prompt, images=[url_dog, url_mountain], return_tensors=\"pt\").to(model.device)\n generate_ids = model.generate(**inputs, max_new_tokens=500)\n output = processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n-\n-EXPECTED_GENERATION = \"\"\"\n-Describe the images.\n-Sure, let's break down each image description:\n-\n-1. **Image 1:**\n-   - **Description:** A black dog with a glossy coat is sitting on a wooden floor. The dog has a focused expression and is looking directly at the camera.\n-   - **Details:** The wooden floor has a rustic appearance with visible wood grain patterns. The dog's eyes are a striking color, possibly brown or amber, which contrasts with its black fur.\n-\n-2. **Image 2:**\n-   - **Description:** A scenic view of a mountainous landscape with a winding road cutting through it. The road is surrounded by lush green vegetation and leads to a distant valley.\n-   - **Details:** The mountains are rugged with steep slopes, and the sky is clear, indicating good weather. The winding road adds a sense of depth and perspective to the image.\n-\n-3. **Image 3:**\n-   - **Description:** A beach scene with waves crashing against the shore. There are several people in the water and on the beach, enjoying the waves and the sunset.\n-   - **Details:** The waves are powerful, creating a dynamic and lively atmosphere. The sky is painted with hues of orange and pink from the setting sun, adding a warm glow to the scene.\n-\n-4. **Image 4:**\n-   - **Description:** A garden path leading to a large tree with a bench underneath it. The path is bordered by well-maintained grass and flowers.\n-   - **Details:** The path is made of small stones or gravel, and the tree provides a shaded area with the bench invitingly placed beneath it. The surrounding area is lush and green, suggesting a well-kept garden.\n-\n-Each image captures a different scene, from a close-up of a dog to expansive natural landscapes, showcasing various elements of nature and human interaction with it.\n-\"\"\"\n-\n ```\n+\n ## PixtralVisionConfig\n \n [[autodoc]] PixtralVisionConfig"
        },
        {
            "sha": "7c864b860bd8ea848c5f91e7c6ecc6ee8bdb5000",
            "filename": "docs/source/en/model_doc/qwen2_vl.md",
            "status": "modified",
            "additions": 17,
            "deletions": 14,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/f5aeb7c1a54465ac39859bd23420b03ae633e5f7/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/f5aeb7c1a54465ac39859bd23420b03ae633e5f7/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fqwen2_vl.md?ref=f5aeb7c1a54465ac39859bd23420b03ae633e5f7",
            "patch": "@@ -14,17 +14,22 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# Qwen2_VL\n-\n+# Qwen2-VL\n \n ## Overview\n \n-The [Qwen2_VL](https://qwenlm.github.io/blog/qwen2-vl/) is a major update to our [Qwen-VL](https://arxiv.org/pdf/2308.12966) model from the Qwen team. \n+The [Qwen2-VL](https://qwenlm.github.io/blog/qwen2-vl/) model is a major update to [Qwen-VL](https://arxiv.org/pdf/2308.12966) from the Qwen team at Alibaba Research. \n \n The abstract from the blog is the following:\n \n *This blog introduces Qwen2-VL, an advanced version of the Qwen-VL model that has undergone significant enhancements over the past year. Key improvements include enhanced image comprehension, advanced video understanding, integrated visual agent functionality, and expanded multilingual support. The model architecture has been optimized for handling arbitrary image resolutions through Naive Dynamic Resolution support and utilizes Multimodal Rotary Position Embedding (M-ROPE) to effectively process both 1D textual and multi-dimensional visual data. This updated model demonstrates competitive performance against leading AI systems like GPT-4o and Claude 3.5 Sonnet in vision-related tasks and ranks highly among open-source models in text capabilities. These advancements make Qwen2-VL a versatile tool for various applications requiring robust multimodal processing and reasoning abilities.*\n \n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/qwen2_vl_architecture.jpeg\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> Qwen2-VL architecture. Taken from the <a href=\"https://qwenlm.github.io/blog/qwen2-vl/\">blog post.</a> </small>\n+\n+This model was contributed by [simonJJJ](https://huggingface.co/simonJJJ).\n \n ## Usage example\n \n@@ -78,8 +83,6 @@ generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(in\n output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n print(output_text)\n \n-\n-\n # Video\n def fetch_video(ele: Dict, nframe_factor=2):\n     if isinstance(ele['video'], str):\n@@ -130,16 +133,13 @@ output_ids = model.generate(**inputs, max_new_tokens=128)\n generated_ids = [output_ids[len(input_ids):] for input_ids, output_ids in zip(inputs.input_ids, output_ids)]\n output_text = processor.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n print(output_text)\n-\n ```\n \n-\n ### Batch Mixed Media Inference\n \n The model can batch inputs composed of mixed samples of various types such as images, videos, and text. Here is an example.\n \n ```python\n-\n image1 = Image.open(\"/path/to/image1.jpg\")\n image2 = Image.open(\"/path/to/image2.jpg\")\n image3 = Image.open(\"/path/to/image3.jpg\")\n@@ -217,26 +217,30 @@ print(output_text)\n \n ### Usage Tips\n \n-#### Image Resolution for performance boost\n+#### Image Resolution trade-off\n \n The model supports a wide range of resolution inputs. By default, it uses the native resolution for input, but higher resolutions can enhance performance at the cost of more computation. Users can set the minimum and maximum number of pixels to achieve an optimal configuration for their needs.\n \n ```python\n-\n min_pixels = 224*224\n max_pixels = 2048*2048\n processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n+```\n+\n+In case of limited GPU RAM, one can reduce the resolution as follows:\n \n+```python\n+min_pixels = 256*28*28\n+max_pixels = 1024*28*28 \n+processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2-VL-7B-Instruct\", min_pixels=min_pixels, max_pixels=max_pixels)\n ```\n+This ensures each image gets encoded using a number between 256-1024 tokens. The 28 comes from the fact that the model uses a patch size of 14 and a temporal patch size of 2 (14 x 2 = 28).\n \n #### Multiple Image Inputs\n \n By default, images and video content are directly included in the conversation. When handling multiple images, it's helpful to add labels to the images and videos for better reference. Users can control this behavior with the following settings:\n \n-\n-\n ```python\n-\n conversation = [\n     {\n         \"role\": \"user\",\n@@ -302,7 +306,6 @@ model = Qwen2VLForConditionalGeneration.from_pretrained(\n )\n ```\n \n-\n ## Qwen2VLConfig\n \n [[autodoc]] Qwen2VLConfig"
        }
    ],
    "stats": {
        "total": 184,
        "additions": 117,
        "deletions": 67
    }
}