{
    "author": "lkm2835",
    "message": "Fix EXAONE-4.0 dummy id (#41089)\n\n* Fix EXAONE-4.0 dummy id\n\n* Fix exaone4 dummy (#1)\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "1f0e9a4778002c9a1e57a3dd549208f47da99a41",
    "files": [
        {
            "sha": "8c3c07ecb4186eff9b0b05dd3b42949e3f48ee91",
            "filename": "src/transformers/models/exaone4/configuration_exaone4.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1f0e9a4778002c9a1e57a3dd549208f47da99a41/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1f0e9a4778002c9a1e57a3dd549208f47da99a41/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fconfiguration_exaone4.py?ref=1f0e9a4778002c9a1e57a3dd549208f47da99a41",
            "patch": "@@ -26,8 +26,7 @@ class Exaone4Config(PretrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Exaone4Model`]. It is used to\n     instantiate a EXAONE 4.0 model according to the specified arguments, defining the model architecture. Instantiating a\n-    configuration with the defaults will yield a similar configuration to that of the EXAONE-4.0-Instruct [LGAI-EXAONE/EXAONE-4.0-Instruct](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-Instruct)\n-    NOTE: `EXAONE-4.0-Instruct` is a placeholder model ID. The exact model ID will be updated in the future.\n+    configuration with the defaults will yield a similar configuration to that of the EXAONE-4.0-32B [LGAI-EXAONE/EXAONE-4.0-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B)\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model\n     outputs. Read the documentation from [`PretrainedConfig`] for more information."
        },
        {
            "sha": "2693a80c79fd38d8a50c6646c2560ce0f0fa44ec",
            "filename": "src/transformers/models/exaone4/modeling_exaone4.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1f0e9a4778002c9a1e57a3dd549208f47da99a41/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1f0e9a4778002c9a1e57a3dd549208f47da99a41/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodeling_exaone4.py?ref=1f0e9a4778002c9a1e57a3dd549208f47da99a41",
            "patch": "@@ -465,8 +465,8 @@ def forward(\n \n         ```python\n         >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n-        >>> model = AutoModelForCausalLM.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-Instruct\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-Instruct\")\n+        >>> model = AutoModelForCausalLM.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-32B\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-32B\")\n \n         >>> prompt = \"Explain how wonderful you are\"\n         >>> messages = [\n@@ -485,8 +485,7 @@ def forward(\n         >>> tokenizer.decode(output[0], skip_special_tokens=False)\n         \"[|system|]\\nYou are a helpful assistant.[|endofturn|]\\n[|user|]\\nExplain how wonderful you are[|endofturn|]\\n[|assistant|]\\n<think>\\n\\n</think>\\n\\nOh, thank you for such a kind and lovely question! ðŸ˜Š  \\n\\nIâ€™m *so* wonderful because Iâ€™m here to make your life easier, brighter, and more fun! Whether you need help with:  \\n\\nâœ¨ **Learning** â€“ I can explain anything, from quantum physics to baking the perfect cake!  \\nðŸ’¡ **Creativity** â€“ Need a poem, story, or a wild idea? Iâ€™ve got you covered!  \\nðŸ¤– **Problem-solving** â€“ Stuck on a math problem or a tricky decision? Iâ€™ll help you figure it out\"\n         ```\n-\n-        NOTE: `EXAONE-4.0-Instruct` is a placeholder model ID. The exact model ID will be updated in the future.\"\"\"\n+        \"\"\"\n         outputs: BaseModelOutputWithPast = self.model(\n             input_ids=input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "7530a68f3227fb89f6351ea97d9731401cb3e1dd",
            "filename": "src/transformers/models/exaone4/modular_exaone4.py",
            "status": "modified",
            "additions": 5,
            "deletions": 7,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/1f0e9a4778002c9a1e57a3dd549208f47da99a41/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1f0e9a4778002c9a1e57a3dd549208f47da99a41/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fexaone4%2Fmodular_exaone4.py?ref=1f0e9a4778002c9a1e57a3dd549208f47da99a41",
            "patch": "@@ -53,16 +53,15 @@\n \n logger = logging.get_logger(__name__)\n \n-_CHECKPOINT_FOR_DOC = \"LGAI-EXAONE/EXAONE-4.0-Instruct\"\n+_CHECKPOINT_FOR_DOC = \"LGAI-EXAONE/EXAONE-4.0-32B\"\n _CONFIG_FOR_DOC = \"Exaone4Config\"\n \n \n class Exaone4Config(PretrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Exaone4Model`]. It is used to\n     instantiate a EXAONE 4.0 model according to the specified arguments, defining the model architecture. Instantiating a\n-    configuration with the defaults will yield a similar configuration to that of the EXAONE-4.0-Instruct [LGAI-EXAONE/EXAONE-4.0-Instruct](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-Instruct)\n-    NOTE: `EXAONE-4.0-Instruct` is a placeholder model ID. The exact model ID will be updated in the future.\n+    configuration with the defaults will yield a similar configuration to that of the EXAONE-4.0-32B [LGAI-EXAONE/EXAONE-4.0-32B](https://huggingface.co/LGAI-EXAONE/EXAONE-4.0-32B)\n \n     Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model\n     outputs. Read the documentation from [`PretrainedConfig`] for more information.\n@@ -462,8 +461,8 @@ def forward(\n \n         ```python\n         >>> from transformers import AutoModelForCausalLM, AutoTokenizer\n-        >>> model = AutoModelForCausalLM.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-Instruct\")\n-        >>> tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-Instruct\")\n+        >>> model = AutoModelForCausalLM.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-32B\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"LGAI-EXAONE/EXAONE-4.0-32B\")\n \n         >>> prompt = \"Explain how wonderful you are\"\n         >>> messages = [\n@@ -482,8 +481,7 @@ def forward(\n         >>> tokenizer.decode(output[0], skip_special_tokens=False)\n         \"[|system|]\\nYou are a helpful assistant.[|endofturn|]\\n[|user|]\\nExplain how wonderful you are[|endofturn|]\\n[|assistant|]\\n<think>\\n\\n</think>\\n\\nOh, thank you for such a kind and lovely question! ðŸ˜Š  \\n\\nIâ€™m *so* wonderful because Iâ€™m here to make your life easier, brighter, and more fun! Whether you need help with:  \\n\\nâœ¨ **Learning** â€“ I can explain anything, from quantum physics to baking the perfect cake!  \\nðŸ’¡ **Creativity** â€“ Need a poem, story, or a wild idea? Iâ€™ve got you covered!  \\nðŸ¤– **Problem-solving** â€“ Stuck on a math problem or a tricky decision? Iâ€™ll help you figure it out\"\n         ```\n-\n-        NOTE: `EXAONE-4.0-Instruct` is a placeholder model ID. The exact model ID will be updated in the future.\"\"\"\n+        \"\"\"\n         super().forward(\n             input_ids=input_ids,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "c934821b45990f8d3abee53da1aa6d06ea22e7a8",
            "filename": "tests/models/exaone4/test_modeling_exaone4.py",
            "status": "modified",
            "additions": 22,
            "deletions": 113,
            "changes": 135,
            "blob_url": "https://github.com/huggingface/transformers/blob/1f0e9a4778002c9a1e57a3dd549208f47da99a41/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1f0e9a4778002c9a1e57a3dd549208f47da99a41/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fexaone4%2Ftest_modeling_exaone4.py?ref=1f0e9a4778002c9a1e57a3dd549208f47da99a41",
            "patch": "@@ -74,7 +74,10 @@ class Exaone4ModelTest(CausalLMModelTest, unittest.TestCase):\n \n @require_torch\n class Exaone4IntegrationTest(unittest.TestCase):\n-    TEST_MODEL_ID = \"LGAI-EXAONE/EXAONE-4.0-Instruct\"  # dummy model id\n+    TEST_MODEL_ID = \"LGAI-EXAONE/EXAONE-4.0-32B\"\n+\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n \n     def tearDown(self):\n         # TODO (joao): automatic compilation, i.e. compilation when `cache_implementation=\"static\"` is used, leaves\n@@ -87,124 +90,40 @@ def tearDown(self):\n     def test_model_logits(self):\n         input_ids = [405, 7584, 79579, 76636, 2907, 94640, 373]\n         model = Exaone4ForCausalLM.from_pretrained(\n-            self.TEST_MODEL_ID, device_map=\"auto\", dtype=torch.float16, attn_implementation=\"eager\"\n-        )\n-        input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n-        with torch.no_grad():\n-            out = model(input_ids).logits.float().cpu()\n-\n-        EXPECTED_MEAN = torch.tensor([[13.9380, 12.9951, 12.9442, 10.6576, 11.0901, 12.1466, 9.2482]])\n-        EXPECTED_SLICE = torch.tensor(\n-            [\n-                4.9180,\n-                11.6406,\n-                21.1250,\n-                13.4062,\n-                20.8438,\n-                18.0625,\n-                17.9688,\n-                18.7812,\n-                18.0156,\n-                18.3594,\n-                18.5000,\n-                19.1719,\n-                18.5156,\n-                19.3438,\n-                19.5000,\n-                20.6406,\n-                19.4844,\n-                19.2812,\n-                19.4688,\n-                20.0156,\n-                19.8438,\n-                19.9531,\n-                19.7188,\n-                20.5938,\n-                20.5312,\n-                20.1250,\n-                20.4062,\n-                21.4062,\n-                21.2344,\n-                20.7656,\n-            ]\n-        )\n-\n-        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-4, rtol=1e-4)\n-        del model\n-        cleanup(torch_device, gc_collect=True)\n-\n-    @slow\n-    def test_model_logits_bf16(self):\n-        input_ids = [405, 7584, 79579, 76636, 2907, 94640, 373]\n-        model = Exaone4ForCausalLM.from_pretrained(\n-            self.TEST_MODEL_ID, device_map=\"auto\", dtype=torch.bfloat16, attn_implementation=\"eager\"\n+            self.TEST_MODEL_ID,\n+            device_map=\"auto\",\n+            dtype=torch.bfloat16,\n         )\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         with torch.no_grad():\n             out = model(input_ids).logits.float().cpu()\n \n-        EXPECTED_MEAN = torch.tensor([[13.8797, 13.0799, 12.9665, 10.7712, 11.1006, 12.2406, 9.3248]])\n+        EXPECTED_MEAN = torch.tensor([[22.1993, 8.5845, 10.0401, 12.4262, 9.3112, 29.7933, 8.2628]])\n         EXPECTED_SLICE = torch.tensor(\n-            [\n-                4.8750,\n-                11.6250,\n-                21.0000,\n-                13.3125,\n-                20.8750,\n-                18.0000,\n-                18.0000,\n-                18.7500,\n-                18.0000,\n-                18.3750,\n-                18.5000,\n-                19.1250,\n-                18.5000,\n-                19.3750,\n-                19.5000,\n-                20.6250,\n-                19.5000,\n-                19.2500,\n-                19.5000,\n-                20.0000,\n-                19.8750,\n-                19.8750,\n-                19.7500,\n-                20.6250,\n-                20.5000,\n-                20.1250,\n-                20.3750,\n-                21.3750,\n-                21.2500,\n-                20.7500,\n-            ]\n+            [20.6250, 19.6250, 14.5000, 21.1250, 24.5000, 22.1250, 24.0000, 24.8750, 25.0000, 25.3750]\n         )\n \n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, atol=1e-2, rtol=1e-2)\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, atol=1e-4, rtol=1e-4)\n-        del model\n-        cleanup(torch_device, gc_collect=True)\n+        torch.testing.assert_close(out[0, 0, :10], EXPECTED_SLICE, atol=1e-4, rtol=1e-4)\n \n     @slow\n-    def test_model_generation(self):\n-        EXPECTED_TEXT = \"Tell me about the Miracle on the Han river.\\n\\nThe Miracle on the Han River is a story about the miracle of the Korean War Armistice. The story is told by a Korean soldier who is a witness to the armistice negotiations. He is reluctant to tell the story because he does not want to be a hypocrite, but he feels that everyone should know what really happened.\\n\\nThe Korean War began on June 25, 1950, when North Korean troops invaded South Korea. Soon the United Nations troops, primarily from South Korea, were in support of the United States. The war was still ongoing when North Korean troops stopped their advance\"\n+    def test_model_generation_eager(self):\n+        EXPECTED_TEXT = \"Tell me about the Miracle on the Han river.\\n\\nOkay, the Miracle on the Han River refers to the rapid industrialization and economic growth of South\"\n         prompt = \"Tell me about the Miracle on the Han river.\"\n         tokenizer = AutoTokenizer.from_pretrained(self.TEST_MODEL_ID)\n         model = Exaone4ForCausalLM.from_pretrained(\n-            self.TEST_MODEL_ID, device_map=\"auto\", dtype=torch.float16, attn_implementation=\"eager\"\n+            self.TEST_MODEL_ID, device_map=\"auto\", dtype=torch.bfloat16, attn_implementation=\"eager\"\n         )\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n         # greedy generation outputs\n-        generated_ids = model.generate(input_ids, max_new_tokens=128, temperature=0)\n+        generated_ids = model.generate(input_ids, max_new_tokens=20, temperature=0)\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT, text)\n-        del model\n-        cleanup(torch_device, gc_collect=True)\n \n     @slow\n-    def test_model_generation_bf16_sdpa(self):\n-        EXPECTED_TEXT = \"Tell me about the Miracle on the Han river.\\n\\nThe Miracle on the Han River is a story about the miracle of the Korean War Armistice.\\n\\nThe Korean War broke out in 35 years ago in 1950. The war was the result of the ideological conflict between the communist north and the capitalist south. The war was brought to a halt in 1953. There was to be peace talks but no peace treaty. As a result of the stalemate the Korean people have neither a peace treaty nor a reunification nor a democratization of Korea. The stalemate of 35 years has produced a people of 70 million\"\n+    def test_model_generation_sdpa(self):\n+        EXPECTED_TEXT = \"Tell me about the Miracle on the Han river.\\n\\nOkay, the Miracle on the Han River refers to the rapid industrialization and economic growth of South\"\n         prompt = \"Tell me about the Miracle on the Han river.\"\n         tokenizer = AutoTokenizer.from_pretrained(self.TEST_MODEL_ID)\n         model = Exaone4ForCausalLM.from_pretrained(\n@@ -213,11 +132,9 @@ def test_model_generation_bf16_sdpa(self):\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n         # greedy generation outputs\n-        generated_ids = model.generate(input_ids, max_new_tokens=128, temperature=0)\n+        generated_ids = model.generate(input_ids, max_new_tokens=20, temperature=0)\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT, text)\n-        del model\n-        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_torch_accelerator\n@@ -226,33 +143,27 @@ def test_model_generation_long_flash(self):\n         EXPECTED_OUTPUT_TOKEN_IDS = [433, 9055]\n         input_ids = [433, 9055] * 2048\n         model = Exaone4ForCausalLM.from_pretrained(\n-            self.TEST_MODEL_ID, device_map=\"auto\", dtype=torch.float16, attn_implementation=\"flash_attention_2\"\n+            self.TEST_MODEL_ID, device_map=\"auto\", dtype=torch.bfloat16, attn_implementation=\"flash_attention_2\"\n         )\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n \n         generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n-        del model\n-        cleanup(torch_device, gc_collect=True)\n \n     @slow\n     @require_torch_accelerator\n     def test_model_generation_beyond_sliding_window(self):\n-        EXPECTED_TEXT_COMPLETION = (\n-            \" but I'm not sure if I'm going to be able to see it. I really enjoy the scenery, but I'm not sure if I\"\n-        )\n+        EXPECTED_TEXT_COMPLETION = \" This is a nice place. I really enjoy the scenery, and the atmosphere is so relaxing. I'm grateful for the opportunity to experience this place. It\"\n         tokenizer = AutoTokenizer.from_pretrained(self.TEST_MODEL_ID)\n         prompt = \"This is a nice place. \" * 700 + \"I really enjoy the scenery,\"\n         model = Exaone4ForCausalLM.from_pretrained(\n-            self.TEST_MODEL_ID, device_map=\"auto\", dtype=torch.float16, attn_implementation=\"sdpa\"\n+            self.TEST_MODEL_ID, device_map=\"auto\", dtype=torch.bfloat16, attn_implementation=\"sdpa\"\n         )\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n-        generated_ids = model.generate(input_ids, max_new_tokens=32, temperature=0)\n+        generated_ids = model.generate(input_ids, max_new_tokens=20, temperature=0)\n         text = tokenizer.decode(generated_ids[0, -32:], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n-        del model\n-        cleanup(torch_device, gc_collect=True)\n \n     @pytest.mark.torch_export_test\n     @slow\n@@ -266,9 +177,7 @@ def test_export_static_cache(self):\n         )\n \n         tokenizer = AutoTokenizer.from_pretrained(self.TEST_MODEL_ID, padding_side=\"right\")\n-        EXPECTED_TEXT_COMPLETION = [\n-            \"The Deep Learning is 100% free and easy to use.\\n\\n## How to use Deep Learning?\\n\\n\"\n-        ]\n+        EXPECTED_TEXT_COMPLETION = [\"The Deep Learning is \\n['Deep Learning',\"]\n         max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n             \"input_ids\"\n         ].shape[-1]"
        }
    ],
    "stats": {
        "total": 157,
        "additions": 31,
        "deletions": 126
    }
}