{
    "author": "qubvel",
    "message": "Fix tests for vision models (#35654)\n\n* Trigger tests\r\n\r\n* [run-slow] beit, detr, dinov2, vit, textnet\r\n\r\n* Fix BEiT interpolate_pos_encoding\r\n\r\n* Fix DETR test\r\n\r\n* Update DINOv2 test\r\n\r\n* Fix textnet\r\n\r\n* Fix vit\r\n\r\n* Fix DPT\r\n\r\n* fix data2vec test\r\n\r\n* Fix textnet test\r\n\r\n* Update interpolation check\r\n\r\n* Fix ZoeDepth tests\r\n\r\n* Update interpolate embeddings for BEiT\r\n\r\n* Apply suggestions from code review",
    "sha": "d419862889f16f46cad2cd0be6c8c323c0368a5a",
    "files": [
        {
            "sha": "711e688e564ffe7e724108455de16a63a96603c7",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 14,
            "deletions": 29,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/d419862889f16f46cad2cd0be6c8c323c0368a5a/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d419862889f16f46cad2cd0be6c8c323c0368a5a/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=d419862889f16f46cad2cd0be6c8c323c0368a5a",
            "patch": "@@ -16,6 +16,7 @@\n \n import collections.abc\n import math\n+import warnings\n from dataclasses import dataclass\n from typing import List, Optional, Tuple, Union\n \n@@ -196,12 +197,16 @@ def forward(\n         self,\n         pixel_values: torch.Tensor,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        interpolate_pos_encoding: bool = False,\n+        interpolate_pos_encoding: Optional[bool] = None,\n     ) -> torch.Tensor:\n+        if self.position_embeddings is not None and interpolate_pos_encoding is not None:\n+            warnings.warn(\n+                \"`interpolate_pos_encoding` argument has no effect for BEiTEmbeddings, embeddings are always \"\n+                \"interpolated to the input image size. The argument will be removed in transformers v4.51.0.\"\n+            )\n+\n         _, _, height, width = pixel_values.shape\n-        embeddings, (patch_height, patch_width) = self.patch_embeddings(\n-            pixel_values, self.position_embeddings[:, 1:, :] if self.position_embeddings is not None else None\n-        )\n+        embeddings, (patch_height, patch_width) = self.patch_embeddings(pixel_values)\n         batch_size, seq_len, _ = embeddings.size()\n \n         if bool_masked_pos is not None:\n@@ -211,14 +216,11 @@ def forward(\n             embeddings = embeddings * (1 - w) + mask_tokens * w\n \n         cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n-        if self.position_embeddings is not None:\n-            if interpolate_pos_encoding:\n-                cls_tokens = cls_tokens + self.interpolate_pos_encoding(embeddings, height, width)\n-            else:\n-                cls_tokens = cls_tokens + self.position_embeddings[:, :1, :]\n-\n         embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n \n+        if self.position_embeddings is not None:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+\n         embeddings = self.dropout(embeddings)\n \n         return embeddings, (patch_height, patch_width)\n@@ -248,11 +250,7 @@ def __init__(self, config):\n \n         self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n \n-    def forward(\n-        self,\n-        pixel_values: torch.Tensor,\n-        position_embedding: Optional[torch.Tensor] = None,\n-    ) -> torch.Tensor:\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         batch_size, num_channels, height, width = pixel_values.shape\n         if num_channels != self.num_channels:\n             raise ValueError(\n@@ -261,17 +259,6 @@ def forward(\n \n         embeddings = self.projection(pixel_values)\n         patch_height, patch_width = embeddings.shape[2], embeddings.shape[3]\n-\n-        if position_embedding is not None:\n-            # interpolate the position embedding to the corresponding size\n-            position_embedding = position_embedding.view(1, self.patch_shape[0], self.patch_shape[1], -1).permute(\n-                0, 3, 1, 2\n-            )\n-            position_embedding = nn.functional.interpolate(\n-                position_embedding, size=(patch_height, patch_width), mode=\"bicubic\"\n-            )\n-            embeddings = embeddings + position_embedding\n-\n         embeddings = embeddings.flatten(2).transpose(1, 2)\n \n         return embeddings, (patch_height, patch_width)\n@@ -887,9 +874,7 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        embedding_output, _ = self.embeddings(\n-            pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n-        )\n+        embedding_output, _ = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n         resolution = pixel_values.shape[2:]\n \n         encoder_outputs = self.encoder("
        },
        {
            "sha": "f6dba8235df105291dc38352b23315abb43cd38c",
            "filename": "src/transformers/models/data2vec/modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 14,
            "deletions": 29,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/d419862889f16f46cad2cd0be6c8c323c0368a5a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d419862889f16f46cad2cd0be6c8c323c0368a5a/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdata2vec%2Fmodeling_data2vec_vision.py?ref=d419862889f16f46cad2cd0be6c8c323c0368a5a",
            "patch": "@@ -16,6 +16,7 @@\n \n import collections.abc\n import math\n+import warnings\n from dataclasses import dataclass\n from typing import List, Optional, Tuple, Union\n \n@@ -195,12 +196,16 @@ def forward(\n         self,\n         pixel_values: torch.Tensor,\n         bool_masked_pos: Optional[torch.BoolTensor] = None,\n-        interpolate_pos_encoding: bool = False,\n+        interpolate_pos_encoding: Optional[bool] = None,\n     ) -> torch.Tensor:\n+        if self.position_embeddings is not None and interpolate_pos_encoding is not None:\n+            warnings.warn(\n+                \"`interpolate_pos_encoding` argument has no effect for BEiTEmbeddings, embeddings are always \"\n+                \"interpolated to the input image size. The argument will be removed in transformers v4.51.0.\"\n+            )\n+\n         _, _, height, width = pixel_values.shape\n-        embeddings, (patch_height, patch_width) = self.patch_embeddings(\n-            pixel_values, self.position_embeddings[:, 1:, :] if self.position_embeddings is not None else None\n-        )\n+        embeddings, (patch_height, patch_width) = self.patch_embeddings(pixel_values)\n         batch_size, seq_len, _ = embeddings.size()\n \n         if bool_masked_pos is not None:\n@@ -210,14 +215,11 @@ def forward(\n             embeddings = embeddings * (1 - w) + mask_tokens * w\n \n         cls_tokens = self.cls_token.expand(batch_size, -1, -1)\n-        if self.position_embeddings is not None:\n-            if interpolate_pos_encoding:\n-                cls_tokens = cls_tokens + self.interpolate_pos_encoding(embeddings, height, width)\n-            else:\n-                cls_tokens = cls_tokens + self.position_embeddings[:, :1, :]\n-\n         embeddings = torch.cat((cls_tokens, embeddings), dim=1)\n \n+        if self.position_embeddings is not None:\n+            embeddings = embeddings + self.interpolate_pos_encoding(embeddings, height, width)\n+\n         embeddings = self.dropout(embeddings)\n \n         return embeddings, (patch_height, patch_width)\n@@ -248,11 +250,7 @@ def __init__(self, config):\n \n         self.projection = nn.Conv2d(num_channels, hidden_size, kernel_size=patch_size, stride=patch_size)\n \n-    def forward(\n-        self,\n-        pixel_values: torch.Tensor,\n-        position_embedding: Optional[torch.Tensor] = None,\n-    ) -> torch.Tensor:\n+    def forward(self, pixel_values: torch.Tensor) -> torch.Tensor:\n         batch_size, num_channels, height, width = pixel_values.shape\n         if num_channels != self.num_channels:\n             raise ValueError(\n@@ -261,17 +259,6 @@ def forward(\n \n         embeddings = self.projection(pixel_values)\n         patch_height, patch_width = embeddings.shape[2], embeddings.shape[3]\n-\n-        if position_embedding is not None:\n-            # interpolate the position embedding to the corresponding size\n-            position_embedding = position_embedding.view(1, self.patch_shape[0], self.patch_shape[1], -1).permute(\n-                0, 3, 1, 2\n-            )\n-            position_embedding = nn.functional.interpolate(\n-                position_embedding, size=(patch_height, patch_width), mode=\"bicubic\"\n-            )\n-            embeddings = embeddings + position_embedding\n-\n         embeddings = embeddings.flatten(2).transpose(1, 2)\n \n         return embeddings, (patch_height, patch_width)\n@@ -902,9 +889,7 @@ def forward(\n         # and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\n         head_mask = self.get_head_mask(head_mask, self.config.num_hidden_layers)\n \n-        embedding_output, _ = self.embeddings(\n-            pixel_values, bool_masked_pos=bool_masked_pos, interpolate_pos_encoding=interpolate_pos_encoding\n-        )\n+        embedding_output, _ = self.embeddings(pixel_values, bool_masked_pos=bool_masked_pos)\n         resolution = pixel_values.shape[2:]\n \n         encoder_outputs = self.encoder("
        },
        {
            "sha": "c455c9eebb1610bea2498bc47325f2019aa0fd86",
            "filename": "tests/models/beit/test_modeling_beit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbeit%2Ftest_modeling_beit.py?ref=d419862889f16f46cad2cd0be6c8c323c0368a5a",
            "patch": "@@ -774,7 +774,9 @@ def test_inference_interpolate_pos_encoding(self):\n         with torch.no_grad():\n             outputs = model(pixel_values, interpolate_pos_encoding=True)\n \n-        expected_shape = torch.Size((1, 1801, 768))\n+        # num_cls_tokens + (height / patch_size) * (width / patch_size)\n+        # 1 + (480 / 16) * (480 / 16) = 1 + 30 * 30 = 901\n+        expected_shape = torch.Size((1, 901, 768))\n         self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n \n "
        },
        {
            "sha": "f297d3a3c6db443ad2170fb6693d56c84874c14b",
            "filename": "tests/models/data2vec/test_modeling_data2vec_vision.py",
            "status": "modified",
            "additions": 3,
            "deletions": 8,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdata2vec%2Ftest_modeling_data2vec_vision.py?ref=d419862889f16f46cad2cd0be6c8c323c0368a5a",
            "patch": "@@ -565,17 +565,12 @@ def test_inference_interpolate_pos_encoding(self):\n         inputs = processor(images=image, return_tensors=\"pt\", size={\"height\": 480, \"width\": 480})\n         pixel_values = inputs.pixel_values.to(torch_device)\n \n-        # with interpolate_pos_encoding being False an exception should be raised with higher resolution\n-        # images than what the model supports.\n-        self.assertFalse(processor.do_center_crop)\n-        with torch.no_grad():\n-            with self.assertRaises(ValueError, msg=\"doesn't match model\"):\n-                model(pixel_values, interpolate_pos_encoding=False)\n-\n         # with interpolate_pos_encoding being True the model should process the higher resolution image\n         # successfully and produce the expected output.\n         with torch.no_grad():\n             outputs = model(pixel_values, interpolate_pos_encoding=True)\n \n-        expected_shape = torch.Size((1, 1801, 768))\n+        # num_cls_tokens + (height / patch_size) * (width / patch_size)\n+        # 1 + (480 / 16) * (480 / 16) = 901\n+        expected_shape = torch.Size((1, 901, 768))\n         self.assertEqual(outputs.last_hidden_state.shape, expected_shape)"
        },
        {
            "sha": "e92cc6ddc289f6c5f19429319370af573a3cff21",
            "filename": "tests/models/detr/test_modeling_detr.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdetr%2Ftest_modeling_detr.py?ref=d419862889f16f46cad2cd0be6c8c323c0368a5a",
            "patch": "@@ -684,7 +684,12 @@ def test_inference_panoptic_segmentation_head(self):\n         self.assertTrue(results[\"segmentation\"].shape, expected_shape)\n         torch.testing.assert_close(results[\"segmentation\"][:3, :3], expected_slice_segmentation, rtol=1e-4, atol=1e-4)\n         self.assertTrue(len(results[\"segments_info\"]), expected_number_of_segments)\n-        self.assertDictEqual(results[\"segments_info\"][0], expected_first_segment)\n+\n+        predicted_first_segment = results[\"segments_info\"][0]\n+        self.assertEqual(predicted_first_segment[\"id\"], expected_first_segment[\"id\"])\n+        self.assertEqual(predicted_first_segment[\"label_id\"], expected_first_segment[\"label_id\"])\n+        self.assertEqual(predicted_first_segment[\"was_fused\"], expected_first_segment[\"was_fused\"])\n+        self.assertAlmostEqual(predicted_first_segment[\"score\"], expected_first_segment[\"score\"], places=3)\n \n \n @require_vision"
        },
        {
            "sha": "3e52ad49af3d1d3f4214485a46503305e7b1aac6",
            "filename": "tests/models/dinov2/test_modeling_dinov2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov2%2Ftest_modeling_dinov2.py?ref=d419862889f16f46cad2cd0be6c8c323c0368a5a",
            "patch": "@@ -329,10 +329,10 @@ def test_inference_no_head(self):\n         self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n \n         expected_slice = torch.tensor(\n-            [[-2.1747, -0.4729, 1.0936], [-3.2780, -0.8269, -0.9210], [-2.9129, 1.1284, -0.7306]],\n+            [[-2.2005, -0.4495, 1.0964], [-3.3959, -0.8942, -1.0315], [-2.9355, 1.1564, -0.7656]],\n             device=torch_device,\n         )\n-        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-3, atol=1e-3)\n \n \n @require_torch"
        },
        {
            "sha": "1c88c95cc8bb37253a5f231f11d2d079711ed129",
            "filename": "tests/models/textnet/test_modeling_textnet.py",
            "status": "modified",
            "additions": 8,
            "deletions": 4,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftextnet%2Ftest_modeling_textnet.py?ref=d419862889f16f46cad2cd0be6c8c323c0368a5a",
            "patch": "@@ -328,14 +328,18 @@ def test_inference_no_head(self):\n         with torch.no_grad():\n             output = model(**inputs)\n \n-        # verify logits\n-        self.assertEqual(output.logits.shape, torch.Size([1, 2]))\n+        # verify output\n+        self.assertEqual(output.last_hidden_state.shape, torch.Size([1, 512, 20, 27]))\n         expected_slice_backbone = torch.tensor(\n-            [0.9210, 0.6099, 0.0000, 0.0000, 0.0000, 0.0000, 3.2207, 2.6602, 1.8925, 0.0000],\n+            [\n+                [0.0000, 1.7415, 1.2660],\n+                [0.0000, 1.0084, 1.9692],\n+                [0.0000, 1.7464, 1.7892],\n+            ],\n             device=torch_device,\n         )\n         torch.testing.assert_close(\n-            output.feature_maps[-1][0][10][12][:10], expected_slice_backbone, rtol=1e-3, atol=1e-3\n+            output.last_hidden_state[0, 12, :3, :3], expected_slice_backbone, rtol=1e-2, atol=1e-2\n         )\n \n "
        },
        {
            "sha": "32812e3328063ca33296f35e7b32215143836da5",
            "filename": "tests/models/vit/test_modeling_vit.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvit%2Ftest_modeling_vit.py?ref=d419862889f16f46cad2cd0be6c8c323c0368a5a",
            "patch": "@@ -310,10 +310,10 @@ def test_inference_interpolate_pos_encoding(self):\n         self.assertEqual(outputs.last_hidden_state.shape, expected_shape)\n \n         expected_slice = torch.tensor(\n-            [[4.2340, 4.3906, -6.6692], [4.5463, 1.8928, -6.7257], [4.4429, 0.8496, -5.8585]]\n+            [[4.2325, 4.3882, -6.6678], [4.5372, 1.8933, -6.7355], [4.4454, 0.8514, -5.8747]]\n         ).to(torch_device)\n \n-        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-4, atol=1e-4)\n+        torch.testing.assert_close(outputs.last_hidden_state[0, :3, :3], expected_slice, rtol=1e-3, atol=1e-3)\n \n     @slow\n     @require_accelerate"
        },
        {
            "sha": "342ae269d39d764f1ab1b580f9a8114666aa8d6c",
            "filename": "tests/models/zoedepth/test_modeling_zoedepth.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d419862889f16f46cad2cd0be6c8c323c0368a5a/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fzoedepth%2Ftest_modeling_zoedepth.py?ref=d419862889f16f46cad2cd0be6c8c323c0368a5a",
            "patch": "@@ -301,8 +301,8 @@ def check_target_size(\n             out_l_reduced = torch.nn.functional.interpolate(\n                 out_l.unsqueeze(0).unsqueeze(1), size=img.size[::-1], mode=\"bicubic\", align_corners=False\n             )\n-            self.assertTrue((np.array(out_l.shape)[::-1] == np.array(img.size) * 2).all())\n-            torch.testing.assert_close(out, out_l_reduced, rtol=2e-2)\n+            out_l_reduced = out_l_reduced.squeeze(0).squeeze(0)\n+            torch.testing.assert_close(out, out_l_reduced, rtol=2e-2, atol=2e-2)\n \n     def check_post_processing_test(self, image_processor, images, model, pad_input=True, flip_aug=True):\n         inputs = image_processor(images=images, return_tensors=\"pt\", do_pad=pad_input).to(torch_device)\n@@ -324,7 +324,7 @@ def check_post_processing_test(self, image_processor, images, model, pad_input=T\n         for img, out, expected_slice in zip(images, outputs, expected_slices):\n             out = out[\"predicted_depth\"]\n             self.assertTrue(img.size == out.shape[::-1])\n-            torch.testing.assert_close(expected_slice, out[:3, :3], atol=1e-3, rtol=1e-3)\n+            torch.testing.assert_close(expected_slice, out[:3, :3], rtol=1e-3, atol=1e-3)\n \n         self.check_target_size(image_processor, pad_input, images, outputs, raw_outputs, raw_outputs_flipped)\n "
        }
    ],
    "stats": {
        "total": 134,
        "additions": 55,
        "deletions": 79
    }
}