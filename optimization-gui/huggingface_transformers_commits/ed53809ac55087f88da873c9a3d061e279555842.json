{
    "author": "yao-matrix",
    "message": "enable 6 rt_detr_v2 cases on xpu (#37548)\n\n* enable 6 rt_detr_v2 cases on xpu\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n* fix style\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\n\n---------\n\nSigned-off-by: YAO Matrix <matrix.yao@intel.com>\nCo-authored-by: Yih-Dar <2521628+ydshieh@users.noreply.github.com>",
    "sha": "ed53809ac55087f88da873c9a3d061e279555842",
    "files": [
        {
            "sha": "e9874f7c51531a122199071e68480f3eead75c73",
            "filename": "tests/models/rt_detr_v2/test_modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 9,
            "deletions": 3,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ed53809ac55087f88da873c9a3d061e279555842/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ed53809ac55087f88da873c9a3d061e279555842/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr_v2%2Ftest_modeling_rt_detr_v2.py?ref=ed53809ac55087f88da873c9a3d061e279555842",
            "patch": "@@ -27,7 +27,13 @@\n     is_torch_available,\n     is_vision_available,\n )\n-from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow, torch_device\n+from transformers.testing_utils import (\n+    require_torch,\n+    require_torch_accelerator,\n+    require_vision,\n+    slow,\n+    torch_device,\n+)\n from transformers.utils import cached_property\n \n from ...test_configuration_common import ConfigTester\n@@ -636,7 +642,7 @@ def test_initialization(self):\n         self.assertTrue(not failed_cases, message)\n \n     @parameterized.expand([\"float32\", \"float16\", \"bfloat16\"])\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_inference_with_different_dtypes(self, torch_dtype_str):\n         torch_dtype = {\n@@ -658,7 +664,7 @@ def test_inference_with_different_dtypes(self, torch_dtype_str):\n                 _ = model(**self._prepare_for_class(inputs_dict, model_class))\n \n     @parameterized.expand([\"float32\", \"float16\", \"bfloat16\"])\n-    @require_torch_gpu\n+    @require_torch_accelerator\n     @slow\n     def test_inference_equivalence_for_static_and_dynamic_anchors(self, torch_dtype_str):\n         torch_dtype = {"
        }
    ],
    "stats": {
        "total": 12,
        "additions": 9,
        "deletions": 3
    }
}