{
    "author": "0x-avi",
    "message": "Biogptlogits (#41270)\n\nadded logits slicing to BioGpt for seq classifier\n\nSigned-off-by: Aviral <aviralkamaljain@gmail.com>",
    "sha": "4f286fbbf83b7688f4bf299264c1327f5347d3ba",
    "files": [
        {
            "sha": "3558193b1fccd247f74156360138b0a2e4521cfb",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f286fbbf83b7688f4bf299264c1327f5347d3ba/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f286fbbf83b7688f4bf299264c1327f5347d3ba/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=4f286fbbf83b7688f4bf299264c1327f5347d3ba",
            "patch": "@@ -854,6 +854,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -876,7 +877,8 @@ def forward(\n             cache_position=cache_position,\n         )\n         hidden_states = transformer_outputs[0]\n-        logits = self.score(hidden_states)\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.score(hidden_states[:, slice_indices, :])\n \n         if input_ids is not None:\n             batch_size, sequence_length = input_ids.shape[:2]"
        },
        {
            "sha": "8000b77beb06a6a79a6679837fc5f0a9c0c15b9e",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/4f286fbbf83b7688f4bf299264c1327f5347d3ba/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4f286fbbf83b7688f4bf299264c1327f5347d3ba/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=4f286fbbf83b7688f4bf299264c1327f5347d3ba",
            "patch": "@@ -682,6 +682,7 @@ def forward(\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n         cache_position: Optional[torch.Tensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n     ) -> Union[tuple, SequenceClassifierOutputWithPast]:\n         r\"\"\"\n         labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n@@ -704,7 +705,8 @@ def forward(\n             cache_position=cache_position,\n         )\n         hidden_states = transformer_outputs[0]\n-        logits = self.score(hidden_states)\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.score(hidden_states[:, slice_indices, :])\n \n         if input_ids is not None:\n             batch_size, sequence_length = input_ids.shape[:2]"
        }
    ],
    "stats": {
        "total": 8,
        "additions": 6,
        "deletions": 2
    }
}