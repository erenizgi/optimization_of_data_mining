{
    "author": "MekkCyber",
    "message": "Define warmup allocator for torchao quantization (#37764)\n\n* torchao allocator\n\n* add comment\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "f466603963069c6a88d88c5fdda3e82f73b3617e",
    "files": [
        {
            "sha": "5b23a6173d819f13c1531ff5646d714d2dddfecd",
            "filename": "src/transformers/quantizers/quantizer_torchao.py",
            "status": "modified",
            "additions": 39,
            "deletions": 0,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/f466603963069c6a88d88c5fdda3e82f73b3617e/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/f466603963069c6a88d88c5fdda3e82f73b3617e/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fquantizers%2Fquantizer_torchao.py?ref=f466603963069c6a88d88c5fdda3e82f73b3617e",
            "patch": "@@ -277,6 +277,45 @@ def is_serializable(self, safe_serialization=None) -> bool:\n             return False\n         return _is_torchao_serializable\n \n+    def get_cuda_warm_up_factor(self):\n+        \"\"\"\n+        This factor is used in caching_allocator_warmup to determine how many bytes to pre-allocate for CUDA warmup.\n+        - A factor of 2 means we pre-allocate the full memory footprint of the model.\n+        - A factor of 4 means we pre-allocate half of that, and so on\n+\n+        However, when using TorchAO, calculating memory usage with param.numel() * param.element_size() doesn't give the correct size for quantized weights (like int4 or int8)\n+        That's because TorchAO internally represents quantized tensors using subtensors and metadata, and the reported element_size() still corresponds to the torch_dtype\n+        not the actual bit-width of the quantized data.\n+\n+        To correct for this:\n+        - Use a division factor of 8 for int4 weights\n+        - Use a division factor of 4 for int8 weights\n+        \"\"\"\n+        if self.quantization_config._get_ao_version() > version.Version(\"0.9.0\"):\n+            from torchao.core.config import AOBaseConfig\n+\n+            quant_type = self.quantization_config.quant_type\n+            # For autoquant case, it will be treated in the string implementation below in map_to_target_dtype\n+            if isinstance(quant_type, AOBaseConfig):\n+                # Extract size digit using fuzzy match on the class name\n+                config_name = quant_type.__class__.__name__\n+                size_digit = fuzzy_match_size(config_name)\n+\n+                if size_digit == \"4\":\n+                    return 8\n+                else:\n+                    return 4\n+\n+        # Original mapping for non-AOBaseConfig types\n+        map_to_target_dtype = {\n+            \"int4_weight_only\": 8,\n+            \"int8_weight_only\": 4,\n+            \"int8_dynamic_activation_int8_weight\": 4,\n+            \"autoquant\": 4,\n+        }\n+\n+        return map_to_target_dtype[self.quantization_config.quant_type]\n+\n     @property\n     def is_trainable(self) -> bool:\n         supported_quant_types_for_training = ["
        }
    ],
    "stats": {
        "total": 39,
        "additions": 39,
        "deletions": 0
    }
}