{
    "author": "yao-matrix",
    "message": "[3/3] make docs device agnostic, all en docs for existing models done  (#40298)\n\ndocs to device agnostic cont.\n\nSigned-off-by: Yao, Matrix <matrix.yao@intel.com>",
    "sha": "0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
    "files": [
        {
            "sha": "ee37a75d3a0c63125f5ff87346d1dc62cbb37296",
            "filename": "docs/source/en/cache_explanation.md",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fcache_explanation.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -99,18 +99,20 @@ The example below demonstrates how to create a generation loop with [`DynamicCac\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infer_device\n+\n+device = f\"{infer_device()}:0\"\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n past_key_values = DynamicCache()\n messages = [{\"role\": \"user\", \"content\": \"Hello, what's your name.\"}]\n-inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(\"cuda:0\")\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n \n generated_ids = inputs.input_ids\n-cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=\"cuda:0\")\n+cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=model.device)\n max_new_tokens = 10\n \n for _ in range(max_new_tokens):\n@@ -143,14 +145,16 @@ The generation loop usually takes care of the cache position, but if you're writ\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infer_device\n+\n+device = f\"{infer_device()}:0\"\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n messages = [{\"role\": \"user\", \"content\": \"You are a helpful assistant.\"}]\n-inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(\"cuda:0\")\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=10)\n \n ```"
        },
        {
            "sha": "97553cd94a6e82f57efbafdaba13d104372eea73",
            "filename": "docs/source/en/kv_cache.md",
            "status": "modified",
            "additions": 7,
            "deletions": 5,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fkv_cache.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fkv_cache.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fkv_cache.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -87,7 +87,7 @@ from transformers import AutoTokenizer, AutoModelForCausalLM\n \n ckpt = \"microsoft/Phi-3-mini-4k-instruct\"\n tokenizer = AutoTokenizer.from_pretrained(ckpt)\n-model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to(\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, device_map=\"auto\")\n inputs = tokenizer(\"Fun fact: The shortest\", return_tensors=\"pt\").to(model.device)\n \n out = model.generate(**inputs, do_sample=False, max_new_tokens=23, cache_implementation=\"offloaded\")\n@@ -99,24 +99,26 @@ The example below shows how you can fallback on [`OffloadedCache`] if you run ou\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM\n+from transformers import AutoTokenizer, AutoModelForCausalLM, infer_device\n \n def resilient_generate(model, *args, **kwargs):\n     oom = False\n+    device = infer_device()\n+    torch_device_module = getattr(torch, device, torch.cuda)\n     try:\n         return model.generate(*args, **kwargs)\n-    except torch.cuda.OutOfMemoryError as e:\n+    except torch.OutOfMemoryError as e:\n         print(e)\n         print(\"retrying with cache_implementation='offloaded'\")\n         oom = True\n     if oom:\n-        torch.cuda.empty_cache()\n+        torch_device_module.empty_cache()\n         kwargs[\"cache_implementation\"] = \"offloaded\"\n         return model.generate(*args, **kwargs)\n \n ckpt = \"microsoft/Phi-3-mini-4k-instruct\"\n tokenizer = AutoTokenizer.from_pretrained(ckpt)\n-model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16).to(\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(ckpt, torch_dtype=torch.float16, device_map=\"auto\")\n prompt = [\"okay \"*1000 + \"Fun fact: The most\"]\n inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n beams = { \"num_beams\": 40, \"num_beam_groups\": 40, \"num_return_sequences\": 40, \"diversity_penalty\": 1.0, \"max_new_tokens\": 23, \"early_stopping\": True, }"
        },
        {
            "sha": "495c5d12630d34c50169bcaa33fd86610cb848bd",
            "filename": "docs/source/en/llm_optims.md",
            "status": "modified",
            "additions": 10,
            "deletions": 15,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fllm_optims.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fllm_optims.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fllm_optims.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -114,18 +114,17 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n Another option for using [`StaticCache`] is to pass it to a models forward pass using the same `past_key_values` argument. This allows you to write your own custom decoding function to decode the next token given the current token, position, and cache position of previously generated tokens.\n \n ```py\n-from transformers import LlamaTokenizer, LlamaForCausalLM, StaticCache, logging\n+from transformers import LlamaTokenizer, LlamaForCausalLM, StaticCache, logging, infer_device\n from transformers.testing_utils import CaptureLogger\n import torch\n-from accelerate.test_utils.testing import get_backend\n \n prompts = [\n     \"Simply put, the theory of relativity states that \",\n     \"My favorite all time favorite condiment is ketchup.\",\n ]\n \n NUM_TOKENS_TO_GENERATE = 40\n-torch_device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+torch_device = infer_device()\n \n tokenizer = LlamaTokenizer.from_pretrained(\"meta-llama/Llama-2-7b-hf\", pad_token=\"</s>\", padding_side=\"right\")\n model = LlamaForCausalLM.from_pretrained(\"meta-llama/Llama-2-7b-hf\", device_map=\"sequential\")\n@@ -239,11 +238,10 @@ Enable speculative decoding by loading an assistant model and passing it to [`~G\n <hfoption id=\"greedy search\">\n \n ```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n import torch\n-from accelerate.test_utils.testing import get_backend\n \n-device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n@@ -261,11 +259,10 @@ tokenizer.batch_decode(outputs, skip_special_tokens=True)\n For speculative sampling decoding, add the [do_sample](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.do_sample) and [temperature](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.temperature) parameters to [`~GenerationMixin.generate`].\n \n ```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n import torch\n-from accelerate.test_utils.testing import get_backend\n \n-device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"Einstein's theory of relativity states\", return_tensors=\"pt\").to(device)\n@@ -290,11 +287,10 @@ To enable prompt lookup decoding, specify the number of tokens that should be ov\n <hfoption id=\"greedy decoding\">\n \n ```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n import torch\n-from accelerate.test_utils.testing import get_backend\n \n-device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)\n@@ -312,11 +308,10 @@ print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n For prompt lookup decoding with sampling, add the [do_sample](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.do_sample) and [temperature](https://hf.co/docs/transformers/main/en/main_classes/text_generation#transformers.GenerationConfig.temperature) parameters to [`~GenerationMixin.generate`].\n \n ```py\n-from transformers import AutoModelForCausalLM, AutoTokenizer\n+from transformers import AutoModelForCausalLM, AutoTokenizer, infer_device\n import torch\n-from accelerate.test_utils.testing import get_backend\n \n-device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+device = infer_device()\n \n tokenizer = AutoTokenizer.from_pretrained(\"facebook/opt-1.3b\")\n inputs = tokenizer(\"The second law of thermodynamics states\", return_tensors=\"pt\").to(device)"
        },
        {
            "sha": "ee54ad221d08417eec38658f4219effc0734550a",
            "filename": "docs/source/en/model_doc/bamba.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbamba.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -62,7 +62,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer\n \n tokenizer = AutoTokenizer.from_pretrained(\"ibm-ai-platform/Bamba-9B-v2\")\n model = AutoModelForCausalLM.from_pretrained(\"ibm-ai-platform/Bamba-9B-v2\", torch_dtype=torch.bfloat16, device_map=\"auto\", attn_implementation=\"sdpa\")\n-input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n \n output = model.generate(**input_ids)\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n@@ -94,7 +94,7 @@ model = AutoModelForCausalLM.from_pretrained(\n    attn_implementation=\"sdpa\"\n )\n \n-inputs = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n output = model.generate(**inputs)\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```"
        },
        {
            "sha": "641835bbe54b0bf024a06cfc04e11c0e38dd073c",
            "filename": "docs/source/en/model_doc/bark.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fbark.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -114,10 +114,10 @@ At batch size 8, on an NVIDIA A100, Flash Attention 2 is also 10% faster than Be\n You can combine optimization techniques, and use CPU offload, half-precision and Flash Attention 2 (or ü§ó Better Transformer) all at once.\n \n ```python\n-from transformers import BarkModel\n+from transformers import BarkModel, infer_device\n import torch\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n \n # load in fp16 and use Flash Attention 2\n model = BarkModel.from_pretrained(\"suno/bark-small\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(device)"
        },
        {
            "sha": "b1555e3c5fbbefd2d741347fa8a5a2ee822e14e5",
            "filename": "docs/source/en/model_doc/colpali.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fcolpali.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -47,7 +47,7 @@ model_name = \"vidore/colpali-v1.3-hf\"\n model = ColPaliForRetrieval.from_pretrained(\n     model_name,\n     torch_dtype=torch.bfloat16,\n-    device_map=\"auto\",  # \"cpu\", \"cuda\", or \"mps\" for Apple Silicon\n+    device_map=\"auto\",  # \"cpu\", \"cuda\", \"xpu\", or \"mps\" for Apple Silicon\n )\n processor = ColPaliProcessor.from_pretrained(model_name)\n \n@@ -119,7 +119,7 @@ bnb_config = BitsAndBytesConfig(\n model = ColPaliForRetrieval.from_pretrained(\n     model_name,\n     quantization_config=bnb_config,\n-    device_map=\"cuda\",\n+    device_map=\"auto\",\n )\n \n processor = ColPaliProcessor.from_pretrained(model_name)"
        },
        {
            "sha": "bac73a47ab75dced23bd01a0c5a8f199e70a1e04",
            "filename": "docs/source/en/model_doc/emu3.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Femu3.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -63,7 +63,7 @@ from PIL import Image\n import requests\n \n processor = Emu3Processor.from_pretrained(\"BAAI/Emu3-Chat-hf\")\n-model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", torch_dtype=torch.bfloat16, device_map=\"cuda\")\n+model = Emu3ForConditionalGeneration.from_pretrained(\"BAAI/Emu3-Chat-hf\", torch_dtype=torch.bfloat16, device_map=\"auto\")\n \n # prepare image and text prompt\n url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n@@ -92,10 +92,10 @@ inputs = processor(\n     return_tensors=\"pt\",\n     return_for_image_generation=True,\n )\n-inputs = inputs.to(device=\"cuda:0\", dtype=torch.bfloat16)\n+inputs = inputs.to(device=model.device, dtype=torch.bfloat16)\n \n neg_prompt = \"lowres, bad anatomy, bad hands, text, error, missing fingers, extra digit, fewer digits, cropped, worst quality, low quality, normal quality, jpeg artifacts, signature, watermark, username, blurry.\"\n-neg_inputs = processor(text=[neg_prompt] * 2, return_tensors=\"pt\").to(device=\"cuda:0\")\n+neg_inputs = processor(text=[neg_prompt] * 2, return_tensors=\"pt\").to(device=model.device)\n \n image_sizes = inputs.pop(\"image_sizes\")\n HEIGHT, WIDTH = image_sizes[0]"
        },
        {
            "sha": "ad4a3e2b1647d40fe81a2722ca14eb2e649cff36",
            "filename": "docs/source/en/model_doc/gemma.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgemma.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -53,7 +53,7 @@ pipeline = pipeline(\n     task=\"text-generation\",\n     model=\"google/gemma-2b\",\n     torch_dtype=torch.bfloat16,\n-    device=\"cuda\",\n+    device_map=\"auto\",\n )\n \n pipeline(\"LLMs generate text through a process known as\", max_new_tokens=50)\n@@ -75,7 +75,7 @@ model = AutoModelForCausalLM.from_pretrained(\n )\n \n input_text = \"LLMs generate text through a process known as\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n \n outputs = model.generate(**input_ids, max_new_tokens=50, cache_implementation=\"static\")\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n@@ -114,7 +114,7 @@ model = AutoModelForCausalLM.from_pretrained(\n )\n \n input_text = \"LLMs generate text through a process known as.\"\n-input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n outputs = model.generate(\n     **input_ids,\n     max_new_tokens=50,\n@@ -152,7 +152,7 @@ visualizer(\"LLMs generate text through a process known as\")\n        attn_implementation=\"sdpa\"\n    )\n    input_text = \"LLMs generate text through a process known as\"\n-   input_ids = tokenizer(input_text, return_tensors=\"pt\").to(\"cuda\")\n+   input_ids = tokenizer(input_text, return_tensors=\"pt\").to(model.device)\n    past_key_values = DynamicCache()\n    outputs = model.generate(**input_ids, max_new_tokens=50, past_key_values=past_key_values)\n    print(tokenizer.decode(outputs[0], skip_special_tokens=True))"
        },
        {
            "sha": "f19a1a1130a137822496b4e718ee85bb8ddda571",
            "filename": "docs/source/en/model_doc/glm4v.md",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fglm4v.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -112,7 +112,7 @@ inputs = processor.apply_chat_template(\n     tokenize=True,\n     return_dict=True,\n     return_tensors=\"pt\"\n-).to(\"cuda\")\n+).to(model.device)\n \n generated_ids = model.generate(**inputs, max_new_tokens=128)\n generated_ids_trimmed = [\n@@ -130,14 +130,16 @@ Using GLM-4.1V with video input is similar to using it with image input.\n The model can process video data and generate text based on the content of the video.\n \n ```python\n-from transformers import AutoProcessor, Glm4vForConditionalGeneration\n+from transformers import AutoProcessor, Glm4vForConditionalGeneration, infer_device\n import torch\n \n+device = f\"{infer_device()}:0\"\n+\n processor = AutoProcessor.from_pretrained(\"THUDM/GLM-4.1V-9B-Thinking\")\n model = Glm4vForConditionalGeneration.from_pretrained(\n     pretrained_model_name_or_path=\"THUDM/GLM-4.1V-9B-Thinking\",\n     torch_dtype=torch.bfloat16,\n-    device_map=\"cuda:0\"\n+    device_map=device\n )\n \n messages = [\n@@ -155,7 +157,7 @@ messages = [\n         ],\n     }\n ]\n-inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\", padding=True).to(\"cuda:0\")\n+inputs = processor.apply_chat_template(messages, tokenize=True, add_generation_prompt=True, return_dict=True, return_tensors=\"pt\", padding=True).to(model.device)\n generated_ids = model.generate(**inputs, max_new_tokens=1024, do_sample=True, temperature=1.0)\n output_text = processor.decode(generated_ids[0][inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n print(output_text)"
        },
        {
            "sha": "5be09a2f96a8b86a8766b0db9146e31179e22527",
            "filename": "docs/source/en/model_doc/gpt_neox_japanese.md",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt_neox_japanese.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -55,15 +55,14 @@ pipeline(\"‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅ\")\n <hfoption id=\"AutoModel\">\n \n ```py\n-import torch  \n-from transformers import AutoModelForCausalLM, AutoTokenizer  \n-\n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"  \n-model = AutoModelForCausalLM.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\", torch_dtype=torch.float16, device_map=\"auto\").to(device)  \n-tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")  \n-input_ids = tokenizer(\"‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅ\", return_tensors=\"pt\").input_ids.to(device)  \n-outputs = model.generate(input_ids)  \n-print(tokenizer.decode(outputs[0], skip_special_tokens=True))  \n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+model = AutoModelForCausalLM.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\", torch_dtype=torch.float16, device_map=\"auto\")\n+tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n+input_ids = tokenizer(\"‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅ\", return_tensors=\"pt\").input_ids.to(model.device)\n+outputs = model.generate(input_ids)\n+print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```\n \n </hfoption>\n@@ -96,7 +95,7 @@ model = AutoModelForCausalLM.from_pretrained(\n )\n \n tokenizer = AutoTokenizer.from_pretrained(\"abeja/gpt-neox-japanese-2.7b\")\n-input_ids = tokenizer.encode(\"‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅ\", return_tensors=\"pt\").to(\"cuda\")\n+input_ids = tokenizer.encode(\"‰∫∫„Å®AI„ÅåÂçîË™ø„Åô„Çã„Åü„ÇÅ„Å´„ÅØ„ÄÅ\", return_tensors=\"pt\").to(model.device)\n output = model.generate(input_ids)\n print(tokenizer.decode(output[0], skip_special_tokens=True))\n ```"
        },
        {
            "sha": "82928d083d920f98b2a54c85d50030dca4e8bb5d",
            "filename": "docs/source/en/model_doc/gptsan-japanese.md",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgptsan-japanese.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -42,14 +42,15 @@ fine-tune for translation or summarization.\n The `generate()` method can be used to generate text using GPTSAN-Japanese model.\n \n ```python\n->>> from transformers import AutoModel, AutoTokenizer\n+>>> from transformers import AutoModel, AutoTokenizer, infer_device\n >>> import torch\n \n+>>> device = infer_device()\n >>> tokenizer = AutoTokenizer.from_pretrained(\"Tanrei/GPTSAN-japanese\")\n->>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").cuda()\n+>>> model = AutoModel.from_pretrained(\"Tanrei/GPTSAN-japanese\").to(device)\n >>> x_tok = tokenizer(\"„ÅØ„ÄÅ\", prefix_text=\"ÁπîÁî∞‰ø°Èï∑\", return_tensors=\"pt\")\n >>> torch.manual_seed(0)\n->>> gen_tok = model.generate(x_tok.input_ids.cuda(), token_type_ids=x_tok.token_type_ids.cuda(), max_new_tokens=20)\n+>>> gen_tok = model.generate(x_tok.input_ids.to(model.device), token_type_ids=x_tok.token_type_ids.to(mdoel.device), max_new_tokens=20)\n >>> tokenizer.decode(gen_tok[0])\n 'ÁπîÁî∞‰ø°Èï∑„ÅØ„ÄÅ2004Âπ¥„Å´„ÄéÊà¶ÂõΩBASARA„Äè„ÅÆ„Åü„ÇÅ„Å´„ÄÅË±äËá£ÁßÄÂêâ'\n ```"
        },
        {
            "sha": "d3bfcd0564231dc28fb9cc43f9e50763eb8fa894",
            "filename": "docs/source/en/model_doc/idefics2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fidefics2.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -57,10 +57,10 @@ Example of how to use the processor on chat messages:\n ```python\n import requests\n from PIL import Image\n-from transformers import Idefics2Processor, Idefics2ForConditionalGeneration\n+from transformers import Idefics2Processor, Idefics2ForConditionalGeneration, infer_device\n import torch\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n \n url_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n url_2 = \"http://images.cocodataset.org/val2017/000000219578.jpg\"\n@@ -87,7 +87,7 @@ text = processor.apply_chat_template(messages, add_generation_prompt=True)\n print(text)\n # 'User: What‚Äôs the difference between these two images?<image><image><end_of_utterance>\\nAssistant:'\n \n-inputs = processor(images=images, text=text, return_tensors=\"pt\").to(device)\n+inputs = processor(images=images, text=text, return_tensors=\"pt\").to(model.device)\n \n generated_text = model.generate(**inputs, max_new_tokens=500)\n generated_text = processor.batch_decode(generated_text, skip_special_tokens=True)[0]\n@@ -99,7 +99,7 @@ print(\"Generated text:\", generated_text)\n ```python\n import requests\n from PIL import Image\n-from transformers import Idefics2Processor, Idefics2ForConditionalGeneration\n+from transformers import Idefics2Processor, Idefics2ForConditionalGeneration, infer_device\n import torch\n \n url_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n@@ -124,14 +124,14 @@ messages = [{\n     ],\n }]\n \n-device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n+device = infer_device()\n \n processor = Idefics2Processor.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\n model = Idefics2ForConditionalGeneration.from_pretrained(\"HuggingFaceM4/idefics2-8b\")\n model.to(device)\n \n text = processor.apply_chat_template(messages, add_generation_prompt=False)\n-inputs = processor(images=images, text=text, return_tensors=\"pt\").to(device)\n+inputs = processor(images=images, text=text, return_tensors=\"pt\").to(model.device)\n \n labels = inputs.input_ids.clone()\n labels[labels == processor.tokenizer.pad_token_id] = -100"
        },
        {
            "sha": "f525e2ce7f7b15a0aa646cb8a10faf3775436157",
            "filename": "docs/source/en/model_doc/m2m_100.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fm2m_100.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -154,13 +154,13 @@ To load a model using Flash Attention 2, we can pass the argument `attn_implemen\n >>> import torch\n >>> from transformers import M2M100ForConditionalGeneration, M2M100Tokenizer\n \n->>> model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\").to(\"cuda\").eval()\n+>>> model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\", torch_dtype=torch.float16, attn_implementation=\"flash_attention_2\", device_map=\"auto\").eval()\n >>> tokenizer = M2M100Tokenizer.from_pretrained(\"facebook/m2m100_418M\")\n \n >>> # translate Hindi to French\n >>> hi_text = \"‡§ú‡•Ä‡§µ‡§® ‡§è‡§ï ‡§ö‡•â‡§ï‡§≤‡•á‡§ü ‡§¨‡•â‡§ï‡•ç‡§∏ ‡§ï‡•Ä ‡§§‡§∞‡§π ‡§π‡•à‡•§\"\n >>> tokenizer.src_lang = \"hi\"\n->>> encoded_hi = tokenizer(hi_text, return_tensors=\"pt\").to(\"cuda\")\n+>>> encoded_hi = tokenizer(hi_text, return_tensors=\"pt\").to(model.device)\n >>> generated_tokens = model.generate(**encoded_hi, forced_bos_token_id=tokenizer.get_lang_id(\"fr\"))\n >>> tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n \"La vie est comme une bo√Æte de chocolat.\"\n@@ -190,4 +190,4 @@ model = M2M100ForConditionalGeneration.from_pretrained(\"facebook/m2m100_418M\", t\n ...\n ```\n \n-For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`).\n\\ No newline at end of file\n+For the best speedups, we recommend loading the model in half-precision (e.g. `torch.float16` or `torch.bfloat16`)."
        },
        {
            "sha": "58c1252cb48b8d68fbea3a67e3312f9472df11a8",
            "filename": "docs/source/en/model_doc/modernbert-decoder.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmodernbert-decoder.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -78,7 +78,7 @@ model = AutoModelForCausalLM.from_pretrained(\n )\n \n prompt = \"The future of artificial intelligence is\"\n-inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model.generate(\n@@ -104,7 +104,7 @@ classifier_model = AutoModelForSequenceClassification.from_pretrained(\n )\n \n text = \"This movie is really great!\"\n-inputs = tokenizer(text, return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(text, return_tensors=\"pt\").to(classifier_model.device)\n \n with torch.no_grad():\n     outputs = classifier_model(**inputs)\n@@ -136,7 +136,7 @@ model = AutoModelForCausalLM.from_pretrained(\n )\n \n prompt = \"The future of artificial intelligence is\"\n-inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     outputs = model.generate("
        },
        {
            "sha": "d1fa44f150075fdb376d1423afc6de699ddca315",
            "filename": "docs/source/en/model_doc/ovis2.md",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fovis2.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -38,12 +38,14 @@ import torch\n from torchvision import io\n from typing import Dict\n from transformers.image_utils import load_images, load_video\n-from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoProcessor\n+from transformers import AutoModelForVision2Seq, AutoTokenizer, AutoProcessor, infer_device\n+\n+device = f\"{infer_device()}:0\"\n \n model = AutoModelForVision2Seq.from_pretrained(\n     \"thisisiron/Ovis2-2B-hf\",\n     torch_dtype=torch.bfloat16,\n-).eval().to(\"cuda:0\")\n+).eval().to(device)\n processor = AutoProcessor.from_pretrained(\"thisisiron/Ovis2-2B-hf\")\n \n messages = [\n@@ -65,7 +67,7 @@ inputs = processor(\n     text=messages,\n     return_tensors=\"pt\",\n )\n-inputs = inputs.to(\"cuda:0\")\n+inputs = inputs.to(model.device)\n inputs['pixel_values'] = inputs['pixel_values'].to(torch.bfloat16)\n \n with torch.inference_mode():"
        },
        {
            "sha": "4303b8d7a7f81e1ed34ab83ac996461e12f892c3",
            "filename": "docs/source/en/model_doc/smollm3.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsmollm3.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -79,7 +79,7 @@ text = tokenizer.apply_chat_template(\n     tokenize=False,\n     add_generation_prompt=True\n )\n-model_inputs = tokenizer([text], return_tensors=\"pt\").to(\"cuda\")\n+model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n \n generated_ids = model.generate(\n     model_inputs.input_ids,\n@@ -134,7 +134,7 @@ model = AutoModelForCausalLM.from_pretrained(\n     attn_implementation=\"flash_attention_2\"\n )\n \n-inputs = tokenizer(\"Gravity is the force\", return_tensors=\"pt\").to(\"cuda\")\n+inputs = tokenizer(\"Gravity is the force\", return_tensors=\"pt\").to(model.device)\n outputs = model.generate(**inputs, max_new_tokens=100)\n print(tokenizer.decode(outputs[0], skip_special_tokens=True))\n ```"
        },
        {
            "sha": "4350b444e62f148ec0209dabc4a7f408d7adffd0",
            "filename": "docs/source/en/perplexity.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fperplexity.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fperplexity.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fperplexity.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -72,10 +72,9 @@ predictions at each step.\n Let's demonstrate this process with GPT-2.\n \n ```python\n-from transformers import GPT2LMHeadModel, GPT2TokenizerFast\n-from accelerate.test_utils.testing import get_backend\n+from transformers import GPT2LMHeadModel, GPT2TokenizerFast, infer_device\n \n-device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+device = infer_device()\n model_id = \"openai-community/gpt2-large\"\n model = GPT2LMHeadModel.from_pretrained(model_id).to(device)\n tokenizer = GPT2TokenizerFast.from_pretrained(model_id)"
        },
        {
            "sha": "174a726dc35e3fd4044580268e1772a40b0e0a5e",
            "filename": "docs/source/en/quantization/auto_round.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fauto_round.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -231,7 +231,7 @@ print(tokenizer.decode(model.generate(**inputs, max_new_tokens=50, do_sample=Fal\n AutoRound automatically selects the backend for each layer based on compatibility. In general, the priority order is Marlin > ExLLaMAV2 > Triton, but the final choice depends on factors such as group size, bit width, packing format, hardware device, and other implementation details. For more details, please refer to [backends](https://github.com/intel/auto-round?tab=readme-ov-file#specify-backend),\n \n The backend may not always be the most suitable for certain devices. \n-You can specify your preferred backend such as \"ipex\" for CPU and CPU, \"marlin/exllamav2/triton\" for CUDA, according to your needs or hardware compatibility. Please note that additional corresponding libraries may be required.\n+You can specify your preferred backend such as \"ipex\" for CPU, \"ipex/triton\" for XPU, \"marlin/exllamav2/triton\" for CUDA, according to your needs or hardware compatibility. Please note that additional corresponding libraries may be required.\n \n ```python\n from transformers import AutoModelForCausalLM, AutoTokenizer, AutoRoundConfig\n@@ -283,4 +283,4 @@ Special thanks to open-source low precision libraries such as AutoGPTQ, AutoAWQ,\n \n ## Contribution\n Contributions to [AutoRound](https://github.com/intel/auto-round/pulls) are welcome and greatly appreciated!\n-Whether it's fixing bugs, improving documentation, adding new features, or suggesting improvements, your help is always valued.\n\\ No newline at end of file\n+Whether it's fixing bugs, improving documentation, adding new features, or suggesting improvements, your help is always valued."
        },
        {
            "sha": "abe645acbff6ea02f4cc70466e96516df9e95494",
            "filename": "docs/source/en/quantization/fp_quant.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Ffp_quant.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -27,7 +27,7 @@ import torch\n model = AutoModelForCausalLM.from_pretrained(\n     \"qwen/Qwen3-8B\",\n     quantization_config=FPQuantConfig(),\n-    device_map=\"cuda\",\n+    device_map=\"auto\",\n     torch_dtype=torch.bfloat16,\n )\n ```\n@@ -52,7 +52,7 @@ from transformers import AutoModelForCausalLM, AutoTokenizer, FPQuantConfig\n model = AutoModelForCausalLM.from_pretrained(\n     \"qwen/Qwen3-8B\",\n     quantization_config=FPQuantConfig(),\n-    device_map=\"cuda\",\n+    device_map=\"auto\",\n     torch_dtype=torch.bfloat16,\n )\n "
        },
        {
            "sha": "94250a8c2f6e9363ac319d2279715e085272b5c9",
            "filename": "docs/source/en/quantization/quark.md",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fquantization%2Fquark.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Fquantization%2Fquark.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fquantization%2Fquark.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -61,8 +61,7 @@ Here is an example of how one can load a Quark model in Transformers:\n from transformers import AutoModelForCausalLM, AutoTokenizer\n \n model_id = \"EmbeddedLLM/Llama-3.1-8B-Instruct-w_fp8_per_channel_sym\"\n-model = AutoModelForCausalLM.from_pretrained(model_id)\n-model = model.to(\"cuda\")\n+model = AutoModelForCausalLM.from_pretrained(model_id, device_map=\"auto\")\n \n print(model.model.layers[0].self_attn.q_proj)\n # QParamsLinear(\n@@ -73,12 +72,12 @@ print(model.model.layers[0].self_attn.q_proj)\n \n tokenizer = AutoTokenizer.from_pretrained(model_id)\n inp = tokenizer(\"Where is a good place to cycle around Tokyo?\", return_tensors=\"pt\")\n-inp = inp.to(\"cuda\")\n+inp = inp.to(model.device)\n \n res = model.generate(**inp, min_new_tokens=50, max_new_tokens=100)\n \n print(tokenizer.batch_decode(res)[0])\n # <|begin_of_text|>Where is a good place to cycle around Tokyo? There are several places in Tokyo that are suitable for cycling, depending on your skill level and interests. Here are a few suggestions:\n # 1. Yoyogi Park: This park is a popular spot for cycling and has a wide, flat path that's perfect for beginners. You can also visit the Meiji Shrine, a famous Shinto shrine located in the park.\n # 2. Imperial Palace East Garden: This beautiful garden has a large, flat path that's perfect for cycling. You can also visit the\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "f9716f29a20447152c78eb8af7b25f3e16d57bc8",
            "filename": "docs/source/en/tasks/image_captioning.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_captioning.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -256,9 +256,9 @@ image\n Prepare image for the model.\n \n ```python\n-from accelerate.test_utils.testing import get_backend\n-# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n-device, _, _ = get_backend()\n+from transformers import infer_device\n+\n+device = infer_device()\n inputs = processor(images=image, return_tensors=\"pt\").to(device)\n pixel_values = inputs.pixel_values\n ```"
        },
        {
            "sha": "455a2b425d412b199374f79c3b04be50bee968e3",
            "filename": "docs/source/en/tasks/image_feature_extraction.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_feature_extraction.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -42,10 +42,9 @@ Let's see the pipeline in action. First, initialize the pipeline. If you don't p\n \n ```python\n import torch\n-from transformers import pipeline\n-from accelerate.test_utils.testing import get_backend\n+from transformers import pipeline, infer_device\n # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n-DEVICE, _, _ = get_backend()\n+DEVICE = infer_device()\n pipe = pipeline(task=\"image-feature-extraction\", model_name=\"google/vit-base-patch16-384\", device=DEVICE, pool=True)\n ```\n "
        },
        {
            "sha": "da6a57ac9aa99e8122a0779dcd76fd2ed40e916e",
            "filename": "docs/source/en/tasks/image_to_image.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fimage_to_image.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -35,11 +35,10 @@ pip install transformers\n We can now initialize the pipeline with a [Swin2SR model](https://huggingface.co/caidas/swin2SR-lightweight-x2-64). We can then infer with the pipeline by calling it with an image. As of now, only [Swin2SR models](https://huggingface.co/models?sort=trending&search=swin2sr) are supported in this pipeline. \n \n ```python\n-from transformers import pipeline\n+from transformers import pipeline, infer_device\n import torch\n-from accelerate.test_utils.testing import get_backend\n # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n-device, _, _ = get_backend()\n+device = infer_device()\n pipe = pipeline(task=\"image-to-image\", model=\"caidas/swin2SR-lightweight-x2-64\", device=device)\n ```\n "
        },
        {
            "sha": "7c4a684d3c057f7c23616d94dcfcffe42739609c",
            "filename": "docs/source/en/tasks/knowledge_distillation_for_image_classification.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fknowledge_distillation_for_image_classification.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -54,19 +54,18 @@ Essentially, we want the student model (a randomly initialized MobileNet) to mim\n \n \n ```python\n-from transformers import TrainingArguments, Trainer\n+from transformers import TrainingArguments, Trainer, infer_device\n import torch\n import torch.nn as nn\n import torch.nn.functional as F\n-from accelerate.test_utils.testing import get_backend\n \n class ImageDistilTrainer(Trainer):\n     def __init__(self, teacher_model=None, student_model=None, temperature=None, lambda_param=None,  *args, **kwargs):\n         super().__init__(model=student_model, *args, **kwargs)\n         self.teacher = teacher_model\n         self.student = student_model\n         self.loss_function = nn.KLDivLoss(reduction=\"batchmean\")\n-        device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+        device = infer_device()\n         self.teacher.to(device)\n         self.teacher.eval()\n         self.temperature = temperature"
        },
        {
            "sha": "28ecfd193be67af211d9b32dc4303e6e5a8bff70",
            "filename": "docs/source/en/tasks/mask_generation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmask_generation.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -123,11 +123,9 @@ You can also use the model without the pipeline. To do so, initialize the model\n the processor.\n \n ```python\n-from transformers import SamModel, SamProcessor\n+from transformers import SamModel, SamProcessor, infer_device\n import torch\n-from accelerate.test_utils.testing import get_backend\n-# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n-device, _, _ = get_backend()\n+device = infer_device()\n model = SamModel.from_pretrained(\"facebook/sam-vit-base\").to(device)\n processor = SamProcessor.from_pretrained(\"facebook/sam-vit-base\")\n ```"
        },
        {
            "sha": "c90abce1cd57b4d2525d541c6c9c3f41acbfc4f1",
            "filename": "docs/source/en/tasks/monocular_depth_estimation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 4,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fmonocular_depth_estimation.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -51,11 +51,9 @@ The simplest way to try out inference with a model supporting depth estimation i\n Instantiate a pipeline from a [checkpoint on the Hugging Face Hub](https://huggingface.co/models?pipeline_tag=depth-estimation&sort=downloads):\n \n ```py\n->>> from transformers import pipeline\n+>>> from transformers import pipeline, infer_device\n >>> import torch\n->>> from accelerate.test_utils.testing import get_backend\n-# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n->>> device, _, _ = get_backend()\n+>>> device = infer_device()\n >>> checkpoint = \"depth-anything/Depth-Anything-V2-base-hf\"\n >>> pipe = pipeline(\"depth-estimation\", model=checkpoint, device=device)\n ```"
        },
        {
            "sha": "394e77104b74c764fca4476aa3bb9af6b7c7958a",
            "filename": "docs/source/en/tasks/object_detection.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fobject_detection.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -1488,9 +1488,9 @@ Now that you have finetuned a model, evaluated it, and uploaded it to the Huggin\n \n Load model and image processor from the Hugging Face Hub (skip to use already trained in this session):\n ```py\n->>> from accelerate.test_utils.testing import get_backend\n-# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n->>> device, _, _ = get_backend()\n+>>> from transformers import infer_device\n+\n+>>> device = infer_device()\n >>> model_repo = \"qubvel-hf/detr_finetuned_cppe5\"\n \n >>> image_processor = AutoImageProcessor.from_pretrained(model_repo)"
        },
        {
            "sha": "3bede1a448beeb0db30e22a6cf9c189b0e78a15e",
            "filename": "docs/source/en/tasks/semantic_segmentation.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fsemantic_segmentation.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -689,9 +689,8 @@ Reload the dataset and load an image for inference.\n We will now see how to infer without a pipeline. Process the image with an image processor and place the `pixel_values` on a GPU:\n \n ```py\n->>> from accelerate.test_utils.testing import get_backend\n-# automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n->>> device, _, _ = get_backend()\n+>>> from transformers import infer_device\n+>>> device = infer_device()\n >>> encoding = image_processor(image, return_tensors=\"pt\")\n >>> pixel_values = encoding.pixel_values.to(device)\n ```"
        },
        {
            "sha": "28153cd1160eda0de08f300488ff23af5f10240a",
            "filename": "docs/source/en/tasks/text-to-speech.md",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Ftext-to-speech.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -282,10 +282,10 @@ containing the corresponding speaker embedding.\n >>> import os\n >>> import torch\n >>> from speechbrain.inference.classifiers import EncoderClassifier\n->>> from accelerate.test_utils.testing import get_backend\n+>>> from transformers import infer_device\n \n >>> spk_model_name = \"speechbrain/spkrec-xvect-voxceleb\"\n->>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+>>> device = infer_device()\n >>> speaker_model = EncoderClassifier.from_hparams(\n ...     source=spk_model_name,\n ...     run_opts={\"device\": device},"
        },
        {
            "sha": "24c5a3c7ddb456b9287bce01c5cd51d92d0cd6c9",
            "filename": "docs/source/en/tasks/visual_document_retrieval.md",
            "status": "modified",
            "additions": 9,
            "deletions": 7,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_document_retrieval.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -51,7 +51,9 @@ Let's load the model and the tokenizer.\n \n ```python\n import torch\n-from transformers import ColPaliForRetrieval, ColPaliProcessor\n+from transformers import ColPaliForRetrieval, ColPaliProcessor, infer_device\n+\n+device = infer_device()\n \n model_name = \"vidore/colpali-v1.2-hf\"\n \n@@ -60,24 +62,24 @@ processor = ColPaliProcessor.from_pretrained(model_name)\n model = ColPaliForRetrieval.from_pretrained(\n     model_name,\n     torch_dtype=torch.bfloat16,\n-    device_map=\"cuda\",\n+    device_map=\"auto\",\n ).eval()\n ```\n \n Pass the text query to the processor and return the indexed text embeddings from the model. For image-to-text search, replace the `text` parameter in [`ColPaliProcessor`] with the `images` parameter to pass images.\n \n ```python\n-inputs = processor(text=\"a document about Mars expedition\").to(\"cuda\")\n+inputs = processor(text=\"a document about Mars expedition\").to(model.device)\n with torch.no_grad():\n   text_embeds = model(**inputs, return_tensors=\"pt\").embeddings\n ```\n \n Index the images offline, and during inference, return the query text embeddings to get its closest image embeddings.\n \n-Store the image and image embeddings by writing them to the dataset with [`~datasets.Dataset.map`] as shown below. Add an `embeddings` column that contains the indexed embeddings. ColPali embeddings take up a lot of storage, so remove them from the GPU and store them in the CPU as NumPy vectors.\n+Store the image and image embeddings by writing them to the dataset with [`~datasets.Dataset.map`] as shown below. Add an `embeddings` column that contains the indexed embeddings. ColPali embeddings take up a lot of storage, so remove them from the accelerator and store them in the CPU as NumPy vectors.\n \n ```python\n-ds_with_embeddings = dataset.map(lambda example: {'embeddings': model(**processor(images=example[\"image\"]).to(\"cuda\"), return_tensors=\"pt\").embeddings.to(torch.float32).detach().cpu().numpy()})\n+ds_with_embeddings = dataset.map(lambda example: {'embeddings': model(**processor(images=example[\"image\"]).to(devide), return_tensors=\"pt\").embeddings.to(torch.float32).detach().cpu().numpy()})\n ```\n \n For online inference, create a function to search the image embeddings in batches and retrieve the k-most relevant images. The function below returns the indices in the dataset and their scores for a given indexed dataset, text embeddings, number of top results, and the batch size.\n@@ -112,7 +114,7 @@ Generate the text embeddings and pass them to the function above to return the d\n \n ```python\n with torch.no_grad():\n-  text_embeds = model(**processor(text=\"a document about Mars expedition\").to(\"cuda\"), return_tensors=\"pt\").embeddings\n+  text_embeds = model(**processor(text=\"a document about Mars expedition\").to(model.device), return_tensors=\"pt\").embeddings\n indices, scores = find_top_k_indices_batched(ds_with_embeddings, text_embeds, processor, k=3, batch_size=4)\n print(indices, scores)\n ```\n@@ -141,4 +143,4 @@ for i in indices:\n     <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/doc_3.png\" \n          alt=\"Document 3\" \n          style=\"height: 200px; object-fit: contain;\">\n-</div>\n\\ No newline at end of file\n+</div>"
        },
        {
            "sha": "f63a59a611474aa3fb457ae4cf075506457178a5",
            "filename": "docs/source/en/tasks/visual_question_answering.md",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Ftasks%2Fvisual_question_answering.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -361,13 +361,12 @@ Let's illustrate how you can use this model for VQA. First, let's load the model\n GPU, if available, which we didn't need to do earlier when training, as [`Trainer`] handles this automatically:\n \n ```py\n->>> from transformers import AutoProcessor, Blip2ForConditionalGeneration\n+>>> from transformers import AutoProcessor, Blip2ForConditionalGeneration, infer_device\n >>> import torch\n->>> from accelerate.test_utils.testing import get_backend\n \n >>> processor = AutoProcessor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n >>> model = Blip2ForConditionalGeneration.from_pretrained(\"Salesforce/blip2-opt-2.7b\", torch_dtype=torch.float16)\n->>> device, _, _ = get_backend() # automatically detects the underlying device type (CUDA, CPU, XPU, MPS, etc.)\n+>>> device = infer_device()\n >>> model.to(device)\n ```\n "
        },
        {
            "sha": "671382f8c13806c2eefdd1ea20f711d1434394ae",
            "filename": "docs/source/ja/llm_tutorial.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fja%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fja%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fllm_tutorial.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -101,7 +101,7 @@ LLMÔºàLanguage ModelÔºâ„Å´„Çà„ÇãËá™Â∑±ÂõûÂ∏∞ÁîüÊàê„ÅÆÈáçË¶Å„Å™ÂÅ¥Èù¢„ÅÆ1„Å§„ÅØ\n >>> from transformers import AutoTokenizer\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"openlm-research/open_llama_7b\")\n->>> model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(model.device)\n ```\n \n \n@@ -137,7 +137,7 @@ LLMÔºàLanguage ModelÔºâ„Å´„Çà„ÇãËá™Â∑±ÂõûÂ∏∞ÁîüÊàê„ÅÆÈáçË¶Å„Å™ÂÅ¥Èù¢„ÅÆ1„Å§„ÅØ\n [`~generation.GenerationConfig`] „Éï„Ç°„Ç§„É´„ÅßÊåáÂÆö„Åï„Çå„Å¶„ÅÑ„Å™„ÅÑÂ†¥Âêà„ÄÅ`generate` „ÅØ„Éá„Éï„Ç©„É´„Éà„ÅßÊúÄÂ§ß„Åß 20 „Éà„Éº„ÇØ„É≥„Åæ„ÅßËøî„Åó„Åæ„Åô„ÄÇÊàë„ÄÖ„ÅØ `generate` „Ç≥„Éº„É´„Åß `max_new_tokens` „ÇíÊâãÂãï„ÅßË®≠ÂÆö„Åô„Çã„Åì„Å®„ÇíÂº∑„Åè„ÅäÂãß„ÇÅ„Åó„Åæ„Åô„ÄÇ„Åì„Çå„Å´„Çà„Çä„ÄÅËøî„Åï„Çå„ÇãÊñ∞„Åó„ÅÑ„Éà„Éº„ÇØ„É≥„ÅÆÊúÄÂ§ßÊï∞„ÇíÂà∂Âæ°„Åß„Åç„Åæ„Åô„ÄÇLLMÔºàÊ≠£Á¢∫„Å´„ÅØ„ÄÅ[„Éá„Ç≥„Éº„ÉÄ„ÉºÂ∞ÇÁî®„É¢„Éá„É´](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)Ôºâ„ÇÇÂá∫Âäõ„ÅÆ‰∏ÄÈÉ®„Å®„Åó„Å¶ÂÖ•Âäõ„Éó„É≠„É≥„Éó„Éà„ÇíËøî„Åô„Åì„Å®„Å´Ê≥®ÊÑè„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ\n \n ```py\n->>> model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(model.device)\n \n >>> # By default, the output will contain up to 20 tokens\n >>> generated_ids = model.generate(**model_inputs)\n@@ -159,7 +159,7 @@ LLMÔºàLanguage ModelÔºâ„Å´„Çà„ÇãËá™Â∑±ÂõûÂ∏∞ÁîüÊàê„ÅÆÈáçË¶Å„Å™ÂÅ¥Èù¢„ÅÆ1„Å§„ÅØ\n >>> from transformers import set_seed\n >>> set_seed(0)\n \n->>> model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(model.device)\n \n >>> # LLM + greedy decoding = repetitive, boring output\n >>> generated_ids = model.generate(**model_inputs)\n@@ -182,7 +182,7 @@ LLMÔºàLarge Language ModelsÔºâ„ÅØ[„Éá„Ç≥„Éº„ÉÄ„ÉºÂ∞ÇÁî®](https://huggingface.co/\n >>> # which is shorter, has padding on the right side. Generation fails.\n >>> model_inputs = tokenizer(\n ...     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n-... ).to(\"cuda\")\n+... ).to(model.device)\n >>> generated_ids = model.generate(**model_inputs)\n >>> tokenizer.batch_decode(generated_ids[0], skip_special_tokens=True)[0]\n ''\n@@ -192,7 +192,7 @@ LLMÔºàLarge Language ModelsÔºâ„ÅØ[„Éá„Ç≥„Éº„ÉÄ„ÉºÂ∞ÇÁî®](https://huggingface.co/\n >>> tokenizer.pad_token = tokenizer.eos_token  # Llama has no pad token by default\n >>> model_inputs = tokenizer(\n ...     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n-... ).to(\"cuda\")\n+... ).to(model.device)\n >>> generated_ids = model.generate(**model_inputs)\n >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n '1, 2, 3, 4, 5, 6,'"
        },
        {
            "sha": "e2f0acebd6c4f60d319ce30711f415f8e6f3dd6a",
            "filename": "docs/source/ja/perf_torch_compile.md",
            "status": "modified",
            "additions": 7,
            "deletions": 7,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fja%2Fperf_torch_compile.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fja%2Fperf_torch_compile.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fja%2Fperf_torch_compile.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -28,7 +28,7 @@ rendered properly in your Markdown viewer.\n ```diff\n from transformers import AutoModelForImageClassification\n \n-model = AutoModelForImageClassification.from_pretrained(MODEL_ID).to(\"cuda\")\n+model = AutoModelForImageClassification.from_pretrained(MODEL_ID, device_map=\"auto\")\n + model = torch.compile(model)\n ```\n \n@@ -52,10 +52,10 @@ url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n image = Image.open(requests.get(url, stream=True).raw)\n \n processor = AutoImageProcessor.from_pretrained(\"google/vit-base-patch16-224\")\n-model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\").to(\"cuda\")\n+model = AutoModelForImageClassification.from_pretrained(\"google/vit-base-patch16-224\", device_map=\"auto\")\n model = torch.compile(model)\n \n-processed_input = processor(image, return_tensors='pt').to(device=\"cuda\")\n+processed_input = processor(image, return_tensors='pt').to(model.device)\n \n with torch.no_grad():\n     _ = model(**processed_input)\n@@ -67,11 +67,11 @@ with torch.no_grad():\n from transformers import AutoImageProcessor, AutoModelForObjectDetection\n \n processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n-model = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\").to(\"cuda\")\n+model = AutoModelForObjectDetection.from_pretrained(\"facebook/detr-resnet-50\", device_map=\"auto\")\n model = torch.compile(model)\n \n texts = [\"a photo of a cat\", \"a photo of a dog\"]\n-inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(\"cuda\")\n+inputs = processor(text=texts, images=image, return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     _ = model(**inputs)\n@@ -83,9 +83,9 @@ with torch.no_grad():\n from transformers import SegformerImageProcessor, SegformerForSemanticSegmentation\n \n processor = SegformerImageProcessor.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\")\n-model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\").to(\"cuda\")\n+model = SegformerForSemanticSegmentation.from_pretrained(\"nvidia/segformer-b0-finetuned-ade-512-512\", device_map=\"auto\")\n model = torch.compile(model)\n-seg_inputs = processor(images=image, return_tensors=\"pt\").to(\"cuda\")\n+seg_inputs = processor(images=image, return_tensors=\"pt\").to(model.device)\n \n with torch.no_grad():\n     _ = model(**seg_inputs)"
        },
        {
            "sha": "dcf60b4a0ee9c92cae8ebbc0a0e18e1ff4caaa83",
            "filename": "docs/source/ko/cache_explanation.md",
            "status": "modified",
            "additions": 11,
            "deletions": 7,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fko%2Fcache_explanation.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fcache_explanation.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -99,18 +99,20 @@ cache.layers[idx].values = torch.cat([cache.layers[idx].values, value_states], d\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infer_device\n+\n+device = f\"{infer_device()}:0\"\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n past_key_values = DynamicCache()\n messages = [{\"role\": \"user\", \"content\": \"Hello, what's your name.\"}]\n-inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(\"cuda:0\")\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n \n generated_ids = inputs.input_ids\n-cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=\"cuda:0\")\n+cache_position = torch.arange(inputs.input_ids.shape[1], dtype=torch.int64, device=model.device)\n max_new_tokens = 10\n \n for _ in range(max_new_tokens):\n@@ -143,14 +145,16 @@ print(tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0])\n \n ```py\n import torch\n-from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache\n+from transformers import AutoTokenizer, AutoModelForCausalLM, DynamicCache, infer_device\n+\n+device = f\"{infer_device()}:0\"\n \n model_id = \"meta-llama/Llama-2-7b-chat-hf\"\n-model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=\"cuda:0\")\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, device_map=device)\n tokenizer = AutoTokenizer.from_pretrained(model_id)\n \n messages = [{\"role\": \"user\", \"content\": \"You are a helpful assistant.\"}]\n-inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(\"cuda:0\")\n+inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\", return_dict=True).to(model.device)\n generated_ids = model.generate(**inputs, use_cache=True, max_new_tokens=10)\n \n ```"
        },
        {
            "sha": "a0942ceb96e819836c35a0bbced7b28603a1a822",
            "filename": "docs/source/ko/model_doc/gemma3.md",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fgemma3.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -92,7 +92,7 @@ inputs = processor.apply_chat_template(\n     return_dict=True,\n     return_tensors=\"pt\",\n     add_generation_prompt=True,\n-).to(\"cuda\")\n+).to(model.device)\n \n output = model.generate(**inputs, max_new_tokens=50, cache_implementation=\"static\")\n print(processor.decode(output[0], skip_special_tokens=True))\n@@ -149,7 +149,7 @@ inputs = processor.apply_chat_template(\n     return_dict=True,\n     return_tensors=\"pt\",\n     add_generation_prompt=True,\n-).to(\"cuda\")\n+).to(model.device)\n \n output = model.generate(**inputs, max_new_tokens=50, cache_implementation=\"static\")\n print(processor.decode(output[0], skip_special_tokens=True))\n@@ -206,7 +206,7 @@ visualizer(\"<img>What is shown in this image?\")\n         return_tensors=\"pt\",\n         add_generation_prompt=True,\n     +   do_pan_and_scan=True,\n-        ).to(\"cuda\")\n+        ).to(model.device)\n     ```\n - ÌÖçÏä§Ìä∏ Ï†ÑÏö© Î™®ÎìúÎ°ú ÌõàÎ†®Îêú Gemma-3 1B Ï≤¥ÌÅ¨Ìè¨Ïù∏Ìä∏Ïùò Í≤ΩÏö∞, [`AutoModelForCausalLM`]ÏùÑ ÎåÄÏã† ÏÇ¨Ïö©ÌïòÏÑ∏Ïöî.\n \n@@ -223,7 +223,7 @@ visualizer(\"<img>What is shown in this image?\")\n         device_map=\"auto\",\n         attn_implementation=\"sdpa\"\n     )\n-    input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(\"cuda\")\n+    input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n \n     output = model.generate(**input_ids, cache_implementation=\"static\")\n     print(tokenizer.decode(output[0], skip_special_tokens=True))"
        },
        {
            "sha": "746a9a036048579950a5b17f5d200e315d9a053a",
            "filename": "docs/source/ko/model_doc/mistral.md",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Fmistral.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -58,7 +58,7 @@ rendered properly in your Markdown viewer.\n \n >>> prompt = \"My favourite condiment is\"\n \n->>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n >>> model.to(device)\n \n >>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n@@ -80,7 +80,7 @@ rendered properly in your Markdown viewer.\n ...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n ... ]\n \n->>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n \n >>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n >>> tokenizer.batch_decode(generated_ids)[0]\n@@ -112,7 +112,7 @@ pip install -U flash-attn --no-build-isolation\n \n >>> prompt = \"My favourite condiment is\"\n \n->>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n >>> model.to(device)\n \n >>> generated_ids = model.generate(**model_inputs, max_new_tokens=100, do_sample=True)\n@@ -162,7 +162,7 @@ pip install -U flash-attn --no-build-isolation\n ...     {\"role\": \"user\", \"content\": \"Do you have mayonnaise recipes?\"}\n ... ]\n \n->>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer.apply_chat_template(messages, return_tensors=\"pt\").to(model.device)\n \n >>> generated_ids = model.generate(model_inputs, max_new_tokens=100, do_sample=True)\n >>> tokenizer.batch_decode(generated_ids)[0]\n@@ -230,4 +230,4 @@ pip install -U flash-attn --no-build-isolation\n ## TFMistralForSequenceClassification[[transformers.TFMistralForSequenceClassification]]\n \n [[autodoc]] TFMistralForSequenceClassification\n-    - call\n\\ No newline at end of file\n+    - call"
        },
        {
            "sha": "331d94c8cbf7df81fdf12bb3ac3706ca373efeb6",
            "filename": "docs/source/ko/tasks/idefics.md",
            "status": "modified",
            "additions": 8,
            "deletions": 10,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fko%2Ftasks%2Fidefics.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fko%2Ftasks%2Fidefics.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fidefics.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -113,7 +113,7 @@ IDEFICSÎäî ÌÖçÏä§Ìä∏ Î∞è Ïù¥ÎØ∏ÏßÄ ÌîÑÎ°¨ÌîÑÌä∏Î•º Î™®Îëê ÏàòÏö©Ìï©ÎãàÎã§. Í∑∏\n ...     \"https://images.unsplash.com/photo-1583160247711-2191776b4b91?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=3542&q=80\",\n ... ]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\n@@ -146,7 +146,7 @@ A puppy in a flower bed\n ...     \"This is an image of \",\n ... ]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\n@@ -178,7 +178,7 @@ IDEFICSÎäî ÌõåÎ•≠Ìïú Ï†úÎ°úÏÉ∑ Í≤∞Í≥ºÎ•º Î≥¥Ïó¨Ï£ºÏßÄÎßå, ÏûëÏóÖÏóê ÌäπÏ†ï Ìòï\n ...            \"Describe this image.\\nAssistant:\"\n ...            ]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=30, bad_words_ids=bad_words_ids)\n@@ -213,7 +213,7 @@ Assistant: An image of the Statue of Liberty. Fun fact: the Statue of Liberty is\n ...     \"Question: Where are these people and what's the weather like? Answer:\"\n ... ]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=20, bad_words_ids=bad_words_ids)\n@@ -244,7 +244,7 @@ IDEFICSÎäî ÌäπÏ†ï Ïπ¥ÌÖåÍ≥†Î¶¨Ïùò ÎùºÎ≤®Ïù¥ Ìè¨Ìï®Îêú Îç∞Ïù¥ÌÑ∞Î°ú Î™ÖÏãúÏ†ÅÏúº\n ...     \"Category: \"\n ... ]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=6, bad_words_ids=bad_words_ids)\n@@ -273,7 +273,7 @@ Category: Vegetables\n ...     \"https://images.unsplash.com/photo-1517086822157-2b0358e7684a?ixlib=rb-4.0.3&ixid=M3wxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8fA%3D%3D&auto=format&fit=crop&w=2203&q=80\",\n ...     \"Story: \\n\"]\n \n->>> inputs = processor(prompt, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompt, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, num_beams=2, max_new_tokens=200, bad_words_ids=bad_words_ids)\n@@ -324,7 +324,7 @@ IDEFICSÍ∞Ä Î¨∏ ÏïûÏóê ÏûàÎäî Ìò∏Î∞ïÏùÑ Î≥¥Í≥† Ïú†Î†πÏóê ÎåÄÌïú ÏúºÏä§Ïä§Ìïú Ìï†\n ...     ],\n ... ]\n \n->>> inputs = processor(prompts, return_tensors=\"pt\").to(\"cuda\")\n+>>> inputs = processor(prompts, return_tensors=\"pt\").to(model.device)\n >>> bad_words_ids = processor.tokenizer([\"<image>\", \"<fake_token_around_image>\"], add_special_tokens=False).input_ids\n \n >>> generated_ids = model.generate(**inputs, max_new_tokens=10, bad_words_ids=bad_words_ids)\n@@ -353,10 +353,8 @@ This is an image of a vegetable stand.\n >>> import torch\n >>> from transformers import IdeficsForVisionText2Text, AutoProcessor\n \n->>> device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n-\n >>> checkpoint = \"HuggingFaceM4/idefics-9b-instruct\"\n->>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16).to(device)\n+>>> model = IdeficsForVisionText2Text.from_pretrained(checkpoint, torch_dtype=torch.bfloat16, device_map=\"auto\")\n >>> processor = AutoProcessor.from_pretrained(checkpoint)\n \n >>> prompts = ["
        },
        {
            "sha": "dac40e590b420521d92c3afe9b245f92d3f7002c",
            "filename": "docs/source/ko/tasks/image_feature_extraction.md",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fko%2Ftasks%2Fimage_feature_extraction.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fko%2Ftasks%2Fimage_feature_extraction.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Ftasks%2Fimage_feature_extraction.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -45,9 +45,9 @@ image_gen = Image.open(requests.get(img_urls[1], stream=True).raw).convert(\"RGB\"\n \n ```python\n import torch\n-from transformers import pipeline\n+from transformers import pipeline, infer_device\n \n-DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n+DEVICE = infer_device()\n pipe = pipeline(task=\"image-feature-extraction\", model_name=\"google/vit-base-patch16-384\", device=DEVICE, pool=True)\n ```\n \n@@ -133,4 +133,4 @@ similarity_score = cosine_similarity(embed_real, embed_gen, dim=1)\n print(similarity_score)\n \n # tensor([0.6061], device='cuda:0', grad_fn=<SumBackward1>)\n-```\n\\ No newline at end of file\n+```"
        },
        {
            "sha": "e9ea6470c39c6bb7281db24742bd7a2118135db2",
            "filename": "docs/source/zh/llm_tutorial.md",
            "status": "modified",
            "additions": 8,
            "deletions": 8,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fzh%2Fllm_tutorial.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/0f9c9088d0260565cd89af2925836ea2f8bfe5f3/docs%2Fsource%2Fzh%2Fllm_tutorial.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fzh%2Fllm_tutorial.md?ref=0f9c9088d0260565cd89af2925836ea2f8bfe5f3",
            "patch": "@@ -99,7 +99,7 @@ pip install transformers bitsandbytes>=0.39.0 -q\n >>> from transformers import AutoTokenizer\n \n >>> tokenizer = AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-v0.1\", padding_side=\"left\")\n->>> model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([\"A list of colors: red, blue\"], return_tensors=\"pt\").to(model.device)\n ```\n \n `model_inputs`ÂèòÈáè‰øùÂ≠òÁùÄÂàÜËØçÂêéÁöÑÊñáÊú¨ËæìÂÖ•‰ª•ÂèäÊ≥®ÊÑèÂäõÊé©Á†Å„ÄÇÂ∞ΩÁÆ°[`~generation.GenerationMixin.generate`]Âú®Êú™‰º†ÈÄíÊ≥®ÊÑèÂäõÊé©Á†ÅÊó∂‰ºöÂ∞ΩÂÖ∂ÊâÄËÉΩÊé®Êñ≠Âá∫Ê≥®ÊÑèÂäõÊé©Á†ÅÔºå‰ΩÜÂª∫ËÆÆÂ∞ΩÂèØËÉΩ‰º†ÈÄíÂÆÉ‰ª•Ëé∑ÂæóÊúÄ‰Ω≥ÁªìÊûú„ÄÇ\n@@ -118,7 +118,7 @@ pip install transformers bitsandbytes>=0.39.0 -q\n >>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n >>> model_inputs = tokenizer(\n ...     [\"A list of colors: red, blue\", \"Portugal is\"], return_tensors=\"pt\", padding=True\n-... ).to(\"cuda\")\n+... ).to(model.device)\n >>> generated_ids = model.generate(**model_inputs)\n >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)\n ['A list of colors: red, blue, green, yellow, orange, purple, pink,',\n@@ -147,7 +147,7 @@ pip install transformers bitsandbytes>=0.39.0 -q\n Â¶ÇÊûúÂú®[`~generation.GenerationConfig`]Êñá‰ª∂‰∏≠Ê≤°ÊúâÊåáÂÆöÔºå`generate`ÈªòËÆ§ËøîÂõû20‰∏™tokens„ÄÇÊàë‰ª¨Âº∫ÁÉàÂª∫ËÆÆÂú®ÊÇ®ÁöÑ`generate`Ë∞ÉÁî®‰∏≠ÊâãÂä®ËÆæÁΩÆ`max_new_tokens`‰ª•ÊéßÂà∂ÂÆÉÂèØ‰ª•ËøîÂõûÁöÑÊúÄÂ§ßÊñ∞tokensÊï∞Èáè„ÄÇËØ∑Ê≥®ÊÑèÔºåLLMsÔºàÊõ¥ÂáÜÁ°ÆÂú∞ËØ¥Ôºå‰ªÖ[Ëß£Á†ÅÂô®Ê®°Âûã](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)Ôºâ‰πüÂ∞ÜËæìÂÖ•ÊèêÁ§∫‰Ωú‰∏∫ËæìÂá∫ÁöÑ‰∏ÄÈÉ®ÂàÜËøîÂõû„ÄÇ\n \n ```py\n->>> model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([\"A sequence of numbers: 1, 2\"], return_tensors=\"pt\").to(model.device)\n \n >>> # By default, the output will contain up to 20 tokens\n >>> generated_ids = model.generate(**model_inputs)\n@@ -169,7 +169,7 @@ pip install transformers bitsandbytes>=0.39.0 -q\n >>> from transformers import set_seed\n >>> set_seed(42)\n \n->>> model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([\"I am a cat.\"], return_tensors=\"pt\").to(model.device)\n \n >>> # LLM + greedy decoding = repetitive, boring output\n >>> generated_ids = model.generate(**model_inputs)\n@@ -191,7 +191,7 @@ LLMsÊòØ[‰ªÖËß£Á†ÅÂô®](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)\n >>> # which is shorter, has padding on the right side. Generation fails to capture the logic.\n >>> model_inputs = tokenizer(\n ...     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n-... ).to(\"cuda\")\n+... ).to(model.device)\n >>> generated_ids = model.generate(**model_inputs)\n >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n '1, 2, 33333333333'\n@@ -201,7 +201,7 @@ LLMsÊòØ[‰ªÖËß£Á†ÅÂô®](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)\n >>> tokenizer.pad_token = tokenizer.eos_token  # Most LLMs don't have a pad token by default\n >>> model_inputs = tokenizer(\n ...     [\"1, 2, 3\", \"A, B, C, D, E\"], padding=True, return_tensors=\"pt\"\n-... ).to(\"cuda\")\n+... ).to(model.device)\n >>> generated_ids = model.generate(**model_inputs)\n >>> tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n '1, 2, 3, 4, 5, 6,'\n@@ -218,7 +218,7 @@ LLMsÊòØ[‰ªÖËß£Á†ÅÂô®](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)\n ... )\n >>> set_seed(0)\n >>> prompt = \"\"\"How many helicopters can a human eat in one sitting? Reply as a thug.\"\"\"\n->>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer([prompt], return_tensors=\"pt\").to(model.device)\n >>> input_length = model_inputs.input_ids.shape[1]\n >>> generated_ids = model.generate(**model_inputs, max_new_tokens=20)\n >>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])\n@@ -234,7 +234,7 @@ LLMsÊòØ[‰ªÖËß£Á†ÅÂô®](https://huggingface.co/learn/nlp-course/chapter1/6?fw=pt)\n ...     },\n ...     {\"role\": \"user\", \"content\": \"How many helicopters can a human eat in one sitting?\"},\n ... ]\n->>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(\"cuda\")\n+>>> model_inputs = tokenizer.apply_chat_template(messages, add_generation_prompt=True, return_tensors=\"pt\").to(model.device)\n >>> input_length = model_inputs.shape[1]\n >>> generated_ids = model.generate(model_inputs, do_sample=True, max_new_tokens=20)\n >>> print(tokenizer.batch_decode(generated_ids[:, input_length:], skip_special_tokens=True)[0])"
        }
    ],
    "stats": {
        "total": 342,
        "additions": 170,
        "deletions": 172
    }
}