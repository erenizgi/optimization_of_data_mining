{
    "author": "ydshieh",
    "message": "Update `squad_convert_example_to_features` to work with numpy v2 (#35955)\n\n* Fix\r\n\r\n* Fix\r\n\r\n* Fix\r\n\r\n---------\r\n\r\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "692afa102d202716f0073cd54c07c78a25eebb07",
    "files": [
        {
            "sha": "4677af124e9b1042287670a7239ec7cd815801f6",
            "filename": "src/transformers/data/processors/squad.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/692afa102d202716f0073cd54c07c78a25eebb07/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/692afa102d202716f0073cd54c07c78a25eebb07/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fdata%2Fprocessors%2Fsquad.py?ref=692afa102d202716f0073cd54c07c78a25eebb07",
            "patch": "@@ -249,7 +249,7 @@ def squad_convert_example_to_features(\n         else:\n             p_mask[-len(span[\"tokens\"]) : -(len(truncated_query) + sequence_added_tokens)] = 0\n \n-        pad_token_indices = np.where(span[\"input_ids\"] == tokenizer.pad_token_id)\n+        pad_token_indices = np.where(np.atleast_1d(span[\"input_ids\"] == tokenizer.pad_token_id))\n         special_token_indices = np.asarray(\n             tokenizer.get_special_tokens_mask(span[\"input_ids\"], already_has_special_tokens=True)\n         ).nonzero()"
        }
    ],
    "stats": {
        "total": 2,
        "additions": 1,
        "deletions": 1
    }
}