{
    "author": "qubvel",
    "message": "Remove loading custom kernel for RT-DETRv2 (#36098)\n\n* Remove loading custom kernels\r\n\r\n* Remove config param\r\n\r\n* Fixup",
    "sha": "6397916dd22a87569e16da89f25a33c4599b7f19",
    "files": [
        {
            "sha": "1a16e226c893857282d25042da7b7d891e97eed2",
            "filename": "src/transformers/models/rt_detr_v2/configuration_rt_detr_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 4,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/6397916dd22a87569e16da89f25a33c4599b7f19/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6397916dd22a87569e16da89f25a33c4599b7f19/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py?ref=6397916dd22a87569e16da89f25a33c4599b7f19",
            "patch": "@@ -130,8 +130,6 @@ class RTDetrV2Config(PretrainedConfig):\n             Indicates whether the initial query embeddings for the decoder should be learned during training\n         anchor_image_size (`Tuple[int, int]`, *optional*):\n             Height and width of the input image used during evaluation to generate the bounding box anchors. If None, automatic generate anchor is applied.\n-        disable_custom_kernels (`bool`, *optional*, defaults to `True`):\n-            Whether to disable custom kernels.\n         with_box_refine (`bool`, *optional*, defaults to `True`):\n             Whether to apply iterative bounding box refinement, where each decoder layer refines the bounding boxes\n             based on the predictions from the previous layer.\n@@ -238,7 +236,6 @@ def __init__(\n         box_noise_scale=1.0,\n         learn_initial_query=False,\n         anchor_image_size=None,\n-        disable_custom_kernels=True,\n         with_box_refine=True,\n         is_encoder_decoder=True,\n         # Loss\n@@ -336,7 +333,6 @@ def __init__(\n         self.learn_initial_query = learn_initial_query\n         self.anchor_image_size = anchor_image_size\n         self.auxiliary_loss = auxiliary_loss\n-        self.disable_custom_kernels = disable_custom_kernels\n         self.with_box_refine = with_box_refine\n         # Loss\n         self.matcher_alpha = matcher_alpha"
        },
        {
            "sha": "e258d0d71c67c77262deac18ed4743735effad31",
            "filename": "src/transformers/models/rt_detr_v2/modeling_rt_detr_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 53,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/6397916dd22a87569e16da89f25a33c4599b7f19/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6397916dd22a87569e16da89f25a33c4599b7f19/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodeling_rt_detr_v2.py?ref=6397916dd22a87569e16da89f25a33c4599b7f19",
            "patch": "@@ -19,11 +19,9 @@\n # See the License for the specific language governing permissions and\n # limitations under the License.\n import math\n-import os\n import warnings\n from dataclasses import dataclass\n from functools import partial\n-from pathlib import Path\n from typing import Dict, List, Optional, Tuple, Union\n \n import torch\n@@ -39,52 +37,15 @@\n     ModelOutput,\n     add_start_docstrings,\n     add_start_docstrings_to_model_forward,\n-    is_ninja_available,\n-    is_torch_cuda_available,\n     is_torchdynamo_compiling,\n-    logging,\n     replace_return_docstrings,\n )\n from ...utils.backbone_utils import load_backbone\n from .configuration_rt_detr_v2 import RTDetrV2Config\n \n \n-logger = logging.get_logger(__name__)\n-\n _CONFIG_FOR_DOC = \"RTDetrV2Config\"\n \n-MultiScaleDeformableAttention = None\n-\n-\n-def load_cuda_kernels():\n-    from torch.utils.cpp_extension import load\n-\n-    global MultiScaleDeformableAttention\n-\n-    root = Path(__file__).resolve().parent.parent.parent / \"kernels\" / \"deformable_detr\"\n-    src_files = [\n-        root / filename\n-        for filename in [\n-            \"vision.cpp\",\n-            os.path.join(\"cpu\", \"ms_deform_attn_cpu.cpp\"),\n-            os.path.join(\"cuda\", \"ms_deform_attn_cuda.cu\"),\n-        ]\n-    ]\n-\n-    MultiScaleDeformableAttention = load(\n-        \"MultiScaleDeformableAttention\",\n-        src_files,\n-        with_cuda=True,\n-        extra_include_paths=[str(root)],\n-        extra_cflags=[\"-DWITH_CUDA=1\"],\n-        extra_cuda_cflags=[\n-            \"-DCUDA_HAS_FP16=1\",\n-            \"-D__CUDA_NO_HALF_OPERATORS__\",\n-            \"-D__CUDA_NO_HALF_CONVERSIONS__\",\n-            \"-D__CUDA_NO_HALF2_OPERATORS__\",\n-        ],\n-    )\n-\n \n def multi_scale_deformable_attention_v2(\n     value: Tensor,\n@@ -160,10 +121,9 @@ def multi_scale_deformable_attention_v2(\n     return output.transpose(1, 2).contiguous()\n \n \n+# the main change\n class RTDetrV2MultiscaleDeformableAttention(nn.Module):\n     \"\"\"\n-    Multiscale deformable attention as proposed in Deformable DETR.\n-\n     RTDetrV2 version of multiscale deformable attention, extending the base implementation\n     with improved offset handling and initialization.\n     \"\"\"\n@@ -173,13 +133,6 @@ def __init__(self, config: RTDetrV2Config):\n         num_heads = config.decoder_attention_heads\n         n_points = config.decoder_n_points\n \n-        kernel_loaded = MultiScaleDeformableAttention is not None\n-        if is_torch_cuda_available() and is_ninja_available() and not kernel_loaded:\n-            try:\n-                load_cuda_kernels()\n-            except Exception as e:\n-                logger.warning(f\"Could not load the custom kernel for multi-scale deformable attention: {e}\")\n-\n         if config.d_model % num_heads != 0:\n             raise ValueError(\n                 f\"embed_dim (d_model) must be divisible by num_heads, but got {config.d_model} and {num_heads}\"\n@@ -207,18 +160,15 @@ def __init__(self, config: RTDetrV2Config):\n         self.value_proj = nn.Linear(config.d_model, config.d_model)\n         self.output_proj = nn.Linear(config.d_model, config.d_model)\n \n-        self.disable_custom_kernels = config.disable_custom_kernels\n         self.offset_scale = config.decoder_offset_scale\n         self.method = config.decoder_method\n+\n         # Initialize n_points list and scale\n         n_points_list = [self.n_points for _ in range(self.n_levels)]\n         self.n_points_list = n_points_list\n         n_points_scale = [1 / n for n in n_points_list for _ in range(n)]\n         self.register_buffer(\"n_points_scale\", torch.tensor(n_points_scale, dtype=torch.float32))\n \n-    def with_pos_embed(self, tensor: torch.Tensor, position_embeddings: Optional[Tensor]):\n-        return tensor if position_embeddings is None else tensor + position_embeddings\n-\n     def forward(\n         self,\n         hidden_states: torch.Tensor,\n@@ -234,7 +184,7 @@ def forward(\n     ):\n         # Process inputs up to sampling locations calculation using parent class logic\n         if position_embeddings is not None:\n-            hidden_states = self.with_pos_embed(hidden_states, position_embeddings)\n+            hidden_states = hidden_states + position_embeddings\n \n         batch_size, num_queries, _ = hidden_states.shape\n         batch_size, sequence_length, _ = encoder_hidden_states.shape"
        },
        {
            "sha": "dfd2a4613192c4cc062f3c6063aeb01f1623aec2",
            "filename": "src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py",
            "status": "modified",
            "additions": 30,
            "deletions": 9,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/6397916dd22a87569e16da89f25a33c4599b7f19/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6397916dd22a87569e16da89f25a33c4599b7f19/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py?ref=6397916dd22a87569e16da89f25a33c4599b7f19",
            "patch": "@@ -12,6 +12,7 @@\n # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n # See the License for the specific language governing permissions and\n # limitations under the License.\n+import warnings\n from functools import partial\n from typing import List, Optional\n \n@@ -31,7 +32,6 @@\n     RTDetrForObjectDetection,\n     RTDetrMLPPredictionHead,\n     RTDetrModel,\n-    RTDetrMultiscaleDeformableAttention,\n     RTDetrPreTrainedModel,\n )\n \n@@ -141,8 +141,6 @@ class RTDetrV2Config(PretrainedConfig):\n             Indicates whether the initial query embeddings for the decoder should be learned during training\n         anchor_image_size (`Tuple[int, int]`, *optional*):\n             Height and width of the input image used during evaluation to generate the bounding box anchors. If None, automatic generate anchor is applied.\n-        disable_custom_kernels (`bool`, *optional*, defaults to `True`):\n-            Whether to disable custom kernels.\n         with_box_refine (`bool`, *optional*, defaults to `True`):\n             Whether to apply iterative bounding box refinement, where each decoder layer refines the bounding boxes\n             based on the predictions from the previous layer.\n@@ -249,7 +247,6 @@ def __init__(\n         box_noise_scale=1.0,\n         learn_initial_query=False,\n         anchor_image_size=None,\n-        disable_custom_kernels=True,\n         with_box_refine=True,\n         is_encoder_decoder=True,\n         # Loss\n@@ -347,7 +344,6 @@ def __init__(\n         self.learn_initial_query = learn_initial_query\n         self.anchor_image_size = anchor_image_size\n         self.auxiliary_loss = auxiliary_loss\n-        self.disable_custom_kernels = disable_custom_kernels\n         self.with_box_refine = with_box_refine\n         # Loss\n         self.matcher_alpha = matcher_alpha\n@@ -466,22 +462,47 @@ def multi_scale_deformable_attention_v2(\n \n \n # the main change\n-class RTDetrV2MultiscaleDeformableAttention(RTDetrMultiscaleDeformableAttention):\n+class RTDetrV2MultiscaleDeformableAttention(nn.Module):\n     \"\"\"\n     RTDetrV2 version of multiscale deformable attention, extending the base implementation\n     with improved offset handling and initialization.\n     \"\"\"\n \n     def __init__(self, config: RTDetrV2Config):\n+        super().__init__()\n         num_heads = config.decoder_attention_heads\n         n_points = config.decoder_n_points\n-        # Initialize parent class with config parameters\n-        super().__init__(config=config, num_heads=num_heads, n_points=n_points)\n+\n+        if config.d_model % num_heads != 0:\n+            raise ValueError(\n+                f\"embed_dim (d_model) must be divisible by num_heads, but got {config.d_model} and {num_heads}\"\n+            )\n+        dim_per_head = config.d_model // num_heads\n+        # check if dim_per_head is power of 2\n+        if not ((dim_per_head & (dim_per_head - 1) == 0) and dim_per_head != 0):\n+            warnings.warn(\n+                \"You'd better set embed_dim (d_model) in RTDetrV2MultiscaleDeformableAttention to make the\"\n+                \" dimension of each attention head a power of 2 which is more efficient in the authors' CUDA\"\n+                \" implementation.\"\n+            )\n+\n+        self.im2col_step = 64\n+\n+        self.d_model = config.d_model\n \n         # V2-specific attributes\n         self.n_levels = config.decoder_n_levels\n+        self.n_heads = num_heads\n+        self.n_points = n_points\n+\n+        self.sampling_offsets = nn.Linear(config.d_model, num_heads * self.n_levels * n_points * 2)\n+        self.attention_weights = nn.Linear(config.d_model, num_heads * self.n_levels * n_points)\n+        self.value_proj = nn.Linear(config.d_model, config.d_model)\n+        self.output_proj = nn.Linear(config.d_model, config.d_model)\n+\n         self.offset_scale = config.decoder_offset_scale\n         self.method = config.decoder_method\n+\n         # Initialize n_points list and scale\n         n_points_list = [self.n_points for _ in range(self.n_levels)]\n         self.n_points_list = n_points_list\n@@ -503,7 +524,7 @@ def forward(\n     ):\n         # Process inputs up to sampling locations calculation using parent class logic\n         if position_embeddings is not None:\n-            hidden_states = self.with_pos_embed(hidden_states, position_embeddings)\n+            hidden_states = hidden_states + position_embeddings\n \n         batch_size, num_queries, _ = hidden_states.shape\n         batch_size, sequence_length, _ = encoder_hidden_states.shape"
        }
    ],
    "stats": {
        "total": 99,
        "additions": 33,
        "deletions": 66
    }
}