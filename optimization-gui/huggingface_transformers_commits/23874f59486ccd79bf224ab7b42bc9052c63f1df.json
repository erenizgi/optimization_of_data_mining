{
    "author": "zucchini-nlp",
    "message": "Idefics: enable generation tests (#34062)\n\n* add idefics\r\n\r\n* conflicts after merging main\r\n\r\n* enable tests but need to fix some\r\n\r\n* fix tests\r\n\r\n* no print\r\n\r\n* fix/skip some slow tests\r\n\r\n* continue not skip\r\n\r\n* rebasing broken smth, this is the fix",
    "sha": "23874f59486ccd79bf224ab7b42bc9052c63f1df",
    "files": [
        {
            "sha": "1e4d7a4702453a8f287ee2561b95ecd6f656d512",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 10,
            "deletions": 1,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/23874f59486ccd79bf224ab7b42bc9052c63f1df/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23874f59486ccd79bf224ab7b42bc9052c63f1df/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=23874f59486ccd79bf224ab7b42bc9052c63f1df",
            "patch": "@@ -726,14 +726,23 @@ def _prepare_attention_mask(model_kwargs: Dict[str, Any], new_length: int, is_en\n     elif mask_length_diff > 0:\n         model_kwargs[mask_key] = torch.cat([mask, mask.new_ones((mask.shape[0], mask_length_diff))], dim=-1)\n \n+    # Handle cross attention models\n     if \"cross_attention_mask\" in model_kwargs:\n-        # Mllama case is special and has another mask for cross attention model\n+        # Mllama case\n         cross_mask = model_kwargs[\"cross_attention_mask\"]\n         if mask_length_diff < 0:\n             model_kwargs[\"cross_attention_mask\"] = cross_mask[:, :mask_length_diff]\n         elif mask_length_diff > 0:\n             new_mask = cross_mask[:, -1:, :, :].repeat(1, mask_length_diff, 1, 1)\n             model_kwargs[\"cross_attention_mask\"] = torch.cat([cross_mask, new_mask], dim=1)\n+    elif \"image_attention_mask\" in model_kwargs:\n+        # IDEFICS case\n+        cross_mask = model_kwargs[\"image_attention_mask\"]\n+        if mask_length_diff < 0:\n+            model_kwargs[\"image_attention_mask\"] = cross_mask[:, :mask_length_diff]\n+        elif mask_length_diff > 0:\n+            new_mask = cross_mask[:, -1:, :].repeat(1, mask_length_diff, 1)\n+            model_kwargs[\"image_attention_mask\"] = torch.cat([cross_mask, new_mask], dim=1)\n \n     return model_kwargs\n "
        },
        {
            "sha": "09be2f6bc224eef6eec3b1414bc4e1406472b9c2",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 1,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/23874f59486ccd79bf224ab7b42bc9052c63f1df/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23874f59486ccd79bf224ab7b42bc9052c63f1df/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=23874f59486ccd79bf224ab7b42bc9052c63f1df",
            "patch": "@@ -2005,6 +2005,7 @@ def generate(\n         # generating the first new token or not, and we only want to use the embeddings for the first new token)\n         if not self.config.is_encoder_decoder and model_input_name == \"inputs_embeds\":\n             model_kwargs[\"use_cache\"] = True\n+            generation_config.use_cache = True\n         else:\n             model_kwargs[\"use_cache\"] = generation_config.use_cache\n \n@@ -4299,7 +4300,8 @@ def _assisted_decoding(\n                             newly_added_length,\n                             is_decoder_attention=True,\n                         )\n-                    else:\n+                    # some (V)LLMs have hard requirement on SDPA and thus never return attn\n+                    elif outputs.attentions[0] is not None:\n                         decoder_attentions = _split_model_outputs(\n                             decoder_attentions,\n                             outputs.attentions,"
        },
        {
            "sha": "81159ee1c0cd30ed02c7997fcd39c74857d06d11",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 17,
            "deletions": 20,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/23874f59486ccd79bf224ab7b42bc9052c63f1df/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23874f59486ccd79bf224ab7b42bc9052c63f1df/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=23874f59486ccd79bf224ab7b42bc9052c63f1df",
            "patch": "@@ -28,12 +28,12 @@\n from torch import nn\n from torch.nn import CrossEntropyLoss\n \n-from ... import PreTrainedModel\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, StaticCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import AttentionMaskConverter\n from ...modeling_outputs import ModelOutput\n-from ...modeling_utils import PretrainedConfig\n+from ...modeling_utils import PretrainedConfig, PreTrainedModel\n from ...pytorch_utils import ALL_LAYERNORM_LAYERS\n from ...utils import (\n     add_start_docstrings,\n@@ -622,11 +622,9 @@ def forward(\n             query_states = self.q_layer_norm(query_states)\n             key_states = self.k_layer_norm(key_states)\n \n+        causal_mask = attention_mask\n         if attention_mask is not None:\n-            if attention_mask.size() != (bsz, 1, q_len, kv_seq_len):\n-                raise ValueError(\n-                    f\"Attention mask should be of size {(bsz, 1, q_len, kv_seq_len)}, but is {attention_mask.size()}\"\n-                )\n+            causal_mask = causal_mask[:, :, :, : key_states.shape[-2]]\n \n         # SDPA with memory-efficient backend is currently (torch==2.1.2) bugged with non-contiguous inputs with custom attn_mask,\n         # Reference: https://github.com/pytorch/pytorch/issues/112577.\n@@ -638,13 +636,13 @@ def forward(\n         # We dispatch to SDPA's Flash Attention or Efficient kernels via this `is_causal` if statement instead of an inline conditional assignment\n         # in SDPA to support both torch.compile's dynamic shapes and full graph options. An inline conditional prevents dynamic shapes from compiling.\n         # The q_len > 1 is necessary to match with AttentionMaskConverter.to_causal_4d that does not create a causal mask in case q_len == 1.\n-        is_causal = True if self.is_causal and attention_mask is None and q_len > 1 else False\n+        is_causal = True if self.is_causal and causal_mask is None and q_len > 1 else False\n \n         attn_output = torch.nn.functional.scaled_dot_product_attention(\n             query_states,\n             key_states,\n             value_states,\n-            attn_mask=attention_mask,\n+            attn_mask=causal_mask,\n             dropout_p=self.dropout if self.training else 0.0,\n             is_causal=is_causal,\n         )\n@@ -1490,7 +1488,7 @@ def _prepare_4d_causal_attention_mask_with_cache_position(\n         return causal_mask\n \n \n-class IdeficsForVisionText2Text(IdeficsPreTrainedModel):\n+class IdeficsForVisionText2Text(IdeficsPreTrainedModel, GenerationMixin):\n     _keys_to_ignore_on_load_missing = [r\"lm_head.weight\"]\n     _tied_weights_keys = [\"model.embed_tokens.weight\", \"lm_head.weight\"]\n \n@@ -1670,6 +1668,7 @@ def prepare_inputs_for_generation(\n         position_ids=None,\n         pixel_values=None,\n         image_hidden_states=None,\n+        image_attention_mask=None,\n         use_cache=None,\n         cache_position=None,\n         **kwargs,\n@@ -1678,6 +1677,8 @@ def prepare_inputs_for_generation(\n         if past_key_values is not None:\n             if input_ids.shape[1] != cache_position.shape[0]:\n                 input_ids = input_ids[:, cache_position]\n+                if image_attention_mask is not None:\n+                    image_attention_mask = image_attention_mask[:, -input_ids.shape[1] :]\n \n         if attention_mask is not None and position_ids is None:\n             # create position_ids on the fly for batch generation\n@@ -1696,7 +1697,8 @@ def prepare_inputs_for_generation(\n                 model_inputs[\"perceiver_embeddings\"] = image_hidden_states\n             else:\n                 model_inputs[\"image_encoder_embeddings\"] = image_hidden_states\n-            pixel_values = None\n+        else:\n+            model_inputs[\"pixel_values\"] = pixel_values\n \n         model_inputs.update(\n             {\n@@ -1706,21 +1708,13 @@ def prepare_inputs_for_generation(\n                 \"cache_position\": cache_position,\n                 \"position_ids\": position_ids,\n                 \"attention_mask\": attention_mask,\n-                \"pixel_values\": pixel_values,\n-                \"image_attention_mask\": kwargs.get(\"image_attention_mask\", None),\n+                \"image_attention_mask\": image_attention_mask,\n                 \"interpolate_pos_encoding\": kwargs.get(\"interpolate_pos_encoding\", False),\n             }\n         )\n \n         return model_inputs\n \n-    @staticmethod\n-    def _expand_inputs_for_generation(\n-        *args,\n-        **model_kwargs,\n-    ):\n-        return expand_inputs_for_generation(*args, **model_kwargs)\n-\n     def _update_model_kwargs_for_generation(\n         self,\n         outputs: ModelOutput,\n@@ -1738,7 +1732,10 @@ def _update_model_kwargs_for_generation(\n         if \"image_attention_mask\" in model_kwargs:\n             image_attention_mask = model_kwargs[\"image_attention_mask\"]\n             last_mask = image_attention_mask[:, -1, :].unsqueeze(1)\n-            model_kwargs[\"image_attention_mask\"] = last_mask\n+            if model_kwargs.get(\"use_cache\", True):\n+                model_kwargs[\"image_attention_mask\"] = last_mask\n+            else:\n+                model_kwargs[\"image_attention_mask\"] = torch.cat([image_attention_mask, last_mask], dim=1)\n \n         # Get the precomputed image_hidden_states\n         model_kwargs[\"image_hidden_states\"] = outputs.image_hidden_states"
        },
        {
            "sha": "d34e0acde4c8144e0a8b094636fc4a2d5720e3a1",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 17,
            "deletions": 31,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/23874f59486ccd79bf224ab7b42bc9052c63f1df/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23874f59486ccd79bf224ab7b42bc9052c63f1df/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=23874f59486ccd79bf224ab7b42bc9052c63f1df",
            "patch": "@@ -1427,6 +1427,7 @@ def forward(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n+            use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -1657,35 +1658,19 @@ def prepare_inputs_for_generation(\n         past_key_values=None,\n         attention_mask=None,\n         inputs_embeds=None,\n+        cache_position=None,\n+        pixel_values=None,\n+        pixel_attention_mask=None,\n+        image_hidden_states=None,\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n-        past_length = 0\n-        # Omit tokens covered by past_key_values\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         if past_key_values is not None:\n-            # Past key values are always initialized with a `Cache` object -> no need for if-else anymore\n-            past_length = past_key_values.get_seq_length()\n-            max_cache_length = past_key_values.get_max_cache_shape()\n-\n-            # Keep only the unprocessed tokens:\n-            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n-            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n-            # input)\n-            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n-                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n-            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n-            # input_ids based on the past_length.\n-            elif past_length < input_ids.shape[1]:\n-                input_ids = input_ids[:, past_length:]\n-            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n-\n-            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n-            if (\n-                max_cache_length is not None\n-                and attention_mask is not None\n-                and past_length + input_ids.shape[1] > max_cache_length\n-            ):\n-                attention_mask = attention_mask[:, -max_cache_length:]\n+            if inputs_embeds is not None:  # Exception 1\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:\n+                input_ids = input_ids[:, cache_position]\n \n         position_ids = kwargs.get(\"position_ids\", None)\n         if attention_mask is not None and position_ids is None:\n@@ -1696,21 +1681,22 @@ def prepare_inputs_for_generation(\n                 position_ids = position_ids[:, -input_ids.shape[1] :]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and past_length == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n+        # but IDEFICS requires noth ids and embeds to be present\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": input_ids}\n         else:\n-            model_inputs = {\"input_ids\": input_ids}\n+            # The clone here is for the same reason as for `position_ids`.\n+            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n \n         if num_logits_to_keep is not None:\n             model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n \n-        image_hidden_states = kwargs.get(\"image_hidden_states\", None)\n         if image_hidden_states is not None:\n             pixel_values = None\n             pixel_attention_mask = None\n         else:\n-            pixel_values = kwargs.get(\"pixel_values\", None)\n-            pixel_attention_mask = kwargs.get(\"pixel_attention_mask\", None)\n+            pixel_values = pixel_values\n+            pixel_attention_mask = pixel_attention_mask\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,"
        },
        {
            "sha": "e653fd3d2a6ba251ea7317064e71efd4bbce8647",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 22,
            "deletions": 33,
            "changes": 55,
            "blob_url": "https://github.com/huggingface/transformers/blob/23874f59486ccd79bf224ab7b42bc9052c63f1df/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23874f59486ccd79bf224ab7b42bc9052c63f1df/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=23874f59486ccd79bf224ab7b42bc9052c63f1df",
            "patch": "@@ -24,7 +24,8 @@\n \n from ... import PreTrainedModel\n from ...activations import ACT2FN\n-from ...cache_utils import Cache\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_outputs import BaseModelOutput, ModelOutput\n from ...utils import (\n@@ -953,6 +954,8 @@ def forward(\n \n         past_seen_tokens = 0\n         if use_cache:\n+            if past_key_values is None:\n+                past_key_values = DynamicCache()\n             past_seen_tokens = past_key_values.get_seq_length()\n \n         if inputs_embeds is not None and input_ids is None and past_seen_tokens == 0:\n@@ -1019,6 +1022,7 @@ def forward(\n             attention_mask=attention_mask,\n             position_ids=position_ids,\n             past_key_values=past_key_values,\n+            use_cache=use_cache,\n             output_attentions=output_attentions,\n             output_hidden_states=output_hidden_states,\n             return_dict=return_dict,\n@@ -1040,7 +1044,7 @@ def forward(\n     \"\"\"The Idefics3 Model with a language modeling head. It is made up a SigLIP vision encoder, with a language modeling head on top. \"\"\",\n     IDEFICS3_START_DOCSTRING,\n )\n-class Idefics3ForConditionalGeneration(Idefics3PreTrainedModel):\n+class Idefics3ForConditionalGeneration(Idefics3PreTrainedModel, GenerationMixin):\n     _tied_weights_keys = [\"lm_head.weight\"]\n \n     # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.__init__ with Idefics2->Idefics3\n@@ -1245,35 +1249,19 @@ def prepare_inputs_for_generation(\n         past_key_values=None,\n         attention_mask=None,\n         inputs_embeds=None,\n+        cache_position=None,\n+        pixel_values=None,\n+        pixel_attention_mask=None,\n+        image_hidden_states=None,\n         num_logits_to_keep=None,\n         **kwargs,\n     ):\n-        past_length = 0\n-        # Omit tokens covered by past_key_values\n+        # If we have cache: let's slice `input_ids` through `cache_position`, to keep only the unprocessed tokens\n         if past_key_values is not None:\n-            # Past key values are always initialized with a `Cache` object -> no need for if-else anymore\n-            past_length = past_key_values.get_seq_length()\n-            max_cache_length = past_key_values.get_max_cache_shape()\n-\n-            # Keep only the unprocessed tokens:\n-            # 1 - If the length of the attention_mask exceeds the length of input_ids, then we are in a setting where\n-            # some of the inputs are exclusively passed as part of the cache (e.g. when passing input_embeds as\n-            # input)\n-            if attention_mask is not None and attention_mask.shape[1] > input_ids.shape[1]:\n-                input_ids = input_ids[:, -(attention_mask.shape[1] - past_length) :]\n-            # 2 - If the past_length is smaller than input_ids', then input_ids holds all input tokens. We can discard\n-            # input_ids based on the past_length.\n-            elif past_length < input_ids.shape[1]:\n-                input_ids = input_ids[:, past_length:]\n-            # 3 - Otherwise (past_length >= input_ids.shape[1]), let's assume input_ids only has unprocessed tokens.\n-\n-            # If we are about to go beyond the maximum cache length, we need to crop the input attention mask.\n-            if (\n-                max_cache_length is not None\n-                and attention_mask is not None\n-                and past_length + input_ids.shape[1] > max_cache_length\n-            ):\n-                attention_mask = attention_mask[:, -max_cache_length:]\n+            if inputs_embeds is not None:  # Exception 1\n+                input_ids = input_ids[:, -cache_position.shape[0] :]\n+            elif input_ids.shape[1] != cache_position.shape[0]:\n+                input_ids = input_ids[:, cache_position]\n \n         position_ids = kwargs.get(\"position_ids\", None)\n         if attention_mask is not None and position_ids is None:\n@@ -1284,21 +1272,22 @@ def prepare_inputs_for_generation(\n                 position_ids = position_ids[:, -input_ids.shape[1] :]\n \n         # if `inputs_embeds` are passed, we only want to use them in the 1st generation step\n-        if inputs_embeds is not None and past_length == 0:\n-            model_inputs = {\"inputs_embeds\": inputs_embeds}\n+        # but IDEFICS requires noth ids and embeds to be present\n+        if inputs_embeds is not None and cache_position[0] == 0:\n+            model_inputs = {\"inputs_embeds\": inputs_embeds, \"input_ids\": input_ids}\n         else:\n-            model_inputs = {\"input_ids\": input_ids}\n+            # The clone here is for the same reason as for `position_ids`.\n+            model_inputs = {\"input_ids\": input_ids.clone(memory_format=torch.contiguous_format), \"inputs_embeds\": None}\n \n         if num_logits_to_keep is not None:\n             model_inputs[\"num_logits_to_keep\"] = num_logits_to_keep\n \n-        image_hidden_states = kwargs.get(\"image_hidden_states\", None)\n         if image_hidden_states is not None:\n             pixel_values = None\n             pixel_attention_mask = None\n         else:\n-            pixel_values = kwargs.get(\"pixel_values\", None)\n-            pixel_attention_mask = kwargs.get(\"pixel_attention_mask\", None)\n+            pixel_values = pixel_values\n+            pixel_attention_mask = pixel_attention_mask\n         model_inputs.update(\n             {\n                 \"position_ids\": position_ids,"
        },
        {
            "sha": "a1bc526566726f2ebff16b9d53f3753c88bd68d2",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/23874f59486ccd79bf224ab7b42bc9052c63f1df/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23874f59486ccd79bf224ab7b42bc9052c63f1df/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=23874f59486ccd79bf224ab7b42bc9052c63f1df",
            "patch": "@@ -153,7 +153,11 @@ def _get_logits_processor_kwargs(self, do_sample=False, config=None):\n         # This is a band-aid for VLM models, to ensure they don't generate image/video tokens which would cause them\n         # to crash. On pretrained models this isn't a risk, as they are trained to not generate these tokens.\n         if config is not None:\n-            image_token_index = config.image_token_index if hasattr(config, \"image_token_index\") else None\n+            image_token_index = (\n+                config.image_token_index\n+                if getattr(config, \"image_token_index\", None) is not None\n+                else getattr(config, \"image_token_id\", None)\n+            )\n             video_token_index = config.video_token_index if hasattr(config, \"video_token_index\") else None\n             if image_token_index is not None and image_token_index < config.get_text_config().vocab_size:\n                 logits_processor_kwargs[\"bad_words_ids\"].append([image_token_index])\n@@ -1496,13 +1500,14 @@ def test_past_key_values_format(self):\n             if \"past_key_values\" not in outputs:\n                 self.skipTest(reason=\"This model doesn't return `past_key_values`\")\n \n+            text_config = config.get_text_config()\n             num_hidden_layers = (\n-                getattr(config, \"decoder_layers\", None)\n-                or getattr(config, \"num_decoder_layers\", None)\n-                or config.num_hidden_layers\n+                getattr(text_config, \"decoder_layers\", None)\n+                or getattr(text_config, \"num_decoder_layers\", None)\n+                or text_config.num_hidden_layers\n             )\n-            num_attention_heads = getattr(config, \"decoder_attention_heads\", config.num_attention_heads)\n-            embed_dim = getattr(config, \"d_model\", config.hidden_size)\n+            num_attention_heads = getattr(text_config, \"decoder_attention_heads\", text_config.num_attention_heads)\n+            embed_dim = getattr(text_config, \"d_model\", text_config.hidden_size)\n             per_head_embed_dim = embed_dim // num_attention_heads\n \n             past_kv = outputs[\"past_key_values\"]"
        },
        {
            "sha": "62b6ca22293b6f3747a1eec8395634eca927f7f1",
            "filename": "tests/models/idefics/test_modeling_idefics.py",
            "status": "modified",
            "additions": 187,
            "deletions": 1,
            "changes": 188,
            "blob_url": "https://github.com/huggingface/transformers/blob/23874f59486ccd79bf224ab7b42bc9052c63f1df/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23874f59486ccd79bf224ab7b42bc9052c63f1df/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics%2Ftest_modeling_idefics.py?ref=23874f59486ccd79bf224ab7b42bc9052c63f1df",
            "patch": "@@ -14,8 +14,10 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Idefics model.\"\"\"\n \n+import inspect\n import unittest\n \n+import pytest\n from parameterized import parameterized\n \n from transformers import BitsAndBytesConfig, IdeficsConfig, is_torch_available, is_vision_available\n@@ -31,6 +33,7 @@\n )\n from transformers.utils import cached_property\n \n+from ...generation.test_utils import GenerationTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor, random_attention_mask\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -318,6 +321,12 @@ def prepare_pixel_values(self):\n     def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n         self.skipTest(reason=\"Idefics has a hard requirement on SDPA, skipping this test\")\n \n+    @require_torch_sdpa\n+    @slow\n+    @parameterized.expand([(\"float16\",), (\"bfloat16\",), (\"float32\",)])\n+    def test_eager_matches_sdpa_generate(self):\n+        self.skipTest(reason=\"Idefics has a hard requirement on SDPA, skipping this test\")\n+\n \n @unittest.skipIf(not is_torch_greater_or_equal_than_2_0, reason=\"pytorch 2.0 or higher is required\")\n @require_torch\n@@ -580,8 +589,9 @@ def test_eager_matches_sdpa_inference(self, torch_dtype: str):\n \n @unittest.skipIf(not is_torch_greater_or_equal_than_2_0, reason=\"pytorch 2.0 or higher is required\")\n @require_torch\n-class IdeficsForVisionText2TextTest(IdeficsModelTest, unittest.TestCase):\n+class IdeficsForVisionText2TextTest(IdeficsModelTest, GenerationTesterMixin, unittest.TestCase):\n     all_model_classes = (IdeficsForVisionText2Text,) if is_torch_available() else ()\n+    all_generative_model_classes = (IdeficsForVisionText2Text,) if is_torch_available() else ()\n \n     def setUp(self):\n         self.model_tester = IdeficsModelTester(\n@@ -590,6 +600,182 @@ def setUp(self):\n         )\n         self.config_tester = ConfigTester(self, config_class=IdeficsConfig, hidden_size=37)\n \n+    @pytest.mark.generate\n+    def test_left_padding_compatibility(self):\n+        \"\"\"Overwrite because IDEFICS needs image attention mask to be also padded\"\"\"\n+        # NOTE: left-padding results in small numerical differences. This is expected.\n+        # See https://github.com/huggingface/transformers/issues/25420#issuecomment-1775317535\n+\n+        def _prepare_model_kwargs(input_ids, attention_mask, image_attention_mask, signature):\n+            model_kwargs = {\n+                \"input_ids\": input_ids,\n+                \"attention_mask\": attention_mask,\n+                \"image_attention_mask\": image_attention_mask,\n+            }\n+            if \"position_ids\" in signature:\n+                position_ids = torch.cumsum(attention_mask, dim=-1) - 1\n+                position_ids.masked_fill_(attention_mask == 0, 1)\n+                model_kwargs[\"position_ids\"] = position_ids\n+            if \"cache_position\" in signature:\n+                cache_position = torch.arange(input_ids.shape[-1], device=torch_device)\n+                model_kwargs[\"cache_position\"] = cache_position\n+            return model_kwargs\n+\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+            input_ids = inputs_dict.pop(\"input_ids\")\n+            attention_mask = inputs_dict.pop(\"attention_mask\")\n+            if attention_mask is None:\n+                attention_mask = torch.ones_like(input_ids)\n+            image_attention_mask = inputs_dict.pop(\"image_attention_mask\", None)\n+\n+            model = model_class(config).to(torch_device).eval()\n+            signature = inspect.signature(model.forward).parameters.keys()\n+\n+            # no cache as some models require special cache classes to be init outside forward\n+            model.generation_config.use_cache = False\n+\n+            # Without padding\n+            model_kwargs = _prepare_model_kwargs(input_ids, attention_mask, image_attention_mask, signature)\n+            next_logits_wo_padding = model(**model_kwargs, **inputs_dict).logits[:, -1, :]\n+\n+            # With left-padding (length 32)\n+            # can hardcode pad_token to be 0 as we'll do attn masking anyway\n+            pad_token_id = (\n+                config.get_text_config().pad_token_id if config.get_text_config().pad_token_id is not None else 0\n+            )\n+            pad_size = (input_ids.shape[0], 32)\n+            padding = torch.ones(pad_size, dtype=input_ids.dtype, device=torch_device) * pad_token_id\n+            padded_input_ids = torch.cat((padding, input_ids), dim=1)\n+            padded_attention_mask = torch.cat((torch.zeros_like(padding), attention_mask), dim=1)\n+\n+            pad_size_img = (input_ids.shape[0], 32, image_attention_mask.shape[-1])\n+            extra_img_mask = torch.zeros(pad_size_img, dtype=image_attention_mask.dtype, device=torch_device)\n+            padded_image_attention_mask = torch.cat([extra_img_mask, image_attention_mask], dim=1)\n+            model_kwargs = _prepare_model_kwargs(\n+                padded_input_ids, padded_attention_mask, padded_image_attention_mask, signature\n+            )\n+            next_logits_with_padding = model(**model_kwargs, **inputs_dict).logits[:, -1, :]\n+\n+            # They should result in very similar logits\n+            self.assertTrue(torch.allclose(next_logits_wo_padding, next_logits_with_padding, atol=1e-5))\n+\n+    @pytest.mark.generate\n+    def test_generate_continue_from_past_key_values(self):\n+        \"\"\"Overwrite because IDEFICS needs image attention mask to be also processed\"\"\"\n+\n+        # Tests that we can continue generating from past key values, returned from a previous `generate` call\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+            # Let's make it always:\n+            # 1. use cache (for obvious reasons)\n+            # 2. generate to max length (which can be achieved by setting the eos token to an invalid value), which\n+            #    would make the test flaky (e.g. EOS is generated on iteration 1 on both generations, but the\n+            #    continuation would force it to generate beyond an EOS token)\n+            # 3. ignore `token_type_ids` for simplicity\n+            # 4. ignore `forced_eos_token_id`, which requires further manipulation of the continuation inputs and is\n+            #    active by default on some models\n+            # 5. ignore `encoder_no_repeat_ngram_size`, which is set by default in some encoder-decoder models. When\n+            #    we use their decoder as a stand-alone model, `encoder_no_repeat_ngram_size` actually prevents\n+            #    repetition exclusively from the prompt. This test relies on comparing one call vs 2 calls\n+            #    with cache, what is considered a prompt is different in the two cases.\n+\n+            model = model_class(config).to(torch_device)\n+            model.eval()\n+            model.generation_config.pad_token_id = model.generation_config.eos_token_id = -1\n+            model.generation_config.forced_eos_token_id = None\n+            model.generation_config.encoder_no_repeat_ngram_size = 0\n+            model.generation_config.use_cache = True\n+\n+            # Traditional way of generating text, with `return_dict_in_generate` to return the past key values\n+            outputs = model.generate(**inputs, do_sample=False, max_new_tokens=4, return_dict_in_generate=True)\n+\n+            # Let's generate again, but passing the past key values in between (3 + 1 = 4 tokens). Note that the\n+            # inputs may need to be tweaked across `generate` calls (like the attention mask).\n+            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=3, return_dict_in_generate=True)\n+\n+            # Continue from the tokens generated above, preparing the inputs accordingly\n+            inputs[\"past_key_values\"] = outputs_cached.past_key_values\n+            new_attention_len = outputs_cached.sequences.shape[-1]\n+            inputs[\"input_ids\"] = outputs_cached.sequences\n+            if \"attention_mask\" in inputs:\n+                inputs[\"attention_mask\"] = torch.nn.functional.pad(\n+                    inputs[\"attention_mask\"],\n+                    (0, new_attention_len - inputs[\"attention_mask\"].shape[1]),\n+                    mode=\"constant\",\n+                    value=1,\n+                )\n+            if \"image_attention_mask\" in inputs:\n+                inputs[\"image_attention_mask\"] = inputs[\"image_attention_mask\"][:, -1:, :]\n+\n+            outputs_cached = model.generate(**inputs, do_sample=False, max_new_tokens=1, return_dict_in_generate=True)\n+\n+            # The two sets of generated text and past kv should be equal to each other\n+            self.assertListEqual(outputs.sequences.tolist(), outputs_cached.sequences.tolist())\n+            for layer_idx in range(len(outputs_cached.past_key_values)):\n+                for kv_idx in range(len(outputs_cached.past_key_values[layer_idx])):\n+                    self.assertTrue(\n+                        torch.allclose(\n+                            outputs.past_key_values[layer_idx][kv_idx],\n+                            outputs_cached.past_key_values[layer_idx][kv_idx],\n+                        )\n+                    )\n+\n+    @pytest.mark.generate\n+    def test_generate_without_input_ids(self):\n+        \"\"\"Overwrite because IDEFICS needs image attention mask to be also processed and requires image at input always.\"\"\"\n+\n+        config, input_dict = self.prepare_config_and_inputs_for_generate()\n+        pixel_values = input_dict[\"pixel_values\"]\n+        image_attention_mask = input_dict[\"image_attention_mask\"][:, -1:, :]\n+\n+        # hack in case they are equal, otherwise the attn mask will be [0]\n+        if config.bos_token_id == config.pad_token_id:\n+            config.pad_token_id = None\n+\n+        for model_class in self.all_generative_model_classes:\n+            model = model_class(config).to(torch_device)\n+            model.eval()\n+\n+            output_ids_generate = model.generate(\n+                pixel_values=pixel_values,\n+                image_attention_mask=image_attention_mask,\n+                do_sample=False,\n+                max_new_tokens=self.max_new_tokens,\n+                remove_invalid_values=True,\n+            )\n+            self.assertIsNotNone(output_ids_generate)\n+\n+    def _check_attentions_for_generate(\n+        self, batch_size, attentions, min_length, max_length, config, use_cache=False, num_beam_groups=1\n+    ):\n+        \"\"\"\n+        Overwrite from generation tests because Idefics has only SDPA layers.\n+        Do not skip because we still want generation tests to run. Rather we can remove checks for shape.\n+        \"\"\"\n+        pass\n+\n+    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @unittest.skip(reason=\"We only test the model that takes in multiple images\")\n+    def test_custom_4d_attention_mask(self):\n+        pass\n+\n+    @unittest.skip(reason=\"IDEFICS cannot compile due to dynamic control flow when checking inputs\")\n+    def test_generate_compile_fullgraph(self):\n+        pass\n+\n     @unittest.skip(reason=\"We only test the model that takes in multiple images\")\n     def test_model(self):\n         pass"
        },
        {
            "sha": "f87e87607c2a17d76827025ec899503ec6c7e990",
            "filename": "tests/models/idefics2/test_modeling_idefics2.py",
            "status": "modified",
            "additions": 69,
            "deletions": 1,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/23874f59486ccd79bf224ab7b42bc9052c63f1df/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23874f59486ccd79bf224ab7b42bc9052c63f1df/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_modeling_idefics2.py?ref=23874f59486ccd79bf224ab7b42bc9052c63f1df",
            "patch": "@@ -19,6 +19,7 @@\n import unittest\n from io import BytesIO\n \n+import pytest\n import requests\n \n from transformers import (\n@@ -96,7 +97,7 @@ def __init__(\n             \"pad_token_id\": 0,  # None in the original configuration_mistral, we set it to the unk_token_id\n             \"bos_token_id\": 1,\n             \"eos_token_id\": 2,\n-            \"image_token_id\": 32_001,\n+            \"image_token_id\": 99,\n             \"tie_word_embeddings\": False,\n             \"rope_theta\": 10000.0,\n             \"sliding_window\": 32,\n@@ -334,6 +335,7 @@ class Idefics2ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n     \"\"\"\n \n     all_model_classes = (Idefics2ForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (Idefics2ForConditionalGeneration,) if is_torch_available() else ()\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = True\n@@ -356,6 +358,72 @@ def test_flash_attn_2_generate_padding_right(self):\n     def test_flash_attn_2_inference_padding_right(self):\n         pass\n \n+    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Prompt lookup decoding needs a way to indicate `bad_word_ids` that should not be suggested as candidates\"\n+    )\n+    def test_prompt_lookup_decoding_matches_greedy_search(self):\n+        pass\n+\n+    @unittest.skip(reason=\" FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    def test_generate_from_inputs_embeds_decoder_only(self):\n+        # overwrite because IDEFICS needs ids and embeds at the input to be not None\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+\n+            # Ignore:\n+            # a) eos (to always output 20 tokens) and pad (so we don't try to infer the attn mask from the input_ids,\n+            #   which would cause a mismatch),\n+            config.pad_token_id = config.eos_token_id = -1\n+            config.is_decoder = True\n+            model = model_class(config).to(torch_device).eval()\n+            input_ids = inputs_dict.pop(\"input_ids\")\n+\n+            # Traditional way of generating text\n+            outputs_from_ids = model.generate(\n+                input_ids, max_new_tokens=5, return_dict_in_generate=True, output_scores=True\n+            )\n+            self.assertEqual(outputs_from_ids.sequences.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n+\n+            # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output)\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+            outputs_from_embeds = model.generate(\n+                input_ids,\n+                inputs_embeds=inputs_embeds,\n+                max_new_tokens=5,\n+                return_dict_in_generate=True,\n+                output_scores=True,\n+            )\n+            self.assertListEqual(outputs_from_ids.sequences.tolist(), outputs_from_embeds.sequences.tolist())\n+\n+            # But if we pass different inputs_embeds, we should get different outputs (the output text may be the\n+            # same, but the logits will almost surely be different)\n+            random_embeds = torch.rand_like(inputs_embeds)\n+            outputs_from_rand_embeds = model.generate(\n+                input_ids,\n+                inputs_embeds=random_embeds,\n+                max_new_tokens=5,\n+                return_dict_in_generate=True,\n+                output_scores=True,\n+            )\n+            for i in range(len(outputs_from_rand_embeds.scores)):\n+                self.assertFalse(torch.allclose(outputs_from_embeds.scores[i], outputs_from_rand_embeds.scores[i]))\n+\n     # We need to override as we need to prepare such that the image token is the last token\n     def test_resize_tokens_embeddings(self):\n         (original_config, inputs_dict) = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "44e06b07c5475232597c0b70e31b6da8e62d572e",
            "filename": "tests/models/idefics3/test_modeling_idefics3.py",
            "status": "modified",
            "additions": 68,
            "deletions": 0,
            "changes": 68,
            "blob_url": "https://github.com/huggingface/transformers/blob/23874f59486ccd79bf224ab7b42bc9052c63f1df/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23874f59486ccd79bf224ab7b42bc9052c63f1df/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_modeling_idefics3.py?ref=23874f59486ccd79bf224ab7b42bc9052c63f1df",
            "patch": "@@ -19,6 +19,7 @@\n import unittest\n from io import BytesIO\n \n+import pytest\n import requests\n \n from transformers import (\n@@ -321,6 +322,7 @@ class Idefics3ForConditionalGenerationModelTest(GenerationTesterMixin, ModelTest\n     \"\"\"\n \n     all_model_classes = (Idefics3ForConditionalGeneration,) if is_torch_available() else ()\n+    all_generative_model_classes = (Idefics3ForConditionalGeneration,) if is_torch_available() else ()\n     fx_compatible = False\n     test_pruning = False\n     test_resize_embeddings = True\n@@ -343,6 +345,72 @@ def test_flash_attn_2_generate_padding_right(self):\n     def test_flash_attn_2_inference_padding_right(self):\n         pass\n \n+    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n+    def test_contrastive_generate(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n+    def test_contrastive_generate_dict_outputs_use_cache(self):\n+        pass\n+\n+    @unittest.skip(reason=\"Contrastive search is not implemented for VLMs that do cross-attn\")\n+    def test_contrastive_generate_low_memory(self):\n+        pass\n+\n+    @unittest.skip(\n+        reason=\"Prompt lookup decoding needs a way to indicate `bad_word_ids` that should not be suggested as candidates\"\n+    )\n+    def test_prompt_lookup_decoding_matches_greedy_search(self):\n+        pass\n+\n+    @unittest.skip(reason=\" FlashAttention only support fp16 and bf16 data type\")\n+    def test_flash_attn_2_fp32_ln(self):\n+        pass\n+\n+    @pytest.mark.generate\n+    def test_generate_from_inputs_embeds_decoder_only(self):\n+        # overwrite because IDEFICS needs ids and embeds at the input to be not None\n+        for model_class in self.all_generative_model_classes:\n+            config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n+\n+            # Ignore:\n+            # a) eos (to always output 20 tokens) and pad (so we don't try to infer the attn mask from the input_ids,\n+            #   which would cause a mismatch),\n+            config.pad_token_id = config.eos_token_id = -1\n+            config.is_decoder = True\n+            model = model_class(config).to(torch_device).eval()\n+            input_ids = inputs_dict.pop(\"input_ids\")\n+\n+            # Traditional way of generating text\n+            outputs_from_ids = model.generate(\n+                input_ids, max_new_tokens=5, return_dict_in_generate=True, output_scores=True\n+            )\n+            self.assertEqual(outputs_from_ids.sequences.shape, (input_ids.shape[0], input_ids.shape[1] + 5))\n+\n+            # Same thing, but from input embeddings (`input_ids` is passed so the prompt is present in the output)\n+            inputs_embeds = model.get_input_embeddings()(input_ids)\n+            outputs_from_embeds = model.generate(\n+                input_ids,\n+                inputs_embeds=inputs_embeds,\n+                max_new_tokens=5,\n+                return_dict_in_generate=True,\n+                output_scores=True,\n+            )\n+            self.assertListEqual(outputs_from_ids.sequences.tolist(), outputs_from_embeds.sequences.tolist())\n+\n+            # But if we pass different inputs_embeds, we should get different outputs (the output text may be the\n+            # same, but the logits will almost surely be different)\n+            random_embeds = torch.rand_like(inputs_embeds)\n+            outputs_from_rand_embeds = model.generate(\n+                input_ids,\n+                inputs_embeds=random_embeds,\n+                max_new_tokens=5,\n+                return_dict_in_generate=True,\n+                output_scores=True,\n+            )\n+            for i in range(len(outputs_from_rand_embeds.scores)):\n+                self.assertFalse(torch.allclose(outputs_from_embeds.scores[i], outputs_from_rand_embeds.scores[i]))\n+\n     # We need to override as we need to prepare such that the image token is the last token\n     def test_resize_tokens_embeddings(self):\n         (original_config, inputs_dict) = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "38c1f5ff1774b7d433f674f3e5105b091fabf307",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/23874f59486ccd79bf224ab7b42bc9052c63f1df/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/23874f59486ccd79bf224ab7b42bc9052c63f1df/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=23874f59486ccd79bf224ab7b42bc9052c63f1df",
            "patch": "@@ -4768,7 +4768,7 @@ def test_flash_attn_2_from_config(self):\n \n             config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n             # TODO: to change it in the future with other relevant auto classes\n-            fa2_model = AutoModelForCausalLM.from_config(\n+            fa2_model = model_class._from_config(\n                 config, attn_implementation=\"flash_attention_2\", torch_dtype=torch.bfloat16\n             ).to(torch_device)\n \n@@ -4789,7 +4789,7 @@ def test_flash_attn_2_from_config(self):\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 fa2_model.save_pretrained(tmpdirname)\n \n-                model_from_pretrained = AutoModelForCausalLM.from_pretrained(tmpdirname)\n+                model_from_pretrained = model_class.from_pretrained(tmpdirname)\n \n                 self.assertTrue(model_from_pretrained.config._attn_implementation != \"flash_attention_2\")\n "
        }
    ],
    "stats": {
        "total": 502,
        "additions": 406,
        "deletions": 96
    }
}