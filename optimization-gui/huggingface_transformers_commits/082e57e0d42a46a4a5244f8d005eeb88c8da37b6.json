{
    "author": "yonigottesman",
    "message": "Fix  #34494 assistant tokens when truncated (#34531)\n\n* Fix assistant tokens when truncated\r\n\r\n* fix test\r\n\r\n* fix test\r\n\r\n* step",
    "sha": "082e57e0d42a46a4a5244f8d005eeb88c8da37b6",
    "files": [
        {
            "sha": "381f3ef497d9bd34ceffcd9abbcd680b8aca8a25",
            "filename": "src/transformers/tokenization_utils_base.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/src%2Ftransformers%2Ftokenization_utils_base.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/src%2Ftransformers%2Ftokenization_utils_base.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_base.py?ref=082e57e0d42a46a4a5244f8d005eeb88c8da37b6",
            "patch": "@@ -1722,7 +1722,7 @@ def apply_chat_template(\n                             if start_token is None:\n                                 # start_token is out of bounds maybe due to truncation.\n                                 break\n-                            for token_id in range(start_token, end_token + 1 if end_token else len(input_ids)):\n+                            for token_id in range(start_token, end_token + 1 if end_token else len(input_ids[i])):\n                                 current_mask[token_id] = 1\n                         assistant_masks.append(current_mask)\n                     out[\"assistant_masks\"] = assistant_masks if is_batched else assistant_masks[0]"
        },
        {
            "sha": "7dcf5399703103ef037671be7fd291875d1956f3",
            "filename": "tests/models/layoutlmv2/test_tokenization_layoutlmv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv2%2Ftest_tokenization_layoutlmv2.py?ref=082e57e0d42a46a4a5244f8d005eeb88c8da37b6",
            "patch": "@@ -2497,3 +2497,7 @@ def test_chat_template(self):\n     @unittest.skip(\"Chat is not supported\")\n     def test_chat_template_return_assistant_tokens_mask(self):\n         pass\n+\n+    @unittest.skip(\"Chat is not supported\")\n+    def test_chat_template_return_assistant_tokens_mask_truncated(self):\n+        pass"
        },
        {
            "sha": "9af0861536f73d9cffe0d38f090752e5dbc43ce4",
            "filename": "tests/models/layoutlmv3/test_tokenization_layoutlmv3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutlmv3%2Ftest_tokenization_layoutlmv3.py?ref=082e57e0d42a46a4a5244f8d005eeb88c8da37b6",
            "patch": "@@ -2450,3 +2450,7 @@ def test_chat_template(self):\n     @unittest.skip(\"Chat is not supported\")\n     def test_chat_template_return_assistant_tokens_mask(self):\n         pass\n+\n+    @unittest.skip(\"Chat is not supported\")\n+    def test_chat_template_return_assistant_tokens_mask_truncated(self):\n+        pass"
        },
        {
            "sha": "f387e52790fce301b38d62749b050458ce4cfae0",
            "filename": "tests/models/layoutxlm/test_tokenization_layoutxlm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Flayoutxlm%2Ftest_tokenization_layoutxlm.py?ref=082e57e0d42a46a4a5244f8d005eeb88c8da37b6",
            "patch": "@@ -1991,3 +1991,7 @@ def test_chat_template(self):\n     @unittest.skip(\"Chat is not supported\")\n     def test_chat_template_return_assistant_tokens_mask(self):\n         pass\n+\n+    @unittest.skip(\"Chat is not supported\")\n+    def test_chat_template_return_assistant_tokens_mask_truncated(self):\n+        pass"
        },
        {
            "sha": "eaf30131d340541485459096f5289f9f9f1dde77",
            "filename": "tests/models/markuplm/test_tokenization_markuplm.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmarkuplm%2Ftest_tokenization_markuplm.py?ref=082e57e0d42a46a4a5244f8d005eeb88c8da37b6",
            "patch": "@@ -2330,3 +2330,7 @@ def test_added_tokens_serialization(self):\n     @unittest.skip(\"Chat is not supported\")\n     def test_chat_template_return_assistant_tokens_mask(self):\n         pass\n+\n+    @unittest.skip(\"Chat is not supported\")\n+    def test_chat_template_return_assistant_tokens_mask_truncated(self):\n+        pass"
        },
        {
            "sha": "0a911f7182b4a0ff5fea58c989545ed889be3f31",
            "filename": "tests/models/tapas/test_tokenization_tapas.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ftapas%2Ftest_tokenization_tapas.py?ref=082e57e0d42a46a4a5244f8d005eeb88c8da37b6",
            "patch": "@@ -1290,3 +1290,7 @@ def test_chat_template(self):\n     @unittest.skip(\"Chat is not supported\")\n     def test_chat_template_return_assistant_tokens_mask(self):\n         pass\n+\n+    @unittest.skip(\"Chat is not supported\")\n+    def test_chat_template_return_assistant_tokens_mask_truncated(self):\n+        pass"
        },
        {
            "sha": "a6ac2ff3d38096d17d99611a8e4b775ba7b795ad",
            "filename": "tests/models/udop/test_tokenization_udop.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fudop%2Ftest_tokenization_udop.py?ref=082e57e0d42a46a4a5244f8d005eeb88c8da37b6",
            "patch": "@@ -1161,6 +1161,10 @@ def test_chat_template(self):\n     def test_chat_template_return_assistant_tokens_mask(self):\n         pass\n \n+    @unittest.skip(\"Chat is not supported\")\n+    def test_chat_template_return_assistant_tokens_mask_truncated(self):\n+        pass\n+\n     @unittest.skip(reason=\"Chat template tests don't play well with table/layout models.\")\n     def test_chat_template_batched(self):\n         pass"
        },
        {
            "sha": "a3bbbf3c9e97b2b61e4743ff20c1aa1f95f3b63f",
            "filename": "tests/test_tokenization_common.py",
            "status": "modified",
            "additions": 104,
            "deletions": 0,
            "changes": 104,
            "blob_url": "https://github.com/huggingface/transformers/blob/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Ftest_tokenization_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/082e57e0d42a46a4a5244f8d005eeb88c8da37b6/tests%2Ftest_tokenization_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_common.py?ref=082e57e0d42a46a4a5244f8d005eeb88c8da37b6",
            "patch": "@@ -1327,6 +1327,110 @@ def test_chat_template_return_assistant_tokens_mask(self):\n                     [0] * (assistant_start2 - assistant_end - 1),\n                 )\n \n+    @require_jinja\n+    def test_chat_template_return_assistant_tokens_mask_truncated(self):\n+        dummy_template = (\n+            \"{% for message in messages %}\"\n+            \"{% if (message['role'] != 'assistant') %}\"\n+            \"{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}\"\n+            \"{% elif (message['role'] == 'assistant')%}\"\n+            \"{{'<|im_start|>' + message['role'] + '\\n'}}\"\n+            \"{% generation %}\"\n+            \"{{message['content'] + '<|im_end|>'}}\"\n+            \"{% endgeneration %}\"\n+            \"{{'\\n'}}\"\n+            \"{% endif %}\"\n+            \"{% endfor %}\"\n+        )\n+        conversations = [\n+            [\n+                {\"role\": \"system\", \"content\": \"system message\"},\n+                {\"role\": \"user\", \"content\": \"user message\"},\n+                {\n+                    \"role\": \"assistant\",\n+                    \"content\": (\n+                        \"start turn assistant. long string to be truncated, long string to be truncated, \"\n+                        \"long string to be truncated, long string to be truncated, long string to be truncated\"\n+                    ),\n+                },\n+                {\"role\": \"user\", \"content\": \"another user message\"},\n+            ],\n+            [\n+                {\"role\": \"system\", \"content\": \"system message\"},\n+                {\"role\": \"user\", \"content\": \"user message\"},\n+                {\n+                    \"role\": \"assistant\",\n+                    \"content\": (\n+                        \"start turn assistant. long string to be truncated, long string to be truncated, \"\n+                        \"long string to be truncated, long string to be truncated, long string to be truncated\"\n+                    ),\n+                },\n+                {\"role\": \"user\", \"content\": \"another user message\"},\n+            ],\n+        ]\n+\n+        for tokenizer, pretrained_name, _ in self.tokenizers_list:\n+            with self.subTest(f\"{tokenizer.__class__.__name__} ({pretrained_name})\"):\n+                if not self.test_rust_tokenizer:\n+                    self.skipTest(reason=\"No fast tokenizer defined\")\n+\n+                tokenizer_r = self.rust_tokenizer_class.from_pretrained(pretrained_name)\n+\n+                # Find where to truncate, as the amount of tokens is different for different tokenizers and I want the\n+                # truncation to happen in the middle of the assistant content.\n+                full_encoding = tokenizer_r.apply_chat_template(\n+                    conversations[0],\n+                    chat_template=dummy_template,\n+                    tokenize=True,\n+                    return_dict=True,\n+                )\n+                chat_string = tokenizer_r.apply_chat_template(\n+                    conversations[0], tokenize=False, chat_template=dummy_template\n+                )\n+                truncation_position = full_encoding.char_to_token(chat_string.index(\", long string to be truncated,\"))\n+\n+                # check batched\n+                output = tokenizer_r.apply_chat_template(\n+                    conversations,\n+                    chat_template=dummy_template,\n+                    tokenize=True,\n+                    return_assistant_tokens_mask=True,\n+                    max_length=truncation_position,\n+                    truncation=True,\n+                    return_dict=True,\n+                )\n+                for i, conv in enumerate(conversations):\n+                    chat_string = tokenizer_r.apply_chat_template(conv, tokenize=False, chat_template=dummy_template)\n+                    assistant_start = output.char_to_token(i, chat_string.index(\"start turn assistant\"))\n+\n+                    # assert 1 from assistant_start to the end because the rest is truncated.\n+                    self.assertEqual(\n+                        output[\"assistant_masks\"][i][assistant_start:],\n+                        [1] * (len(output[\"assistant_masks\"][i]) - assistant_start),\n+                    )\n+\n+                # check not batched\n+                output = tokenizer_r.apply_chat_template(\n+                    conversations[0],\n+                    chat_template=dummy_template,\n+                    tokenize=True,\n+                    return_assistant_tokens_mask=True,\n+                    return_dict=True,\n+                    max_length=truncation_position,\n+                    truncation=True,\n+                )\n+\n+                chat_string = tokenizer_r.apply_chat_template(\n+                    conversations[0], tokenize=False, chat_template=dummy_template\n+                )\n+                assistant_start = output.char_to_token(0, chat_string.index(\"start turn assistant\"))\n+\n+                # assert 1 from assistant_start to the end because the rest is truncated.\n+                self.assertEqual(\n+                    output[\"assistant_masks\"][assistant_start:],\n+                    [1] * (len(output[\"assistant_masks\"]) - assistant_start),\n+                )\n+\n     @require_jinja\n     def test_continue_final_message(self):\n         dummy_template = \"\"\""
        }
    ],
    "stats": {
        "total": 130,
        "additions": 129,
        "deletions": 1
    }
}