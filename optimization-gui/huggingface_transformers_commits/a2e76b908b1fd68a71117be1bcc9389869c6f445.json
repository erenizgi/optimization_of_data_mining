{
    "author": "Cyrilvallez",
    "message": "ðŸš¨ðŸš¨ Switch default compilation to fullgraph=False (#40137)\n\n* switch default\n\n* docstring\n\n* docstring\n\n* rework tests and remove outdated restrictions\n\n* simplify\n\n* we need a check for static cache\n\n* fix\n\n* rename var\n\n* fix\n\n* revert\n\n* style\n\n* rename test",
    "sha": "a2e76b908b1fd68a71117be1bcc9389869c6f445",
    "files": [
        {
            "sha": "b8e8dcad8c517e9c0904c2e596a1d9024de78a3b",
            "filename": "src/transformers/generation/configuration_utils.py",
            "status": "modified",
            "additions": 8,
            "deletions": 6,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fconfiguration_utils.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -43,7 +43,7 @@\n \n logger = logging.get_logger(__name__)\n METADATA_FIELDS = (\"_from_model_config\", \"_commit_hash\", \"_original_object_hash\", \"transformers_version\")\n-NEED_SETUP_CACHE_CLASSES_MAPPING = {}\n+STATIC_CACHE_CLASSES_MAPPING = {}\n QUANT_BACKEND_CLASSES_MAPPING = {}\n ALL_CACHE_IMPLEMENTATIONS = []\n \n@@ -60,7 +60,7 @@\n     )\n     from .logits_process import SynthIDTextWatermarkLogitsProcessor, WatermarkLogitsProcessor\n \n-    NEED_SETUP_CACHE_CLASSES_MAPPING = {\n+    STATIC_CACHE_CLASSES_MAPPING = {\n         \"static\": StaticCache,\n         \"offloaded_static\": OffloadedStaticCache,\n         \"sliding_window\": SlidingWindowCache,\n@@ -70,7 +70,7 @@\n         \"offloaded_hybrid_chunked\": OffloadedHybridCache,\n     }\n     QUANT_BACKEND_CLASSES_MAPPING = {\"quanto\": QuantoQuantizedCache, \"HQQ\": HQQQuantizedCache}\n-    ALL_CACHE_IMPLEMENTATIONS = list(NEED_SETUP_CACHE_CLASSES_MAPPING.keys()) + [\"offloaded\", \"dynamic\", \"quantized\"]\n+    ALL_CACHE_IMPLEMENTATIONS = list(STATIC_CACHE_CLASSES_MAPPING.keys()) + [\"offloaded\", \"dynamic\", \"quantized\"]\n \n \n class GenerationMode(ExplicitEnum):\n@@ -1536,8 +1536,10 @@ class CompileConfig:\n     See [`torch.compile`](https://pytorch.org/docs/stable/generated/torch.compile.html) for more details on the arguments.\n \n     Args:\n-        fullgraph (`bool`, *optional*, defaults to `True`):\n-            If `True`, requires that the whole forward be capturable in a single graph.\n+        fullgraph (`bool`, *optional*, defaults to `False`):\n+            If False (default), attempts to discover compileable regions that will be optimized. If True, then require\n+            that the entire function be capturable into a single graph. If this is not possible (that is, if there are\n+            graph breaks), then an error will be raised.\n         dynamic (`bool` or `None`, *optional*):\n             Whether to try to use dynamic shape graphs.\n         backend (`str` or `Callable`, *optional*, defaults to `\"inductor\"`):\n@@ -1566,7 +1568,7 @@ class CompileConfig:\n     ```\n     \"\"\"\n \n-    fullgraph: bool = True\n+    fullgraph: bool = False\n     dynamic: Optional[bool] = None\n     backend: Union[str, Callable] = \"inductor\"\n     mode: str = \"reduce-overhead\""
        },
        {
            "sha": "8f091d7f87029e1497b321ec3d02a4f0b8700875",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 7,
            "deletions": 18,
            "changes": 25,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -71,9 +71,8 @@\n     _prepare_token_type_ids,\n )\n from .configuration_utils import (\n-    NEED_SETUP_CACHE_CLASSES_MAPPING,\n     QUANT_BACKEND_CLASSES_MAPPING,\n-    CompileConfig,\n+    STATIC_CACHE_CLASSES_MAPPING,\n     GenerationConfig,\n     GenerationMode,\n )\n@@ -1826,7 +1825,7 @@ def _get_cache(self, cache_implementation: str, batch_size: int, max_cache_len:\n         if cache_implementation == \"hybrid\" and \"llama4\" in getattr(self.config, \"model_type\", \"\"):\n             cache_implementation = \"hybrid_chunked\"\n \n-        cache_cls: Cache = NEED_SETUP_CACHE_CLASSES_MAPPING[cache_implementation]\n+        cache_cls: Cache = STATIC_CACHE_CLASSES_MAPPING[cache_implementation]\n         requires_cross_attention_cache = (\n             self.config.is_encoder_decoder or model_kwargs.get(\"encoder_outputs\") is not None\n         )\n@@ -1958,12 +1957,7 @@ def _prepare_cache_for_generation(\n             else {}\n         )\n         if generation_config.cache_implementation is not None:\n-            if generation_config.cache_implementation in NEED_SETUP_CACHE_CLASSES_MAPPING:\n-                if generation_config.cache_implementation == \"static\" and not self._can_compile_fullgraph:\n-                    raise ValueError(\n-                        \"This model does not support `cache_implementation='static'`. Please check the following \"\n-                        \"issue: https://github.com/huggingface/transformers/issues/28981\"\n-                    )\n+            if generation_config.cache_implementation in STATIC_CACHE_CLASSES_MAPPING:\n                 model_kwargs[cache_name] = self._get_cache(\n                     cache_implementation=generation_config.cache_implementation,\n                     batch_size=max(generation_config.num_beams, generation_config.num_return_sequences) * batch_size,\n@@ -2115,8 +2109,7 @@ def _valid_auto_compile_criteria(self, model_kwargs: dict, generation_config: Ge\n         using_compilable_cache = (\n             isinstance(model_kwargs.get(\"past_key_values\"), Cache) and model_kwargs[\"past_key_values\"].is_compileable\n         )\n-        # TODO @raushan `self._can_compile_fullgraph` can be removed and inferred from model arch (e.g. MoE doesn't support compile)\n-        can_compile = valid_hardware and using_compilable_cache and self._can_compile_fullgraph\n+        can_compile = valid_hardware and using_compilable_cache\n \n         # Exception 1: Some quantization methods do not support compilation\n         if getattr(self, \"hf_quantizer\", None) is not None:\n@@ -3475,13 +3468,9 @@ def _sample(\n         if compile_forward:\n             os.environ[\"TOKENIZERS_PARALLELISM\"] = \"0\"\n             # If we use FA2 and a static cache, we cannot compile with fullgraph\n-            if self.config._attn_implementation == \"flash_attention_2\" and getattr(\n-                model_kwargs.get(\"past_key_values\"), \"is_compileable\", False\n-            ):\n-                if generation_config.compile_config is None:\n-                    generation_config.compile_config = CompileConfig(fullgraph=False)\n-                # only raise warning if the user passed an explicit compile-config (otherwise, simply change the default without confusing the user)\n-                elif generation_config.compile_config.fullgraph:\n+            if self.config._attn_implementation == \"flash_attention_2\":\n+                # only raise warning if the user passed an explicit compile-config\n+                if generation_config.compile_config is not None and generation_config.compile_config.fullgraph:\n                     logger.warning_once(\n                         \"When using Flash Attention 2 and a static cache, you cannot use the option `CompileConfig(fullgraph=True)` as \"\n                         \"FA2 introduces graph breaks. We overrode the option with `fullgraph=False`.\""
        },
        {
            "sha": "a204a865ba06c96c84efcb173c1c5031f52d4d57",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -460,8 +460,7 @@ class DeepseekV2PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n-    _can_compile_fullgraph = True\n+    _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": DeepseekV2DecoderLayer,"
        },
        {
            "sha": "38508db89dd076068c9d0ca416f52ed2e36e4d92",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -504,6 +504,8 @@ def __init__(self, config: DeepseekV2Config, layer_idx: int):\n \n \n class DeepseekV2PreTrainedModel(LlamaPreTrainedModel):\n+    _can_compile_fullgraph = False\n+\n     def _init_weights(self, module):\n         LlamaPreTrainedModel._init_weights(module)\n         if isinstance(module, DeepseekV2MoEGate):"
        },
        {
            "sha": "8f886779418460b03313cbee53bd8e69d7ae9e2b",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -502,8 +502,7 @@ class DeepseekV3PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n-    _can_compile_fullgraph = True\n+    _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": DeepseekV3DecoderLayer,"
        },
        {
            "sha": "9c5fe07999dd0cfb69d744cff3dcb4a2e912c667",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -340,6 +340,8 @@ def __init__(self, config: DeepseekV3Config, layer_idx: int):\n \n \n class DeepseekV3PreTrainedModel(LlamaPreTrainedModel):\n+    _can_compile_fullgraph = False\n+\n     def _init_weights(self, module):\n         LlamaPreTrainedModel._init_weights(module)\n         if isinstance(module, DeepseekV3TopkRouter):"
        },
        {
            "sha": "67a685a04ed90170747668f6462e8bcddd916ab9",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -422,8 +422,7 @@ class Dots1PreTrainedModel(PreTrainedModel):\n     _supports_flash_attn = True\n     _supports_sdpa = True\n     _supports_flex_attn = True\n-\n-    _can_compile_fullgraph = True\n+    _can_compile_fullgraph = False\n     _supports_attention_backend = True\n     _can_record_outputs = {\n         \"hidden_states\": Dots1DecoderLayer,"
        },
        {
            "sha": "bd01c49fd6ea188fd25942569e490a2455816530",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -1736,6 +1736,9 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n         to verify that the cache length is indeed set correctly and we don't run out of index when slicing the cache.\n         \"\"\"\n         for model_class in self.all_generative_model_classes:\n+            # Here, we should ideally not skip any model, and test them all. However, some old models cannot correctly\n+            # use a static cache because they don't create the causal masks correctly.\n+            # TODO: cyril -> relax this by adding a `_support_static_cache` attribute\n             if not model_class._can_compile_fullgraph:\n                 self.skipTest(reason=\"This model does not support the static cache format\")\n \n@@ -1956,6 +1959,9 @@ def test_generate_with_static_cache(self):\n         \"\"\"\n         set_model_tester_for_less_flaky_test(self)\n         for model_class in self.all_generative_model_classes:\n+            # Here, we should ideally not skip any model, and test them all. However, some old models cannot correctly\n+            # use a static cache because they don't create the causal masks correctly.\n+            # TODO: cyril -> relax this by adding a `_support_static_cache` attribute\n             if not model_class._can_compile_fullgraph:\n                 self.skipTest(reason=\"This model does not support the static cache format\")\n \n@@ -2050,7 +2056,7 @@ def test_generate_with_quant_cache(self):\n     @pytest.mark.generate\n     @pytest.mark.torch_compile_test\n     @require_torch_greater_or_equal(\"2.6\")  # Uses torch.compiler.set_stance\n-    def test_generate_compile_model_forward(self):\n+    def test_generate_compile_model_forward_fullgraph(self):\n         \"\"\"\n         Tests that `.generate` is compatible with torch.compile, keeping the same results. Also confirms that\n         `.forward` called from `.generate` sees no graph breaks or recompilations when compiled.\n@@ -2098,7 +2104,7 @@ def test_generate_compile_model_forward(self):\n             # 3. compilation-specific setup and generation parameterization\n             torch.compiler.reset()  # prevent cached compilation from being used in the test\n             has_defined_cache_implementation = model.generation_config.cache_implementation is not None\n-            compile_config = CompileConfig(dynamic=False)  # Error out on dynamic shapes\n+            compile_config = CompileConfig(fullgraph=True, dynamic=False)  # Error out on dynamic shapes\n             compile_config._compile_all_devices = True  # force compilation (e.g. fast CI, CPU)\n \n             generation_kwargs = {\n@@ -2174,8 +2180,11 @@ def test_generate_compilation_all_outputs(self):\n         In essence, it's the same as `test_greedy_generate_dict_outputs`, but with automatic compilation triggered.\n         \"\"\"\n         for model_class in self.all_generative_model_classes:\n+            # Here, we should ideally not skip any model, and test them all. However, some old models cannot correctly\n+            # use a static cache because they don't create the causal masks correctly.\n+            # TODO: cyril -> relax this by adding a `_support_static_cache` attribute\n             if not model_class._can_compile_fullgraph:\n-                self.skipTest(\"This model doesn't support compilation without graph breaks\")\n+                self.skipTest(reason=\"This model does not support the static cache format\")\n \n             config, inputs_dict = self.prepare_config_and_inputs_for_generate()\n             if self.has_attentions:"
        },
        {
            "sha": "3a518bf959b3515a45dc1834fff6b330106a293b",
            "filename": "tests/models/deepseek_v2/test_modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 19,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v2%2Ftest_modeling_deepseek_v2.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -174,25 +174,6 @@ def _check_past_key_values_for_generate(self, batch_size, decoder_past_key_value\n                 self.assertEqual(layer.keys.shape, expected_key_shape)\n                 self.assertEqual(layer.values.shape, expected_value_shape)\n \n-    @unittest.skip(\"Deepseek-V2 uses MLA which has a special head dim and is not compatible with StaticCache shape\")\n-    @pytest.mark.torch_compile_test\n-    def test_generate_compilation_all_outputs(self):\n-        pass\n-\n-    @unittest.skip(\"Deepseek-V2 uses MLA which has a special head dim and is not compatible with StaticCache shape\")\n-    @pytest.mark.torch_compile_test\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n-    @unittest.skip(\"Deepseek-V2 uses MLA which has a special head dim and is not compatible with StaticCache shape\")\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"Deepseek-V2 uses MLA which has a special head dim and is not compatible with StaticCache shape\")\n-    @pytest.mark.torch_compile_test\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n     @unittest.skip(\"Dynamic control flow in MoE\")\n     @pytest.mark.torch_compile_test\n     def test_torch_compile_for_training(self):"
        },
        {
            "sha": "282ec77c3f63cb87d4d02ecf81e596c6c0870520",
            "filename": "tests/models/deepseek_v3/test_modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_v3%2Ftest_modeling_deepseek_v3.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -285,18 +285,6 @@ def test_contrastive_generate_dict_outputs_use_cache(self):\n     def test_contrastive_generate_low_memory(self):\n         pass\n \n-    @unittest.skip(\n-        \"DeepseekV3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n-    )\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\n-        \"DeepseekV3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n-    )\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n     @unittest.skip(\n         \"DeepseekV3 has HybridCache and doesn't support StaticCache. Though it could, it shouldn't support.\"\n     )\n@@ -307,15 +295,6 @@ def test_generate_continue_from_inputs_embeds(self):\n     def test_beam_search_generate_dict_outputs_use_cache(self):\n         pass\n \n-    @unittest.skip(\"Deepseek-V3 uses MLA so it is not compatible with the standard cache format\")\n-    def test_generate_compilation_all_outputs(self):\n-        pass\n-\n-    @unittest.skip(\"Deepseek-V3 uses MLA so it is not compatible with the standard cache format\")\n-    @pytest.mark.torch_compile_test\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n     @unittest.skip(\"Deepseek-V3 uses MLA so it is not compatible with the standard cache format\")\n     def test_greedy_generate_dict_outputs_use_cache(self):\n         pass"
        },
        {
            "sha": "d0ffff68d37b9f1bc0351f11be842e370817c80d",
            "filename": "tests/models/dots1/test_modeling_dots1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 17,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdots1%2Ftest_modeling_dots1.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -87,23 +87,6 @@ class Dots1ModelTest(CausalLMModelTest, unittest.TestCase):\n     test_pruning = False\n     model_tester_class = Dots1ModelTester\n \n-    @unittest.skip(\"dots.llm1's moe is not compatible `token_indices, weight_indices = torch.where(mask)`.\")\n-    def test_generate_with_static_cache(self):\n-        pass\n-\n-    @unittest.skip(\"dots.llm1's moe is not compatible `token_indices, weight_indices = torch.where(mask)`.\")\n-    def test_generate_compilation_all_outputs(self):\n-        pass\n-\n-    @unittest.skip(\"dots.llm1's moe is not compatible `token_indices, weight_indices = torch.where(mask)`\")\n-    @pytest.mark.torch_compile_test\n-    def test_generate_compile_model_forward(self):\n-        pass\n-\n-    @unittest.skip(\"dots.llm1's moe is not compatible token_indices, weight_indices = torch.where(mask).\")\n-    def test_generate_from_inputs_embeds_with_static_cache(self):\n-        pass\n-\n     @require_flash_attn\n     @require_torch_gpu\n     @pytest.mark.flash_attn_test"
        },
        {
            "sha": "8c184531f33ebf4b08e1afe4fc6dd7144f268b83",
            "filename": "tests/models/janus/test_modeling_janus.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_modeling_janus.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -296,7 +296,7 @@ def check_training_gradient_checkpointing(self, gradient_checkpointing_kwargs=No\n \n     @unittest.skip(\"There are recompilations in Janus\")  # TODO (joao, raushan): fix me\n     @pytest.mark.torch_compile_test\n-    def test_generate_compile_model_forward(self):\n+    def test_generate_compile_model_forward_fullgraph(self):\n         pass\n \n "
        },
        {
            "sha": "c775a3f4b9d201b1d33252a795e3700946e90a4f",
            "filename": "tests/models/paligemma2/test_modeling_paligemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 7,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma2%2Ftest_modeling_paligemma2.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -25,7 +25,6 @@\n     is_torch_available,\n )\n from transformers.testing_utils import (\n-    is_flaky,\n     require_torch,\n     torch_device,\n )\n@@ -317,12 +316,6 @@ def test_contrastive_generate_low_memory(self):\n     def test_generate_with_static_cache(self):\n         pass\n \n-    @pytest.mark.generate\n-    @pytest.mark.torch_compile_test\n-    @is_flaky\n-    def test_generate_compile_model_forward(self):\n-        super().test_generate_compile_model_forward()\n-\n     @unittest.skip(\"Paligemma position ids are 1 indexed\")\n     def test_eager_padding_matches_padding_free_with_position_ids(self):\n         pass"
        },
        {
            "sha": "e9ac04fce5b0dd5a91ee42186c84d8f9701d831b",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -255,7 +255,7 @@ def test_generate_compilation_all_outputs(self):\n         reason=\"Supported only for text-only inputs (otherwise dynamic control flows for multimodal inputs)\"\n     )\n     @pytest.mark.torch_compile_test\n-    def test_generate_compile_model_forward(self):\n+    def test_generate_compile_model_forward_fullgraph(self):\n         pass\n \n     @parameterized.expand([(\"random\",), (\"same\",)])"
        },
        {
            "sha": "fef502751f05cc44c89a21d7fcab1beca06bf0c0",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -447,7 +447,7 @@ def test_generate_from_inputs_embeds_with_static_cache(self):\n     # passing, fix me\n     @unittest.skip(\"Cannot handle 4D attention mask\")\n     @pytest.mark.torch_compile_test\n-    def test_generate_compile_model_forward(self):\n+    def test_generate_compile_model_forward_fullgraph(self):\n         pass\n \n     @unittest.skip(\"Cannot handle 4D attention mask\")"
        },
        {
            "sha": "3c67e9300efea929324fef8298a757eb7393eda5",
            "filename": "tests/models/whisper/test_modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/a2e76b908b1fd68a71117be1bcc9389869c6f445/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fwhisper%2Ftest_modeling_whisper.py?ref=a2e76b908b1fd68a71117be1bcc9389869c6f445",
            "patch": "@@ -1421,7 +1421,7 @@ def test_labels_sequence_max_length_error_after_changing_config(self):\n     # TODO (joao, eustache): fix me :) The model is not returning a `Cache` by default\n     @unittest.skip(reason=\"Whisper's custom generate is not consistent regarding the cache return types\")\n     @pytest.mark.torch_compile_test\n-    def test_generate_compile_model_forward(self):\n+    def test_generate_compile_model_forward_fullgraph(self):\n         pass\n \n     # TODO (joao, eustache): fix me :)"
        }
    ],
    "stats": {
        "total": 139,
        "additions": 38,
        "deletions": 101
    }
}