{
    "author": "thisisiron",
    "message": "Add Swin2SR ImageProcessorFast (#37169)\n\n* Add fast image processor support for Swin2SR\n\n* Add Swin2SR tests of fast image processing\n\n* Update docs and remove unnecessary test func\n\n* Fix docstring formatting\n\n* Skip fast vs slow processing test\n\n---------\n\nCo-authored-by: Yoni Gozlan <74535834+yonigozlan@users.noreply.github.com>",
    "sha": "5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace",
    "files": [
        {
            "sha": "3ea713fdc788d5aef2cb53243d4e3bea6b8f77d6",
            "filename": "docs/source/en/model_doc/swin2sr.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin2sr.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin2sr.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fswin2sr.md?ref=5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace",
            "patch": "@@ -50,6 +50,11 @@ A demo Space for image super-resolution with SwinSR can be found [here](https://\n [[autodoc]] Swin2SRImageProcessor\n     - preprocess\n \n+## Swin2SRImageProcessorFast\n+\n+[[autodoc]] Swin2SRImageProcessorFast\n+    - preprocess\n+\n ## Swin2SRConfig\n \n [[autodoc]] Swin2SRConfig"
        },
        {
            "sha": "41d446cf4e5548069aadd22e2cb92e5b4842dc44",
            "filename": "src/transformers/models/auto/image_processing_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fimage_processing_auto.py?ref=5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace",
            "patch": "@@ -150,7 +150,7 @@\n             (\"superglue\", (\"SuperGlueImageProcessor\",)),\n             (\"swiftformer\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"swin\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n-            (\"swin2sr\", (\"Swin2SRImageProcessor\",)),\n+            (\"swin2sr\", (\"Swin2SRImageProcessor\", \"Swin2SRImageProcessorFast\")),\n             (\"swinv2\", (\"ViTImageProcessor\", \"ViTImageProcessorFast\")),\n             (\"table-transformer\", (\"DetrImageProcessor\",)),\n             (\"timesformer\", (\"VideoMAEImageProcessor\",)),"
        },
        {
            "sha": "082570230cc84b99e76aa5b4943c2f9013efc7b0",
            "filename": "src/transformers/models/swin2sr/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace/src%2Ftransformers%2Fmodels%2Fswin2sr%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace/src%2Ftransformers%2Fmodels%2Fswin2sr%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2F__init__.py?ref=5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace",
            "patch": "@@ -20,6 +20,7 @@\n if TYPE_CHECKING:\n     from .configuration_swin2sr import *\n     from .image_processing_swin2sr import *\n+    from .image_processing_swin2sr_fast import *\n     from .modeling_swin2sr import *\n else:\n     import sys"
        },
        {
            "sha": "9dd056af1e8951fa7faf422355977684ea95412e",
            "filename": "src/transformers/models/swin2sr/image_processing_swin2sr_fast.py",
            "status": "added",
            "additions": 138,
            "deletions": 0,
            "changes": 138,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswin2sr%2Fimage_processing_swin2sr_fast.py?ref=5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace",
            "patch": "@@ -0,0 +1,138 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Fast Image processor class for Swin2SR.\"\"\"\n+\n+from typing import List, Optional, Union\n+\n+from ...image_processing_utils import (\n+    BatchFeature,\n+    ChannelDimension,\n+    get_image_size,\n+)\n+from ...image_processing_utils_fast import (\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+    BaseImageProcessorFast,\n+    DefaultFastImageProcessorKwargs,\n+    group_images_by_shape,\n+    reorder_images,\n+)\n+from ...image_utils import ImageInput\n+from ...processing_utils import Unpack\n+from ...utils import (\n+    TensorType,\n+    add_start_docstrings,\n+    is_torch_available,\n+    is_torchvision_available,\n+    is_torchvision_v2_available,\n+)\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+if is_torchvision_available():\n+    if is_torchvision_v2_available():\n+        from torchvision.transforms.v2 import functional as F\n+    else:\n+        from torchvision.transforms import functional as F\n+\n+\n+class Swin2SRFastImageProcessorKwargs(DefaultFastImageProcessorKwargs):\n+    do_pad: Optional[bool]\n+    pad_size: Optional[int]\n+\n+\n+@add_start_docstrings(\n+    \"Constructs a fast Swin2SR image processor.\",\n+    BASE_IMAGE_PROCESSOR_FAST_DOCSTRING,\n+    \"\"\"\n+        do_pad (`bool`, *optional*, defaults to `True`):\n+            Whether to pad the image to make the height and width divisible by `window_size`.\n+        pad_size (`int`, *optional*, defaults to `8`):\n+            The size of the sliding window for the local attention.\n+    \"\"\",\n+)\n+class Swin2SRImageProcessorFast(BaseImageProcessorFast):\n+    do_rescale = True\n+    rescale_factor = 1 / 255\n+    do_pad = True\n+    pad_size = 8\n+    valid_kwargs = Swin2SRFastImageProcessorKwargs\n+\n+    def __init__(self, **kwargs: Unpack[Swin2SRFastImageProcessorKwargs]):\n+        super().__init__(**kwargs)\n+\n+    @add_start_docstrings(\n+        BASE_IMAGE_PROCESSOR_FAST_DOCSTRING_PREPROCESS,\n+        \"\"\"\n+            do_pad (`bool`, *optional*, defaults to `True`):\n+                Whether to pad the image to make the height and width divisible by `window_size`.\n+            pad_size (`int`, *optional*, defaults to `8`):\n+                The size of the sliding window for the local attention.\n+        \"\"\",\n+    )\n+    def preprocess(self, images: ImageInput, **kwargs: Unpack[Swin2SRFastImageProcessorKwargs]) -> BatchFeature:\n+        return super().preprocess(images, **kwargs)\n+\n+    def pad(self, images: \"torch.Tensor\", size: int) -> \"torch.Tensor\":\n+        \"\"\"\n+        Pad an image to make the height and width divisible by `size`.\n+\n+        Args:\n+            images (`torch.Tensor`):\n+                Images to pad.\n+            size (`int`):\n+                The size to make the height and width divisible by.\n+\n+        Returns:\n+            `torch.Tensor`: The padded images.\n+        \"\"\"\n+        height, width = get_image_size(images, ChannelDimension.FIRST)\n+        pad_height = (height // size + 1) * size - height\n+        pad_width = (width // size + 1) * size - width\n+\n+        return F.pad(\n+            images,\n+            (0, 0, pad_width, pad_height),\n+            padding_mode=\"symmetric\",\n+        )\n+\n+    def _preprocess(\n+        self,\n+        images: List[\"torch.Tensor\"],\n+        do_rescale: bool,\n+        rescale_factor: float,\n+        do_pad: bool,\n+        pad_size: int,\n+        return_tensors: Optional[Union[str, TensorType]],\n+        interpolation: Optional[\"F.InterpolationMode\"],\n+        **kwargs,\n+    ) -> BatchFeature:\n+        grouped_images, grouped_images_index = group_images_by_shape(images)\n+        processed_image_grouped = {}\n+        for shape, stacked_images in grouped_images.items():\n+            if do_rescale:\n+                stacked_images = self.rescale(stacked_images, scale=rescale_factor)\n+            if do_pad:\n+                stacked_images = self.pad(stacked_images, size=pad_size)\n+            processed_image_grouped[shape] = stacked_images\n+        processed_images = reorder_images(processed_image_grouped, grouped_images_index)\n+        processed_images = torch.stack(processed_images, dim=0) if return_tensors else processed_images\n+\n+        return BatchFeature(data={\"pixel_values\": processed_images}, tensor_type=return_tensors)\n+\n+\n+__all__ = [\"Swin2SRImageProcessorFast\"]"
        },
        {
            "sha": "f4e70d1b0e486f6b38c920ceeb057815748223e8",
            "filename": "tests/models/swin2sr/test_image_processing_swin2sr.py",
            "status": "modified",
            "additions": 26,
            "deletions": 6,
            "changes": 32,
            "blob_url": "https://github.com/huggingface/transformers/blob/5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fswin2sr%2Ftest_image_processing_swin2sr.py?ref=5c47d08b0d6835b8d8fc1c06d9a1bc71f6e78ace",
            "patch": "@@ -18,7 +18,7 @@\n import numpy as np\n \n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_torch_available, is_vision_available\n+from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n \n@@ -30,6 +30,9 @@\n     from PIL import Image\n \n     from transformers import Swin2SRImageProcessor\n+\n+    if is_torchvision_available():\n+        from transformers import Swin2SRImageProcessorFast\n     from transformers.image_transforms import get_image_size\n \n \n@@ -97,6 +100,7 @@ def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=F\n @require_vision\n class Swin2SRImageProcessingTest(ImageProcessingTestMixin, unittest.TestCase):\n     image_processing_class = Swin2SRImageProcessor if is_vision_available() else None\n+    fast_image_processing_class = Swin2SRImageProcessorFast if is_torchvision_available() else None\n \n     def setUp(self):\n         super().setUp()\n@@ -107,11 +111,12 @@ def image_processor_dict(self):\n         return self.image_processor_tester.prepare_image_processor_dict()\n \n     def test_image_processor_properties(self):\n-        image_processor = self.image_processing_class(**self.image_processor_dict)\n-        self.assertTrue(hasattr(image_processor, \"do_rescale\"))\n-        self.assertTrue(hasattr(image_processor, \"rescale_factor\"))\n-        self.assertTrue(hasattr(image_processor, \"do_pad\"))\n-        self.assertTrue(hasattr(image_processor, \"pad_size\"))\n+        for image_processing_class in self.image_processor_list:\n+            image_processing = image_processing_class(**self.image_processor_dict)\n+            self.assertTrue(hasattr(image_processing, \"do_rescale\"))\n+            self.assertTrue(hasattr(image_processing, \"rescale_factor\"))\n+            self.assertTrue(hasattr(image_processing, \"do_pad\"))\n+            self.assertTrue(hasattr(image_processing, \"pad_size\"))\n \n     def calculate_expected_size(self, image):\n         old_height, old_width = get_image_size(image)\n@@ -181,3 +186,18 @@ def test_call_pytorch(self):\n         encoded_images = image_processing(image_inputs[0], return_tensors=\"pt\").pixel_values\n         expected_output_image_shape = self.image_processor_tester.expected_output_image_shape([image_inputs[0]])\n         self.assertEqual(tuple(encoded_images.shape), (1, *expected_output_image_shape))\n+\n+    @unittest.skip(reason=\"No speed gain on CPU due to minimal processing.\")\n+    def test_fast_is_faster_than_slow(self):\n+        pass\n+\n+    def test_slow_fast_equivalence_batched(self):\n+        image_inputs = self.image_processor_tester.prepare_image_inputs(equal_resolution=True, torchify=True)\n+\n+        image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n+        image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n+\n+        encoded_slow = image_processor_slow(image_inputs, return_tensors=\"pt\").pixel_values\n+        encoded_fast = image_processor_fast(image_inputs, return_tensors=\"pt\").pixel_values\n+\n+        self.assertTrue(torch.allclose(encoded_slow, encoded_fast, atol=1e-1))"
        }
    ],
    "stats": {
        "total": 178,
        "additions": 171,
        "deletions": 7
    }
}