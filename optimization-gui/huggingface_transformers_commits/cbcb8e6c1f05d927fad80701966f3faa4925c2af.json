{
    "author": "cassiasamp",
    "message": "updated mistral3 model card (#39531)\n\n* updated mistral3 model card (#1)\n\n* updated mistral3 model card\n\n* applying suggestions from code review\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* made all changes to mistral3.md\n\n* adding space between paragraphs in docs/source/en/model_doc/mistral3.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* removing duplicate in mistral3.md\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* adding 4 backticks to preserve formatting\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "cbcb8e6c1f05d927fad80701966f3faa4925c2af",
    "files": [
        {
            "sha": "74805551dbbe5a9dfe976aee21d060bc60d03ef6",
            "filename": "docs/source/en/model_doc/mistral3.md",
            "status": "modified",
            "additions": 177,
            "deletions": 173,
            "changes": 350,
            "blob_url": "https://github.com/huggingface/transformers/blob/cbcb8e6c1f05d927fad80701966f3faa4925c2af/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/cbcb8e6c1f05d927fad80701966f3faa4925c2af/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fmistral3.md?ref=cbcb8e6c1f05d927fad80701966f3faa4925c2af",
            "patch": "@@ -13,116 +13,125 @@ specific language governing permissions and limitations under the License.\n rendered properly in your Markdown viewer.\n \n -->\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+           <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&amp;logo=pytorch&amp;logoColor=white\">\n+    </div>\n+</div>\n \n-# Mistral3\n+# Mistral 3\n \n-## Overview\n+[Mistral 3](https://mistral.ai/news/mistral-small-3) is a latency optimized model with a lot fewer layers to reduce the time per forward pass. This model adds vision understanding and supports long context lengths of up to 128K tokens without compromising performance.\n \n-Building upon Mistral Small 3 (2501), Mistral Small 3.1 (2503) adds state-of-the-art vision understanding and enhances long context capabilities up to 128k tokens without compromising text performance. With 24 billion parameters, this model achieves top-tier capabilities in both text and vision tasks.\n+You can find the original Mistral 3 checkpoints under the [Mistral AI](https://huggingface.co/mistralai/models?search=mistral-small-3) organization.\n \n-It is ideal for:\n-- Fast-response conversational agents.\n-- Low-latency function calling.\n-- Subject matter experts via fine-tuning.\n-- Local inference for hobbyists and organizations handling sensitive data.\n-- Programming and math reasoning.\n-- Long document understanding.\n-- Visual understanding.\n \n-This model was contributed by [cyrilvallez](https://huggingface.co/cyrilvallez) and [yonigozlan](https://huggingface.co/yonigozlan).\n+> [!TIP]\n+> This model was contributed by [cyrilvallez](https://huggingface.co/cyrilvallez) and [yonigozlan](https://huggingface.co/yonigozlan).\n+> Click on the Mistral3 models in the right sidebar for more examples of how to apply Mistral3 to different tasks.\n \n-The original code can be found [here](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/pixtral.py) and [here](https://github.com/mistralai/mistral-common).\n+The example below demonstrates how to generate text for an image with [`Pipeline`] and the [`AutoModel`] class.\n \n-## Usage example\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n-### Inference with Pipeline\n+```py\n+import torch\n+from transformers import pipeline\n \n-Here is how you can use the `image-text-to-text` pipeline to perform inference with the `Mistral3` models in just a few lines of code:\n-```python\n->>> from transformers import pipeline\n+messages = [\n+    {\"role\": \"user\",\n+        \"content\":[\n+            {\"type\": \"image\",\n+            \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",},\n+            {\"type\": \"text\", \"text\": \"Describe this image.\"}\n+        ,]\n+    ,}\n+,]\n \n->>> messages = [\n-...     {\n-...         \"role\": \"user\",\n-...         \"content\": [\n-...             {\n-...                 \"type\": \"image\",\n-...                 \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",\n-...             },\n-...             {\"type\": \"text\", \"text\": \"Describe this image.\"},\n-...         ],\n-...     },\n-... ]\n+pipeline = pipeline(\n+    task=\"image-text-to-text\", \n+    model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", \n+    torch_dtype=torch.bfloat16,\n+    device=0\n+)\n+outputs = pipeline(text=messages, max_new_tokens=50, return_full_text=False)\n \n->>> pipe = pipeline(\"image-text-to-text\", model=\"mistralai/Mistral-Small-3.1-24B-Instruct-2503\", torch_dtype=torch.bfloat16)\n->>> outputs = pipe(text=messages, max_new_tokens=50, return_full_text=False)\n->>> outputs[0][\"generated_text\"]\n+outputs[0][\"generated_text\"]\n 'The image depicts a vibrant and lush garden scene featuring a variety of wildflowers and plants. The central focus is on a large, pinkish-purple flower, likely a Greater Celandine (Chelidonium majus), with a'\n ```\n-### Inference on a single image\n-\n-This example demonstrates how to perform inference on a single image with the Mistral3 models using chat templates.\n-\n-```python\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText\n->>> import torch\n-\n->>> torch_device = \"cuda\"\n->>> model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n->>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n->>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n-\n->>> messages = [\n-...     {\n-...         \"role\": \"user\",\n-...         \"content\": [\n-...             {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n-...             {\"type\": \"text\", \"text\": \"Describe this image\"},\n-...         ],\n-...     }\n-... ]\n-\n->>> inputs = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n-\n->>> generate_ids = model.generate(**inputs, max_new_tokens=20)\n->>> decoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n-\n->>> decoded_output\n-\"The image depicts two cats lying on a pink blanket. The larger cat, which appears to be an\"...\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoProcessor, AutoModelForImageTextToText \n+\n+torch_device = \"cuda\"\n+model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n+processor = AutoProcessor.from_pretrained(model_checkpoint)\n+model = AutoModelForImageTextToText.from_pretrained(\n+    model_checkpoint, \n+    device_map=torch_device, \n+    torch_dtype=torch.bfloat16\n+)\n+\n+messages = [\n+    {\"role\": \"user\",\n+        \"content\":[\n+            {\"type\": \"image\",\n+            \"image\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg\",},\n+            {\"type\": \"text\", \"text\": \"Describe this image.\"}\n+        ,]\n+    ,}\n+,]\n+\n+inputs = processor.apply_chat_template(\n+    messages, \n+    add_generation_prompt=True, \n+    tokenize=True, return_dict=True, \n+    return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+generate_ids = model.generate(**inputs, max_new_tokens=20)\n+decoded_output = processor.decode(generate_ids[0, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)\n+\n+decoded_output\n+'The image depicts a vibrant and lush garden scene featuring a variety of wildflowers and plants. The central focus is on a large, pinkish-purple flower, likely a Greater Celandine (Chelidonium majus), with a'\n ```\n+</hfoption>\n+</hfoptions>\n \n-### Text-only generation\n-This example shows how to generate text using the Mistral3 model without providing any image input.\n+## Notes \n \n+- Mistral 3 supports text-only generation. \n+```py \n+from transformers import AutoProcessor, AutoModelForImageTextToText\n+import torch\n \n-````python\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText\n->>> import torch\n+torch_device = \"cuda\"\n+model_checkpoint = \".mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n+processor = AutoProcessor.from_pretrained(model_checkpoint)\n+model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n \n->>> torch_device = \"cuda\"\n->>> model_checkpoint = \".mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n->>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n->>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+SYSTEM_PROMPT = \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n+user_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\n \n->>> SYSTEM_PROMPT = \"You are a conversational agent that always answers straight to the point, always end your accurate response with an ASCII drawing of a cat.\"\n->>> user_prompt = \"Give me 5 non-formal ways to say 'See you later' in French.\"\n+messages = [\n+    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n+    {\"role\": \"user\", \"content\": user_prompt},\n+]\n \n->>> messages = [\n-...    {\"role\": \"system\", \"content\": SYSTEM_PROMPT},\n-...    {\"role\": \"user\", \"content\": user_prompt},\n-... ]\n+text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n+inputs = processor(text=text, return_tensors=\"pt\").to(0, dtype=torch.float16)\n+generate_ids = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n+decoded_output = processor.batch_decode(generate_ids[:, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)[0]\n \n->>> text = processor.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n->>> inputs = processor(text=text, return_tensors=\"pt\").to(0, dtype=torch.float16)\n->>> generate_ids = model.generate(**inputs, max_new_tokens=50, do_sample=False)\n->>> decoded_output = processor.batch_decode(generate_ids[:, inputs[\"input_ids\"].shape[1] :], skip_special_tokens=True)[0]\n-\n->>> print(decoded_output)\n+print(decoded_output)\n \"1. À plus tard!\n-2. Salut, à plus!\n-3. À toute!\n-4. À la prochaine!\n-5. Je me casse, à plus!\n+ 2. Salut, à plus!\n+ 3. À toute!\n+ 4. À la prochaine!\n+ 5. Je me casse, à plus!\n \n ```\n  /\\_/\\\n@@ -131,98 +140,93 @@ This example shows how to generate text using the Mistral3 model without providi\n ```\"\n ````\n \n-### Batched image and text inputs\n-Mistral3 models also support batched image and text inputs.\n-\n-```python\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText\n->>> import torch\n-\n->>> torch_device = \"cuda\"\n->>> model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n->>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n->>> model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n-\n->>> messages = [\n-...     [\n-...         {\n-...             \"role\": \"user\",\n-...             \"content\": [\n-...                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n-...                 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n-...             ],\n-...         },\n-...     ],\n-...     [\n-...         {\n-...             \"role\": \"user\",\n-...             \"content\": [\n-...                 {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n-...                 {\"type\": \"text\", \"text\": \"Describe this image\"},\n-...             ],\n-...         },\n-...     ],\n-... ]\n-\n-\n->>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n-\n->>> output = model.generate(**inputs, max_new_tokens=25)\n-\n->>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n->>> decoded_outputs\n+- Mistral 3 accepts batched image and text inputs. \n+```py\n+from transformers import AutoProcessor, AutoModelForImageTextToText\n+import torch\n+\n+torch_device = \"cuda\"\n+model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n+processor = AutoProcessor.from_pretrained(model_checkpoint)\n+model = AutoModelForImageTextToText.from_pretrained(model_checkpoint, device_map=torch_device, torch_dtype=torch.bfloat16)\n+\n+messages = [\n+     [\n+         {\n+             \"role\": \"user\",\n+             \"content\": [\n+                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+                 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+             ],\n+         },\n+     ],\n+     [\n+         {\n+             \"role\": \"user\",\n+             \"content\": [\n+                 {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                 {\"type\": \"text\", \"text\": \"Describe this image\"},\n+             ],\n+         },\n+     ],\n+ ]\n+\n+\n+ inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+ output = model.generate(**inputs, max_new_tokens=25)\n+\n+ decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n+ decoded_outputs\n [\"Write a haiku for this imageCalm waters reflect\\nWhispers of the forest's breath\\nPeace on wooden path\"\n , \"Describe this imageThe image depicts a vibrant street scene in what appears to be a Chinatown district. The focal point is a traditional Chinese\"]\n ```\n \n-### Batched multi-image input and quantization with BitsAndBytes\n-This implementation of the Mistral3 models supports batched text-images inputs with different number of images for each text.\n-This example also how to use `BitsAndBytes` to load the model in 4bit quantization.\n-\n-```python\n->>> from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n->>> import torch\n-\n->>> torch_device = \"cuda\"\n->>> model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n->>> processor = AutoProcessor.from_pretrained(model_checkpoint)\n->>> quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n->>> model = AutoModelForImageTextToText.from_pretrained(\n-...     model_checkpoint, quantization_config=quantization_config\n-... )\n-\n->>> messages = [\n-...     [\n-...         {\n-...             \"role\": \"user\",\n-...             \"content\": [\n-...                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n-...                 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n-...             ],\n-...         },\n-...     ],\n-...     [\n-...         {\n-...             \"role\": \"user\",\n-...             \"content\": [\n-...                 {\"type\": \"image\", \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"},\n-...                 {\"type\": \"image\", \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"},\n-...                 {\"type\": \"text\", \"text\": \"These images depict two different landmarks. Can you identify them?\"},\n-...             ],\n-...         },\n-...     ],\n->>> ]\n-\n->>> inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n-\n->>> output = model.generate(**inputs, max_new_tokens=25)\n-\n->>> decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n->>> decoded_outputs\n+- Mistral 3 also supported batched image and text inputs with a different number of images for each text. The example below quantizes the model with bitsandbytes. \n+\n+```py \n+from transformers import AutoProcessor, AutoModelForImageTextToText, BitsAndBytesConfig\n+import torch\n+\n+torch_device = \"cuda\"\n+model_checkpoint = \"mistralai/Mistral-Small-3.1-24B-Instruct-2503\"\n+processor = AutoProcessor.from_pretrained(model_checkpoint)\n+quantization_config = BitsAndBytesConfig(load_in_4bit=True)\n+model = AutoModelForImageTextToText.from_pretrained(\n+     model_checkpoint, quantization_config=quantization_config\n+ )\n+\n+messages = [\n+     [\n+         {\n+             \"role\": \"user\",\n+             \"content\": [\n+                 {\"type\": \"image\", \"url\": \"https://llava-vl.github.io/static/images/view.jpg\"},\n+                 {\"type\": \"text\", \"text\": \"Write a haiku for this image\"},\n+             ],\n+         },\n+     ],\n+     [\n+         {\n+             \"role\": \"user\",\n+             \"content\": [\n+                 {\"type\": \"image\", \"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"},\n+                 {\"type\": \"image\", \"url\": \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"},\n+                 {\"type\": \"text\", \"text\": \"These images depict two different landmarks. Can you identify them?\"},\n+             ],\n+         },\n+     ],\n+ ]\n+\n+ inputs = processor.apply_chat_template(messages, padding=True, add_generation_prompt=True, tokenize=True, return_dict=True, return_tensors=\"pt\").to(model.device, dtype=torch.bfloat16)\n+\n+ output = model.generate(**inputs, max_new_tokens=25)\n+\n+ decoded_outputs = processor.batch_decode(output, skip_special_tokens=True)\n+ decoded_outputs\n [\"Write a haiku for this imageSure, here is a haiku inspired by the image:\\n\\nCalm lake's wooden path\\nSilent forest stands guard\\n\", \"These images depict two different landmarks. Can you identify them? Certainly! The images depict two iconic landmarks:\\n\\n1. The first image shows the Statue of Liberty in New York City.\"]\n ```\n \n-\n ## Mistral3Config\n \n [[autodoc]] Mistral3Config"
        }
    ],
    "stats": {
        "total": 350,
        "additions": 177,
        "deletions": 173
    }
}