{
    "author": "SunMarc",
    "message": "[v5] More Training Args cleaning  (#41131)\n\nclean",
    "sha": "dded9fd1122a504be7b6be254f39f6cac220ce3f",
    "files": [
        {
            "sha": "44f5a75eda0a690ac289b27a59781c457e9ad43a",
            "filename": "examples/legacy/seq2seq/finetune_trainer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/dded9fd1122a504be7b6be254f39f6cac220ce3f/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dded9fd1122a504be7b6be254f39f6cac220ce3f/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Flegacy%2Fseq2seq%2Ffinetune_trainer.py?ref=dded9fd1122a504be7b6be254f39f6cac220ce3f",
            "patch": "@@ -32,7 +32,7 @@\n     MBartTokenizerFast,\n     set_seed,\n )\n-from transformers.trainer_utils import EvaluationStrategy, is_main_process\n+from transformers.trainer_utils import is_main_process\n from transformers.training_args import ParallelMode\n from utils import (\n     Seq2SeqDataCollator,\n@@ -271,7 +271,7 @@ def main():\n             max_source_length=data_args.max_source_length,\n             prefix=model.config.prefix or \"\",\n         )\n-        if training_args.do_eval or training_args.eval_strategy != EvaluationStrategy.NO\n+        if training_args.do_eval\n         else None\n     )\n     test_dataset = ("
        },
        {
            "sha": "331b0b61d74f279e7245fca6a4ab8628f3238c5f",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/dded9fd1122a504be7b6be254f39f6cac220ce3f/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dded9fd1122a504be7b6be254f39f6cac220ce3f/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=dded9fd1122a504be7b6be254f39f6cac220ce3f",
            "patch": "@@ -224,12 +224,6 @@ class SaveStrategy(ExplicitEnum):\n     BEST = \"best\"\n \n \n-class EvaluationStrategy(ExplicitEnum):\n-    NO = \"no\"\n-    STEPS = \"steps\"\n-    EPOCH = \"epoch\"\n-\n-\n class HubStrategy(ExplicitEnum):\n     END = \"end\"\n     EVERY_SAVE = \"every_save\""
        },
        {
            "sha": "38ed037d0d61a4831f54e93e860fcccc699b07e5",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/dded9fd1122a504be7b6be254f39f6cac220ce3f/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/dded9fd1122a504be7b6be254f39f6cac220ce3f/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=dded9fd1122a504be7b6be254f39f6cac220ce3f",
            "patch": "@@ -25,7 +25,6 @@\n \n from .debug_utils import DebugOption\n from .trainer_utils import (\n-    EvaluationStrategy,\n     FSDPOption,\n     HubStrategy,\n     IntervalStrategy,\n@@ -1481,15 +1480,6 @@ def __post_init__(self):\n         if self.disable_tqdm is None:\n             self.disable_tqdm = logger.getEffectiveLevel() > logging.WARN\n \n-        if isinstance(self.eval_strategy, EvaluationStrategy):\n-            warnings.warn(\n-                \"using `EvaluationStrategy` for `eval_strategy` is deprecated and will be removed in version 5\"\n-                \" of ðŸ¤— Transformers. Use `IntervalStrategy` instead\",\n-                FutureWarning,\n-            )\n-            # Go back to the underlying string or we won't be able to instantiate `IntervalStrategy` on it.\n-            self.eval_strategy = self.eval_strategy.value\n-\n         self.eval_strategy = IntervalStrategy(self.eval_strategy)\n         self.logging_strategy = IntervalStrategy(self.logging_strategy)\n         self.save_strategy = SaveStrategy(self.save_strategy)"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 2,
        "deletions": 18
    }
}