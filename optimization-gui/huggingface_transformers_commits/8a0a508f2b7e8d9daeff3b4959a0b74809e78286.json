{
    "author": "ariG23498",
    "message": "Aligning modling code for GPT2 to work with vLLM (fallback) (#36934)\n\n* aligning for vllm\n\n* using input shape rather than attn outputs\n\n* remove demo\n\n* revert Conv1D\n\n* style\n\n* style\n\n* Update src/transformers/models/gpt2/modeling_gpt2.py\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\n\n* fix copies\n\n* Apply suggestions from code review\n\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>\n\n* adding docs about vllm\n\n* chore: style\n\n---------\n\nCo-authored-by: Arthur <48595927+ArthurZucker@users.noreply.github.com>\nCo-authored-by: Harry Mellor <19981378+hmellor@users.noreply.github.com>",
    "sha": "8a0a508f2b7e8d9daeff3b4959a0b74809e78286",
    "files": [
        {
            "sha": "d67aabfa832b226710b0a4bc19893e19033a1a77",
            "filename": "docs/source/en/model_doc/gpt2.md",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a0a508f2b7e8d9daeff3b4959a0b74809e78286/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a0a508f2b7e8d9daeff3b4959a0b74809e78286/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fgpt2.md?ref=8a0a508f2b7e8d9daeff3b4959a0b74809e78286",
            "patch": "@@ -73,6 +73,12 @@ echo -e \"Hello, I'm a language model\" | transformers run --task text-generation\n </hfoption>\n </hfoptions>\n \n+One can also serve the model using vLLM with the `transformers backend`.\n+\n+```\n+vllm serve openai-community/gpt2 --model-imp transformers\n+```\n+\n Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to 4-bits."
        },
        {
            "sha": "ab2a3024052b019a170c7fb33be87aab9bb22cc6",
            "filename": "src/transformers/models/decision_transformer/modeling_decision_transformer.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a0a508f2b7e8d9daeff3b4959a0b74809e78286/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a0a508f2b7e8d9daeff3b4959a0b74809e78286/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdecision_transformer%2Fmodeling_decision_transformer.py?ref=8a0a508f2b7e8d9daeff3b4959a0b74809e78286",
            "patch": "@@ -396,6 +396,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n         residual = hidden_states\n         hidden_states = self.ln_1(hidden_states)\n@@ -407,6 +408,7 @@ def forward(\n             head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            **kwargs,\n         )\n         # residual connection\n         hidden_states = attn_output + residual"
        },
        {
            "sha": "0af4d99065520b2f010c63d21cac980820e44426",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8a0a508f2b7e8d9daeff3b4959a0b74809e78286/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8a0a508f2b7e8d9daeff3b4959a0b74809e78286/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=8a0a508f2b7e8d9daeff3b4959a0b74809e78286",
            "patch": "@@ -401,6 +401,7 @@ def forward(\n         encoder_attention_mask: Optional[torch.FloatTensor] = None,\n         use_cache: Optional[bool] = False,\n         output_attentions: Optional[bool] = False,\n+        **kwargs,\n     ) -> Union[Tuple[torch.Tensor], Optional[Tuple[torch.Tensor, Tuple[torch.FloatTensor, ...]]]]:\n         residual = hidden_states\n         hidden_states = self.ln_1(hidden_states)\n@@ -412,6 +413,7 @@ def forward(\n             head_mask=head_mask,\n             use_cache=use_cache,\n             output_attentions=output_attentions,\n+            **kwargs,\n         )\n         # residual connection\n         hidden_states = attn_output + residual\n@@ -567,6 +569,7 @@ class GPT2PreTrainedModel(PreTrainedModel):\n     _skip_keys_device_placement = \"past_key_values\"\n     _supports_flash_attn_2 = True\n     _supports_sdpa = True\n+    _supports_attention_backend = True\n     _supports_cache_class = True\n     _supports_static_cache = True\n \n@@ -903,6 +906,7 @@ def forward(\n         output_attentions: Optional[bool] = None,\n         output_hidden_states: Optional[bool] = None,\n         return_dict: Optional[bool] = None,\n+        **kwargs,\n     ) -> Union[Tuple, BaseModelOutputWithPastAndCrossAttentions]:\n         output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n         output_hidden_states = (\n@@ -1046,6 +1050,7 @@ def forward(\n                     encoder_attention_mask=encoder_attention_mask,\n                     use_cache=use_cache,\n                     output_attentions=output_attentions,\n+                    **kwargs,\n                 )\n \n             hidden_states = outputs[0]"
        }
    ],
    "stats": {
        "total": 13,
        "additions": 13,
        "deletions": 0
    }
}