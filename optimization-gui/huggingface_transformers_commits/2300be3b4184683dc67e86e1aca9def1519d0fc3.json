{
    "author": "nayana1729",
    "message": "sped up gguf tokenizer for nemotron test (#40509)\n\nsped up tokenizer for nemotron test",
    "sha": "2300be3b4184683dc67e86e1aca9def1519d0fc3",
    "files": [
        {
            "sha": "8be00dfde8141eef83fd6d05d64ff6cad2afef59",
            "filename": "tests/quantization/ggml/test_ggml.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/2300be3b4184683dc67e86e1aca9def1519d0fc3/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/2300be3b4184683dc67e86e1aca9def1519d0fc3/tests%2Fquantization%2Fggml%2Ftest_ggml.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fquantization%2Fggml%2Ftest_ggml.py?ref=2300be3b4184683dc67e86e1aca9def1519d0fc3",
            "patch": "@@ -825,8 +825,8 @@ def test_nemotron_q6_k(self):\n             gguf_file=self.q6_k_nemotron_model_id,\n             dtype=torch.float16,\n         )\n-\n-        tokenizer = AutoTokenizer.from_pretrained(self.nemotron_model_id, gguf_file=self.q6_k_nemotron_model_id)\n+        # use the original tokenizer from nvidia to avoid long load times\n+        tokenizer = AutoTokenizer.from_pretrained(\"nvidia/Nemotron-Mini-4B-Instruct\")\n         text = tokenizer(self.example_text, return_tensors=\"pt\")[\"input_ids\"]\n         out = model.generate(text, max_new_tokens=16)\n "
        }
    ],
    "stats": {
        "total": 4,
        "additions": 2,
        "deletions": 2
    }
}