{
    "author": "inkcherry",
    "message": "fix total batch size calculation in trainer (#38286)\n\n* fix total batch size calculation\n\n* update\n\nSigned-off-by: inkcherry <mingzhi.liu@intel.com>\n\n* Update src/transformers/trainer.py\n\n---------\n\nSigned-off-by: inkcherry <mingzhi.liu@intel.com>\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "871901cb3dd92145c4187107d6b68a9c0a859e10",
    "files": [
        {
            "sha": "dabc9a6ef954cc091c78980f352bec959d8c4329",
            "filename": "src/transformers/integrations/deepspeed.py",
            "status": "modified",
            "additions": 8,
            "deletions": 3,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/871901cb3dd92145c4187107d6b68a9c0a859e10/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/871901cb3dd92145c4187107d6b68a9c0a859e10/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fdeepspeed.py?ref=871901cb3dd92145c4187107d6b68a9c0a859e10",
            "patch": "@@ -458,11 +458,16 @@ def deepspeed_init(trainer, num_training_steps, inference=False):\n         model_parameters = None\n     else:\n         trainer.optimizer = None  # important for when deepspeed_init is used as re-init\n-        tp_size = hf_deepspeed_config.config.get(\"tensor_parallel\", {}).get(\"autotp_size\", 0)\n-        if tp_size > 1:\n+        deepspeed_tp_size = hf_deepspeed_config.config.get(\"tensor_parallel\", {}).get(\"autotp_size\", 1)\n+        if deepspeed_tp_size > 1:\n             import deepspeed\n \n-            model = deepspeed.tp_model_init(model=model, tp_size=tp_size, dtype=hf_deepspeed_config.dtype())\n+            model = deepspeed.tp_model_init(\n+                model=model,\n+                tp_size=deepspeed_tp_size,\n+                dtype=hf_deepspeed_config.dtype(),\n+                config=hf_deepspeed_config.config,\n+            )\n         model_parameters = list(filter(lambda p: p.requires_grad, model.parameters()))\n         optimizer, lr_scheduler = deepspeed_optim_sched(\n             trainer, hf_deepspeed_config, args, num_training_steps, model_parameters"
        },
        {
            "sha": "7044ea040bf6c44b2a044216a1923fdc9cabbae8",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 23,
            "deletions": 1,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/871901cb3dd92145c4187107d6b68a9c0a859e10/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/871901cb3dd92145c4187107d6b68a9c0a859e10/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=871901cb3dd92145c4187107d6b68a9c0a859e10",
            "patch": "@@ -2238,6 +2238,27 @@ def train(\n                 ignore_keys_for_eval=ignore_keys_for_eval,\n             )\n \n+    def get_tp_size(self) -> int:\n+        \"\"\"Get the tensor parallel size from either the model or DeepSpeed config.\"\"\"\n+\n+        # 1. Check model.tp_size first\n+        if (model_tp := getattr(self.model, \"_tp_size\", None)) is not None:\n+            return model_tp\n+\n+        # 2. Fall back to DeepSpeed config if enabled\n+        if self.is_deepspeed_enabled and (deepspeed_config := getattr(self.args, \"hf_deepspeed_config\", None)):\n+            return deepspeed_config.config.get(\"tensor_parallel\", {}).get(\"autotp_size\", 1)\n+\n+        # 3. Default fallback\n+        return 1\n+\n+    def get_total_train_batch_size(self, args) -> int:\n+        \"\"\"Calculates total batch size (micro_batch * grad_accum * dp_world_size).\n+\n+        Note: Only considers DP and TP (dp_world_size = world_size // tp_size).\"\"\"\n+        dp_world_size = args.world_size // self.get_tp_size()\n+        return self._train_batch_size * args.gradient_accumulation_steps * dp_world_size\n+\n     def _inner_training_loop(\n         self, batch_size=None, args=None, resume_from_checkpoint=None, trial=None, ignore_keys_for_eval=None\n     ):\n@@ -2268,7 +2289,8 @@ def _inner_training_loop(\n         # number of training epochs: num_train_epochs\n         # number of training steps per epoch: num_update_steps_per_epoch\n         # total number of training steps to execute: max_steps\n-        total_train_batch_size = self._train_batch_size * args.gradient_accumulation_steps * args.world_size\n+        total_train_batch_size = self.get_total_train_batch_size(args)\n+\n         (\n             num_train_epochs,\n             num_update_steps_per_epoch,"
        }
    ],
    "stats": {
        "total": 35,
        "additions": 31,
        "deletions": 4
    }
}