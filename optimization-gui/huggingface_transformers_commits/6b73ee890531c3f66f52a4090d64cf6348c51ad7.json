{
    "author": "tomaarsen",
    "message": "ModernBert: reuse GemmaRotaryEmbedding via modular + Integration tests (#35459)\n\n* Introduce 5 integration tests for the 4 model classes + torch export\r\n\r\n* ModernBert: reuse GemmaRotaryEmbedding via modular\r\n\r\n* Revert #35589, keep rope_kwargs; rely on them in modular_modernbert\r\n\r\n* Revert \"Revert #35589, keep rope_kwargs; rely on them in modular_modernbert\"\r\n\r\nThis reverts commit 11b44b9ee83e199cbfb7c5ba2d11f7a7fdbba2d3.\r\n\r\n* Don't set rope_kwargs; override 'self.rope_init_fn' call instead",
    "sha": "6b73ee890531c3f66f52a4090d64cf6348c51ad7",
    "files": [
        {
            "sha": "e13eb1dea643c306140f1fa11251d7008f66b2ff",
            "filename": "src/transformers/models/modernbert/modeling_modernbert.py",
            "status": "modified",
            "additions": 42,
            "deletions": 14,
            "changes": 56,
            "blob_url": "https://github.com/huggingface/transformers/blob/6b73ee890531c3f66f52a4090d64cf6348c51ad7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6b73ee890531c3f66f52a4090d64cf6348c51ad7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodeling_modernbert.py?ref=6b73ee890531c3f66f52a4090d64cf6348c51ad7",
            "patch": "@@ -31,6 +31,7 @@\n from ...activations import ACT2FN\n from ...modeling_attn_mask_utils import _prepare_4d_attention_mask\n from ...modeling_outputs import BaseModelOutput, MaskedLMOutput, SequenceClassifierOutput, TokenClassifierOutput\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS\n from ...modeling_utils import PreTrainedModel\n from ...utils import (\n     add_code_sample_docstrings,\n@@ -241,30 +242,59 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n \n \n class ModernBertRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n+    def __init__(self, config: ModernBertConfig, dim: int, base: float, device: Optional[torch.device] = None):\n         super().__init__()\n+        # BC: \"rope_type\" was originally \"type\"\n+        if hasattr(config, \"rope_scaling\") and config.rope_scaling is not None:\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+        inv_freq, self.attention_scaling = self.rope_init_fn(None, device, dim=dim, base=base)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    def _dynamic_frequency_update(self, position_ids, device):\n+        \"\"\"\n+        dynamic RoPE layers should recompute `inv_freq` in the following situations:\n+        1 - growing beyond the cached sequence length (allow scaling)\n+        2 - the current sequence length is in the original scale (avoid losing precision with small sequences)\n+        \"\"\"\n+        seq_len = torch.max(position_ids) + 1\n+        if seq_len > self.max_seq_len_cached:  # growth\n+            inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device, seq_len=seq_len)\n+            self.register_buffer(\"inv_freq\", inv_freq, persistent=False)  # TODO joao: may break with compilation\n+            self.max_seq_len_cached = seq_len\n \n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n-        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n+        if seq_len < self.original_max_seq_len and self.max_seq_len_cached > self.original_max_seq_len:  # reset\n+            self.register_buffer(\"inv_freq\", self.original_inv_freq, persistent=False)\n+            self.max_seq_len_cached = self.original_max_seq_len\n \n     @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        self.inv_freq.to(x.device)\n+    def forward(self, x, position_ids):\n+        if \"dynamic\" in self.rope_type:\n+            self._dynamic_frequency_update(position_ids, device=x.device)\n+\n+        # Core RoPE block\n         inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n         position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n+        # Force float32 (see https://github.com/huggingface/transformers/pull/29285)\n         device_type = x.device.type\n         device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n         with torch.autocast(device_type=device_type, enabled=False):\n             freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n             emb = torch.cat((freqs, freqs), dim=-1)\n             cos = emb.cos()\n             sin = emb.sin()\n+\n+        # Advanced RoPE types (e.g. yarn) apply a post-processing scaling factor, equivalent to scaling attention\n+        cos = cos * self.attention_scaling\n+        sin = sin * self.attention_scaling\n+\n         return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n \n \n@@ -468,9 +498,7 @@ def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n                 dim=self.head_dim, max_seqlen=max_position_embeddings, base=rope_theta\n             )\n         else:\n-            self.rotary_emb = ModernBertRotaryEmbedding(\n-                dim=self.head_dim, max_position_embeddings=max_position_embeddings, base=rope_theta\n-            )\n+            self.rotary_emb = ModernBertRotaryEmbedding(config=config, dim=self.head_dim, base=rope_theta)\n \n         self.Wo = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)\n         self.out_drop = nn.Dropout(config.attention_dropout) if config.attention_dropout > 0.0 else nn.Identity()"
        },
        {
            "sha": "edfdc94346bf122b71f25f981dc30af89bfc5c7a",
            "filename": "src/transformers/models/modernbert/modular_modernbert.py",
            "status": "modified",
            "additions": 6,
            "deletions": 30,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/6b73ee890531c3f66f52a4090d64cf6348c51ad7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6b73ee890531c3f66f52a4090d64cf6348c51ad7/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmodernbert%2Fmodular_modernbert.py?ref=6b73ee890531c3f66f52a4090d64cf6348c51ad7",
            "patch": "@@ -41,7 +41,7 @@\n     logging,\n )\n from ...utils.import_utils import is_triton_available\n-from ..gemma.modeling_gemma import apply_rotary_pos_emb\n+from ..gemma.modeling_gemma import GemmaRotaryEmbedding, apply_rotary_pos_emb\n \n \n if is_flash_attn_2_available():\n@@ -504,32 +504,10 @@ def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:\n         return self.Wo(self.drop(self.act(input) * gate))\n \n \n-class ModernBertRotaryEmbedding(nn.Module):\n-    def __init__(self, dim, max_position_embeddings=2048, base=10000, device=None):\n-        super().__init__()\n-\n-        self.dim = dim\n-        self.max_position_embeddings = max_position_embeddings\n-        self.base = base\n-        inv_freq = 1.0 / (self.base ** (torch.arange(0, self.dim, 2, dtype=torch.int64).float() / self.dim))\n-        self.register_buffer(\"inv_freq\", tensor=inv_freq, persistent=False)\n-\n-    @torch.no_grad()\n-    def forward(self, x, position_ids, seq_len=None):\n-        # x: [bs, num_attention_heads, seq_len, head_size]\n-        self.inv_freq.to(x.device)\n-        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n-        position_ids_expanded = position_ids[:, None, :].float()\n-        # Force float32 since bfloat16 loses precision on long contexts\n-        # See https://github.com/huggingface/transformers/pull/29285\n-        device_type = x.device.type\n-        device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n-        with torch.autocast(device_type=device_type, enabled=False):\n-            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n-            emb = torch.cat((freqs, freqs), dim=-1)\n-            cos = emb.cos()\n-            sin = emb.sin()\n-        return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n+class ModernBertRotaryEmbedding(GemmaRotaryEmbedding):\n+    def __init__(self, config: ModernBertConfig, dim: int, base: float, device: Optional[torch.device] = None):\n+        super().__init__(self, config=config, device=device)\n+        inv_freq, self.attention_scaling = self.rope_init_fn(None, device, dim=dim, base=base)\n \n \n def eager_attention_forward(\n@@ -698,9 +676,7 @@ def __init__(self, config: ModernBertConfig, layer_id: Optional[int] = None):\n                 dim=self.head_dim, max_seqlen=max_position_embeddings, base=rope_theta\n             )\n         else:\n-            self.rotary_emb = ModernBertRotaryEmbedding(\n-                dim=self.head_dim, max_position_embeddings=max_position_embeddings, base=rope_theta\n-            )\n+            self.rotary_emb = ModernBertRotaryEmbedding(config=config, dim=self.head_dim, base=rope_theta)\n \n         self.Wo = nn.Linear(config.hidden_size, config.hidden_size, bias=config.attention_bias)\n         self.out_drop = nn.Dropout(config.attention_dropout) if config.attention_dropout > 0.0 else nn.Identity()"
        },
        {
            "sha": "9f286cf3985f0ad643040ba00483c2f4adb2a5f1",
            "filename": "tests/models/modernbert/test_modeling_modernbert.py",
            "status": "modified",
            "additions": 130,
            "deletions": 4,
            "changes": 134,
            "blob_url": "https://github.com/huggingface/transformers/blob/6b73ee890531c3f66f52a4090d64cf6348c51ad7/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6b73ee890531c3f66f52a4090d64cf6348c51ad7/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmodernbert%2Ftest_modeling_modernbert.py?ref=6b73ee890531c3f66f52a4090d64cf6348c51ad7",
            "patch": "@@ -16,8 +16,9 @@\n import unittest\n \n import pytest\n+from packaging import version\n \n-from transformers import ModernBertConfig, is_torch_available\n+from transformers import AutoTokenizer, ModernBertConfig, is_torch_available\n from transformers.models.auto import get_values\n from transformers.testing_utils import (\n     CaptureLogger,\n@@ -362,6 +363,131 @@ def test_flash_attn_2_conversion(self):\n \n @require_torch\n class ModernBertModelIntegrationTest(unittest.TestCase):\n-    \"\"\"\n-    These still need to be written, once public models are available.\n-    \"\"\"\n+    @slow\n+    def test_inference_masked_lm(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        model = ModernBertForMaskedLM.from_pretrained(\n+            \"answerdotai/ModernBERT-base\", reference_compile=False, attn_implementation=\"sdpa\"\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n+\n+        inputs = tokenizer(\"Hello World!\", return_tensors=\"pt\")\n+        with torch.no_grad():\n+            output = model(**inputs)[0]\n+        expected_shape = torch.Size((1, 5, 50368))\n+        self.assertEqual(output.shape, expected_shape)\n+\n+        # compare the actual values for a slice.\n+        expected_slice = torch.tensor(\n+            [[[3.8387, -0.2017, 12.2839], [3.6300, 0.6869, 14.7123], [-5.1137, -3.8122, 11.9874]]]\n+        )\n+        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+\n+    @slow\n+    def test_inference_no_head(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        model = ModernBertModel.from_pretrained(\n+            \"answerdotai/ModernBERT-base\", reference_compile=False, attn_implementation=\"sdpa\"\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"answerdotai/ModernBERT-base\")\n+\n+        inputs = tokenizer(\"Hello World!\", return_tensors=\"pt\")\n+        with torch.no_grad():\n+            output = model(**inputs)[0]\n+        expected_shape = torch.Size((1, 5, 768))\n+        self.assertEqual(output.shape, expected_shape)\n+\n+        # compare the actual values for a slice.\n+        expected_slice = torch.tensor(\n+            [[[0.3151, -0.6417, -0.7027], [-0.7834, -1.5810, 0.4576], [1.0614, -0.7268, -0.0871]]]\n+        )\n+        self.assertTrue(torch.allclose(output[:, :3, :3], expected_slice, atol=1e-4))\n+\n+    @slow\n+    def test_inference_token_classification(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        model = ModernBertForTokenClassification.from_pretrained(\n+            \"hf-internal-testing/tiny-random-ModernBertForTokenClassification\",\n+            reference_compile=False,\n+            attn_implementation=\"sdpa\",\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\"hf-internal-testing/tiny-random-ModernBertForTokenClassification\")\n+\n+        inputs = tokenizer(\"Hello World!\", return_tensors=\"pt\")\n+        with torch.no_grad():\n+            output = model(**inputs)[0]\n+        expected_shape = torch.Size((1, 5, 2))\n+        self.assertEqual(output.shape, expected_shape)\n+\n+        expected = torch.tensor(\n+            [[[2.0159, 4.6569], [-0.9430, 3.1595], [-3.8770, 3.2653], [1.5752, 4.5167], [-1.6939, 1.2524]]]\n+        )\n+        self.assertTrue(torch.allclose(output, expected, atol=1e-4))\n+\n+    @slow\n+    def test_inference_sequence_classification(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        model = ModernBertForSequenceClassification.from_pretrained(\n+            \"hf-internal-testing/tiny-random-ModernBertForSequenceClassification\",\n+            reference_compile=False,\n+            attn_implementation=\"sdpa\",\n+        )\n+        tokenizer = AutoTokenizer.from_pretrained(\n+            \"hf-internal-testing/tiny-random-ModernBertForSequenceClassification\"\n+        )\n+\n+        inputs = tokenizer(\"Hello World!\", return_tensors=\"pt\")\n+        with torch.no_grad():\n+            output = model(**inputs)[0]\n+        expected_shape = torch.Size((1, 2))\n+        self.assertEqual(output.shape, expected_shape)\n+\n+        expected = torch.tensor([[1.6466, 4.5662]])\n+        self.assertTrue(torch.allclose(output, expected, atol=1e-4))\n+\n+    @slow\n+    def test_export(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        bert_model = \"answerdotai/ModernBERT-base\"\n+        device = \"cpu\"\n+        attn_implementation = \"sdpa\"\n+        max_length = 512\n+\n+        tokenizer = AutoTokenizer.from_pretrained(bert_model)\n+        inputs = tokenizer(\n+            \"the man worked as a [MASK].\",\n+            return_tensors=\"pt\",\n+            padding=\"max_length\",\n+            max_length=max_length,\n+        )\n+\n+        model = ModernBertForMaskedLM.from_pretrained(\n+            bert_model,\n+            device_map=device,\n+            attn_implementation=attn_implementation,\n+        )\n+\n+        logits = model(**inputs).logits\n+        eg_predicted_mask = tokenizer.decode(logits[0, 6].topk(5).indices)\n+        self.assertEqual(eg_predicted_mask.split(), [\"lawyer\", \"mechanic\", \"teacher\", \"doctor\", \"waiter\"])\n+\n+        exported_program = torch.export.export(\n+            model,\n+            args=(inputs[\"input_ids\"],),\n+            kwargs={\"attention_mask\": inputs[\"attention_mask\"]},\n+            strict=True,\n+        )\n+\n+        result = exported_program.module().forward(inputs[\"input_ids\"], inputs[\"attention_mask\"])\n+        ep_predicted_mask = tokenizer.decode(result.logits[0, 6].topk(5).indices)\n+        self.assertEqual(eg_predicted_mask, ep_predicted_mask)"
        }
    ],
    "stats": {
        "total": 226,
        "additions": 178,
        "deletions": 48
    }
}