{
    "author": "nevertmr",
    "message": "ğŸŒ [i18n-KO] Translated `perf_infer_gpu_multi.md` to Korean (#39441)\n\n* docs: ko: perf_infer_gpu_many.md\n\n* feat: nmt draft\n\n* docs: refine KO translation and enhance naturalness\n\n* docs: add missing TOC to documentation\n\n* Align toctree and filename with original: perf_infer_gpu_multi\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\n\n* Refine Korean translation\n\n* Update docs/source/ko/perf_infer_gpu_multi.md\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n* Update docs/source/ko/perf_infer_gpu_multi.md\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n* Update docs/source/ko/perf_infer_gpu_multi.md\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n* Update docs/source/ko/perf_infer_gpu_multi.md\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n* Update docs/source/ko/perf_infer_gpu_multi.md\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n* Update docs/source/ko/perf_infer_gpu_multi.md\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n* Update docs/source/ko/perf_infer_gpu_multi.md\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n* Update docs/source/ko/perf_infer_gpu_multi.md\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n* Update docs/source/ko/perf_infer_gpu_multi.md\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n* Update docs/source/ko/perf_infer_gpu_multi.md\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>\n\n* Update docs/source/ko/perf_infer_gpu_multi.md\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n* Apply suggestions from code review\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n---------\n\nCo-authored-by: YONGSANG <71686691+4N3MONE@users.noreply.github.com>\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\nCo-authored-by: Yijun Lee <119404328+yijun-lee@users.noreply.github.com>",
    "sha": "2da97f09436fa346ecb5c388037d76589f1579a6",
    "files": [
        {
            "sha": "130e1a93356fe2a053c14a9fb2a56c4b442f0d32",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/2da97f09436fa346ecb5c388037d76589f1579a6/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/2da97f09436fa346ecb5c388037d76589f1579a6/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=2da97f09436fa346ecb5c388037d76589f1579a6",
            "patch": "@@ -202,6 +202,8 @@\n       title: CPUë¡œ ì¶”ë¡ í•˜ê¸°\n     - local: perf_infer_gpu_one\n       title: í•˜ë‚˜ì˜ GPUë¥¼ í™œìš©í•œ ì¶”ë¡ \n+    - local: perf_infer_gpu_multi\n+      title: ë‹¤ì¤‘ GPUë¥¼ í™œìš©í•œ ì¶”ë¡ \n     title: ì¶”ë¡  ìµœì í™”í•˜ê¸°\n   - local: big_models\n     title: ëŒ€í˜• ëª¨ë¸ì„ ì¸ìŠ¤í„´ìŠ¤í™”"
        },
        {
            "sha": "3d66b7398a2af223a90277c0f952b8ce2cdd08e5",
            "filename": "docs/source/ko/perf_infer_gpu_multi.md",
            "status": "added",
            "additions": 311,
            "deletions": 0,
            "changes": 311,
            "blob_url": "https://github.com/huggingface/transformers/blob/2da97f09436fa346ecb5c388037d76589f1579a6/docs%2Fsource%2Fko%2Fperf_infer_gpu_multi.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/2da97f09436fa346ecb5c388037d76589f1579a6/docs%2Fsource%2Fko%2Fperf_infer_gpu_multi.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fperf_infer_gpu_multi.md?ref=2da97f09436fa346ecb5c388037d76589f1579a6",
            "patch": "@@ -0,0 +1,311 @@\n+<!--Copyright 2024 The HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+\n+-->\n+\n+# ë¶„ì‚° ì¶”ë¡ [[distributed-inference]]\n+\n+ëª¨ë¸ì´ ë‹¨ì¼ GPUì— ì˜¬ë¼ê°€ì§€ ì•ŠëŠ” ê²½ìš°, [í…ì„œ ë³‘ë ¬ ì²˜ë¦¬](./perf_train_gpu_many#tensor-parallelism)ë¥¼ ì‚¬ìš©í•œ ë¶„ì‚° ì¶”ë¡ ì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í…ì„œ ë³‘ë ¬í™”ëŠ” ëª¨ë¸ì„ ì—¬ëŸ¬ ê°€ì†ê¸°(CUDA GPU, Intel XPU ë“±)ì— ë¶„í• í•˜ì—¬ í–‰ë ¬ ê³±ì…ˆê³¼ ê°™ì€ ê³„ì‚°ì„ ë³‘ë ¬í™”í•©ë‹ˆë‹¤. ì´ë¥¼ í†µí•´ ë” í° ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— ì˜¬ë¦´ ìˆ˜ ìˆìœ¼ë©°, ê° ê°€ì†ê¸°ê°€ í…ì„œì˜ ì¼ë¶€ë¥¼ ì²˜ë¦¬í•˜ë¯€ë¡œ ì¶”ë¡  ì†ë„ê°€ í–¥ìƒë©ë‹ˆë‹¤.\n+\n+ê·¸ëŸ¬ë‚˜ í…ì„œ ë³‘ë ¬í™”ëŠ” í†µì‹  ì˜¤ë²„í—¤ë“œë¥¼ ë°œìƒì‹œí‚¤ë¯€ë¡œ, ë¹ ë¥¸ ë…¸ë“œ ë‚´ í†µì‹ ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ë‹¤ì¤‘ ê°€ì†ê¸° í™˜ê²½ì—ì„œ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ê°€ì¥ íš¨ê³¼ì ì…ë‹ˆë‹¤. ë‹¤ì¤‘ ë…¸ë“œ í•™ìŠµ í™˜ê²½ì—ì„œëŠ” ì‚¬ìš© ì‚¬ë¡€ì— ë”°ë¼ íŒŒì´í”„ë¼ì¸ ë³‘ë ¬í™”ë‚˜ ë°ì´í„° ë³‘ë ¬í™”ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ë” íš¨ìœ¨ì ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+> [!TIP]\n+> í…ì„œ ë³‘ë ¬í™”ì— ëŒ€í•´ ë” ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism)ì˜ í…ì„œ ë³‘ë ¬í™” ì„¹ì…˜ì„ ì°¸ì¡°í•˜ì„¸ìš”.\n+\n+ì•„ë˜ ëª©ë¡ì—ì„œ í…ì„œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ê¸°ë³¸ì ìœ¼ë¡œ ì§€ì›í•˜ëŠ” ëª¨ë¸ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ ëª¨ë¸ì— ëŒ€í•œ ì§€ì›ì„ ì¶”ê°€í•˜ë ¤ë©´ GitHub ì´ìŠˆë‚˜ í’€ ë¦¬í€˜ìŠ¤íŠ¸ë¥¼ ì—´ì–´ì£¼ì„¸ìš”.\n+\n+<details>\n+<summary>ì§€ì›ë˜ëŠ” ëª¨ë¸ ë³´ê¸°</summary>\n+\n+* [Cohere](./model_doc/cohere) ë° [Cohere 2](./model_doc/cohere2)\n+* [Gemma](./model_doc/gemma) ë° [Gemma 2](./model_doc/gemma2)\n+* [GLM](./model_doc/glm)\n+* [Granite](./model_doc/granite)\n+* [Llama](./model_doc/llama)\n+* [Mistral](./model_doc/mistral)\n+* [Mixtral](./model_doc/mixtral)\n+* [OLMo](./model_doc/olmo) ë° [OLMo2](./model_doc/olmo2)\n+* [Phi](./model_doc/phi) ë° [Phi-3](./model_doc/phi3)\n+* [Qwen2](./model_doc/qwen2), [Qwen2Moe](./model_doc/qwen2_moe), ë° [Qwen2-VL](./model_doc/qwen2_5_vl)\n+* [Starcoder2](./model_doc/starcoder2)\n+\n+</details>\n+\n+ì´ ê°€ì´ë“œëŠ” Transformersì—ì„œ ë‹¤ì–‘í•œ ë¶„í•  ì „ëµì„ ì‚¬ìš©í•˜ì—¬ í…ì„œ ë³‘ë ¬í™”ë¥¼ í™œì„±í™”í•˜ëŠ” ë°©ë²•ì„ ì„¤ëª…í•©ë‹ˆë‹¤.\n+\n+## ëª¨ë¸ ë¶„í• [[partitioning-a-model]]\n+\n+TransformersëŠ” `tp_plan`ë§¤ê°œë³€ìˆ˜ë¥¼ í™œìš©í•  ìˆ˜ ìˆëŠ” ëª¨ë¸ì— ëŒ€í•´ í…ì„œ ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ì§€ì›í•©ë‹ˆë‹¤. ëª¨ë¸ ë¶„í•  ë°©ì‹ì€ ë‘ ê°€ì§€ê°€ ìˆìŠµë‹ˆë‹¤.\n+\n+- `auto` í…ì„œ ë³‘ë ¬í™” ê³„íšì€ ì‚¬ì „ ì •ì˜ëœ êµ¬ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ëª¨ë¸(ìœ„ì— ì–¸ê¸‰ëœ ì§€ì› ëª¨ë¸)ì„ ìë™ìœ¼ë¡œ ë¶„í• í•©ë‹ˆë‹¤.\n+- ì‚¬ìš©ì ì§€ì • ë¶„í•  ê³„íšì„ ì§ì ‘ ì •ì˜í•˜ì—¬ [~PreTrainedModel.from_pretrained] ë©”ì†Œë“œì˜ `tp_plan` ë§¤ê°œë³€ìˆ˜ë¡œ ì „ë‹¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+<hfoptions id=\"sharding\">\n+<hfoption id=\"auto plan\">\n+\n+```py\n+import os\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+# model_id = \"meta-llama/Llama-4-Scout-17B-16E-Instruct\" # ëª¨ë“  ê°€ëŠ¥í•œ ì „ëµì„ ì‹œê°í™”í•˜ê¸°ì— ë” ì¢‹ìŒ\n+model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"  # ì ì€ ìˆ˜ì˜ GPUì— ë” ì¢‹ìŒ\n+\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=\"auto\")\n+print(model._tp_plan)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Meta-Llama-3-8B-Instruct\")\n+prompt = \"Can I help\"\n+inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(model.device)\n+\n+# ë¶„ì‚° ì‹¤í–‰\n+outputs = model(inputs)\n+```\n+\n+ìœ„ì˜ ì¶”ë¡  ìŠ¤í¬ë¦½íŠ¸ë¥¼ GPUë‹¹ 4ê°œ í”„ë¡œì„¸ìŠ¤ë¡œ [torchrun](https://pytorch.org/docs/stable/elastic/run.html)ì—ì„œ ì‹¤í–‰í•˜ì„¸ìš”.\n+\n+```bash\n+torchrun --nproc-per-node 4 demo.py\n+```\n+\n+</hfoption>\n+<hfoption id=\"manual plan\">\n+\n+ê° ë ˆì´ì–´ì— ëŒ€í•œ í…ì„œ ë³‘ë ¬ ê³„íšì„ `tp_plan`ì— ì •ì˜í•œ í›„ [`~PreTrainedModel.from_pretrained`]ì— ì „ë‹¬í•˜ì„¸ìš”. ì•„ë˜ ì˜ˆì‹œëŠ” ì—´ ë° í–‰ ë¶„í• ì„ ì¡°í•©í•˜ì—¬ ì‚¬ìš©í•©ë‹ˆë‹¤. ì§€ì›ë˜ëŠ” ë‹¤ë¥¸ ë¶„í•  ì „ëµì€ [ë¶„í•  ì „ëµ](#partitioning-strategies) ì„¹ì…˜ì„ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+> [!WARNING]\n+> ì‚¬ìš©ì ì§€ì • ë¶„í•  ê³„íšì„ ìˆ˜ë™ìœ¼ë¡œ ì§€ì •í•˜ë ¤ë©´ ëª¨ë¸ ì•„í‚¤í…ì²˜ì™€ ë¶„í•  ì „ëµì´ í•¨ê»˜ ìƒí˜¸ ì‘ìš©í•˜ëŠ” ë°©ì‹ì— ëŒ€í•œ ì¶©ë¶„í•œ ì´í•´ê°€ í•„ìš”í•©ë‹ˆë‹¤. ë¶„í•  ì „ëµì„ ì˜ëª» ì„¤ì •í•˜ë©´ ëª¨ë¸ì´ ë§¤ìš° ëŠë ¤ì§€ê±°ë‚˜, ì˜¤ë¥˜ê°€ ë°œìƒí•˜ê±°ë‚˜, ë¶€ì •í™•í•œ ê²°ê³¼ë¥¼ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ [Ultra-Scale Playbook](https://huggingface.co/spaces/nanotron/ultrascale-playbook?section=tensor_parallelism)ì„ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+```py\n+from transformers import AutoModelForCausalLM\n+\n+tp_plan = {\n+    \"model.layers.*.self_attn.q_proj\": \"colwise\",\n+    \"model.layers.*.self_attn.k_proj\": \"colwise\",\n+    \"model.layers.*.self_attn.v_proj\": \"colwise\",\n+    \"model.layers.*.self_attn.o_proj\": \"rowwise\",\n+    ...\n+}\n+\n+model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=tp_plan)\n+print(model._tp_plan)\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+## ë¶„í•  ì „ëµ[[partitioning-strategies]]\n+\n+ëª¨ë“  ë¶„í•  ì „ëµì€ ë¬¸ìì—´ì„ ì „ëµ êµ¬í˜„ì— ë§¤í•‘í•˜ëŠ” [`ParallelInterface`] í´ë˜ìŠ¤ì—ì„œ ì •ì˜ë©ë‹ˆë‹¤. ëª¨ë“  ì „ëµì€ [`~PreTrainedModel.from_pretrained`]ì˜ `tp_plan`ì„ í†µí•´ ì„¤ì •ë˜ë¯€ë¡œ ì´ í´ë˜ìŠ¤ì™€ ì§ì ‘ ìƒí˜¸ ì‘ìš©í•  í•„ìš”ëŠ” ì—†ì§€ë§Œ, ì–´ë–¤ ì „ëµì„ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ”ì§€ í™•ì¸í•  ë•Œ ìœ ìš©í•©ë‹ˆë‹¤.\n+\n+```py\n+class ParallelInterface(MutableMapping):\n+    \"\"\"\n+    í—ˆìš©ëœ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ì¶”ì í•˜ëŠ” ë”•ì…”ë„ˆë¦¬ ê°™ì€ ê°ì²´ì…ë‹ˆë‹¤. `register()` í˜¸ì¶œë¡œ ìƒˆë¡œìš´ ì–´í…ì…˜ í•¨ìˆ˜ë¥¼ ì‰½ê²Œ ì¶”ê°€í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. \n+    ëª¨ë¸ì´ ê¸°ì¡´ ì–´í…ì…˜ í•¨ìˆ˜(ì˜ˆ: `sdpa`)ë¥¼ ë¡œì»¬ì—ì„œ ë®ì–´ì“°ë ¤ë©´ `modeling_<model>.py` ë‚´ë¶€ì—ì„œ ì´ í´ë˜ìŠ¤ì˜ ìƒˆ ì¸ìŠ¤í„´ìŠ¤ë¥¼ ì„ ì–¸í•˜ê³  \n+    í•´ë‹¹ ì¸ìŠ¤í„´ìŠ¤ì—ì„œ ì„ ì–¸í•´ì•¼ í•©ë‹ˆë‹¤.\n+    \"\"\"\n+    _global_mapping = {\n+        \"colwise\": ColwiseParallel(),\n+        \"rowwise\": RowwiseParallel(),\n+        \"colwise_rep\": ColwiseParallel(output_layouts=Replicate()),\n+        \"rowwise_rep\": RowwiseParallel(input_layouts=Replicate()),\n+        \"local_colwise\": ColwiseParallel(use_dtensor=False),\n+        \"local_rowwise\": RowwiseParallel(use_dtensor=False),\n+        \"local\": IsolatedParallel(),\n+        \"gather\": GatherParallel(),\n+        \"local_packed_rowwise\": PackedRowwiseParallel(use_dtensor=False),\n+        \"sequence_parallel\": SequenceParallel(),\n+        \"replicate\": ReplicateParallel(),\n+    }\n+```\n+\n+ê° ì „ëµì— ëŒ€í•´ ìì„¸íˆ ì•Œì•„ë³´ë ¤ë©´ ì•„ë˜ í‘œë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+| ì „ëµ | ì„¤ëª… |\n+|---|---|\n+| `ColwiseParallel` | ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì˜ ì—´ ë°©í–¥ ë¶„í• . |\n+| `RowwiseParallel` | ê°€ì¤‘ì¹˜ì™€ í¸í–¥ì˜ í–‰ ë°©í–¥ ë¶„í• . `nn.Embedding` ëª¨ë“ˆ ë¶„í• ë„ ì§€ì›. |\n+| `SequenceParallel` | `LayerNorm`ê³¼ `Dropout` ë ˆì´ì–´ë¥¼ ì§€ì›í•˜ëŠ” ì‹œí€€ìŠ¤ ë³‘ë ¬ êµ¬í˜„. [RMSNorm](https://github.com/facebookresearch/llama/blob/main/llama/model.py#L34)ì˜ Python êµ¬í˜„ë„ ì§€ì›. |\n+| `PackedColwiseParallel` | íŒ¨í‚¹ëœ ê°€ì¤‘ì¹˜ë¥¼ ì§€ì›í•˜ëŠ” `ColwiseParallel`ì˜ ë³€í˜•(ì˜ˆ: `up_proj`ì™€ `gate_proj`ë¥¼ í•¨ê»˜ íŒ¨í‚¹). ìì„¸í•œ ë‚´ìš©ì€ [ì½”ë“œ](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108)ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”. |\n+| `PackedRowwiseParallel` | íŒ¨í‚¹ëœ ê°€ì¤‘ì¹˜ë¥¼ ì§€ì›í•˜ëŠ” `RowwiseParallel`ì˜ ë³€í˜•([ì½”ë“œ](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108) ì°¸ì¡°). |\n+| `GatherParallel` | ê¸°ê¸° ê°„ ëª¨ë“ˆì˜ ì¶œë ¥ì„ ìˆ˜ì§‘. |\n+| `IsolatedParallel` | Mixture-of-Experts(MoE) ë ˆì´ì–´ì˜ ì „ë¬¸ê°€ì— ì‚¬ìš©ë˜ì–´ ë‹¤ë¥¸ ê¸°ê¸°ë¡œë¶€í„° ëª¨ë“ˆì„ ê²©ë¦¬. |\n+| `ReplicateParallel` | ë¶€ë¶„ì ìœ¼ë¡œ ë¶„í• ëœ ëª¨ë¸ë¡œ ì¸í•´ `torch.distributed` APIê°€ ì¤‘ë‹¨ë˜ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•´ ëª¨ë“  ê¸°ê¸°ì— ëª¨ë“ˆì„ ë³µì œ. |\n+\n+### íŒ¨í‚¹ëœ ì „ëµ[[packed-strategies]]\n+\n+ê°€ì¤‘ì¹˜ íŒ¨í‚¹ì€ ì—¬ëŸ¬ ì„ í˜• ë ˆì´ì–´ë¥¼ í•˜ë‚˜ì˜ ë” í° ë ˆì´ì–´ë¡œ í•©ì¹˜ëŠ” ê¸°ë²•ì…ë‹ˆë‹¤. íŒ¨í‚¹ëœ ì „ëµì¸ `PackedColwiseParallel`ê³¼ `PackedRowwiseParallel`ì€ íŒ¨í‚¹ëœ ê°€ì¤‘ì¹˜ë¥¼ ë¶„í• í•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤. ê¸°ë³¸ì ì¸ `ColwiseParallel`ì´ë‚˜ `RowwiseParallel`ì€ íŒ¨í‚¹ëœ ê°€ì¤‘ì¹˜ë¥¼ ì˜¬ë°”ë¥´ê²Œ ë¶„í• í•˜ì§€ ëª»í•©ë‹ˆë‹¤.\n+\n+ì•„ë˜ ì˜ˆì‹œëŠ” `up_proj`ì™€ `gate_proj`ë¥¼ ë‹¨ì¼ `gate_up_proj` ëª¨ë“ˆë¡œ íŒ¨í‚¹í•˜ê³  `gate_up_proj`ë¥¼ ë¶„í• í•˜ê¸° ìœ„í•´ `PackedRowwiseParallel` ì „ëµì´ í•„ìš”í•©ë‹ˆë‹¤.\n+\n+```python\n+class Llama4TextExperts(nn.Module):\n+    ...\n+    self.gate_up_proj = nn.Parameter(torch.empty(self.num_experts, self.hidden_size, 2 * self.expert_dim))\n+```\n+\n+ë°°ì¹˜ í–‰ë ¬ ê³±ì…ˆì„ `forward` íŒ¨ìŠ¤ì—ì„œ ì‚¬ìš©í•˜ì—¬ `gate_up_proj` ëª¨ë“ˆì˜ ì¶œë ¥ì„ ê³„ì‚°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```python\n+def forward(self, hidden_states):\n+    ...\n+    gate_up = torch.bmm(hidden_states, self.gate_up_proj) # gate_up_proj ëª¨ë“ˆì˜ ì¶œë ¥ ê³„ì‚°\n+    gate, up = gate_up.chunk(2, dim=-1) # ì¶œë ¥ì„ gateì™€ upìœ¼ë¡œ ë¶„í• \n+```\n+\n+> [!TIP]\n+> `Packed*`ë¥¼ ì‚¬ìš©í•´ì•¼ í•˜ëŠ” ì´ìœ ì— ëŒ€í•œ ì‹œê°ì  í‘œí˜„ì€ [ì´ ì£¼ì„](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py#L79-#L108)ì„ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+### ë¡œì»¬ ì „ëµ[[local-strategies]]\n+\n+ë¡œì»¬ ì „ëµ(`local_colwise`, `local_rowwise`, `local_packed_rowwise`)ì€ [torch.chunk](https://docs.pytorch.org/docs/stable/generated/torch.chunk.html)ì™€ ê°™ì€ ì¼ë¶€ ì—°ì‚°ì—ì„œ ì§€ì›ë˜ì§€ ì•Šê¸° ë•Œë¬¸ì— [DTensor](https://docs.pytorch.org/docs/stable/distributed.tensor.html)ë¥¼ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ëŒ€ì‹  ë¡œì»¬ ì „ëµì€ ê¸°ë³¸ [torch.Tensor](https://docs.pytorch.org/docs/stable/tensors.html)ë¥¼ ì‚¬ìš©í•˜ê³  ì¼ë¶€ ë¶„ì‚° ë¡œì§ì„ ìˆ˜ë™ìœ¼ë¡œ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n+\n+<!--\n+Readd this when I get the exact error message\n+> [!TIP]\n+> ì‚¬ìš©ì ì •ì˜ ë¶„í•  ì „ëµì„ ì‚¬ìš©í•˜ëŠ”ë° `... is not supported` ì˜¤ë¥˜ë¡œ ì‘ë™í•˜ì§€ ì•ŠëŠ” ê²½ìš°, `local*` ì „ëµì„ ì‚¬ìš©í•´ì„œ ë” ì˜ ì‘ë™í•˜ëŠ”ì§€ ì‹œë„í•´ë³´ì„¸ìš”.\n+-->\n+\n+## ì‚¬ìš©ì ì •ì˜ ë¶„í•  ì „ëµ[[custom-partitioning-strategies]]\n+\n+ì‚¬ìš©ì ì •ì˜ ë¶„í•  ì „ëµì€ [`TensorParallelLayer`](https://github.com/huggingface/transformers/blob/main/src/transformers/integrations/tensor_parallel.py)ë¥¼ ìƒì†í•˜ê³  `partition_tensor`, `_prepare_input_fn`, `_prepare_output_fn`ì„ êµ¬í˜„í•´ì•¼ í•©ë‹ˆë‹¤.\n+\n+ê·¸ëŸ° ë‹¤ìŒ `tp_plan`ì—ì„œ í•´ë‹¹ ì „ëµì„ ì§€ì •í–ˆì„ ë•Œ ë””ìŠ¤íŒ¨ì¹­ ë¡œì§ì´ ì°¾ì„ ìˆ˜ ìˆë„ë¡ `ParallelInterface` ë§¤í•‘ì— ë“±ë¡í•´ì•¼ í•©ë‹ˆë‹¤.\n+\n+ì•„ë˜ ì˜ˆì‹œëŠ” ì´ ì›Œí¬í”Œë¡œìš°ë¡œ `ColwiseParallel`ì„ êµ¬í˜„í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+\n+1. `TensorParallelLayer`ë¥¼ ìƒì†í•©ë‹ˆë‹¤. `__init__` ë©”ì†Œë“œì—ì„œ ì…ë ¥ ë° ì¶œë ¥ í…ì„œê°€ ê¸°ê¸°ì— ì–´ë–»ê²Œ ë°°ì¹˜ë˜ì–´ì•¼ í•˜ëŠ”ì§€ ì„¤ëª…í•˜ëŠ” `input_layouts`ê³¼ `output_layouts`ì„ ì •ì˜í•©ë‹ˆë‹¤. `desired_input_layouts` ì†ì„±ì€ ì…ë ¥ì´ ê¸°ê¸°ì— ì–´ë–»ê²Œ ë°°ì¹˜*ë˜ì–´ì•¼ë§Œ* í•˜ëŠ”ì§€ë¥¼ ëª…ì‹œí•˜ëŠ” ë° ì‚¬ìš©ë©ë‹ˆë‹¤.\n+\n+    ```python\n+    class ColwiseParallel(TensorParallelLayer):\n+        def __init__(\n+            self,\n+            *,\n+            input_layouts: Optional[Placement] = None, # ì´ì „ ë ˆì´ì–´ì—ì„œ ì˜¤ëŠ” ì…ë ¥ ë ˆì´ì•„ì›ƒ\n+            output_layouts: Optional[Placement] = None, # ë‹¬ì„±í•˜ê³ ì í•˜ëŠ” ì¶œë ¥ ë ˆì´ì•„ì›ƒ\n+            use_local_output: bool = True, # ë¡œì»¬ ì¶œë ¥ ì‚¬ìš© ì—¬ë¶€\n+            use_dtensor=True, # DTensor ì‚¬ìš© ì—¬ë¶€\n+        ):\n+            self.input_layouts = (input_layouts or Replicate(),) # ì´ì „ ë ˆì´ì–´ì—ì„œ ì˜¤ëŠ” ì…ë ¥ ë¶„í• \n+            self.output_layouts = (output_layouts or Shard(-1),) # ì›í•˜ëŠ” ì¶œë ¥ ë¶„í• \n+            self.desired_input_layouts = (Replicate(),) # ì›í•˜ëŠ” ì…ë ¥ ë¶„í• , ì…ë ¥ì€ GPU ê°„ì— ë³µì œë˜ì–´ì•¼ í•¨\n+            self.use_local_output = use_local_output\n+            self.use_dtensor = use_dtensor\n+    ```\n+\n+2. `partition_tensor`, `_prepare_input_fn`, `_prepare_output_fn` ë©”ì„œë“œë¥¼ êµ¬í˜„í•©ë‹ˆë‹¤.\n+\n+    `partition_tensor` ë©”ì†Œë“œëŠ” í…ì„œë¥¼ ë¶„í• í•˜ê³  ë¶„í• ëœ í…ì„œë¡œ `empty_param`ì„ ì±„ì›ë‹ˆë‹¤. ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ `get_tensor_shard`ë¥¼ ì‚¬ìš©í•˜ì—¬ ì£¼ì–´ì§„ ë­í¬ì— ëŒ€í•œ ì›ë³¸ ë§¤ê°œë³€ìˆ˜ì˜ ì˜¬ë°”ë¥¸ ë¶„í• ì„ ì–»ê³ , íŒ¨í‚¹ëœ ê°€ì¤‘ì¹˜ì— ëŒ€í•´ì„œëŠ” `get_packed_weights`ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”.\n+\n+    ```python\n+    def partition_tensor(\n+        self,\n+        param, # ë§¤ê°œë³€ìˆ˜ì˜ ì „ì²´ í…ì„œ\n+        empty_param, # ë§¤ê°œë³€ìˆ˜ì˜ ë¹ˆ í…ì„œ, ë¶„í• ëœ í…ì„œë¡œ ì±„ì›Œì§\n+        param_type, # ë§¤ê°œë³€ìˆ˜ ìœ í˜•, `bias` ë˜ëŠ” `weight`\n+        param_casting_dtype, # ë§¤ê°œë³€ìˆ˜ë¥¼ ìºìŠ¤íŒ…í•  ìœ í˜•\n+        to_contiguous, # í…ì„œë¥¼ ì—°ì†ì ì¸ ë©”ëª¨ë¦¬ ë ˆì´ì•„ì›ƒìœ¼ë¡œ ë³€í™˜í• ì§€ ì—¬ë¶€\n+        rank, # í˜„ì¬ ê¸°ê¸°ì˜ ë­í¬\n+        device_mesh, # ê¸°ê¸° ë©”ì‹œ\n+    ) -> nn.Parameter: # ë¶„í• ëœ ë§¤ê°œë³€ìˆ˜ ë°˜í™˜\n+        ...\n+    ```\n+\n+    `_prepare_input_fn`ê³¼ `_prepare_output_fn` ë©”ì†Œë“œëŠ” [ì‚¬ì „ í¬ì›Œë“œ](https://docs.pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_pre_hook.html) ë° [í¬ì›Œë“œ](https://docs.pytorch.org/docs/stable/generated/torch.nn.modules.module.register_module_forward_hook.html) í›…ì—ì„œ ì‚¬ìš©ë©ë‹ˆë‹¤. `__init__`ì—ì„œ ì§€ì •ëœ ëŒ€ë¡œ ì…ë ¥ê³¼ ì¶œë ¥ì„ ì›í•˜ëŠ” ë ˆì´ì•„ì›ƒìœ¼ë¡œ ì¬ë¶„ë°°í•©ë‹ˆë‹¤.\n+\n+    ```python\n+    def _prepare_input_fn(input_layouts, desired_input_layouts, mod, inputs, device_mesh):\n+        ...\n+        # ì‚¬ìš©ì ì •ì˜ ë¡œì§ ìˆ˜í–‰, DTensorë¡œ ìºìŠ¤íŒ… ë“±.\n+        ...\n+        return inputs.redistribute(placements=desired_input_layouts, device_mesh=device_mesh)\n+    def _prepare_output_fn(output_layouts, use_local_output, mod, outputs, device_mesh):\n+        ...\n+        # ì‚¬ìš©ì ì •ì˜ ë¡œì§ ìˆ˜í–‰, DTensorë¡œ ìºìŠ¤íŒ… ë“±.\n+        ...\n+        return outputs.redistribute(placements=output_layouts, device_mesh=device_mesh)\n+    ```\n+\n+3. `tp_plan`ê³¼ í•¨ê»˜ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ì „ëµì„ [`ParallelInterface`]ì— ë“±ë¡í•©ë‹ˆë‹¤.\n+\n+    ```python\n+    from transformers.integrations.tensor_parallel import ParallelInterface\n+\n+    ParallelInterface.register_strategy(\"colwise_custom\", ColwiseParallel)\n+    tp_plan = {\n+        \"model.layers.*.self_attn.q_proj\": \"colwise_custom\",\n+        ...\n+    }\n+    model = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.bfloat16, tp_plan=tp_plan)\n+    ```\n+\n+## ë²¤ì¹˜ë§ˆí¬[[benchmarks]]\n+\n+í…ì„œ ë³‘ë ¬í™”ëŠ” íŠ¹íˆ í° ë°°ì¹˜ í¬ê¸°ë‚˜ ê¸´ ì‹œí€€ìŠ¤ë¥¼ ê°€ì§„ ì…ë ¥ì— ëŒ€í•œ ì¶”ë¡  ì†ë„ë¥¼ í¬ê²Œ í–¥ìƒì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 512ì¸ [Llama](./model_doc/llama)ì—ì„œ ë‹¨ì¼ í¬ì›Œë“œ íŒ¨ìŠ¤ì— ëŒ€í•œ ì˜ˆìƒ ì†ë„ í–¥ìƒ ìˆ˜ì¹˜ëŠ” ì•„ë˜ ì°¨íŠ¸ë¥¼ ì°¸ì¡°í•˜ì„¸ìš”.\n+\n+<div style=\"text-align: center\">\n+    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/Meta-Llama-3-8B-Instruct%2C%20seqlen%20%3D%20512%2C%20python%2C%20w_%20compile.png\">\n+</div>\n+\n+## ì„¤ê³„ êµ¬í˜„[[design-implementation]]\n+\n+Transformers í…ì„œ ë³‘ë ¬í™” êµ¬í˜„ì€ í”„ë ˆì„ì›Œí¬ì— êµ¬ì• ë°›ì§€ ì•Šì§€ë§Œ, êµ¬ì²´ì ì¸ êµ¬í˜„ì„ ìœ„í•´ì„œëŠ” [DeviceMesh](https://docs.pytorch.org/tutorials/recipes/distributed_device_mesh.html)ì™€ [torch.distributed](https://docs.pytorch.org/tutorials/beginner/dist_overview.html)ì˜ [DTensor](https://docs.pytorch.org/docs/stable/distributed.tensor.html)ì— ì˜ì¡´í•˜ì—¬ ê°„ë‹¨í•˜ê³  í™•ì¥ ê°€ëŠ¥í•œ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì œê³µí•©ë‹ˆë‹¤.\n+\n+### DeviceMesh[[devicemesh]]\n+\n+`DeviceMesh`ë¥¼ í•¨ê»˜ í†µì‹ í•˜ëŠ” ê¸°ê¸°ë“¤ì˜ ë‹¤ì°¨ì› ê·¸ë¦¬ë“œë¡œ ìƒìƒí•´ë³´ì„¸ìš”. ë³‘ë ¬ ì²˜ë¦¬ ì „ëµë§ˆë‹¤ ê°ê¸° ë‹¤ë¥¸ í†µì‹  íŒ¨í„´ì´ í•„ìš”í•˜ë¯€ë¡œ, ì—¬ëŸ¬ í•˜ìœ„ ë©”ì‹œë¥¼ ê°€ì§„ `DeviceMesh`ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+```python\n+from torch.distributed.device_mesh import init_device_mesh\n+\n+# 4ê°œ GPUì˜ 1D ë©”ì‹œ ìƒì„±\n+device_mesh = init_device_mesh(\"cuda\", (4,), mesh_dim_names=[\"tp\"])\n+```\n+\n+`torch.distributed`ì—ì„œ ì •ì˜ëœ ëŒ€ë¶€ë¶„ì˜ ë³‘ë ¬í™” ì „ëµì€ ë©”ì‹œ ìì²´ë‚˜ í•˜ìœ„ ë©”ì‹œì— ì ìš©í•  ìˆ˜ ìˆìœ¼ë©°, ìë™ìœ¼ë¡œ í†µì‹  íŒ¨í„´ì„ ì²˜ë¦¬í•©ë‹ˆë‹¤.\n+\n+### DTensor[[dtensor]]\n+\n+`DTensor`(ë¶„ì‚° í…ì„œ)ëŠ” ì¼ë°˜ì ì¸ í…ì„œ ì—°ì‚° ìœ„ì— ë¶„ì‚° ë¡œì§ì„ ì²˜ë¦¬í•˜ëŠ” í…ì„œ í•˜ìœ„ í´ë˜ìŠ¤ì…ë‹ˆë‹¤. í…ì„œ ë³‘ë ¬í™”ì˜ ëŒ€ë¶€ë¶„ì˜ ëª¨ë¸ ê°€ì¤‘ì¹˜ëŠ” `DTensor` í˜•íƒœë¡œ ì €ì¥ë©ë‹ˆë‹¤.\n+\n+DTensorì˜ ê°€ì¥ ì¤‘ìš”í•œ ë¶€ë¶„ì€ `placement` ì†ì„±ì…ë‹ˆë‹¤. ì´ëŠ” PyTorchì—ê²Œ í…ì„œê°€ `DeviceMesh`ì˜ ê¸°ê¸°ì— ì–´ë–»ê²Œ ë°°ì¹˜ë˜ëŠ”ì§€ ì•Œë ¤ì£¼ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. `placement` ì†ì„±ì€ ë‹¤ìŒ ê°’ì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+- `Shard(dimension)` - `DTensor`ê°€ êµ¬ì„±ëœ `DeviceMesh`ì—ì„œ ì£¼ì–´ì§„ ì°¨ì›ì— ê±¸ì³ ì–´ë–»ê²Œ ë¶„í• ë˜ëŠ”ì§€ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì•„ë˜ ì˜ˆì‹œëŠ” ì—´ ë°©í–¥ ë¶„í• ì„ ìœ„í•´ ë‹¤ì–‘í•œ ì°¨ì›ì— ê±¸ì³ ê°€ì¤‘ì¹˜ë¥¼ ë¶„í• í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+\n+    ```python\n+    weight = ...\n+    weight = DTensor.from_local(weight, device_mesh[\"tp\"], placements=[Shard(0)]) # ì²« ë²ˆì§¸(ì—´ ë°©í–¥) ì°¨ì›ì— ê±¸ì³ ë¶„í• \n+    bias = ...\n+    bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Shard(-1)]) # ìœ ì¼í•œ ì°¨ì›ì— ê±¸ì³ ë¶„í• \n+    ```\n+\n+    ì´ ì˜ˆì‹œëŠ” í–‰ ë°©í–¥ ë¶„í• ì„ ìœ„í•´ ì—¬ëŸ¬ ì°¨ì›ì— ê±¸ì³ ê°€ì¤‘ì¹˜ë¥¼ ë¶„í• í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+\n+    ```python\n+    weight = ...\n+    weight = DTensor.from_local(weight, device_mesh[\"tp\"], placements=[Shard(1)]) # ë‘ ë²ˆì§¸(í–‰ ë°©í–¥) ì°¨ì›ì— ê±¸ì³ ë¶„í• \n+    bias = ...\n+    bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Replicate()]) # ëª¨ë“  GPUì— í¸í–¥ ë³µì œ\n+    ```\n+\n+- `Replicate()` - `DTensor`ê°€ `DeviceMesh`ì— ê±¸ì³ ë³µì œë¨ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ê° ê¸°ê¸°ì— í…ì„œì˜ ì „ì²´ ì‚¬ë³¸ë§Œ ìƒì„±í•©ë‹ˆë‹¤.\n+\n+    ```py\n+    bias = ...\n+    bias = DTensor.from_local(bias, device_mesh[\"tp\"], placements=[Replicate()]) # ëª¨ë“  GPUì— í¸í–¥ ë³µì œ\n+    ```\n+\n+- `Partial()` - í…ì„œê°€ ê°ì†Œ ì—°ì‚°ì„ ê¸°ë‹¤ë¦¬ê³  ìˆëŠ” ìƒíƒœì„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤ (ì¼ë°˜ì ìœ¼ë¡œ Transformersì—ì„œì˜ ì‚¬ìš© ì‚¬ë¡€ì™€ëŠ” ì§ì ‘ì ì¸ ê´€ë ¨ì´ ì ìŠµë‹ˆë‹¤)."
        }
    ],
    "stats": {
        "total": 313,
        "additions": 313,
        "deletions": 0
    }
}