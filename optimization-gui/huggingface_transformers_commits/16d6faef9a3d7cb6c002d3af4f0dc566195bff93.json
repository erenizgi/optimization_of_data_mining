{
    "author": "qubvel",
    "message": "[core] Fix attn_implementation setter with missing `sub_configs` (#39855)\n\n* fix\n\n* add sub_configs\n\n* remove case for attention setter\n\n* fix None\n\n* Add test\n\n* Fix sub-configs\n\n* fix tests_config\n\n* fix consistency\n\n* fix fsmt\n\n* fix",
    "sha": "16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
    "files": [
        {
            "sha": "26a8ed05140c681c8eb9a97d6d34226afab26719",
            "filename": "src/transformers/models/conditional_detr/configuration_conditional_detr.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fconditional_detr%2Fconfiguration_conditional_detr.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -253,6 +253,14 @@ def num_attention_heads(self) -> int:\n     def hidden_size(self) -> int:\n         return self.d_model\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n \n class ConditionalDetrOnnxConfig(OnnxConfig):\n     torch_onnx_minimum_version = version.parse(\"1.11\")"
        },
        {
            "sha": "7484d9a347e534f3ebfff5a0776a0413a5a416dc",
            "filename": "src/transformers/models/d_fine/configuration_d_fine.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fconfiguration_d_fine.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -404,6 +404,14 @@ def num_attention_heads(self) -> int:\n     def hidden_size(self) -> int:\n         return self.d_model\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n     @classmethod\n     def from_backbone_configs(cls, backbone_config: PretrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`DFineConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model"
        },
        {
            "sha": "883f07a9979b213d1c536549ea5e7cb6f90e8e50",
            "filename": "src/transformers/models/d_fine/modular_d_fine.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodular_d_fine.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -423,6 +423,14 @@ def num_attention_heads(self) -> int:\n     def hidden_size(self) -> int:\n         return self.d_model\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n     @classmethod\n     def from_backbone_configs(cls, backbone_config: PretrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`DFineConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model"
        },
        {
            "sha": "e53d7783a6f431feafb8a3946b85118c5c436858",
            "filename": "src/transformers/models/dab_detr/configuration_dab_detr.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdab_detr%2Fconfiguration_dab_detr.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -256,5 +256,13 @@ def __init__(\n         self.initializer_bias_prior_prob = initializer_bias_prior_prob\n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n \n __all__ = [\"DabDetrConfig\"]"
        },
        {
            "sha": "b85a7399908d4f25ce220aa1edab4b0e74c9bdb3",
            "filename": "src/transformers/models/deformable_detr/configuration_deformable_detr.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeformable_detr%2Fconfiguration_deformable_detr.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -278,5 +278,13 @@ def num_attention_heads(self) -> int:\n     def hidden_size(self) -> int:\n         return self.d_model\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n \n __all__ = [\"DeformableDetrConfig\"]"
        },
        {
            "sha": "2109902ac06546ce4e235403dd3b95bf27486272",
            "filename": "src/transformers/models/deprecated/deta/configuration_deta.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fdeta%2Fconfiguration_deta.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -266,5 +266,13 @@ def num_attention_heads(self) -> int:\n     def hidden_size(self) -> int:\n         return self.d_model\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n \n __all__ = [\"DetaConfig\"]"
        },
        {
            "sha": "e96c6c4a1b58cd40e86dfac71dfcb9c0eb19f57c",
            "filename": "src/transformers/models/deprecated/vit_hybrid/configuration_vit_hybrid.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fvit_hybrid%2Fconfiguration_vit_hybrid.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -168,5 +168,13 @@ def __init__(\n         self.num_channels = num_channels\n         self.qkv_bias = qkv_bias\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n \n __all__ = [\"ViTHybridConfig\"]"
        },
        {
            "sha": "65884fe67c3cc85afe5b474eb5756665b8b20e11",
            "filename": "src/transformers/models/depth_anything/configuration_depth_anything.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdepth_anything%2Fconfiguration_depth_anything.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -151,6 +151,14 @@ def __init__(\n         self.depth_estimation_type = depth_estimation_type\n         self.max_depth = max_depth if max_depth else 1\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n     def to_dict(self):\n         \"\"\"\n         Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`]. Returns:"
        },
        {
            "sha": "c9540382927cf2c9849b188ed4b53dea3315f4c4",
            "filename": "src/transformers/models/detr/configuration_detr.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdetr%2Fconfiguration_detr.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -252,6 +252,14 @@ def num_attention_heads(self) -> int:\n     def hidden_size(self) -> int:\n         return self.d_model\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n     @classmethod\n     def from_backbone_config(cls, backbone_config: PretrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`DetrConfig`] (or a derived class) from a pre-trained backbone model configuration."
        },
        {
            "sha": "390e6f1ab5065e1be0b3abac472e2e57b351b2a1",
            "filename": "src/transformers/models/fsmt/configuration_fsmt.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffsmt%2Fconfiguration_fsmt.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -28,8 +28,8 @@ class DecoderConfig(PretrainedConfig):\n \n     model_type = \"fsmt_decoder\"\n \n-    def __init__(self, vocab_size=0, bos_token_id=0, is_encoder_decoder=True):\n-        super().__init__()\n+    def __init__(self, vocab_size=0, bos_token_id=0, is_encoder_decoder=True, **kwargs):\n+        super().__init__(**kwargs)\n         self.vocab_size = vocab_size\n         self.bos_token_id = bos_token_id\n         self.is_encoder_decoder = is_encoder_decoder\n@@ -134,6 +134,7 @@ class FSMTConfig(PretrainedConfig):\n \n     model_type = \"fsmt\"\n     attribute_map = {\"num_attention_heads\": \"encoder_attention_heads\", \"hidden_size\": \"d_model\"}\n+    sub_configs = {\"decoder\": DecoderConfig}\n \n     # update the defaults from config file\n     def __init__("
        },
        {
            "sha": "838a897f70afcc099df80de3d9058173a36a0b3e",
            "filename": "src/transformers/models/grounding_dino/configuration_grounding_dino.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgrounding_dino%2Fconfiguration_grounding_dino.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -294,5 +294,16 @@ def num_attention_heads(self) -> int:\n     def hidden_size(self) -> int:\n         return self.d_model\n \n+    @property\n+    def sub_configs(self):\n+        sub_configs = {}\n+        backbone_config = getattr(self, \"backbone_config\", None)\n+        text_config = getattr(self, \"text_config\", None)\n+        if isinstance(backbone_config, PretrainedConfig):\n+            sub_configs[\"backbone_config\"] = type(backbone_config)\n+        if isinstance(text_config, PretrainedConfig):\n+            sub_configs[\"text_config\"] = type(self.text_config)\n+        return sub_configs\n+\n \n __all__ = [\"GroundingDinoConfig\"]"
        },
        {
            "sha": "9ae93892aebd4a071e3e0d5a17fdbe35d1eb7d2d",
            "filename": "src/transformers/models/mask2former/configuration_mask2former.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmask2former%2Fconfiguration_mask2former.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -236,6 +236,14 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n     @classmethod\n     def from_backbone_config(cls, backbone_config: PretrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`Mask2FormerConfig`] (or a derived class) from a pre-trained backbone model configuration."
        },
        {
            "sha": "d988acb45e95fae720c178165c911dc410ad1748",
            "filename": "src/transformers/models/maskformer/configuration_maskformer.py",
            "status": "modified",
            "additions": 9,
            "deletions": 0,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmaskformer%2Fconfiguration_maskformer.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -200,6 +200,15 @@ def __init__(\n         self.backbone_kwargs = backbone_kwargs\n         super().__init__(**kwargs)\n \n+    @property\n+    def sub_configs(self):\n+        sub_configs = {}\n+        if self.backbone_config is not None and self.backbone_config != {}:\n+            sub_configs[\"backbone_config\"] = type(self.backbone_config)\n+        if self.decoder_config is not None and self.decoder_config != {}:\n+            sub_configs[\"decoder_config\"] = type(self.decoder_config)\n+        return sub_configs\n+\n     @classmethod\n     def from_backbone_and_decoder_configs(\n         cls, backbone_config: PretrainedConfig, decoder_config: PretrainedConfig, **kwargs"
        },
        {
            "sha": "193faecf39c6c22f5647abc96a38834d592973b7",
            "filename": "src/transformers/models/mm_grounding_dino/configuration_mm_grounding_dino.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmm_grounding_dino%2Fconfiguration_mm_grounding_dino.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -288,5 +288,16 @@ def num_attention_heads(self) -> int:\n     def hidden_size(self) -> int:\n         return self.d_model\n \n+    @property\n+    def sub_configs(self):\n+        sub_configs = {}\n+        backbone_config = getattr(self, \"backbone_config\", None)\n+        text_config = getattr(self, \"text_config\", None)\n+        if isinstance(backbone_config, PretrainedConfig):\n+            sub_configs[\"backbone_config\"] = type(backbone_config)\n+        if isinstance(text_config, PretrainedConfig):\n+            sub_configs[\"text_config\"] = type(self.text_config)\n+        return sub_configs\n+\n \n __all__ = [\"MMGroundingDinoConfig\"]"
        },
        {
            "sha": "e11cc563db13ec69ed9e7c175541c91f41f4dbf2",
            "filename": "src/transformers/models/omdet_turbo/configuration_omdet_turbo.py",
            "status": "modified",
            "additions": 11,
            "deletions": 0,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fomdet_turbo%2Fconfiguration_omdet_turbo.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -289,5 +289,16 @@ def __init__(\n \n         super().__init__(is_encoder_decoder=is_encoder_decoder, **kwargs)\n \n+    @property\n+    def sub_configs(self):\n+        sub_configs = {}\n+        backbone_config = getattr(self, \"backbone_config\", None)\n+        text_config = getattr(self, \"text_config\", None)\n+        if isinstance(backbone_config, PretrainedConfig):\n+            sub_configs[\"backbone_config\"] = type(backbone_config)\n+        if isinstance(text_config, PretrainedConfig):\n+            sub_configs[\"text_config\"] = type(text_config)\n+        return sub_configs\n+\n \n __all__ = [\"OmDetTurboConfig\"]"
        },
        {
            "sha": "1b9229f040d11c2ca2ea1e18f0fdfd927e159e14",
            "filename": "src/transformers/models/oneformer/configuration_oneformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Foneformer%2Fconfiguration_oneformer.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -273,5 +273,13 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n \n __all__ = [\"OneFormerConfig\"]"
        },
        {
            "sha": "89109350d95a0c6e8d65c2611d75b1c4c29b7211",
            "filename": "src/transformers/models/pix2struct/configuration_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -293,6 +293,7 @@ class Pix2StructConfig(PretrainedConfig):\n     ```\"\"\"\n \n     model_type = \"pix2struct\"\n+    sub_configs = {\"text_config\": Pix2StructTextConfig, \"vision_config\": Pix2StructVisionConfig}\n \n     def __init__(\n         self,"
        },
        {
            "sha": "a45ae66cf3b7bbc7ba72f462ecffbd182547947c",
            "filename": "src/transformers/models/prompt_depth_anything/configuration_prompt_depth_anything.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprompt_depth_anything%2Fconfiguration_prompt_depth_anything.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -154,6 +154,14 @@ def __init__(\n         self.depth_estimation_type = depth_estimation_type\n         self.max_depth = max_depth if max_depth else 1\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n     def to_dict(self):\n         \"\"\"\n         Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`]. Returns:"
        },
        {
            "sha": "994d4a6fd6f02a9c11817c3ad1bdcdd83a14bb24",
            "filename": "src/transformers/models/rt_detr/configuration_rt_detr.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fconfiguration_rt_detr.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -343,6 +343,14 @@ def num_attention_heads(self) -> int:\n     def hidden_size(self) -> int:\n         return self.d_model\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n     @classmethod\n     def from_backbone_configs(cls, backbone_config: PretrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`RTDetrConfig`] (or a derived class) from a pre-trained backbone model configuration and DETR model"
        },
        {
            "sha": "6f4a53483b1f7f0f10a2cf208df241165d2e9411",
            "filename": "src/transformers/models/rt_detr_v2/configuration_rt_detr_v2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fconfiguration_rt_detr_v2.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -358,6 +358,14 @@ def __init__(\n         self.decoder_offset_scale = decoder_offset_scale\n         self.decoder_method = decoder_method\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n     @classmethod\n     def from_backbone_configs(cls, backbone_config: PretrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`RTDetrV2Config`] (or a derived class) from a pre-trained backbone model configuration and DETR model"
        },
        {
            "sha": "9c8477c86e0f2b11e68c46f3a4c71430784a7c2e",
            "filename": "src/transformers/models/rt_detr_v2/modular_rt_detr_v2.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr_v2%2Fmodular_rt_detr_v2.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -369,6 +369,14 @@ def __init__(\n         self.decoder_offset_scale = decoder_offset_scale\n         self.decoder_method = decoder_method\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n     @classmethod\n     def from_backbone_configs(cls, backbone_config: PretrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`RTDetrV2Config`] (or a derived class) from a pre-trained backbone model configuration and DETR model"
        },
        {
            "sha": "74bd991f95e70867d505e39d1826209fc8b3bf34",
            "filename": "src/transformers/models/superglue/configuration_superglue.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsuperglue%2Fconfiguration_superglue.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -114,5 +114,9 @@ def __init__(\n \n         super().__init__(**kwargs)\n \n+    @property\n+    def sub_configs(self):\n+        return {\"keypoint_detector_config\": type(self.keypoint_detector_config)}\n+\n \n __all__ = [\"SuperGlueConfig\"]"
        },
        {
            "sha": "32eed6ce0df168c0f6a71c0bd30d19f42d47729d",
            "filename": "src/transformers/models/table_transformer/configuration_table_transformer.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftable_transformer%2Fconfiguration_table_transformer.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -253,6 +253,14 @@ def num_attention_heads(self) -> int:\n     def hidden_size(self) -> int:\n         return self.d_model\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n \n # Copied from transformers.models.detr.configuration_detr.DetrOnnxConfig\n class TableTransformerOnnxConfig(OnnxConfig):"
        },
        {
            "sha": "be7c785084d17c4e35720ee353bb1b217eff95f7",
            "filename": "src/transformers/models/tvp/configuration_tvp.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftvp%2Fconfiguration_tvp.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -172,6 +172,14 @@ def __init__(\n         self.initializer_range = initializer_range\n         self.attention_probs_dropout_prob = attention_probs_dropout_prob\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n     @classmethod\n     def from_backbone_config(cls, backbone_config: PretrainedConfig, **kwargs):\n         \"\"\"Instantiate a [`TvpConfig`] (or a derived class) from a pre-trained backbone model configuration."
        },
        {
            "sha": "d116b22fcfba681d5a1b25d52a597ab6b28f9362",
            "filename": "src/transformers/models/upernet/configuration_upernet.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fupernet%2Fconfiguration_upernet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fupernet%2Fconfiguration_upernet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fupernet%2Fconfiguration_upernet.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -136,5 +136,13 @@ def __init__(\n         self.auxiliary_concat_input = auxiliary_concat_input\n         self.loss_ignore_index = loss_ignore_index\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n \n __all__ = [\"UperNetConfig\"]"
        },
        {
            "sha": "f63c3e4eb85dfbe77dd03abdae6b5ae2e5a90954",
            "filename": "src/transformers/models/vitmatte/configuration_vitmatte.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitmatte%2Fconfiguration_vitmatte.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -121,6 +121,14 @@ def __init__(\n         self.convstream_hidden_sizes = convstream_hidden_sizes\n         self.fusion_hidden_sizes = fusion_hidden_sizes\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n     def to_dict(self):\n         \"\"\"\n         Serializes this instance to a Python dictionary. Override the default [`~PretrainedConfig.to_dict`]. Returns:"
        },
        {
            "sha": "777e3d3c60cca9b5d955463573a52f118a639197",
            "filename": "src/transformers/models/vitpose/configuration_vitpose.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvitpose%2Fconfiguration_vitpose.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -122,5 +122,13 @@ def __init__(\n         self.scale_factor = scale_factor\n         self.use_simple_decoder = use_simple_decoder\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n \n __all__ = [\"VitPoseConfig\"]"
        },
        {
            "sha": "ac89f815f82cca720838257c190bfd0bb034d60d",
            "filename": "src/transformers/models/zoedepth/configuration_zoedepth.py",
            "status": "modified",
            "additions": 8,
            "deletions": 0,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzoedepth%2Fconfiguration_zoedepth.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -233,5 +233,13 @@ def __init__(\n         self.patch_transformer_intermediate_size = patch_transformer_intermediate_size\n         self.patch_transformer_num_attention_heads = patch_transformer_num_attention_heads\n \n+    @property\n+    def sub_configs(self):\n+        return (\n+            {\"backbone_config\": type(self.backbone_config)}\n+            if getattr(self, \"backbone_config\", None) is not None\n+            else {}\n+        )\n+\n \n __all__ = [\"ZOEDEPTH_PRETRAINED_CONFIG_ARCHIVE_MAP\", \"ZoeDepthConfig\"]"
        },
        {
            "sha": "c47e23cc3ae5ad9ac7ed5afa2533102e980967ce",
            "filename": "tests/test_configuration_common.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/tests%2Ftest_configuration_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/tests%2Ftest_configuration_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_configuration_common.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -141,6 +141,7 @@ def create_and_test_config_from_and_save_pretrained_composite(self):\n                 # Verify that loading with subconfig class results in same dict as if we loaded with general composite config class\n                 sub_config_loaded_dict = sub_config_loaded.to_dict()\n                 sub_config_loaded_dict.pop(\"transformers_version\", None)\n+                general_config_dict[sub_config_key].pop(\"transformers_version\", None)\n                 self.parent.assertEqual(sub_config_loaded_dict, general_config_dict[sub_config_key])\n \n                 # Verify that the loaded config type is same as in the general config"
        },
        {
            "sha": "8081317505e8a4f54a61721673fe0cfe38ee2a17",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 19,
            "deletions": 0,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/16d6faef9a3d7cb6c002d3af4f0dc566195bff93/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=16d6faef9a3d7cb6c002d3af4f0dc566195bff93",
            "patch": "@@ -4812,6 +4812,25 @@ def test_can_load_with_meta_device_context_manager(self):\n                 f\"All parameters should be on meta device, but found {unique_devices}.\",\n             )\n \n+    def test_config_attn_implementation_setter(self):\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        def check_attn_implementation_setter(config: PretrainedConfig, attn_implementation: str):\n+            if not config._attn_implementation == attn_implementation:\n+                raise ValueError(\n+                    f\"Unexpected attn_implementation for config {config.__class__.__name__}: \"\n+                    f\"{config._attn_implementation} != {attn_implementation}\"\n+                )\n+            for attribute_value in config.__dict__.values():\n+                if isinstance(attribute_value, PretrainedConfig):\n+                    check_attn_implementation_setter(attribute_value, attn_implementation)\n+\n+        config._attn_implementation = \"eager\"\n+        check_attn_implementation_setter(config, \"eager\")\n+\n+        config._attn_implementation = \"sdpa\"\n+        check_attn_implementation_setter(config, \"sdpa\")\n+\n     def test_internal_model_config_and_subconfig_are_same(self):\n         config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n         subconfig_keys = list(config.sub_configs.keys())"
        }
    ],
    "stats": {
        "total": 240,
        "additions": 238,
        "deletions": 2
    }
}