{
    "author": "merveenoyan",
    "message": "Add DINOv3Backbone for ConvNext variant (#40651)\n\n\n---------\n\nCo-authored-by: Pavel Iakubovskii <qubvel@gmail.com>",
    "sha": "3648fde4864600daea5d0d6af33e99effc5af82e",
    "files": [
        {
            "sha": "38923278cc79920b55c862d3229e48929e249de3",
            "filename": "docs/source/en/model_doc/dinov3.md",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/3648fde4864600daea5d0d6af33e99effc5af82e/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/3648fde4864600daea5d0d6af33e99effc5af82e/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fdinov3.md?ref=3648fde4864600daea5d0d6af33e99effc5af82e",
            "patch": "@@ -178,3 +178,8 @@ print(\"Pooled output shape:\", pooled_output.shape)\n \n [[autodoc]] DINOv3ViTImageProcessorFast\n     - preprocess\n+\n+## DINOv3ConvNextBackbone\n+\n+[[autodoc]] DINOv3ConvNextBackbone\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "197029464efd28feafd144c190bf0d48b6aca217",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/3648fde4864600daea5d0d6af33e99effc5af82e/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3648fde4864600daea5d0d6af33e99effc5af82e/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=3648fde4864600daea5d0d6af33e99effc5af82e",
            "patch": "@@ -1699,6 +1699,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"dinat\", \"DinatBackbone\"),\n         (\"dinov2\", \"Dinov2Backbone\"),\n         (\"dinov2_with_registers\", \"Dinov2WithRegistersBackbone\"),\n+        (\"dinov3_convnext\", \"DINOv3ConvNextBackbone\"),\n         (\"focalnet\", \"FocalNetBackbone\"),\n         (\"hgnet_v2\", \"HGNetV2Backbone\"),\n         (\"hiera\", \"HieraBackbone\"),"
        },
        {
            "sha": "6024ce318eec8ebd402147987469b12407b1fd29",
            "filename": "src/transformers/models/dinov3_convnext/configuration_dinov3_convnext.py",
            "status": "modified",
            "additions": 18,
            "deletions": 1,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/3648fde4864600daea5d0d6af33e99effc5af82e/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconfiguration_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3648fde4864600daea5d0d6af33e99effc5af82e/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconfiguration_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fconfiguration_dinov3_convnext.py?ref=3648fde4864600daea5d0d6af33e99effc5af82e",
            "patch": "@@ -18,12 +18,13 @@\n \n from ...configuration_utils import PreTrainedConfig\n from ...utils import logging\n+from ...utils.backbone_utils import BackboneConfigMixin, get_aligned_output_features_output_indices\n \n \n logger = logging.get_logger(__name__)\n \n \n-class DINOv3ConvNextConfig(PreTrainedConfig):\n+class DINOv3ConvNextConfig(BackboneConfigMixin, PreTrainedConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`DINOv3ConvNextModel`]. It is used to instantiate an\n     DINOv3ConvNext model according to the specified arguments, defining the model architecture. Instantiating a configuration\n@@ -53,6 +54,16 @@ class DINOv3ConvNextConfig(PreTrainedConfig):\n             The drop rate for stochastic depth.\n         image_size (`int`, *optional*, defaults to 224):\n             The size (resolution) of input images.\n+        out_features (`list[str]`, *optional*):\n+            If used as backbone, list of features to output. Can be any of `\"stem\"`, `\"stage1\"`, `\"stage2\"`, etc.\n+            (depending on how many stages the model has). If unset and `out_indices` is set, will default to the\n+            corresponding stages. If unset and `out_indices` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n+        out_indices (`list[int]`, *optional*):\n+            If used as backbone, list of indices of features to output. Can be any of 0, 1, 2, etc. (depending on how\n+            many stages the model has). If unset and `out_features` is set, will default to the corresponding stages.\n+            If unset and `out_features` is unset, will default to the last stage. Must be in the\n+            same order as defined in the `stage_names` attribute.\n \n     Example:\n     ```python\n@@ -81,6 +92,8 @@ def __init__(\n         layer_scale_init_value: float = 1e-6,\n         drop_path_rate: float = 0.0,\n         image_size: int = 224,\n+        out_features: Optional[list[str]] = None,\n+        out_indices: Optional[list[int]] = None,\n         **kwargs,\n     ):\n         super().__init__(**kwargs)\n@@ -94,6 +107,10 @@ def __init__(\n         self.layer_scale_init_value = layer_scale_init_value\n         self.drop_path_rate = drop_path_rate\n         self.image_size = image_size\n+        self.stage_names = [\"stem\"] + [f\"stage{idx}\" for idx in range(1, len(self.depths) + 1)]\n+        self._out_features, self._out_indices = get_aligned_output_features_output_indices(\n+            out_features=out_features, out_indices=out_indices, stage_names=self.stage_names\n+        )\n \n     @property\n     def num_stages(self) -> int:"
        },
        {
            "sha": "d5d741f660815558df38c4b39ffa8430aea2a3fd",
            "filename": "src/transformers/models/dinov3_convnext/modeling_dinov3_convnext.py",
            "status": "modified",
            "additions": 50,
            "deletions": 4,
            "changes": 54,
            "blob_url": "https://github.com/huggingface/transformers/blob/3648fde4864600daea5d0d6af33e99effc5af82e/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3648fde4864600daea5d0d6af33e99effc5af82e/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdinov3_convnext%2Fmodeling_dinov3_convnext.py?ref=3648fde4864600daea5d0d6af33e99effc5af82e",
            "patch": "@@ -21,11 +21,10 @@\n from torch import nn\n \n from ...activations import ACT2FN\n-from ...modeling_outputs import (\n-    BaseModelOutputWithPoolingAndNoAttention,\n-)\n+from ...modeling_outputs import BackboneOutput, BaseModelOutputWithPoolingAndNoAttention\n from ...modeling_utils import PreTrainedModel\n from ...utils import auto_docstring, logging\n+from ...utils.backbone_utils import BackboneMixin\n from ...utils.generic import can_return_tuple\n from .configuration_dinov3_convnext import DINOv3ConvNextConfig\n \n@@ -250,4 +249,51 @@ def forward(\n         )\n \n \n-__all__ = [\"DINOv3ConvNextModel\", \"DINOv3ConvNextPreTrainedModel\"]\n+@auto_docstring\n+class DINOv3ConvNextBackbone(DINOv3ConvNextPreTrainedModel, BackboneMixin):\n+    config: DINOv3ConvNextConfig\n+\n+    def __init__(self, config: DINOv3ConvNextConfig):\n+        super().__init__(config)\n+        super()._init_backbone(config)\n+\n+        self.num_features = [config.num_channels] + list(config.hidden_sizes)\n+\n+        self.stages = nn.ModuleList([DINOv3ConvNextStage(config, s) for s in range(config.num_stages)])\n+\n+        self.post_init()\n+\n+    def get_input_embeddings(self):\n+        return None\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        pixel_values: torch.FloatTensor,\n+        output_hidden_states: Optional[bool] = None,\n+        **kwargs,\n+    ) -> BackboneOutput:\n+        if output_hidden_states is None:\n+            output_hidden_states = self.config.output_hidden_states\n+\n+        hidden_states = pixel_values\n+        all_hidden_states: list[torch.Tensor] = [hidden_states]\n+\n+        for stage in self.stages:\n+            hidden_states = stage(hidden_states)\n+            all_hidden_states.append(hidden_states)\n+\n+        # hidden_states are already in NCHW (batch_size, channels, height, width) format\n+        feature_maps: list[torch.Tensor] = []\n+        for stage, hidden_states in zip(self.stage_names, all_hidden_states):\n+            if stage in self.out_features:\n+                feature_maps.append(hidden_states)\n+\n+        return BackboneOutput(\n+            feature_maps=tuple(feature_maps),\n+            hidden_states=tuple(all_hidden_states) if output_hidden_states else None,\n+        )\n+\n+\n+__all__ = [\"DINOv3ConvNextModel\", \"DINOv3ConvNextPreTrainedModel\", \"DINOv3ConvNextBackbone\"]"
        },
        {
            "sha": "f3540d90dcecc42675fe72af0295d33d816fb60c",
            "filename": "tests/models/dinov3_convnext/test_modeling_dinov3_convnext.py",
            "status": "modified",
            "additions": 60,
            "deletions": 1,
            "changes": 61,
            "blob_url": "https://github.com/huggingface/transformers/blob/3648fde4864600daea5d0d6af33e99effc5af82e/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3648fde4864600daea5d0d6af33e99effc5af82e/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdinov3_convnext%2Ftest_modeling_dinov3_convnext.py?ref=3648fde4864600daea5d0d6af33e99effc5af82e",
            "patch": "@@ -20,6 +20,7 @@\n from transformers.testing_utils import require_torch, require_vision, slow, torch_device\n from transformers.utils import is_torch_available, is_vision_available\n \n+from ...test_backbone_common import BackboneTesterMixin\n from ...test_configuration_common import ConfigTester\n from ...test_modeling_common import ModelTesterMixin, floats_tensor, ids_tensor\n from ...test_pipeline_mixin import PipelineTesterMixin\n@@ -28,7 +29,7 @@\n if is_torch_available():\n     import torch\n \n-    from transformers import DINOv3ConvNextModel\n+    from transformers import DINOv3ConvNextBackbone, DINOv3ConvNextModel\n \n \n if is_vision_available():\n@@ -104,6 +105,49 @@ def create_and_check_model(self, config, pixel_values, labels):\n             ),\n         )\n \n+    def create_and_check_backbone(self, config, pixel_values, labels):\n+        model = DINOv3ConvNextBackbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify hidden states\n+        self.parent.assertEqual(len(result.feature_maps), len(config.out_features))\n+        expected_size = self.image_size // (4 * (2 ** (len(config.depths) - 1)))\n+        self.parent.assertListEqual(\n+            list(result.feature_maps[0].shape), [self.batch_size, model.channels[0], expected_size, expected_size]\n+        )\n+\n+        # verify channels\n+        self.parent.assertEqual(len(model.channels), len(config.out_features))\n+\n+        # verify backbone works with out_features=None\n+        config.out_features = None\n+        model = DINOv3ConvNextBackbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify feature maps\n+        self.parent.assertEqual(len(result.feature_maps), 1)\n+        self.parent.assertListEqual(\n+            list(result.feature_maps[0].shape), [self.batch_size, model.channels[0], expected_size, expected_size]\n+        )\n+\n+        # verify channels\n+        self.parent.assertEqual(len(model.channels), 1)\n+\n+        model = DINOv3ConvNextBackbone(config=config)\n+        model.to(torch_device)\n+        model.eval()\n+        result = model(pixel_values)\n+\n+        # verify feature maps\n+        self.parent.assertEqual(len(result.feature_maps), 1)\n+        self.parent.assertListEqual(\n+            list(result.feature_maps[0].shape), [self.batch_size, model.channels[0], expected_size, expected_size]\n+        )\n+\n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()\n         config, pixel_values, labels = config_and_inputs\n@@ -156,6 +200,10 @@ def test_model(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_model(*config_and_inputs)\n \n+    def test_backbone(self):\n+        config_and_inputs = self.model_tester.prepare_config_and_inputs()\n+        self.model_tester.create_and_check_backbone(*config_and_inputs)\n+\n     def test_hidden_states_output(self):\n         def check_hidden_states_output(inputs_dict, config, model_class):\n             model = model_class(config)\n@@ -240,3 +288,14 @@ def test_inference_no_head(self):\n         last_layer_patch_tokens = outputs.last_hidden_state[:, 1:]\n         expected_slice = torch.tensor([0.4905, -3.7135, 1.8485, -1.0403, -1.0908], device=torch_device)\n         torch.testing.assert_close(last_layer_patch_tokens[0, 0, :5], expected_slice, rtol=1e-4, atol=1e-4)\n+\n+\n+@require_torch\n+class DINOv3ConvNextBackboneTest(unittest.TestCase, BackboneTesterMixin):\n+    all_model_classes = (DINOv3ConvNextBackbone,) if is_torch_available() else ()\n+    config_class = DINOv3ConvNextConfig\n+\n+    has_attentions = False\n+\n+    def setUp(self):\n+        self.model_tester = DINOv3ConvNextModelTester(self)"
        }
    ],
    "stats": {
        "total": 140,
        "additions": 134,
        "deletions": 6
    }
}