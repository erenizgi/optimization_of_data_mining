{
    "author": "ivarflakstad",
    "message": "Skip sdpa tests if submodule does not support sdpa (#38907)",
    "sha": "af6120b3eb2470b994c21421bb6eaa76576128b0",
    "files": [
        {
            "sha": "4c7cef05c35699f2db2a126e1d10a2b88fc0eb6e",
            "filename": "tests/test_modeling_common.py",
            "status": "modified",
            "additions": 24,
            "deletions": 0,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/af6120b3eb2470b994c21421bb6eaa76576128b0/tests%2Ftest_modeling_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/af6120b3eb2470b994c21421bb6eaa76576128b0/tests%2Ftest_modeling_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_modeling_common.py?ref=af6120b3eb2470b994c21421bb6eaa76576128b0",
            "patch": "@@ -3799,8 +3799,20 @@ def test_sdpa_can_dispatch_on_flash(self):\n                 self.skipTest(reason=\"Idefics currently (transformers==4.39.1) requires an image_attention_mask input\")\n             if config.model_type in [\"sam\"]:\n                 self.skipTest(reason=\"SAM requires an attention_mask input for relative positional embeddings\")\n+\n             model = model_class(config)\n \n+            sub_models_supporting_sdpa = [\n+                module._supports_sdpa\n+                for name, module in model.named_modules()\n+                if isinstance(module, PreTrainedModel) and name != \"\"\n+            ]\n+            supports_sdpa_all_modules = (\n+                all(sub_models_supporting_sdpa) if len(sub_models_supporting_sdpa) > 0 else model._supports_sdpa\n+            )\n+            if not supports_sdpa_all_modules:\n+                self.skipTest(reason=\"This models' submodels does not support sdpa\")\n+\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, attn_implementation=\"sdpa\")\n@@ -3848,8 +3860,20 @@ def test_sdpa_can_compile_dynamic(self):\n                     \"Cannot compile forward without an existing cache with Hybrid, as `torch._dynamo.mark_static_address` \"\n                     \"is a forbidden call.\"\n                 )\n+\n             model = model_class(config)\n \n+            sub_models_supporting_sdpa = [\n+                module._supports_sdpa\n+                for name, module in model.named_modules()\n+                if isinstance(module, PreTrainedModel) and name != \"\"\n+            ]\n+            supports_sdpa_all_modules = (\n+                all(sub_models_supporting_sdpa) if len(sub_models_supporting_sdpa) > 0 else model._supports_sdpa\n+            )\n+            if not supports_sdpa_all_modules:\n+                self.skipTest(reason=\"This models' submodels does not support sdpa\")\n+\n             with tempfile.TemporaryDirectory() as tmpdirname:\n                 model.save_pretrained(tmpdirname)\n                 model = model_class.from_pretrained(tmpdirname, torch_dtype=torch.float16, attn_implementation=\"sdpa\")"
        }
    ],
    "stats": {
        "total": 24,
        "additions": 24,
        "deletions": 0
    }
}