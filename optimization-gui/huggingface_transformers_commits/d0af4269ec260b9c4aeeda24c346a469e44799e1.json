{
    "author": "2015aroras",
    "message": "Add Olmo3 model (#40778)\n\n* transformers add-new-model-like for Olmo3\n\n* Implement modular Olmo3\n\n* Update Olmo3 tests\n\n* Copy Olmo2 weight converter to Olmo3\n\n* Implement Olmo3 weight converter\n\n* Fix code quality errors\n\n* Remove unused import\n\n* Address rope-related PR comments\n\n* Update Olmo3 model doc with minimal details\n\n* Fix Olmo3 rope test failure\n\n* Fix 7B integration test",
    "sha": "d0af4269ec260b9c4aeeda24c346a469e44799e1",
    "files": [
        {
            "sha": "b496fcb4e4b9f92c5b4e05575bdce11c7408b39c",
            "filename": "docs/source/en/_toctree.yml",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/docs%2Fsource%2Fen%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/docs%2Fsource%2Fen%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2F_toctree.yml?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -625,6 +625,8 @@\n         title: OLMo\n       - local: model_doc/olmo2\n         title: OLMo2\n+      - local: model_doc/olmo3\n+        title: Olmo3\n       - local: model_doc/olmoe\n         title: OLMoE\n       - local: model_doc/open-llama"
        },
        {
            "sha": "e320181925cacf6ebbe02f344ee077d07e0e11ff",
            "filename": "docs/source/en/model_doc/olmo3.md",
            "status": "added",
            "additions": 147,
            "deletions": 0,
            "changes": 147,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Folmo3.md?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -0,0 +1,147 @@\n+<!--Copyright 2025 the HuggingFace Team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\");\n+you may not use this file except in compliance with the License.\n+You may obtain a copy of the License at\n+\n+    http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software\n+distributed under the License is distributed on an \"AS IS\" BASIS,\n+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+See the License for the specific language governing permissions and\n+limitations under the License.\n+\n+\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be rendered properly in your Markdown viewer.\n+\n+-->\n+*This model was released on {release_date} and added to Hugging Face Transformers on 2025-09-08.*\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+        <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+        <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+        <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n+</div>\n+\n+# OLMo3\n+Olmo3 is an improvement on [OLMo2](./olmo2). More details will be released on *soon*.\n+\n+> [!TIP]\n+> Click on the OLMo3 models in the right sidebar for more examples of how to apply OLMo3 to different language tasks.\n+\n+The example below demonstrates how to generate text with [`Pipeline`], [`AutoModel`] and from the command line.\n+\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n+\n+```py\n+import torch\n+from transformers import pipeline\n+\n+pipe = pipeline(\n+    task=\"text-generation\",\n+    model=\"allenai/TBA\",\n+    dtype=torch.bfloat16,\n+    device=0,\n+)\n+    \n+result = pipe(\"Plants create energy through a process known as\")\n+print(result)\n+```\n+\n+</hfoption>\n+<hfoption id=\"AutoModel\">\n+\n+```py\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"allenai/TBA\"\n+)\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"allenai/TBA\",\n+    dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n+\n+output = model.generate(**input_ids, max_length=50, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+```\n+\n+</hfoption>\n+<hfoption id=\"transformers CLI\">\n+\n+```bash\n+echo -e \"Plants create energy through a process known as\" | transformers-cli run --task text-generation --model allenai/TBA --device 0\n+```\n+\n+</hfoption>\n+</hfoptions>\n+\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n+\n+The example below uses [torchao](../quantization/torchao) to only quantize the weights to 4-bits.\n+```py\n+\n+#pip install torchao\n+import torch\n+from transformers import AutoModelForCausalLM, AutoTokenizer, TorchAoConfig\n+\n+torchao_config = TorchAoConfig(\n+    \"int4_weight_only\",\n+    group_size=128\n+)\n+\n+tokenizer = AutoTokenizer.from_pretrained(\n+    \"allenai/TBA\"\n+)\n+\n+model = AutoModelForCausalLM.from_pretrained(\n+    \"allenai/TBA\",\n+    quantization_config=torchao_config,\n+    dtype=torch.bfloat16,\n+    device_map=\"auto\",\n+    attn_implementation=\"sdpa\"\n+)\n+input_ids = tokenizer(\"Plants create energy through a process known as\", return_tensors=\"pt\").to(model.device)\n+\n+output = model.generate(**input_ids, max_length=50, cache_implementation=\"static\")\n+print(tokenizer.decode(output[0], skip_special_tokens=True))\n+\n+```\n+\n+\n+## Notes\n+\n+- Load specific intermediate checkpoints by adding the `revision` parameter to [`~PreTrainedModel.from_pretrained`]. \n+\n+    ```py\n+    from transformers import AutoModelForCausalLM\n+    \n+    model = AutoModelForCausalLM.from_pretrained(\"allenai/TBA\", revision=\"stage1-step140000-tokens294B\")\n+    ```\n+\n+\n+## Olmo3Config\n+\n+[[autodoc]] Olmo3Config\n+\n+## Olmo3ForCausalLM\n+\n+[[autodoc]] Olmo3ForCausalLM\n+\n+## Olmo3Model\n+\n+[[autodoc]] Olmo3Model\n+    - forward\n+\n+## Olmo3PreTrainedModel\n+\n+[[autodoc]] Olmo3PreTrainedModel\n+    - forward\n\\ No newline at end of file"
        },
        {
            "sha": "13e616ca51cad4ad7fd5becc4fe8fcdce0b414a3",
            "filename": "src/transformers/models/__init__.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2F__init__.py?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -239,6 +239,7 @@\n     from .nystromformer import *\n     from .olmo import *\n     from .olmo2 import *\n+    from .olmo3 import *\n     from .olmoe import *\n     from .omdet_turbo import *\n     from .oneformer import *"
        },
        {
            "sha": "7a69ab18215d95d8bc964c650a52730564bc5809",
            "filename": "src/transformers/models/auto/configuration_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fconfiguration_auto.py?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -281,6 +281,7 @@\n         (\"nystromformer\", \"NystromformerConfig\"),\n         (\"olmo\", \"OlmoConfig\"),\n         (\"olmo2\", \"Olmo2Config\"),\n+        (\"olmo3\", \"Olmo3Config\"),\n         (\"olmoe\", \"OlmoeConfig\"),\n         (\"omdet-turbo\", \"OmDetTurboConfig\"),\n         (\"oneformer\", \"OneFormerConfig\"),\n@@ -723,6 +724,7 @@\n         (\"nystromformer\", \"NystrÃ¶mformer\"),\n         (\"olmo\", \"OLMo\"),\n         (\"olmo2\", \"OLMo2\"),\n+        (\"olmo3\", \"Olmo3\"),\n         (\"olmoe\", \"OLMoE\"),\n         (\"omdet-turbo\", \"OmDet-Turbo\"),\n         (\"oneformer\", \"OneFormer\"),"
        },
        {
            "sha": "e871a4848c01a226d8d19e9fd4eb96a545aef3c8",
            "filename": "src/transformers/models/auto/modeling_auto.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Fmodeling_auto.py?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -280,6 +280,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"nystromformer\", \"NystromformerModel\"),\n         (\"olmo\", \"OlmoModel\"),\n         (\"olmo2\", \"Olmo2Model\"),\n+        (\"olmo3\", \"Olmo3Model\"),\n         (\"olmoe\", \"OlmoeModel\"),\n         (\"omdet-turbo\", \"OmDetTurboForObjectDetection\"),\n         (\"oneformer\", \"OneFormerModel\"),\n@@ -704,6 +705,7 @@ class _BaseModelWithGenerate(PreTrainedModel, GenerationMixin):\n         (\"nemotron\", \"NemotronForCausalLM\"),\n         (\"olmo\", \"OlmoForCausalLM\"),\n         (\"olmo2\", \"Olmo2ForCausalLM\"),\n+        (\"olmo3\", \"Olmo3ForCausalLM\"),\n         (\"olmoe\", \"OlmoeForCausalLM\"),\n         (\"open-llama\", \"OpenLlamaForCausalLM\"),\n         (\"openai-gpt\", \"OpenAIGPTLMHeadModel\"),"
        },
        {
            "sha": "eae569f1dae41cfbea02715b3e50f0b23c474a2a",
            "filename": "src/transformers/models/auto/tokenization_auto.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fauto%2Ftokenization_auto.py?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -485,6 +485,7 @@\n         ),\n         (\"olmo\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n         (\"olmo2\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n+        (\"olmo3\", (None, \"GPT2TokenizerFast\" if is_tokenizers_available() else None)),\n         (\"olmoe\", (None, \"GPTNeoXTokenizerFast\" if is_tokenizers_available() else None)),\n         (\n             \"omdet-turbo\","
        },
        {
            "sha": "e743c2ee3dae54b234ee40cfba63812f137822a6",
            "filename": "src/transformers/models/olmo3/__init__.py",
            "status": "added",
            "additions": 29,
            "deletions": 0,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Folmo3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Folmo3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2F__init__.py?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -0,0 +1,29 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import TYPE_CHECKING\n+\n+from ...utils import _LazyModule\n+from ...utils.import_utils import define_import_structure\n+\n+\n+if TYPE_CHECKING:\n+    from .configuration_olmo3 import *\n+    from .modeling_olmo3 import *\n+else:\n+    import sys\n+\n+    _file = globals()[\"__file__\"]\n+    sys.modules[__name__] = _LazyModule(__name__, _file, define_import_structure(_file), module_spec=__spec__)"
        },
        {
            "sha": "a6ea71f3a97a58e590cf41694124d96710e73bce",
            "filename": "src/transformers/models/olmo3/configuration_olmo3.py",
            "status": "added",
            "additions": 225,
            "deletions": 0,
            "changes": 225,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fconfiguration_olmo3.py?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -0,0 +1,225 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/olmo3/modular_olmo3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_olmo3.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from ...configuration_utils import PretrainedConfig, layer_type_validation\n+from ...modeling_rope_utils import rope_config_validation\n+\n+\n+class Olmo3Config(PretrainedConfig):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Olmo3Model`]. It is used to instantiate an OLMo3\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the [allenai/OLMo-3-0725-1B](https://huggingface.co/allenai/OLMo-3-0725-1B).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 50304):\n+            Vocabulary size of the Olmo3 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Olmo3Model`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 11008):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 1):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 50279):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            Size of the sliding window for sliding window attention.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer. Defaults to sliding window attention\n+            for 3 out of 4 layers, and full attention for every 4th layer.\n+\n+    ```python\n+    >>> from transformers import Olmo3Model, Olmo3Config\n+\n+    >>> # Initializing a Olmo3 7B style configuration\n+    >>> configuration = Olmo3Config()\n+\n+    >>> # Initializing a model from the Olmo3 7B style configuration\n+    >>> model = Olmo3Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"olmo3\"\n+    keys_to_ignore_at_inference = [\"past_key_values\"]\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=50304,\n+        hidden_size=4096,\n+        intermediate_size=11008,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=2048,\n+        initializer_range=0.02,\n+        use_cache=True,\n+        pad_token_id=1,\n+        bos_token_id=None,\n+        eos_token_id=50279,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        rms_norm_eps=1e-5,\n+        sliding_window=4096,\n+        layer_types=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            **kwargs,\n+        )\n+        self.vocab_size = vocab_size\n+        self.max_position_embeddings = max_position_embeddings\n+        self.hidden_size = hidden_size\n+        self.intermediate_size = intermediate_size\n+        self.num_hidden_layers = num_hidden_layers\n+        self.num_attention_heads = num_attention_heads\n+\n+        # for backward compatibility\n+        if num_key_value_heads is None:\n+            num_key_value_heads = num_attention_heads\n+\n+        self.num_key_value_heads = num_key_value_heads\n+        self.hidden_act = hidden_act\n+        self.initializer_range = initializer_range\n+        self.use_cache = use_cache\n+        self.rope_theta = rope_theta\n+        self.rope_scaling = rope_scaling\n+        self._rope_scaling_validation()\n+        self.attention_bias = attention_bias\n+        self.attention_dropout = attention_dropout\n+\n+        self.rms_norm_eps = rms_norm_eps\n+\n+        self.sliding_window = sliding_window\n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if (i + 1) % 4 != 0 else \"full_attention\" for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n+    def _rope_scaling_validation(self):\n+        \"\"\"\n+        Validate the `rope_scaling` configuration.\n+        \"\"\"\n+        rope_config_validation(self)\n+\n+\n+__all__ = [\"Olmo3Config\"]"
        },
        {
            "sha": "ce6d85f6535831ca971e51230da1fee3b6e4554d",
            "filename": "src/transformers/models/olmo3/convert_olmo3_weights_to_hf.py",
            "status": "added",
            "additions": 459,
            "deletions": 0,
            "changes": 459,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Folmo3%2Fconvert_olmo3_weights_to_hf.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Folmo3%2Fconvert_olmo3_weights_to_hf.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fconvert_olmo3_weights_to_hf.py?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -0,0 +1,459 @@\n+# Copyright 2025 EleutherAI and The HuggingFace Inc. team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+from __future__ import annotations\n+\n+import argparse\n+import gc\n+import io\n+import json\n+import os\n+import pickle\n+import shutil\n+import traceback\n+import uuid\n+from collections.abc import Sequence\n+from concurrent.futures import ThreadPoolExecutor, as_completed\n+from dataclasses import dataclass\n+from pathlib import Path\n+from typing import Any, cast\n+\n+import torch\n+import torch.distributed.checkpoint as dist_cp\n+from torch.distributed.checkpoint.metadata import Metadata, MetadataIndex, StorageMeta\n+from torch.distributed.checkpoint.planner import (\n+    LoadItemType,\n+    ReadItem,\n+)\n+from torch.futures import Future\n+\n+from transformers import AutoTokenizer, Olmo3Config, Olmo3ForCausalLM\n+\n+\n+\"\"\"\n+Sample usage:\n+\n+```\n+python src/transformers/models/olmo3/convert_olmo3_weights_to_hf.py \\\n+    --input_dir /path/to/downloaded/olmo3/weights --model_size 7B --output_dir /output/path\n+```\n+\n+Thereafter, models can be loaded via:\n+\n+```py\n+from transformers import Olmo3ForCausalLM, AutoTokenizer\n+\n+model = Olmo3ForCausalLM.from_pretrained(\"/output/path\")\n+tokenizer = AutoTokenizer.from_pretrained(\"/output/path\")\n+```\n+\n+Important note: you need to be able to host the whole model in RAM to execute this script (even if the biggest versions\n+come in several checkpoints they each contain a part of each weight of the model, so we need to load them all in RAM).\n+\"\"\"\n+\n+\n+def compute_intermediate_size(n, ffn_dim_multiplier=1, multiple_of=256):\n+    return multiple_of * ((int(ffn_dim_multiplier * int(8 * n / 3)) + multiple_of - 1) // multiple_of)\n+\n+\n+def read_json(path):\n+    with open(path, \"r\") as f:\n+        return json.load(f)\n+\n+\n+def write_json(text, path):\n+    with open(path, \"w\") as f:\n+        json.dump(text, f)\n+\n+\n+def normalize_path(path: Path | str) -> str:\n+    return str(path).rstrip(\"/\").replace(\"file://\", \"\")\n+\n+\n+def generate_uuid() -> str:\n+    return str(uuid.uuid4())\n+\n+\n+def get_bytes_range(path: Path | str, bytes_start: int, num_bytes: int) -> bytes:\n+    with open(path, \"rb\") as f:\n+        f.seek(bytes_start)\n+        return f.read(num_bytes)\n+\n+\n+def _narrow_tensor_by_index(tensor: torch.Tensor, offsets: Sequence[int], sizes: Sequence[int]) -> torch.Tensor:\n+    \"\"\"\n+    Narrow the tensor according to ``offsets`` and ``sizes``.\n+    \"\"\"\n+    narrowed_tensor = tensor\n+    for idx, (offset, size) in enumerate(zip(offsets, sizes)):\n+        if size < tensor.size(idx):\n+            # Reshape to get shard for this rank and we don't want autograd\n+            # recording here for the narrow op and 'local_shard' should be a\n+            # leaf variable in the autograd graph.\n+            narrowed_tensor = narrowed_tensor.narrow(idx, offset, size)\n+    return narrowed_tensor\n+\n+\n+@dataclass\n+class _StorageInfo:\n+    \"\"\"This is the per entry storage info.\"\"\"\n+\n+    relative_path: str\n+    offset: int\n+    length: int\n+\n+\n+@dataclass\n+class _StoragePrefix:\n+    prefix: str\n+\n+\n+class RemoteFileSystemReader(dist_cp.StorageReader):\n+    \"\"\"\n+    A :class:`~torch.distributed.checkpoint.StorageReader` based on :class:`~torch.distributed.checkpoint.FileSystemReader`\n+    that can read data directly from cloud storage as well as a local directory.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        path: Path | str,\n+        *,\n+        thread_count: int | None = None,\n+        pre_download: bool = False,\n+        work_dir: Path | str | None = None,\n+    ):\n+        super().__init__()\n+        if thread_count is not None and thread_count <= 0:\n+            raise ValueError(\"thread count must be at least 1\")\n+        self.path = normalize_path(path)\n+        self.thread_count = thread_count or 1\n+        self.pre_download = pre_download\n+        self.work_dir = normalize_path(work_dir) if work_dir is not None else None\n+        self.storage_data: dict[MetadataIndex, _StorageInfo] = {}\n+        self.load_id = generate_uuid()\n+        self._metadata: Metadata | None = None\n+\n+    def _get_bytes(self, relative_path: str, offset: int, length: int) -> bytes:\n+        full_path = f\"{self.path}/{relative_path}\"\n+        return get_bytes_range(full_path, offset, length)\n+\n+    def _get_content_for_read(self, read_item: ReadItem) -> tuple[ReadItem, bytes]:\n+        sinfo = self.storage_data[read_item.storage_index]\n+        content = self._get_bytes(sinfo.relative_path, sinfo.offset, sinfo.length)\n+        return (read_item, content)\n+\n+    def reset(self, checkpoint_id: Path | str | None = None) -> None:\n+        self.storage_data = {}\n+        if checkpoint_id:\n+            self.path = normalize_path(checkpoint_id)\n+        self.load_id = generate_uuid()\n+\n+    def read_data(self, plan: dist_cp.LoadPlan, planner: dist_cp.LoadPlanner) -> Future[None]:\n+        with ThreadPoolExecutor(max_workers=self.thread_count) as executor:\n+            read_item_content_futures = []\n+            for read_item in plan.items:\n+                read_item_content_futures.append(executor.submit(self._get_content_for_read, read_item))\n+            read_item_content_results = []\n+            for f in as_completed(read_item_content_futures):\n+                try:\n+                    read_item_content_results.append(f.result())\n+                except BaseException:\n+                    # NOTE: we might get an error here that can't be pickled, which causes a different failure\n+                    # later when PyTorch tries to reduce that error across ranks. So here we just make\n+                    # sure we're raising a simple error type that can be pickled.\n+                    raise RuntimeError(f\"Original error:\\n{traceback.format_exc()}\")\n+\n+        # Modified from `FileSystemReader.read_data()`\n+        for read_item, content in read_item_content_results:\n+            bytes = io.BytesIO(content)\n+            bytes.seek(0)\n+            if read_item.type == LoadItemType.BYTE_IO:\n+                planner.load_bytes(read_item, bytes)\n+            else:\n+                # NOTE: 'weights_only=False' needed to load torchao's float8 linear layer checkpoints\n+                tensor = cast(torch.Tensor, torch.load(bytes, map_location=\"cpu\", weights_only=False))\n+                tensor = _narrow_tensor_by_index(tensor, read_item.storage_offsets, read_item.lengths)\n+                target_tensor = planner.resolve_tensor(read_item).detach()\n+\n+                assert target_tensor.size() == tensor.size(), (\n+                    f\"req {read_item.storage_index} mismatch sizes {target_tensor.size()} vs {tensor.size()}\"\n+                )\n+                target_tensor.copy_(tensor)\n+                planner.commit_tensor(read_item, target_tensor)\n+\n+        fut: Future = Future()\n+        fut.set_result(None)\n+        return fut\n+\n+    def read_metadata(self) -> Metadata:\n+        if self._metadata is None:\n+            try:\n+                with (Path(self.path) / \".metadata\").open(\"rb\") as metadata_file:\n+                    metadata = pickle.load(metadata_file)\n+            except FileNotFoundError as exc:\n+                msg = f\"'{self.path}' is not a distributed checkpoint folder.\"\n+                suggested_dir = os.path.join(self.path, \"model_and_optim\")\n+                if Path(os.path.join(suggested_dir, \".metadata\")).exists():\n+                    msg += f\" Did you mean to use '{suggested_dir}'?\"\n+                raise FileNotFoundError(msg) from exc\n+\n+            if getattr(metadata, \"storage_meta\", None) is None:\n+                metadata.storage_meta = StorageMeta()\n+            metadata.storage_meta.load_id = self.load_id\n+\n+            self._metadata = metadata\n+\n+        return self._metadata\n+\n+    def set_up_storage_reader(self, metadata: Metadata, is_coordinator: bool) -> None:\n+        del is_coordinator\n+        self.storage_data = metadata.storage_data\n+        assert self.storage_data is not None\n+\n+    def prepare_local_plan(self, plan: dist_cp.LoadPlan) -> dist_cp.LoadPlan:\n+        return plan\n+\n+    def prepare_global_plan(self, global_plan: list[dist_cp.LoadPlan]) -> list[dist_cp.LoadPlan]:\n+        return global_plan\n+\n+    @property\n+    def checkpoint_id(self) -> str:\n+        return self.path\n+\n+    @classmethod\n+    def validate_checkpoint_id(cls, checkpoint_id: Path | str) -> bool:\n+        del checkpoint_id\n+        return True\n+\n+\n+def load_model(model_path: str):\n+    def _load_unsharded_keys(\n+        dir: Path | str,\n+        keys: list[str],\n+        *,\n+        pre_download: bool = False,\n+        work_dir: Path | str | None = None,\n+    ) -> dict[str, Any]:\n+        from torch.distributed.checkpoint.default_planner import _EmptyStateDictLoadPlanner\n+        from torch.distributed.checkpoint.state_dict_loader import _load_state_dict\n+\n+        state_dict: dict[str, Any] = {}\n+        _load_state_dict(\n+            state_dict,\n+            storage_reader=RemoteFileSystemReader(dir, pre_download=pre_download, work_dir=work_dir),\n+            planner=_EmptyStateDictLoadPlanner(keys=keys),\n+            no_dist=True,\n+        )\n+        return state_dict\n+\n+    with (Path(model_path) / \".metadata\").open(\"rb\") as metadata_file:\n+        metadata = pickle.load(metadata_file)\n+        keys = [key for key in metadata.state_dict_metadata.keys() if key.startswith(\"model.\")]\n+\n+    # keys = [\"model.blocks.0.attention.w_q.weight\"]\n+\n+    return _load_unsharded_keys(\n+        model_path,\n+        keys,\n+        # model_path, [\"model.blocks.0.attention.w_q.weight\", \"model.blocks.0.attention.w_k.weight\"]\n+    )\n+\n+\n+def write_model(\n+    model_path,\n+    input_base_path,\n+    include_tokenizer=True,\n+    tokenizer_id=None,\n+    safe_serialization=True,\n+    tmp_cleanup=True,\n+):\n+    os.makedirs(model_path, exist_ok=True)\n+    tmp_model_path = os.path.join(model_path, \"tmp\")\n+    os.makedirs(tmp_model_path, exist_ok=True)\n+\n+    config_path = Path(input_base_path) / \"config.json\"\n+    olmo3_config = json.loads(config_path.read_text())\n+    model_config = olmo3_config[\"model\"]\n+    block_config = model_config[\"block\"]\n+    attention_config = block_config[\"attention\"]\n+    tokenizer_config = olmo3_config[\"dataset\"][\"tokenizer\"]\n+\n+    n_layers = model_config[\"n_layers\"]\n+    n_heads = attention_config[\"n_heads\"]\n+    dim = model_config[\"d_model\"]\n+    dims_per_head = dim // n_heads\n+    base = attention_config[\"rope\"][\"theta\"]\n+    inv_freq = 1.0 / (base ** (torch.arange(0, dims_per_head, 2).float() / dims_per_head))\n+    max_position_embeddings = olmo3_config[\"train_module\"][\"max_sequence_length\"]\n+\n+    if attention_config.get(\"n_kv_heads\", None) is not None:\n+        num_key_value_heads = model_config[\"n_kv_heads\"]  # for GQA / MQA\n+    else:\n+        num_key_value_heads = n_heads\n+\n+    print(f\"Fetching all parameters from the checkpoint at {input_base_path}.\")\n+\n+    # Not sharded\n+    # (The sharded implementation would also work, but this is simpler.)\n+    loaded = load_model(os.path.join(input_base_path, \"model_and_optim\"))[\"model\"]\n+    print(loaded.keys())\n+    # loaded = torch.load(os.path.join(input_base_path, \"model.pt\"), map_location=\"cpu\", weights_only=True)\n+\n+    param_count = 0\n+    index_dict: dict[str, Any] = {\"weight_map\": {}}\n+    for layer_i in range(n_layers):\n+        filename = f\"pytorch_model-{layer_i + 1}-of-{n_layers + 1}.bin\"\n+        # Unsharded\n+        state_dict = {\n+            f\"model.layers.{layer_i}.self_attn.q_proj.weight\": loaded[f\"blocks.{layer_i}.attention.w_q.weight\"],\n+            f\"model.layers.{layer_i}.self_attn.k_proj.weight\": loaded[f\"blocks.{layer_i}.attention.w_k.weight\"],\n+            f\"model.layers.{layer_i}.self_attn.v_proj.weight\": loaded[f\"blocks.{layer_i}.attention.w_v.weight\"],\n+            f\"model.layers.{layer_i}.self_attn.o_proj.weight\": loaded[f\"blocks.{layer_i}.attention.w_out.weight\"],\n+            f\"model.layers.{layer_i}.self_attn.q_norm.weight\": loaded[f\"blocks.{layer_i}.attention.q_norm.weight\"],\n+            f\"model.layers.{layer_i}.self_attn.k_norm.weight\": loaded[f\"blocks.{layer_i}.attention.k_norm.weight\"],\n+            f\"model.layers.{layer_i}.mlp.gate_proj.weight\": loaded[f\"blocks.{layer_i}.feed_forward.w1.weight\"],\n+            f\"model.layers.{layer_i}.mlp.down_proj.weight\": loaded[f\"blocks.{layer_i}.feed_forward.w2.weight\"],\n+            f\"model.layers.{layer_i}.mlp.up_proj.weight\": loaded[f\"blocks.{layer_i}.feed_forward.w3.weight\"],\n+            f\"model.layers.{layer_i}.post_attention_layernorm.weight\": loaded[\n+                f\"blocks.{layer_i}.attention_norm.weight\"\n+            ],\n+            f\"model.layers.{layer_i}.post_feedforward_layernorm.weight\": loaded[\n+                f\"blocks.{layer_i}.feed_forward_norm.weight\"\n+            ],\n+        }\n+\n+        state_dict[f\"model.layers.{layer_i}.self_attn.rotary_emb.inv_freq\"] = inv_freq\n+\n+        for k, v in state_dict.items():\n+            index_dict[\"weight_map\"][k] = filename\n+            param_count += v.numel()\n+        torch.save(state_dict, os.path.join(tmp_model_path, filename))\n+\n+    filename = f\"pytorch_model-{n_layers + 1}-of-{n_layers + 1}.bin\"\n+\n+    # Unsharded\n+    # TODO: Deal with weight-tying\n+    state_dict = {\n+        \"model.embed_tokens.weight\": loaded[\"embeddings.weight\"],\n+        \"model.norm.weight\": loaded[\"lm_head.norm.weight\"],\n+        \"lm_head.weight\": loaded[\"lm_head.w_out.weight\"],\n+    }\n+\n+    for k, v in state_dict.items():\n+        index_dict[\"weight_map\"][k] = filename\n+        param_count += v.numel()\n+    torch.save(state_dict, os.path.join(tmp_model_path, filename))\n+\n+    # Write configs\n+    index_dict[\"metadata\"] = {\"total_size\": param_count * 2}\n+    write_json(index_dict, os.path.join(tmp_model_path, \"pytorch_model.bin.index.json\"))\n+\n+    config = Olmo3Config(\n+        vocab_size=model_config[\"vocab_size\"],\n+        hidden_size=dim,\n+        intermediate_size=block_config[\"feed_forward\"][\"hidden_size\"],\n+        num_hidden_layers=n_layers,\n+        num_attention_heads=n_heads,\n+        num_key_value_heads=num_key_value_heads,\n+        max_position_embeddings=max_position_embeddings,\n+        pad_token_id=tokenizer_config[\"pad_token_id\"],\n+        bos_token_id=None,\n+        eos_token_id=tokenizer_config[\"eos_token_id\"],\n+        tie_word_embeddings=False,\n+        rms_norm_eps=block_config[\"layer_norm\"][\"eps\"],\n+        rope_theta=base,\n+    )\n+    config.save_pretrained(tmp_model_path)\n+\n+    # Make space so we can load the model properly now.\n+    del state_dict\n+    del loaded\n+    gc.collect()\n+\n+    if include_tokenizer:\n+        tokenizer_id = tokenizer_id or tokenizer_config[\"identifier\"]\n+        _write_tokenizer(model_path, tokenizer_id)\n+\n+    print(\"Loading the checkpoint in a Olmo 3 model.\")\n+    model = Olmo3ForCausalLM.from_pretrained(tmp_model_path, dtype=torch.bfloat16)\n+    print(\"Resizing token embeddings to match tokenizer config.\")\n+    model.resize_token_embeddings(tokenizer_config[\"vocab_size\"])\n+    # Avoid saving this as part of the config.\n+    del model.config._name_or_path\n+    print(\"Saving in the Transformers format.\")\n+    model.save_pretrained(model_path, safe_serialization=safe_serialization)\n+    if tmp_cleanup:\n+        # Make cleanup optional; attempting to `rmtree` the `tmp_model_path` causes\n+        # errors if using NFS.\n+        shutil.rmtree(tmp_model_path)\n+\n+\n+def _write_tokenizer(\n+    output_path: Path,\n+    tokenizer_id: str,\n+) -> None:\n+    print(f\"Saving a tokenizer to {output_path}.\")\n+\n+    tokenizer = AutoTokenizer.from_pretrained(tokenizer_id)\n+    tokenizer.save_pretrained(output_path)\n+\n+\n+def main():\n+    parser = argparse.ArgumentParser()\n+    parser.add_argument(\n+        \"--input_dir\",\n+        required=True,\n+        help=\"Location of Olmo 3 weights, which contains config.yaml and model.pt.\",\n+    )\n+    parser.add_argument(\n+        \"--no_tokenizer\",\n+        action=\"store_false\",\n+        dest=\"include_tokenizer\",\n+        help=\"If set, do not convert OLMo tokenizer to HF tokenizer.\",\n+    )\n+    parser.add_argument(\n+        \"--tokenizer\",\n+        type=Path,\n+        default=None,\n+        help=\"Location of Olmo 3 tokenizer json file. Defaults to what is set in the config file.\",\n+    )\n+    parser.add_argument(\n+        \"--output_dir\",\n+        required=True,\n+        help=\"Location to write HF model and tokenizer\",\n+    )\n+    parser.add_argument(\n+        \"--no_tmp_cleanup\",\n+        action=\"store_false\",\n+        dest=\"tmp_cleanup\",\n+        help=\"If passed, don't remove temp dir at end of HF conversion.\",\n+    )\n+    parser.add_argument(\n+        \"--no_safe_serialization\",\n+        action=\"store_false\",\n+        dest=\"safe_serialization\",\n+        help=\"Whether or not to save using `safetensors`.\",\n+    )\n+    args = parser.parse_args()\n+    write_model(\n+        model_path=args.output_dir,\n+        input_base_path=args.input_dir,\n+        safe_serialization=args.safe_serialization,\n+        include_tokenizer=args.include_tokenizer,\n+        tokenizer_id=args.tokenizer,\n+        tmp_cleanup=args.tmp_cleanup,\n+    )\n+\n+\n+if __name__ == \"__main__\":\n+    main()"
        },
        {
            "sha": "3a7e2b5ff9537d46ac5409adba76efd185ee10d7",
            "filename": "src/transformers/models/olmo3/modeling_olmo3.py",
            "status": "added",
            "additions": 509,
            "deletions": 0,
            "changes": 509,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodeling_olmo3.py?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -0,0 +1,509 @@\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+#           This file was automatically generated from src/transformers/models/olmo3/modular_olmo3.py.\n+#               Do NOT edit this file manually as any edits will be overwritten by the generation of\n+#             the file from the modular. If any change should be done, please apply the change to the\n+#                          modular_olmo3.py file directly. One of our CI enforces this.\n+#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional, Union\n+\n+import torch\n+import torch.nn as nn\n+\n+from transformers.utils.generic import TransformersKwargs\n+\n+from ...activations import ACT2FN\n+from ...cache_utils import Cache, DynamicCache\n+from ...generation import GenerationMixin\n+from ...integrations import use_kernel_forward_from_hub\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_layers import GradientCheckpointingLayer\n+from ...modeling_outputs import BaseModelOutputWithPast, CausalLMOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, dynamic_rope_update\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, can_return_tuple\n+from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import check_model_inputs\n+from .configuration_olmo3 import Olmo3Config\n+\n+\n+@use_kernel_forward_from_hub(\"RMSNorm\")\n+class Olmo3RMSNorm(nn.Module):\n+    def __init__(self, hidden_size, eps=1e-6):\n+        \"\"\"\n+        Olmo3RMSNorm is equivalent to T5LayerNorm\n+        \"\"\"\n+        super().__init__()\n+        self.weight = nn.Parameter(torch.ones(hidden_size))\n+        self.variance_epsilon = eps\n+\n+    def forward(self, hidden_states):\n+        input_dtype = hidden_states.dtype\n+        hidden_states = hidden_states.to(torch.float32)\n+        variance = hidden_states.pow(2).mean(-1, keepdim=True)\n+        hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n+        return (self.weight * hidden_states).to(input_dtype)\n+\n+    def extra_repr(self):\n+        return f\"{tuple(self.weight.shape)}, eps={self.variance_epsilon}\"\n+\n+\n+def repeat_kv(hidden_states: torch.Tensor, n_rep: int) -> torch.Tensor:\n+    \"\"\"\n+    This is the equivalent of torch.repeat_interleave(x, dim=1, repeats=n_rep). The hidden states go from (batch,\n+    num_key_value_heads, seqlen, head_dim) to (batch, num_attention_heads, seqlen, head_dim)\n+    \"\"\"\n+    batch, num_key_value_heads, slen, head_dim = hidden_states.shape\n+    if n_rep == 1:\n+        return hidden_states\n+    hidden_states = hidden_states[:, :, None, :, :].expand(batch, num_key_value_heads, n_rep, slen, head_dim)\n+    return hidden_states.reshape(batch, num_key_value_heads * n_rep, slen, head_dim)\n+\n+\n+def eager_attention_forward(\n+    module: nn.Module,\n+    query: torch.Tensor,\n+    key: torch.Tensor,\n+    value: torch.Tensor,\n+    attention_mask: Optional[torch.Tensor],\n+    scaling: float,\n+    dropout: float = 0.0,\n+    **kwargs: Unpack[TransformersKwargs],\n+):\n+    key_states = repeat_kv(key, module.num_key_value_groups)\n+    value_states = repeat_kv(value, module.num_key_value_groups)\n+\n+    attn_weights = torch.matmul(query, key_states.transpose(2, 3)) * scaling\n+    if attention_mask is not None:\n+        causal_mask = attention_mask[:, :, :, : key_states.shape[-2]]\n+        attn_weights = attn_weights + causal_mask\n+\n+    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)\n+    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)\n+    attn_output = torch.matmul(attn_weights, value_states)\n+    attn_output = attn_output.transpose(1, 2).contiguous()\n+\n+    return attn_output, attn_weights\n+\n+\n+def apply_rotary_pos_emb(q, k, cos, sin, position_ids=None, unsqueeze_dim=1):\n+    \"\"\"Applies Rotary Position Embedding to the query and key tensors.\n+\n+    Args:\n+        q (`torch.Tensor`): The query tensor.\n+        k (`torch.Tensor`): The key tensor.\n+        cos (`torch.Tensor`): The cosine part of the rotary embedding.\n+        sin (`torch.Tensor`): The sine part of the rotary embedding.\n+        position_ids (`torch.Tensor`, *optional*):\n+            Deprecated and unused.\n+        unsqueeze_dim (`int`, *optional*, defaults to 1):\n+            The 'unsqueeze_dim' argument specifies the dimension along which to unsqueeze cos[position_ids] and\n+            sin[position_ids] so that they can be properly broadcasted to the dimensions of q and k. For example, note\n+            that cos[position_ids] and sin[position_ids] have the shape [batch_size, seq_len, head_dim]. Then, if q and\n+            k have the shape [batch_size, heads, seq_len, head_dim], then setting unsqueeze_dim=1 makes\n+            cos[position_ids] and sin[position_ids] broadcastable to the shapes of q and k. Similarly, if q and k have\n+            the shape [batch_size, seq_len, heads, head_dim], then set unsqueeze_dim=2.\n+    Returns:\n+        `tuple(torch.Tensor)` comprising of the query and key tensors rotated using the Rotary Position Embedding.\n+    \"\"\"\n+    q_type, k_type = q.dtype, k.dtype\n+    cos = cos.unsqueeze(unsqueeze_dim)\n+    sin = sin.unsqueeze(unsqueeze_dim)\n+    q_embed = (q * cos) + (rotate_half(q) * sin)\n+    k_embed = (k * cos) + (rotate_half(k) * sin)\n+    return q_embed.to(q_type), k_embed.to(k_type)\n+\n+\n+def rotate_half(x):\n+    \"\"\"Rotates half the hidden dims of the input.\"\"\"\n+    x1 = x[..., : x.shape[-1] // 2]\n+    x2 = x[..., x.shape[-1] // 2 :]\n+    return torch.cat((-x2, x1), dim=-1)\n+\n+\n+class Olmo3Attention(nn.Module):\n+    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n+\n+    def __init__(self, config: Olmo3Config, layer_idx: int):\n+        super().__init__()\n+        self.config = config\n+        self.layer_idx = layer_idx\n+        self.head_dim = getattr(config, \"head_dim\", config.hidden_size // config.num_attention_heads)\n+        self.num_key_value_groups = config.num_attention_heads // config.num_key_value_heads\n+        self.scaling = self.head_dim**-0.5\n+        self.attention_dropout = config.attention_dropout\n+        self.is_causal = True\n+\n+        self.q_proj = nn.Linear(\n+            config.hidden_size, config.num_attention_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.k_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.v_proj = nn.Linear(\n+            config.hidden_size, config.num_key_value_heads * self.head_dim, bias=config.attention_bias\n+        )\n+        self.o_proj = nn.Linear(\n+            config.num_attention_heads * self.head_dim, config.hidden_size, bias=config.attention_bias\n+        )\n+        self.q_norm = Olmo3RMSNorm(config.num_attention_heads * self.head_dim, config.rms_norm_eps)\n+        self.k_norm = Olmo3RMSNorm(config.num_key_value_heads * self.head_dim, config.rms_norm_eps)\n+        assert config.layer_types is not None\n+        self.attention_type = config.layer_types[layer_idx]\n+        self.sliding_window = config.sliding_window if self.attention_type == \"sliding_attention\" else None\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_norm(self.q_proj(hidden_states))\n+        key_states = self.k_norm(self.k_proj(hidden_states))\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Olmo3MLP(nn.Module):\n+    def __init__(self, config):\n+        super().__init__()\n+        self.config = config\n+        self.hidden_size = config.hidden_size\n+        self.intermediate_size = config.intermediate_size\n+        self.gate_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.up_proj = nn.Linear(self.hidden_size, self.intermediate_size, bias=False)\n+        self.down_proj = nn.Linear(self.intermediate_size, self.hidden_size, bias=False)\n+        self.act_fn = ACT2FN[config.hidden_act]\n+\n+    def forward(self, x):\n+        down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n+        return down_proj\n+\n+\n+class Olmo3DecoderLayer(GradientCheckpointingLayer):\n+    def __init__(self, config: Olmo3Config, layer_idx: int):\n+        super().__init__()\n+        self.hidden_size = config.hidden_size\n+        self.self_attn = Olmo3Attention(config=config, layer_idx=layer_idx)\n+\n+        self.mlp = Olmo3MLP(config)\n+        self.post_attention_layernorm = Olmo3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.post_feedforward_layernorm = Olmo3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+\n+    @deprecate_kwarg(\"past_key_value\", new_name=\"past_key_values\", version=\"4.58\")\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        use_cache: Optional[bool] = False,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        position_embeddings: Optional[tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> torch.Tensor:\n+        residual = hidden_states\n+        hidden_states, _ = self.self_attn(\n+            hidden_states=hidden_states,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            position_embeddings=position_embeddings,\n+            **kwargs,\n+        )\n+        hidden_states = self.post_attention_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+\n+        # Fully Connected\n+        residual = hidden_states\n+        hidden_states = self.mlp(hidden_states)\n+        hidden_states = self.post_feedforward_layernorm(hidden_states)\n+        hidden_states = residual + hidden_states\n+        return hidden_states\n+\n+\n+class Olmo3RotaryEmbedding(nn.Module):\n+    inv_freq: torch.Tensor  # fix linting for `register_buffer`\n+\n+    def __init__(self, config: Olmo3Config, device=None, rope_type: Optional[str] = None):\n+        super().__init__()\n+        if rope_type is not None:\n+            self.rope_type = rope_type\n+        elif hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            # BC: \"rope_type\" was originally \"type\"\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        assert self.rope_type is not None\n+\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+    @torch.no_grad()\n+    @dynamic_rope_update  # power user: used with advanced RoPE types (e.g. dynamic rope)\n+    def forward(self, x, position_ids):\n+        inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1).to(x.device)\n+        position_ids_expanded = position_ids[:, None, :].float()\n+\n+        device_type = x.device.type if isinstance(x.device.type, str) and x.device.type != \"mps\" else \"cpu\"\n+        with torch.autocast(device_type=device_type, enabled=False):  # Force float32\n+            freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n+            emb = torch.cat((freqs, freqs), dim=-1)\n+            cos = emb.cos() * self.attention_scaling\n+            sin = emb.sin() * self.attention_scaling\n+            return cos, sin\n+\n+\n+@auto_docstring\n+class Olmo3PreTrainedModel(PreTrainedModel):\n+    config: Olmo3Config\n+    base_model_prefix = \"model\"\n+    supports_gradient_checkpointing = True\n+    _no_split_modules = [\"Olmo3DecoderLayer\"]\n+    _skip_keys_device_placement = [\"past_key_values\"]\n+    _supports_flash_attn = True\n+    _supports_sdpa = True\n+    _supports_flex_attn = True\n+\n+    _can_compile_fullgraph = True\n+    _supports_attention_backend = True\n+    _can_record_outputs = {\n+        \"hidden_states\": Olmo3DecoderLayer,\n+        \"attentions\": Olmo3Attention,\n+    }\n+\n+\n+@auto_docstring\n+class Olmo3Model(Olmo3PreTrainedModel):\n+    def __init__(self, config: Olmo3Config):\n+        super().__init__(config)\n+        self.padding_idx = config.pad_token_id\n+        self.vocab_size = config.vocab_size\n+\n+        self.embed_tokens = nn.Embedding(config.vocab_size, config.hidden_size, self.padding_idx)\n+        self.layers = nn.ModuleList(\n+            [Olmo3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.norm = Olmo3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.gradient_checkpointing = False\n+        self.rotary_embs = nn.ModuleDict(\n+            {\n+                \"sliding_attention\": Olmo3RotaryEmbedding(config=config, rope_type=\"default\"),\n+                \"full_attention\": Olmo3RotaryEmbedding(config=config),\n+            }\n+        )\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @check_model_inputs\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.Tensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings_mapping = {\n+            \"sliding_attention\": self.rotary_embs[\"sliding_attention\"](hidden_states, position_ids),\n+            \"full_attention\": self.rotary_embs[\"full_attention\"](hidden_states, position_ids),\n+        }\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.self_attn.attention_type],\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings_mapping[decoder_layer.self_attn.attention_type],\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+@auto_docstring\n+class Olmo3ForCausalLM(Olmo3PreTrainedModel, GenerationMixin):\n+    _tied_weights_keys = [\"lm_head.weight\"]\n+    _tp_plan = {\"lm_head\": \"colwise_rep\"}\n+    _pp_plan = {\"lm_head\": ([\"hidden_states\"], [\"logits\"])}\n+\n+    def __init__(self, config):\n+        super().__init__(config)\n+        self.model = Olmo3Model(config)\n+        self.vocab_size = config.vocab_size\n+        self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n+\n+        # Initialize weights and apply final processing\n+        self.post_init()\n+\n+    @can_return_tuple\n+    @auto_docstring\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        labels: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        logits_to_keep: Union[int, torch.Tensor] = 0,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> CausalLMOutputWithPast:\n+        r\"\"\"\n+        Example:\n+\n+        ```python\n+        >>> from transformers import AutoTokenizer, Olmo3ForCausalLM\n+\n+        >>> model = Olmo3ForCausalLM.from_pretrained(\"meta-olmo3/Olmo3-2-7b-hf\")\n+        >>> tokenizer = AutoTokenizer.from_pretrained(\"meta-olmo3/Olmo3-2-7b-hf\")\n+\n+        >>> prompt = \"Hey, are you conscious? Can you talk to me?\"\n+        >>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+        >>> # Generate\n+        >>> generate_ids = model.generate(inputs.input_ids, max_length=30)\n+        >>> tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n+        \"Hey, are you conscious? Can you talk to me?\\nI'm not conscious, but I can talk to you.\"\n+        ```\"\"\"\n+        outputs: BaseModelOutputWithPast = self.model(\n+            input_ids=input_ids,\n+            attention_mask=attention_mask,\n+            position_ids=position_ids,\n+            past_key_values=past_key_values,\n+            inputs_embeds=inputs_embeds,\n+            use_cache=use_cache,\n+            cache_position=cache_position,\n+            **kwargs,\n+        )\n+\n+        hidden_states = outputs.last_hidden_state\n+        # Only compute necessary logits, and do not upcast them to float if we are not computing the loss\n+        slice_indices = slice(-logits_to_keep, None) if isinstance(logits_to_keep, int) else logits_to_keep\n+        logits = self.lm_head(hidden_states[:, slice_indices, :])\n+\n+        loss = None\n+        if labels is not None:\n+            loss = self.loss_function(logits=logits, labels=labels, vocab_size=self.config.vocab_size, **kwargs)\n+\n+        return CausalLMOutputWithPast(\n+            loss=loss,\n+            logits=logits,\n+            past_key_values=outputs.past_key_values,\n+            hidden_states=outputs.hidden_states,\n+            attentions=outputs.attentions,\n+        )\n+\n+\n+__all__ = [\"Olmo3ForCausalLM\", \"Olmo3Model\", \"Olmo3PreTrainedModel\"]"
        },
        {
            "sha": "8799c8dc07d75bfdf57f1bc1167925b6c73e2d0f",
            "filename": "src/transformers/models/olmo3/modular_olmo3.py",
            "status": "added",
            "additions": 427,
            "deletions": 0,
            "changes": 427,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo3%2Fmodular_olmo3.py?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -0,0 +1,427 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+from typing import Callable, Optional\n+\n+import torch\n+import torch.nn as nn\n+\n+from transformers.utils.generic import TransformersKwargs\n+\n+from ...cache_utils import Cache, DynamicCache\n+from ...configuration_utils import layer_type_validation\n+from ...masking_utils import create_causal_mask, create_sliding_window_causal_mask\n+from ...modeling_outputs import BaseModelOutputWithPast\n+from ...modeling_rope_utils import ROPE_INIT_FUNCTIONS, rope_config_validation\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...processing_utils import Unpack\n+from ..olmo2.configuration_olmo2 import Olmo2Config\n+from ..olmo2.modeling_olmo2 import (\n+    Olmo2Attention,\n+    Olmo2DecoderLayer,\n+    Olmo2ForCausalLM,\n+    Olmo2Model,\n+    Olmo2PreTrainedModel,\n+    Olmo2RMSNorm,\n+    Olmo2RotaryEmbedding,\n+    apply_rotary_pos_emb,\n+    eager_attention_forward,\n+)\n+\n+\n+class Olmo3Config(Olmo2Config):\n+    r\"\"\"\n+    This is the configuration class to store the configuration of a [`Olmo3Model`]. It is used to instantiate an OLMo3\n+    model according to the specified arguments, defining the model architecture. Instantiating a configuration with the\n+    defaults will yield a similar configuration to that of the [allenai/OLMo-3-0725-1B](https://huggingface.co/allenai/OLMo-3-0725-1B).\n+\n+    Configuration objects inherit from [`PretrainedConfig`] and can be used to control the model outputs. Read the\n+    documentation from [`PretrainedConfig`] for more information.\n+\n+\n+    Args:\n+        vocab_size (`int`, *optional*, defaults to 50304):\n+            Vocabulary size of the Olmo3 model. Defines the number of different tokens that can be represented by the\n+            `inputs_ids` passed when calling [`Olmo3Model`]\n+        hidden_size (`int`, *optional*, defaults to 4096):\n+            Dimension of the hidden representations.\n+        intermediate_size (`int`, *optional*, defaults to 11008):\n+            Dimension of the MLP representations.\n+        num_hidden_layers (`int`, *optional*, defaults to 32):\n+            Number of hidden layers in the Transformer decoder.\n+        num_attention_heads (`int`, *optional*, defaults to 32):\n+            Number of attention heads for each attention layer in the Transformer decoder.\n+        num_key_value_heads (`int`, *optional*):\n+            This is the number of key_value heads that should be used to implement Grouped Query Attention. If\n+            `num_key_value_heads=num_attention_heads`, the model will use Multi Head Attention (MHA), if\n+            `num_key_value_heads=1` the model will use Multi Query Attention (MQA) otherwise GQA is used. When\n+            converting a multi-head checkpoint to a GQA checkpoint, each group key and value head should be constructed\n+            by meanpooling all the original heads within that group. For more details, check out [this\n+            paper](https://huggingface.co/papers/2305.13245). If it is not specified, will default to\n+            `num_attention_heads`.\n+        hidden_act (`str` or `function`, *optional*, defaults to `\"silu\"`):\n+            The non-linear activation function (function or string) in the decoder.\n+        max_position_embeddings (`int`, *optional*, defaults to 2048):\n+            The maximum sequence length that this model might ever be used with.\n+        initializer_range (`float`, *optional*, defaults to 0.02):\n+            The standard deviation of the truncated_normal_initializer for initializing all weight matrices.\n+        use_cache (`bool`, *optional*, defaults to `True`):\n+            Whether or not the model should return the last key/values attentions (not used by all models). Only\n+            relevant if `config.is_decoder=True`.\n+        pad_token_id (`int`, *optional*, defaults to 1):\n+            Padding token id.\n+        bos_token_id (`int`, *optional*):\n+            Beginning of stream token id.\n+        eos_token_id (`int`, *optional*, defaults to 50279):\n+            End of stream token id.\n+        tie_word_embeddings (`bool`, *optional*, defaults to `False`):\n+            Whether to tie weight embeddings\n+        rope_theta (`float`, *optional*, defaults to 10000.0):\n+            The base period of the RoPE embeddings.\n+        rope_scaling (`Dict`, *optional*):\n+            Dictionary containing the scaling configuration for the RoPE embeddings. NOTE: if you apply new rope type\n+            and you expect the model to work on longer `max_position_embeddings`, we recommend you to update this value\n+            accordingly.\n+            Expected contents:\n+                `rope_type` (`str`):\n+                    The sub-variant of RoPE to use. Can be one of ['default', 'linear', 'dynamic', 'yarn', 'longrope',\n+                    'llama3'], with 'default' being the original RoPE implementation.\n+                `factor` (`float`, *optional*):\n+                    Used with all rope types except 'default'. The scaling factor to apply to the RoPE embeddings. In\n+                    most scaling types, a `factor` of x will enable the model to handle sequences of length x *\n+                    original maximum pre-trained length.\n+                `original_max_position_embeddings` (`int`, *optional*):\n+                    Used with 'dynamic', 'longrope' and 'llama3'. The original max position embeddings used during\n+                    pretraining.\n+                `attention_factor` (`float`, *optional*):\n+                    Used with 'yarn' and 'longrope'. The scaling factor to be applied on the attention\n+                    computation. If unspecified, it defaults to value recommended by the implementation, using the\n+                    `factor` field to infer the suggested value.\n+                `beta_fast` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for extrapolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 32.\n+                `beta_slow` (`float`, *optional*):\n+                    Only used with 'yarn'. Parameter to set the boundary for interpolation (only) in the linear\n+                    ramp function. If unspecified, it defaults to 1.\n+                `short_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to short contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `long_factor` (`list[float]`, *optional*):\n+                    Only used with 'longrope'. The scaling factor to be applied to long contexts (<\n+                    `original_max_position_embeddings`). Must be a list of numbers with the same length as the hidden\n+                    size divided by the number of attention heads divided by 2\n+                `low_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to low frequency components of the RoPE\n+                `high_freq_factor` (`float`, *optional*):\n+                    Only used with 'llama3'. Scaling factor applied to high frequency components of the RoPE\n+        attention_bias (`bool`, defaults to `False`, *optional*, defaults to `False`):\n+            Whether to use a bias in the query, key, value and output projection layers during self-attention.\n+        attention_dropout (`float`, *optional*, defaults to 0.0):\n+            The dropout ratio for the attention probabilities.\n+        rms_norm_eps (`float`, *optional*, defaults to 1e-05):\n+            The epsilon used by the rms normalization layers.\n+        sliding_window (`int`, *optional*, defaults to 4096):\n+            Size of the sliding window for sliding window attention.\n+        layer_types (`list`, *optional*):\n+            Attention pattern for each layer. Defaults to sliding window attention\n+            for 3 out of 4 layers, and full attention for every 4th layer.\n+\n+    ```python\n+    >>> from transformers import Olmo3Model, Olmo3Config\n+\n+    >>> # Initializing a Olmo3 7B style configuration\n+    >>> configuration = Olmo3Config()\n+\n+    >>> # Initializing a model from the Olmo3 7B style configuration\n+    >>> model = Olmo3Model(configuration)\n+\n+    >>> # Accessing the model configuration\n+    >>> configuration = model.config\n+    ```\n+    \"\"\"\n+\n+    model_type = \"olmo3\"\n+    base_model_tp_plan = {\n+        \"layers.*.self_attn.q_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.k_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.v_proj\": \"colwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.self_attn.o_proj\": \"rowwise_rep\",  # we need to replicate here due to the added norm on q and k\n+        \"layers.*.mlp.gate_proj\": \"colwise\",\n+        \"layers.*.mlp.up_proj\": \"colwise\",\n+        \"layers.*.mlp.down_proj\": \"rowwise\",\n+    }\n+    base_model_pp_plan = {\n+        \"embed_tokens\": ([\"input_ids\"], [\"inputs_embeds\"]),\n+        \"layers\": ([\"hidden_states\", \"attention_mask\"], [\"hidden_states\"]),\n+        \"norm\": ([\"hidden_states\"], [\"hidden_states\"]),\n+    }\n+\n+    def __init__(\n+        self,\n+        vocab_size=50304,\n+        hidden_size=4096,\n+        intermediate_size=11008,\n+        num_hidden_layers=32,\n+        num_attention_heads=32,\n+        num_key_value_heads=None,\n+        hidden_act=\"silu\",\n+        max_position_embeddings=2048,\n+        initializer_range=0.02,\n+        use_cache=True,\n+        pad_token_id=1,\n+        bos_token_id=None,\n+        eos_token_id=50279,\n+        tie_word_embeddings=False,\n+        rope_theta=10000.0,\n+        rope_scaling=None,\n+        attention_bias=False,\n+        attention_dropout=0.0,\n+        rms_norm_eps=1e-5,\n+        sliding_window=4096,\n+        layer_types=None,\n+        **kwargs,\n+    ):\n+        super().__init__(\n+            vocab_size=vocab_size,\n+            hidden_size=hidden_size,\n+            intermediate_size=intermediate_size,\n+            num_hidden_layers=num_hidden_layers,\n+            num_attention_heads=num_attention_heads,\n+            num_key_value_heads=num_key_value_heads,\n+            hidden_act=hidden_act,\n+            max_position_embeddings=max_position_embeddings,\n+            initializer_range=initializer_range,\n+            use_cache=use_cache,\n+            pad_token_id=pad_token_id,\n+            bos_token_id=bos_token_id,\n+            eos_token_id=eos_token_id,\n+            tie_word_embeddings=tie_word_embeddings,\n+            rope_theta=rope_theta,\n+            rope_scaling=rope_scaling,\n+            attention_bias=attention_bias,\n+            attention_dropout=attention_dropout,\n+            rms_norm_eps=rms_norm_eps,\n+            **kwargs,\n+        )\n+\n+        self.sliding_window = sliding_window\n+        self.layer_types = layer_types\n+        if self.layer_types is None:\n+            self.layer_types = [\n+                \"sliding_attention\" if (i + 1) % 4 != 0 else \"full_attention\" for i in range(self.num_hidden_layers)\n+            ]\n+        layer_type_validation(self.layer_types)\n+\n+    def _rope_scaling_validation(self):\n+        \"\"\"\n+        Validate the `rope_scaling` configuration.\n+        \"\"\"\n+        rope_config_validation(self)\n+\n+\n+class Olmo3RMSNorm(Olmo2RMSNorm):\n+    pass\n+\n+\n+# Olmo3 attention is identical to OLMo 2 attention except:\n+# - Sliding window attention is used for 3 out of 4 layers.\n+class Olmo3Attention(Olmo2Attention):\n+    def __init__(self, config: Olmo3Config, layer_idx: int):\n+        super().__init__(config, layer_idx=layer_idx)\n+        assert config.layer_types is not None\n+        self.attention_type = config.layer_types[layer_idx]\n+        self.sliding_window = config.sliding_window if self.attention_type == \"sliding_attention\" else None\n+\n+    def forward(\n+        self,\n+        hidden_states: torch.Tensor,\n+        position_embeddings: tuple[torch.Tensor, torch.Tensor],\n+        attention_mask: Optional[torch.Tensor],\n+        past_key_values: Optional[Cache] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> tuple[torch.Tensor, Optional[torch.Tensor]]:\n+        input_shape = hidden_states.shape[:-1]\n+        hidden_shape = (*input_shape, -1, self.head_dim)\n+\n+        query_states = self.q_norm(self.q_proj(hidden_states))\n+        key_states = self.k_norm(self.k_proj(hidden_states))\n+        value_states = self.v_proj(hidden_states)\n+\n+        query_states = query_states.view(hidden_shape).transpose(1, 2)\n+        key_states = key_states.view(hidden_shape).transpose(1, 2)\n+        value_states = value_states.view(hidden_shape).transpose(1, 2)\n+\n+        cos, sin = position_embeddings\n+        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n+\n+        if past_key_values is not None:\n+            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n+            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n+            key_states, value_states = past_key_values.update(key_states, value_states, self.layer_idx, cache_kwargs)\n+\n+        attention_interface: Callable = eager_attention_forward\n+        if self.config._attn_implementation != \"eager\":\n+            attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n+\n+        attn_output, attn_weights = attention_interface(\n+            self,\n+            query_states,\n+            key_states,\n+            value_states,\n+            attention_mask,\n+            dropout=0.0 if not self.training else self.attention_dropout,\n+            scaling=self.scaling,\n+            sliding_window=self.sliding_window,\n+            **kwargs,\n+        )\n+\n+        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n+        attn_output = self.o_proj(attn_output)\n+        return attn_output, attn_weights\n+\n+\n+class Olmo3DecoderLayer(Olmo2DecoderLayer):\n+    pass\n+\n+\n+# OLMo 3 RoPE is identical to OLMo 2 RoPE, except:\n+# - RoPE scaling is not applied to sliding window attention layers.\n+class Olmo3RotaryEmbedding(Olmo2RotaryEmbedding):\n+    def __init__(self, config: Olmo3Config, device=None, rope_type: Optional[str] = None):\n+        nn.Module.__init__(self)\n+        if rope_type is not None:\n+            self.rope_type = rope_type\n+        elif hasattr(config, \"rope_scaling\") and isinstance(config.rope_scaling, dict):\n+            # BC: \"rope_type\" was originally \"type\"\n+            self.rope_type = config.rope_scaling.get(\"rope_type\", config.rope_scaling.get(\"type\"))\n+        else:\n+            self.rope_type = \"default\"\n+        assert self.rope_type is not None\n+\n+        self.max_seq_len_cached = config.max_position_embeddings\n+        self.original_max_seq_len = config.max_position_embeddings\n+\n+        self.config = config\n+        self.rope_init_fn = ROPE_INIT_FUNCTIONS[self.rope_type]\n+\n+        inv_freq, self.attention_scaling = self.rope_init_fn(self.config, device)\n+        self.register_buffer(\"inv_freq\", inv_freq, persistent=False)\n+        self.original_inv_freq = self.inv_freq\n+\n+\n+class Olmo3PreTrainedModel(Olmo2PreTrainedModel):\n+    pass\n+\n+\n+# The OLMo 3 model is identical to the OLMo 2 model, except:\n+# - Sliding window attention is used for 3 out of 4 layers.\n+# - RoPE scaling is not applied to sliding window attention layers.\n+class Olmo3Model(Olmo2Model):\n+    def __init__(self, config: Olmo3Config):\n+        super().__init__(config)\n+        self.norm = Olmo3RMSNorm(config.hidden_size, eps=config.rms_norm_eps)\n+        self.layers = nn.ModuleList(\n+            [Olmo3DecoderLayer(config, layer_idx) for layer_idx in range(config.num_hidden_layers)]\n+        )\n+        self.rotary_embs = nn.ModuleDict(\n+            {\n+                \"sliding_attention\": Olmo3RotaryEmbedding(config=config, rope_type=\"default\"),\n+                \"full_attention\": Olmo3RotaryEmbedding(config=config),\n+            }\n+        )\n+        del self.rotary_emb\n+\n+    def forward(\n+        self,\n+        input_ids: Optional[torch.LongTensor] = None,\n+        attention_mask: Optional[torch.Tensor] = None,\n+        position_ids: Optional[torch.LongTensor] = None,\n+        past_key_values: Optional[Cache] = None,\n+        inputs_embeds: Optional[torch.FloatTensor] = None,\n+        cache_position: Optional[torch.LongTensor] = None,\n+        use_cache: Optional[bool] = None,\n+        **kwargs: Unpack[TransformersKwargs],\n+    ) -> BaseModelOutputWithPast:\n+        if (input_ids is None) ^ (inputs_embeds is not None):\n+            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n+\n+        if inputs_embeds is None:\n+            inputs_embeds: torch.Tensor = self.embed_tokens(input_ids)\n+\n+        if use_cache and past_key_values is None:\n+            past_key_values = DynamicCache(config=self.config)\n+\n+        if cache_position is None:\n+            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n+            cache_position: torch.Tensor = torch.arange(\n+                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n+            )\n+\n+        if position_ids is None:\n+            position_ids = cache_position.unsqueeze(0)\n+\n+        # It may already have been prepared by e.g. `generate`\n+        if not isinstance(causal_mask_mapping := attention_mask, dict):\n+            # Prepare mask arguments\n+            mask_kwargs = {\n+                \"config\": self.config,\n+                \"input_embeds\": inputs_embeds,\n+                \"attention_mask\": attention_mask,\n+                \"cache_position\": cache_position,\n+                \"past_key_values\": past_key_values,\n+                \"position_ids\": position_ids,\n+            }\n+            # Create the masks\n+            causal_mask_mapping = {\n+                \"full_attention\": create_causal_mask(**mask_kwargs),\n+                \"sliding_attention\": create_sliding_window_causal_mask(**mask_kwargs),\n+            }\n+\n+        hidden_states = inputs_embeds\n+        position_embeddings_mapping = {\n+            \"sliding_attention\": self.rotary_embs[\"sliding_attention\"](hidden_states, position_ids),\n+            \"full_attention\": self.rotary_embs[\"full_attention\"](hidden_states, position_ids),\n+        }\n+\n+        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n+            hidden_states = decoder_layer(\n+                hidden_states,\n+                attention_mask=causal_mask_mapping[decoder_layer.self_attn.attention_type],\n+                position_ids=position_ids,\n+                past_key_values=past_key_values,\n+                cache_position=cache_position,\n+                position_embeddings=position_embeddings_mapping[decoder_layer.self_attn.attention_type],\n+                **kwargs,\n+            )\n+\n+        hidden_states = self.norm(hidden_states)\n+        return BaseModelOutputWithPast(\n+            last_hidden_state=hidden_states,\n+            past_key_values=past_key_values,\n+        )\n+\n+\n+class Olmo3ForCausalLM(Olmo2ForCausalLM):\n+    pass\n+\n+\n+__all__ = [\n+    \"Olmo3Config\",\n+    \"Olmo3ForCausalLM\",\n+    \"Olmo3Model\",\n+    \"Olmo3PreTrainedModel\",  # noqa: F822\n+]"
        },
        {
            "sha": "e69de29bb2d1d6434b8b29ae775ad8c2e48c5391",
            "filename": "tests/models/olmo3/__init__.py",
            "status": "added",
            "additions": 0,
            "deletions": 0,
            "changes": 0,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/tests%2Fmodels%2Folmo3%2F__init__.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/tests%2Fmodels%2Folmo3%2F__init__.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo3%2F__init__.py?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1"
        },
        {
            "sha": "973bb7aeec19724b18a6f5169a8d3742c65bac2e",
            "filename": "tests/models/olmo3/test_modeling_olmo3.py",
            "status": "added",
            "additions": 299,
            "deletions": 0,
            "changes": 299,
            "blob_url": "https://github.com/huggingface/transformers/blob/d0af4269ec260b9c4aeeda24c346a469e44799e1/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d0af4269ec260b9c4aeeda24c346a469e44799e1/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Folmo3%2Ftest_modeling_olmo3.py?ref=d0af4269ec260b9c4aeeda24c346a469e44799e1",
            "patch": "@@ -0,0 +1,299 @@\n+# coding=utf-8\n+# Copyright 2025 the HuggingFace Team. All rights reserved.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\"\"\"Testing suite for the PyTorch Olmo3 model.\"\"\"\n+\n+import unittest\n+\n+import pytest\n+from packaging import version\n+from parameterized import parameterized\n+\n+from transformers import Olmo3Config, is_torch_available, set_seed\n+from transformers.generation.configuration_utils import GenerationConfig\n+from transformers.models.auto.tokenization_auto import AutoTokenizer\n+from transformers.testing_utils import (\n+    Expectations,\n+    cleanup,\n+    require_torch,\n+    slow,\n+    torch_device,\n+)\n+\n+from ...causal_lm_tester import CausalLMModelTest, CausalLMModelTester\n+from ...test_modeling_common import ids_tensor\n+\n+\n+if is_torch_available():\n+    import torch\n+\n+    from transformers import (\n+        Olmo3ForCausalLM,\n+        Olmo3Model,\n+    )\n+    from transformers.models.olmo3.modeling_olmo3 import Olmo3RotaryEmbedding\n+\n+\n+class Olmo3ModelTester(CausalLMModelTester):\n+    if is_torch_available():\n+        config_class = Olmo3Config\n+        base_model_class = Olmo3Model\n+        causal_lm_class = Olmo3ForCausalLM\n+\n+\n+@require_torch\n+class Olmo3ModelTest(CausalLMModelTest, unittest.TestCase):\n+    all_model_classes = (Olmo3Model, Olmo3ForCausalLM) if is_torch_available() else ()\n+    pipeline_model_mapping = (\n+        {\n+            \"feature-extraction\": Olmo3Model,\n+            \"text-generation\": Olmo3ForCausalLM,\n+        }\n+        if is_torch_available()\n+        else {}\n+    )\n+    test_headmasking = False\n+    test_pruning = False\n+    fx_compatible = False\n+    test_torchscript = False\n+    test_all_params_have_gradient = False\n+    model_tester_class = Olmo3ModelTester\n+    rotary_embedding_layer = Olmo3RotaryEmbedding\n+\n+    # Need to use `0.8` instead of `0.9` for `test_cpu_offload`\n+    # This is because we are hitting edge cases with the causal_mask buffer\n+    model_split_percents = [0.5, 0.7, 0.8]\n+\n+    # used in `test_torch_compile_for_training`\n+    _torch_compile_train_cls = Olmo3ForCausalLM if is_torch_available() else None\n+\n+    @parameterized.expand([(\"linear\",), (\"dynamic\",), (\"yarn\",)])\n+    def test_model_rope_scaling_from_config(self, scaling_type):\n+        if self.rotary_embedding_layer is None:\n+            self.skipTest(\"Rotary embedding layer not set\")\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # Rope only gets applied to full attention layers in Olmo3, so make all layers full attention.\n+        config.layer_types = [\"full_attention\"] * len(config.layer_types)\n+\n+        short_input = ids_tensor([1, 10], config.vocab_size)\n+        long_input = ids_tensor([1, int(config.max_position_embeddings * 1.5)], config.vocab_size)\n+\n+        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n+        original_model = self.model_tester_class.base_model_class(config)\n+        original_model.to(torch_device)\n+        original_model.eval()\n+        original_short_output = original_model(short_input).last_hidden_state\n+        original_long_output = original_model(long_input).last_hidden_state\n+\n+        set_seed(42)  # Fixed seed at init time so the two models get the same random weights\n+        config.rope_scaling = {\"type\": scaling_type, \"factor\": 10.0}\n+        scaled_model = self.model_tester_class.base_model_class(config)\n+        scaled_model.to(torch_device)\n+        scaled_model.eval()\n+        scaled_short_output = scaled_model(short_input).last_hidden_state\n+        scaled_long_output = scaled_model(long_input).last_hidden_state\n+\n+        # Dynamic scaling does not change the RoPE embeddings until it receives an input longer than the original\n+        # maximum sequence length, so the outputs for the short input should match.\n+        if scaling_type == \"dynamic\":\n+            torch.testing.assert_close(original_short_output, scaled_short_output, rtol=1e-5, atol=1e-5)\n+        else:\n+            self.assertFalse(torch.allclose(original_short_output, scaled_short_output, atol=1e-5))\n+\n+        # The output should be different for long inputs\n+        self.assertFalse(torch.allclose(original_long_output, scaled_long_output, atol=1e-5))\n+\n+    def test_model_rope_scaling_frequencies(self):\n+        \"\"\"Tests the frequency properties of the different RoPE scaling types on the model RoPE layer.\"\"\"\n+        config, _ = self.model_tester.prepare_config_and_inputs_for_common()\n+\n+        # Parent test class's attempt to find Olmo3 rope fails, so we pass here explicitly.\n+        rope_class = Olmo3RotaryEmbedding\n+\n+        scaling_factor = 10\n+        short_input_length = 10\n+        long_input_length = int(config.max_position_embeddings * 1.5)\n+\n+        # Inputs\n+        x = torch.randn(\n+            1, dtype=torch.float32, device=torch_device\n+        )  # used exclusively to get the dtype and the device\n+        position_ids_short = torch.arange(short_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_short = position_ids_short.unsqueeze(0)\n+        position_ids_long = torch.arange(long_input_length, dtype=torch.long, device=torch_device)\n+        position_ids_long = position_ids_long.unsqueeze(0)\n+\n+        # Sanity check original RoPE\n+        config.rope_scaling = {\"rope_type\": \"default\"}\n+        original_rope = rope_class(config=config).to(torch_device)\n+        original_cos_short, original_sin_short = original_rope(x, position_ids_short)\n+        original_cos_long, original_sin_long = original_rope(x, position_ids_long)\n+        torch.testing.assert_close(original_cos_short, original_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(original_sin_short, original_sin_long[:, :short_input_length, :])\n+\n+        # Sanity check linear RoPE scaling\n+        # New position \"x\" should match original position with index \"x/scaling_factor\"\n+        config.rope_scaling = {\"rope_type\": \"linear\", \"factor\": scaling_factor}\n+        linear_scaling_rope = rope_class(config=config).to(torch_device)\n+        linear_cos_short, linear_sin_short = linear_scaling_rope(x, position_ids_short)\n+        linear_cos_long, linear_sin_long = linear_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(linear_cos_short, linear_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(linear_sin_short, linear_sin_long[:, :short_input_length, :])\n+        for new_position in range(0, long_input_length, scaling_factor):\n+            original_position = int(new_position // scaling_factor)\n+            torch.testing.assert_close(linear_cos_long[:, new_position, :], original_cos_long[:, original_position, :])\n+            torch.testing.assert_close(linear_sin_long[:, new_position, :], original_sin_long[:, original_position, :])\n+\n+        # Sanity check Dynamic NTK RoPE scaling\n+        # Scaling should only be observed after a long input is fed. We can observe that the frequencies increase\n+        # with scaling_factor (or that `inv_freq` decreases)\n+        config.rope_scaling = {\"rope_type\": \"dynamic\", \"factor\": scaling_factor}\n+        ntk_scaling_rope = rope_class(config=config).to(torch_device)\n+        ntk_cos_short, ntk_sin_short = ntk_scaling_rope(x, position_ids_short)\n+        ntk_cos_long, ntk_sin_long = ntk_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(ntk_cos_short, original_cos_short)\n+        torch.testing.assert_close(ntk_sin_short, original_sin_short)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(ntk_cos_long, original_cos_long)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(ntk_sin_long, original_sin_long)\n+        self.assertTrue((ntk_scaling_rope.inv_freq <= original_rope.inv_freq).all())\n+\n+        # Sanity check Yarn RoPE scaling\n+        # Scaling should be over the entire input\n+        config.rope_scaling = {\"rope_type\": \"yarn\", \"factor\": scaling_factor}\n+        yarn_scaling_rope = rope_class(config=config).to(torch_device)\n+        yarn_cos_short, yarn_sin_short = yarn_scaling_rope(x, position_ids_short)\n+        yarn_cos_long, yarn_sin_long = yarn_scaling_rope(x, position_ids_long)\n+        torch.testing.assert_close(yarn_cos_short, yarn_cos_long[:, :short_input_length, :])\n+        torch.testing.assert_close(yarn_sin_short, yarn_sin_long[:, :short_input_length, :])\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_cos_short, original_cos_short)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_sin_short, original_sin_short)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_cos_long, original_cos_long)\n+        with self.assertRaises(AssertionError):\n+            torch.testing.assert_close(yarn_sin_long, original_sin_long)\n+\n+\n+@require_torch\n+class Olmo3IntegrationTest(unittest.TestCase):\n+    def setUp(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n+    @slow\n+    def test_model_7b_logits(self):\n+        input_ids = [[1, 306, 4658, 278, 6593, 310, 2834, 338]]\n+        model = Olmo3ForCausalLM.from_pretrained(\"shanearora/2025-sep-a-base-model\").to(\n+            torch_device, dtype=torch.bfloat16\n+        )\n+        out = model(torch.tensor(input_ids, device=torch_device)).logits.float()\n+        # Expected mean on dim = -1\n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 8): [[1.9575, -2.4659, 0.5985, 1.3795, -0.5207, -0.9844, -2.7795, -1.0069]],\n+            }\n+        )\n+        EXPECTED_MEAN = torch.tensor(expectations.get_expectation(), device=torch_device)\n+        torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n+        # slicing logits[0, 0, 0:30]\n+        expectations = Expectations(\n+            {\n+                (\"cuda\", 8): [8.5625, 5.7812, 4.4688, 2.7031, 3.1094, 4.8125, 5.7188, 3.4219, 2.3906, 2.0938, 3.9844, 5.4688, 3.5312, 5.0938, 2.7656, 8.8125, 9.4375, 9.0625, 8.5000, 8.1875, 7.8750, 7.5312, 7.3125, 7.2812, 7.0000, 2.5625, 4.0312, 3.1719, 7.6562, 4.5625],\n+            }\n+        )  # fmt: skip\n+        EXPECTED_SLICE = torch.tensor(expectations.get_expectation(), device=torch_device)\n+        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-2, atol=1e-2)\n+\n+    @slow\n+    def test_model_7b_greedy_generation(self):\n+        EXPECTED_TEXT_COMPLETION = \"\"\"Simply put, the theory of relativity states that 1) the laws of physics are the same for all observers, and 2) the speed of light is the same for all observers. The first part of the theory is called the principle of relativity, and the second part is called the principle of the constancy of the speed of light. The theory of rel\"\"\"\n+        prompt = \"Simply put, the theory of relativity states that \"\n+        tokenizer = AutoTokenizer.from_pretrained(\"allenai/dolma2-tokenizer\", device_map=\"auto\")\n+        model = Olmo3ForCausalLM.from_pretrained(\"shanearora/2025-sep-a-base-model\", device_map=\"auto\")\n+        input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.device)\n+\n+        # greedy generation outputs\n+        generated_ids = model.generate(input_ids, max_new_tokens=64, top_p=None, temperature=1, do_sample=False)\n+        text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n+\n+    @pytest.mark.torch_export_test\n+    @slow\n+    def test_export_static_cache(self):\n+        if version.parse(torch.__version__) < version.parse(\"2.4.0\"):\n+            self.skipTest(reason=\"This test requires torch >= 2.4 to run.\")\n+\n+        from transformers.integrations.executorch import (\n+            TorchExportableModuleWithStaticCache,\n+            convert_and_export_with_cache,\n+        )\n+\n+        olmo3_model = \"shanearora/2025-sep-a-base-model\"\n+\n+        tokenizer = AutoTokenizer.from_pretrained(olmo3_model, pad_token=\"</s>\", padding_side=\"right\")\n+        EXPECTED_TEXT_COMPLETION = [\n+            \"Simply put, the theory of relativity states that 1) the laws of physics are the same for all observers, and 2\",\n+        ]\n+        max_generation_length = tokenizer(EXPECTED_TEXT_COMPLETION, return_tensors=\"pt\", padding=True)[\n+            \"input_ids\"\n+        ].shape[-1]\n+\n+        # Load model\n+        device = \"cpu\"  # TODO (joao / export experts): should be on `torch_device`, but causes GPU OOM\n+        dtype = torch.bfloat16\n+        cache_implementation = \"static\"\n+        attn_implementation = \"sdpa\"\n+        batch_size = 1\n+        generation_config = GenerationConfig(\n+            use_cache=True,\n+            cache_implementation=cache_implementation,\n+            max_length=max_generation_length,\n+            cache_config={\n+                \"batch_size\": batch_size,\n+                \"max_cache_len\": max_generation_length,\n+            },\n+        )\n+        model = Olmo3ForCausalLM.from_pretrained(\n+            olmo3_model,\n+            device_map=device,\n+            dtype=dtype,\n+            attn_implementation=attn_implementation,\n+            generation_config=generation_config,\n+        )\n+\n+        prompts = [\"Simply put, the theory of relativity states that \"]\n+        prompt_tokens = tokenizer(prompts, return_tensors=\"pt\", padding=True).to(model.device)\n+        prompt_token_ids = prompt_tokens[\"input_ids\"]\n+        max_new_tokens = max_generation_length - prompt_token_ids.shape[-1]\n+\n+        # Static Cache + eager\n+        eager_generated_ids = model.generate(\n+            **prompt_tokens, max_new_tokens=max_new_tokens, do_sample=False, cache_implementation=cache_implementation\n+        )\n+        eager_generated_text = tokenizer.batch_decode(eager_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, eager_generated_text)\n+\n+        # Static Cache + export\n+        exported_program = convert_and_export_with_cache(model)\n+        ep_generated_ids = TorchExportableModuleWithStaticCache.generate(\n+            exported_program=exported_program, prompt_token_ids=prompt_token_ids, max_new_tokens=max_new_tokens\n+        )\n+        ep_generated_text = tokenizer.batch_decode(ep_generated_ids, skip_special_tokens=True)\n+        self.assertEqual(EXPECTED_TEXT_COMPLETION, ep_generated_text)"
        }
    ],
    "stats": {
        "total": 2103,
        "additions": 2103,
        "deletions": 0
    }
}