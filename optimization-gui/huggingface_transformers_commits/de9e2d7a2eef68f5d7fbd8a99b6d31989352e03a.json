{
    "author": "ydshieh",
    "message": "Skip some flex attn tests (#40519)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "de9e2d7a2eef68f5d7fbd8a99b6d31989352e03a",
    "files": [
        {
            "sha": "cdb78895e8668fd06251ec49b0f2e7bbd08da6be",
            "filename": "tests/models/cohere2/test_modeling_cohere2.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de9e2d7a2eef68f5d7fbd8a99b6d31989352e03a/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de9e2d7a2eef68f5d7fbd8a99b6d31989352e03a/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2%2Ftest_modeling_cohere2.py?ref=de9e2d7a2eef68f5d7fbd8a99b6d31989352e03a",
            "patch": "@@ -231,6 +231,12 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         we need to correctly slice the attention mask in all cases (because we use a hybrid cache).\n         Outputs for every attention functions should be coherent and identical.\n         \"\"\"\n+        # Impossible to test it with this model (even with < 100 tokens), probably due to the compilation of a large model.\n+        if attn_implementation == \"flex_attention\":\n+            self.skipTest(\n+                reason=\"`flex_attention` gives `torch._inductor.exc.InductorError: RuntimeError: No valid triton configs. OutOfMemoryError: out of resource: triton_tem_fused_0 Required: 147456 Hardware limit:101376 Reducing block sizes or `num_stages` may help.`\"\n+            )\n+\n         if attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():\n             self.skipTest(\"FlashAttention2 is required for this test.\")\n "
        },
        {
            "sha": "9717ea94297224f3bb2ffbf183094ef6ba02b526",
            "filename": "tests/models/gemma2/test_modeling_gemma2.py",
            "status": "modified",
            "additions": 12,
            "deletions": 0,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/de9e2d7a2eef68f5d7fbd8a99b6d31989352e03a/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de9e2d7a2eef68f5d7fbd8a99b6d31989352e03a/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma2%2Ftest_modeling_gemma2.py?ref=de9e2d7a2eef68f5d7fbd8a99b6d31989352e03a",
            "patch": "@@ -377,6 +377,12 @@ def test_generation_beyond_sliding_window(self, attn_implementation: str):\n         we need to correctly slice the attention mask in all cases (because we use a hybrid cache).\n         Outputs for every attention functions should be coherent and identical.\n         \"\"\"\n+        # Impossible to test it with this model (even with < 100 tokens), probably due to the compilation of a large model.\n+        if attn_implementation == \"flex_attention\":\n+            self.skipTest(\n+                reason=\"`flex_attention` gives `torch._inductor.exc.InductorError: RuntimeError: No valid triton configs. OutOfMemoryError: out of resource: triton_tem_fused_0 Required: 147456 Hardware limit:101376 Reducing block sizes or `num_stages` may help.`\"\n+            )\n+\n         if attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():\n             self.skipTest(\"FlashAttention2 is required for this test.\")\n \n@@ -417,6 +423,12 @@ def test_generation_beyond_sliding_window_dynamic(self, attn_implementation: str\n         Same as above, but explicitly setting the cache to Dynamic, as it's otherwise static by default for\n         the model on the hub\n         \"\"\"\n+        # Impossible to test it with this model (even with < 100 tokens), probably due to the compilation of a large model.\n+        if attn_implementation == \"flex_attention\":\n+            self.skipTest(\n+                reason=\"`flex_attention` gives `torch._inductor.exc.InductorError: RuntimeError: No valid triton configs. OutOfMemoryError: out of resource: triton_tem_fused_0 Required: 147456 Hardware limit:101376 Reducing block sizes or `num_stages` may help.`\"\n+            )\n+\n         if attn_implementation == \"flash_attention_2\" and not is_flash_attn_2_available():\n             self.skipTest(\"FlashAttention2 is required for this test.\")\n "
        },
        {
            "sha": "06e60c90a9697c511f81e2bc3ffd8258fd50c5d9",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/de9e2d7a2eef68f5d7fbd8a99b6d31989352e03a/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de9e2d7a2eef68f5d7fbd8a99b6d31989352e03a/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=de9e2d7a2eef68f5d7fbd8a99b6d31989352e03a",
            "patch": "@@ -199,6 +199,12 @@ def setUp(self):\n         self.model_tester = InternVLVisionText2TextModelTester(self)\n         self.config_tester = ConfigTester(self, config_class=InternVLConfig, has_text_modality=False)\n \n+    @unittest.skip(\n+        reason=\"Failing with `torch._inductor.exc.InductorError: RuntimeError: No valid triton configs. OutOfMemoryError: out of resource: triton_tem_fused_0 Required: 147456 Hardware limit:101376 Reducing block sizes or `num_stages` may help.`\"\n+    )\n+    def test_flex_attention_with_grads(self):\n+        pass\n+\n     def test_config(self):\n         self.config_tester.run_common_tests()\n "
        }
    ],
    "stats": {
        "total": 24,
        "additions": 24,
        "deletions": 0
    }
}