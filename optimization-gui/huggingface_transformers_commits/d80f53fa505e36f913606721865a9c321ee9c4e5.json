{
    "author": "unknown",
    "message": "[generate] Fix `vocab_size` access for multimodal models (#37937)\n\nImplements last migrations for generation from `config.vocab_size` to `config.get_text_config().vocab.size`\n\nIn doing so, we enable multimodal models to fully leverage all existing generation features.",
    "sha": "d80f53fa505e36f913606721865a9c321ee9c4e5",
    "files": [
        {
            "sha": "36e65949a48f06b7d6e13c2d511fa1116f910222",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 4,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/d80f53fa505e36f913606721865a9c321ee9c4e5/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/d80f53fa505e36f913606721865a9c321ee9c4e5/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=d80f53fa505e36f913606721865a9c321ee9c4e5",
            "patch": "@@ -968,7 +968,7 @@ def _get_candidate_generator(\n                 atm_translator = AssistantVocabTranslatorCache.get_translator(\n                     target_tokenizer,\n                     assistant_tokenizer,\n-                    self.config.vocab_size,\n+                    self.config.get_text_config().vocab_size,\n                     assistant_model=assistant_model,\n                     assistant_prune_lm_head=True,  # prune LM head of assistant model\n                 )\n@@ -1234,7 +1234,9 @@ def _get_logits_processor(\n         # Watermarking should be after all logits processing is finished (see #34630)\n         if generation_config.watermarking_config is not None:\n             processors.append(\n-                generation_config.watermarking_config.construct_processor(self.config.vocab_size, device)\n+                generation_config.watermarking_config.construct_processor(\n+                    self.config.get_text_config().vocab_size, device\n+                )\n             )\n \n         # `LogitNormalization` should always be the last logit processor, when present\n@@ -1412,7 +1414,7 @@ def compute_transition_scores(\n \n         # 3. Optionally normalize the logits (across the vocab dimension)\n         if normalize_logits:\n-            scores = scores.reshape(-1, self.config.vocab_size, scores.shape[-1])\n+            scores = scores.reshape(-1, self.config.get_text_config().vocab_size, scores.shape[-1])\n             scores = torch.nn.functional.log_softmax(scores, dim=1)\n             scores = scores.reshape(-1, scores.shape[-1])\n \n@@ -1426,7 +1428,7 @@ def compute_transition_scores(\n         beam_indices[beam_indices_mask] = 0\n \n         # 6. multiply beam_indices with vocab size to gather correctly from scores\n-        beam_sequence_indices = beam_indices * self.config.vocab_size\n+        beam_sequence_indices = beam_indices * self.config.get_text_config().vocab_size\n \n         # 7. Define which indices contributed to scores\n         cut_idx = sequences.shape[-1] - max_beam_length"
        }
    ],
    "stats": {
        "total": 10,
        "additions": 6,
        "deletions": 4
    }
}