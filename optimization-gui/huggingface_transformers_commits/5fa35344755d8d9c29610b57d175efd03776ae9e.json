{
    "author": "ivarflakstad",
    "message": "Use AMD CI workflow defined in hf-workflows (#35058)\n\n* Use AMD CI workflow defined in hf-workflows",
    "sha": "5fa35344755d8d9c29610b57d175efd03776ae9e",
    "files": [
        {
            "sha": "6109faca00932e9a2fdb11c1098aba1d7f7a8a5b",
            "filename": ".github/workflows/self-scheduled-amd-mi210-caller.yml",
            "status": "modified",
            "additions": 55,
            "deletions": 55,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fa35344755d8d9c29610b57d175efd03776ae9e/.github%2Fworkflows%2Fself-scheduled-amd-mi210-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fa35344755d8d9c29610b57d175efd03776ae9e/.github%2Fworkflows%2Fself-scheduled-amd-mi210-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-amd-mi210-caller.yml?ref=5fa35344755d8d9c29610b57d175efd03776ae9e",
            "patch": "@@ -1,55 +1,55 @@\n-name: Self-hosted runner (AMD mi210 scheduled CI caller)\r\n-\r\n-on:\r\n-  workflow_run:\r\n-    workflows: [\"Self-hosted runner (AMD scheduled CI caller)\"]\r\n-    branches: [\"main\"]\r\n-    types: [completed]\r\n-  push:\r\n-    branches:\r\n-      - run_amd_scheduled_ci_caller*\r\n-\r\n-jobs:\r\n-  model-ci:\r\n-    name: Model CI\r\n-    uses: ./.github/workflows/self-scheduled-amd.yml\r\n-    with:\r\n-      job: run_models_gpu\r\n-      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n-      runner: mi210\r\n-      docker: huggingface/transformers-pytorch-amd-gpu\r\n-      ci_event: Scheduled CI (AMD) - mi210\r\n-    secrets: inherit\r\n-\r\n-  torch-pipeline:\r\n-    name: Torch pipeline CI\r\n-    uses: ./.github/workflows/self-scheduled-amd.yml\r\n-    with:\r\n-      job: run_pipelines_torch_gpu\r\n-      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n-      runner: mi210\r\n-      docker: huggingface/transformers-pytorch-amd-gpu\r\n-      ci_event: Scheduled CI (AMD) - mi210\r\n-    secrets: inherit\r\n-\r\n-  example-ci:\r\n-    name: Example CI\r\n-    uses: ./.github/workflows/self-scheduled-amd.yml\r\n-    with:\r\n-      job: run_examples_gpu\r\n-      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n-      runner: mi210\r\n-      docker: huggingface/transformers-pytorch-amd-gpu\r\n-      ci_event: Scheduled CI (AMD) - mi210\r\n-    secrets: inherit\r\n-\r\n-  deepspeed-ci:\r\n-    name: DeepSpeed CI\r\n-    uses: ./.github/workflows/self-scheduled-amd.yml\r\n-    with:\r\n-      job: run_torch_cuda_extensions_gpu\r\n-      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n-      runner: mi210\r\n-      docker: huggingface/transformers-pytorch-deepspeed-amd-gpu\r\n-      ci_event: Scheduled CI (AMD) - mi210\r\n-    secrets: inherit\r\n+name: Self-hosted runner (AMD mi210 scheduled CI caller)\n+\n+on:\n+  workflow_run:\n+    workflows: [\"Self-hosted runner (AMD scheduled CI caller)\"]\n+    branches: [\"main\"]\n+    types: [completed]\n+  push:\n+    branches:\n+      - run_amd_scheduled_ci_caller*\n+\n+jobs:\n+  model-ci:\n+    name: Model CI\n+    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main\n+    with:\n+      job: run_models_gpu\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\n+      runner: mi210\n+      docker: huggingface/transformers-pytorch-amd-gpu\n+      ci_event: Scheduled CI (AMD) - mi210\n+    secrets: inherit\n+\n+  torch-pipeline:\n+    name: Torch pipeline CI\n+    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main\n+    with:\n+      job: run_pipelines_torch_gpu\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\n+      runner: mi210\n+      docker: huggingface/transformers-pytorch-amd-gpu\n+      ci_event: Scheduled CI (AMD) - mi210\n+    secrets: inherit\n+\n+  example-ci:\n+    name: Example CI\n+    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main\n+    with:\n+      job: run_examples_gpu\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\n+      runner: mi210\n+      docker: huggingface/transformers-pytorch-amd-gpu\n+      ci_event: Scheduled CI (AMD) - mi210\n+    secrets: inherit\n+\n+  deepspeed-ci:\n+    name: DeepSpeed CI\n+    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main\n+    with:\n+      job: run_torch_cuda_extensions_gpu\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\n+      runner: mi210\n+      docker: huggingface/transformers-pytorch-deepspeed-amd-gpu\n+      ci_event: Scheduled CI (AMD) - mi210\n+    secrets: inherit"
        },
        {
            "sha": "a33b6e579c0ef33a84471080a680672cc7f7ec0b",
            "filename": ".github/workflows/self-scheduled-amd-mi250-caller.yml",
            "status": "modified",
            "additions": 55,
            "deletions": 55,
            "changes": 110,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fa35344755d8d9c29610b57d175efd03776ae9e/.github%2Fworkflows%2Fself-scheduled-amd-mi250-caller.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fa35344755d8d9c29610b57d175efd03776ae9e/.github%2Fworkflows%2Fself-scheduled-amd-mi250-caller.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-amd-mi250-caller.yml?ref=5fa35344755d8d9c29610b57d175efd03776ae9e",
            "patch": "@@ -1,55 +1,55 @@\n-name: Self-hosted runner (AMD mi250 scheduled CI caller)\r\n-\r\n-on:\r\n-  workflow_run:\r\n-    workflows: [\"Self-hosted runner (AMD scheduled CI caller)\"]\r\n-    branches: [\"main\"]\r\n-    types: [completed]\r\n-  push:\r\n-    branches:\r\n-      - run_amd_scheduled_ci_caller*\r\n-\r\n-jobs:\r\n-  model-ci:\r\n-    name: Model CI\r\n-    uses: ./.github/workflows/self-scheduled-amd.yml\r\n-    with:\r\n-      job: run_models_gpu\r\n-      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n-      runner: mi250\r\n-      docker: huggingface/transformers-pytorch-amd-gpu\r\n-      ci_event: Scheduled CI (AMD) - mi250\r\n-    secrets: inherit\r\n-\r\n-  torch-pipeline:\r\n-    name: Torch pipeline CI\r\n-    uses: ./.github/workflows/self-scheduled-amd.yml\r\n-    with:\r\n-      job: run_pipelines_torch_gpu\r\n-      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n-      runner: mi250\r\n-      docker: huggingface/transformers-pytorch-amd-gpu\r\n-      ci_event: Scheduled CI (AMD) - mi250\r\n-    secrets: inherit\r\n-\r\n-  example-ci:\r\n-    name: Example CI\r\n-    uses: ./.github/workflows/self-scheduled-amd.yml\r\n-    with:\r\n-      job: run_examples_gpu\r\n-      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n-      runner: mi250\r\n-      docker: huggingface/transformers-pytorch-amd-gpu\r\n-      ci_event: Scheduled CI (AMD) - mi250\r\n-    secrets: inherit\r\n-\r\n-  deepspeed-ci:\r\n-    name: DeepSpeed CI\r\n-    uses: ./.github/workflows/self-scheduled-amd.yml\r\n-    with:\r\n-      job: run_torch_cuda_extensions_gpu\r\n-      slack_report_channel: \"#transformers-ci-daily-amd\"\r\n-      runner: mi250\r\n-      docker: huggingface/transformers-pytorch-deepspeed-amd-gpu\r\n-      ci_event: Scheduled CI (AMD) - mi250\r\n-    secrets: inherit\r\n+name: Self-hosted runner (AMD mi250 scheduled CI caller)\n+\n+on:\n+  workflow_run:\n+    workflows: [\"Self-hosted runner (AMD scheduled CI caller)\"]\n+    branches: [\"main\"]\n+    types: [completed]\n+  push:\n+    branches:\n+      - run_amd_scheduled_ci_caller*\n+\n+jobs:\n+  model-ci:\n+    name: Model CI\n+    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main\n+    with:\n+      job: run_models_gpu\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\n+      runner: mi250\n+      docker: huggingface/transformers-pytorch-amd-gpu\n+      ci_event: Scheduled CI (AMD) - mi250\n+    secrets: inherit\n+\n+  torch-pipeline:\n+    name: Torch pipeline CI\n+    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main\n+    with:\n+      job: run_pipelines_torch_gpu\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\n+      runner: mi250\n+      docker: huggingface/transformers-pytorch-amd-gpu\n+      ci_event: Scheduled CI (AMD) - mi250\n+    secrets: inherit\n+\n+  example-ci:\n+    name: Example CI\n+    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main\n+    with:\n+      job: run_examples_gpu\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\n+      runner: mi250\n+      docker: huggingface/transformers-pytorch-amd-gpu\n+      ci_event: Scheduled CI (AMD) - mi250\n+    secrets: inherit\n+\n+  deepspeed-ci:\n+    name: DeepSpeed CI\n+    uses: huggingface/hf-workflows/.github/workflows/transformers_amd_ci_scheduled.yaml@main\n+    with:\n+      job: run_torch_cuda_extensions_gpu\n+      slack_report_channel: \"#transformers-ci-daily-amd\"\n+      runner: mi250\n+      docker: huggingface/transformers-pytorch-deepspeed-amd-gpu\n+      ci_event: Scheduled CI (AMD) - mi250\n+    secrets: inherit"
        },
        {
            "sha": "47f92cd6a2b086c5f7d71cdf1484f512b82caf5d",
            "filename": ".github/workflows/self-scheduled-amd.yml",
            "status": "removed",
            "additions": 0,
            "deletions": 349,
            "changes": 349,
            "blob_url": "https://github.com/huggingface/transformers/blob/7d4b3ddde4da3cba17403072cfdb8b8c76ca1c7c/.github%2Fworkflows%2Fself-scheduled-amd.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/7d4b3ddde4da3cba17403072cfdb8b8c76ca1c7c/.github%2Fworkflows%2Fself-scheduled-amd.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fself-scheduled-amd.yml?ref=7d4b3ddde4da3cba17403072cfdb8b8c76ca1c7c",
            "patch": "@@ -1,349 +0,0 @@\n-name: Self-hosted runner (scheduled-amd)\n-\n-# Note: For the AMD CI, we rely on a caller workflow and on the workflow_call event to trigger the\n-# CI in order to run it on both MI210 and MI250, without having to use matrix here which pushes\n-# us towards the limit of allowed jobs on GitHub Actions.\n-\n-on:\n-  workflow_call:\n-    inputs:\n-      job:\n-        required: true\n-        type: string\n-      slack_report_channel:\n-        required: true\n-        type: string\n-      runner:\n-        required: true\n-        type: string\n-      docker:\n-        required: true\n-        type: string\n-      ci_event:\n-        required: true\n-        type: string\n-\n-env:\n-  HF_HOME: /mnt/cache\n-  TRANSFORMERS_IS_CI: yes\n-  OMP_NUM_THREADS: 8\n-  MKL_NUM_THREADS: 8\n-  RUN_SLOW: yes\n-  HF_HUB_READ_TOKEN: ${{ secrets.HF_HUB_READ_TOKEN }}\n-  SIGOPT_API_TOKEN: ${{ secrets.SIGOPT_API_TOKEN }}\n-  NUM_SLICES: 2\n-\n-# Important note: each job (run_tests_single_gpu, run_tests_multi_gpu, run_examples_gpu, run_pipelines_torch_gpu) requires all the previous jobs before running.\n-# This is done so that we avoid parallelizing the scheduled tests, to leave available\n-# runners for the push CI that is running on the same machine.\n-jobs:\n-  check_runner_status:\n-    name: Check Runner Status\n-    runs-on: ubuntu-22.04\n-    steps:\n-      - name: Checkout transformers\n-        uses: actions/checkout@v4\n-        with:\n-          fetch-depth: 2\n-\n-      - name: Check Runner Status\n-        run: python utils/check_self_hosted_runner.py --target_runners hf-amd-mi210-ci-1gpu-1,hf-amd-mi250-ci-1gpu-1,hf-amd-mi300-ci-1gpu-1 --token ${{ secrets.ACCESS_REPO_INFO_TOKEN }}\n-\n-  check_runners:\n-    name: Check Runners\n-    needs: check_runner_status\n-    strategy:\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-  setup:\n-    if: contains(fromJSON('[\"run_models_gpu\"]'), inputs.job)\n-    name: Setup\n-    needs: check_runners\n-    strategy:\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n-    container:\n-      image: huggingface/transformers-pytorch-amd-gpu\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    outputs:\n-      folder_slices: ${{ steps.set-matrix.outputs.folder_slices }}\n-      slice_ids: ${{ steps.set-matrix.outputs.slice_ids }}\n-    steps:\n-      - name: Update clone\n-        working-directory: /transformers\n-        run: |\n-          git fetch && git checkout ${{ github.sha }}\n-\n-      - name: Cleanup\n-        working-directory: /transformers\n-        run: |\n-          rm -rf tests/__pycache__\n-          rm -rf tests/models/__pycache__\n-          rm -rf reports\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - id: set-matrix\n-        name: Identify models to test\n-        working-directory: /transformers/tests\n-        run: |\n-          echo \"folder_slices=$(python3 ../utils/split_model_tests.py --num_splits ${{ env.NUM_SLICES }})\" >> $GITHUB_OUTPUT\n-          echo \"slice_ids=$(python3 -c 'd = list(range(${{ env.NUM_SLICES }})); print(d)')\" >> $GITHUB_OUTPUT\n-\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-  run_models_gpu:\n-    if: ${{ inputs.job == 'run_models_gpu' }}\n-    name: Single GPU tests\n-    needs: setup\n-    strategy:\n-      max-parallel: 1  # For now, not to parallelize. Can change later if it works well.\n-      fail-fast: false\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-        slice_id: ${{ fromJSON(needs.setup.outputs.slice_ids) }}\n-    uses: ./.github/workflows/model_jobs_amd.yml\n-    with:\n-      folder_slices: ${{ needs.setup.outputs.folder_slices }}\n-      machine_type: ${{ matrix.machine_type }}\n-      slice_id: ${{ matrix.slice_id }}\n-      runner: ${{ inputs.runner }}\n-      docker: ${{ inputs.docker }}\n-    secrets: inherit\n-\n-  run_pipelines_torch_gpu:\n-    if: ${{ inputs.job == 'run_pipelines_torch_gpu' }}\n-    name: PyTorch pipelines\n-    needs: check_runners\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n-    container:\n-      image: ${{ inputs.docker }}\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: Update clone\n-        working-directory: /transformers\n-        run: git fetch && git checkout ${{ github.sha }}\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all pipeline tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          python3 -m pytest -n 1 -v --dist=loadfile --make-reports=${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports tests/pipelines -m \"not not_device_test\"\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_pipelines_torch_gpu_test_reports\n-\n-  run_examples_gpu:\n-    if: ${{ inputs.job == 'run_examples_gpu' }}\n-    name: Examples directory\n-    needs: check_runners\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        machine_type: [single-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n-    container:\n-      image: ${{ inputs.docker }}\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: Update clone\n-        working-directory: /transformers\n-        run: git fetch && git checkout ${{ github.sha }}\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run examples tests on GPU\n-        working-directory: /transformers\n-        run: |\n-          pip install -r examples/pytorch/_tests_requirements.txt\n-          python3 -m pytest -v --make-reports=${{ matrix.machine_type }}_run_examples_gpu_test_reports examples/pytorch -m \"not not_device_test\"\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_examples_gpu_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_examples_gpu_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ matrix.machine_type }}_run_examples_gpu_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_examples_gpu_test_reports\n-\n-  run_torch_cuda_extensions_gpu:\n-    if: ${{ inputs.job == 'run_torch_cuda_extensions_gpu' }}\n-    name: Torch ROCm deepspeed tests\n-    needs: check_runners\n-    strategy:\n-      fail-fast: false\n-      matrix:\n-        machine_type: [single-gpu, multi-gpu]\n-    runs-on: ['${{ matrix.machine_type }}', self-hosted, amd-gpu, '${{ inputs.runner }}']\n-    container:\n-      image: ${{ inputs.docker }}\n-      options: --device /dev/kfd --device /dev/dri --env ROCR_VISIBLE_DEVICES --shm-size \"16gb\" --ipc host -v /mnt/cache/.cache/huggingface:/mnt/cache/\n-    steps:\n-      - name: Update clone\n-        working-directory: /transformers\n-        run: git fetch && git checkout ${{ github.sha }}\n-\n-      - name: Reinstall transformers in edit mode (remove the one installed during docker image build)\n-        working-directory: /transformers\n-        run: python3 -m pip uninstall -y transformers && python3 -m pip install -e .\n-\n-      - name: ROCM-SMI\n-        run: |\n-          rocm-smi\n-\n-      - name: ROCM-INFO\n-        run: |\n-          rocminfo  | grep \"Agent\" -A 14\n-\n-      - name: Show ROCR environment\n-        run: |\n-          echo \"ROCR: $ROCR_VISIBLE_DEVICES\"\n-\n-      - name: Environment\n-        working-directory: /transformers\n-        run: |\n-          python3 utils/print_env.py\n-\n-      - name: Show installed libraries and their versions\n-        working-directory: /transformers\n-        run: pip freeze\n-\n-      - name: Run all tests on GPU\n-        working-directory: /transformers\n-        run: python3 -m pytest -v --make-reports=${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports tests/deepspeed tests/extended -m \"not not_device_test\"\n-\n-      - name: Failure short reports\n-        if: ${{ failure() }}\n-        continue-on-error: true\n-        run: cat /transformers/reports/${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports/failures_short.txt\n-\n-      - name: \"Test suite reports artifacts: ${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\"\n-        if: ${{ always() }}\n-        uses: actions/upload-artifact@v4\n-        with:\n-          name: ${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-          path: /transformers/reports/${{ matrix.machine_type }}_run_torch_cuda_extensions_gpu_test_reports\n-\n-  send_results:\n-    name: Slack Report\n-    needs: [\n-      check_runner_status,\n-      check_runners,\n-      setup,\n-      run_models_gpu,\n-      run_pipelines_torch_gpu,\n-      run_examples_gpu,\n-      run_torch_cuda_extensions_gpu\n-    ]\n-    if: ${{ always() }}\n-    uses: ./.github/workflows/slack-report.yml\n-    with:\n-      job: ${{ inputs.job }}\n-      # This would be `skipped` if `setup` is skipped.\n-      setup_status: ${{ needs.setup.result }}\n-      slack_report_channel: ${{ inputs.slack_report_channel }}\n-      # This would be an empty string if `setup` is skipped.\n-      folder_slices: ${{ needs.setup.outputs.folder_slices }}\n-      quantization_matrix: ${{ needs.setup.outputs.quantization_matrix }}\n-      ci_event: ${{ inputs.ci_event }}\n-\n-    secrets: inherit"
        },
        {
            "sha": "cbea37ff567a96272cfc5f8d8c4cfd6a8947b87c",
            "filename": ".github/workflows/slack-report.yml",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/5fa35344755d8d9c29610b57d175efd03776ae9e/.github%2Fworkflows%2Fslack-report.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/5fa35344755d8d9c29610b57d175efd03776ae9e/.github%2Fworkflows%2Fslack-report.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/.github%2Fworkflows%2Fslack-report.yml?ref=5fa35344755d8d9c29610b57d175efd03776ae9e",
            "patch": "@@ -70,7 +70,7 @@ jobs:\n         with:\n           name: ci_results_${{ inputs.job }}\n           path: ci_results_${{ inputs.job }}\n-      \n+\n       - uses: actions/checkout@v4\n       - uses: actions/download-artifact@v4\n       - name: Send message to Slack for quantization workflow\n@@ -90,12 +90,12 @@ jobs:\n           pip install huggingface_hub\n           pip install slack_sdk\n           pip show slack_sdk\n-          python utils/notification_service_quantization.py \"${{ inputs.quantization_matrix }}\" \n+          python utils/notification_service_quantization.py \"${{ inputs.quantization_matrix }}\"\n \n       # Upload complete failure tables, as they might be big and only truncated versions could be sent to Slack.\n       - name: Failure table artifacts\n         if: ${{ inputs.job == 'run_quantization_torch_gpu' }}\n         uses: actions/upload-artifact@v4\n         with:\n           name: ci_results_${{ inputs.job }}\n-          path: ci_results_${{ inputs.job }}\n\\ No newline at end of file\n+          path: ci_results_${{ inputs.job }}"
        }
    ],
    "stats": {
        "total": 575,
        "additions": 113,
        "deletions": 462
    }
}