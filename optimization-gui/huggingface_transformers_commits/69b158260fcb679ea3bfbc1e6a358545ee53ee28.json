{
    "author": "molbap",
    "message": "Refactor embedding input/output getter/setter (#39339)\n\n* simplify common get/set\n\n* remove some noise\n\n* change some 5 years old modeling utils\n\n* update examples\n\n* fix copies\n\n* revert some changes\n\n* fixes, gah\n\n* format\n\n* move to Mixin\n\n* remove smolvlm specific require grad\n\n* skip\n\n* force defaults\n\n* remodularise some stuff\n\n* remodularise more stuff\n\n* add safety for audio models\n\n* style\n\n* have a correct fallback, you daft donkey\n\n* remove this argh\n\n* change heuristic for audio models\n\n* fixup\n\n* revert\n\n* this works\n\n* revert again\n\n* ðŸ§ \n\n* aaah ESM has two modelings aaah\n\n* add informative but short comment\n\n* add `input_embed_layer` mixin attribute\n\n* style\n\n* walrus has low precedence\n\n* modular fix\n\n* this was breaking parser",
    "sha": "69b158260fcb679ea3bfbc1e6a358545ee53ee28",
    "files": [
        {
            "sha": "19b059699e283133404cc349aedfc83c1f5cb99a",
            "filename": "examples/modular-transformers/modeling_my_new_model2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_my_new_model2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -333,12 +333,6 @@ def __init__(self, config: MyNewModel2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -433,12 +427,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "9111883cfef685a38ba5af8bfcf9e1f847b823d6",
            "filename": "examples/modular-transformers/modeling_new_task_model.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_new_task_model.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -389,12 +389,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "fc90cce75a5f05f637359d86a7f70e96f7e26216",
            "filename": "examples/modular-transformers/modeling_super.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/examples%2Fmodular-transformers%2Fmodeling_super.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/examples%2Fmodular-transformers%2Fmodeling_super.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -332,12 +332,6 @@ def __init__(self, config: SuperConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "f6d0a85857d816b9ab254bd7f26675dd185f6f70",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 188,
            "deletions": 36,
            "changes": 224,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1902,7 +1902,97 @@ def floating_point_ops(\n         return 6 * self.estimate_tokens(input_dict) * self.num_parameters(exclude_embeddings=exclude_embeddings)\n \n \n-class PreTrainedModel(nn.Module, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMixin):\n+class EmbeddingAccessMixin:\n+    \"\"\"\n+    Base utilities to regroup getters and setters for embeddings.\n+    Introduces the `input_layer_embed` attribute, which indicates\n+    where the input embeddings come from and where they\n+    should be set.\n+    \"\"\"\n+\n+    _input_embed_layer = \"embed_tokens\"  # default layer that holds input embeddings.\n+\n+    def get_input_embeddings(self) -> nn.Module:\n+        \"\"\"\n+        Returns the model's input embeddings.\n+\n+        Returns:\n+            `nn.Module`: A torch module mapping vocabulary to hidden states.\n+        \"\"\"\n+\n+        # 1) Check if the model has an attribute named 'embed_tokens' (the standard input embedding layer\n+        #  for most NLP models), and if so, return it.\n+\n+        name = getattr(self, \"_input_embed_layer\", \"embed_tokens\")\n+\n+        if (default_embedding := getattr(self, name, None)) is not None:\n+            return default_embedding\n+        # 2) encoder/decoder and VLMs like `Gemma3nForConditionalGeneration`\n+\n+        if hasattr(self, \"model\") and hasattr(self.model, \"embed_tokens\"):\n+            return self.model.embed_tokens\n+\n+        # 3) vanilla decoderâ€‘only architectures\n+        elif hasattr(self, \"embed_tokens\"):\n+            return self.embed_tokens\n+        else:\n+            base_model = getattr(self, \"base_model_prefix\", None)\n+            if base_model is not None:\n+                base_model = getattr(self, base_model, None)\n+                if base_model is not None and base_model is not self:\n+                    return base_model.get_input_embeddings()\n+            raise NotImplementedError(\n+                f\"`get_input_embeddings` not autoâ€‘handled for {self.__class__.__name__}; \"\n+                \"please override in the subclass.\"\n+            )\n+\n+    def set_input_embeddings(self, value: nn.Module):\n+        \"\"\"Fallback setter that handles **~70â€¯%** of models in the codeâ€‘base.\n+\n+        Order of attempts:\n+        1. `self.model.embed_tokens`\n+        2. `self.embed_tokens`\n+        3. delegate to the *base model* if one exists\n+        4. otherwise raise `NotImplementedError` so subclasses still can (and\n+            should) override for exotic layouts.\n+        \"\"\"\n+\n+        # 1) encoder/decoder and VLMs like `Gemma3nForConditionalGeneration`\n+        name = getattr(self, \"_input_embed_layer\", \"embed_tokens\")\n+        if hasattr(self, \"model\") and hasattr(self.model, name):\n+            setattr(self.model, name, value)\n+        # 2) as well as vanilla decoderâ€‘only architectures\n+        elif hasattr(self, name):\n+            setattr(self, name, value)\n+        # 3) recurse once into the registered *base* model (e.g. for encoder/decoder)\n+        elif getattr(self, self.base_model_prefix, self) is not self:\n+            base_model = getattr(self, self.base_model_prefix, self)\n+            base_model.set_input_embeddings(value)\n+        else:\n+            raise NotImplementedError(\n+                f\"`set_input_embeddings` not autoâ€‘handled for {self.__class__.__name__}; please override in the subclass.\"\n+            )\n+\n+    def get_output_embeddings(self):\n+        if not hasattr(self, \"lm_head\"):\n+            return None\n+        try:\n+            # Speech / vision backbones raise here, so we return None.\n+            # Legit use of get_input_embs?\n+            self.get_input_embeddings()\n+        except NotImplementedError:\n+            return None\n+        return self.lm_head\n+\n+    def set_output_embeddings(self, new_embeddings):\n+        \"\"\"\n+        Sets the model's output embedding, defaulting to setting new_embeddings to lm_head.\n+        \"\"\"\n+        if getattr(self, \"lm_head\"):\n+            self.lm_head = new_embeddings\n+\n+\n+class PreTrainedModel(nn.Module, EmbeddingAccessMixin, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMixin):\n     r\"\"\"\n     Base class for all models.\n \n@@ -2004,6 +2094,8 @@ class PreTrainedModel(nn.Module, ModuleUtilsMixin, PushToHubMixin, PeftAdapterMi\n     _supports_attention_backend = False\n     _can_record_outputs = None\n \n+    # This attribute sets the default parameter to be\n+\n     @property\n     @torch._dynamo.allow_in_graph\n     def can_record_outputs(self) -> dict[str, OutputRecorder]:\n@@ -2267,6 +2359,101 @@ def _from_config(cls, config, **kwargs):\n \n         return model\n \n+    @classmethod\n+    def _check_attn_implementation(cls, attn_implementation: Union[str, dict]) -> Union[str, dict]:\n+        \"\"\"\n+        Checks that the requested attention implementation exists and tries to get the kernel from hub\n+        if `attn_implementation` matches hf kernels pattern.\n+        \"\"\"\n+        if isinstance(attn_implementation, str) and re.match(r\"^[^/:]+/[^/:]+:[^/:]+$\", attn_implementation):\n+            if not is_kernels_available():\n+                raise ValueError(\"kernels is not installed. Please install it with `pip install kernels`.\")\n+\n+            # Extract repo_id and kernel_name from the string\n+            repo_id, kernel_name = attn_implementation.split(\":\")\n+            kernel_name = kernel_name.strip()\n+            repo_id = repo_id.strip()\n+\n+            try:\n+                kernel = get_kernel(repo_id)\n+                ALL_ATTENTION_FUNCTIONS.register(f\"kernel_{repo_id.replace('/', '_')}\", getattr(kernel, kernel_name))\n+                attn_implementation = f\"kernel_{repo_id.replace('/', '_')}\"\n+            except FileNotFoundError as e:\n+                logger.warning(\n+                    f\"Could not find a kernel repository '{repo_id}' compatible with your devicein the hub: {e}. Using eager attention implementation instead.\"\n+                )\n+                attn_implementation = None  # try to dispatch SDPA and fallback eager if not available\n+            except AttributeError:\n+                raise ValueError(\n+                    \"the kernel function name or class specified in the attn_implementation argument is not valid. \\\n+                                 Please check the documentation for the correct format, \\\n+                                 and check that the kernel exports the class and the function correctly.\"\n+                )\n+        if (\n+            not isinstance(attn_implementation, dict)\n+            and attn_implementation not in [\"eager\", None] + ALL_ATTENTION_FUNCTIONS.valid_keys()\n+        ):\n+            message = f'Specified `attn_implementation=\"{attn_implementation}\"` is not supported. The only possible arguments are `attn_implementation=\"eager\"` (manual attention implementation)'\n+            # check `supports_flash_attn_2` for BC with custom code. TODO: remove after a few releases\n+            if cls._supports_flash_attn or getattr(cls, \"_supports_flash_attn_2\", False):\n+                message += (\n+                    ', `\"attn_implementation=flash_attention_3\"` (implementation using flash attention 3)'\n+                    ', `\"attn_implementation=flash_attention_2\"` (implementation using flash attention 2)'\n+                )\n+            if cls._supports_sdpa:\n+                message += ', `\"attn_implementation=sdpa\"` (implementation using torch.nn.functional.scaled_dot_product_attention)'\n+            if cls._supports_flex_attn:\n+                message += ', `\"attn_implementation=flex_attention\"` (implementation using torch\\'s flex_attention)'\n+            raise ValueError(message + \".\")\n+\n+        return attn_implementation\n+\n+    def set_attention_implementation(self, attn_implementation: Union[str, dict]):\n+        \"\"\"\n+        Checks and dispatches to the requested attention implementation.\n+        \"\"\"\n+        requested_attn_implementation = self._check_attn_implementation(attn_implementation)\n+\n+        # Composite models consisting of several PretrainedModels can specify attention implementation as a dict where\n+        # keys are sub-config names. But most people will specify one `str` which means that should dispatch it for all sub-models.\n+        # See https://github.com/huggingface/transformers/pull/32238\n+        for key in self.config.sub_configs.keys():\n+            sub_config = getattr(self.config, key)\n+            curr_attn_implementation = (\n+                requested_attn_implementation\n+                if not isinstance(requested_attn_implementation, dict)\n+                else requested_attn_implementation.get(key, None)\n+            )\n+            # For models with backbone sub-config might be not initialized. Set the requested att\n+            # if the config hasn't got any attn pre-set and the requested attn in not `None` (i.e not the default attn)\n+            if (\n+                sub_config is not None\n+                and sub_config._attn_implementation_internal is None\n+                and curr_attn_implementation is not None\n+            ):\n+                sub_config._attn_implementation_internal = curr_attn_implementation\n+\n+        if requested_attn_implementation == \"flash_attention_3\" and self._flash_attn_3_can_dispatch():\n+            self.config._attn_implementation = \"flash_attention_3\"\n+        if requested_attn_implementation == \"flash_attention_2\" and self._flash_attn_2_can_dispatch():\n+            self.config._attn_implementation = \"flash_attention_2\"\n+        elif requested_attn_implementation == \"flex_attention\" and self._flex_attn_can_dispatch():\n+            self.config._attn_implementation = \"flex_attention\"\n+        elif (\n+            requested_attn_implementation in [None, \"sdpa\"]\n+            and not is_torch_xla_available()\n+            and self._sdpa_can_dispatch(hard_check_only=requested_attn_implementation is not None)\n+        ):\n+            self.config._attn_implementation = \"sdpa\"\n+        elif requested_attn_implementation in ALL_ATTENTION_FUNCTIONS.valid_keys():\n+            self.config._attn_implementation = requested_attn_implementation\n+        elif isinstance(requested_attn_implementation, dict):\n+            self.config._attn_implementation = requested_attn_implementation.get(\"\", None)\n+        else:\n+            self.config._attn_implementation = \"eager\"\n+\n+        self.config._attn_implementation_autoset = True\n+\n     @classmethod\n     def _set_default_torch_dtype(cls, dtype: torch.dtype) -> torch.dtype:\n         \"\"\"\n@@ -2769,41 +2956,6 @@ def disable_input_require_grads(self):\n         \"\"\"\n         self._require_grads_hook.remove()\n \n-    def get_input_embeddings(self) -> nn.Module:\n-        \"\"\"\n-        Returns the model's input embeddings.\n-\n-        Returns:\n-            `nn.Module`: A torch module mapping vocabulary to hidden states.\n-        \"\"\"\n-        base_model = getattr(self, self.base_model_prefix, self)\n-        if base_model is not self:\n-            return base_model.get_input_embeddings()\n-        else:\n-            raise NotImplementedError\n-\n-    def set_input_embeddings(self, value: nn.Module):\n-        \"\"\"\n-        Set model's input embeddings.\n-\n-        Args:\n-            value (`nn.Module`): A module mapping vocabulary to hidden states.\n-        \"\"\"\n-        base_model = getattr(self, self.base_model_prefix, self)\n-        if base_model is not self:\n-            base_model.set_input_embeddings(value)\n-        else:\n-            raise NotImplementedError\n-\n-    def get_output_embeddings(self) -> nn.Module:\n-        \"\"\"\n-        Returns the model's output embeddings.\n-\n-        Returns:\n-            `nn.Module`: A torch module mapping hidden states to vocabulary.\n-        \"\"\"\n-        return None  # Overwrite for models with output embeddings\n-\n     def _init_weights(self, module):\n         \"\"\"\n         Initialize the weights. This method should be overridden by derived class and is"
        },
        {
            "sha": "43a02ebd8cb8e25822146c91bffd7ad709691099",
            "filename": "src/transformers/models/arcee/modeling_arcee.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Farcee%2Fmodeling_arcee.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -356,12 +356,6 @@ def __init__(self, config: ArceeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -438,18 +432,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -533,12 +515,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -685,12 +661,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "6c5c972b1f9a609d9c1109022fee06baa4921376",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -740,12 +740,6 @@ def __init__(self, config: AriaTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -822,18 +816,6 @@ def __init__(self, config: AriaTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -1146,9 +1128,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "cb48470687781fac8faf4f8bbb054522a11827be",
            "filename": "src/transformers/models/aya_vision/modeling_aya_vision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faya_vision%2Fmodeling_aya_vision.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -368,9 +368,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "3548e706a5e05b2505c5ce937a47457801cc53d5",
            "filename": "src/transformers/models/bamba/modeling_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodeling_bamba.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1126,12 +1126,6 @@ def __init__(self, config: BambaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1376,18 +1370,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "9bfbfd159f3b42b669eb3178188e049bc4a8da92",
            "filename": "src/transformers/models/bamba/modular_bamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbamba%2Fmodular_bamba.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -854,12 +854,6 @@ def __init__(self, config: BambaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "2e199d4fdd6aac2078b9eac86c5149ef6a46345c",
            "filename": "src/transformers/models/bark/modeling_bark.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbark%2Fmodeling_bark.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -397,6 +397,11 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_output_embeddings(self):\n+        # NOTE: get_output_embeddings() must return None to prevent accidental weight tying.\n+        # See e.g. https://github.com/huggingface/transformers/pull/39339#discussion_r2219126400\n+        return None\n+\n     def get_input_embeddings(self):\n         return self.input_embeds_layer\n "
        },
        {
            "sha": "055b696405dc671612a5f68e85f1ba0753a1ee28",
            "filename": "src/transformers/models/bart/modeling_bart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbart%2Fmodeling_bart.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -764,12 +764,6 @@ def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = No\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -930,12 +924,6 @@ def __init__(self, config: BartConfig, embed_tokens: Optional[nn.Embedding] = No\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1369,12 +1357,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n             self.model._tie_weights()\n@@ -1857,12 +1839,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.decoder = decoder\n "
        },
        {
            "sha": "01d7a0e826687679c33c76012ae944adc1b57713",
            "filename": "src/transformers/models/beit/modeling_beit.py",
            "status": "modified",
            "additions": 6,
            "deletions": 0,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbeit%2Fmodeling_beit.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -888,6 +888,12 @@ def __init__(self, config: BeitConfig) -> None:\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_output_embeddings(self):\n+        # NOTE: get_output_embeddings() must return None to prevent accidental weight tying.\n+        # Vision models like BEiT use a Conv2d patch embed layer (no `.weight`) so calling tie_weights() would fail.\n+        # See e.g. https://github.com/huggingface/transformers/pull/39339#discussion_r2219126400\n+        return None\n+\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "d567808f95af3144c06d375a6f6ea985eddab6d1",
            "filename": "src/transformers/models/bigbird_pegasus/modeling_bigbird_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbigbird_pegasus%2Fmodeling_bigbird_pegasus.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -2084,12 +2084,6 @@ def __init__(self, config: BigBirdPegasusConfig, embed_tokens: Optional[nn.Embed\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.Tensor] = None,\n@@ -2506,12 +2500,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n             self.model._tie_weights()\n@@ -2938,12 +2926,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.decoder = decoder\n "
        },
        {
            "sha": "a873cd6b696746bfe06e649496f51074f524a7d7",
            "filename": "src/transformers/models/biogpt/modeling_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodeling_biogpt.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -522,12 +522,6 @@ def __init__(self, config: BioGptConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "3f63caddeaee9d6e07f985f16e45cf8f9eb86c04",
            "filename": "src/transformers/models/biogpt/modular_biogpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbiogpt%2Fmodular_biogpt.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -347,12 +347,6 @@ def __init__(self, config: BioGptConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "c373d659c7436692bfb92ad30ef83b7028bc59da",
            "filename": "src/transformers/models/bitnet/modeling_bitnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbitnet%2Fmodeling_bitnet.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -351,12 +351,6 @@ def __init__(self, config: BitNetConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -433,18 +427,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "65f0378ef53193f27e43ceec6bcb1956b58d49c1",
            "filename": "src/transformers/models/blenderbot/modeling_blenderbot.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot%2Fmodeling_blenderbot.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -879,12 +879,6 @@ def __init__(self, config: BlenderbotConfig, embed_tokens: Optional[nn.Embedding\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids=None,\n@@ -1329,12 +1323,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1503,12 +1491,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.decoder = decoder\n "
        },
        {
            "sha": "9030bd1e5ce6c8e6e923279dbaccb30516bb3981",
            "filename": "src/transformers/models/blenderbot_small/modeling_blenderbot_small.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblenderbot_small%2Fmodeling_blenderbot_small.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -866,12 +866,6 @@ def __init__(self, config: BlenderbotSmallConfig, embed_tokens: Optional[nn.Embe\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids=None,\n@@ -1288,12 +1282,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1462,12 +1450,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.decoder = decoder\n "
        },
        {
            "sha": "f999872bef55093053f44abeffd8579a0fa705a3",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -759,9 +759,6 @@ def __init__(self, config: BloomConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     def set_output_embeddings(self, new_embeddings: torch.Tensor):\n         self.lm_head = new_embeddings\n "
        },
        {
            "sha": "5d8e6fc210736ba147db4c641308cd24cfd0b0fe",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -886,12 +886,6 @@ def __init__(self, config: ChameleonConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def get_image_tokens(self, pixel_values: torch.FloatTensor):\n         \"\"\"\n         Tokenizes images into discrete tokens with VQGAN module. Converts\n@@ -1181,18 +1175,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "b0170e6402255d0880f27cf7d21c301551d4ee04",
            "filename": "src/transformers/models/clvp/modeling_clvp.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclvp%2Fmodeling_clvp.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1259,6 +1259,11 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n+    def get_output_embeddings(self):\n+        # NOTE: get_output_embeddings() must return None to prevent accidental weight tying.\n+        # See e.g. https://github.com/huggingface/transformers/pull/39339#discussion_r2219126400\n+        return None\n+\n     def get_input_embeddings(self):\n         return self.model.decoder.input_embeds_layer\n "
        },
        {
            "sha": "b1378f55175bd9e212391c6697687258e9396b49",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -592,12 +592,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "1fb91bccaa61fa01f94049c03014bc7637d19577",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -384,12 +384,6 @@ def __init__(self, config: CohereConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -468,18 +462,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "f3dc518f9246b0a646c4fb1f82bb3356d2256f84",
            "filename": "src/transformers/models/cohere2/modeling_cohere2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere2%2Fmodeling_cohere2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -361,12 +361,6 @@ def __init__(self, config: Cohere2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -449,18 +443,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "91dba7da6c8e0b08b102d9aea1da81041c58d834",
            "filename": "src/transformers/models/cpmant/modeling_cpmant.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcpmant%2Fmodeling_cpmant.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -800,11 +800,12 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, embeddings):\n         self.cpmant.input_embedding = embeddings\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n+    def _reorder_cache(self, past_key_values, beam_idx):\n+        past_key_values = [list(each) if each is not None else each for each in past_key_values]\n+        for key_value_layer in past_key_values:\n+            key_value_layer[0] = key_value_layer[0][beam_idx]\n+            key_value_layer[1] = key_value_layer[1][beam_idx]\n+        return past_key_values\n \n \n __all__ = [\"CpmAntForCausalLM\", \"CpmAntModel\", \"CpmAntPreTrainedModel\"]"
        },
        {
            "sha": "31e9ff368978f2242b45a59b89ea8663c413e67b",
            "filename": "src/transformers/models/csm/modeling_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodeling_csm.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -416,12 +416,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -553,12 +547,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -687,12 +675,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -793,12 +775,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.backbone_model.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def _tie_weights(self):\n         if self.config.tie_codebooks_embeddings:\n             self._tie_or_clone_weights("
        },
        {
            "sha": "470161275244ec987cee025be9476ba8338dd8dd",
            "filename": "src/transformers/models/csm/modular_csm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcsm%2Fmodular_csm.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -296,12 +296,6 @@ def __init__(self, config):\n         self.codebooks_head = CsmCodebooksHead(config.hidden_size, config.num_codebooks, config.vocab_size)\n         self.model = CsmDepthDecoderModel(config)\n \n-    def get_output_embeddings(self):\n-        raise AttributeError(\"Not needed for Csm\")\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        raise AttributeError(\"Not needed for Csm\")\n-\n     def prepare_inputs_for_generation(\n         self,\n         input_ids: torch.LongTensor,\n@@ -458,12 +452,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.backbone_model.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def _tie_weights(self):\n         if self.config.tie_codebooks_embeddings:\n             self._tie_or_clone_weights("
        },
        {
            "sha": "f942cb1531c130e874ddc8ad07e90803fdf02577",
            "filename": "src/transformers/models/ctrl/modeling_ctrl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fctrl%2Fmodeling_ctrl.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -455,12 +455,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "595953fd6c18f51ea5ead9003323e02510e77f1d",
            "filename": "src/transformers/models/deepseek_v2/modeling_deepseek_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodeling_deepseek_v2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -500,12 +500,6 @@ def __init__(self, config: DeepseekV2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -582,18 +576,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -690,12 +672,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "05171a8359d7e52134a3700f91da9b3f861c7533",
            "filename": "src/transformers/models/deepseek_v3/modeling_deepseek_v3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodeling_deepseek_v3.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -541,12 +541,6 @@ def __init__(self, config: DeepseekV3Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -623,18 +617,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "ae15ffd41569f7a7633999233c9f8bc5f9685313",
            "filename": "src/transformers/models/deprecated/gptsan_japanese/modeling_gptsan_japanese.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fgptsan_japanese%2Fmodeling_gptsan_japanese.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -871,9 +871,6 @@ def __init__(self, config: GPTSanJapaneseConfig):\n \n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n@@ -1315,12 +1312,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, new_embeddings):\n         self.model.set_input_embeddings(new_embeddings)\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     def _unpack_router_logits(self, router_outputs):\n         total_router_logits = []\n         total_expert_indexes = []"
        },
        {
            "sha": "9eb1c5c1d6e7c5bbf5c924d8b86ca0f764a7f637",
            "filename": "src/transformers/models/deprecated/mega/modeling_mega.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fmega%2Fmodeling_mega.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1662,12 +1662,6 @@ def __init__(self, config: MegaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @add_start_docstrings_to_model_forward(MEGA_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n     @replace_return_docstrings(output_type=CausalLMOutputWithCrossAttentions, config_class=_CONFIG_FOR_DOC)\n     def forward("
        },
        {
            "sha": "abdcc2b40e9826972ec3755fb9c7bb95fed6a48c",
            "filename": "src/transformers/models/deprecated/open_llama/modeling_open_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fopen_llama%2Fmodeling_open_llama.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -544,12 +544,6 @@ def __init__(self, config: OpenLlamaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @add_start_docstrings_to_model_forward(OPEN_LLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,\n@@ -678,18 +672,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -864,12 +846,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @add_start_docstrings_to_model_forward(OPEN_LLAMA_INPUTS_DOCSTRING)\n     def forward(\n         self,"
        },
        {
            "sha": "bf99ba6255f3cb45358cc4617c0fab8debe0a24c",
            "filename": "src/transformers/models/deprecated/speech_to_text_2/modeling_speech_to_text_2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fspeech_to_text_2%2Fmodeling_speech_to_text_2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -453,12 +453,6 @@ def __init__(self, config: Speech2Text2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids=None,\n@@ -697,12 +691,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.decoder = decoder\n "
        },
        {
            "sha": "7b0c3aede21c0925d806d624e1ebe120b29652a8",
            "filename": "src/transformers/models/deprecated/xlm_prophetnet/modeling_xlm_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeprecated%2Fxlm_prophetnet%2Fmodeling_xlm_prophetnet.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1843,12 +1843,6 @@ def __init__(self, config: XLMProphetNetConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n             self._tie_or_clone_weights(self.prophetnet.word_embeddings, self.lm_head)\n@@ -2074,12 +2068,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.prophetnet.decoder.word_embeddings = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n             self._tie_or_clone_weights(self.prophetnet.decoder.word_embeddings, self.lm_head)"
        },
        {
            "sha": "92badf62f26766b3171724d8a87638564fd7689f",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -612,12 +612,6 @@ def __init__(self, config: DiffLlamaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -694,18 +688,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -797,12 +779,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -949,12 +925,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "21b2794c03bdb9dc76c3a6ec14157c5355644f15",
            "filename": "src/transformers/models/doge/modeling_doge.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodeling_doge.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -544,12 +544,6 @@ def __init__(self, config: DogeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -739,18 +733,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -867,12 +849,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "06df98b83568db47ecd8c84038dc839cd28f39e1",
            "filename": "src/transformers/models/dots1/modeling_dots1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdots1%2Fmodeling_dots1.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -460,12 +460,6 @@ def __init__(self, config: Dots1Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -555,18 +549,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "fd7cbf39e1837eb6889217466ea03e2e5fc15656",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1174,12 +1174,6 @@ def __init__(self, config: Emu3Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -1257,18 +1251,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -1499,9 +1481,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "580bf670e3ca994895e863aa7bb410764d67fcf3",
            "filename": "src/transformers/models/emu3/modular_emu3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodular_emu3.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1063,9 +1063,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "71772e4ffadfb57734bbf0fda028e6f56a3a2995",
            "filename": "src/transformers/models/esm/modeling_esm.py",
            "status": "modified",
            "additions": 5,
            "deletions": 0,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fesm%2Fmodeling_esm.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -764,6 +764,11 @@ def _init_weights(self, module):\n         elif isinstance(module, EsmLMHead):\n             module.bias.data.zero_()\n \n+    def get_output_embeddings(self):\n+        # NOTE: get_output_embeddings() must return None to prevent accidental weight tying.\n+        # See e.g. https://github.com/huggingface/transformers/pull/39339#discussion_r2219126400\n+        return None\n+\n \n @auto_docstring\n class EsmModel(EsmPreTrainedModel):"
        },
        {
            "sha": "4033e2c14d58d17e6fd0324c6068c0f2b937dd50",
            "filename": "src/transformers/models/falcon/modeling_falcon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon%2Fmodeling_falcon.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1002,9 +1002,6 @@ def __init__(self, config: FalconConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     def set_output_embeddings(self, new_embeddings: torch.Tensor):\n         self.lm_head = new_embeddings\n "
        },
        {
            "sha": "33955d3a6b0ed8d4f9bffb0d9743d8b0cca444ac",
            "filename": "src/transformers/models/falcon_h1/modeling_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodeling_falcon_h1.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1238,12 +1238,6 @@ def __init__(self, config: FalconH1Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1484,18 +1478,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "47e958eb0c9c4683fa863a63c0ee8da36ac3e51f",
            "filename": "src/transformers/models/falcon_h1/modular_falcon_h1.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_h1%2Fmodular_falcon_h1.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1016,12 +1016,6 @@ def __init__(self, config: FalconH1Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "0a60b2b6bf65c0eb3a93296c12fbf43d178c2b80",
            "filename": "src/transformers/models/falcon_mamba/modeling_falcon_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffalcon_mamba%2Fmodeling_falcon_mamba.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -776,12 +776,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_input_embeddings(self):\n         return self.backbone.get_input_embeddings()\n "
        },
        {
            "sha": "ff163aacd4108520eb0e61023ed43c4a62a92d2a",
            "filename": "src/transformers/models/fuyu/modeling_fuyu.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ffuyu%2Fmodeling_fuyu.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -254,12 +254,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "287c9b3013cc5d445dc7c25335b7cd83f39c0e24",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -353,12 +353,6 @@ def __init__(self, config: GemmaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -444,18 +438,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -547,12 +529,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -639,12 +615,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "6db1b1f7bbfb3f0d013a84c736deb44376b163da",
            "filename": "src/transformers/models/gemma2/modeling_gemma2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma2%2Fmodeling_gemma2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -383,12 +383,6 @@ def __init__(self, config: Gemma2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -516,18 +510,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -638,12 +620,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -730,12 +706,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "c02f2862c505ac91347a9a772a1703c3d925f2b2",
            "filename": "src/transformers/models/gemma3/modeling_gemma3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodeling_gemma3.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -488,12 +488,6 @@ def __init__(self, config: Gemma3TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -621,18 +615,6 @@ def __init__(self, config: Gemma3TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -991,12 +973,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "1411cccef9a9981602e3b2f7065f9150e7e9793b",
            "filename": "src/transformers/models/gemma3n/modeling_gemma3n.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodeling_gemma3n.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1582,12 +1582,6 @@ def __init__(self, config: Gemma3nTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1793,18 +1787,6 @@ def __init__(self, config: Gemma3nTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -2212,12 +2194,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "72733a80f769428e2c397f2ae1216dd1ee4e2e7c",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -370,12 +370,6 @@ def __init__(self, config: GlmConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -452,18 +446,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -560,12 +542,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -652,12 +628,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "e4dd64102d5685e0139ba3d70e5a1ea4e66317be",
            "filename": "src/transformers/models/glm4/modeling_glm4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4%2Fmodeling_glm4.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -374,12 +374,6 @@ def __init__(self, config: Glm4Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -456,18 +450,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -564,12 +546,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -656,12 +632,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "31ad8ede952fe2fd128e9543cb64d8673b70da24",
            "filename": "src/transformers/models/glm4_moe/modeling_glm4_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4_moe%2Fmodeling_glm4_moe.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -480,12 +480,6 @@ def __init__(self, config: Glm4MoeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -562,18 +556,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "41e37b3e1a7af41cc55f7262748d7245fdb92c59",
            "filename": "src/transformers/models/glm4v/modeling_glm4v.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodeling_glm4v.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -837,12 +837,6 @@ def __init__(self, config: Glm4vTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     @can_return_tuple\n     def forward(\n@@ -1406,12 +1400,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "99959cd74bd56edb0f41239d43ff0f32ad9c6856",
            "filename": "src/transformers/models/got_ocr2/modeling_got_ocr2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodeling_got_ocr2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -682,9 +682,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "c853d80e4ae53f398458914fb0d9b6a065f5ce72",
            "filename": "src/transformers/models/gpt2/modeling_gpt2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt2%2Fmodeling_gpt2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1139,12 +1139,6 @@ def deparallelize(self):\n         self.model_parallel = False\n         torch.cuda.empty_cache()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1294,12 +1288,6 @@ def deparallelize(self):\n         self.model_parallel = False\n         torch.cuda.empty_cache()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "8855d6a593d4dddc93d75ae33bb80e31f4066baa",
            "filename": "src/transformers/models/gpt_bigcode/modeling_gpt_bigcode.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_bigcode%2Fmodeling_gpt_bigcode.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -618,12 +618,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "7d655bd0e6a590deaddb22d3bc882526d5e41ae5",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -788,12 +788,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "15cc664d74b822e327af65dbba767fac4556561d",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -403,12 +403,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_in\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_in = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -521,6 +515,12 @@ def forward(\n             attentions=all_attentions,\n         )\n \n+    def get_input_embeddings(self):\n+        return self.embed_in\n+\n+    def set_input_embeddings(self, value):\n+        self.embed_in = value\n+\n \n @auto_docstring(\n     custom_intro=\"\"\""
        },
        {
            "sha": "43822682df35e526d0e03a2f419571bb6f3551bc",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -927,12 +927,6 @@ def deparallelize(self):\n         self.model_parallel = False\n         torch.cuda.empty_cache()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "d9327bb50a5d998bb42d23b5eda4680e3854482f",
            "filename": "src/transformers/models/gptj/modeling_tf_gptj.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_tf_gptj.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -751,12 +751,6 @@ def __init__(self, config, *inputs, **kwargs):\n         )\n         self.config = config\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def prepare_inputs_for_generation(self, inputs, past_key_values=None, use_cache=None, **kwargs):\n         token_type_ids = kwargs.get(\"token_type_ids\", None)\n         # only last token for inputs_ids if past is defined in kwargs"
        },
        {
            "sha": "8bebef03c22535aaef143275dff0b3e01fe64569",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -383,12 +383,6 @@ def __init__(self, config: GraniteConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -504,18 +498,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "bf72cc85da70be16e8e14d2b135b5a21bc5daade",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -636,12 +636,6 @@ def __init__(self, config: GraniteMoeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -894,18 +888,6 @@ def __init__(self, config: GraniteMoeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "8b3f3d1dccd92f24a69720ae12143e5ef64e277c",
            "filename": "src/transformers/models/granitemoehybrid/modeling_granitemoehybrid.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoehybrid%2Fmodeling_granitemoehybrid.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1298,12 +1298,6 @@ def __init__(self, config: GraniteMoeHybridConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1654,18 +1648,6 @@ def __init__(self, config: GraniteMoeHybridConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "83f78ae32774e842c7d49957d4f8e399d080ebec",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -588,12 +588,6 @@ def __init__(self, config: GraniteMoeSharedConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -926,18 +920,6 @@ def __init__(self, config: GraniteMoeSharedConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "f15fbd48ddadb3e374870e4800bb663b31288122",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -355,12 +355,6 @@ def __init__(self, config: HeliumConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -437,18 +431,6 @@ def __init__(self, config: HeliumConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -540,12 +522,6 @@ def __init__(self, config: HeliumConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -632,12 +608,6 @@ def __init__(self, config: HeliumConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "741886c1425b68d6f9c15a7dd97fff397f9d5582",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -993,12 +993,6 @@ def freeze_text_layers(self, module_exceptions=[]):\n     def freeze_vision_layers(self, module_exceptions=[]):\n         freeze_model(self.vision_model, module_exceptions=module_exceptions)\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1344,18 +1338,6 @@ def __init__(self, config, vision_model=None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "f006862284d0d84aa552156601a732cf1c822b65",
            "filename": "src/transformers/models/idefics/modeling_tf_idefics.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_tf_idefics.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1223,12 +1223,6 @@ def freeze_text_layers(self, module_exceptions=[]):\n     def freeze_vision_layers(self, module_exceptions=[]):\n         freeze_model(self.vision_model, module_exceptions=module_exceptions)\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n         # create causal mask\n         # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n@@ -1613,18 +1607,6 @@ def __init__(self, config, vision_model=None, **kwargs):\n             name=\"lm_head\",\n         )\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "a2afefa82723db77b3453ad1e6fe7f3b1f23ba0e",
            "filename": "src/transformers/models/idefics2/modeling_idefics2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics2%2Fmodeling_idefics2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1141,12 +1141,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.text_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n         return self.model.get_image_features(pixel_values=pixel_values, pixel_attention_mask=pixel_attention_mask)\n "
        },
        {
            "sha": "9200bb71596b0ff44605c68c5f55c47e35a71dee",
            "filename": "src/transformers/models/idefics3/modeling_idefics3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 8,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics3%2Fmodeling_idefics3.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -869,14 +869,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.text_model.set_input_embeddings(value)\n \n-    # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    # Copied from transformers.models.idefics2.modeling_idefics2.Idefics2ForConditionalGeneration.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n         return self.model.get_image_features(pixel_values=pixel_values, pixel_attention_mask=pixel_attention_mask)\n "
        },
        {
            "sha": "cf9d4339fab65107daa977a4e7d11f938acae905",
            "filename": "src/transformers/models/imagegpt/modeling_imagegpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fimagegpt%2Fmodeling_imagegpt.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -799,12 +799,6 @@ def __init__(self, config: ImageGPTConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "46ef56dc4600372fbf905474da9d10faffd709e3",
            "filename": "src/transformers/models/internvl/modeling_internvl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finternvl%2Fmodeling_internvl.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -843,9 +843,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "d6a2aaabd83b4f08f4825c7b313ab50a9d597e43",
            "filename": "src/transformers/models/jamba/modeling_jamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjamba%2Fmodeling_jamba.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1125,12 +1125,6 @@ def __init__(self, config: JambaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1298,18 +1292,6 @@ def __init__(self, config: JambaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -1495,12 +1477,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "2a2257dfecf8782e54e647476097ccd7fe8d5c47",
            "filename": "src/transformers/models/janus/modeling_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodeling_janus.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1145,12 +1145,6 @@ def prepare_embeddings_for_image_generation(self, inputs: torch.Tensor) -> torch\n         hidden_state = self.model.generation_aligner(hidden_state)\n         return hidden_state\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "81e63901831eb6a14aa0c5c016fc47d6495686d3",
            "filename": "src/transformers/models/janus/modular_janus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjanus%2Fmodular_janus.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1004,12 +1004,6 @@ def prepare_embeddings_for_image_generation(self, inputs: torch.Tensor) -> torch\n         hidden_state = self.model.generation_aligner(hidden_state)\n         return hidden_state\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "996eece4894675bcf15b801946a3015cf362982f",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -875,14 +875,6 @@ def __init__(self, config: JetMoeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel.get_input_embeddings\n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel.set_input_embeddings\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1130,22 +1122,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_input_embeddings\n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_input_embeddings\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n     def set_decoder(self, decoder):\n         self.model = decoder\n@@ -1258,12 +1234,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "67d53f2a7ab2fe696fdbe5bc6e3f916a9ecb22ed",
            "filename": "src/transformers/models/kosmos2/modeling_kosmos2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkosmos2%2Fmodeling_kosmos2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1254,9 +1254,6 @@ def __init__(self, config: Kosmos2TextConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.model.embed_tokens\n \n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1337,15 +1334,9 @@ def __init__(self, config: Kosmos2TextConfig):\n     def get_input_embeddings(self) -> nn.Module:\n         return self.model.embed_tokens\n \n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "d2e9d92e787733729a705a56c4daaa51a21a19f7",
            "filename": "src/transformers/models/kyutai_speech_to_text/modeling_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodeling_kyutai_speech_to_text.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -815,12 +815,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1094,18 +1088,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "f0af302bf85c02549cc6f7a7d61243ba6b5f3377",
            "filename": "src/transformers/models/led/modeling_led.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fled%2Fmodeling_led.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -2054,12 +2054,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "0d383769d1c49a9b18140acc96296f18684091de",
            "filename": "src/transformers/models/lfm2/modeling_lfm2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flfm2%2Fmodeling_lfm2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -582,12 +582,6 @@ def __init__(self, config: Lfm2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -669,18 +663,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "4bab75a87ce48fd14013c2f8b029e5a956d735de",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -354,12 +354,6 @@ def __init__(self, config: LlamaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -436,18 +430,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -544,12 +526,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -697,12 +673,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "85aeb70ce38b01e1dcc02a8891b09ea96e3fb60c",
            "filename": "src/transformers/models/llama4/modeling_llama4.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama4%2Fmodeling_llama4.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -489,12 +489,6 @@ def __init__(self, config: Llama4TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -619,18 +613,6 @@ def __init__(self, config: Llama4TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "032751a4e11622791f051efbe021e4be4b51d397",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -351,9 +351,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "7cbad1b98093cf3b4bd841d57dcfdc73851e509f",
            "filename": "src/transformers/models/llava_next/modeling_llava_next.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next%2Fmodeling_llava_next.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -552,9 +552,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "7721d760eaccbe9a9b19a488fcaf95ec012c79ae",
            "filename": "src/transformers/models/llava_next_video/modeling_llava_next_video.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_next_video%2Fmodeling_llava_next_video.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -689,9 +689,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "ea5ca1e5ea63225cf25c89894e9bcc0ce2942a6a",
            "filename": "src/transformers/models/llava_onevision/modeling_llava_onevision.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava_onevision%2Fmodeling_llava_onevision.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -722,9 +722,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "d4e29c619b115ca3a1c085857759b8ab3ff22d80",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1367,10 +1367,6 @@ def __init__(self, config, embed_tokens=None):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.t5.modeling_t5.T5Stack.get_input_embeddings\n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n     # Copied from transformers.models.t5.modeling_t5.T5Stack.set_input_embeddings\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n@@ -1929,12 +1925,6 @@ def _tie_weights(self):\n             self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n             self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     def get_encoder(self):\n         return self.encoder\n "
        },
        {
            "sha": "6790872107a1310b1ae364fa25ee7dd961337537",
            "filename": "src/transformers/models/m2m_100/modeling_m2m_100.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fm2m_100%2Fmodeling_m2m_100.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1330,12 +1330,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "83de5dd6aebc48ef2fc881ce71b35dd60ae1e60a",
            "filename": "src/transformers/models/mamba/modeling_mamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba%2Fmodeling_mamba.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -714,12 +714,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_input_embeddings(self):\n         return self.backbone.get_input_embeddings()\n "
        },
        {
            "sha": "7809707d4c844b8563e0e7d1e0d92ad9aa098a57",
            "filename": "src/transformers/models/mamba2/modeling_mamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmamba2%2Fmodeling_mamba2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -947,12 +947,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_input_embeddings(self):\n         return self.backbone.get_input_embeddings()\n "
        },
        {
            "sha": "5f988b3a82ee1557bbc0b4e439bf711773401329",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 0,
            "deletions": 21,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -724,12 +724,6 @@ def __init__(self, config: MarianConfig, embed_tokens: Optional[nn.Embedding] =\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -881,12 +875,6 @@ def __init__(self, config: MarianConfig, embed_tokens: Optional[nn.Embedding] =\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1424,9 +1412,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     def set_output_embeddings(self, new_embeddings: nn.Embedding):\n         self.lm_head = new_embeddings\n \n@@ -1617,12 +1602,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.decoder = decoder\n "
        },
        {
            "sha": "0a6880415f9e685914f983c769aab424760186da",
            "filename": "src/transformers/models/mbart/modeling_mbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmbart%2Fmodeling_mbart.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -920,12 +920,6 @@ def __init__(self, config: MBartConfig, embed_tokens: Optional[nn.Embedding] = N\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1341,12 +1335,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1814,12 +1802,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.decoder = decoder\n "
        },
        {
            "sha": "6923fdc91abb383fb8be8dc0f5d08a689bafb9ad",
            "filename": "src/transformers/models/minimax/modeling_minimax.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fminimax%2Fmodeling_minimax.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -663,12 +663,6 @@ def __init__(self, config: MiniMaxConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -845,18 +839,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -974,12 +956,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1066,12 +1042,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "1189f1b34d38d12f35fd0324d70670663c2a2af6",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -333,12 +333,6 @@ def __init__(self, config: MistralConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -416,18 +410,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -518,12 +500,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -593,12 +569,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "b9afd8ab268dbb9695429cc342d2335a76d798c1",
            "filename": "src/transformers/models/mistral/modeling_tf_mistral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_tf_mistral.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -499,12 +499,6 @@ def __init__(self, config: MistralConfig, **kwargs):\n         self.norm = TFMistralRMSNorm(config.hidden_size, eps=config.rms_norm_eps, name=\"norm\")\n         self.config = config\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def _prepare_decoder_attention_mask(self, attention_mask, input_shape, inputs_embeds, past_key_values_length):\n         # create causal mask\n         # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]\n@@ -815,18 +809,6 @@ def __init__(self, config, *inputs, **kwargs):\n         )\n         self.config = config\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -952,12 +934,6 @@ def __init__(self, config, *inputs, **kwargs):\n         )\n         self.config = config\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @unpack_inputs\n     @add_start_docstrings_to_model_forward(MISTRAL_INPUTS_DOCSTRING.format(\"batch_size, sequence_length\"))\n     def call("
        },
        {
            "sha": "d1a9c83f9dadcc5c10e931941333dd7d3135fdde",
            "filename": "src/transformers/models/mistral3/modeling_mistral3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral3%2Fmodeling_mistral3.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -390,9 +390,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "ec185002691d6d2b2d1c41f302fc18403984da93",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -428,12 +428,6 @@ def __init__(self, config: MixtralConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -599,18 +593,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -728,12 +710,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -820,12 +796,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "7c126f42f1e5c129db9fefd8fd0369945d262e82",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1273,12 +1273,6 @@ def __init__(self, config: MllamaTextConfig):\n         self.gradient_checkpointing = False\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1468,18 +1462,6 @@ def __init__(self, config):\n \n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -1775,12 +1757,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "9b229e4074c0bc5c2d3df1a3534b6bf019273954",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -614,12 +614,6 @@ def __init__(self, config: MoonshineConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     def forward(\n         self,"
        },
        {
            "sha": "5a9aa1e993d3d431d01f4897105fb72f414e15fe",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1226,12 +1226,6 @@ def __init__(self, config: MoshiConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1503,18 +1497,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "849b3c48519e4496123ae20be081f2495bfec437",
            "filename": "src/transformers/models/mpt/modeling_mpt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmpt%2Fmodeling_mpt.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -438,9 +438,6 @@ def __init__(self, config: MptConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     def set_output_embeddings(self, new_embeddings: torch.Tensor):\n         self.lm_head = new_embeddings\n "
        },
        {
            "sha": "a9d0fd9781a7a91e888f6c93e46362510e0e2e2b",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 13,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -927,9 +927,6 @@ def deparallelize(self):\n         self.final_layer_norm = self.final_layer_norm.to(\"cpu\")\n         torch.cuda.empty_cache()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n@@ -1633,24 +1630,14 @@ def deparallelize(self):\n         self.device_map = None\n         torch.cuda.empty_cache()\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.shared\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.set_input_embeddings\n     def set_input_embeddings(self, new_embeddings):\n         self.shared = new_embeddings\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_encoder\n     def get_encoder(self):\n         return self.encoder"
        },
        {
            "sha": "64edaee56f2c0b92f4aa1dd25d0610dd62808ca9",
            "filename": "src/transformers/models/musicgen/modeling_musicgen.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen%2Fmodeling_musicgen.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -476,12 +476,6 @@ def __init__(self, config: MusicgenDecoderConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "a454d9fe24bee9ad2e7fb12b3ab5be56a4288359",
            "filename": "src/transformers/models/musicgen_melody/modeling_musicgen_melody.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmusicgen_melody%2Fmodeling_musicgen_melody.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -442,12 +442,6 @@ def __init__(self, config: MusicgenMelodyDecoderConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     # Ignore copy\n     def forward("
        },
        {
            "sha": "52da33565d1aeb5f7921c6f9ed77b289e8e4e1eb",
            "filename": "src/transformers/models/mvp/modeling_mvp.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmvp%2Fmodeling_mvp.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -560,12 +560,6 @@ def __init__(\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -747,12 +741,6 @@ def __init__(\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1165,12 +1153,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_lightweight_tuning(self):\n         self.model.set_lightweight_tuning()\n         self.lm_head.requires_grad_(False)\n@@ -1695,12 +1677,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.decoder = decoder\n "
        },
        {
            "sha": "df64eec95c31c6afc7cfc441e7ea3cc65f0625b6",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -631,12 +631,6 @@ def __init__(self, config: NemotronConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -869,18 +863,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -987,12 +969,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1142,12 +1118,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "909993b5d3d053023cdb638eb0ecaa0e08e1879a",
            "filename": "src/transformers/models/nllb_moe/modeling_nllb_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnllb_moe%2Fmodeling_nllb_moe.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1605,12 +1605,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "817341cc258fdbcca5fce5862a518d33a9e30fb8",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -331,12 +331,6 @@ def __init__(self, config: OlmoConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -413,18 +407,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "b589364df59a91ea0c44d0926d0970e1f1c36d6b",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -338,12 +338,6 @@ def __init__(self, config: Olmo2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -420,18 +414,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "8caff4d9abf1b66e1d2daa7f672c568eef23af54",
            "filename": "src/transformers/models/olmoe/modeling_olmoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmoe%2Fmodeling_olmoe.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -740,12 +740,6 @@ def __init__(self, config: OlmoeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -992,18 +986,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "27c84910cb43340b06e3b19a30ffbd793072ea3e",
            "filename": "src/transformers/models/openai/modeling_openai.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopenai%2Fmodeling_openai.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -542,12 +542,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -633,12 +627,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "2fe9a146774c657216f0aa9d16a82308eda54bb7",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -375,12 +375,6 @@ def __init__(self, config: OPTConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     # Copied from transformers.models.gptj.modeling_gptj.GPTJModel._update_causal_mask\n     def _update_causal_mask(\n         self,\n@@ -775,12 +769,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.decoder = decoder\n "
        },
        {
            "sha": "b7c817a0348fd51506c0c03923d81517cde61812",
            "filename": "src/transformers/models/opt/modeling_tf_opt.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_tf_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_tf_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_tf_opt.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -544,9 +544,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens.vocab_size = new_embeddings.shape[0]\n         self.embed_tokens.weight = new_embeddings\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n     def _prepare_decoder_attention_mask(self, attention_mask, input_shape, past_key_values_length):\n         # create causal mask\n         # # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]"
        },
        {
            "sha": "581269653c317b5355f14e1c918cd1e181f74697",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -404,12 +404,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "05e34da4d2b362213390de2c571b632723de199e",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -896,12 +896,6 @@ def __init__(self, config: PegasusConfig, embed_tokens: Optional[nn.Embedding] =\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def resize_position_embeddings(self, new_num_position_embeddings: int):\n         \"\"\"\n         Resizes position embeddings matrix of the model if `new_num_position_embeddings !=\n@@ -1372,12 +1366,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def resize_position_embeddings(self, new_num_position_embeddings: int):\n         \"\"\"\n         Resizes position embeddings matrix of the model if `new_num_position_embeddings !=\n@@ -1561,12 +1549,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.decoder = decoder\n "
        },
        {
            "sha": "ee48a02b04cf7fc15a9429d826d497bd47c31188",
            "filename": "src/transformers/models/pegasus_x/modeling_pegasus_x.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus_x%2Fmodeling_pegasus_x.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1200,12 +1200,6 @@ def __init__(self, config: PegasusXConfig, embed_tokens: Optional[nn.Embedding]\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids=None,\n@@ -1596,12 +1590,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def resize_position_embeddings(self, new_num_position_embeddings: int):\n         \"\"\"\n         Resizes position embeddings matrix of the model if `new_num_position_embeddings !="
        },
        {
            "sha": "657aa805691bcc075595ac12388f338acd409ddd",
            "filename": "src/transformers/models/perception_lm/modeling_perception_lm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fperception_lm%2Fmodeling_perception_lm.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -326,9 +326,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "64f3bdd7b559303b2aa790c6d11c6b2af0f27d68",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -435,12 +435,6 @@ def __init__(self, config: PersimmonConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -676,22 +670,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_input_embeddings\n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_input_embeddings\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n     def set_decoder(self, decoder):\n         self.model = decoder\n@@ -807,12 +785,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -900,12 +872,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "ea77b2d4719a3737b3240b0b845d3c92cecd85dc",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -340,12 +340,6 @@ def __init__(self, config: PhiConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -460,18 +454,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -568,12 +550,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -660,12 +636,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "c896def491e322919a3956e476d619e053533e44",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -365,12 +365,6 @@ def __init__(self, config: Phi3Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -448,18 +442,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -595,12 +577,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -687,12 +663,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "855e8b7fc1b26eb2f3ef46dca6c9f6caec5dc5cc",
            "filename": "src/transformers/models/phi4_multimodal/modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodeling_phi4_multimodal.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1641,12 +1641,6 @@ def __init__(self, config: Phi4MultimodalConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     def forward(\n         self,\n@@ -1757,18 +1751,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "0662acf7e698f72f03fd639b957d592eb1ca97d9",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 28,
            "changes": 28,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -932,12 +932,6 @@ def __init__(self, config: PhimoeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1213,22 +1207,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_input_embeddings\n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_input_embeddings\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n     def set_decoder(self, decoder):\n         self.model = decoder\n@@ -1399,12 +1377,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "5d080e8f0c99d6c5350c32e23fc011941713e516",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1037,18 +1037,9 @@ def __init__(self, config):\n         self.post_init()\n         self.gradient_checkpointing = False\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "4236476349b51923b2ce2e880ee77363d0b850c6",
            "filename": "src/transformers/models/plbart/modeling_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodeling_plbart.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -575,12 +575,6 @@ def __init__(self, config: PLBartConfig, embed_tokens: Optional[nn.Embedding] =\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -857,12 +851,6 @@ def __init__(self, config: PLBartConfig, embed_tokens: Optional[nn.Embedding] =\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: Optional[torch.LongTensor] = None,\n@@ -1298,12 +1286,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1645,12 +1627,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.decoder.embed_tokens = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.decoder = decoder\n "
        },
        {
            "sha": "3de32a625a61f6bacd15960cb6ccbb570c19beb8",
            "filename": "src/transformers/models/plbart/modular_plbart.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fplbart%2Fmodular_plbart.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -464,12 +464,6 @@ def _resize_final_logits_bias(self, new_num_tokens: int) -> None:\n             new_bias = torch.cat([self.final_logits_bias, extra_bias], dim=1)\n         self.register_buffer(\"final_logits_bias\", new_bias)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "795dfb5874214d5159b76d861e35e415a0573948",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 0,
            "deletions": 10,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -678,10 +678,6 @@ def __init__(self, config, embed_tokens=None):\n         self.device_map = None\n         self.gradient_checkpointing = False\n \n-    # Copied from transformers.models.t5.modeling_t5.T5Stack.get_input_embeddings\n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n     # Copied from transformers.models.t5.modeling_t5.T5Stack.set_input_embeddings\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n@@ -1045,12 +1041,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     def get_encoder(self):\n         return self.encoder\n "
        },
        {
            "sha": "a7558b16ed45f7a701c9d28fc3b69588c3a53241",
            "filename": "src/transformers/models/prophetnet/modeling_prophetnet.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fprophetnet%2Fmodeling_prophetnet.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1637,12 +1637,6 @@ def __init__(self, config: ProphetNetConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n             self._tie_or_clone_weights(self.prophetnet.word_embeddings, self.lm_head)\n@@ -1846,12 +1840,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.prophetnet.decoder.word_embeddings = value\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def _tie_weights(self):\n         if self.config.tie_word_embeddings:\n             self._tie_or_clone_weights(self.prophetnet.decoder.word_embeddings, self.lm_head)"
        },
        {
            "sha": "e8c5a1bc8a8a6380a79b502a578913464e182495",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -337,12 +337,6 @@ def __init__(self, config: Qwen2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -432,18 +426,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -540,12 +522,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -632,12 +608,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "37c0c6da43def243425f8af93fe46b329402adb9",
            "filename": "src/transformers/models/qwen2_5_omni/modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 12,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodeling_qwen2_5_omni.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1553,12 +1553,6 @@ def __init__(self, config: Qwen2_5OmniTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1702,6 +1696,7 @@ def forward(\n class Qwen2_5OmniThinkerForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration, GenerationMixin):\n     config: Qwen2_5OmniThinkerConfig\n     base_model_prefix = \"thinker\"\n+    _tied_weights_keys = [\"model.embed_tokens.weight\", \"lm_head.weight\"]\n     _no_split_modules = [\"Qwen2_5OmniAudioEncoder\", \"Qwen2_5OmniVisionEncoder\"]\n \n     def __init__(self, config: Qwen2_5OmniThinkerConfig):\n@@ -2110,12 +2105,6 @@ def __init__(self, config: Qwen2_5OmniTalkerConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "d40b0a073c79171c37cc1d3693fefc5357322b9d",
            "filename": "src/transformers/models/qwen2_5_omni/modular_qwen2_5_omni.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_omni%2Fmodular_qwen2_5_omni.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -2142,6 +2142,7 @@ def __init__(self, config: Qwen2_5OmniTextConfig):\n class Qwen2_5OmniThinkerForConditionalGeneration(Qwen2_5OmniPreTrainedModelForConditionalGeneration, GenerationMixin):\n     config: Qwen2_5OmniThinkerConfig\n     base_model_prefix = \"thinker\"\n+    _tied_weights_keys = [\"model.embed_tokens.weight\", \"lm_head.weight\"]\n     _no_split_modules = [\"Qwen2_5OmniAudioEncoder\", \"Qwen2_5OmniVisionEncoder\"]\n \n     def __init__(self, config: Qwen2_5OmniThinkerConfig):"
        },
        {
            "sha": "66fb7a7c06d5cf146bcf8a8f0b5c0b6a940b6036",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -824,12 +824,6 @@ def __init__(self, config: Qwen2_5_VLTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1402,12 +1396,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "ff6eb1908d6f83b1fb0886fee87d208c01473937",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 37,
            "changes": 37,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -780,12 +780,6 @@ def __init__(self, config: Qwen2MoeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1065,18 +1059,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -1202,12 +1184,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1295,12 +1271,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1347,7 +1317,6 @@ def forward(\n \n \n @auto_docstring\n-# Copied from transformers.models.mistral.modeling_mistral.MistralForQuestionAnswering with Mistral->Qwen2Moe, MISTRAL->QWEN2MOE, BaseModelOutputWithPast->MoeModelOutputWithPast\n class Qwen2MoeForQuestionAnswering(Qwen2MoePreTrainedModel):\n     base_model_prefix = \"model\"\n \n@@ -1359,12 +1328,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "b14264f455166c01afa24a1d848dbf616dd25bdb",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -799,12 +799,6 @@ def __init__(self, config: Qwen2VLTextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1295,12 +1289,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "73f063148089b17f3a24cfe884956307b5a1ce18",
            "filename": "src/transformers/models/qwen3/modeling_qwen3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3%2Fmodeling_qwen3.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -363,12 +363,6 @@ def __init__(self, config: Qwen3Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -458,18 +452,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -566,12 +548,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -658,12 +634,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "f37568777b9272e4484037ae6f3bab9e3207c354",
            "filename": "src/transformers/models/qwen3_moe/modeling_qwen3_moe.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodeling_qwen3_moe.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -451,12 +451,6 @@ def __init__(self, config: Qwen3MoeConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -622,18 +616,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -751,12 +733,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -843,12 +819,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "2385f8e62325d144e27f9000855cd0a6cca164b7",
            "filename": "src/transformers/models/recurrent_gemma/modeling_recurrent_gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 20,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frecurrent_gemma%2Fmodeling_recurrent_gemma.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -587,14 +587,6 @@ def __init__(self, config: RecurrentGemmaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel.get_input_embeddings\n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaModel.set_input_embeddings\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -705,18 +697,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n "
        },
        {
            "sha": "1c47aec148b5cb7bc2781e329d23458e8b34c328",
            "filename": "src/transformers/models/seamless_m4t/modeling_seamless_m4t.py",
            "status": "modified",
            "additions": 0,
            "deletions": 42,
            "changes": 42,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t%2Fmodeling_seamless_m4t.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1753,12 +1753,6 @@ def __init__(\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -2025,12 +2019,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.decoder\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_input_embeddings(self):\n         return self.model.decoder.embed_tokens\n \n@@ -2498,12 +2486,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.text_decoder\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_input_embeddings(self):\n         return self.text_decoder.embed_tokens\n \n@@ -2759,12 +2741,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.text_decoder\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_input_embeddings(self):\n         return self.text_decoder.embed_tokens\n \n@@ -3031,12 +3007,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.text_decoder\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_input_embeddings(self):\n         return self.text_decoder.embed_tokens\n \n@@ -3358,12 +3328,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.text_decoder\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_input_embeddings(self):\n         return self.text_decoder.embed_tokens\n \n@@ -3714,12 +3678,6 @@ def get_encoder(self):\n         else:\n             return self.speech_encoder\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_input_embeddings(self):\n         return self.text_decoder.embed_tokens\n "
        },
        {
            "sha": "6160d311c8a288bee0ca4d8394135e5374df7fae",
            "filename": "src/transformers/models/seamless_m4t_v2/modeling_seamless_m4t_v2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 58,
            "changes": 58,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fseamless_m4t_v2%2Fmodeling_seamless_m4t_v2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1796,12 +1796,6 @@ def __init__(\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -2001,12 +1995,6 @@ def __init__(\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         char_input_ids: Optional[torch.LongTensor] = None,\n@@ -2236,14 +2224,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.decoder\n \n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TTextToUnitForConditionalGeneration.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TTextToUnitForConditionalGeneration.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TTextToUnitForConditionalGeneration.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.model.decoder.embed_tokens\n@@ -2714,12 +2694,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.text_decoder\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_input_embeddings(self):\n         return self.text_decoder.embed_tokens\n \n@@ -2978,14 +2952,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.text_decoder\n \n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToText.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.text_decoder.embed_tokens\n@@ -3260,14 +3226,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.text_decoder\n \n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForTextToSpeech.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.text_decoder.embed_tokens\n@@ -3627,14 +3585,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.text_decoder\n \n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TForSpeechToSpeech.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.text_decoder.embed_tokens\n@@ -4022,14 +3972,6 @@ def get_encoder(self):\n         else:\n             return self.speech_encoder\n \n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     # Copied from transformers.models.seamless_m4t.modeling_seamless_m4t.SeamlessM4TModel.get_input_embeddings\n     def get_input_embeddings(self):\n         return self.text_decoder.embed_tokens"
        },
        {
            "sha": "92a3205a7b263039ea83fd172aefccbe5880eb8b",
            "filename": "src/transformers/models/smollm3/modeling_smollm3.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmollm3%2Fmodeling_smollm3.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -367,12 +367,6 @@ def __init__(self, config: SmolLM3Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     @auto_docstring\n     def forward(\n@@ -462,18 +456,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -570,12 +552,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -662,12 +638,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "d83fe01c144f3df1bafca66102424e7a15a2aa04",
            "filename": "src/transformers/models/smolvlm/modeling_smolvlm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsmolvlm%2Fmodeling_smolvlm.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -843,12 +843,6 @@ def get_input_embeddings(self):\n     def set_input_embeddings(self, value):\n         self.model.text_model.set_input_embeddings(value)\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def get_image_features(self, pixel_values: torch.FloatTensor, pixel_attention_mask: torch.LongTensor = None):\n         return self.model.get_image_features(pixel_values=pixel_values, pixel_attention_mask=pixel_attention_mask)\n "
        },
        {
            "sha": "d85b75b8f232d30b67f15697cb85d7a27d922847",
            "filename": "src/transformers/models/speech_to_text/modeling_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_speech_to_text.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -761,12 +761,6 @@ def __init__(self, config: Speech2TextConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids=None,\n@@ -1220,12 +1214,6 @@ def get_encoder(self):\n     def get_decoder(self):\n         return self.model.get_decoder()\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "81fbf13dafdd0789e299f93df4d4ed31eb8f0022",
            "filename": "src/transformers/models/speech_to_text/modeling_tf_speech_to_text.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeech_to_text%2Fmodeling_tf_speech_to_text.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1428,12 +1428,6 @@ def resize_token_embeddings(self, new_num_tokens: int) -> tf.Variable:\n         new_embeddings = super().resize_token_embeddings(new_num_tokens)\n         return new_embeddings\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @unpack_inputs\n     @add_start_docstrings_to_model_forward(SPEECH_TO_TEXT_INPUTS_DOCSTRING)\n     @replace_return_docstrings(output_type=TFSeq2SeqLMOutput, config_class=_CONFIG_FOR_DOC)"
        },
        {
            "sha": "15d044d0f0c35b16463f9c5b8b249dc8ca63c9b0",
            "filename": "src/transformers/models/speecht5/modeling_speecht5.py",
            "status": "modified",
            "additions": 6,
            "deletions": 16,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fspeecht5%2Fmodeling_speecht5.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -37,7 +37,7 @@\n     Seq2SeqModelOutput,\n     Seq2SeqSpectrogramOutput,\n )\n-from ...modeling_utils import PreTrainedModel\n+from ...modeling_utils import EmbeddingAccessMixin, PreTrainedModel\n from ...utils import auto_docstring, logging\n from .configuration_speecht5 import SpeechT5Config, SpeechT5HifiGanConfig\n \n@@ -762,7 +762,7 @@ def postnet(self, hidden_states: torch.Tensor):\n         return hidden_states + layer_output.transpose(1, 2)\n \n \n-class SpeechT5TextEncoderPrenet(nn.Module):\n+class SpeechT5TextEncoderPrenet(nn.Module, EmbeddingAccessMixin):\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n@@ -773,19 +773,13 @@ def __init__(self, config):\n             config.max_text_positions,\n         )\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(self, input_ids: torch.Tensor):\n         inputs_embeds = self.embed_tokens(input_ids)\n         inputs_embeds = self.encode_positions(inputs_embeds)\n         return inputs_embeds\n \n \n-class SpeechT5TextDecoderPrenet(nn.Module):\n+class SpeechT5TextDecoderPrenet(nn.Module, EmbeddingAccessMixin):\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n@@ -800,12 +794,6 @@ def __init__(self, config):\n             config.pad_token_id,\n         )\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: torch.Tensor,\n@@ -835,7 +823,7 @@ def forward(\n         return inputs_embeds, attention_mask\n \n \n-class SpeechT5TextDecoderPostnet(nn.Module):\n+class SpeechT5TextDecoderPostnet(nn.Module, EmbeddingAccessMixin):\n     def __init__(self, config):\n         super().__init__()\n         self.config = config\n@@ -845,6 +833,8 @@ def forward(self, hidden_states: torch.Tensor):\n         return self.lm_head(hidden_states)\n \n     def get_output_embeddings(self):\n+        # Post-net has no token embeddings, but its lm_head must still be\n+        # tied to the decoder weights when `tie_word_embeddings=True`.\n         return self.lm_head\n \n     def set_output_embeddings(self, new_embeddings):"
        },
        {
            "sha": "b7a61f3360debdf1d9486f66514abf72e4db0489",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 34,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -664,12 +664,6 @@ def __init__(self, config: StableLmConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -904,22 +898,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_input_embeddings\n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_input_embeddings\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     # Copied from transformers.models.llama.modeling_llama.LlamaForCausalLM.set_decoder\n     def set_decoder(self, decoder):\n         self.model = decoder\n@@ -1035,12 +1013,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -1128,12 +1100,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "9e574d534980d02fc1ba2cd53a8e50e2edf2eb95",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 30,
            "changes": 30,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -338,12 +338,6 @@ def __init__(self, config: Starcoder2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     def forward(\n         self,\n@@ -426,18 +420,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -534,12 +516,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward(\n@@ -626,12 +602,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @can_return_tuple\n     @auto_docstring\n     def forward("
        },
        {
            "sha": "d71dffb987824c83fea4986974d74423c8a0e7a7",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -887,9 +887,6 @@ def __init__(self, config, embed_tokens=None):\n         self.device_map = None\n         self.gradient_checkpointing = False\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n@@ -1478,12 +1475,6 @@ def _tie_weights(self):\n             self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n             self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     def get_encoder(self):\n         return self.encoder\n "
        },
        {
            "sha": "b5ff699f69a388382e507ddba3b55b89bb45540b",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -940,9 +940,6 @@ def deparallelize(self):\n         self.final_layer_norm = self.final_layer_norm.to(\"cpu\")\n         torch.cuda.empty_cache()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n@@ -1619,12 +1616,6 @@ def _tie_weights(self):\n             self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n             self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     def get_encoder(self):\n         return self.encoder\n "
        },
        {
            "sha": "c2fdbf5fc7d4018fd70044ba17a64318fd5ce97d",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -707,12 +707,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     def forward(\n         self,"
        },
        {
            "sha": "5c72d76b4e17ab2d6b370d0a2c4b76f2935baafb",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -574,12 +574,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @check_model_inputs\n     def forward(\n         self,"
        },
        {
            "sha": "fd1cc21fff190de125da3b453e35596870f0edb3",
            "filename": "src/transformers/models/trocr/modeling_trocr.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftrocr%2Fmodeling_trocr.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -472,12 +472,6 @@ def __init__(self, config: TrOCRConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids=None,"
        },
        {
            "sha": "70a174474b7d42cd70db9645259ae9322c8b30c0",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 0,
            "deletions": 9,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1132,9 +1132,6 @@ def _get_relative_bias(config: UdopConfig) -> RelativePositionBiasAggregated:\n         relative_bias_list = create_relative_bias(config)\n         return RelativePositionBiasAggregated(relative_bias_list)\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n     def get_output_embeddings(self):\n         return self.embed_tokens\n \n@@ -1713,12 +1710,6 @@ def set_input_embeddings(self, new_embeddings):\n         self.encoder.set_input_embeddings(new_embeddings)\n         self.decoder.set_input_embeddings(new_embeddings)\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     def get_encoder(self):\n         return self.encoder\n "
        },
        {
            "sha": "47b11acfd8dc042b740e49fb28577a25a6250f6f",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -630,9 +630,6 @@ def __init__(self, config, embed_tokens=None):\n         self.gradient_checkpointing = False\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n     def set_input_embeddings(self, new_embeddings):\n         self.embed_tokens = new_embeddings\n \n@@ -1206,14 +1203,6 @@ def _tie_weights(self):\n             self._tie_or_clone_weights(self.encoder.embed_tokens, self.shared)\n             self._tie_or_clone_weights(self.decoder.embed_tokens, self.shared)\n \n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.set_output_embeddings\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n-    # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_output_embeddings\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n     # Copied from transformers.models.t5.modeling_t5.T5ForConditionalGeneration.get_encoder\n     def get_encoder(self):\n         return self.encoder"
        },
        {
            "sha": "aea725686822dae0d3e71bcf37d0f79bf89d28fd",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -430,9 +430,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "d3c263807bea41cdfaba34a16572b13f3dcd91f8",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 0,
            "deletions": 3,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -313,9 +313,6 @@ def set_input_embeddings(self, value):\n     def get_output_embeddings(self) -> nn.Module:\n         return self.lm_head\n \n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model.set_decoder(decoder)\n "
        },
        {
            "sha": "40ce1e1083c8009b0df852e3445f4e6025cf28ef",
            "filename": "src/transformers/models/vits/modeling_vits.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvits%2Fmodeling_vits.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1167,12 +1167,6 @@ def __init__(self, config: VitsConfig):\n         self.encoder = VitsEncoder(config)\n         self.project = nn.Conv1d(config.hidden_size, config.flow_size * 2, kernel_size=1)\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids: torch.Tensor,"
        },
        {
            "sha": "a3c71745c5a5f5a10038ae3defcd913b3e4ef553",
            "filename": "src/transformers/models/whisper/modeling_tf_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_tf_whisper.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -867,12 +867,6 @@ def __init__(self, config: WhisperConfig, **kwargs):\n \n         self.layer_norm = keras.layers.LayerNormalization(epsilon=1e-5, name=\"layer_norm\")\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def _prepare_decoder_attention_mask(self, attention_mask, input_shape, past_key_values_length):\n         # create causal mask\n         # [bsz, seq_len] -> [bsz, 1, tgt_seq_len, src_seq_len]"
        },
        {
            "sha": "7b74c4c0b8539d0f5b6724a557ac7236d5d86f8e",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -762,12 +762,6 @@ def __init__(self, config: WhisperConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     def forward(\n         self,\n         input_ids=None,"
        },
        {
            "sha": "5915e2e9aebbe0f982bae45c18196628539d244f",
            "filename": "src/transformers/models/xglm/modeling_tf_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 6,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_tf_xglm.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -885,12 +885,6 @@ def __init__(\n         )\n         self.config = config\n \n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def prepare_inputs_for_generation(self, inputs, past_key_values=None, use_cache=None, **kwargs):\n         # only last token for inputs_ids if past is defined in kwargs\n         if past_key_values:"
        },
        {
            "sha": "d5f9f0918722898e5ebd427e7bf114701896e21a",
            "filename": "src/transformers/models/xglm/modeling_xglm.py",
            "status": "modified",
            "additions": 0,
            "deletions": 18,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fxglm%2Fmodeling_xglm.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -425,12 +425,6 @@ def __init__(self, config: XGLMConfig, embed_tokens: Optional[nn.Embedding] = No\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -617,18 +611,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "b1a8f8c51696a32ba8714cfe07b95eadd7831410",
            "filename": "src/transformers/models/zamba/modeling_zamba.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba%2Fmodeling_zamba.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -880,12 +880,6 @@ def __init__(self, config: ZambaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1045,18 +1039,6 @@ def __init__(self, config: ZambaConfig):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -1230,12 +1212,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "fe1482bcdfcba5eea0db5ef227b8f51230e58c27",
            "filename": "src/transformers/models/zamba2/modeling_zamba2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 24,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fzamba2%2Fmodeling_zamba2.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -1254,12 +1254,6 @@ def __init__(self, config: Zamba2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,\n@@ -1477,18 +1471,6 @@ def __init__(self, config: Zamba2Config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n-    def get_output_embeddings(self):\n-        return self.lm_head\n-\n-    def set_output_embeddings(self, new_embeddings):\n-        self.lm_head = new_embeddings\n-\n     def set_decoder(self, decoder):\n         self.model = decoder\n \n@@ -1662,12 +1644,6 @@ def __init__(self, config):\n         # Initialize weights and apply final processing\n         self.post_init()\n \n-    def get_input_embeddings(self):\n-        return self.model.embed_tokens\n-\n-    def set_input_embeddings(self, value):\n-        self.model.embed_tokens = value\n-\n     @auto_docstring\n     def forward(\n         self,"
        },
        {
            "sha": "642759b00dcc56b79464619fcb9035422526a0d9",
            "filename": "tests/models/speecht5/test_modeling_speecht5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 0,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_modeling_speecht5.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -401,6 +401,10 @@ def test_decoder_model_past_with_large_inputs(self):\n         config_and_inputs = self.model_tester.prepare_config_and_inputs()\n         self.model_tester.create_and_check_decoder_model_past_large_inputs(*config_and_inputs)\n \n+    @unittest.skip(reason=\"skipped because of dropout\")\n+    def test_batching_equivalence(self):\n+        pass\n+\n     def test_attention_outputs(self):\n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()\n         config.return_dict = True"
        },
        {
            "sha": "75340fa4e0ced4ff82f1390337c997acac9f6f34",
            "filename": "tests/models/speecht5/test_processor_speecht5.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/69b158260fcb679ea3bfbc1e6a358545ee53ee28/tests%2Fmodels%2Fspeecht5%2Ftest_processor_speecht5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/69b158260fcb679ea3bfbc1e6a358545ee53ee28/tests%2Fmodels%2Fspeecht5%2Ftest_processor_speecht5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fspeecht5%2Ftest_processor_speecht5.py?ref=69b158260fcb679ea3bfbc1e6a358545ee53ee28",
            "patch": "@@ -21,7 +21,7 @@\n \n from transformers import is_speech_available, is_torch_available\n from transformers.models.speecht5 import SpeechT5Tokenizer\n-from transformers.testing_utils import get_tests_dir, require_torch\n+from transformers.testing_utils import get_tests_dir, require_speech, require_torch\n from transformers.utils import FEATURE_EXTRACTOR_NAME\n \n \n@@ -35,6 +35,7 @@\n \n \n @require_torch\n+@require_speech\n class SpeechT5ProcessorTest(unittest.TestCase):\n     @classmethod\n     def setUpClass(cls):"
        }
    ],
    "stats": {
        "total": 2623,
        "additions": 235,
        "deletions": 2388
    }
}