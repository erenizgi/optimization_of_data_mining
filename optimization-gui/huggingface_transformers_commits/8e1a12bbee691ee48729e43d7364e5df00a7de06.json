{
    "author": "MekkCyber",
    "message": "Fixing the call to kernelize (#40628)\n\n* fix\n\n* style\n\n* overload train and eval\n\n* add getter and setter",
    "sha": "8e1a12bbee691ee48729e43d7364e5df00a7de06",
    "files": [
        {
            "sha": "4d2011b09a15010f7f2cc8e1d54bb0e835ffd364",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 40,
            "deletions": 8,
            "changes": 48,
            "blob_url": "https://github.com/huggingface/transformers/blob/8e1a12bbee691ee48729e43d7364e5df00a7de06/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8e1a12bbee691ee48729e43d7364e5df00a7de06/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=8e1a12bbee691ee48729e43d7364e5df00a7de06",
            "patch": "@@ -5203,14 +5203,7 @@ def _assign_original_dtype(module):\n \n         # check if using kernels\n         if use_kernels:\n-            if not is_kernels_available():\n-                raise ValueError(\n-                    \"Kernels are not available. To use kernels, please install kernels using `pip install kernels`\"\n-                )\n-\n-            from kernels import Device, kernelize\n-\n-            kernelize(model, device=Device(type=model.device.type))\n+            model.use_kernels = True\n \n         # If it is a model with generation capabilities, attempt to load generation files (generation config,\n         # custom generate function)\n@@ -5971,6 +5964,36 @@ def loss_function(self):\n     def loss_function(self, value):\n         self._loss_function = value\n \n+    def kernelize(self):\n+        if not is_kernels_available():\n+            raise ValueError(\n+                \"Kernels are not available. To use kernels, please install kernels using `pip install kernels`\"\n+            )\n+        from kernels import Device, Mode, kernelize\n+\n+        mode = Mode.INFERENCE if not self.training else Mode.TRAINING\n+        kernelize(self, device=Device(type=self.device.type), mode=mode)\n+        self._use_kernels = True\n+\n+    @property\n+    def use_kernels(self) -> bool:\n+        return getattr(self, \"_use_kernels\", False)\n+\n+    @use_kernels.setter\n+    def use_kernels(self, value: bool) -> None:\n+        # Avoid re-kernelizing if already enabled\n+        if bool(value) and getattr(self, \"_use_kernels\", False):\n+            return\n+\n+        if value:\n+            self.kernelize()\n+        else:\n+            if getattr(self, \"_use_kernels\", False):\n+                logger.warning_once(\n+                    \"Disabling kernels at runtime is a no-op as there is no 'unkernelize' routine; keeping current kernels active.\"\n+                )\n+            self._use_kernels = False\n+\n     def get_compiled_call(self, compile_config: Optional[CompileConfig]) -> Callable:\n         \"\"\"Return a `torch.compile`'d version of `self.__call__`. This is useful to dynamically choose between\n         non-compiled/compiled `forward` during inference, especially to switch between prefill (where we don't\n@@ -6093,6 +6116,15 @@ def get_parameter_or_buffer(self, target: str):\n \n         raise AttributeError(f\"`{target}` is neither a parameter, buffer, nor extra state.\")\n \n+    def train(self, mode: bool = True):\n+        out = super().train(mode)\n+        if self.use_kernels:\n+            self.kernelize()\n+        return out\n+\n+    def eval(self):\n+        return self.train(False)\n+\n \n PreTrainedModel.push_to_hub = copy_func(PreTrainedModel.push_to_hub)\n if PreTrainedModel.push_to_hub.__doc__ is not None:"
        }
    ],
    "stats": {
        "total": 48,
        "additions": 40,
        "deletions": 8
    }
}