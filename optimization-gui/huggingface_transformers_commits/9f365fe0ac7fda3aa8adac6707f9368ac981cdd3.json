{
    "author": "zucchini-nlp",
    "message": "Fix right padding in LLaVA models (#34305)\n\n* fix right pad llavas\r\n\r\n* device mismatch",
    "sha": "9f365fe0ac7fda3aa8adac6707f9368ac981cdd3",
    "files": [
        {
            "sha": "0b2492fc711206ef9a6029a1e632400c75bdb781",
            "filename": "src/transformers/models/llava/modeling_llava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f365fe0ac7fda3aa8adac6707f9368ac981cdd3/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f365fe0ac7fda3aa8adac6707f9368ac981cdd3/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllava%2Fmodeling_llava.py?ref=9f365fe0ac7fda3aa8adac6707f9368ac981cdd3",
            "patch": "@@ -354,7 +354,12 @@ def _merge_input_ids_with_image_features(self, image_features, inputs_embeds, in\n             (batch_size, max_embed_dim), True, dtype=torch.bool, device=inputs_embeds.device\n         )\n         image_to_overwrite[batch_indices, text_to_overwrite] = False\n-        image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n+        if left_padding:\n+            image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n+        else:\n+            mask = torch.ones_like(image_to_overwrite, dtype=torch.bool).cumsum(-1) - 1\n+            padding_mask = mask <= new_token_positions[:, -1:].to(target_device)\n+            image_to_overwrite &= padding_mask\n \n         if image_to_overwrite.sum() != image_features.shape[:-1].numel():\n             raise ValueError("
        },
        {
            "sha": "a9bd8b745a6f68f6b29cd25e8ca6ba2ecd7c3a38",
            "filename": "src/transformers/models/video_llava/modeling_video_llava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f365fe0ac7fda3aa8adac6707f9368ac981cdd3/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f365fe0ac7fda3aa8adac6707f9368ac981cdd3/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvideo_llava%2Fmodeling_video_llava.py?ref=9f365fe0ac7fda3aa8adac6707f9368ac981cdd3",
            "patch": "@@ -339,7 +339,12 @@ def _merge_input_ids_with_visual_features(\n         # 5. Fill the embeddings corresponding to the images. Anything that is still zeros needs filling\n         image_to_overwrite = torch.full((batch_size, max_seq_len), True, dtype=torch.bool, device=inputs_embeds.device)\n         image_to_overwrite[batch_indices, text_to_overwrite] = False\n-        image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n+        if left_padding:\n+            image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n+        else:\n+            mask = torch.ones_like(image_to_overwrite, dtype=torch.bool).cumsum(-1) - 1\n+            padding_mask = mask <= new_token_positions[:, -1:].to(target_device)\n+            image_to_overwrite &= padding_mask\n \n         if image_to_overwrite.sum() != visual_features.shape[:-1].numel():\n             visual_type = \"videos\" if num_frames == 8 else \"images\""
        },
        {
            "sha": "987ae0ad0c61fe00dcf48fb48ba76569423efde4",
            "filename": "src/transformers/models/vipllava/modeling_vipllava.py",
            "status": "modified",
            "additions": 6,
            "deletions": 1,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/9f365fe0ac7fda3aa8adac6707f9368ac981cdd3/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/9f365fe0ac7fda3aa8adac6707f9368ac981cdd3/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvipllava%2Fmodeling_vipllava.py?ref=9f365fe0ac7fda3aa8adac6707f9368ac981cdd3",
            "patch": "@@ -350,7 +350,12 @@ def _merge_input_ids_with_image_features(self, image_features, inputs_embeds, in\n             (batch_size, max_embed_dim), True, dtype=torch.bool, device=inputs_embeds.device\n         )\n         image_to_overwrite[batch_indices, text_to_overwrite] = False\n-        image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n+        if left_padding:\n+            image_to_overwrite &= image_to_overwrite.cumsum(-1) - 1 >= nb_image_pad[:, None].to(target_device)\n+        else:\n+            mask = torch.ones_like(image_to_overwrite, dtype=torch.bool).cumsum(-1) - 1\n+            padding_mask = mask <= new_token_positions[:, -1:].to(target_device)\n+            image_to_overwrite &= padding_mask\n \n         if image_to_overwrite.sum() != image_features.shape[:-1].numel():\n             raise ValueError("
        }
    ],
    "stats": {
        "total": 21,
        "additions": 18,
        "deletions": 3
    }
}