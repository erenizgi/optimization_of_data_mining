{
    "author": "saswatmeher",
    "message": "chore: update SigLIP2 model card (#37624)\n\n* update siglip2 model card\n\n* Update docs/source/en/model_doc/siglip2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* address comments\n\n* separate naflex and fixres variant\n\n* Update docs/source/en/model_doc/siglip2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n* Update docs/source/en/model_doc/siglip2.md\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Steven Liu <59462357+stevhliu@users.noreply.github.com>",
    "sha": "57c620bf8ad2d8eb2210ce3fa0faacbb9b5e579f",
    "files": [
        {
            "sha": "830258f2fc5c61f8555a937aec7718659fa0594d",
            "filename": "docs/source/en/model_doc/siglip2.md",
            "status": "modified",
            "additions": 110,
            "deletions": 175,
            "changes": 285,
            "blob_url": "https://github.com/huggingface/transformers/blob/57c620bf8ad2d8eb2210ce3fa0faacbb9b5e579f/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/57c620bf8ad2d8eb2210ce3fa0faacbb9b5e579f/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodel_doc%2Fsiglip2.md?ref=57c620bf8ad2d8eb2210ce3fa0faacbb9b5e579f",
            "patch": "@@ -14,225 +14,160 @@ rendered properly in your Markdown viewer.\n \n -->\n \n-# SigLIP2\n-\n-<div class=\"flex flex-wrap space-x-1\">\n-<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n-<img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n-<img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+<div style=\"float: right;\">\n+    <div class=\"flex flex-wrap space-x-1\">\n+            <img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+            <img alt=\"FlashAttention\" src=\"https://img.shields.io/badge/%E2%9A%A1%EF%B8%8E%20FlashAttention-eae0c8?style=flat\">\n+            <img alt=\"SDPA\" src=\"https://img.shields.io/badge/SDPA-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+    </div>\n </div>\n \n-## Overview\n-\n-The SigLIP2 model was proposed in [SigLIP 2: Multilingual Vision-Language Encoders with Improved Semantic Understanding, Localization, and Dense Features](https://huggingface.co/papers/2502.14786) by Michael Tschannen, Alexey Gritsenko, Xiao Wang, Muhammad Ferjad Naeem, Ibrahim Alabdulmohsin,\n-Nikhil Parthasarathy, Talfan Evans, Lucas Beyer, Ye Xia, Basil Mustafa, Olivier HÃ©naff, Jeremiah Harmsen,\n-Andreas Steiner and Xiaohua Zhai.\n-\n-The model comes in two variants\n-\n- 1) FixRes - model works with fixed resolution images (backward compatible with SigLIP v1)\n- 2) NaFlex - model works with variable image aspect ratios and resolutions (SigLIP2 in `transformers`)\n-\n-The abstract from the paper is the following:\n-\n-*We introduce SigLIP 2, a family of new multilingual vision-language encoders that build on the success\n-of the original SigLIP. In this second iteration, we extend the original image-text training objective with\n-several prior, independently developed techniques into a unified recipeâ€”this includes decoder-based\n-pretraining, self-supervised losses (self-distillation, masked prediction) and online data curation. With\n-these changes, SigLIP 2 models outperform their SigLIP counterparts at all model scales in core capabilities, \n-including zero-shot classification (best SigLIP 2 ViT-g/16 achieves 85.0% ImageNet zero-shot\n-accuracy), image-text retrieval, and transfer performance when extracting visual representations for\n-Vision-Language Models (VLMs). Furthermore, the new training recipe leads to significant improvements \n-on localization and dense prediction tasks. We also train variants which support multiple resolutions \n-and preserve the inputâ€™s native aspect ratio. Finally, we train on a more diverse data-mixture that\n-includes de-biasing techniques, leading to much better multilingual understanding and improved fair-\n-ness. To provide users with the ability to trade-off inference cost with performance, we release model\n-checkpoints at four sizes (ViT-B/86M, L/303M, So400m/400M, and g/1B).*\n-\n-## Usage tips\n-\n-- Usage of SigLIP2 is similar to [SigLIP](siglip) and [CLIP](clip). The main difference from CLIP is the training loss, which does not require a global view of all the pairwise similarities of images and texts within a batch. One needs to apply the sigmoid activation function to the logits, rather than the softmax.\n-- Training is supported but does not use `torch.distributed` utilities which may limit the scalability of batch size. However, DDP and FDSP works on single-node multi-gpu setup.\n-- When using the standalone [`GemmaTokenizerFast`] make sure to pass `padding=\"max_length\"` and `max_length=64` as that's how the model was trained.\n-- Model was trained with *lowercased* text, make sure you make the same preprocessing for your text labels.\n-- To get the same results as the pipeline, a prompt template of \"this is a photo of {label}\" should be used.\n-- The NaFlex variant supports processing images at higher resolutions by adjusting the `max_num_patches` parameter in the `Processor`. The default value is `max_num_patches=256`. Increasing `max_num_patches` to 1024 (4x) will approximately double processed image height and width, while preserving the aspect ratio.\n+# SigLIP2\n \n-<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/siglip2_metrics_table.png\"\n-alt=\"drawing\" width=\"600\"/>\n+## Overview\n \n-This model was contributed by [qubvel](https://huggingface.co/qubvel-hf).\n-The original code can be found [here](https://github.com/google-research/big_vision/tree/main).\n+[SigLIP2](https://huggingface.co/papers/2502.14786) is a family of multilingual vision-language encoders that builds on the [SigLIP](./siglip) training recipe. It includes decoder-based pretraining, self-distillation, and masked prediction to improve dense prediction tasks (segmentation, depth estimation, etc.). This model is available in two variants:\n \n-## Usage example\n+- NaFlex supports different resolutions and maintains the native image aspect ratio\n+- FixRes supports fixed resolutions and is backwards compatible with [SigLIP](./siglip)\n \n-There are 2 main ways to use SigLIP2: either using the pipeline API, which abstracts away all the complexity for you, or by using the `Siglip2Model` class yourself.\n \n-### FixRes variant\n+You can find all the original SigLIP2 checkpoints under the [SigLIP2](https://huggingface.co/collections/google/siglip2-67b5dcef38c175486e240107) collection.\n \n-**Pipeline API**\n+> [!TIP]\n+> Click on the SigLIP2 models in the right sidebar for more examples of how to apply SigLIP2 to different image and text tasks.\n \n-The pipeline allows to use the model in a few lines of code:\n+The example below demonstrates zero-shot classification with [`Pipeline`] or the [`AutoModel`] class.\n \n-```python\n->>> from transformers import pipeline\n->>> from PIL import Image\n->>> import requests\n+<hfoptions id=\"usage\">\n+<hfoption id=\"Pipeline\">\n \n->>> # load pipe\n->>> image_classifier = pipeline(\n-...     task=\"zero-shot-image-classification\",\n-...     model=\"google/siglip2-base-patch16-224\",\n-... )\n+```py\n+import torch\n+from transformers import pipeline\n \n->>> # load image\n->>> url = 'http://images.cocodataset.org/val2017/000000039769.jpg'\n->>> image = Image.open(requests.get(url, stream=True).raw)\n+image = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+candidate_labels = [\"a Pallas cat\", \"a lion\", \"a Siberian tiger\"]\n \n->>> # inference\n->>> candidate_labels = [\"2 cats\", \"a plane\", \"a remote\"]\n->>> outputs = image_classifier(image, candidate_labels=candidate_labels)\n->>> outputs = [{\"score\": round(output[\"score\"], 4), \"label\": output[\"label\"] } for output in outputs]\n->>> print(outputs)\n-[{'score': 0.1499, 'label': '2 cats'}, {'score': 0.0008, 'label': 'a remote'}, {'score': 0.0, 'label': 'a plane'}]\n+pipeline = pipeline(task=\"zero-shot-image-classification\", model=\"google/siglip2-base-patch16-224\", device=0, torch_dtype=torch.bfloat16)\n+pipeline(image, candidate_labels=candidate_labels)\n ```\n \n-**Using the model yourself**\n-\n-If you want to do the pre- and postprocessing yourself, here's how to do that:\n+</hfoption>\n+<hfoption id=\"AutoModel (FixRes)\">\n \n-```python\n->>> from PIL import Image\n->>> import requests\n->>> from transformers import AutoProcessor, AutoModel\n->>> import torch\n+```py\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, AutoModel\n \n->>> model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-224\")\n->>> processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n+model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-224\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n \n->>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n->>> image = Image.open(requests.get(url, stream=True).raw)\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+candidate_labels = [\"a Pallas cat\", \"a lion\", \"a Siberian tiger\"]\n \n->>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n # follows the pipeline prompt template to get same results\n->>> texts = [f\"This is a photo of {label}.\" for label in candidate_labels]\n+texts = [f'This is a photo of {label}.' for label in candidate_labels]\n \n # IMPORTANT: we pass `padding=max_length` and `max_length=64` since the model was trained with this\n->>> inputs = processor(text=texts, images=image, padding=\"max_length\", max_length=64, return_tensors=\"pt\")\n+inputs = processor(text=texts, images=image, padding=\"max_length\", max_length=64, return_tensors=\"pt\").to(\"cuda\")\n \n->>> with torch.no_grad():\n-...     outputs = model(**inputs)\n+with torch.no_grad():\n+    outputs = model(**inputs)\n \n->>> logits_per_image = outputs.logits_per_image\n->>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n->>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n-15.0% that image 0 is '2 cats'\n+logits_per_image = outputs.logits_per_image\n+probs = torch.sigmoid(logits_per_image)\n+print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n ```\n \n-### NaFlex variant\n-\n-NaFlex combines ideas from FlexiViT, i.e. supporting multiple, predefined sequence lengths \n-with a single ViT model, and NaViT, namely processing images at their native aspect ratio.\n-This enables processing different types of images at appropriate resolution, e.g. using a\n-larger resolution to process document images, while at the same time minimizing the impact \n-of aspect ratio distortion on certain inference tasks, e.g. on OCR.\n-\n-Given a patch size and target sequence length, NaFlex preprocesses the data by first resizing \n-the input image such that the height and width after resizing are multiples of the patch size,\n-while \n-    \n-    1. keeping the aspect ratio distortion as small as possible\n-    2. producing a sequence length of at most the desired target sequence length (`max_num_patches`)\n-    \n-The resulting distortion in width and height is at most `(patch_size - 1) / width` and\n-`(patch_size - 1) / height`, respectively, which tends to be small for common resolutions and aspect ratios. \n-After resizing, the image is split into a sequence of patches, and a mask with padding information is added.\n-\n-```python\n->>> from PIL import Image\n->>> import requests\n->>> from transformers import AutoProcessor, AutoModel\n->>> import torch\n-\n->>> model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-naflex\")\n->>> processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-naflex\")\n-\n->>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n->>> image = Image.open(requests.get(url, stream=True).raw)\n-\n->>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n-# follows the pipeline prompt template to get same results\n->>> texts = [f\"This is a photo of {label}.\" for label in candidate_labels]\n-\n-# default value for `max_num_patches` is 256, but you can increase resulted image resolution providing\n-# higher values e.g. `max_num_patches=512`\n->>> inputs = processor(text=texts, images=image, max_num_patches=256, return_tensors=\"pt\")\n+</hfoption>\n+<hfoption id=\"AutoModel (NaFlex)\">\n \n->>> with torch.no_grad():\n-...     outputs = model(**inputs)\n+```py\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, AutoModel\n \n->>> logits_per_image = outputs.logits_per_image\n->>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n->>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n-21.1% that image 0 is '2 cats'\n-```\n+model = AutoModel.from_pretrained(\"google/siglip2-base-patch16-naflex\", torch_dtype=torch.float16, device_map=\"auto\", attn_implementation=\"sdpa\")\n+processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-naflex\")\n \n-## Resources\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+candidate_labels = [\"a Pallas cat\", \"a lion\", \"a Siberian tiger\"]\n+texts = [f'This is a photo of {label}.' for label in candidate_labels]\n \n-A list of official Hugging Face and community (indicated by ðŸŒŽ) resources to help you get started with SigLIP2.\n+# default value for `max_num_patches` is 256, but you can increase resulted image resolution providing higher values e.g. `max_num_patches=512`\n+inputs = processor(text=texts, images=image, padding=\"max_length\", max_num_patches=256, return_tensors=\"pt\").to(\"cuda\")\n \n-- [Zero-shot image classification task guide](../tasks/zero_shot_image_classification)\n-- Demo notebook for SigLIP2 can be found [here](https://github.com/qubvel/transformers-notebooks/tree/master/notebooks/SigLIP2_inference.ipynb). ðŸŒŽ\n+with torch.no_grad():\n+    outputs = model(**inputs)\n \n-If you're interested in submitting a resource to be included here, please feel free to open a Pull Request and we'll review it! The resource should ideally demonstrate something new instead of duplicating an existing resource.\n+logits_per_image = outputs.logits_per_image\n+probs = torch.sigmoid(logits_per_image)\n+print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n+```\n \n+</hfoption>\n+</hfoptions>\n \n-## Combining SigLIP2 and Flash Attention 2\n+Quantization reduces the memory burden of large models by representing the weights in a lower precision. Refer to the [Quantization](../quantization/overview) overview for more available quantization backends.\n \n-First, make sure to install the latest version of Flash Attention 2.\n+The example below uses [bitsandbytes](../quantization/bitsandbytes) to only quantize the weights to int4.\n \n-```bash\n-pip install -U flash-attn --no-build-isolation\n-```\n+```py\n+import torch\n+import requests\n+from PIL import Image\n+from transformers import AutoProcessor, AutoModel, BitsAndBytesConfig\n \n-Make also sure that you have a hardware that is compatible with Flash-Attention 2. Read more about it in the official documentation of flash-attn repository. Make also sure to load your model in half-precision (e.g. `torch.float16``)\n+bnb_config = BitsAndBytesConfig(load_in_4bit=True)\n+model = AutoModel.from_pretrained(\"google/siglip2-large-patch16-512\", quantization_config=bnb_config, device_map=\"auto\", attn_implementation=\"sdpa\")\n+processor = AutoProcessor.from_pretrained(\"google/siglip2-base-patch16-224\")\n \n-To load and run a model using Flash Attention 2, refer to the snippet below:\n+url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n+image = Image.open(requests.get(url, stream=True).raw)\n+candidate_labels = [\"a Pallas cat\", \"a lion\", \"a Siberian tiger\"]\n \n-```python\n->>> import torch\n->>> import requests\n->>> from PIL import Image\n->>> from transformers import AutoProcessor, AutoModel\n->>> device = \"cuda\" # the device to load the model onto\n+# follows the pipeline prompt template to get same results\n+texts = [f'This is a photo of {label}.' for label in candidate_labels]\n \n->>> model = AutoModel.from_pretrained(\n-...     \"google/siglip2-so400m-patch14-384\",\n-...     attn_implementation=\"flash_attention_2\",\n-...     torch_dtype=torch.float16,\n-...     device_map=device,\n-... )\n->>> processor = AutoProcessor.from_pretrained(\"google/siglip2-so400m-patch14-384\")\n+# IMPORTANT: we pass `padding=max_length` and `max_length=64` since the model was trained with this\n+inputs = processor(text=texts, images=image, padding=\"max_length\", max_length=64, return_tensors=\"pt\").to(\"cuda\")\n \n->>> url = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n->>> image = Image.open(requests.get(url, stream=True).raw)\n+with torch.no_grad():\n+    outputs = model(**inputs)\n \n->>> candidate_labels = [\"2 cats\", \"2 dogs\"]\n-# follows the pipeline prompt template to get same results\n->>> texts = [f'This is a photo of {label}.' for label in candidate_labels]\n-# important: we pass `padding=max_length` since the model was trained with this\n->>> inputs = processor(text=texts, images=image, padding=\"max_length\", return_tensors=\"pt\").to(device)\n-\n->>> with torch.no_grad():\n-...     with torch.autocast(device):\n-...         outputs = model(**inputs)\n-\n->>> logits_per_image = outputs.logits_per_image\n->>> probs = torch.sigmoid(logits_per_image) # these are the probabilities\n->>> print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n-19.8% that image 0 is '2 cats'\n+logits_per_image = outputs.logits_per_image\n+probs = torch.sigmoid(logits_per_image)\n+print(f\"{probs[0][0]:.1%} that image 0 is '{candidate_labels[0]}'\")\n ```\n \n+## Notes\n+\n+- Training is supported for DDP and FSDP on single-node multi-GPU setups. However, it does not use [torch.distributed](https://pytorch.org/tutorials/beginner/dist_overview.html) utilities which may limit the scalability of batch size.\n+- When using the standalone [`GemmaTokenizerFast`] make sure to pass `padding=\"max_length\"` and `max_length=64` as that's how the model was trained.\n+- Model was trained with *lowercased* text, so make sure your text labels are preprocessed the same way.\n+- To get the same results as the [`Pipeline`], a prompt template of `\"This is a photo of {label}.\"` should be passed to the processor.\n+- The NaFlex variant processes different types of images at the appropriate resolution (using a larger resolution to process document images for example), while also minimizing the impact of aspect ratio distortion for certain inference tasks like OCR.\n+\n+   NaFlex resizes the input image so the height and width are multiples of the patch size after resizing. It keeps the aspect ratio distortion as low as possible and produces a sequence length of at most the desired target sequence length (`max_num_patches`). After resizing, the image is split into a sequence of patches and a mask with padding information is added.\n+- Toggle the `attn_implementation` parameter to either `\"sdpa\"` or `\"flash_attention_2\"` to use a more memory-efficient attention.\n+    ```py\n+    # pip install -U flash-attn --no-build-isolation\n+\n+    from transformers import SiglipModel\n+\n+    model = SiglipModel.from_pretrained(\n+        \"google/siglip2-so400m-patch14-384\",\n+        attn_implementation=\"flash_attention_2\",\n+        torch_dtype=torch.float16,\n+        device_map=device,\n+    )\n+    ```\n ## Siglip2Config\n \n [[autodoc]] Siglip2Config"
        }
    ],
    "stats": {
        "total": 285,
        "additions": 110,
        "deletions": 175
    }
}