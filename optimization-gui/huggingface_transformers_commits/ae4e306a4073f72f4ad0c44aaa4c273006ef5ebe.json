{
    "author": "cyyever",
    "message": "Defaults to adamw_torch_fused for  Pytorch>=2.8 (#37358)\n\n* Defaults to adamw_torch_fused for latest Pytorch\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n* Fix test\n\nSigned-off-by: cyy <cyyever@outlook.com>\n\n---------\n\nSigned-off-by: cyy <cyyever@outlook.com>",
    "sha": "ae4e306a4073f72f4ad0c44aaa4c273006ef5ebe",
    "files": [
        {
            "sha": "c3cc4579e5c65fd5b89d0bee8f49b2679e0f1cf8",
            "filename": "src/transformers/pytorch_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae4e306a4073f72f4ad0c44aaa4c273006ef5ebe/src%2Ftransformers%2Fpytorch_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae4e306a4073f72f4ad0c44aaa4c273006ef5ebe/src%2Ftransformers%2Fpytorch_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpytorch_utils.py?ref=ae4e306a4073f72f4ad0c44aaa4c273006ef5ebe",
            "patch": "@@ -28,6 +28,7 @@\n \n logger = logging.get_logger(__name__)\n \n+is_torch_greater_or_equal_than_2_8 = is_torch_greater_or_equal(\"2.8\", accept_dev=True)\n is_torch_greater_or_equal_than_2_6 = is_torch_greater_or_equal(\"2.6\", accept_dev=True)\n is_torch_greater_or_equal_than_2_4 = is_torch_greater_or_equal(\"2.4\", accept_dev=True)\n is_torch_greater_or_equal_than_2_3 = is_torch_greater_or_equal(\"2.3\", accept_dev=True)"
        },
        {
            "sha": "4ae3983c131a58dd18faaaa946a5281fcf8bbdcd",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae4e306a4073f72f4ad0c44aaa4c273006ef5ebe/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae4e306a4073f72f4ad0c44aaa4c273006ef5ebe/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=ae4e306a4073f72f4ad0c44aaa4c273006ef5ebe",
            "patch": "@@ -609,7 +609,7 @@ class TrainingArguments:\n             - `\"tpu_metrics_debug\"`: print debug metrics on TPU\n \n             The options should be separated by whitespaces.\n-        optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"`):\n+        optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch\"` (for torch>=2.8 `\"adamw_torch_fused\"`)):\n             The optimizer to use, such as \"adamw_torch\", \"adamw_torch_fused\", \"adamw_apex_fused\", \"adamw_anyprecision\",\n             \"adafactor\". See `OptimizerNames` in [training_args.py](https://github.com/huggingface/transformers/blob/main/src/transformers/training_args.py)\n             for a full list of optimizers.\n@@ -1280,11 +1280,11 @@ class TrainingArguments:\n     )\n \n     default_optim = \"adamw_torch\"\n-    # XXX: enable when pytorch==2.0.1 comes out - we want to give it time to get all the bugs sorted out\n-    # if is_torch_available():\n-    #     default_optim = \"adamw_torch_fused\"\n-    # and update the doc above to:\n-    # optim (`str` or [`training_args.OptimizerNames`], *optional*, defaults to `\"adamw_torch_fused\"` (for torch<2.1.0 `\"adamw_torch\"`):\n+    if is_torch_available():\n+        from .pytorch_utils import is_torch_greater_or_equal_than_2_8\n+\n+        if is_torch_greater_or_equal_than_2_8:\n+            default_optim = \"adamw_torch_fused\"\n     optim: Union[OptimizerNames, str] = field(\n         default=default_optim,\n         metadata={\"help\": \"The optimizer to use.\"},"
        },
        {
            "sha": "36683777b4333c52759d574f121a47fc48213180",
            "filename": "tests/models/git/test_modeling_git.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/ae4e306a4073f72f4ad0c44aaa4c273006ef5ebe/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/ae4e306a4073f72f4ad0c44aaa4c273006ef5ebe/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgit%2Ftest_modeling_git.py?ref=ae4e306a4073f72f4ad0c44aaa4c273006ef5ebe",
            "patch": "@@ -347,7 +347,8 @@ def _test_beam_search_generate(self, config, input_ids, input_mask, pixel_values\n             num_return_sequences=2,\n         )\n \n-        self.parent.assertEqual(generated_ids.shape, (self.batch_size * 2, 20))\n+        self.parent.assertEqual(generated_ids.shape[0], self.batch_size * 2)\n+        self.parent.assertTrue(generated_ids.shape[1] < 20)\n \n     def _test_batched_generate_captioning(self, config, input_ids, input_mask, pixel_values):\n         model = GitForCausalLM(config=config)"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 9,
        "deletions": 7
    }
}