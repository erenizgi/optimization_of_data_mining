{
    "author": "jiqing-feng",
    "message": "fix pegasus init weights and other copied models (#36844)\n\n* fix pegasus init weights\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix the rest of models\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix test\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix informer init\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* init weight before checking\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix roformer tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n* fix roformer tests\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>\n\n---------\n\nSigned-off-by: jiqing-feng <jiqing.feng@intel.com>",
    "sha": "0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9",
    "files": [
        {
            "sha": "1697ae3c5c46083701cc3d9053aafede1547d935",
            "filename": "src/transformers/models/autoformer/modeling_autoformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fautoformer%2Fmodeling_autoformer.py?ref=0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9",
            "patch": "@@ -360,7 +360,6 @@ class AutoformerSinusoidalPositionalEmbedding(nn.Embedding):\n \n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n         super().__init__(num_positions, embedding_dim)\n-        self.weight = self._init_weight(self.weight)\n \n     @staticmethod\n     def _init_weight(out: nn.Parameter) -> nn.Parameter:\n@@ -904,7 +903,7 @@ def _init_weights(self, module):\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, AutoformerSinusoidalPositionalEmbedding):\n-            pass\n+            module.weight = module._init_weight(module.weight)\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:"
        },
        {
            "sha": "b35863251f11f228da65fd408af6cae3a67eccf0",
            "filename": "src/transformers/models/informer/modeling_informer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodeling_informer.py?ref=0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9",
            "patch": "@@ -233,7 +233,6 @@ class InformerSinusoidalPositionalEmbedding(nn.Embedding):\n \n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n         super().__init__(num_positions, embedding_dim)\n-        self.weight = self._init_weight(self.weight)\n \n     @staticmethod\n     def _init_weight(out: nn.Parameter) -> nn.Parameter:\n@@ -887,7 +886,9 @@ def _init_weights(self, module):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.bias is not None:\n                 module.bias.data.zero_()\n-        elif isinstance(module, nn.Embedding) and not isinstance(module, InformerSinusoidalPositionalEmbedding):\n+        elif isinstance(module, InformerSinusoidalPositionalEmbedding):\n+            module.weight = module._init_weight(module.weight)\n+        elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n                 module.weight.data[module.padding_idx].zero_()"
        },
        {
            "sha": "b71c6464485f9e2e90687681e187e66f6d80f064",
            "filename": "src/transformers/models/marian/modeling_marian.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmarian%2Fmodeling_marian.py?ref=0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9",
            "patch": "@@ -73,7 +73,6 @@ class MarianSinusoidalPositionalEmbedding(nn.Embedding):\n \n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n         super().__init__(num_positions, embedding_dim)\n-        self.weight = self._init_weight(self.weight)\n \n     @staticmethod\n     def _init_weight(out: nn.Parameter) -> nn.Parameter:\n@@ -468,7 +467,7 @@ def _init_weights(self, module: Union[nn.Linear, nn.Embedding, MarianSinusoidalP\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, MarianSinusoidalPositionalEmbedding):\n-            pass\n+            module.weight = module._init_weight(module.weight)\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:"
        },
        {
            "sha": "fe7008f1612f0bd82ee59cd6f59570eaba2efd9f",
            "filename": "src/transformers/models/pegasus/modeling_pegasus.py",
            "status": "modified",
            "additions": 3,
            "deletions": 2,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpegasus%2Fmodeling_pegasus.py?ref=0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9",
            "patch": "@@ -74,7 +74,6 @@ class PegasusSinusoidalPositionalEmbedding(nn.Embedding):\n \n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n         super().__init__(num_positions, embedding_dim)\n-        self.weight = self._init_weight(self.weight)\n \n     @staticmethod\n     def _init_weight(out: nn.Parameter) -> nn.Parameter:\n@@ -469,7 +468,7 @@ def _init_weights(self, module):\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, PegasusSinusoidalPositionalEmbedding):\n-            pass\n+            module.weight = module._init_weight(module.weight)\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:\n@@ -665,6 +664,7 @@ def resize_position_embeddings(self, new_num_position_embeddings: int):\n             self.config.d_model,\n             self.padding_idx,\n         )\n+        self.embed_positions.weight = self.embed_positions._init_weight(self.embed_positions.weight)\n         self.embed_positions.to(self.device)\n \n     def get_position_embeddings(self) -> nn.Embedding:\n@@ -868,6 +868,7 @@ def resize_position_embeddings(self, new_num_position_embeddings: int):\n             self.config.d_model,\n             self.padding_idx,\n         )\n+        self.embed_positions.weight = self.embed_positions._init_weight(self.embed_positions.weight)\n         self.embed_positions.to(self.device)\n \n     def get_position_embeddings(self) -> nn.Embedding:"
        },
        {
            "sha": "437a149d605774693cbf217014c77f6d0331e797",
            "filename": "src/transformers/models/roformer/modeling_roformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Froformer%2Fmodeling_roformer.py?ref=0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9",
            "patch": "@@ -59,7 +59,6 @@ class RoFormerSinusoidalPositionalEmbedding(nn.Embedding):\n \n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n         super().__init__(num_positions, embedding_dim)\n-        self.weight = self._init_weight(self.weight)\n \n     @staticmethod\n     def _init_weight(out: nn.Parameter) -> nn.Parameter:\n@@ -694,7 +693,7 @@ def _init_weights(self, module):\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, RoFormerSinusoidalPositionalEmbedding):\n-            pass\n+            module.weight = module._init_weight(module.weight)\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n             if module.padding_idx is not None:"
        },
        {
            "sha": "ba4a376e80fd286d17bc9faaf7b2f4cf82125c2d",
            "filename": "src/transformers/models/time_series_transformer/modeling_time_series_transformer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ftime_series_transformer%2Fmodeling_time_series_transformer.py?ref=0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9",
            "patch": "@@ -233,7 +233,6 @@ class TimeSeriesSinusoidalPositionalEmbedding(nn.Embedding):\n \n     def __init__(self, num_positions: int, embedding_dim: int, padding_idx: Optional[int] = None) -> None:\n         super().__init__(num_positions, embedding_dim)\n-        self.weight = self._init_weight(self.weight)\n \n     @staticmethod\n     def _init_weight(out: nn.Parameter) -> nn.Parameter:\n@@ -641,7 +640,7 @@ def _init_weights(self, module):\n             if module.bias is not None:\n                 module.bias.data.zero_()\n         elif isinstance(module, TimeSeriesSinusoidalPositionalEmbedding):\n-            pass\n+            module.weight = module._init_weight(module.weight)\n         elif isinstance(module, nn.Embedding):\n             module.weight.data.normal_(mean=0.0, std=std)\n             if module.padding_idx is not None:"
        },
        {
            "sha": "fb91a652004b2e1c3e64da113180a3966bcf80f9",
            "filename": "tests/models/informer/test_modeling_informer.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finformer%2Ftest_modeling_informer.py?ref=0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9",
            "patch": "@@ -171,6 +171,7 @@ def check_encoder_decoder_model_standalone(self, config, inputs_dict):\n         embed_positions = InformerSinusoidalPositionalEmbedding(\n             config.context_length + config.prediction_length, config.d_model\n         ).to(torch_device)\n+        embed_positions.weight = embed_positions._init_weight(embed_positions.weight)\n         self.parent.assertTrue(torch.equal(model.encoder.embed_positions.weight, embed_positions.weight))\n         self.parent.assertTrue(torch.equal(model.decoder.embed_positions.weight, embed_positions.weight))\n "
        },
        {
            "sha": "83ef12be4fed320e2a8b8018256876911163a84f",
            "filename": "tests/models/pegasus/test_modeling_pegasus.py",
            "status": "modified",
            "additions": 13,
            "deletions": 0,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpegasus%2Ftest_modeling_pegasus.py?ref=0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9",
            "patch": "@@ -348,6 +348,19 @@ class PegasusXSUMIntegrationTest(AbstractSeq2SeqIntegrationTest):\n     def model(self):\n         return AutoModelForSeq2SeqLM.from_pretrained(self.checkpoint_name).to(torch_device)\n \n+    @slow\n+    def test_device_map(self):\n+        model_no_device_map = AutoModelForSeq2SeqLM.from_pretrained(self.checkpoint_name).to(torch_device)\n+        model_with_device_map = AutoModelForSeq2SeqLM.from_pretrained(self.checkpoint_name, device_map=\"auto\")\n+        assert torch.equal(\n+            model_no_device_map.model.decoder.embed_positions.weight,\n+            model_with_device_map.model.decoder.embed_positions.weight,\n+        )\n+        assert torch.equal(\n+            model_no_device_map.model.encoder.embed_positions.weight,\n+            model_with_device_map.model.encoder.embed_positions.weight,\n+        )\n+\n     @slow\n     @require_torch_fp16\n     def test_pegasus_xsum_summary(self):"
        },
        {
            "sha": "fbcc2361289bf6118c29fc2ebfd184de44d8bab7",
            "filename": "tests/models/roformer/test_modeling_roformer.py",
            "status": "modified",
            "additions": 3,
            "deletions": 0,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Froformer%2Ftest_modeling_roformer.py?ref=0e56fb69a23ad2bb9b56583d1ba130aa37c5b8e9",
            "patch": "@@ -534,6 +534,7 @@ class RoFormerSinusoidalPositionalEmbeddingTest(unittest.TestCase):\n     def test_basic(self):\n         input_ids = torch.tensor([[4, 10]], dtype=torch.long, device=torch_device)\n         emb1 = RoFormerSinusoidalPositionalEmbedding(num_positions=6, embedding_dim=6).to(torch_device)\n+        emb1.weight = emb1._init_weight(emb1.weight)\n         emb = emb1(input_ids.shape)\n         desired_weights = torch.tensor(\n             [[0.0000, 0.0000, 0.0000, 1.0000, 1.0000, 1.0000], [0.8415, 0.0464, 0.0022, 0.5403, 0.9989, 1.0000]]\n@@ -552,6 +553,7 @@ def test_positional_emb_weights_against_roformer(self):\n             ]\n         ).to(torch_device)\n         emb1 = RoFormerSinusoidalPositionalEmbedding(num_positions=512, embedding_dim=512).to(torch_device)\n+        emb1.weight = emb1._init_weight(emb1.weight)\n         weights = emb1.weight.data[:3, :5].to(torch_device)\n \n         self.assertTrue(\n@@ -573,6 +575,7 @@ def test_apply_rotary_position_embeddings(self):\n             -torch.arange(2 * 12 * 16 * 64, dtype=torch.float, device=torch_device).reshape(2, 12, 16, 64) / 100\n         ).to(torch_device)\n         embed_positions = RoFormerSinusoidalPositionalEmbedding(num_positions=32, embedding_dim=64).to(torch_device)\n+        embed_positions.weight = embed_positions._init_weight(embed_positions.weight)\n         sinusoidal_pos = embed_positions([2, 16, 768])[None, None, :, :]\n \n         query_layer, key_layer = RoFormerSelfAttention.apply_rotary_position_embeddings("
        }
    ],
    "stats": {
        "total": 39,
        "additions": 27,
        "deletions": 12
    }
}