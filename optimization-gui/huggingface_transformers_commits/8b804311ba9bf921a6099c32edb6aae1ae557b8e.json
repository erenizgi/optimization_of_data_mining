{
    "author": "Cyrilvallez",
    "message": "[modular] Remove ambiguity in all calls to parent class methods + fix dependency graph (#40456)\n\n* fix in modular\n\n* remove leftover print\n\n* fix everything except when it's in assignment\n\n* fix assignment as well\n\n* more general\n\n* better\n\n* better\n\n* better comment\n\n* docstring\n\n* cleaner\n\n* remove base\n\n* doc",
    "sha": "8b804311ba9bf921a6099c32edb6aae1ae557b8e",
    "files": [
        {
            "sha": "39d29f8a6cd46ffd7f044832a454ab5a18f49082",
            "filename": "docs/source/en/modular_transformers.md",
            "status": "modified",
            "additions": 10,
            "deletions": 16,
            "changes": 26,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/docs%2Fsource%2Fen%2Fmodular_transformers.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fmodular_transformers.md?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -21,10 +21,10 @@ Model users still import and use the single-file interface they've grown familia\n \n A linter \"unravels\" the modular file into a `modeling.py` file to preserve the single model, single file directory structure (modeling, processor, etc.). Inheritance is flattened to only a **single** level.\n \n-Run the command below to automatically generate a `modeling.py` file from a modular file.\n+Run the command below to automatically generate a `modeling.py` file from a modular file (assuming the snake lowercase name of the model you want to convert is `your_model`).\n \n ```bash\n-python utils/modular_model_converter.py --files-to-parse src/transformers/models/<your_model>/modular_<your_model>.py\n+python utils/modular_model_converter.py  your_model\n ```\n \n For example:\n@@ -35,12 +35,6 @@ For example:\n \n You should be able to write everything (tokenizer, image processor, model, config, etc.) in a modular and their corresponding single-files are generated.\n \n-Run the command below to ensure the generated content matches `modular_<your_model>.py`.\n-\n-```bash\n-python utils/check_modular_conversion.py --files src/transformers/models/<your_model>/modular_<your_model>.py\n-```\n-\n The example below demonstrates how a model can be added with significantly fewer lines of code with Modular Transformers.\n \n ### BERT and RoBERTa\n@@ -412,17 +406,17 @@ class MyNewDummyModel(DummyModel):\n     del self.attribute\n ```\n \n-## Explicit super() calls\n+## Calling parent methods without unravelling their definition\n \n-If you still want to inherit from `DummyModel` but don't want to remove the `self.attribute`, be explicit about which class' `super()` you're calling. The example below shows how to call the `super()` of `nn.Module` (unraveled code shown on the right)\n+If you want to inherit from a module `DummyModule` and want to call `super()` WITHOUT unravelling the parent's code (that is, you want to call `super()` on the *generated* class parent), be explicit about which class' `super()` you're calling. The example below shows how to call the `super()` of `nn.Module` (unraveled code shown on the right). In this example, as `DummyModule` is itself a `nn.Module`, it makes sense to call `nn.Module.__init__(self)` as it's what was the initial intention. It's then unravelled as `super()` in `MyNewDummyModule` to follow Python's best-practices.\n \n ```py\n-class MyNewDummyModel(DummyModel, nn.Module):        |     class MyNewDummyModel(nn.Module):\n-                                                     |\n-  def __init__(self, config: MyNewDummyConfig):      |       def __init__(self, config: MyNewDummyConfig):\n-    nn.Module.__init__(config)                       |         super().__init__()\n-    self.foo = config.foo                            |         self.foo = config.foo\n-    ...                                              |         ...\n+class MyNewDummyModule(DummyModule):                   |     class MyNewDummyModule(nn.Module):\n+                                                       |\n+  def __init__(self):                                  |       def __init__(self):\n+    nn.Module.__init__(self)                           |         super().__init__()\n+    self.foo = config.foo                              |         self.foo = config.foo\n+    ...                                                |         ...\n ```\n \n ## Deleting unused methods"
        },
        {
            "sha": "5991b928a2f0cf3fe684463c9dedf6c34f0bc930",
            "filename": "src/transformers/models/aimv2/modular_aimv2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fmodular_aimv2.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -613,11 +613,11 @@ def forward(\n \n \n @auto_docstring\n-class Aimv2Model(CLIPModel, nn.Module):\n+class Aimv2Model(CLIPModel):\n     _supports_flash_attn = True\n \n     def __init__(self, config: Aimv2Config):\n-        nn.Module().__init__(config)\n+        PreTrainedModel.__init__(self, config)\n \n         self.projection_dim = config.projection_dim\n         self.vision_embed_dim = config.vision_config.hidden_size"
        },
        {
            "sha": "8696095588c9e06fd5c2ca9c8a5ab80cf8a7d288",
            "filename": "src/transformers/models/aria/modular_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodular_aria.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -1292,7 +1292,7 @@ class AriaPreTrainedModel(LlamaPreTrainedModel):\n     _supports_attention_backend = True\n \n     def _init_weights(self, module):\n-        LlamaPreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, AriaProjector):\n             nn.init.trunc_normal_(module.query, std=self.config.initializer_range)\n "
        },
        {
            "sha": "530caef7d97302b61182f7f360564185637d3529",
            "filename": "src/transformers/models/colqwen2/modular_colqwen2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 5,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcolqwen2%2Fmodular_colqwen2.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -16,15 +16,14 @@\n from dataclasses import dataclass\n from typing import Optional, Union\n \n-from transformers.models.colpali.modeling_colpali import ColPaliForRetrieval, ColPaliPreTrainedModel\n-from transformers.models.colpali.processing_colpali import ColPaliProcessor\n-\n from ...cache_utils import Cache\n from ...feature_extraction_utils import BatchFeature\n from ...image_utils import ImageInput, is_valid_image\n-from ...processing_utils import MultiModalData, ProcessingKwargs, Unpack\n+from ...processing_utils import MultiModalData, ProcessingKwargs, ProcessorMixin, Unpack\n from ...tokenization_utils_base import PreTokenizedInput, TextInput\n from ...utils import ModelOutput, auto_docstring, can_return_tuple, is_torch_available, logging\n+from ..colpali.modeling_colpali import ColPaliForRetrieval, ColPaliPreTrainedModel\n+from ..colpali.processing_colpali import ColPaliProcessor\n from .configuration_colqwen2 import ColQwen2Config\n \n \n@@ -79,7 +78,7 @@ def __init__(\n         query_prefix: Optional[str] = None,\n         **kwargs,\n     ):\n-        ColPaliProcessor().__init__(image_processor, tokenizer, chat_template=chat_template)\n+        ProcessorMixin.__init__(self, image_processor, tokenizer, chat_template=chat_template)\n         self.image_token = \"<|image_pad|>\" if not hasattr(tokenizer, \"image_token\") else tokenizer.image_token\n         self.video_token = \"<|video_pad|>\" if not hasattr(tokenizer, \"video_token\") else tokenizer.video_token\n "
        },
        {
            "sha": "7fa565ea21f41c75096d5b361dbc997b84d41b18",
            "filename": "src/transformers/models/d_fine/modeling_d_fine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fd_fine%2Fmodeling_d_fine.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -2022,7 +2022,7 @@ class DFineHybridEncoder(nn.Module):\n     \"\"\"\n \n     def __init__(self, config: DFineConfig):\n-        nn.Module.__init__(self)\n+        super().__init__()\n         self.config = config\n         self.in_channels = config.encoder_in_channels\n         self.num_fpn_stages = len(self.in_channels) - 1"
        },
        {
            "sha": "7f6635400e6a71d3e8bdb38fb4adc520f448ffdb",
            "filename": "src/transformers/models/deepseek_v2/modular_deepseek_v2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v2%2Fmodular_deepseek_v2.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -21,7 +21,7 @@\n from torch import nn\n \n from ...cache_utils import Cache\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...utils import (\n     logging,\n )\n@@ -507,7 +507,7 @@ class DeepseekV2PreTrainedModel(LlamaPreTrainedModel):\n     _can_compile_fullgraph = False\n \n     def _init_weights(self, module):\n-        LlamaPreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DeepseekV2MoEGate):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n "
        },
        {
            "sha": "4730fc6ac37f499b1eccd1fb5948a96b2a6bdc78",
            "filename": "src/transformers/models/deepseek_v3/modular_deepseek_v3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_v3%2Fmodular_deepseek_v3.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -10,7 +10,7 @@\n from ...cache_utils import Cache\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GenericForSequenceClassification\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import logging\n from ...utils.deprecation import deprecate_kwarg\n@@ -324,9 +324,9 @@ def forward(\n         return attn_output, attn_weights\n \n \n-class DeepseekV3DecoderLayer(LlamaDecoderLayer, nn.Module):\n+class DeepseekV3DecoderLayer(LlamaDecoderLayer):\n     def __init__(self, config: DeepseekV3Config, layer_idx: int):\n-        nn.Module().__init__()\n+        nn.Module.__init__(self)\n         self.hidden_size = config.hidden_size\n \n         self.self_attn = DeepseekV3Attention(config=config, layer_idx=layer_idx)\n@@ -344,7 +344,7 @@ class DeepseekV3PreTrainedModel(LlamaPreTrainedModel):\n     _can_compile_fullgraph = False\n \n     def _init_weights(self, module):\n-        LlamaPreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DeepseekV3TopkRouter):\n             module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n "
        },
        {
            "sha": "c0d423809256f733832bca51931d11c0327a9fdb",
            "filename": "src/transformers/models/deepseek_vl_hybrid/modular_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdeepseek_vl_hybrid%2Fmodular_deepseek_vl_hybrid.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -19,6 +19,7 @@\n \n from ...cache_utils import Cache\n from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n     BatchFeature,\n     DefaultFastImageProcessorKwargs,\n     get_size_dict,\n@@ -760,7 +761,7 @@ def __init__(self, **kwargs: Unpack[DeepseekVLHybridFastImageProcessorKwargs]):\n             high_res_background_color = (127, 127, 127)\n         else:\n             high_res_background_color = tuple(int(x * 255) for x in kwargs.get(\"high_res_image_mean\"))\n-        DeepseekVLImageProcessorFast().__init__(**kwargs)\n+        BaseImageProcessorFast.__init__(self, **kwargs)\n         self.background_color = tuple(background_color)\n         self.high_res_background_color = tuple(high_res_background_color)\n "
        },
        {
            "sha": "fc0b7a9172d37b78c384177b29959b56ec0437d2",
            "filename": "src/transformers/models/diffllama/modular_diffllama.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodular_diffllama.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...cache_utils import Cache, StaticCache\n from ...modeling_flash_attention_utils import _flash_attention_forward, flash_attn_supports_top_left_mask\n+from ...modeling_utils import PreTrainedModel\n from ...utils import logging\n from ...utils.deprecation import deprecate_kwarg\n from ..gemma.modeling_gemma import GemmaForCausalLM\n@@ -408,7 +409,7 @@ class DiffLlamaPreTrainedModel(LlamaPreTrainedModel):\n     _supports_attention_backend = False\n \n     def _init_weights(self, module):\n-        LlamaPreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DiffLlamaAttention):\n             module.lambda_q1.data.normal_(0, self.config.lambda_std_dev)\n             module.lambda_k1.data.normal_(0, self.config.lambda_std_dev)"
        },
        {
            "sha": "fdee6e09c465894255eaa8cb81eaf527d68bd64f",
            "filename": "src/transformers/models/doge/modular_doge.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdoge%2Fmodular_doge.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -30,7 +30,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import MoeCausalLMOutputWithPast, MoeModelOutputWithPast\n from ...modeling_rope_utils import rope_config_validation\n-from ...modeling_utils import AttentionInterface\n+from ...modeling_utils import AttentionInterface, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, is_torch_flex_attn_available\n from ...utils.deprecation import deprecate_kwarg\n@@ -576,7 +576,7 @@ class DogePreTrainedModel(LlamaPreTrainedModel):\n \n     def _init_weights(self, module):\n         \"\"\"Initialize the weights\"\"\"\n-        LlamaPreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, DogeAttention):\n             if hasattr(module, \"A\"):\n                 module.A.data.zero_()"
        },
        {
            "sha": "acfa82c4694af9e29d8b168238628be2f5ec5744",
            "filename": "src/transformers/models/dpt/image_processing_dpt_fast.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fimage_processing_dpt_fast.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -24,10 +24,9 @@\n from collections.abc import Iterable\n from typing import TYPE_CHECKING, Optional, Union\n \n-from transformers.image_processing_base import BatchFeature\n-from transformers.image_transforms import group_images_by_shape, reorder_images\n-\n+from ...image_processing_base import BatchFeature\n from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n+from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import (\n     IMAGENET_STANDARD_MEAN,\n     IMAGENET_STANDARD_STD,"
        },
        {
            "sha": "9c74b4c570ae57785068333c0cbdfc61d12b4e50",
            "filename": "src/transformers/models/dpt/modular_dpt.py",
            "status": "modified",
            "additions": 7,
            "deletions": 8,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdpt%2Fmodular_dpt.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -18,13 +18,9 @@\n from collections.abc import Iterable\n from typing import TYPE_CHECKING, Optional, Union\n \n-from transformers.image_processing_base import BatchFeature\n-from transformers.image_transforms import group_images_by_shape, reorder_images\n-from transformers.models.beit.image_processing_beit_fast import BeitImageProcessorFast\n-\n-from ...image_processing_utils_fast import (\n-    DefaultFastImageProcessorKwargs,\n-)\n+from ...image_processing_base import BatchFeature\n+from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n+from ...image_transforms import group_images_by_shape, reorder_images\n from ...image_utils import (\n     IMAGENET_STANDARD_MEAN,\n     IMAGENET_STANDARD_STD,\n@@ -39,6 +35,7 @@\n     is_torchvision_v2_available,\n     requires_backends,\n )\n+from ..beit.image_processing_beit_fast import BeitImageProcessorFast\n \n \n if TYPE_CHECKING:\n@@ -177,7 +174,9 @@ def resize(\n             keep_aspect_ratio=keep_aspect_ratio,\n             multiple=ensure_multiple_of,\n         )\n-        return BeitImageProcessorFast().resize(image, output_size, interpolation=interpolation, antialias=antialias)\n+        return BaseImageProcessorFast.resize(\n+            self, image, output_size, interpolation=interpolation, antialias=antialias\n+        )\n \n     def pad_image(\n         self,"
        },
        {
            "sha": "7d9a6100a08277cd721ee2aecec17ee79d4a6445",
            "filename": "src/transformers/models/eomt/modular_eomt.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Feomt%2Fmodular_eomt.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -249,9 +249,9 @@ class EomtPatchEmbeddings(Dinov2PatchEmbeddings):\n     pass\n \n \n-class EomtEmbeddings(Dinov2Embeddings, nn.Module):\n+class EomtEmbeddings(Dinov2Embeddings):\n     def __init__(self, config: EomtConfig) -> None:\n-        Dinov2Embeddings().__init__()\n+        nn.Module.__init__(self)\n \n         self.config = config\n         self.patch_size = config.patch_size\n@@ -431,9 +431,9 @@ def _init_weights(self, module: nn.Module) -> None:\n     The EoMT Model with head on top for instance/semantic/panoptic segmentation.\n     \"\"\"\n )\n-class EomtForUniversalSegmentation(Mask2FormerForUniversalSegmentation, nn.Module):\n+class EomtForUniversalSegmentation(Mask2FormerForUniversalSegmentation):\n     def __init__(self, config: EomtConfig):\n-        nn.Module().__init__(config)\n+        PreTrainedModel.__init__(self, config)\n         self.config = config\n         self.num_hidden_layers = config.num_hidden_layers\n         self.embeddings = EomtEmbeddings(config)"
        },
        {
            "sha": "ddc63122515338ddb46e3a2d0909ccfbdaf8cc62",
            "filename": "src/transformers/models/ernie4_5_moe/modular_ernie4_5_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 5,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fernie4_5_moe%2Fmodular_ernie4_5_moe.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -22,6 +22,7 @@\n from ...cache_utils import Cache, DynamicCache\n from ...masking_utils import create_causal_mask\n from ...modeling_outputs import MoeModelOutputWithPast\n+from ...modeling_utils import PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.generic import OutputRecorder, check_model_inputs\n@@ -190,9 +191,9 @@ def forward(\n         return final_hidden_states, router_logits\n \n \n-class Ernie4_5_MoeDecoderLayer(Qwen3MoeDecoderLayer, nn.Module):\n+class Ernie4_5_MoeDecoderLayer(Qwen3MoeDecoderLayer):\n     def __init__(self, config, layer_idx):\n-        nn.Module().__init__()\n+        nn.Module.__init__(self)\n         self.hidden_size = config.hidden_size\n \n         self.self_attn = Ernie4_5_MoeAttention(config, layer_idx)\n@@ -224,7 +225,7 @@ class Ernie4_5_MoePreTrainedModel(MixtralPreTrainedModel):\n     }\n \n     def _init_weights(self, module):\n-        MixtralPreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Ernie4_5_MoeStatics):\n             module.e_score_correction_bias.data.zero_()\n \n@@ -312,9 +313,9 @@ def forward(\n \n \n @auto_docstring\n-class Ernie4_5_MoeForCausalLM(MixtralForCausalLM, Ernie4_5_MoePreTrainedModel):\n+class Ernie4_5_MoeForCausalLM(MixtralForCausalLM):\n     def __init__(self, config):\n-        Ernie4_5_MoePreTrainedModel().__init__(config)\n+        PreTrainedModel.__init__(self, config)\n         self.model = Ernie4_5_MoeModel(config)\n         self.vocab_size = config.vocab_size\n         self.lm_head = nn.Linear(config.hidden_size, config.vocab_size, bias=config.use_bias)"
        },
        {
            "sha": "5c1b6bb0e93f327d1621487b615c5ee2ff2ccb5a",
            "filename": "src/transformers/models/evolla/modular_evolla.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fevolla%2Fmodular_evolla.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -782,7 +782,7 @@ class EvollaPreTrainedModel(LlamaPreTrainedModel):\n \n     def _init_weights(self, module):\n         std = self.config.initializer_range\n-        LlamaPreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, EvollaSequenceAlignerCrossAttention):\n             module.gate_attention.zero_()\n             module.gate_ffw.zero_()"
        },
        {
            "sha": "d507397b0e3139ec77222a4b225fefc175d11ab6",
            "filename": "src/transformers/models/gemma3/modular_gemma3.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fmodular_gemma3.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -29,7 +29,7 @@\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_outputs import BaseModelOutputWithPast, SequenceClassifierOutputWithPast\n from ...modeling_rope_utils import rope_config_validation\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ...utils.deprecation import deprecate_kwarg\n@@ -530,7 +530,7 @@ class Gemma3PreTrainedModel(Gemma2PreTrainedModel):\n     ]\n \n     def _init_weights(self, module):\n-        Gemma2PreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Gemma3MultiModalProjector):\n             module.mm_input_projection_weight.data.zero_()\n "
        },
        {
            "sha": "3a8db73cdd660957e5f7213cb23396564e31760e",
            "filename": "src/transformers/models/gemma3n/modular_gemma3n.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3n%2Fmodular_gemma3n.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -1922,7 +1922,7 @@ class Gemma3nPreTrainedModel(Gemma2PreTrainedModel):\n     _no_split_modules = [\"Gemma3nTextDecoderLayer\"]\n \n     def _init_weights(self, module):\n-        Gemma2PreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Gemma3nAudioCumulativeGroupNorm):\n             module.weight.data.fill_(1.0)\n         elif isinstance(module, Gemma3nAudioAttention):"
        },
        {
            "sha": "ca17a630a8255a62a8355b376cef69fe67a94959",
            "filename": "src/transformers/models/glm4v/modular_glm4v.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v%2Fmodular_glm4v.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -387,7 +387,7 @@ def __init__(self, config, bias: bool = False):\n \n class Glm4vVisionPatchEmbed(Qwen2_5_VisionPatchEmbed):\n     def __init__(self, config: Glm4vVisionConfig) -> None:\n-        Qwen2_5_VisionPatchEmbed.__init__()\n+        nn.Module.__init__(self)\n         self.patch_size = config.patch_size\n         self.temporal_patch_size = config.temporal_patch_size\n         self.in_channels = config.in_channels"
        },
        {
            "sha": "52004b560da764a293c70b8021de1d167d8e9cc8",
            "filename": "src/transformers/models/glm4v_moe/configuration_glm4v_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 4,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fconfiguration_glm4v_moe.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -268,10 +268,7 @@ def __init__(\n         norm_topk_prob=True,\n         **kwargs,\n     ):\n-        super().__init__(\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n+        super().__init__(tie_word_embeddings=tie_word_embeddings, **kwargs)\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size"
        },
        {
            "sha": "0dfe28ff19da878a689afb0ab6621e8cfb35f340",
            "filename": "src/transformers/models/glm4v_moe/modular_glm4v_moe.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm4v_moe%2Fmodular_glm4v_moe.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -18,6 +18,7 @@\n import torch.nn as nn\n \n from ...cache_utils import Cache\n+from ...configuration_utils import PretrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_rope_utils import rope_config_validation\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n@@ -48,7 +49,7 @@ class Glm4vMoeVisionConfig(Glm4vVisionConfig):\n     pass\n \n \n-class Glm4vMoeTextConfig(Glm4MoeConfig, nn.Module):\n+class Glm4vMoeTextConfig(Glm4MoeConfig):\n     r\"\"\"\n     This is the configuration class to store the configuration of a [`Glm4vMoeModel`]. It is used to instantiate a\n     GLM-4.5V model according to the specified arguments, defining the model architecture. Instantiating a\n@@ -197,10 +198,7 @@ def __init__(\n         norm_topk_prob=True,\n         **kwargs,\n     ):\n-        nn.Module().__init__(\n-            tie_word_embeddings=tie_word_embeddings,\n-            **kwargs,\n-        )\n+        PretrainedConfig.__init__(self, tie_word_embeddings=tie_word_embeddings, **kwargs)\n         self.vocab_size = vocab_size\n         self.max_position_embeddings = max_position_embeddings\n         self.hidden_size = hidden_size"
        },
        {
            "sha": "cf98029307c0296dcc5d68709b9cb9072f43dd63",
            "filename": "src/transformers/models/got_ocr2/modular_got_ocr2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgot_ocr2%2Fmodular_got_ocr2.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -19,29 +19,29 @@\n import torch\n import torch.nn as nn\n \n-from transformers.models.llava.modeling_llava import (\n+from ...cache_utils import Cache\n+from ...configuration_utils import PretrainedConfig\n+from ...modeling_flash_attention_utils import FlashAttentionKwargs\n+from ...modeling_utils import PreTrainedModel\n+from ...processing_utils import Unpack\n+from ...utils import auto_docstring, can_return_tuple, logging\n+from ..auto import CONFIG_MAPPING, AutoConfig\n+from ..llava.modeling_llava import (\n     LlavaCausalLMOutputWithPast,\n     LlavaForConditionalGeneration,\n     LlavaModel,\n     LlavaModelOutputWithPast,\n     LlavaPreTrainedModel,\n     TransformersKwargs,\n )\n-from transformers.models.sam.modeling_sam import (\n+from ..sam.modeling_sam import (\n     SamMLPBlock,\n     SamPreTrainedModel,\n     SamVisionAttention,\n     SamVisionEncoder,\n     SamVisionLayer,\n )\n \n-from ...cache_utils import Cache\n-from ...configuration_utils import PretrainedConfig\n-from ...modeling_flash_attention_utils import FlashAttentionKwargs\n-from ...processing_utils import Unpack\n-from ...utils import auto_docstring, can_return_tuple, logging\n-from ..auto import CONFIG_MAPPING, AutoConfig\n-\n \n logger = logging.get_logger(__name__)\n \n@@ -291,7 +291,7 @@ class GotOcr2PreTrainedModel(LlavaPreTrainedModel):\n     _supports_flex_attn = False\n \n     def _init_weights(self, module):\n-        LlavaPreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, GotOcr2VisionAttention):\n             if module.use_rel_pos:\n                 module.rel_pos_h.data.zero_()"
        },
        {
            "sha": "0d86a92a4f261bb7c7df5acdb53dcb1f52f75abd",
            "filename": "src/transformers/models/gpt_neox/modular_gpt_neox.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodular_gpt_neox.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -17,7 +17,7 @@\n     SequenceClassifierOutputWithPast,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, logging\n from ..llama.modeling_llama import LlamaModel, LlamaPreTrainedModel, LlamaRotaryEmbedding, rotate_half\n@@ -252,9 +252,9 @@ class GPTNeoXPreTrainedModel(LlamaPreTrainedModel):\n GPT_NEOX_INPUTS_DOCSTRING = None  # Will be picked up by modular\n \n \n-class GPTNeoXModel(LlamaModel, nn.Module):\n+class GPTNeoXModel(LlamaModel):\n     def __init__(self, config):\n-        nn.Module.__init__(config)\n+        PreTrainedModel.__init__(self, config)\n         self.config = config\n \n         self.embed_in = nn.Embedding(config.vocab_size, config.hidden_size)"
        },
        {
            "sha": "f11f887e165d1be39659640b8e845cf107c39a7f",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 2,
            "deletions": 0,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -516,6 +516,8 @@ def forward(\n         else:\n             hidden_states = moe_hidden_states + self.shared_mlp(hidden_states)\n \n+        del moe_hidden_states\n+\n         hidden_states = residual + hidden_states * self.residual_multiplier\n \n         outputs = (hidden_states,)"
        },
        {
            "sha": "4fd28bc5282210820ba83d94120b5add3a143f80",
            "filename": "src/transformers/models/informer/modular_informer.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Finformer%2Fmodular_informer.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -650,9 +650,9 @@ def __init__(self, config: InformerConfig):\n         self.post_init()\n \n \n-class InformerModel(TimeSeriesTransformerModel, nn.Module):\n+class InformerModel(TimeSeriesTransformerModel):\n     def __init__(self, config: InformerConfig):\n-        nn.Module().__init__(config)\n+        PreTrainedModel.__init__(self, config)\n \n         if config.scaling == \"mean\" or config.scaling is True:\n             self.scaler = InformerMeanScaler(config)\n@@ -800,9 +800,9 @@ def forward(self, **super_kwargs):\n         super().forward(**super_kwargs)\n \n \n-class InformerForPrediction(TimeSeriesTransformerForPrediction, nn.Module):\n+class InformerForPrediction(TimeSeriesTransformerForPrediction):\n     def __init__(self, config: InformerConfig):\n-        nn.Module().__init__(config)\n+        PreTrainedModel.__init__(self, config)\n \n         self.model = InformerModel(config)\n         if config.distribution_output == \"student_t\":"
        },
        {
            "sha": "03b442b2edbd8b94058f804c33a91ea8e3d3abc5",
            "filename": "src/transformers/models/kyutai_speech_to_text/modular_kyutai_speech_to_text.py",
            "status": "modified",
            "additions": 4,
            "deletions": 3,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fkyutai_speech_to_text%2Fmodular_kyutai_speech_to_text.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -251,7 +251,7 @@ def __init__(self, config):\n         self.embed_tokens = KyutaiSpeechToTextEmbeddings(config)\n \n \n-class KyutaiSpeechToTextForConditionalGeneration(LlamaForCausalLM, GenerationMixin, PreTrainedModel):\n+class KyutaiSpeechToTextForConditionalGeneration(LlamaForCausalLM, GenerationMixin):\n     _keep_in_fp32_modules_strict = [\"codec_model\"]\n \n     def __init__(self, config):\n@@ -299,7 +299,7 @@ def forward(self, **super_kwargs):\n         super().forward(**super_kwargs)\n \n     def _prepare_generation_config(self, *args, **kwargs):\n-        generation_config, model_kwargs = GenerationMixin._prepare_generation_config(*args, **kwargs)\n+        generation_config, model_kwargs = GenerationMixin._prepare_generation_config(self, *args, **kwargs)\n         # this should be passed to the model kwargs for the input preparation\n         model_kwargs[\"audio_window_size\"] = (\n             generation_config.audio_window_size if hasattr(generation_config, \"audio_window_size\") else None\n@@ -313,6 +313,7 @@ def _prepare_model_inputs(\n         model_kwargs: Optional[dict[str, torch.Tensor]] = None,\n     ) -> tuple[torch.Tensor, Optional[str], dict[str, torch.Tensor]]:\n         inputs, input_name, model_kwargs = GenerationMixin._prepare_model_inputs(\n+            self,\n             inputs=inputs,\n             bos_token_id=bos_token_id,\n             model_kwargs=model_kwargs,\n@@ -397,7 +398,7 @@ def prepare_inputs_for_generation(\n         padding_cache: Optional[KyutaiSpeechToTextConv1dPaddingCache] = None,\n         **kwargs,\n     ):\n-        model_inputs = GenerationMixin.prepare_inputs_for_generation(*args, **kwargs)\n+        model_inputs = GenerationMixin.prepare_inputs_for_generation(self, *args, **kwargs)\n \n         if input_values is not None:\n             cache_position = model_inputs[\"cache_position\"]"
        },
        {
            "sha": "799b9bbaa70477f5405a18e6534cb155ee29b1b2",
            "filename": "src/transformers/models/owlv2/modular_owlv2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fowlv2%2Fmodular_owlv2.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -17,9 +17,8 @@\n import warnings\n from typing import Optional, Union\n \n-from transformers.models.owlvit.image_processing_owlvit_fast import OwlViTImageProcessorFast\n-\n from ...image_processing_utils_fast import (\n+    BaseImageProcessorFast,\n     BatchFeature,\n     DefaultFastImageProcessorKwargs,\n )\n@@ -40,6 +39,7 @@\n     is_torchvision_available,\n     is_torchvision_v2_available,\n )\n+from ..owlvit.image_processing_owlvit_fast import OwlViTImageProcessorFast\n \n \n if is_torch_available():\n@@ -78,11 +78,11 @@ class Owlv2ImageProcessorFast(OwlViTImageProcessorFast):\n     do_center_crop = None\n \n     def __init__(self, **kwargs: Unpack[Owlv2FastImageProcessorKwargs]):\n-        OwlViTImageProcessorFast().__init__(**kwargs)\n+        BaseImageProcessorFast.__init__(self, **kwargs)\n \n     @auto_docstring\n     def preprocess(self, images: ImageInput, **kwargs: Unpack[Owlv2FastImageProcessorKwargs]):\n-        return OwlViTImageProcessorFast().preprocess(images, **kwargs)\n+        return BaseImageProcessorFast.preprocess(self, images, **kwargs)\n \n     def _pad_images(self, images: \"torch.Tensor\", constant_value: float = 0.5) -> \"torch.Tensor\":\n         \"\"\""
        },
        {
            "sha": "a945fb21b935082b8fe5a3f801e760b1d12fe76b",
            "filename": "src/transformers/models/phi3/modular_phi3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodular_phi3.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -23,6 +23,7 @@\n \n from ...activations import ACT2FN\n from ...cache_utils import Cache\n+from ...generation import GenerationMixin\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n from ...processing_utils import Unpack\n@@ -213,7 +214,7 @@ class Phi3PreTrainedModel(MistralPreTrainedModel):\n     _version = \"0.0.5\"\n \n \n-class Phi3ForCausalLM(MistralForCausalLM, Phi3PreTrainedModel):\n+class Phi3ForCausalLM(MistralForCausalLM):\n     def prepare_inputs_for_generation(\n         self,\n         input_ids,\n@@ -240,7 +241,8 @@ def prepare_inputs_for_generation(\n             if past_length <= self.config.original_max_position_embeddings:\n                 past_key_values = None\n \n-        model_inputs = Phi3PreTrainedModel().prepare_inputs_for_generation(\n+        model_inputs = GenerationMixin.prepare_inputs_for_generation(\n+            self,\n             input_ids=input_ids,\n             past_key_values=past_key_values,\n             attention_mask=attention_mask,"
        },
        {
            "sha": "c6bdc0cc6c3403eb4c70ebe3a0c49ab228486521",
            "filename": "src/transformers/models/phi4_multimodal/modular_phi4_multimodal.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi4_multimodal%2Fmodular_phi4_multimodal.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -1449,7 +1449,7 @@ class Phi4MultimodalRotaryEmbedding(Phi3RotaryEmbedding):\n \n class Phi4MultimodalPreTrainedModel(Phi3PreTrainedModel):\n     def _init_weights(self, module):\n-        Phi3PreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         if isinstance(module, Phi4MultimodalImageEmbedding):\n             module.global_img_feature_extensor.data.zero_()\n             module.sub_img_feature_extensor.data.zero_()"
        },
        {
            "sha": "3ccaf4475fb7b29a14350c7b8ac181401fda1a70",
            "filename": "src/transformers/models/qwen3_moe/modular_qwen3_moe.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen3_moe%2Fmodular_qwen3_moe.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -123,9 +123,9 @@ class Qwen3MoeRMSNorm(LlamaRMSNorm):\n     pass\n \n \n-class Qwen3MoeDecoderLayer(Qwen2MoeDecoderLayer, nn.Module):\n+class Qwen3MoeDecoderLayer(Qwen2MoeDecoderLayer):\n     def __init__(self, config: Qwen3MoeConfig, layer_idx: int):\n-        nn.Module().__init__()\n+        nn.Module.__init__(self)\n         self.hidden_size = config.hidden_size\n \n         self.self_attn = Qwen3MoeAttention(config, layer_idx)"
        },
        {
            "sha": "a0e1dfd3a4d1a8a30b3cec56af4af327a3bfe46e",
            "filename": "src/transformers/models/rt_detr/modular_rt_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Frt_detr%2Fmodular_rt_detr.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -106,7 +106,7 @@ class RTDetrFastImageProcessorKwargs(DetrFastImageProcessorKwargs):\n     pass\n \n \n-class RTDetrImageProcessorFast(DetrImageProcessorFast, BaseImageProcessorFast):\n+class RTDetrImageProcessorFast(DetrImageProcessorFast):\n     resample = PILImageResampling.BILINEAR\n     image_mean = IMAGENET_DEFAULT_MEAN\n     image_std = IMAGENET_DEFAULT_STD\n@@ -128,7 +128,7 @@ def __init__(self, **kwargs: Unpack[RTDetrFastImageProcessorKwargs]) -> None:\n         if do_convert_annotations is None and getattr(self, \"do_convert_annotations\", None) is None:\n             self.do_convert_annotations = do_normalize if do_normalize is not None else self.do_normalize\n \n-        BaseImageProcessorFast.__init__(**kwargs)\n+        BaseImageProcessorFast.__init__(self, **kwargs)\n \n     def preprocess(\n         self,\n@@ -137,7 +137,7 @@ def preprocess(\n         masks_path: Optional[Union[str, pathlib.Path]] = None,\n         **kwargs: Unpack[RTDetrFastImageProcessorKwargs],\n     ) -> BatchFeature:\n-        return BaseImageProcessorFast().preprocess(images, annotations, masks_path, **kwargs)\n+        return BaseImageProcessorFast.preprocess(self, images, annotations, masks_path, **kwargs)\n \n     def prepare_annotation(\n         self,"
        },
        {
            "sha": "c182a2e999f32ee14c59b7260a7d529fbea60253",
            "filename": "src/transformers/models/sam2/modeling_sam2.py",
            "status": "modified",
            "additions": 2,
            "deletions": 1,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodeling_sam2.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -29,7 +29,7 @@\n import torch.nn.functional as F\n from torch import Tensor\n \n-from transformers.utils.generic import OutputRecorder, TransformersKwargs, check_model_inputs\n+from transformers.utils.generic import OutputRecorder\n \n from ...activations import ACT2FN\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -41,6 +41,7 @@\n     ModelOutput,\n     auto_docstring,\n )\n+from ...utils.generic import TransformersKwargs, check_model_inputs\n from ..auto import AutoModel\n from .configuration_sam2 import (\n     Sam2Config,"
        },
        {
            "sha": "adc373021258b23594df70316f307543c5ed8086",
            "filename": "src/transformers/models/sam2/modular_sam2.py",
            "status": "modified",
            "additions": 20,
            "deletions": 23,
            "changes": 43,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2%2Fmodular_sam2.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -23,26 +23,9 @@\n import torch.nn.functional as F\n import torch.utils.checkpoint\n \n-from transformers.models.maskformer.modeling_maskformer import MaskFormerSinePositionEmbedding\n-from transformers.models.sam.image_processing_sam_fast import SamImageProcessorFast\n-from transformers.models.sam.modeling_sam import (\n-    SamLayerNorm,\n-    SamMaskDecoder,\n-    SamMaskEmbedding,\n-    SamModel,\n-    SamPromptEncoder,\n-    SamTwoWayAttentionBlock,\n-    SamTwoWayTransformer,\n-    eager_attention_forward,\n-)\n-from transformers.models.vitdet.modeling_vitdet import window_partition, window_unpartition\n-from transformers.utils.generic import TransformersKwargs, check_model_inputs\n-\n from ...activations import ACT2FN\n from ...image_processing_utils import BatchFeature, get_size_dict\n-from ...image_processing_utils_fast import (\n-    DefaultFastImageProcessorKwargs,\n-)\n+from ...image_processing_utils_fast import BaseImageProcessorFast, DefaultFastImageProcessorKwargs\n from ...image_utils import (\n     IMAGENET_DEFAULT_MEAN,\n     IMAGENET_DEFAULT_STD,\n@@ -62,7 +45,21 @@\n     is_torch_available,\n     logging,\n )\n+from ...utils.generic import TransformersKwargs, check_model_inputs\n from ..auto import AutoModel\n+from ..maskformer.modeling_maskformer import MaskFormerSinePositionEmbedding\n+from ..sam.image_processing_sam_fast import SamImageProcessorFast\n+from ..sam.modeling_sam import (\n+    SamLayerNorm,\n+    SamMaskDecoder,\n+    SamMaskEmbedding,\n+    SamModel,\n+    SamPromptEncoder,\n+    SamTwoWayAttentionBlock,\n+    SamTwoWayTransformer,\n+    eager_attention_forward,\n+)\n+from ..vitdet.modeling_vitdet import window_partition, window_unpartition\n from .configuration_sam2 import (\n     Sam2Config,\n     Sam2HieraDetConfig,\n@@ -109,7 +106,7 @@ class Sam2ImageProcessorFast(SamImageProcessorFast):\n     mask_pad_size = None\n \n     def __init__(self, **kwargs: Unpack[Sam2FastImageProcessorKwargs]):\n-        SamImageProcessorFast().__init__(**kwargs)\n+        BaseImageProcessorFast.__init__(self, **kwargs)\n \n     def pad_image():\n         raise NotImplementedError(\"No pad_image for SAM 2.\")\n@@ -126,7 +123,7 @@ def _preprocess(\n         return_tensors: Optional[Union[str, TensorType]],\n         **kwargs,\n     ) -> \"torch.Tensor\":\n-        return SamImageProcessorFast()._preprocess(images, return_tensors=return_tensors, **kwargs).pixel_values\n+        return BaseImageProcessorFast._preprocess(self, images, return_tensors=return_tensors, **kwargs).pixel_values\n \n     def _preprocess_image_like_inputs(\n         self,\n@@ -845,7 +842,7 @@ class Sam2MaskEmbedding(SamMaskEmbedding):\n \n class Sam2PromptEncoder(SamPromptEncoder):\n     def __init__(self, config: Sam2PromptEncoderConfig):\n-        SamPromptEncoder().__init__()\n+        nn.Module.__init__(self)\n         self.shared_embedding = Sam2PositionalEmbedding(config)\n         self.mask_embed = Sam2MaskEmbedding(config)\n         self.no_mask_embed = nn.Embedding(1, config.hidden_size)\n@@ -959,7 +956,7 @@ def forward(\n \n class Sam2TwoWayAttentionBlock(SamTwoWayAttentionBlock, GradientCheckpointingLayer):\n     def __init__(self, config: Sam2MaskDecoderConfig, skip_first_layer_pe: bool = False):\n-        SamTwoWayAttentionBlock().__init__()\n+        nn.Module.__init__(self)\n         self.self_attn = Sam2Attention(config, downsample_rate=1)\n         self.layer_norm1 = nn.LayerNorm(config.hidden_size)\n \n@@ -1186,7 +1183,7 @@ class Sam2Model(SamModel):\n     ]\n \n     def __init__(self, config: Sam2Config):\n-        SamModel().__init__(config)\n+        PreTrainedModel.__init__(self, config)\n         self.shared_image_embedding = Sam2PositionalEmbedding(config.prompt_encoder_config)\n         self.vision_encoder = AutoModel.from_config(config.vision_config)\n         self.prompt_encoder = Sam2PromptEncoder(config.prompt_encoder_config)"
        },
        {
            "sha": "bbb71746c3e00883a2148d69d48362471322e085",
            "filename": "src/transformers/models/sam2_video/modeling_sam2_video.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodeling_sam2_video.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -31,8 +31,6 @@\n from torch import Tensor\n from tqdm import tqdm\n \n-from transformers.utils.generic import OutputRecorder, TransformersKwargs\n-\n from ...activations import ACT2FN\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n@@ -44,6 +42,7 @@\n     ModelOutput,\n     auto_docstring,\n )\n+from ...utils.generic import OutputRecorder, TransformersKwargs\n from ..auto import AutoModel\n from .configuration_sam2_video import Sam2VideoConfig, Sam2VideoMaskDecoderConfig, Sam2VideoPromptEncoderConfig\n "
        },
        {
            "sha": "109e9ece5d935e8dfcd3b2e8751c971246c16f0c",
            "filename": "src/transformers/models/sam2_video/modular_sam2_video.py",
            "status": "modified",
            "additions": 17,
            "deletions": 18,
            "changes": 35,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsam2_video%2Fmodular_sam2_video.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -27,28 +27,12 @@\n from torch import Tensor\n from tqdm import tqdm\n \n-from transformers.models.sam2.configuration_sam2 import (\n-    Sam2MaskDecoderConfig,\n-    Sam2PromptEncoderConfig,\n-)\n-from transformers.models.sam2.modeling_sam2 import (\n-    Sam2FeedForward,\n-    Sam2ImageSegmentationOutput,\n-    Sam2LayerNorm,\n-    Sam2Model,\n-    Sam2SinePositionEmbedding,\n-    Sam2TwoWayAttentionBlock,\n-    eager_attention_forward,\n-)\n-from transformers.models.sam2.processing_sam2 import Sam2Processor\n-from transformers.utils.generic import OutputRecorder, TransformersKwargs\n-\n from ...activations import ACT2FN\n from ...configuration_utils import PretrainedConfig\n from ...modeling_flash_attention_utils import FlashAttentionKwargs\n from ...modeling_layers import GradientCheckpointingLayer\n from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n-from ...processing_utils import Unpack\n+from ...processing_utils import ProcessorMixin, Unpack\n from ...utils import (\n     ModelOutput,\n     auto_docstring,\n@@ -57,8 +41,23 @@\n     is_torchvision_v2_available,\n     logging,\n )\n+from ...utils.generic import OutputRecorder, TransformersKwargs\n from ...video_utils import VideoInput\n from ..auto import CONFIG_MAPPING, AutoConfig\n+from ..sam2.configuration_sam2 import (\n+    Sam2MaskDecoderConfig,\n+    Sam2PromptEncoderConfig,\n+)\n+from ..sam2.modeling_sam2 import (\n+    Sam2FeedForward,\n+    Sam2ImageSegmentationOutput,\n+    Sam2LayerNorm,\n+    Sam2Model,\n+    Sam2SinePositionEmbedding,\n+    Sam2TwoWayAttentionBlock,\n+    eager_attention_forward,\n+)\n+from ..sam2.processing_sam2 import Sam2Processor\n \n \n if is_torch_available():\n@@ -637,7 +636,7 @@ class Sam2VideoProcessor(Sam2Processor):\n     def __init__(\n         self, image_processor, video_processor, target_size: Optional[int] = None, point_pad_value: int = -10, **kwargs\n     ):\n-        Sam2Processor().__init__(image_processor, video_processor, **kwargs)\n+        ProcessorMixin.__init__(self, image_processor, video_processor, **kwargs)\n         self.point_pad_value = point_pad_value\n         self.target_size = target_size if target_size is not None else self.image_processor.size[\"height\"]\n "
        },
        {
            "sha": "65eb25c5c8aecc0d19435e217433a8138b5a4ef1",
            "filename": "src/transformers/models/t5gemma/modeling_t5gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodeling_t5gemma.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -24,8 +24,6 @@\n import torch\n import torch.nn as nn\n \n-from transformers.utils.generic import OutputRecorder, check_model_inputs\n-\n from ...activations import ACT2FN\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...generation import GenerationMixin\n@@ -45,6 +43,7 @@\n from ...processing_utils import Unpack\n from ...utils import TransformersKwargs, auto_docstring, can_return_tuple, is_torchdynamo_compiling, logging\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from .configuration_t5gemma import T5GemmaConfig, T5GemmaModuleConfig\n \n "
        },
        {
            "sha": "51e2a7ccb5d483fe02a5f0c707bdb962f671f797",
            "filename": "src/transformers/models/t5gemma/modular_t5gemma.py",
            "status": "modified",
            "additions": 3,
            "deletions": 9,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5gemma%2Fmodular_t5gemma.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -18,8 +18,6 @@\n import torch\n import torch.nn as nn\n \n-from transformers.utils.generic import OutputRecorder, check_model_inputs\n-\n from ...cache_utils import Cache, DynamicCache, EncoderDecoderCache\n from ...configuration_utils import PretrainedConfig\n from ...generation import GenerationMixin\n@@ -33,17 +31,17 @@\n     SequenceClassifierOutput,\n     TokenClassifierOutput,\n )\n-from ...modeling_utils import ALL_ATTENTION_FUNCTIONS\n+from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel\n from ...processing_utils import Unpack\n from ...utils import (\n     TransformersKwargs,\n     auto_docstring,\n     can_return_tuple,\n-    is_torch_flex_attn_available,\n     is_torchdynamo_compiling,\n     logging,\n )\n from ...utils.deprecation import deprecate_kwarg\n+from ...utils.generic import OutputRecorder, check_model_inputs\n from ..gemma2.configuration_gemma2 import Gemma2Config\n from ..gemma2.modeling_gemma2 import (\n     Gemma2Attention,\n@@ -60,10 +58,6 @@\n _CHECKPOINT_FOR_DOC = \"google/t5gemma-2b-2b-prefixlm-it\"\n \n \n-if is_torch_flex_attn_available():\n-    pass\n-\n-\n logger = logging.get_logger(__name__)\n \n \n@@ -487,7 +481,7 @@ class T5GemmaPreTrainedModel(Gemma2PreTrainedModel):\n \n     def _init_weights(self, module):\n         # TODO: support intialization for encoders and decoders separately(?)\n-        Gemma2PreTrainedModel._init_weights(self, module)\n+        PreTrainedModel._init_weights(self, module)\n         std = self.config.initializer_range\n         if isinstance(module, T5GemmaClassificationHead):\n             scale = module.out_proj.weight.shape[0] ** -0.5"
        },
        {
            "sha": "debcd0ae7d9d887d58f374cf67b2a889ed78fbe0",
            "filename": "utils/create_dependency_mapping.py",
            "status": "modified",
            "additions": 53,
            "deletions": 31,
            "changes": 84,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/utils%2Fcreate_dependency_mapping.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/utils%2Fcreate_dependency_mapping.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcreate_dependency_mapping.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -1,18 +1,21 @@\n import ast\n+import re\n from collections import defaultdict\n \n \n # Function to perform topological sorting\n def topological_sort(dependencies: dict) -> list[list[str]]:\n-    \"\"\"Given the dependencies graph construct sorted list of list of modular files\n+    \"\"\"Given the dependencies graph, construct a sorted list of list of modular files.\n \n-    For example, returned list of lists might be:\n+    Examples:\n+\n+        The returned list of lists might be:\n         [\n-            [\"../modular_llama.py\", \"../modular_gemma.py\"],    # level 0\n+            [\"../modular_mistral.py\", \"../modular_gemma.py\"],  # level 0\n             [\"../modular_llama4.py\", \"../modular_gemma2.py\"],  # level 1\n             [\"../modular_glm4.py\"],                            # level 2\n         ]\n-        which means llama and gemma do not depend on any other modular models, while llama4 and gemma2\n+        which means mistral and gemma do not depend on any other modular models, while llama4 and gemma2\n         depend on the models in the first list, and glm4 depends on the models in the second and (optionally) in the first list.\n     \"\"\"\n \n@@ -40,52 +43,71 @@ def topological_sort(dependencies: dict) -> list[list[str]]:\n     return sorting_list\n \n \n-# Function to extract class and import info from a file\n-def extract_classes_and_imports(file_path):\n+# All the model file types that may be imported in modular files\n+ALL_FILE_TYPES = (\n+    \"modeling\",\n+    \"configuration\",\n+    \"tokenization\",\n+    \"processing\",\n+    \"image_processing\",\n+    \"video_processing\",\n+    \"feature_extraction\",\n+)\n+\n+\n+def is_model_import(module: str) -> bool:\n+    \"\"\"Check whether `module` is a model import or not.\"\"\"\n+    patterns = \"|\".join(ALL_FILE_TYPES)\n+    regex = rf\"(\\w+)\\.(?:{patterns})_(\\w+)\"\n+    match_object = re.search(regex, module)\n+    if match_object is not None:\n+        model_name = match_object.group(1)\n+        if model_name in match_object.group(2) and model_name != \"auto\":\n+            return True\n+    return False\n+\n+\n+def extract_model_imports_from_file(file_path):\n+    \"\"\"From a python file `file_path`, extract the model-specific imports (the imports related to any model file in\n+    Transformers)\"\"\"\n     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n         tree = ast.parse(file.read(), filename=file_path)\n     imports = set()\n \n     for node in ast.walk(tree):\n-        if isinstance(node, (ast.Import, ast.ImportFrom)):\n-            module = node.module if isinstance(node, ast.ImportFrom) else None\n-            if module and (\".modeling_\" in module or \"transformers.models\" in module):\n-                imports.add(module)\n+        if isinstance(node, ast.ImportFrom):\n+            if is_model_import(node.module):\n+                imports.add(node.module)\n     return imports\n \n \n-# Function to map dependencies between classes\n-def map_dependencies(py_files):\n-    dependencies = defaultdict(set)\n-    # First pass: Extract all classes and map to files\n-    for file_path in py_files:\n-        # dependencies[file_path].add(None)\n-        class_to_file = extract_classes_and_imports(file_path)\n-        for module in class_to_file:\n-            dependencies[file_path].add(module)\n-    return dependencies\n-\n-\n-def find_priority_list(py_files):\n+def find_priority_list(modular_files: list[str]) -> tuple[list[list[str]], dict[str, set]]:\n     \"\"\"\n     Given a list of modular files, sorts them by topological order. Modular models that DON'T depend on other modular\n-    models will be higher in the topological order.\n+    models will be lower in the topological order.\n \n     Args:\n-        py_files: List of paths to the modular files\n+        modular_files (`list[str]`):\n+            List of paths to the modular files.\n \n     Returns:\n-        Ordered list of lists of files and their dependencies (dict)\n+        A tuple `ordered_files` and `dependencies`.\n \n-        For example, ordered_files might be:\n+        `ordered_file` is a list of lists consisting of the models at each level of the dependency graph. For example,\n+        it might be:\n         [\n-            [\"../modular_llama.py\", \"../modular_gemma.py\"],    # level 0\n+            [\"../modular_mistral.py\", \"../modular_gemma.py\"],  # level 0\n             [\"../modular_llama4.py\", \"../modular_gemma2.py\"],  # level 1\n             [\"../modular_glm4.py\"],                            # level 2\n         ]\n-        which means llama and gemma do not depend on any other modular models, while llama4 and gemma2\n-        depend on the models in the first list, and glm4 depends on the models in the second and (optionally) in the first list.\n+        which means mistral and gemma do not depend on any other modular models, while llama4 and gemma2 depend on the\n+        models in the first list, and glm4 depends on the models in the second and (optionally) in the first list.\n+\n+        `dependencies` is a dictionary mapping each modular file to the models on which it relies (the models that are\n+        imported in order to use inheritance).\n     \"\"\"\n-    dependencies = map_dependencies(py_files)\n+    dependencies = defaultdict(set)\n+    for file_path in modular_files:\n+        dependencies[file_path].update(extract_model_imports_from_file(file_path))\n     ordered_files = topological_sort(dependencies)\n     return ordered_files, dependencies"
        },
        {
            "sha": "0cd0f0c2f0e97837f2297972c5f6aa0444ef3977",
            "filename": "utils/modular_model_converter.py",
            "status": "modified",
            "additions": 73,
            "deletions": 92,
            "changes": 165,
            "blob_url": "https://github.com/huggingface/transformers/blob/8b804311ba9bf921a6099c32edb6aae1ae557b8e/utils%2Fmodular_model_converter.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/8b804311ba9bf921a6099c32edb6aae1ae557b8e/utils%2Fmodular_model_converter.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fmodular_model_converter.py?ref=8b804311ba9bf921a6099c32edb6aae1ae557b8e",
            "patch": "@@ -170,16 +170,6 @@ def leave_ImportFrom(self, original_node, updated_node):\n )\n \n \n-def SUPER_CALL_NODE(func_name):\n-    return m.Call(func=m.Attribute(value=m.Call(func=m.Name(\"super\")), attr=m.Name(func_name)))\n-\n-\n-def is_call_to_super(node, func_name):\n-    return m.matches(\n-        node, m.SimpleStatementLine(body=[m.Return(SUPER_CALL_NODE(func_name)) | m.Expr(SUPER_CALL_NODE(func_name))])\n-    )\n-\n-\n def get_full_attribute_name(node: Union[cst.Attribute, cst.Name]) -> Optional[str]:\n     \"\"\"Get the full name of an Attribute or Name node (e.g. `\"nn.Module\"` for an Attribute representing it). If the\n     successive value of an Attribute are not Name nodes, return `None`.\"\"\"\n@@ -201,71 +191,67 @@ def get_full_attribute_name(node: Union[cst.Attribute, cst.Name]) -> Optional[st\n     return None\n \n \n-# Transformer class to replace ClassB.call_to_method and ClassB().call_to_method with super().call_to_method\n-class ReplaceMethodCallTransformer(cst.CSTTransformer):\n-    def __init__(self, all_bases: set[str]):\n-        self.all_bases = all_bases\n-\n-    def leave_Attribute(self, original_node: cst.Attribute, updated_node: cst.Attribute) -> cst.CSTNode:\n-        # Handle ClassB.call_to_method or module.classB.call_to_method\n-        if (\n-            m.matches(original_node.value, m.Name() | m.Attribute())\n-            and get_full_attribute_name(original_node.value) in self.all_bases\n-            and m.matches(original_node.attr, m.Name())\n-        ):\n-            # Replace with super().call_to_method\n-            return updated_node.with_changes(\n-                value=cst.Call(cst.Name(\"super\")),\n-            )\n-        # Handle ClassB().call_to_method or module.ClassB().call_to_method\n-        elif (\n-            m.matches(original_node.value, m.Call())\n-            and m.matches(original_node.value.func, m.Name() | m.Attribute())\n-            and get_full_attribute_name(original_node.value.func) in self.all_bases\n-            and m.matches(original_node.attr, m.Name())\n-        ):\n-            # Replace with super().call_to_method\n-            return updated_node.with_changes(value=cst.Call(cst.Name(\"super\")))\n-        return updated_node\n-\n-    def leave_Call(self, original_node: cst.Call, updated_node: cst.Call) -> cst.CSTNode:\n-        # Check if the function being called is of the form ClassB().func_a or ClassB.func_a\n-        if m.matches(original_node.func, m.Attribute()) and (\n-            # Match ClassB().func_a(...) or module\n-            (\n-                m.matches(original_node.func.value, m.Call())\n-                and m.matches(original_node.func.value.func, m.Name() | m.Attribute())\n-                and get_full_attribute_name(original_node.func.value.func) in self.all_bases\n-            )\n-            or\n-            # Match ClassB.func_a(...)\n-            (\n-                m.matches(original_node.func.value, m.Name() | m.Attribute())\n-                and get_full_attribute_name(original_node.func.value) in self.all_bases\n-            )\n-        ):\n-            # Check if the first argument is 'self', and remove it\n-            if len(original_node.args) > 0 and m.matches(original_node.args[0].value, m.Name(\"self\")):\n-                # Create the new argument list without 'self'\n-                new_args = updated_node.args[1:]\n-            else:\n-                new_args = updated_node.args\n+class ReplaceParentClassCallTransformer(cst.CSTTransformer):\n+    \"\"\"\n+    This Transformer is used to replace all calls of the form `module.Class.func(...)` by a call of the form\n+    `super().func(...)`.\n+    \"\"\"\n \n-            return updated_node.with_changes(args=new_args)\n+    def __init__(self, new_bases: list[str]):\n+        self.new_bases = new_bases\n+\n+    def is_call_to_parent_class(self, node: cst.SimpleStatementLine):\n+        \"\"\"Check whether `node` corresponds to a call to a parent class function, such as `module.Parent.func_name(...)`\"\"\"\n+        return m.matches(node, m.Call(func=m.Attribute(value=m.Name() | m.Attribute())))\n+\n+    def leave_Call(self, original_node: cst.Call, updated_node: cst.Call) -> cst.Call:\n+        \"\"\"Replace a call of the form `module.Class.func(...)` by a call of the form `super().func(...)`\n+        if the `Class` being called is one of the bases.\"\"\"\n+        if self.is_call_to_parent_class(updated_node):\n+            full_parent_class_name = get_full_attribute_name(updated_node.func.value)\n+            # Replace only if it's a base, or a few special rules\n+            if (\n+                full_parent_class_name in self.new_bases\n+                or (full_parent_class_name == \"nn.Module\" and \"GradientCheckpointingLayer\" in self.new_bases)\n+                or (\n+                    full_parent_class_name == \"PreTrainedModel\"\n+                    and any(\"PreTrainedModel\" in base for base in self.new_bases)\n+                )\n+            ):\n+                # Replace `full_parent_class_name.func(...)` with `super().func(...)`\n+                attribute_node = updated_node.func.with_changes(value=cst.Call(func=cst.Name(\"super\")))\n+                # Check if the first argument is 'self', and remove it\n+                new_args = (\n+                    updated_node.args[1:]\n+                    if len(updated_node.args) > 0 and m.matches(updated_node.args[0].value, m.Name(\"self\"))\n+                    else updated_node.args\n+                )\n+                return updated_node.with_changes(func=attribute_node, args=new_args)\n         return updated_node\n \n \n-class SuperTransformer(cst.CSTTransformer):\n-    METADATA_DEPENDENCIES = (ParentNodeProvider,)\n+class ReplaceSuperCallTransformer(cst.CSTTransformer):\n+    \"\"\"\n+    This Transformer is used to unravel all calls to `super().func(...)` in class methods by the explicit parent's\n+    code. It will also in turn replace all calls of the form `module.Class.func(...)` by a call of the form\n+    `super().func(...)`. Those calls are used to explicitly skip the unravelling of code, but we should still follow\n+    python's standards and use `super().func(...)` instead of `Parent.func(self, ...)`.\n+    \"\"\"\n \n-    def __init__(self, python_module: cst.Module, original_modeling_methods, modular_methods, all_bases=None):\n+    def __init__(\n+        self,\n+        python_module: cst.Module,\n+        original_modeling_methods: dict[str, cst.FunctionDef],\n+        modular_methods: dict[str, cst.FunctionDef],\n+        new_bases: list[cst.Arg],\n+    ):\n         self.python_module = python_module\n         self.original_modeling_methods = original_modeling_methods\n         self.modular_methods = modular_methods\n         self.all_assign_target = {}\n         self.deleted_targets = {}  # child node can delete some arguments\n-        self.all_bases = all_bases or []\n-        self.transformer = ReplaceMethodCallTransformer(set(self.all_bases))\n+        new_bases = [get_full_attribute_name(base.value) for base in new_bases]\n+        self.parent_class_call_transformer = ReplaceParentClassCallTransformer(new_bases)\n \n     def update_body(self, existing_body, new_statements):\n         \"\"\"\n@@ -343,32 +329,28 @@ def _fix_init_location(self, new_body):\n                 break\n         return new_body\n \n-    def replace_super_calls(self, node: cst.BaseSuite, func_name: str) -> cst.BaseSuite:\n-        \"\"\"Updates the body of the input `node`'s `func_name` function by replacing calls\n-        to super().func_name() with the source code of the parent class' `func_name`.\n-        It keeps everything that is defined before `super().func_name()`.\n-        \"\"\"\n-        new_body = []\n-        modular_node_body = node.body\n-\n-        for i, expr in enumerate(modular_node_body):\n-            if is_call_to_super(expr, func_name):\n-                original_modeling_method_body = self.original_modeling_methods[func_name].body.body\n-                new_body.extend(self.update_body(original_modeling_method_body, modular_node_body[i + 1 :]))\n-                new_body = self._fix_init_location(new_body)\n-                return node.with_changes(body=new_body)\n-            else:\n-                expr = expr.visit(self.transformer)\n-            if not m.matches(expr, m.SimpleStatementLine(body=[m.Del()])):\n-                new_body.append(expr)\n-\n-        return node.with_changes(body=new_body)\n+    def is_call_to_super(self, node: cst.BaseStatement, func_name: str):\n+        \"\"\"Check whether `node` corresponds to a call to `super().func_name(...)`\"\"\"\n+        super_call_node = m.Call(func=m.Attribute(value=m.Call(func=m.Name(\"super\")), attr=m.Name(func_name)))\n+        return m.matches(node, m.SimpleStatementLine(body=[m.Return(super_call_node) | m.Expr(super_call_node)]))\n \n     def leave_FunctionDef(self, original_node: cst.FunctionDef, updated_node: cst.FunctionDef) -> cst.FunctionDef:\n-        name = updated_node.name.value\n-        if name in self.modular_methods:\n-            new_body = self.replace_super_calls(updated_node.body, name)\n-            return updated_node.with_changes(body=new_body, params=updated_node.params)\n+        func_name = updated_node.name.value\n+        self.should_check_statements = False\n+        if func_name in self.modular_methods:\n+            actual_body = updated_node.body.body  # first body is an `IndentedBlock` wrapper\n+            new_body = []\n+            for i, base_statement_node in enumerate(actual_body):\n+                if self.is_call_to_super(base_statement_node, func_name):\n+                    original_modeling_method_body = self.original_modeling_methods[func_name].body.body\n+                    new_body.extend(self.update_body(original_modeling_method_body, actual_body[i + 1 :]))\n+                    new_body = self._fix_init_location(new_body)\n+                    # Break here as all future statement were already accounted for in `update_body`\n+                    break\n+                # If not a call to super, this will replace all calls of the form `module.Class.func(...)` by a\n+                # call of the form `super().func(...)\n+                new_body.append(base_statement_node.visit(self.parent_class_call_transformer))\n+            return updated_node.with_changes(body=updated_node.body.with_changes(body=new_body))\n         return updated_node\n \n \n@@ -1059,9 +1041,8 @@ def replace_class_node(\n     # Replace the calls to `super()` of the redefined modular methods with the unrolled code\n     result_node = original_modeling_node.with_changes(body=cst.IndentedBlock(body=new_class_body))\n     temp_module = cst.Module(body=[result_node])\n-    new_module = MetadataWrapper(temp_module)\n-    new_replacement_class = new_module.visit(\n-        SuperTransformer(temp_module, original_modeling_methods, modular_methods, all_bases)\n+    new_replacement_class = temp_module.visit(\n+        ReplaceSuperCallTransformer(temp_module, original_modeling_methods, modular_methods, new_class_bases)\n     )\n     new_class_body = new_replacement_class.body[0].body  # get the indented block\n "
        }
    ],
    "stats": {
        "total": 544,
        "additions": 265,
        "deletions": 279
    }
}