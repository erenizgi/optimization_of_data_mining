{
    "author": "AhmedAlmaghz",
    "message": "[i18n-ar] Translated file: `docs/source/ar/tasks/masked_language_modeling.md` into Arabic (#35198)\n\n* Ø¥Ø¶Ø§ÙØ© Ø§Ù„ØªØ±Ø¬Ù…Ø© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©: masked_language_modeling.md\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update docs/source/ar/tasks/masked_language_modeling.md\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>\n\n* Update _toctree.yml\n\n* Update _toctree.yml\n\n* Add language_modeling.md\n\n* Add Sequence_classifiation.md\n\n* Update _toctree.yml\n\n---------\n\nCo-authored-by: Abdullah Mohammed <554032+abodacs@users.noreply.github.com>",
    "sha": "741d55237a6d8660c8782966c18119ccd9aac2f7",
    "files": [
        {
            "sha": "3b60897f1abef3dc5633f458c8de3872c28a9477",
            "filename": "docs/source/ar/_toctree.yml",
            "status": "modified",
            "additions": 6,
            "deletions": 6,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/741d55237a6d8660c8782966c18119ccd9aac2f7/docs%2Fsource%2Far%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/741d55237a6d8660c8782966c18119ccd9aac2f7/docs%2Fsource%2Far%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2F_toctree.yml?ref=741d55237a6d8660c8782966c18119ccd9aac2f7",
            "patch": "@@ -33,16 +33,16 @@\n - sections:\n   - isExpanded: false\n     sections:\n-#     - local: tasks/sequence_classification\n-#       title: ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ\n+    - local: tasks/sequence_classification\n+      title: ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ\n     - local: tasks/token_classification\n       title: ØªØµÙ†ÙŠÙ Ø§Ù„Ø±Ù…ÙˆØ²\n     - local: tasks/question_answering\n       title: Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø£Ø³Ø¦Ù„Ø©\n-#     - local: tasks/language_modeling\n-#       title: Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©\n-#     - local: tasks/masked_language_modeling\n-#       title: Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø©\n+    - local: tasks/language_modeling\n+      title: Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©\n+    - local: tasks/masked_language_modeling\n+      title: Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø©\n     - local: tasks/translation\n       title: Ø§Ù„ØªØ±Ø¬Ù…Ø©\n     - local: tasks/summarization"
        },
        {
            "sha": "24f2db00a7a69a446895fbf3e5c915bece3bc0c1",
            "filename": "docs/source/ar/tasks/language_modeling.md",
            "status": "added",
            "additions": 422,
            "deletions": 0,
            "changes": 422,
            "blob_url": "https://github.com/huggingface/transformers/blob/741d55237a6d8660c8782966c18119ccd9aac2f7/docs%2Fsource%2Far%2Ftasks%2Flanguage_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/741d55237a6d8660c8782966c18119ccd9aac2f7/docs%2Fsource%2Far%2Ftasks%2Flanguage_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Flanguage_modeling.md?ref=741d55237a6d8660c8782966c18119ccd9aac2f7",
            "patch": "@@ -0,0 +1,422 @@\n+<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+-->\n+\n+# Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ© (Causal language modeling)\n+\n+[[open-in-colab]]\n+\n+Ù‡Ù†Ø§Ùƒ Ù†ÙˆØ¹Ø§Ù† Ù…Ù† Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ©ØŒ Ø§Ù„Ø³Ø¨Ø¨ÙŠØ© ÙˆØ§Ù„Ù…Ù‚Ù†Ø¹Ø©. ÙŠÙˆØ¶Ø­ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©.\n+ØªÙØ³ØªØ®Ø¯Ù… Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ© ØºØ§Ù„Ø¨Ù‹Ø§ Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ. ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù‡Ø°Ù‡ Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ù„Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¥Ø¨Ø¯Ø§Ø¹ÙŠØ© Ù…Ø«Ù„\n+Ø§Ø®ØªÙŠØ§Ø± Ù…ØºØ§Ù…Ø±Ø© Ø§Ù„Ù†Øµ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø£Ùˆ Ù…Ø³Ø§Ø¹Ø¯ ØªØ±Ù…ÙŠØ² Ø°ÙƒÙŠ Ù…Ø«Ù„ Copilot Ø£Ùˆ CodeParrot.\n+\n+<Youtube id=\"Vpjb1lu0MDk\"/>\n+\n+ØªØªÙ†Ø¨Ø£ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ© Ø¨Ø§Ù„Ø±Ù…Ø² Ø§Ù„ØªØ§Ù„ÙŠ ÙÙŠ ØªØ³Ù„Ø³Ù„ Ù…Ù† Ø§Ù„Ø±Ù…ÙˆØ²ØŒ ÙˆÙ„Ø§ ÙŠÙ…ÙƒÙ† Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø³ÙˆÙ‰ Ø§Ù„Ø§Ù‡ØªÙ…Ø§Ù… Ø¨Ø§Ù„Ø±Ù…ÙˆØ² Ø¹Ù„Ù‰\n+Ø§Ù„ÙŠØ³Ø§Ø±. Ù‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø§ ÙŠÙ…ÙƒÙ†Ù‡ Ø±Ø¤ÙŠØ© Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…Ø³ØªÙ‚Ø¨Ù„ÙŠØ©. GPT-2 Ù‡Ùˆ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù„ØºØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©.\n+\n+Ø³ÙŠÙˆØ¶Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:\n+\n+1. Ø¶Ø¨Ø· Ø¯Ù‚ÙŠÙ‚ [DistilRoBERTa](https://huggingface.co/distilbert/distilroberta-base) Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© [r/askscience](https://www.reddit.com/r/askscience/) Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [ELI5](https://huggingface.co/datasets/eli5).\n+2.  Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„Ù„Ø§Ø³ØªÙ†ØªØ§Ø¬.\n+\n+<Tip>\n+\n+Ù„Ø±Ø¤ÙŠØ© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¹Ù…Ø§Ø±Ø§Øª ÙˆÙ†Ù‚Ø§Ø· Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† [task-page](https://huggingface.co/tasks/text-generation)\n+\n+</Tip>\n+\n+Ù‚Ø¨Ù„ Ø£Ù† ØªØ¨Ø¯Ø£ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:\n+\n+```bash\n+pip install transformers datasets evaluate\n+```\n+\n+Ù†Ø­Ù† Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Hugging Face Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø´Ø§Ø±ÙƒØ© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©ØŒ Ø£Ø¯Ø®Ù„ Ø±Ù…Ø²Ùƒ Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„:\n+\n+```py\n+>>> from huggingface_hub import notebook_login\n+\n+>>> notebook_login()\n+```\n+\n+## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ELI5\n+\n+Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ø£ÙˆÙ„ 5000 Ù…Ø«Ø§Ù„ Ù…Ù† [ELI5-Category](https://huggingface.co/datasets/eli5_category) Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ù…ÙƒØªØ¨Ø© ğŸ¤— Datasets. Ø³ÙŠØ¹Ø·ÙŠÙƒ Ù‡Ø°Ø§ ÙØ±ØµØ© Ù„Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† ÙƒÙ„ Ø´ÙŠØ¡ ÙŠØ¹Ù…Ù„ Ù‚Ø¨Ù„ Ù‚Ø¶Ø§Ø¡ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ÙˆÙ‚Øª ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©.\n+\n+```py\n+>>> from datasets import load_dataset\n+\n+>>> eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")\n+```\n+\n+Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª `train` Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹ØªÙŠ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø®Ø§ØµÙŠØ© [`~datasets.Dataset.train_test_split`]:\n+\n+```py\n+>>> eli5 = eli5.train_test_split(test_size=0.2)\n+```\n+\n+Ø«Ù… Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:\n+\n+```py\n+>>> eli5[\"train\"][0]\n+{'q_id': '7h191n',\n+ 'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',\n+ 'selftext': '',\n+ 'category': 'Economics',\n+ 'subreddit': 'explainlikeimfive',\n+ 'answers': {'a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],\n+  'text': [\"The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.\",\n+   'None yet. It has to be reconciled with a vastly different house bill and then passed again.',\n+   'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',\n+   'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],\n+  'score': [21, 19, 5, 3],\n+  'text_urls': [[],\n+   [],\n+   [],\n+   ['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']]},\n+ 'title_urls': ['url'],\n+ 'selftext_urls': ['url']}\n+```\n+\n+Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ù‡Ø°Ø§ Ù‚Ø¯ ÙŠØ¨Ø¯Ùˆ Ù…Ø¹Ù‚Ø¯Ù‹Ø§ØŒ Ø¥Ù„Ø§ Ø£Ù†Ùƒ Ù…Ù‡ØªÙ… Ø­Ù‚Ù‹Ø§ Ø¨Ø­Ù‚Ù„ `text`. Ù…Ø§ Ù‡Ùˆ Ø±Ø§Ø¦Ø¹ Ø­ÙˆÙ„ Ù…Ù‡Ø§Ù… Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ©\n+Ø£Ù†Øª Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ³Ù…ÙŠØ§Øª (ØªÙØ¹Ø±Ù Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ø§Ø³Ù… Ø§Ù„Ù…Ù‡Ù…Ø© ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹Ø© Ù„Ù„Ø¥Ø´Ø±Ø§Ù) Ù„Ø£Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© ØªØ¹Ù…Ù„ ÙƒØªØ³Ù…ÙŠØ©.\n+\n+## Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© (Preprocess)\n+\n+<Youtube id=\"ma1TrR7gE7I\"/>\n+\n+Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ø²Ø¡ Ø§Ù„Ù†Øµ DistilGPT2 Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø­Ù‚Ù„ `text` Ø§Ù„ÙØ±Ø¹ÙŠ:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilgpt2\")\n+```\n+\n+Ø³ØªÙ„Ø§Ø­Ø¸ Ù…Ù† Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¹Ù„Ø§Ù‡ØŒ Ø§Ù„Ø­Ù‚Ù„ `text` Ù‡Ùˆ ÙÙŠ Ø§Ù„ÙˆØ§Ù‚Ø¹ Ù…ØªØ¯Ø§Ø®Ù„ Ø¯Ø§Ø®Ù„ `answers`. Ù‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ùƒ Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰\n+Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ù‚Ù„ `text` Ø§Ù„ÙØ±Ø¹ÙŠ Ù…Ù† Ø¨Ù†ÙŠØªÙ‡ Ø§Ù„Ù…ØªØ¯Ø§Ø®Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`flatten`](https://huggingface.co/docs/datasets/process#flatten):\n+\n+```py\n+>>> eli5 = eli5.flatten()\n+>>> eli5[\"train\"][0]\n+{'q_id': '7h191n',\n+ 'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',\n+ 'selftext': '',\n+ 'category': 'Economics',\n+ 'subreddit': 'explainlikeimfive',\n+ 'answers.a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],\n+ 'answers.text': [\"The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.\",\n+  'None yet. It has to be reconciled with a vastly different house bill and then passed again.',\n+  'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',\n+  'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],\n+ 'answers.score': [21, 19, 5, 3],\n+ 'answers.text_urls': [[],\n+  [],\n+  [],\n+  ['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']],\n+ 'title_urls': ['url'],\n+ 'selftext_urls': ['url']}\n+```\n+\n+ÙƒÙ„ Ø­Ù‚Ù„ ÙØ±Ø¹ÙŠ Ù‡Ùˆ Ø§Ù„Ø¢Ù† Ø¹Ù…ÙˆØ¯Ø§Ù‹ Ù…Ù†ÙØµÙ„Ø§Ù‹ Ù…Ø³Ø¨ÙˆÙ‚Ø§Ù‹ Ø¨Ù€ `answers`ØŒ ÙˆØ­Ù‚Ù„ `text` Ù‡Ùˆ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¢Ù†. Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø°Ù„Ùƒ\n+Ù…Ù† ØªØ¬Ø²Ø§Ø¦Ø© Ù†Øµ ÙƒÙ„ Ø¬Ù…Ù„Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„ØŒ Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù„Ù‰ Ø³Ù„Ø³Ù„Ø© Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ¬Ø²Ø¦Ø© Ù†ØµÙ‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø¬Ù…Ù‘Ø¹.\n+\n+Ù‡Ù†Ø§ Ø£ÙˆÙ„ Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© Ù„Ø¯Ù…Ø¬ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ù„ÙƒÙ„ Ù…Ø«Ø§Ù„ ÙˆÙ…Ø¬Ø²Ù‰Ø¡ Ø§Ù„Ù†ØªÙŠØ¬Ø©:\n+\n+```py\n+>>> def preprocess_function(examples):\n+...     return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n+```\n+\n+Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ù‡Ø°Ù‡ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯Ø§Ù„Ø© ğŸ¤— Datasets [`~datasets.Dataset.map`]. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø±ÙŠØ¹  Ù‡Ø°Ù‡ Ø§Ù„Ø¹Ù…Ù„ÙŠØ© `map` Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `batched=True` Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ù†Ø§ØµØ± Ù…ØªØ¹Ø¯Ø¯Ø© Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯ØŒ ÙˆØ²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ù…Ø¹ `num_proc`. Ø§Ø­Ø°Ù Ø£ÙŠ Ø£Ø¹Ù…Ø¯Ø© Ù„Ø§ ØªØ­ØªØ§Ø¬Ù‡Ø§:\n+\n+```py\n+>>> tokenized_eli5 = eli5.map(\n+...     preprocess_function,\n+...     batched=True,\n+...     num_proc=4,\n+...     remove_columns=eli5[\"train\"].column_names,\n+... )\n+```\n+\n+ØªØ­ØªÙˆÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ø±Ù…ÙˆØ²ØŒ ÙˆÙ„ÙƒÙ† Ø¨Ø¹Ø¶Ù‡Ø§ Ø£Ø·ÙˆÙ„ Ù…Ù† Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.\n+\n+ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ù…Ø§ Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø«Ø§Ù†ÙŠØ© Ù„Ù€:\n+\n+- ØªØ¬Ù…ÙŠØ¹ ÙƒÙ„ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª.\n+- ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ù‘Ø¹Ø© Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø£Ù‚ØµØ± Ù…Ø­Ø¯Ø¯Ø©ØŒ Ø¨Ø­Ø¬Ù… `block_size`ØŒ ÙˆØ§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø£Ù‚ØµØ± Ù…Ù† Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆÙ…Ù†Ø§Ø³Ø¨Ø© Ù„Ø°Ø§ÙƒØ±Ø© GPU.\n+\n+```py\n+>>> block_size = 128\n+\n+>>> def group_texts(examples):\n+...     # Ø±Ø¨Ø· Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ.\n+...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n+...     total_length = len(concatenated_examples[list(examples.keys())[0]])\n+...     # Ù†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¨Ø§Ù‚ÙŠ Ø§Ù„ØµØºÙŠØ±ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø´Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¯Ø¹Ù…Ù‡ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø³Ù‚Ø§Ø·ØŒ ÙŠÙ…ÙƒÙ†Ùƒ\n+...     # ØªØ®ØµÙŠØµ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ø­Ø³Ø¨ Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙƒ.\n+...     if total_length >= block_size:\n+...         total_length = (total_length // block_size) * block_size\n+...     # Ø§Ù„ØªÙ‚Ø³ÙŠÙ… Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø¨Ø­Ø¬Ù… block_size.\n+...     result = {\n+...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n+...         for k, t in concatenated_examples.items()\n+...     }\n+...     result[\"labels\"] = result[\"input_ids\"].copy()\n+...     return result\n+```\n+\n+Ø·Ø¨Ù‚ Ø¯Ø§Ù„Ø© `group_texts` Ø¹Ù„Ù‰ ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n+\n+```py\n+>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)\n+```\n+\n+Ø§Ù„Ø¢Ù† Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`DataCollatorForLanguageModeling`]. Ù…Ù† Ø§Ù„Ø£ÙØ¶Ù„ Ø£Ù† ØªÙ‚ÙˆÙ… Ø¨Ù€ *Ø§Ù„Ø­Ø´Ùˆ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ* Ù„Ù„Ø¬Ù…Ù„ Ø¥Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ø·ÙˆÙ„ ÙÙŠ Ø§Ù„Ø¯ÙØ¹Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø­Ø´Ùˆ ÙƒØ§Ù…Ù„ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰.\n+\n+<frameworkcontent>\n+<pt>\n+Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…Ø² Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ±Ù…Ø² Ù„Ù„Ø­Ø´ÙˆØŒ ÙˆØ­Ø¯Ø¯ `mlm_probability` Ù„Ø­Ø¬Ø¨ Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø´ÙƒÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¹Ù†Ø¯ ÙƒÙ„ ØªÙƒØ±Ø§Ø± Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n+\n+```py\n+>>> from transformers import DataCollatorForLanguageModeling\n+\n+>>> tokenizer.pad_token = tokenizer.eos_token\n+>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n+```\n+\n+</pt>\n+<tf>\n+Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…Ø² Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ±Ù…Ø² Ù„Ù„Ø­Ø´ÙˆØŒ ÙˆØ­Ø¯Ø¯ `mlm_probability` Ù„Ø­Ø¬Ø¨ Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø´ÙƒÙ„ Ø¹Ø´ÙˆØ§Ø¦ÙŠ Ø¹Ù†Ø¯ ÙƒÙ„ ØªÙƒØ±Ø§Ø± Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n+\n+```py\n+>>> from transformers import DataCollatorForLanguageModeling\n+\n+>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False, return_tensors=\"tf\")\n+```\n+\n+</tf>\n+</frameworkcontent>\n+\n+## Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Train)\n+\n+<frameworkcontent>\n+<pt>\n+\n+<Tip>\n+\n+Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`], Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ [Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ](../training#train-with-pytorch-trainer)!\n+\n+</Tip>\n+\n+Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ø§Ù„Ø¢Ù† Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ DistilGPT2 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`AutoModelForCausalLM`]:\n+\n+```py\n+>>> from transformers import AutoModelForCausalLM, TrainingArguments, Trainer\n+\n+>>> model = AutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n+```\n+\n+ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ ØªØ¨Ù‚Ù‰ Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª ÙÙ‚Ø·:\n+\n+1. Ø­Ø¯Ø¯ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙÙŠ [`TrainingArguments`]. Ø§Ù„Ù…Ø¹Ø§Ù…Ù„ Ø§Ù„ÙˆØ­ÙŠØ¯ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ù‡Ùˆ `output_dir` Ø§Ù„Ø°ÙŠ ÙŠØ­Ø¯Ø¯ Ø£ÙŠÙ† Ø³ÙŠØªÙ… Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬Ùƒ. Ø³ØªÙ‚ÙˆÙ… Ø¨Ø¯ÙØ¹ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub Ø¨ØªØ­Ø¯ÙŠØ¯ `push_to_hub=True` (ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø³Ø¬Ù„Ø§Ù‹ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Hugging Face Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ).\n+2. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ [`Trainer`] Ø¥Ù„Ù‰ Ø¬Ø§Ù†Ø¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆØ§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ…Ø¬Ù…Ù‘Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n+3. Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`~Trainer.train`] Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ.\n+\n+```py\n+>>> training_args = TrainingArguments(\n+...     output_dir=\"my_awesome_eli5_clm-model\",\n+...     eval_strategy=\"epoch\",\n+...     learning_rate=2e-5,\n+...     weight_decay=0.01,\n+...     push_to_hub=True,\n+... )\n+\n+>>> trainer = Trainer(\n+...     model=model,\n+...     args=training_args,\n+...     train_dataset=lm_dataset[\"train\"],\n+...     eval_dataset=lm_dataset[\"test\"],\n+...     data_collator=data_collator,\n+...     tokenizer=tokenizer,\n+... )\n+\n+>>> trainer.train()\n+```\n+\n+Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.evaluate`] Ù„ØªÙ‚ÙŠÙŠÙ… Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø§Ø±ØªØ¨Ø§Ùƒ:\n+\n+```py\n+>>> import math\n+\n+>>> eval_results = trainer.evaluate()\n+>>> print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n+Perplexity: 49.61\n+```\n+\n+Ø«Ù… Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¹Ù„Ù‰ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.push_to_hub`] Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ:\n+\n+```py\n+>>> trainer.push_to_hub()\n+```\n+</pt>\n+<tf>\n+<Tip>\n+\n+Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ [Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ](../training#train-a-tensorflow-model-with-keras)!\n+\n+</Tip>\n+Ù„ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†ØŒ ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…ØŒ ÙˆØ¨Ø¹Ø¶ Ù…Ø¹Ø§Ù…Ù„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n+\n+```py\n+>>> from transformers import create_optimizer, AdamWeightDecay\n+\n+>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n+```\n+\n+Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ DistilGPT2 Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForCausalLM`]:\n+\n+```py\n+>>> from transformers import TFAutoModelForCausalLM\n+\n+>>> model = TFAutoModelForCausalLM.from_pretrained(\"distilbert/distilgpt2\")\n+```\n+\n+Ø­ÙˆÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n+\n+```py\n+>>> tf_train_set = model.prepare_tf_dataset(\n+...     lm_dataset[\"train\"],\n+...     shuffle=True,\n+...     batch_size=16,\n+...     collate_fn=data_collator,\n+... )\n+\n+>>> tf_test_set = model.prepare_tf_dataset(\n+...     lm_dataset[\"test\"],\n+...     shuffle=False,\n+...     batch_size=16,\n+...     collate_fn=data_collator,\n+... )\n+```\n+\n+Ù‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers Ù„Ø¯ÙŠÙ‡Ø§ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø© Ø§Ù„Ø§ÙØªØ±Ø§Ø¶ÙŠØ©ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:\n+\n+```py\n+>>> import tensorflow as tf\n+\n+>>> model.compile(optimizer=optimizer)  # Ù„Ø§ ÙŠÙˆØ¬Ø¯ Ø­Ø¬Ø© Ù„Ù„Ø®Ø³Ø§Ø±Ø©!\n+```\n+\n+ÙŠÙ…ÙƒÙ† Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ø¯ÙŠØ¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø¬Ù…Ù‘Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ÙÙŠ [`~transformers.PushToHubCallback`]:\n+\n+```py\n+>>> from transformers.keras_callbacks import PushToHubCallback\n+\n+>>> callback = PushToHubCallback(\n+...     output_dir=\"my_awesome_eli5_clm-model\",\n+...     tokenizer=tokenizer,\n+... )\n+```\n+\n+Ø£Ø®ÙŠØ±Ø§Ù‹ØŒ Ø£Ù†Øª Ø¬Ø§Ù‡Ø² Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ØµØ­Ø©ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ±ØŒ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n+\n+```py\n+>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n+```\n+\n+Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n+</tf>\n+</frameworkcontent>\n+\n+<Tip>\n+\n+Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø£ÙƒØ«Ø± ØªØ¹Ù…Ù‚Ù‹Ø§ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ø³Ø¨Ø¨ÙŠØ©ØŒ Ø§Ø·Ù„Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙØªØ± Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„\n+[Ø¯ÙØªØ± PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\n+Ø£Ùˆ [Ø¯ÙØªØ± TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n+\n+</Tip>\n+\n+## Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ (Inference)\n+\n+Ø±Ø§Ø¦Ø¹ØŒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ù‚Ù…Øª Ø¨ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„!\n+\n+Ù‚Ù… Ø¨Ø§Ø¨ØªÙƒØ§Ø± Ø³Ø¤Ø§Ù„ ØªÙˆØ¯ ØªÙˆÙ„ÙŠØ¯ Ù†Øµ Ù…Ù†Ù‡:\n+\n+```py\n+>>> prompt = \"Somatic hypermutation allows the immune system to\"\n+```\n+\n+Ø£Ø¨Ø³Ø· Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¬Ø±Ø¨Ø© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù‡ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ [`pipeline`]. Ù‚Ù… Ø¨ØªÙ†ÙÙŠØ° `pipeline` Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙˆÙ…Ø±Ø± Ù†ØµÙƒ Ø¥Ù„ÙŠÙ‡:\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> generator = pipeline(\"text-generation\", model=\"username/my_awesome_eli5_clm-model\")\n+>>> generator(prompt)\n+[{'generated_text': \"Somatic hypermutation allows the immune system to be able to effectively reverse the damage caused by an infection.\\n\\n\\nThe damage caused by an infection is caused by the immune system's ability to perform its own self-correcting tasks.\"}]\n+```\n+\n+<frameworkcontent>\n+<pt>\n+Ù‚Ø³Ù… Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø¹ `input_ids` ÙƒØªÙ†Ø³ÙˆØ±Ø§Øª PyTorch:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n+>>> inputs = tokenizer(prompt, return_tensors=\"pt\").input_ids\n+```\n+\n+Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~generation.GenerationMixin.generate`] Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ.\n+Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ù„Ø¨Ø§Ø±Ø§Ù…ØªØ±Ø§Øª Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ØŒ Ø±Ø§Ø¬Ø¹ ØµÙØ­Ø© [Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ](../generation_strategies).\n+\n+```py\n+>>> from transformers import AutoModelForCausalLM\n+\n+>>> model = AutoModelForCausalLM.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n+>>> outputs = model.generate(inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n+```\n+\n+ÙÙƒ ØªØ±Ù…ÙŠØ² Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ù†Øµ:\n+\n+```py\n+>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+[\"Somatic hypermutation allows the immune system to react to drugs with the ability to adapt to a different environmental situation. In other words, a system of 'hypermutation' can help the immune system to adapt to a different environmental situation or in some cases even a single life. In contrast, researchers at the University of Massachusetts-Boston have found that 'hypermutation' is much stronger in mice than in humans but can be found in humans, and that it's not completely unknown to the immune system. A study on how the immune system\"]\n+```\n+</pt>\n+<tf>\n+Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒÙ€ TensorFlow tensors:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n+>>> inputs = tokenizer(prompt, return_tensors=\"tf\").input_ids\n+```\n+\n+Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.generation_tf_utils.TFGenerationMixin.generate`] Ù„Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù„Ø®Øµ. Ù„Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ØªÙØ§ØµÙŠÙ„ Ø­ÙˆÙ„ Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ Ø§Ù„Ù…Ø®ØªÙ„ÙØ© ÙˆØ§Ù„Ø¨Ø§Ø±Ø§Ù…ØªØ±Ø§Øª Ù„Ù„ØªØ­ÙƒÙ… ÙÙŠ Ø§Ù„ØªÙˆÙ„ÙŠØ¯ØŒ Ø±Ø§Ø¬Ø¹ ØµÙØ­Ø© [Ø§Ø³ØªØ±Ø§ØªÙŠØ¬ÙŠØ§Øª ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ù†Øµ](../generation_strategies).\n+\n+```py\n+>>> from transformers import TFAutoModelForCausalLM\n+\n+>>> model = TFAutoModelForCausalLM.from_pretrained(\"username/my_awesome_eli5_clm-model\")\n+>>> outputs = model.generate(input_ids=inputs, max_new_tokens=100, do_sample=True, top_k=50, top_p=0.95)\n+```\n+\n+ÙÙƒ ØªØ±Ù…ÙŠØ²  Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆÙ„Ø¯Ø© Ù…Ø±Ø© Ø£Ø®Ø±Ù‰ Ø¥Ù„Ù‰ Ù†Øµ:\n+\n+```py\n+>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+['Somatic hypermutation allows the immune system to detect the presence of other viruses as they become more prevalent. Therefore, researchers have identified a high proportion of human viruses. The proportion of virus-associated viruses in our study increases with age. Therefore, we propose a simple algorithm to detect the presence of these new viruses in our samples as a sign of improved immunity. A first study based on this algorithm, which will be published in Science on Friday, aims to show that this finding could translate into the development of a better vaccine that is more effective for']\n+```\n+</tf>\n+</frameworkcontent>\n\\ No newline at end of file"
        },
        {
            "sha": "e8382927d1e6004190ee66da843e41b684fc7971",
            "filename": "docs/source/ar/tasks/masked_language_modeling.md",
            "status": "added",
            "additions": 442,
            "deletions": 0,
            "changes": 442,
            "blob_url": "https://github.com/huggingface/transformers/blob/741d55237a6d8660c8782966c18119ccd9aac2f7/docs%2Fsource%2Far%2Ftasks%2Fmasked_language_modeling.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/741d55237a6d8660c8782966c18119ccd9aac2f7/docs%2Fsource%2Far%2Ftasks%2Fmasked_language_modeling.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Fmasked_language_modeling.md?ref=741d55237a6d8660c8782966c18119ccd9aac2f7",
            "patch": "@@ -0,0 +1,442 @@\n+<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+-->\n+\n+# Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© (Masked language modeling)\n+\n+[[open-in-colab]]\n+\n+<Youtube id=\"mqElG5QJWUg\"/>\n+\n+ØªØªÙ†Ø¨Ø£ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ø¨Ø±Ù…Ø² Ù…Ù‚Ù†Ø¹ ÙÙŠ ØªØ³Ù„Ø³Ù„ØŒ ÙˆÙŠÙ…ÙƒÙ† Ù„Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø§Ù†ØªØ¨Ø§Ù‡ Ø¥Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø¨Ø´ÙƒÙ„ Ø«Ù†Ø§Ø¦ÙŠ Ø§Ù„Ø§ØªØ¬Ø§Ù‡. Ù‡Ø°Ø§\n+ÙŠØ¹Ù†ÙŠ Ø£Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ø¯ÙŠÙ‡ Ø¥Ù…ÙƒØ§Ù†ÙŠØ© Ø§Ù„ÙˆØµÙˆÙ„ Ø§Ù„ÙƒØ§Ù…Ù„Ø© Ø¥Ù„Ù‰ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© Ø¹Ù„Ù‰ Ø§Ù„ÙŠØ³Ø§Ø± ÙˆØ§Ù„ÙŠÙ…ÙŠÙ†. ØªØ¹Ø¯ Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ù…Ù…ØªØ§Ø²Ø© Ù„Ù„Ù…Ù‡Ø§Ù… Ø§Ù„ØªÙŠ\n+ØªØªØ·Ù„Ø¨ ÙÙ‡Ù…Ù‹Ø§ Ø³ÙŠØ§Ù‚ÙŠÙ‹Ø§ Ø¬ÙŠØ¯Ù‹Ø§ Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ§Ù…Ù„. BERT Ù‡Ùˆ Ù…Ø«Ø§Ù„ Ø¹Ù„Ù‰ Ù†Ù…ÙˆØ°Ø¬ Ù„ØºØ© Ù…Ù‚Ù†Ø¹.\n+\n+Ø³ÙŠÙˆØ¶Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:\n+\n+1. ØªÙƒÙŠÙŠÙ [DistilRoBERTa](https://huggingface.co/distilbert/distilroberta-base) Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© ÙØ±Ø¹ÙŠØ© [r/askscience](https://www.reddit.com/r/askscience/) Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [ELI5](https://huggingface.co/datasets/eli5).\n+2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¯Ø±Ø¨ Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„.\n+\n+<Tip>\n+\n+Ù„Ù…Ø¹Ø±ÙØ© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ù†Ù‰ ÙˆØ§Ù„Ù†Ø³Ø® Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† [ØµÙØ­Ø© Ø§Ù„Ù…Ù‡Ù…Ø©](https://huggingface.co/tasks/fill-mask)\n+\n+</Tip>\n+\n+Ù‚Ø¨Ù„ Ø£Ù† ØªØ¨Ø¯Ø£ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:\n+\n+```bash\n+pip install transformers datasets evaluate\n+```\n+\n+Ù†Ø­Ù† Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Hugging Face Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø´Ø§Ø±ÙƒØ© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯Ù…Ø§ ØªØªÙ… Ù…Ø·Ø§Ù„Ø¨ØªÙƒØŒ Ø£Ø¯Ø®Ù„ Ø±Ù…Ø²Ùƒ Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„:\n+\n+```py\n+>>> from huggingface_hub import notebook_login\n+\n+>>> notebook_login()\n+```\n+\n+## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª ELI5\n+\n+Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ø£ÙˆÙ„ 5000 Ù…Ø«Ø§Ù„ Ù…Ù† Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [ELI5-Category](https://huggingface.co/datasets/eli5_category) Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— Datasets. Ø³ÙŠØ¹Ø·ÙŠÙƒ Ù‡Ø°Ø§ ÙØ±ØµØ© Ù„Ù„ØªØ¬Ø±Ø¨Ø© ÙˆØ§Ù„ØªØ£ÙƒØ¯ Ù…Ù† Ø£Ù† ÙƒÙ„ Ø´ÙŠØ¡ ÙŠØ¹Ù…Ù„ Ù‚Ø¨Ù„ Ù‚Ø¶Ø§Ø¡ Ø§Ù„Ù…Ø²ÙŠØ¯ Ù…Ù† Ø§Ù„ÙˆÙ‚Øª ÙÙŠ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ÙƒØ§Ù…Ù„Ø©.\n+\n+```py\n+>>> from datasets import load_dataset\n+\n+>>> eli5 = load_dataset(\"eli5_category\", split=\"train[:5000]\")\n+```\n+\n+Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª `train` Ø¥Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹ØªÙŠ ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ø®ØªØ¨Ø§Ø± Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`~datasets.Dataset.train_test_split`]:\n+\n+```py\n+>>> eli5 = eli5.train_test_split(test_size=0.2)\n+```\n+\n+Ø«Ù… Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:\n+\n+```py\n+>>> eli5[\"train\"][0]\n+{'q_id': '7h191n',\n+ 'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',\n+ 'selftext': '',\n+ 'category': 'Economics',\n+ 'subreddit': 'explainlikeimfive',\n+ 'answers': {'a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],\n+  'text': [\"The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.\",\n+   'None yet. It has to be reconciled with a vastly different house bill and then passed again.',\n+   'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',\n+   'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],\n+  'score': [21, 19, 5, 3],\n+  'text_urls': [[],\n+   [],\n+   [],\n+   ['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']]},\n+ 'title_urls': ['url'],\n+ 'selftext_urls': ['url']}\n+```\n+\n+Ø¹Ù„Ù‰ Ø§Ù„Ø±ØºÙ… Ù…Ù† Ø£Ù† Ù‡Ø°Ø§ Ù‚Ø¯ ÙŠØ¨Ø¯Ùˆ ÙƒØ«ÙŠØ±Ù‹Ø§ØŒ Ø¥Ù„Ø§ Ø£Ù†Ùƒ Ù…Ù‡ØªÙ… Ø­Ù‚Ù‹Ø§ Ø¨Ø­Ù‚Ù„ `text`. Ù…Ø§ Ù‡Ùˆ Ø±Ø§Ø¦Ø¹ Ø­ÙˆÙ„ Ù…Ù‡Ø§Ù… Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ù‡Ùˆ Ø£Ù†Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ³Ù…ÙŠØ§Øª (ØªÙØ¹Ø±Ù Ø£ÙŠØ¶Ù‹Ø§ Ø¨Ø§Ø³Ù… Ø§Ù„Ù…Ù‡Ù…Ø© ØºÙŠØ± Ø§Ù„Ø®Ø§Ø¶Ø¹Ø© Ù„Ù„Ø¥Ø´Ø±Ø§Ù) Ù„Ø£Ù† Ø§Ù„ÙƒÙ„Ù…Ø© Ø§Ù„ØªØ§Ù„ÙŠØ© *Ù‡ÙŠ* Ø§Ù„ØªØ³Ù…ÙŠØ©.\n+\n+## Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© (Preprocess)\n+\n+<Youtube id=\"8PmhEIXhBvI\"/>\n+\n+Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø©ØŒ ÙØ¥Ù† Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ ØªØ­Ù…ÙŠÙ„ Ù…Ø¹Ø§Ù„Ø¬ DistilRoBERTa Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø­Ù‚Ù„ `text` Ø§Ù„ÙØ±Ø¹ÙŠ:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilroberta-base\")\n+```\n+\n+Ø³ØªÙ„Ø§Ø­Ø¸ Ù…Ù† Ø§Ù„Ù…Ø«Ø§Ù„ Ø£Ø¹Ù„Ø§Ù‡ØŒ Ø£Ù† Ø­Ù‚Ù„ `text` Ù…ÙˆØ¬ÙˆØ¯ Ø¨Ø§Ù„ÙØ¹Ù„ Ø¯Ø§Ø®Ù„ `answers`. Ù‡Ø°Ø§ ÙŠØ¹Ù†ÙŠ Ø£Ù†Ùƒ Ø³ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø­Ù‚Ù„ `text` Ø§Ù„ÙØ±Ø¹ÙŠ Ù…Ù† Ø¨Ù†ÙŠØªÙ‡ Ø§Ù„Ù…Ø¶Ù…Ù†Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø¯Ø§Ù„Ø© [`flatten`](https://huggingface.co/docs/datasets/process#flatten):\n+\n+```py\n+>>> eli5 = eli5.flatten()\n+>>> eli5[\"train\"][0]\n+{'q_id': '7h191n',\n+ 'title': 'What does the tax bill that was passed today mean? How will it affect Americans in each tax bracket?',\n+ 'selftext': '',\n+ 'category': 'Economics',\n+ 'subreddit': 'explainlikeimfive',\n+ 'answers.a_id': ['dqnds8l', 'dqnd1jl', 'dqng3i1', 'dqnku5x'],\n+ 'answers.text': [\"The tax bill is 500 pages long and there were a lot of changes still going on right to the end. It's not just an adjustment to the income tax brackets, it's a whole bunch of changes. As such there is no good answer to your question. The big take aways are: - Big reduction in corporate income tax rate will make large companies very happy. - Pass through rate change will make certain styles of business (law firms, hedge funds) extremely happy - Income tax changes are moderate, and are set to expire (though it's the kind of thing that might just always get re-applied without being made permanent) - People in high tax states (California, New York) lose out, and many of them will end up with their taxes raised.\",\n+  'None yet. It has to be reconciled with a vastly different house bill and then passed again.',\n+  'Also: does this apply to 2017 taxes? Or does it start with 2018 taxes?',\n+  'This article explains both the House and senate bills, including the proposed changes to your income taxes based on your income level. URL_0'],\n+ 'answers.score': [21, 19, 5, 3],\n+ 'answers.text_urls': [[],\n+  [],\n+  [],\n+  ['https://www.investopedia.com/news/trumps-tax-reform-what-can-be-done/']],\n+ 'title_urls': ['url'],\n+ 'selftext_urls': ['url']}\n+```\n+\n+ÙƒÙ„ Ø­Ù‚Ù„ ÙØ±Ø¹ÙŠ Ù‡Ùˆ Ø§Ù„Ø¢Ù† Ø¹Ù…ÙˆØ¯ Ù…Ù†ÙØµÙ„ ÙƒÙ…Ø§ Ù‡Ùˆ Ù…ÙˆØ¶Ø­ Ø¨ÙˆØ§Ø³Ø·Ø© Ø¨Ø§Ø¯Ø¦Ø© `answers`ØŒ ÙˆØ­Ù‚Ù„ `text` Ù‡Ùˆ Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø¢Ù†. Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù†\n+Ù…Ø¹Ø§Ù„Ø¬Ø© ÙƒÙ„ Ø¬Ù…Ù„Ø© Ø¨Ø´ÙƒÙ„ Ù…Ù†ÙØµÙ„ØŒ Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù‚Ø§Ø¦Ù…Ø© Ø¥Ù„Ù‰ Ø³Ù„Ø³Ù„Ø© Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† Ù…Ø¹Ø§Ù„Ø¬ØªÙ‡Ø§ Ø¨Ø´ÙƒÙ„ Ù…Ø´ØªØ±Ùƒ.\n+\n+Ù‡Ù†Ø§ Ø£ÙˆÙ„ Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© Ù„Ø±Ø¨Ø· Ù‚Ø§Ø¦Ù…Ø© Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ù„ÙƒÙ„ Ù…Ø«Ø§Ù„ ÙˆÙ…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù†ØªÙŠØ¬Ø©:\n+\n+```py\n+>>> def preprocess_function(examples):\n+...     return tokenizer([\" \".join(x) for x in examples[\"answers.text\"]])\n+```\n+\n+Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø¯Ø§Ù„Ø© ğŸ¤— Datasets [`~datasets.Dataset.map`]. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø±ÙŠØ¹ Ø¯Ø§Ù„Ø© `map` Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `batched=True` Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¹Ø¯Ø© Ø¹Ù†Ø§ØµØ± ÙÙŠ ÙˆÙ‚Øª ÙˆØ§Ø­Ø¯ØŒ ÙˆØ²ÙŠØ§Ø¯Ø© Ø¹Ø¯Ø¯ Ø§Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `num_proc`. Ø§Ø­Ø°Ù Ø£ÙŠ Ø£Ø¹Ù…Ø¯Ø© ØºÙŠØ± Ø¶Ø±ÙˆØ±ÙŠØ©:\n+\n+```py\n+>>> tokenized_eli5 = eli5.map(\n+...     preprocess_function,\n+...     batched=True,\n+...     num_proc=4,\n+...     remove_columns=eli5[\"train\"].column_names,\n+... )\n+```\n+\n+\n+ØªØ­ØªÙˆÙŠ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù‡Ø°Ù‡ Ø¹Ù„Ù‰ ØªØ³Ù„Ø³Ù„Ø§Øª Ø±Ù…Ø²ÙŠØ©ØŒ ÙˆÙ„ÙƒÙ† Ø¨Ø¹Ø¶Ù‡Ø§ Ø£Ø·ÙˆÙ„ Ù…Ù† Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ù„Ù„Ù†Ù…ÙˆØ°Ø¬.\n+\n+ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ù„Ø¢Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø¯Ø§Ù„Ø© Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ø³Ø¨Ù‚Ø© Ø«Ø§Ù†ÙŠØ© Ù„Ù€:\n+- ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª\n+- ØªÙ‚Ø³ÙŠÙ… Ø§Ù„ØªØ³Ù„Ø³Ù„Ø§Øª Ø§Ù„Ù…Ø¬Ù…Ù‘Ø¹Ø© Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø£Ù‚ØµØ± Ù…Ø­Ø¯Ø¯Ø© Ø¨Ù€ `block_size`ØŒ ÙˆØ§Ù„ØªÙŠ ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ø£Ù‚ØµØ± Ù…Ù† Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø·ÙˆÙ„ Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª ÙˆÙ…Ù†Ø§Ø³Ø¨Ø© Ù„Ø°Ø§ÙƒØ±Ø© GPU.\n+\n+```py\n+>>> block_size = 128\n+\n+>>> def group_texts(examples):\n+...     # ØªØ¬Ù…ÙŠØ¹ Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù†ØµÙˆØµ.\n+...     concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n+...     total_length = len(concatenated_examples[list(examples.keys())[0]])\n+...     # Ù†ØªØ¬Ø§Ù‡Ù„ Ø§Ù„Ø¬Ø²Ø¡ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ Ø§Ù„ØµØºÙŠØ±ØŒ ÙŠÙ…ÙƒÙ†Ù†Ø§ Ø¥Ø¶Ø§ÙØ© Ø§Ù„Ø­Ø´Ùˆ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙŠØ¯Ø¹Ù…Ù‡ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ù‡Ø°Ø§ Ø§Ù„Ø¥Ø³Ù‚Ø§Ø·ØŒ ÙŠÙ…ÙƒÙ†Ùƒ\n+...     # ØªØ®ØµÙŠØµ Ù‡Ø°Ø§ Ø§Ù„Ø¬Ø²Ø¡ Ø­Ø³Ø¨ Ø§Ø­ØªÙŠØ§Ø¬Ø§ØªÙƒ.\n+...     if total_length >= block_size:\n+...         total_length = (total_length // block_size) * block_size\n+...     # ØªÙ‚Ø³ÙŠÙ…Ù‡Ø§ Ø¥Ù„Ù‰ Ø£Ø¬Ø²Ø§Ø¡ Ø¨Ø­Ø¬Ù… block_size.\n+...     result = {\n+...         k: [t[i : i + block_size] for i in range(0, total_length, block_size)]\n+...         for k, t in concatenated_examples.items()\n+...     }\n+...     return result\n+```\n+\n+Ø·Ø¨Ù‚ Ø¯Ø§Ù„Ø© `group_texts` Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§:\n+\n+```py\n+>>> lm_dataset = tokenized_eli5.map(group_texts, batched=True, num_proc=4)\n+```\n+\n+Ø§Ù„Ø¢Ù†ØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`DataCollatorForLanguageModeling`]. Ù…Ù† Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ø£Ù† ØªÙ‚ÙˆÙ… Ø¨Ù€ *Ø§Ù„Ø­Ø´Ùˆ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ* Ù„ÙŠØµÙ„ Ø·ÙˆÙ„Ù‡Ø§ Ø¥Ù„Ù‰ Ø£Ø·ÙˆÙ„ Ø¬Ù…Ù„Ø© ÙÙŠ Ø§Ù„Ø¯ÙØ¹Ø© Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¬Ù…ÙŠØ¹ØŒ Ø¨Ø¯Ù„Ø§Ù‹ Ù…Ù† Ø­Ø´Ùˆ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ Ø¥Ù„Ù‰ Ø§Ù„Ø·ÙˆÙ„ Ø§Ù„Ø£Ù‚ØµÙ‰.\n+\n+<frameworkcontent>\n+<pt>\n+\n+Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…Ø² Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ±Ù…Ø² Ø§Ù„Ø­Ø´Ùˆ ÙˆØ­Ø¯Ø¯ `mlm_probability` Ù„Ø­Ø¬Ø¨ Ø§Ù„Ø±Ù…ÙˆØ² Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ ÙƒÙ„ Ù…Ø±Ø© ØªÙƒØ±Ø± ÙÙŠÙ‡Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n+\n+```py\n+>>> from transformers import DataCollatorForLanguageModeling\n+\n+>>> tokenizer.pad_token = tokenizer.eos_token\n+>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)\n+```\n+</pt>\n+<tf>\n+\n+Ø§Ø³ØªØ®Ø¯Ù… Ø±Ù…Ø² Ù†Ù‡Ø§ÙŠØ© Ø§Ù„ØªØ³Ù„Ø³Ù„ ÙƒØ±Ù…Ø² Ø§Ù„Ø­Ø´Ùˆ ÙˆØ­Ø¯Ø¯ `mlm_probability` Ù„Ø­Ø¬Ø¨ Ø§Ù„Ø±Ù…ÙˆØ² Ø¹Ø´ÙˆØ§Ø¦ÙŠØ§Ù‹ ÙƒÙ„ Ù…Ø±Ø© ØªÙƒØ±Ø± ÙÙŠÙ‡Ø§ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n+\n+```py\n+>>> from transformers import DataCollatorForLanguageModeling\n+\n+>>> data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15, return_tensors=\"tf\")\n+```\n+</tf>\n+</frameworkcontent>\n+\n+## Ø§Ù„ØªØ¯Ø±ÙŠØ¨ (Train)\n+\n+<frameworkcontent>\n+<pt>\n+\n+<Tip>\n+\n+Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`], Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-with-pytorch-trainer)!\n+\n+</Tip>\n+\n+Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ø§Ù„Ø¢Ù† Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ DistilRoBERTa Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`AutoModelForMaskedLM`]:\n+\n+```py\n+>>> from transformers import AutoModelForMaskedLM\n+\n+>>> model = AutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n+```\n+\n+ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ ØªØ¨Ù‚Ù‰ Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª ÙÙ‚Ø·:\n+\n+1. Ø­Ø¯Ø¯ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ ÙÙŠ [`TrainingArguments`]. Ø§Ù„Ù…Ø¹Ù„Ù…Ø© Ø§Ù„ÙˆØ­ÙŠØ¯Ø© Ø§Ù„Ù…Ø·Ù„ÙˆØ¨Ø© Ù‡ÙŠ `output_dir` ÙˆØ§Ù„ØªÙŠ ØªØ­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø­ÙØ¸ Ù†Ù…ÙˆØ°Ø¬Ùƒ. Ø³ØªÙ‚ÙˆÙ… Ø¨Ø¯ÙØ¹ Ù‡Ø°Ø§ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ¹ÙŠÙŠÙ† `push_to_hub=True` (ÙŠØ¬Ø¨ Ø£Ù† ØªÙƒÙˆÙ† Ù…Ø³Ø¬Ù„Ø§Ù‹ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Hugging Face Ù„ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ).\n+2. Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ [`Trainer`] Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø§Øª Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙ…Ø¬Ù…Ù‘Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª.\n+3. Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`~Trainer.train`] Ù„ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ.\n+\n+```py\n+>>> training_args = TrainingArguments(\n+...     output_dir=\"my_awesome_eli5_mlm_model\",\n+...     eval_strategy=\"epoch\",\n+...     learning_rate=2e-5,\n+...     num_train_epochs=3,\n+...     weight_decay=0.01,\n+...     push_to_hub=True,\n+... )\n+\n+>>> trainer = Trainer(\n+...     model=model,\n+...     args=training_args,\n+...     train_dataset=lm_dataset[\"train\"],\n+...     eval_dataset=lm_dataset[\"test\"],\n+...     data_collator=data_collator,\n+...     tokenizer=tokenizer,\n+... )\n+\n+>>> trainer.train()\n+```\n+\n+Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.evaluate`] Ù„ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ù‚ÙŠØ§Ø³\n+    Ø§Ù„Ø­ÙŠØ±Ø©:\n+\n+```py\n+>>> import math\n+\n+>>> eval_results = trainer.evaluate()\n+>>> print(f\"Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")\n+Perplexity: 8.76\n+```\n+\n+Ø«Ù… Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¹Ù„Ù‰ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.push_to_hub`] Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬Ùƒ:\n+\n+```py\n+>>> trainer.push_to_hub()\n+```\n+</pt>\n+<tf>\n+<Tip>\n+\n+Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¯Ù„ÙŠÙ„ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!\n+\n+</Tip>\n+Ù„ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ù…Ø­Ø³Ù†ØŒ ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…ØŒ ÙˆØ¨Ø¹Ø¶ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n+\n+```py\n+>>> from transformers import create_optimizer, AdamWeightDecay\n+\n+>>> optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n+```\n+\n+Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ DistilRoBERTa Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`TFAutoModelForMaskedLM`]:\n+\n+```py\n+>>> from transformers import TFAutoModelForMaskedLM\n+\n+>>> model = TFAutoModelForMaskedLM.from_pretrained(\"distilbert/distilroberta-base\")\n+```\n+\n+Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n+\n+```py\n+>>> tf_train_set = model.prepare_tf_dataset(\n+...     lm_dataset[\"train\"],\n+...     shuffle=True,\n+...     batch_size=16,\n+...     collate_fn=data_collator,\n+... )\n+\n+>>> tf_test_set = model.prepare_tf_dataset(\n+...     lm_dataset[\"test\"],\n+...     shuffle=False,\n+...     batch_size=16,\n+...     collate_fn=data_collator,\n+... )\n+```\n+\n+Ù‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ù†Ù…Ø§Ø°Ø¬ Transformers Ù„Ø¯ÙŠÙ‡Ø§ Ø¬Ù…ÙŠØ¹Ù‡Ø§ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø§ÙØªØ±Ø§Ø¶ÙŠØ© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªÙƒÙ† ØªØ±ÙŠØ¯ Ø°Ù„Ùƒ:\n+\n+```py\n+>>> import tensorflow as tf\n+\n+>>> model.compile(optimizer=optimizer)  # Ù„Ø§ ØªÙˆØ¬Ø¯ Ø­Ø¬Ø© Ù„Ù„Ø®Ø³Ø§Ø±Ø©!\n+```\n+\n+ÙŠÙ…ÙƒÙ† Ø§Ù„Ù‚ÙŠØ§Ù… Ø¨Ø°Ù„Ùƒ Ø¹Ù† Ø·Ø±ÙŠÙ‚ ØªØ­Ø¯ÙŠØ¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆÙ…Ø¹Ø§Ù„Ø¬ Ø§Ù„Ø±Ù…ÙˆØ² ÙÙŠ [`~transformers.PushToHubCallback`]:\n+\n+```py\n+>>> from transformers.keras_callbacks import PushToHubCallback\n+\n+>>> callback = PushToHubCallback(\n+...     output_dir=\"my_awesome_eli5_mlm_model\",\n+...     tokenizer=tokenizer,\n+... )\n+```\n+\n+Ø£Ø®ÙŠØ±Ø§Ù‹ØŒ Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø¹ØµÙˆØ±ØŒ ÙˆØ§Ù„ØªØ¹Ù„ÙŠÙ‚Ø§Øª Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ù„ØªØ¹Ø¯ÙŠÙ„ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n+\n+```py\n+>>> model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=3, callbacks=[callback])\n+```\n+\n+Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n+</tf>\n+</frameworkcontent>\n+\n+<Tip>\n+\n+Ù„Ù…Ø«Ø§Ù„ Ø£ÙƒØ«Ø± ØªÙØµÙŠÙ„Ø§Ù‹ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„Ù†Ù…Ø°Ø¬Ø© Ø§Ù„Ù„ØºÙˆÙŠØ© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø©ØŒ Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙØªØ± Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„\n+[Ø¯ÙØªØ± PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling.ipynb)\n+Ø£Ùˆ [Ø¯ÙØªØ± TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/language_modeling-tf.ipynb).\n+\n+</Tip>\n+\n+## Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„\n+\n+Ø±Ø§Ø¦Ø¹ØŒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ù‚Ù…Øª Ø¨ØªØ¹Ø¯ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„!\n+\n+Ø¬Ù‡Ù‘Ø² Ø¨Ø¹Ø¶ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø£Ù† ÙŠÙ…Ù„Ø£ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„ÙØ±Ø§ØºØ§Øª ÙÙŠÙ‡Ø§ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ø±Ù…Ø² Ø§Ù„Ø®Ø§Øµ `<mask>` Ù„Ù„Ø¥Ø´Ø§Ø±Ø© Ø¥Ù„Ù‰ Ø§Ù„ÙØ±Ø§Øº:\n+\n+```py\n+>>> text = \"The Milky Way is a <mask> galaxy.\"\n+```\n+\n+Ø£Ø¨Ø³Ø· Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¬Ø±Ø¨Ø© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø§Ù„Ù…Ø¹Ø¯Ù„ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù‡ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ ÙÙŠ [`pipeline`]. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ ÙƒØ§Ø¦Ù†  `pipeline` Ù„Ù…Ù„Ø¡ Ø§Ù„ÙØ±Ø§Øº Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙˆÙ…Ø±Ø± Ù†ØµÙƒ Ø¥Ù„ÙŠÙ‡. Ø¥Ø°Ø§ Ø£Ø±Ø¯ØªØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…Ø¹Ù„Ù…Ø© `top_k` Ù„ØªØ­Ø¯ÙŠØ¯ Ø¹Ø¯Ø¯ Ø§Ù„ØªÙ†Ø¨Ø¤Ø§Øª Ø§Ù„ØªÙŠ ØªØ±ÙŠØ¯ Ø¥Ø±Ø¬Ø§Ø¹Ù‡Ø§:\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> mask_filler = pipeline(\"fill-mask\", \"username/my_awesome_eli5_mlm_model\")\n+>>> mask_filler(text, top_k=3)\n+[{'score': 0.5150994658470154,\n+  'token': 21300,\n+  'token_str': ' spiral',\n+  'sequence': 'The Milky Way is a spiral galaxy.'},\n+ {'score': 0.07087188959121704,\n+  'token': 2232,\n+  'token_str': ' massive',\n+  'sequence': 'The Milky Way is a massive galaxy.'},\n+ {'score': 0.06434620916843414,\n+  'token': 650,\n+  'token_str': ' small',\n+  'sequence': 'The Milky Way is a small galaxy.'}]\n+```\n+\n+<frameworkcontent>\n+<pt>\n+Ù‚Ù… Ø¨ØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒÙ…ØªØ¬Ù‡Ø§Øª PyTorch. Ø³ØªØ­ØªØ§Ø¬ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¶Ø¹ Ø±Ù…Ø² `<mask>`:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n+>>> inputs = tokenizer(text, return_tensors=\"pt\")\n+>>> mask_token_index = torch.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[1]\n+```\n+\n+Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `logits` Ù„Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ù‚Ù†Ø¹:\n+\n+```py\n+>>> from transformers import AutoModelForMaskedLM\n+\n+>>> model = AutoModelForMaskedLM.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n+>>> logits = model(**inputs).logits\n+>>> mask_token_logits = logits[0, mask_token_index, :]\n+```\n+\n+Ø«Ù… Ù‚Ù… Ø¨Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰ ÙˆØ·Ø¨Ø§Ø¹ØªÙ‡Ø§:\n+\n+```py\n+>>> top_3_tokens = torch.topk(mask_token_logits, 3, dim=1).indices[0].tolist()\n+\n+>>> for token in top_3_tokens:\n+...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))\n+The Milky Way is a spiral galaxy.\n+The Milky Way is a massive galaxy.\n+The Milky Way is a small galaxy.\n+```\n+</pt>\n+<tf>\n+Ù‚Ù… Ø¨ØªÙ‚Ø³ÙŠÙ… Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ Ø±Ù…ÙˆØ² ÙˆØ¥Ø±Ø¬Ø§Ø¹ `input_ids` ÙƒÙ€ TensorFlow tensors. Ø³ØªØ­ØªØ§Ø¬ Ø£ÙŠØ¶Ù‹Ø§ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ Ù…ÙˆØ¶Ø¹ Ø±Ù…Ø² `<mask>`:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n+>>> inputs = tokenizer(text, return_tensors=\"tf\")\n+>>> mask_token_index = tf.where(inputs[\"input_ids\"] == tokenizer.mask_token_id)[0, 1]\n+```\n+\n+Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `logits` Ù„Ù„Ø±Ù…Ø² Ø§Ù„Ù…Ù‚Ù†Ø¹:\n+\n+```py\n+>>> from transformers import TFAutoModelForMaskedLM\n+\n+>>> model = TFAutoModelForMaskedLM.from_pretrained(\"username/my_awesome_eli5_mlm_model\")\n+>>> logits = model(**inputs).logits\n+>>> mask_token_logits = logits[0, mask_token_index, :]\n+```\n+\n+Ø«Ù… Ù‚Ù… Ø¨Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ø±Ù…ÙˆØ² Ø§Ù„Ø«Ù„Ø§Ø«Ø© Ø§Ù„Ù…Ù‚Ù†Ø¹Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰ ÙˆØ·Ø¨Ø§Ø¹ØªÙ‡Ø§:\n+\n+```py\n+>>> top_3_tokens = tf.math.top_k(mask_token_logits, 3).indices.numpy()\n+\n+>>> for token in top_3_tokens:\n+...     print(text.replace(tokenizer.mask_token, tokenizer.decode([token])))\n+The Milky Way is a spiral galaxy.\n+The Milky Way is a massive galaxy.\n+The Milky Way is a small galaxy.\n+```\n+</tf>\n+</frameworkcontent>\n\\ No newline at end of file"
        },
        {
            "sha": "a98964957b47c92bc4a43db701db99b650a2dfce",
            "filename": "docs/source/ar/tasks/sequence_classification.md",
            "status": "added",
            "additions": 387,
            "deletions": 0,
            "changes": 387,
            "blob_url": "https://github.com/huggingface/transformers/blob/741d55237a6d8660c8782966c18119ccd9aac2f7/docs%2Fsource%2Far%2Ftasks%2Fsequence_classification.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/741d55237a6d8660c8782966c18119ccd9aac2f7/docs%2Fsource%2Far%2Ftasks%2Fsequence_classification.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Far%2Ftasks%2Fsequence_classification.md?ref=741d55237a6d8660c8782966c18119ccd9aac2f7",
            "patch": "@@ -0,0 +1,387 @@\n+<!--Copyright 2022 The HuggingFace Team. All rights reserved.\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+http://www.apache.org/licenses/LICENSE-2.0\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+âš ï¸ Note that this file is in Markdown but contain specific syntax for our doc-builder (similar to MDX) that may not be\n+rendered properly in your Markdown viewer.\n+-->\n+\n+# ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ(Text classification)\n+\n+[[open-in-colab]]\n+\n+<Youtube id=\"leNG9fN9FQU\"/>\n+\n+ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ Ù‡Ùˆ Ù…Ù‡Ù…Ø© NLP Ø´Ø§Ø¦Ø¹Ø© Ø­ÙŠØ« ÙŠÙØ¹ÙŠÙ‘Ù† ØªØµÙ†ÙŠÙÙ‹Ø§ Ø£Ùˆ ÙØ¦Ø© Ù„Ù„Ù†Øµ. ØªØ³ØªØ®Ø¯Ù… Ø¨Ø¹Ø¶ Ø£ÙƒØ¨Ø± Ø§Ù„Ø´Ø±ÙƒØ§Øª ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµ ÙÙŠ Ø§Ù„Ø¥Ù†ØªØ§Ø¬ Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© ÙˆØ§Ø³Ø¹Ø© Ù…Ù† Ø§Ù„ØªØ·Ø¨ÙŠÙ‚Ø§Øª Ø§Ù„Ø¹Ù…Ù„ÙŠØ©. Ø£Ø­Ø¯ Ø£ÙƒØ«Ø± Ø£Ø´ÙƒØ§Ù„ ØªØµÙ†ÙŠÙ Ø§Ù„Ù†Øµ Ø´ÙŠÙˆØ¹Ù‹Ø§ Ù‡Ùˆ ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø±ØŒ ÙˆØ§Ù„Ø°ÙŠ ÙŠÙ‚ÙˆÙ… Ø¨ØªØ¹ÙŠÙŠÙ† ØªØ³Ù…ÙŠØ© Ù…Ø«Ù„ ğŸ™‚ Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©ØŒ ğŸ™ Ø³Ù„Ø¨ÙŠØ©ØŒ Ø£Ùˆ ğŸ˜ Ù…Ø­Ø§ÙŠØ¯Ø© Ù„ØªØ³Ù„Ø³Ù„ Ù†ØµÙŠ.\n+\n+Ø³ÙŠÙˆØ¶Ø­ Ù„Ùƒ Ù‡Ø°Ø§ Ø§Ù„Ø¯Ù„ÙŠÙ„ ÙƒÙŠÙÙŠØ©:\n+\n+1. Ø¶Ø¨Ø· [DistilBERT](https://huggingface.co/distilbert/distilbert-base-uncased) Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª [IMDb](https://huggingface.co/datasets/imdb) Ù„ØªØ­Ø¯ÙŠØ¯ Ù…Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù†Øª Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ÙÙŠÙ„Ù… Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ© Ø£Ùˆ Ø³Ù„Ø¨ÙŠØ©.\n+2. Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ø¶Ø¨Ø· Ø§Ù„Ø¯Ù‚ÙŠÙ‚ Ù„Ù„ØªÙ†Ø¨Ø¤.\n+\n+<Tip>\n+\n+Ù„Ø±Ø¤ÙŠØ© Ø¬Ù…ÙŠØ¹ Ø§Ù„Ø¨Ù†Ù‰ ÙˆÙ†Ù‚Ø§Ø· Ø§Ù„ØªØ­Ù‚Ù‚ Ø§Ù„Ù…ØªÙˆØ§ÙÙ‚Ø© Ù…Ø¹ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù†ÙˆØµÙŠ Ø¨Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† [ØµÙØ­Ø© Ø§Ù„Ù…Ù‡Ù…Ø©](https://huggingface.co/tasks/text-classification).\n+\n+</Tip>\n+\n+Ù‚Ø¨Ù„ Ø£Ù† ØªØ¨Ø¯Ø£ØŒ ØªØ£ÙƒØ¯ Ù…Ù† ØªØ«Ø¨ÙŠØª Ø¬Ù…ÙŠØ¹ Ø§Ù„Ù…ÙƒØªØ¨Ø§Øª Ø§Ù„Ø¶Ø±ÙˆØ±ÙŠØ©:\n+\n+```bash\n+pip install transformers datasets evaluate accelerate\n+```\n+\n+Ù†Ø­Ù† Ù†Ø´Ø¬Ø¹Ùƒ Ø¹Ù„Ù‰ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Ø­Ø³Ø§Ø¨ Hugging Face Ø§Ù„Ø®Ø§Øµ Ø¨Ùƒ Ø­ØªÙ‰ ØªØªÙ…ÙƒÙ† Ù…Ù† ØªØ­Ù…ÙŠÙ„ ÙˆÙ…Ø´Ø§Ø±ÙƒØ© Ù†Ù…ÙˆØ°Ø¬Ùƒ Ù…Ø¹ Ø§Ù„Ù…Ø¬ØªÙ…Ø¹. Ø¹Ù†Ø¯ Ø§Ù„Ù…Ø·Ø§Ù„Ø¨Ø©ØŒ Ø£Ø¯Ø®Ù„ Ø±Ù…Ø²Ùƒ Ù„ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„:\n+\n+```py\n+>>> from huggingface_hub import notebook_login\n+\n+>>> notebook_login()\n+```\n+\n+## ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª IMDb\n+\n+Ø§Ø¨Ø¯Ø£ Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø¨ÙŠØ§Ù†Ø§Øª IMDb Ù…Ù† Ù…ÙƒØªØ¨Ø© ğŸ¤— Datasets:\n+\n+```py\n+>>> from datasets import load_dataset\n+\n+>>> imdb = load_dataset(\"imdb\")\n+```\n+\n+Ø«Ù… Ø£Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„:\n+\n+```py\n+>>> imdb[\"test\"][0]\n+{\n+    \"label\": 0,\n+    \"text\": \"I love sci-fi and am willing to put up with a lot. Sci-fi movies/TV are usually underfunded, under-appreciated and misunderstood. I tried to like this, I really did, but it is to good TV sci-fi as Babylon 5 is to Star Trek (the original). Silly prosthetics, cheap cardboard sets, stilted dialogues, CG that doesn't match the background, and painfully one-dimensional characters cannot be overcome with a 'sci-fi' setting. (I'm sure there are those of you out there who think Babylon 5 is good sci-fi TV. It's not. It's clichÃ©d and uninspiring.) While US viewers might like emotion and character development, sci-fi is a genre that does not take itself seriously (cf. Star Trek). It may treat important issues, yet not as a serious philosophy. It's really difficult to care about the characters here as they are not simply foolish, just missing a spark of life. Their actions and reactions are wooden and predictable, often painful to watch. The makers of Earth KNOW it's rubbish as they have to always say \\\"Gene Roddenberry's Earth...\\\" otherwise people would not continue watching. Roddenberry's ashes must be turning in their orbit as this dull, cheap, poorly edited (watching it without advert breaks really brings this home) trudging Trabant of a show lumbers into space. Spoiler. So, kill off a main character. And then bring him back as another actor. Jeeez! Dallas all over again.\",\n+}\n+```\n+\n+Ù‡Ù†Ø§Ùƒ Ø­Ù‚ÙˆÙ„Ø§Ù† ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø¬Ù…ÙˆØ¹Ø© Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n+\n+- `text`: Ù†Øµ Ù…Ø±Ø§Ø¬Ø¹Ø© Ø§Ù„ÙÙŠÙ„Ù….\n+- `label`: Ù‚ÙŠÙ…Ø© Ø¥Ù…Ø§ `0` Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø³Ù„Ø¨ÙŠØ© Ø£Ùˆ `1` Ù„Ù…Ø±Ø§Ø¬Ø¹Ø© Ø¥ÙŠØ¬Ø§Ø¨ÙŠØ©.\n+\n+## Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…Ø³Ø¨Ù‚Ø©(Preprocess)\n+\n+Ø§Ù„Ø®Ø·ÙˆØ© Ø§Ù„ØªØ§Ù„ÙŠØ© Ù‡ÙŠ ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ù…ÙØ¬Ø²ÙÙ‘Ø¦ Ø§Ù„Ù†Øµ DistilBERT Ù„ØªÙ‡ÙŠØ¦Ø© Ù„Ø­Ù‚Ù„ `text`:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")\n+```\n+\n+Ø£Ù†Ø´Ø¦ Ø¯Ø§Ù„Ø© Ù„ØªÙ‡ÙŠØ¦Ø© Ø­Ù‚Ù„ `text` ÙˆØªÙ‚ØµÙŠØ± Ø§Ù„Ø³Ù„Ø§Ø³Ù„ Ø§Ù„Ù†ØµÙŠØ© Ø¨Ø­ÙŠØ« Ù„Ø§ ÙŠØªØ¬Ø§ÙˆØ² Ø·ÙˆÙ„Ù‡Ø§ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ø¥Ø¯Ø®Ø§Ù„Ø§Øª DistilBERT:\n+\n+```py\n+>>> def preprocess_function(examples):\n+...     return tokenizer(examples[\"text\"], truncation=True)\n+```\n+\n+Ù„ØªØ·Ø¨ÙŠÙ‚ Ø¯Ø§Ù„Ø© Ø§Ù„ØªÙ‡ÙŠØ¦Ø© Ø¹Ù„Ù‰ Ù…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø£ÙƒÙ…Ù„Ù‡Ø§ØŒ Ø§Ø³ØªØ®Ø¯Ù… Ø¯Ø§Ù„Ø© ğŸ¤— Datasets [`~datasets.Dataset.map`] . ÙŠÙ…ÙƒÙ†Ùƒ ØªØ³Ø±ÙŠØ¹ `map` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `batched=True` Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø¯ÙØ¹Ø§Øª Ù…Ù† Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª:\n+\n+```py\n+tokenized_imdb = imdb.map(preprocess_function, batched=True)\n+```\n+\n+Ø§Ù„Ø¢Ù† Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø¯ÙØ¹Ø© Ù…Ù† Ø§Ù„Ø£Ù…Ø«Ù„Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`DataCollatorWithPadding`].  Ø§Ù„Ø£ÙƒØ«Ø± ÙƒÙØ§Ø¡Ø© Ù‡Ùˆ Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø­Ø´Ùˆ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ù„Ø¬Ø¹Ù„ Ø§Ù„Ø¬Ù…Ù„ Ù…ØªØ³Ø§ÙˆÙŠØ© ÙÙŠ Ø§Ù„Ø·ÙˆÙ„ Ø¯Ø§Ø®Ù„ ÙƒÙ„ Ø¯ÙØ¹Ø©ØŒ Ø¨Ø¯Ù„Ù‹Ø§ Ù…Ù† Ø­Ø´Ùˆ ÙƒØ§Ù…Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ø­Ø¯ Ø§Ù„Ø£Ù‚ØµÙ‰ Ù„Ù„Ø·ÙˆÙ„.\n+\n+<frameworkcontent>\n+<pt>\n+\n+```py\n+>>> from transformers import DataCollatorWithPadding\n+\n+>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n+```\n+</pt>\n+<tf>\n+\n+```py\n+>>> from transformers import DataCollatorWithPadding\n+\n+>>> data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")\n+```\n+</tf>\n+</frameworkcontent>\n+\n+## Ø§Ù„ØªÙ‚ÙŠÙŠÙ…(Evaluate)\n+\n+ÙŠÙØ¹Ø¯Ù‘ ØªØ¶Ù…ÙŠÙ† Ù…Ù‚ÙŠØ§Ø³ Ø£Ø«Ù†Ø§Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù…ÙÙŠØ¯Ù‹Ø§ Ù„ØªÙ‚ÙŠÙŠÙ… Ø£Ø¯Ø§Ø¡ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ Ø·Ø±ÙŠÙ‚Ø© ØªÙ‚ÙŠÙŠÙ… Ø¨Ø³Ø±Ø¹Ø© Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ù…ÙƒØªØ¨Ø© ğŸ¤— [Evaluate](https://huggingface.co/docs/evaluate/index) . Ø¨Ø§Ù„Ù†Ø³Ø¨Ø© Ù„Ù‡Ø°Ù‡ Ø§Ù„Ù…Ù‡Ù…Ø©ØŒ Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ Ù…Ù‚ÙŠØ§Ø³ [Ø§Ù„Ø¯Ù‚Ø©](https://huggingface.co/spaces/evaluate-metric/accuracy) (Ø±Ø§Ø¬Ø¹ Ø¬ÙˆÙ„Ø© ğŸ¤— Evaluate [Ø§Ù„Ø³Ø±ÙŠØ¹Ø©](https://huggingface.co/docs/evaluate/a_quick_tour) Ù„Ù…Ø¹Ø±ÙØ© Ø§Ù„Ù…Ø²ÙŠØ¯ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© ØªØ­Ù…ÙŠÙ„ ÙˆØ­Ø³Ø§Ø¨ Ù…Ù‚ÙŠØ§Ø³):\n+\n+```py\n+>>> import evaluate\n+\n+>>> accuracy = evaluate.load(\"accuracy\")\n+```\n+\n+Ø«Ù… Ø£Ù†Ø´Ø¦ Ø¯Ø§Ù„Ø© ØªÙ‚ÙˆÙ… Ø¨ØªÙ…Ø±ÙŠØ± ØªÙ†Ø¨Ø¤Ø§ØªÙƒ ÙˆØªØµÙ†ÙŠÙØ§ØªÙƒ Ø¥Ù„Ù‰ [`~evaluate.EvaluationModule.compute`] Ù„Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø©:\n+\n+```py\n+>>> import numpy as np\n+\n+>>> def compute_metrics(eval_pred):\n+...     predictions, labels = eval_pred\n+...     predictions = np.argmax(predictions, axis=1)\n+...     return accuracy.compute(predictions=predictions, references=labels)\n+```\n+\n+Ø¯Ø§Ù„Ø© `compute_metrics` Ø¬Ø§Ù‡Ø²Ø© Ø§Ù„Ø¢Ù†ØŒ ÙˆØ³ØªØ¹ÙˆØ¯ Ø¥Ù„ÙŠÙ‡Ø§ Ø¹Ù†Ø¯ Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØªØ¯Ø±ÙŠØ¨.\n+\n+## Ø§Ù„ØªØ¯Ø±ÙŠØ¨(Train)\n+\n+Ù‚Ø¨Ù„ Ø£Ù† ØªØ¨Ø¯Ø£ ÙÙŠ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ Ø®Ø±ÙŠØ·Ø© Ù…Ù† Ø§Ù„Ù…Ø¹Ø±ÙØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø© Ø¥Ù„Ù‰ ØªØ³Ù…ÙŠØ§ØªÙ‡Ø§ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… `id2label` Ùˆ `label2id`:\n+\n+```py\n+>>> id2label = {0: \"NEGATIVE\", 1: \"POSITIVE\"}\n+>>> label2id = {\"NEGATIVE\": 0, \"POSITIVE\": 1}\n+```\n+\n+<frameworkcontent>\n+<pt>\n+<Tip>\n+\n+Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¯Ù‚ÙŠÙ‚ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`Trainer`], ÙØ§Ù„Ù‚ Ù†Ø¸Ø±Ø© Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-with-pytorch-trainer)!\n+\n+</Tip>\n+\n+Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ø§Ù„Ø¢Ù† Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨ØªØ­Ù…ÙŠÙ„ DistilBERT Ù…Ø¹ [`AutoModelForSequenceClassification`] Ø¬Ù†Ø¨Ù‹Ø§ Ø¥Ù„Ù‰ Ø¬Ù†Ø¨ Ù…Ø¹ Ø¹Ø¯Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©ØŒ ÙˆØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ø®Ø±Ø§Ø¦Ø·:\n+\n+```py\n+>>> from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n+\n+>>> model = AutoModelForSequenceClassification.from_pretrained(\n+...     \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n+... )\n+```\n+\n+ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ù…Ø±Ø­Ù„Ø©ØŒ Ù‡Ù†Ø§Ùƒ Ø«Ù„Ø§Ø« Ø®Ø·ÙˆØ§Øª ÙÙ‚Ø· Ù…ØªØ¨Ù‚ÙŠØ©:\n+\n+1.  Ø­Ø¯Ø¯ Ù…ÙØ¹Ø§Ù…ÙÙ„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙÙŠ [`TrainingArguments`]. Ø§Ù„Ù…ÙØ¹Ø§Ù…Ù„ Ø§Ù„Ù…Ø·Ù„ÙˆØ¨ Ø§Ù„ÙˆØ­ÙŠØ¯ Ù‡Ùˆ `output_dir`ØŒ Ù„ØªØ­Ø¯ÙŠØ¯ Ù…ÙƒØ§Ù† Ø­ÙØ¸ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬. ÙŠÙ…ÙƒÙ†Ùƒ Ø±ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø¥Ù„Ù‰ Hub Ø¨ØªØ¹ÙŠÙŠÙ† `push_to_hub=True` (ÙŠØ¬Ø¨ ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø¥Ù„Ù‰ Hugging Face Ù„Ø±ÙØ¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬). Ø³ÙŠÙ‚ÙˆÙ… `Trainer` Ø¨ØªÙ‚ÙŠÙŠÙ… Ø§Ù„Ø¯Ù‚Ø© ÙˆØ­ÙØ¸ Ù†Ù‚Ø§Ø· Ø§Ù„ØªØ­Ù‚Ù‚ ÙÙŠ Ù†Ù‡Ø§ÙŠØ© ÙƒÙ„ Ø­Ù‚Ø¨Ø©.\n+2.  Ù…Ø±Ø± Ù…ÙØ¹Ø§Ù…ÙÙ„Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ø¥Ù„Ù‰ `Trainer` Ù…Ø¹ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ØŒ ÙˆÙ…Ø¬Ù…ÙˆØ¹Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆØ§Ù„Ù…Ø­Ù„Ù„ Ø§Ù„Ù„ØºÙˆÙŠØŒ ÙˆÙ…ÙØ¬Ù…ÙÙ‘Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§ØªØŒ ÙˆÙˆØ¸ÙŠÙØ© `compute_metrics`.\n+3.  Ø§Ø³ØªØ¯Ø¹Ù [`~Trainer.train`] Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬.\n+\n+```py\n+>>> training_args = TrainingArguments(\n+...     output_dir=\"my_awesome_model\",\n+...     learning_rate=2e-5,\n+...     per_device_train_batch_size=16,\n+...     per_device_eval_batch_size=16,\n+...     num_train_epochs=2,\n+...     weight_decay=0.01,\n+...     eval_strategy=\"epoch\",\n+...     save_strategy=\"epoch\",\n+...     load_best_model_at_end=True,\n+...     push_to_hub=True,\n+... )\n+\n+>>> trainer = Trainer(\n+...     model=model,\n+...     args=training_args,\n+...     train_dataset=tokenized_imdb[\"train\"],\n+...     eval_dataset=tokenized_imdb[\"test\"],\n+...     processing_class=tokenizer,\n+...     data_collator=data_collator,\n+...     compute_metrics=compute_metrics,\n+... )\n+\n+>>> trainer.train()\n+```\n+\n+<Tip>\n+\n+ÙŠØ³ØªØ®Ø¯Ù… [`Trainer`] Ø§Ù„Ø­Ø´Ùˆ Ø§Ù„Ø¯ÙŠÙ†Ø§Ù…ÙŠÙƒÙŠ Ø§ÙØªØ±Ø§Ø¶ÙŠÙ‹Ø§ Ø¹Ù†Ø¯ ØªÙ…Ø±ÙŠØ± `tokenizer` Ø¥Ù„ÙŠÙ‡. ÙÙŠ Ù‡Ø°Ù‡ Ø§Ù„Ø­Ø§Ù„Ø©ØŒ  Ù„Ø§ ØªØ­ØªØ§Ø¬ Ù„ØªØ­Ø¯ÙŠØ¯ Ù…ÙØ¬Ù…ÙÙ‘Ø¹ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª ØµØ±Ø§Ø­Ø©Ù‹. \n+\n+</Tip>\n+\n+Ø¨Ø¹Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ Ø´Ø§Ø±Ùƒ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¹Ù„Ù‰ Hub Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ø·Ø±ÙŠÙ‚Ø© [`~transformers.Trainer.push_to_hub`] Ù„ÙŠØ³ØªØ®Ø¯Ù…Ù‡ Ø§Ù„Ø¬Ù…ÙŠØ¹:\n+\n+```py\n+>>> trainer.push_to_hub()\n+```\n+</pt>\n+<tf>\n+<Tip>\n+\n+Ø¥Ø°Ø§ Ù„Ù… ØªÙƒÙ† Ø¹Ù„Ù‰ Ø¯Ø±Ø§ÙŠØ© Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… KerasØŒ Ù‚Ù… Ø¨Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ø¨Ø±Ù†Ø§Ù…Ø¬ Ø§Ù„ØªØ¹Ù„ÙŠÙ…ÙŠ Ø§Ù„Ø£Ø³Ø§Ø³ÙŠ [Ù‡Ù†Ø§](../training#train-a-tensorflow-model-with-keras)!\n+\n+</Tip>\n+Ù„Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ ÙÙŠ TensorFlowØŒ Ø§Ø¨Ø¯Ø£ Ø¨Ø¥Ø¹Ø¯Ø§Ø¯ Ø¯Ø§Ù„Ø© Ø§Ù„Ù…Ø­Ø³Ù†ØŒ ÙˆØ¬Ø¯ÙˆÙ„ Ù…Ø¹Ø¯Ù„ Ø§Ù„ØªØ¹Ù„Ù…ØŒ ÙˆØ¨Ø¹Ø¶ Ù…Ø¹Ù„Ù…Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨:\n+\n+```py\n+>>> from transformers import create_optimizer\n+>>> import tensorflow as tf\n+\n+>>> batch_size = 16\n+>>> num_epochs = 5\n+>>> batches_per_epoch = len(tokenized_imdb[\"train\"]) // batch_size\n+>>> total_train_steps = int(batches_per_epoch * num_epochs)\n+>>> optimizer, schedule = create_optimizer(init_lr=2e-5, num_warmup_steps=0, num_train_steps=total_train_steps)\n+```\n+\n+Ø«Ù… ÙŠÙ…ÙƒÙ†Ùƒ ØªØ­Ù…ÙŠÙ„ DistilBERT Ù…Ø¹ [`TFAutoModelForSequenceClassification`] Ø¨Ø§Ù„Ø¥Ø¶Ø§ÙØ© Ø¥Ù„Ù‰ Ø¹Ø¯Ø¯ Ø§Ù„ØªØµÙ†ÙŠÙØ§Øª Ø§Ù„Ù…ØªÙˆÙ‚Ø¹Ø©ØŒ ÙˆØªØ¹ÙŠÙŠÙ†Ø§Øª Ø§Ù„ØªØ³Ù…ÙŠØ§Øª:\n+\n+```py\n+>>> from transformers import TFAutoModelForSequenceClassification\n+\n+>>> model = TFAutoModelForSequenceClassification.from_pretrained(\n+...     \"distilbert/distilbert-base-uncased\", num_labels=2, id2label=id2label, label2id=label2id\n+... )\n+```\n+\n+Ù‚Ù… Ø¨ØªØ­ÙˆÙŠÙ„ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§ØªÙƒ Ø¥Ù„Ù‰ ØªÙ†Ø³ÙŠÙ‚ `tf.data.Dataset` Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`~transformers.TFPreTrainedModel.prepare_tf_dataset`]:\n+\n+```py\n+>>> tf_train_set = model.prepare_tf_dataset(\n+...     tokenized_imdb[\"train\"],\n+...     shuffle=True,\n+...     batch_size=16,\n+...     collate_fn=data_collator,\n+... )\n+\n+>>> tf_validation_set = model.prepare_tf_dataset(\n+...     tokenized_imdb[\"test\"],\n+...     shuffle=False,\n+...     batch_size=16,\n+...     collate_fn=data_collator,\n+... )\n+```\n+\n+Ù‚Ù… Ø¨ØªÙ‡ÙŠØ¦Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ù„Ù„ØªØ¯Ø±ÙŠØ¨ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [`compile`](https://keras.io/api/models/model_training_apis/#compile-method). Ù„Ø§Ø­Ø¸ Ø£Ù† Ø¬Ù…ÙŠØ¹ Ù†Ù…Ø§Ø°Ø¬ Transformers Ù„Ø¯ÙŠÙ‡Ø§ Ø¯Ø§Ù„Ø© Ø®Ø³Ø§Ø±Ø© Ø°Ø§Øª ØµÙ„Ø© Ø¨Ø§Ù„Ù…Ù‡Ù…Ø© Ø¨Ø´ÙƒÙ„ Ø§ÙØªØ±Ø§Ø¶ÙŠØŒ Ù„Ø°Ù„Ùƒ Ù„Ø§ ØªØ­ØªØ§Ø¬ Ø¥Ù„Ù‰ ØªØ­Ø¯ÙŠØ¯ ÙˆØ§Ø­Ø¯Ø© Ù…Ø§ Ù„Ù… ØªØ±ØºØ¨ ÙÙŠ Ø°Ù„Ùƒ:\n+\n+```py\n+>>> import tensorflow as tf\n+\n+>>> model.compile(optimizer=optimizer)  # No loss argument!\n+```\n+\n+Ø¢Ø®Ø± Ø£Ù…Ø±ÙŠÙ† ÙŠØ¬Ø¨ Ø¥Ø¹Ø¯Ø§Ø¯Ù‡Ù…Ø§ Ù‚Ø¨Ù„ Ø¨Ø¯Ø¡ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ Ù‡Ùˆ Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø© Ù…Ù† Ø§Ù„ØªÙˆÙ‚Ø¹Ø§ØªØŒ ÙˆØªÙˆÙÙŠØ± Ø·Ø±ÙŠÙ‚Ø© Ù„Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ Ø¥Ù„Ù‰ Hub. ÙŠØªÙ… Ø°Ù„Ùƒ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… [Keras callbacks](../main_classes/keras_callbacks).\n+\n+Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ø¯Ø§Ù„Ø© `compute_metrics` Ø§Ù„Ø®Ø§ØµØ© Ø¨Ùƒ Ø¥Ù„Ù‰ [`~transformers.KerasMetricCallback`]:\n+\n+```py\n+>>> from transformers.keras_callbacks import KerasMetricCallback\n+\n+>>> metric_callback = KerasMetricCallback(metric_fn=compute_metrics, eval_dataset=tf_validation_set)\n+```\n+\n+Ø­Ø¯Ø¯ Ù…ÙƒØ§Ù† Ø¯ÙØ¹ Ù†Ù…ÙˆØ°Ø¬Ùƒ ÙˆØ§Ù„Ù…Ø¬Ø²Ø¦ Ø§Ù„Ù„ØºÙˆÙŠ ÙÙŠ [`~transformers.PushToHubCallback`]:\n+\n+```py\n+>>> from transformers.keras_callbacks import PushToHubCallback\n+\n+>>> push_to_hub_callback = PushToHubCallback(\n+...     output_dir=\"my_awesome_model\",\n+...     tokenizer=tokenizer,\n+... )\n+```\n+\n+Ø«Ù… Ø§Ø¬Ù…Ø¹ Ø§Ù„Ø§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§Øª Ù…Ø¹Ù‹Ø§:\n+\n+```py\n+>>> callbacks = [metric_callback, push_to_hub_callback]\n+```\n+\n+Ø£Ø®ÙŠØ±Ù‹Ø§ØŒ Ø£Ù†Øª Ù…Ø³ØªØ¹Ø¯ Ù„Ø¨Ø¯Ø¡ ØªØ¯Ø±ÙŠØ¨ Ù†Ù…ÙˆØ°Ø¬Ùƒ! Ù‚Ù… Ø¨Ø§Ø³ØªØ¯Ø¹Ø§Ø¡ [`fit`](https://keras.io/api/models/model_training_apis/#fit-method) Ù…Ø¹ Ù…Ø¬Ù…ÙˆØ¹Ø§Øª Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„ØªØ¯Ø±ÙŠØ¨ ÙˆØ§Ù„ØªØ­Ù‚Ù‚ØŒ ÙˆØ¹Ø¯Ø¯ Ø§Ù„Ø­Ù‚Ø¨Ø§ØªØŒ ÙˆØ§Ø³ØªØ¯Ø¹Ø§Ø¡Ø§ØªÙƒ Ù„Ø¶Ø¨Ø· Ø§Ù„Ù†Ù…ÙˆØ°Ø¬:\n+\n+```py\n+>>> model.fit(x=tf_train_set, validation_data=tf_validation_set, epochs=3, callbacks=callbacks)\n+```\n+\n+Ø¨Ù…Ø¬Ø±Ø¯ Ø§ÙƒØªÙ…Ø§Ù„ Ø§Ù„ØªØ¯Ø±ÙŠØ¨ØŒ ÙŠØªÙ… ØªØ­Ù…ÙŠÙ„ Ù†Ù…ÙˆØ°Ø¬Ùƒ ØªÙ„Ù‚Ø§Ø¦ÙŠÙ‹Ø§ Ø¥Ù„Ù‰ Hub Ø­ØªÙ‰ ÙŠØªÙ…ÙƒÙ† Ø§Ù„Ø¬Ù…ÙŠØ¹ Ù…Ù† Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡!\n+</tf>\n+</frameworkcontent>\n+\n+<Tip>\n+\n+Ù„Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ù…Ø«Ø§Ù„ Ø£ÙƒØ«Ø± Ø¹Ù…Ù‚Ù‹Ø§ Ø­ÙˆÙ„ ÙƒÙŠÙÙŠØ© Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ Ù„ØªØµÙ†ÙŠÙ Ø§Ù„Ù†ØµÙˆØµØŒ Ù‚Ù… Ø¨Ø§Ù„Ø§Ø·Ù„Ø§Ø¹ Ø¹Ù„Ù‰ Ø§Ù„Ø¯ÙØªØ± Ø§Ù„Ù…Ù‚Ø§Ø¨Ù„\n+[Ø¯ÙØªØ± PyTorch](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification.ipynb)\n+Ø£Ùˆ [Ø¯ÙØªØ± TensorFlow](https://colab.research.google.com/github/huggingface/notebooks/blob/main/examples/text_classification-tf.ipynb).\n+\n+</Tip>\n+\n+## Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„(Inference)\n+\n+Ø±Ø§Ø¦Ø¹ØŒ Ø§Ù„Ø¢Ù† Ø¨Ø¹Ø¯ Ø£Ù† Ù‚Ù…Øª Ø¨Ø¶Ø¨Ø· Ù†Ù…ÙˆØ°Ø¬ØŒ ÙŠÙ…ÙƒÙ†Ùƒ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„!\n+\n+Ø§Ø­ØµÙ„ Ø¹Ù„Ù‰ Ø¨Ø¹Ø¶ Ø§Ù„Ù†ØµÙˆØµ Ø§Ù„ØªÙŠ ØªØ±ØºØ¨ ÙÙŠ Ø¥Ø¬Ø±Ø§Ø¡ Ø§Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ø¹Ù„ÙŠÙ‡Ø§:\n+\n+```py\n+>>> text = \"This was a masterpiece. Not completely faithful to the books, but enthralling from beginning to end. Might be my favorite of the three.\"\n+```\n+\n+Ø£Ø³Ù‡Ù„ Ø·Ø±ÙŠÙ‚Ø© Ù„ØªØ¬Ø±Ø¨Ø© Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ Ø§Ù„Ù…Ø¶Ø¨ÙˆØ· Ù„Ù„Ø§Ø³ØªØ¯Ù„Ø§Ù„ Ù‡ÙŠ Ø§Ø³ØªØ®Ø¯Ø§Ù…Ù‡ Ø¶Ù…Ù† [`pipeline`]. Ù‚Ù… Ø¨Ø¥Ù†Ø´Ø§Ø¡ `pipeline` Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…Ø´Ø§Ø¹Ø± Ù…Ø¹ Ù†Ù…ÙˆØ°Ø¬ÙƒØŒ ÙˆÙ…Ø±Ø± Ù†ØµÙƒ Ø¥Ù„ÙŠÙ‡:\n+\n+```py\n+>>> from transformers import pipeline\n+\n+>>> classifier = pipeline(\"sentiment-analysis\", model=\"stevhliu/my_awesome_model\")\n+>>> classifier(text)\n+[{'label': 'POSITIVE', 'score': 0.9994940757751465}]\n+```\n+\n+ÙŠÙ…ÙƒÙ†Ùƒ Ø£ÙŠØ¶Ù‹Ø§ ØªÙƒØ±Ø§Ø± Ù†ØªØ§Ø¦Ø¬ `pipeline` ÙŠØ¯ÙˆÙŠÙ‹Ø§ Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª:\n+\n+<frameworkcontent>\n+<pt>\n+Ù‚Ù… ÙŠØªØ¬Ø²Ø¦Ø© Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ ØªÙ†Ø³ÙˆØ±Ø§Øª PyTorch:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n+>>> inputs = tokenizer(text, return_tensors=\"pt\")\n+```\n+\n+Ù…Ø±Ø± Ø§Ù„Ù…Ø¯Ø®Ù„Ø§Øª Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ§Ø³ØªØ±Ø¬Ø¹ `logits`:\n+\n+```py\n+>>> from transformers import AutoModelForSequenceClassification\n+\n+>>> model = AutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n+>>> with torch.no_grad():\n+...     logits = model(**inputs).logits\n+```\n+\n+Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙØ¦Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… `id2label` Ù„ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ù†ØµÙŠ:\n+\n+```py\n+>>> predicted_class_id = logits.argmax().item()\n+>>> model.config.id2label[predicted_class_id]\n+'POSITIVE'\n+```\n+</pt>\n+<tf>\n+Ù‚Ù… Ø¨ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù†Øµ ÙˆØ¥Ø±Ø¬Ø§Ø¹ ØªÙ†Ø³ÙŠÙ‚Ø§Øª TensorFlow:\n+\n+```py\n+>>> from transformers import AutoTokenizer\n+\n+>>> tokenizer = AutoTokenizer.from_pretrained(\"stevhliu/my_awesome_model\")\n+>>> inputs = tokenizer(text, return_tensors=\"tf\")\n+```\n+\n+Ù‚Ù… Ø¨ØªÙ…Ø±ÙŠØ± Ù…Ø¯Ø®Ù„Ø§ØªÙƒ Ø¥Ù„Ù‰ Ø§Ù„Ù†Ù…ÙˆØ°Ø¬ ÙˆØ¥Ø±Ø¬Ø§Ø¹ `logits`:\n+\n+```py\n+>>> from transformers import TFAutoModelForSequenceClassification\n+\n+>>> model = TFAutoModelForSequenceClassification.from_pretrained(\"stevhliu/my_awesome_model\")\n+>>> logits = model(**inputs).logits\n+```\n+\n+Ø§Ø³ØªØ®Ø±Ø¬ Ø§Ù„ÙØ¦Ø© Ø°Ø§Øª Ø§Ù„Ø§Ø­ØªÙ…Ø§Ù„ÙŠØ© Ø§Ù„Ø£Ø¹Ù„Ù‰ØŒ ÙˆØ§Ø³ØªØ®Ø¯Ù… `id2label` Ù„ØªØ­ÙˆÙŠÙ„Ù‡Ø§ Ø¥Ù„Ù‰ ØªØµÙ†ÙŠÙ Ù†ØµÙŠ:\n+\n+```py\n+>>> predicted_class_id = int(tf.math.argmax(logits, axis=-1)[0])\n+>>> model.config.id2label[predicted_class_id]\n+'POSITIVE'\n+```\n+</tf>\n+</frameworkcontent>"
        }
    ],
    "stats": {
        "total": 1263,
        "additions": 1257,
        "deletions": 6
    }
}