{
    "author": "ydshieh",
    "message": "[testing] reduce runtime of `HunYuanMoEV1IntegrationTest:test_model_generation` (#41373)\n\n* fix\n\n* fix\n\n* fix\n\n---------\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "3927ffed31e3c0d2929bf98bd05b7c61fcc48b62",
    "files": [
        {
            "sha": "eb99a71f78bcc29861d94a0483bc46f6290f4bd1",
            "filename": "tests/models/hunyuan_v1_moe/test_modeling_hunyuan_v1_moe.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/3927ffed31e3c0d2929bf98bd05b7c61fcc48b62/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/3927ffed31e3c0d2929bf98bd05b7c61fcc48b62/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fhunyuan_v1_moe%2Ftest_modeling_hunyuan_v1_moe.py?ref=3927ffed31e3c0d2929bf98bd05b7c61fcc48b62",
            "patch": "@@ -16,6 +16,7 @@\n import unittest\n \n import pytest\n+import torch\n from parameterized import parameterized\n \n from transformers import is_torch_available\n@@ -99,10 +100,12 @@ def tearDown(self):\n     def test_model_generation(self):\n         # we will compele this when model file change over\n         # pass\n-        EXPECTED_ANSWER = \"\\nOkay, I need to write a short summary about the benefits of regular exercise. Let me start by recalling what I know. First,\"\n+        EXPECTED_ANSWER = \"\\nOkay, I need to write a\"\n         prompt = \"Write a short summary of the benefits of regular exercise\"\n         tokenizer = AutoTokenizer.from_pretrained(\"tencent/Hunyuan-A13B-Instruct\")\n-        model = AutoModelForCausalLM.from_pretrained(\"tencent/Hunyuan-A13B-Instruct\", device_map=\"auto\")\n+        model = AutoModelForCausalLM.from_pretrained(\n+            \"tencent/Hunyuan-A13B-Instruct\", device_map=\"auto\", dtype=torch.bfloat16\n+        )\n         messages = [\n             {\"role\": \"user\", \"content\": prompt},\n         ]\n@@ -112,7 +115,7 @@ def test_model_generation(self):\n             add_generation_prompt=True,\n             return_tensors=\"pt\",\n         )\n-        generated_ids = model.generate(tokenized_chat.to(model.device), max_new_tokens=30, top_k=1)\n+        generated_ids = model.generate(tokenized_chat.to(model.device), max_new_tokens=10, top_k=1)\n         text = tokenizer.decode(generated_ids[0])\n         output = text.split(\"<think>\")[1]\n         self.assertEqual(EXPECTED_ANSWER, output)"
        }
    ],
    "stats": {
        "total": 9,
        "additions": 6,
        "deletions": 3
    }
}