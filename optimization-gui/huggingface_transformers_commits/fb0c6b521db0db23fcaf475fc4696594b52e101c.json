{
    "author": "danielkorat",
    "message": "Universal Assisted Generation: Assisted generation with any assistant model (by Intel Labs) (#33383)\n\n* Update candidate_generator.py\r\n\r\n* Update utils.py\r\n\r\n* add lookbehind params to _get_candidate_generator\r\n\r\n* make fixup\r\n\r\n* add unit tests\r\n\r\n* fix failing tests\r\n\r\n* add docstrings\r\n\r\n* fix docstrings; remove non-optimized AnyTokenizer\r\n\r\n* added any tokenizer generation correctness test\r\n\r\n* make fixup\r\n\r\n* fix assertion syntax\r\n\r\n* PR review fixes\r\n\r\n* address additional PR comments\r\n\r\n* fix tests\r\n\r\n* remove stropping criteria arg\r\n\r\n* make fixup\r\n\r\n* add AssistantConfig\r\n\r\n* fix prev_tokens branching\r\n\r\n* pass tokenizers through `generate()`kwargs\r\n\r\n* fix lookbehind values; tokenizer params WIP\r\n\r\n* fixup\r\n\r\n* AssistantConfig\r\n\r\n* remove AssistantConfig; apply PR suggestions\r\n\r\n* restructure tests\r\n\r\n* fixup\r\n\r\n* fix assistant_tokenizer arg validation\r\n\r\n* fixup\r\n\r\n* fix tests in TestAssistedCandidateGeneratorDifferentTokenizers\r\n\r\n* fix class docstring\r\n\r\n* PR suggestions\r\n\r\n* doc\r\n\r\n* doc update and improvements to `_validate_assistant()`\r\n\r\n---------\r\n\r\nCo-authored-by: mosheber <moshe.berchansky@intel.com>",
    "sha": "fb0c6b521db0db23fcaf475fc4696594b52e101c",
    "files": [
        {
            "sha": "64ded9613716a58d0759d83ad2e1af49a6007aab",
            "filename": "docs/source/en/generation_strategies.md",
            "status": "modified",
            "additions": 35,
            "deletions": 4,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb0c6b521db0db23fcaf475fc4696594b52e101c/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb0c6b521db0db23fcaf475fc4696594b52e101c/docs%2Fsource%2Fen%2Fgeneration_strategies.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fen%2Fgeneration_strategies.md?ref=fb0c6b521db0db23fcaf475fc4696594b52e101c",
            "patch": "@@ -408,14 +408,24 @@ For the complete list of the available parameters, refer to the [API documentati\n ### Speculative Decoding\n \n Speculative decoding (also known as assisted decoding) is a modification of the decoding strategies above, that uses an\n-assistant model (ideally a much smaller one) with the same tokenizer, to generate a few candidate tokens. The main\n-model then validates the candidate tokens in a single forward pass, which speeds up the decoding process. If\n-`do_sample=True`, then the token validation with resampling introduced in the\n-[speculative decoding paper](https://arxiv.org/pdf/2211.17192.pdf) is used.\n+assistant model (ideally a much smaller one), to generate a few candidate tokens. The main model then validates the candidate\n+tokens in a single forward pass, which speeds up the decoding process. If `do_sample=True`, then the token validation with\n+resampling introduced in the [speculative decoding paper](https://arxiv.org/pdf/2211.17192.pdf) is used.\n+Assisted decoding assumes the main and assistant models have the same tokenizer, otherwise, see Universal Assisted Decoding below.\n \n Currently, only greedy search and sampling are supported with assisted decoding, and assisted decoding doesn't support batched inputs.\n To learn more about assisted decoding, check [this blog post](https://huggingface.co/blog/assisted-generation).\n \n+#### Universal Assisted Decoding\n+\n+Universal Assisted Decoding (UAD) adds support for main and assistant models with different tokenizers.\n+To use it, simply pass the tokenizers using the `tokenizer` and `assistant_tokenizer` arguments (see below).\n+Internally, the main model input tokens are re-encoded into assistant model tokens, then candidate tokens are generated in the assistant encoding, which are\n+in turn re-encoded into main model candidate tokens. Validation then proceeds as explained above.\n+The re-encoding steps involve decoding token ids into text and then encoding the text using a different tokenizer.\n+Since re-encoding the tokens may result in tokenization discrepancies, UAD finds the longest common subsequence between the source and target encodings, \n+to ensure the new tokens include the correct prompt suffix.\n+\n To enable assisted decoding, set the `assistant_model` argument with a model.\n \n ```python\n@@ -435,6 +445,26 @@ To enable assisted decoding, set the `assistant_model` argument with a model.\n ['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n ```\n \n+If the main and assistant models have different tokenizers, use Universal Assisted Decoding.\n+\n+```python\n+>>> from transformers import AutoModelForCausalLM, AutoTokenizer\n+\n+>>> prompt = \"Alice and Bob\"\n+>>> checkpoint = \"google/gemma-2-9b\"\n+>>> assistant_checkpoint = \"double7/vicuna-68m\"\n+\n+>>> assistant_tokenizer = AutoTokenizer.from_pretrained(assistant_checkpoint)\n+>>> tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n+>>> inputs = tokenizer(prompt, return_tensors=\"pt\")\n+\n+>>> model = AutoModelForCausalLM.from_pretrained(checkpoint)\n+>>> assistant_model = AutoModelForCausalLM.from_pretrained(assistant_checkpoint)\n+>>> outputs = model.generate(**inputs, assistant_model=assistant_model, tokenizer=tokenizer, assistant_tokenizer=assistant_tokenizer)\n+>>> tokenizer.batch_decode(outputs, skip_special_tokens=True)\n+['Alice and Bob are sitting in a bar. Alice is drinking a beer and Bob is drinking a']\n+```\n+\n When using assisted decoding with sampling methods, you can use the `temperature` argument to control the randomness,\n just like in multinomial sampling. However, in assisted decoding, reducing the temperature may help improve the latency.\n \n@@ -458,6 +488,7 @@ just like in multinomial sampling. However, in assisted decoding, reducing the t\n \n Alternatively, you can also set the `prompt_lookup_num_tokens` to trigger n-gram based assisted decoding, as opposed\n to model based assisted decoding. You can read more about it [here](https://twitter.com/joao_gante/status/1747322413006643259).\n+\n ### DoLa Decoding\n \n **D**ecoding by C**o**ntrasting **La**yers (DoLa) is a contrastive decoding strategy to improve the factuality and reduce the"
        },
        {
            "sha": "a4c8f79ae925d17910589b8271eea4caefbe214d",
            "filename": "src/transformers/generation/candidate_generator.py",
            "status": "modified",
            "additions": 300,
            "deletions": 0,
            "changes": 300,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb0c6b521db0db23fcaf475fc4696594b52e101c/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb0c6b521db0db23fcaf475fc4696594b52e101c/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Fcandidate_generator.py?ref=fb0c6b521db0db23fcaf475fc4696594b52e101c",
            "patch": "@@ -16,6 +16,7 @@\n import copy\n from typing import TYPE_CHECKING, Any, Dict, Optional, Tuple\n \n+import numpy as np\n import torch\n \n from ..cache_utils import DynamicCache\n@@ -25,6 +26,7 @@\n \n if TYPE_CHECKING:\n     from ..modeling_utils import PreTrainedModel\n+    from ..tokenization_utils_base import PreTrainedTokenizerBase\n     from .configuration_utils import GenerationConfig\n \n \n@@ -156,6 +158,7 @@ def __init__(\n         # Prepare generation-related options.\n         self.logits_processor = logits_processor if logits_processor is not None else LogitsProcessorList()\n         self.generation_config = copy.deepcopy(generation_config)\n+\n         self.generation_config.return_dict_in_generate = True\n         self.generation_config.output_scores = True\n         self.generation_config.assistant_confidence_threshold = self.assistant_confidence_threshold\n@@ -258,6 +261,303 @@ def update_candidate_strategy(self, input_ids: torch.LongTensor, scores: torch.F\n                 self.num_assistant_tokens = max(1.0, self.num_assistant_tokens - 1.0)\n \n \n+class AssistedCandidateGeneratorDifferentTokenizers(AssistedCandidateGenerator):\n+    \"\"\"\n+    `CandidateGenerator` class to be used for Universal Assisted Generation (UAD): assisted generation with different tokenizers\n+    for the assistant and main models. This class generates candidates through the use of a smaller\n+    model.\n+\n+    The main model input tokens are re-encoded into assistant model tokens, then candidate tokens are generated in the assistant encoding, which are\n+    in turn re-encoded into main model candidate tokens. Validation then proceeds as explained above.\n+    The re-encoding steps involve decoding token ids into text and then encoding the text using a different tokenizer.\n+    Since re-encoding the tokens may result in tokenization discrepancies, UAD finds the longest common subsequence between the source and target encodings,\n+    to ensure the new tokens include the correct prompt suffix.\n+\n+    Args:\n+        input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+            Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n+        assistant_model (`PreTrainedModel`):\n+            The model to be used for generating candidates. This model should be smaller than the main model.\n+        target_tokenizer (`PreTrainedTokenizerBase`):\n+            The tokenizer used for the target model.\n+        assistant_tokenizer (`PreTrainedTokenizerBase`):\n+            The tokenizer used for the assistant model.\n+        generation_config (`~generation.GenerationConfig`, *optional*):\n+            The generation configuration to be used as base parametrization for the generation call.\n+        logits_processor (`LogitsProcessorList`):\n+            An instance of [`LogitsProcessorList`]. List of instances of class derived from [`LogitsProcessor`]\n+            used to modify the prediction scores of the language modeling head applied at each generation step.\n+        model_kwargs (`Dict`):\n+            The keyword arguments that will be passed to the main model, and are used as base inputs for the assistant\n+            model as well.\n+        inputs_tensor (`torch.Tensor`, *optional*):\n+            The model input tensor. In encoder-decoder models, this is the encoder input.\n+    \"\"\"\n+\n+    def __init__(\n+        self,\n+        input_ids: torch.LongTensor,\n+        assistant_model: \"PreTrainedModel\",\n+        target_tokenizer: \"PreTrainedTokenizerBase\",\n+        assistant_tokenizer: \"PreTrainedTokenizerBase\",\n+        generation_config: \"GenerationConfig\",\n+        model_kwargs: Dict,\n+        inputs_tensor: Optional[torch.Tensor] = None,\n+        logits_processor: \"LogitsProcessorList\" = None,\n+    ):\n+        super().__init__(input_ids, assistant_model, generation_config, model_kwargs, inputs_tensor, logits_processor)\n+\n+        self.target_tokenizer = target_tokenizer\n+        self.assistant_tokenizer = assistant_tokenizer\n+        self.prev_tokens = None\n+        self.prev_assistant_ids = None\n+        self.target_lookbehind = 10\n+        self.assistant_lookbehind = 10\n+\n+    @staticmethod\n+    def _get_longest_diag_dict(input_matrix, nonzero_idx):\n+        \"\"\"\n+        Calculates the length of the longest diagonal sequence in a given matrix.\n+        Args:\n+            input_matrix (torch.Tensor): The input matrix.\n+            nonzero_idx (torch.Tensor): The indices of the non-zero elements in the matrix.\n+        Returns:\n+            dict: A dictionary where the keys are the indices of the non-zero elements and the values are the lengths of the longest diagonal sequences starting from those indices.\n+        \"\"\"\n+\n+        visited = set()\n+        diags = {}\n+        for idx in nonzero_idx:\n+            start_idx = torch.clone(idx)\n+            tuple_start_idx = tuple(start_idx.tolist())\n+\n+            if tuple_start_idx in visited:\n+                continue\n+\n+            visited.add(tuple_start_idx)\n+            cur_diag_len = 1\n+            start_idx += 1\n+            while start_idx[0] < input_matrix.shape[0] and start_idx[1] < input_matrix.shape[1]:\n+                tuple_start_idx = tuple(start_idx.tolist())\n+                visited.add(tuple_start_idx)\n+\n+                if input_matrix[start_idx[0], start_idx[1]] == 1:\n+                    cur_diag_len += 1\n+                    start_idx += 1\n+                else:\n+                    break\n+\n+            diags[idx] = cur_diag_len\n+        return diags\n+\n+    @staticmethod\n+    def _get_longest_diag_index(input_matrix):\n+        \"\"\"\n+        Returns the start index and length of the longest diagonal in the given input.\n+        Args:\n+            input_matrix (numpy.ndarray): The input matrix.\n+        Returns:\n+            tuple: A tuple containing the start index and length of the longest diagonal.\n+        \"\"\"\n+\n+        diags = AssistedCandidateGeneratorDifferentTokenizers._get_longest_diag_dict(\n+            input_matrix, input_matrix.nonzero()\n+        )\n+        diags_values = list(diags.values())\n+        diags_keys = list(diags.keys())\n+        best_diag = np.argmax(diags_values)\n+        diag_start_index = diags_keys[best_diag]\n+        diag_start_length = diags_values[best_diag]\n+        return diag_start_index, diag_start_length\n+\n+    @staticmethod\n+    def _get_tokens_diag(prompt, prompt_plus_new_tokens):\n+        \"\"\"\n+        Input:\n+            prompt: 2D array of shape (batch_size, prompt_length), represents the original prompt tokens\n+            prompt_plus_new_tokens: 2D array of shape (batch_size, prompt_length), represents the suffix of the original prompt, with additional new tokens.\n+        Output:\n+            discrepancy_length: int, represents the number of tokens that need to be replaced from prompt\n+            new_tokens_only: 2D array of shape (batch_size, new_token_length), represents the new tokens that are not in prompt\n+            discrepancy_only: 2D array of shape (batch_size, discrepancy_length), represents the new tokens that are in prompt but not in prompt_plus_new_tokens\n+        \"\"\"\n+        compare_mat = prompt_plus_new_tokens.T == prompt\n+        if not torch.is_tensor(compare_mat):\n+            compare_mat = torch.tensor(compare_mat)\n+\n+        compare_mat_int = compare_mat.to(int)\n+\n+        if not compare_mat_int.any().item():\n+            # empty intersection between prompt and prompt_plus_new_tokens\n+            return None, None, None\n+\n+        longest_location, longest_diag_length = AssistedCandidateGeneratorDifferentTokenizers._get_longest_diag_index(\n+            compare_mat_int\n+        )\n+        new_token_start_index = longest_location[0] + longest_diag_length\n+        discrepancy_with_old = longest_location[1] + longest_diag_length\n+        discrepancy_length = (prompt.shape[1] - discrepancy_with_old).item()\n+        new_tokens_only = prompt_plus_new_tokens[:, new_token_start_index + discrepancy_length :]\n+        discrepancy_only = prompt_plus_new_tokens[\n+            :, new_token_start_index : new_token_start_index + discrepancy_length\n+        ]\n+        return discrepancy_length, new_tokens_only, discrepancy_only\n+\n+    def convert_source_tokens_to_target_tokens(\n+        self,\n+        input_ids,\n+        source_tokenizer,\n+        destination_tokenizer,\n+    ):\n+        \"\"\"\n+        Convert token IDs from one tokenizer to another.\n+        Args:\n+            input_ids: The input token IDs.\n+            source_tokenizer: The source tokenizer.\n+            destination_tokenizer: The destination tokenizer.\n+        Returns:\n+            The converted token IDs.\n+        \"\"\"\n+        text = source_tokenizer.batch_decode(input_ids, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n+        dest_ids = destination_tokenizer(text, add_special_tokens=True, return_tensors=\"pt\")[\"input_ids\"]\n+        return dest_ids.to(input_ids.device)\n+\n+    def get_candidates(self, input_ids: torch.LongTensor) -> Tuple[torch.LongTensor, Optional[torch.FloatTensor]]:\n+        \"\"\"\n+        Fetches the candidates to be tried for the current input.\n+\n+        Args:\n+            input_ids (`torch.LongTensor` of shape `(batch_size, sequence_length)`):\n+                Indices of input sequence tokens in the vocabulary. [What are input IDs?](../glossary#input-ids)\n+\n+        Return:\n+            `torch.LongTensor` of shape `(batch_size, candidate_length)` containing the candidate sequences to be\n+            assessed by the model and a `torch.FloatTensor` of shape `(batch_size, candidate_length,\n+            vocabulary_size)` containing the logits associated to each candidate.\n+        \"\"\"\n+        max_new_tokens = int(self.num_assistant_tokens)\n+        if max_new_tokens == 0:\n+            return input_ids, None\n+\n+        input_ids = input_ids.to(self.assistant_model.device)\n+        convert_kwargs = {\n+            \"source_tokenizer\": self.target_tokenizer,\n+            \"destination_tokenizer\": self.assistant_tokenizer,\n+        }\n+        remove_from_pkv = 0\n+\n+        # Since re-encoding the tokens may result in tokenization discrepancies, we use 2 look behind values\n+        # (one for each conversion) which mark where to start looking for the overlap between the\n+        # source and target encodings, to ensure the new tokens include the correct prompt suffix.\n+        if self.prev_tokens is not None and self.prev_target_ids.shape[1] > self.target_lookbehind:\n+            # input_ids contains all target prompt input ids and some new target input ids\n+            start_index_in_target_window = self.prev_target_ids.shape[1] - self.target_lookbehind\n+\n+            new_assistant_ids = self.convert_source_tokens_to_target_tokens(\n+                input_ids[:, start_index_in_target_window:], **convert_kwargs\n+            )\n+            prompt_use_length = new_assistant_ids.shape[1]\n+            prompt_use = self.prev_assistant_ids[:, -prompt_use_length:]\n+\n+            discrepancy_length, new_tokens_only, discrepancy_only = (\n+                AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(prompt_use, new_assistant_ids)\n+            )\n+            assistant_input_ids = self.prev_assistant_ids\n+\n+            if new_tokens_only is not None:\n+                if discrepancy_length > 0 and discrepancy_only.shape[1] > 0:\n+                    if discrepancy_length == discrepancy_only.shape[1]:\n+                        assistant_input_ids[:, -discrepancy_length:] = discrepancy_only\n+\n+                    elif discrepancy_length > discrepancy_only.shape[1]:\n+                        discrepancy_length_diff = discrepancy_length - discrepancy_only.shape[1]\n+                        assistant_input_ids = assistant_input_ids[:, :-discrepancy_length_diff]\n+                        assistant_input_ids[:, -discrepancy_only.shape[1] :] = discrepancy_only\n+\n+                    remove_from_pkv = discrepancy_length\n+\n+                if new_tokens_only.shape[1] > 0:\n+                    assistant_input_ids = torch.cat([assistant_input_ids, new_tokens_only], dim=-1)\n+            else:\n+                # edge case: in case of no intersection between prompt and new_assistant_ids\n+                assistant_input_ids = torch.cat([assistant_input_ids, new_assistant_ids], dim=-1)\n+\n+        else:\n+            assistant_input_ids = self.convert_source_tokens_to_target_tokens(input_ids, **convert_kwargs)\n+            self.prev_target_ids = input_ids\n+\n+        self.prev_assistant_ids = assistant_input_ids\n+        new_cur_len = assistant_input_ids.shape[-1]\n+        min_new_tokens = max(min(max_new_tokens, self.main_model_min_length - new_cur_len), 0)\n+\n+        # 1. If it is not the first round of candidate generation, prepare the inputs based on the input_ids length\n+        # (which implicitly contains the number of accepted candidates from the previous round)\n+        has_past_key_values = self.assistant_kwargs.get(\"past_key_values\", None) is not None\n+        if has_past_key_values:\n+            new_cache_size = new_cur_len - 1 - remove_from_pkv\n+            self.assistant_kwargs[\"past_key_values\"] = _crop_past_key_values(\n+                self.assistant_model, self.assistant_kwargs[\"past_key_values\"], new_cache_size - 1\n+            )  # the assistant does not have the token after the last match, hence the -1\n+\n+            self.assistant_kwargs = _prepare_attention_mask(\n+                self.assistant_kwargs, new_cur_len, self.assistant_model.config.is_encoder_decoder\n+            )\n+            self.assistant_kwargs = _prepare_token_type_ids(self.assistant_kwargs, new_cur_len)\n+\n+        # 2. Forecast next N tokens using the assistant model.\n+        assistant_generation_kwargs = {\n+            self.input_ids_key: assistant_input_ids,\n+            \"min_new_tokens\": min_new_tokens,\n+            \"max_new_tokens\": max_new_tokens,\n+            \"generation_config\": self.generation_config,\n+            \"logits_processor\": self.logits_processor,\n+        }\n+\n+        self.assistant_kwargs.pop(\"attention_mask\", None)\n+\n+        assistant_output = self.assistant_model.generate(**assistant_generation_kwargs, **self.assistant_kwargs)\n+\n+        num_prev_assistant = self.prev_assistant_ids.shape[1]\n+        start_assistant_look_index = num_prev_assistant - self.assistant_lookbehind\n+\n+        new_target_ids_from_window = self.convert_source_tokens_to_target_tokens(\n+            assistant_output.sequences[:, start_assistant_look_index:],\n+            source_tokenizer=self.assistant_tokenizer,\n+            destination_tokenizer=self.target_tokenizer,\n+        )\n+        target_prompt_use_length = new_target_ids_from_window.shape[1]\n+\n+        target_prompt_use = input_ids[:, -target_prompt_use_length:]\n+\n+        _, target_new_tokens_only, _ = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n+            target_prompt_use, new_target_ids_from_window\n+        )\n+\n+        new_target_ids = input_ids\n+\n+        if target_new_tokens_only is not None:\n+            if target_new_tokens_only.shape[1] > 0:\n+                new_target_ids = torch.cat([new_target_ids, target_new_tokens_only], dim=-1)\n+        else:\n+            # edge case: in case of no intersection between prompt and new_target_ids\n+            new_target_ids = torch.cat([new_target_ids, new_target_ids_from_window], dim=-1)\n+\n+        self.prev_target_ids = input_ids\n+\n+        if hasattr(self.generation_config, \"max_length\"):\n+            new_target_ids = new_target_ids[:, : self.generation_config.max_length]\n+\n+        # 3. Update variables for the next round of candidate generation\n+        self.assistant_kwargs[\"past_key_values\"] = assistant_output.past_key_values\n+        self.prev_tokens = assistant_output.sequences\n+\n+        # 4. Prepare variables for output\n+        if input_ids.shape[1] >= new_target_ids.shape[1]:\n+            return input_ids, None\n+\n+        return new_target_ids, None\n+\n+\n class PromptLookupCandidateGenerator(CandidateGenerator):\n     \"\"\"\n     `CandidateGenerator` class to be used for prompt lookup generation. This class generates candidates by looking up"
        },
        {
            "sha": "b355bbeaa9f154ead6e33f887384cd0c7b6cc9c2",
            "filename": "src/transformers/generation/utils.py",
            "status": "modified",
            "additions": 38,
            "deletions": 6,
            "changes": 44,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb0c6b521db0db23fcaf475fc4696594b52e101c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb0c6b521db0db23fcaf475fc4696594b52e101c/src%2Ftransformers%2Fgeneration%2Futils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fgeneration%2Futils.py?ref=fb0c6b521db0db23fcaf475fc4696594b52e101c",
            "patch": "@@ -51,6 +51,7 @@\n from .beam_search import BeamScorer, BeamSearchScorer, ConstrainedBeamSearchScorer\n from .candidate_generator import (\n     AssistedCandidateGenerator,\n+    AssistedCandidateGeneratorDifferentTokenizers,\n     CandidateGenerator,\n     PromptLookupCandidateGenerator,\n     _crop_past_key_values,\n@@ -617,7 +618,7 @@ def _prepare_encoder_decoder_kwargs_for_generation(\n         model_input_name = model_input_name if model_input_name is not None else self.main_input_name\n         encoder_kwargs[\"return_dict\"] = True\n         encoder_kwargs[model_input_name] = inputs_tensor\n-        model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)\n+        model_kwargs[\"encoder_outputs\"]: ModelOutput = encoder(**encoder_kwargs)  # type: ignore\n \n         return model_kwargs\n \n@@ -787,18 +788,33 @@ def _get_candidate_generator(\n         inputs_tensor: torch.Tensor,\n         assistant_model: \"PreTrainedModel\",\n         logits_processor: LogitsProcessorList,\n+        target_tokenizer: \"PreTrainedTokenizerBase\",\n+        assistant_tokenizer: \"PreTrainedTokenizerBase\",\n         model_kwargs: Dict,\n     ) -> CandidateGenerator:\n         \"\"\"\n         Returns the candidate generator to be used in `assisted_generation`\n         \"\"\"\n+        different_tokenizers = all(v is not None for v in (assistant_model, target_tokenizer, assistant_tokenizer))\n+\n         if generation_config.prompt_lookup_num_tokens is not None:\n             candidate_generator = PromptLookupCandidateGenerator(\n                 eos_token_id=generation_config._eos_token_tensor,\n                 num_output_tokens=generation_config.prompt_lookup_num_tokens,\n                 max_matching_ngram_size=generation_config.max_matching_ngram_size,\n                 max_length=generation_config.max_length,\n             )\n+        elif different_tokenizers:\n+            candidate_generator = AssistedCandidateGeneratorDifferentTokenizers(\n+                input_ids=input_ids,\n+                assistant_model=assistant_model,\n+                generation_config=generation_config,\n+                model_kwargs=model_kwargs,\n+                inputs_tensor=inputs_tensor,\n+                logits_processor=logits_processor,\n+                target_tokenizer=target_tokenizer,\n+                assistant_tokenizer=assistant_tokenizer,\n+            )\n         else:\n             candidate_generator = AssistedCandidateGenerator(\n                 input_ids=input_ids,\n@@ -1250,7 +1266,7 @@ def _validate_model_class(self):\n                 f\"names: {terminations_with_generation_support}.\"\n             )\n \n-    def _validate_assistant(self, assistant_model):\n+    def _validate_assistant(self, assistant_model, tokenizer, assistant_tokenizer):\n         if assistant_model is None:\n             return\n \n@@ -1266,8 +1282,19 @@ def _validate_assistant(self, assistant_model):\n                     \"Ensure you load the assistant with the correct encoder-decoder class, e.g. `AutoModelForSpeechSeq2Seq` for Whisper.\"\n                 )\n \n-        if not self.config.get_text_config().vocab_size == assistant_model.config.get_text_config().vocab_size:\n-            raise ValueError(\"Make sure the main and assistant model use the same tokenizer\")\n+        doc_reference = (\n+            \"(see https://huggingface.co/docs/transformers/en/generation_strategies#universal-assisted-decoding)\"\n+        )\n+        if self.config.get_text_config().vocab_size == assistant_model.config.get_text_config().vocab_size:\n+            if assistant_tokenizer is not None:\n+                raise ValueError(\n+                    f\"`assistant_tokenizer` is not required when the main and assistant models use the same tokenizer. Please omit `assistant_tokenizer` from `generate()` {doc_reference}.\"\n+                )\n+        else:\n+            if tokenizer is None or assistant_tokenizer is None:\n+                raise ValueError(\n+                    f\"The main and assistant moedels have different tokenizers. Please provide `tokenizer` and `assistant_tokenizer` to `generate()` {doc_reference}.\"\n+                )\n \n     def _validate_model_kwargs(self, model_kwargs: Dict[str, Any]):\n         \"\"\"Validates model kwargs for generation. Generate argument typos will also be caught here.\"\"\"\n@@ -1923,12 +1950,15 @@ def generate(\n                     - [`~generation.GenerateEncoderDecoderOutput`],\n                     - [`~generation.GenerateBeamEncoderDecoderOutput`]\n         \"\"\"\n+\n         # 1. Handle `generation_config` and kwargs that might update it, and validate the `.generate()` call\n         self._validate_model_class()\n         tokenizer = kwargs.pop(\"tokenizer\", None)  # Pull this out first, we only use it for stopping criteria\n+        assistant_tokenizer = kwargs.pop(\"assistant_tokenizer\", None)  # only used for assisted generation\n+\n         generation_config, model_kwargs = self._prepare_generation_config(generation_config, **kwargs)\n         self._validate_model_kwargs(model_kwargs.copy())\n-        self._validate_assistant(assistant_model)\n+        self._validate_assistant(assistant_model, tokenizer, assistant_tokenizer)\n \n         # 2. Set generation parameters if not already defined\n         if synced_gpus is None:\n@@ -2110,6 +2140,8 @@ def generate(\n                 inputs_tensor=inputs_tensor,\n                 assistant_model=assistant_model,\n                 logits_processor=logits_processor,\n+                target_tokenizer=tokenizer,\n+                assistant_tokenizer=assistant_tokenizer,\n                 model_kwargs=model_kwargs,\n             )\n \n@@ -4138,7 +4170,7 @@ def _assisted_decoding(\n \n             #  1. Fetch candidate sequences from a `CandidateGenerator`\n             candidate_input_ids, candidate_logits = candidate_generator.get_candidates(input_ids)\n-            candidate_input_ids = candidate_input_ids.to(self.device)\n+\n             if candidate_logits is not None:\n                 candidate_logits = candidate_logits.to(self.device)\n "
        },
        {
            "sha": "1727aed1117bc6cd1b23fc458e419bb22d010c5f",
            "filename": "tests/generation/test_utils.py",
            "status": "modified",
            "additions": 67,
            "deletions": 0,
            "changes": 67,
            "blob_url": "https://github.com/huggingface/transformers/blob/fb0c6b521db0db23fcaf475fc4696594b52e101c/tests%2Fgeneration%2Ftest_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fb0c6b521db0db23fcaf475fc4696594b52e101c/tests%2Fgeneration%2Ftest_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fgeneration%2Ftest_utils.py?ref=fb0c6b521db0db23fcaf475fc4696594b52e101c",
            "patch": "@@ -88,6 +88,7 @@\n         WatermarkDetector,\n         WatermarkingConfig,\n     )\n+    from transformers.generation.candidate_generator import AssistedCandidateGeneratorDifferentTokenizers\n     from transformers.generation.utils import _speculative_sampling\n \n \n@@ -3510,6 +3511,34 @@ def test_special_tokens_fall_back_to_model_default(self):\n         self.assertTrue(test_bos_id == gen_output[0, 0])\n         self.assertTrue(generation_config.bos_token_id is None)\n \n+    def test_speculative_decoding_equals_regular_decoding(self):\n+        draft_name = \"double7/vicuna-68m\"\n+        target_name = \"Qwen/Qwen2-0.5B-Instruct\"\n+\n+        draft_model = AutoModelForCausalLM.from_pretrained(draft_name)\n+        target_model = AutoModelForCausalLM.from_pretrained(target_name)\n+\n+        assistant_tokenizer = AutoTokenizer.from_pretrained(draft_name)\n+        target_tokenizer = AutoTokenizer.from_pretrained(target_name)\n+\n+        prompt_size = torch.randint(low=20, high=100, size=(1,))\n+        max_new_tokens = torch.randint(low=10, high=50, size=(1,))\n+        input_ids = (torch.rand(1, prompt_size[0]) * 100).to(int) + 50\n+\n+        max_new_tokens_item = max_new_tokens[0].item()\n+        expected_out = target_model.generate(input_ids, do_sample=False, max_new_tokens=max_new_tokens_item)\n+        predicted_out = target_model.generate(\n+            input_ids,\n+            do_sample=False,\n+            max_new_tokens=max_new_tokens_item,\n+            assistant_model=draft_model,\n+            target_tokenizer=target_tokenizer,\n+            assistant_tokenizer=assistant_tokenizer,\n+        )\n+\n+        self.assertEqual(expected_out.shape, predicted_out.shape)\n+        self.assertTrue((expected_out == predicted_out).all().item())\n+\n     @pytest.mark.generate\n     @require_torch_multi_gpu\n     def test_generate_with_static_cache_multi_gpu(self):\n@@ -3884,3 +3913,41 @@ def test_generate_from_inputs_embeds_with_bos_token_id_is_none(self):\n         # bos_token_id is required when no input ids nor inputs_embeds is passed\n         with self.assertRaises(ValueError):\n             model.generate(max_length=20, bos_token_id=None)\n+\n+\n+class TestAssistedCandidateGeneratorDifferentTokenizers(unittest.TestCase):\n+    def test_no_intersection(self):\n+        prompt = np.array([[1, 2, 3]])\n+        prompt_plus_new_tokens = np.array([[4, 5, 6]])\n+        result = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(prompt, prompt_plus_new_tokens)\n+        self.assertEqual(result, (None, None, None))\n+\n+    def test_complete_overlap(self):\n+        prompt = np.array([[1, 2, 3]])\n+        prompt_plus_new_tokens = np.array([[1, 2, 3, 4, 5]])\n+        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n+            prompt, prompt_plus_new_tokens\n+        )\n+        self.assertEqual(discrep_length, 0)\n+        np.testing.assert_array_equal(new_tokens_only, np.array([[4, 5]]))\n+        np.testing.assert_array_equal(discrep_only, np.array([[]]))\n+\n+    def test_partial_overlap(self):\n+        prompt = np.array([[1, 2, 3]])\n+        prompt_plus_new_tokens = np.array([[2, 3, 4, 5]])\n+        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n+            prompt, prompt_plus_new_tokens\n+        )\n+        self.assertEqual(discrep_length, 0)\n+        np.testing.assert_array_equal(new_tokens_only, np.array([[4, 5]]))\n+        np.testing.assert_array_equal(discrep_only, np.array([[]]))\n+\n+    def test_no_new_tokens(self):\n+        prompt = np.array([[1, 2, 3]])\n+        prompt_plus_new_tokens = np.array([[1, 2, 3]])\n+        discrep_length, new_tokens_only, discrep_only = AssistedCandidateGeneratorDifferentTokenizers._get_tokens_diag(\n+            prompt, prompt_plus_new_tokens\n+        )\n+        self.assertEqual(discrep_length, 0)\n+        np.testing.assert_array_equal(new_tokens_only, np.array([[]]))\n+        np.testing.assert_array_equal(discrep_only, np.array([[]]))"
        }
    ],
    "stats": {
        "total": 450,
        "additions": 440,
        "deletions": 10
    }
}