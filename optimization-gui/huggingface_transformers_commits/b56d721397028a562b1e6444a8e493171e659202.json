{
    "author": "zucchini-nlp",
    "message": "[configuration] remove redundant `classmethod` (#38812)\n\n* remove redundant classmethod\n\n* warning message, add space between words\n\n* fix tests\n\n* fix copies",
    "sha": "b56d721397028a562b1e6444a8e493171e659202",
    "files": [
        {
            "sha": "acfd450e870bef7729409c7bf446964c0ba8fe13",
            "filename": "src/transformers/configuration_utils.py",
            "status": "modified",
            "additions": 36,
            "deletions": 0,
            "changes": 36,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fconfiguration_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fconfiguration_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fconfiguration_utils.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -1199,6 +1199,42 @@ def get_text_config(self, decoder=False) -> \"PretrainedConfig\":\n             config_to_return = self\n         return config_to_return\n \n+    @classmethod\n+    def from_text_vision_configs(cls, text_config, vision_config, **kwargs):\n+        r\"\"\"\n+        Instantiate a model config (or a derived class) from text model configuration and vision model\n+        configuration.\n+\n+        Returns:\n+            [`PreTrainedConfig`]: An instance of a configuration object\n+        \"\"\"\n+\n+        warnings.warn(\n+            \"The `from_text_vision_configs` method is deprecated and will be removed in v4.60 of Transformers. Please instantiate \"\n+            \"the config class directly with `MyConfig(text_config=text_config, vision_config=vision_config, **kwargs)` instead.\",\n+            FutureWarning,\n+        )\n+\n+        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n+\n+    @classmethod\n+    def from_text_audio_configs(cls, text_config, audio_config, **kwargs):\n+        r\"\"\"\n+        Instantiate a model config (or a derived class) from text model configuration and audio model\n+        configuration.\n+\n+        Returns:\n+            [`PreTrainedConfig`]: An instance of a configuration object\n+        \"\"\"\n+\n+        warnings.warn(\n+            \"The `from_text_audio_configs` method is deprecated and will be removed in v4.60 of Transformers. Please instantiate \"\n+            \"the config class directly with `MyConfig(text_config=text_config, audio_config=audio_config, **kwargs)` instead.\",\n+            FutureWarning,\n+        )\n+\n+        return cls(text_config=text_config.to_dict(), audio_config=audio_config.to_dict(), **kwargs)\n+\n \n def get_configuration_file(configuration_files: list[str]) -> str:\n     \"\"\""
        },
        {
            "sha": "adab18c7447a803117cb0d89abdf38b4dc96167f",
            "filename": "src/transformers/models/aimv2/configuration_aimv2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faimv2%2Fconfiguration_aimv2.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -280,17 +280,5 @@ def __init__(\n         self.logit_scale_init_value = logit_scale_init_value\n         self.max_logit_scale = 100.0\n \n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: Aimv2TextConfig, vision_config: Aimv2VisionConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`Aimv2Config`] (or a derived class) from aimv2 text model configuration and aimv2 vision\n-        model configuration.\n-\n-        Returns:\n-            [`Aimv2Config`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n __all__ = [\"Aimv2Config\", \"Aimv2VisionConfig\", \"Aimv2TextConfig\"]"
        },
        {
            "sha": "b924d85a6ca6b4a256eedf55221fe379aff5a87a",
            "filename": "src/transformers/models/align/configuration_align.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Falign%2Fconfiguration_align.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -327,17 +327,5 @@ def __init__(\n         self.temperature_init_value = temperature_init_value\n         self.initializer_range = initializer_range\n \n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: AlignTextConfig, vision_config: AlignVisionConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`AlignConfig`] (or a derived class) from align text model configuration and align vision model\n-        configuration.\n-\n-        Returns:\n-            [`AlignConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n __all__ = [\"AlignTextConfig\", \"AlignVisionConfig\", \"AlignConfig\"]"
        },
        {
            "sha": "5e8c0f2a262ef9839de110e5a10af2c64ede9b87",
            "filename": "src/transformers/models/altclip/configuration_altclip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faltclip%2Fconfiguration_altclip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -368,17 +368,5 @@ def __init__(\n         self.logit_scale_init_value = logit_scale_init_value\n         self.initializer_factor = 1.0\n \n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: AltCLIPTextConfig, vision_config: AltCLIPVisionConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`AltCLIPConfig`] (or a derived class) from altclip text model configuration and altclip vision\n-        model configuration.\n-\n-        Returns:\n-            [`AltCLIPConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n __all__ = [\"AltCLIPTextConfig\", \"AltCLIPVisionConfig\", \"AltCLIPConfig\"]"
        },
        {
            "sha": "6e0a5590c3f769896675e801f76ecfa9234eb8be",
            "filename": "src/transformers/models/blip/configuration_blip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fblip%2Fconfiguration_blip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -313,17 +313,5 @@ def __init__(\n         self.image_text_hidden_size = image_text_hidden_size\n         self.label_smoothing = label_smoothing\n \n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: BlipTextConfig, vision_config: BlipVisionConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`BlipConfig`] (or a derived class) from blip text model configuration and blip vision model\n-        configuration.\n-\n-        Returns:\n-            [`BlipConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n __all__ = [\"BlipConfig\", \"BlipTextConfig\", \"BlipVisionConfig\"]"
        },
        {
            "sha": "4c84b0a294dafb35c991654c6e7df79c3fe58452",
            "filename": "src/transformers/models/bridgetower/configuration_bridgetower.py",
            "status": "modified",
            "additions": 0,
            "deletions": 11,
            "changes": 11,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbridgetower%2Fconfiguration_bridgetower.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -304,16 +304,5 @@ def __init__(\n         self.text_config = BridgeTowerTextConfig(**text_config)\n         self.vision_config = BridgeTowerVisionConfig(**vision_config)\n \n-    @classmethod\n-    def from_text_vision_configs(\n-        cls, text_config: BridgeTowerTextConfig, vision_config: BridgeTowerVisionConfig, **kwargs\n-    ):\n-        r\"\"\"\n-        Instantiate a [`BridgeTowerConfig`] (or a derived class) from BridgeTower text model configuration. Returns:\n-            [`BridgeTowerConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n __all__ = [\"BridgeTowerConfig\", \"BridgeTowerTextConfig\", \"BridgeTowerVisionConfig\"]"
        },
        {
            "sha": "e7c98d0d2d9f7bf13946cfaec3f340ebf5b81027",
            "filename": "src/transformers/models/chinese_clip/configuration_chinese_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchinese_clip%2Fconfiguration_chinese_clip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -373,18 +373,6 @@ def __init__(\n         self.initializer_factor = 1.0\n         self.initializer_range = 0.02\n \n-    @classmethod\n-    def from_text_vision_configs(\n-        cls, text_config: ChineseCLIPTextConfig, vision_config: ChineseCLIPVisionConfig, **kwargs\n-    ):\n-        r\"\"\"\n-        Instantiate a [`ChineseCLIPConfig`] (or a derived class) from Chinese-CLIP text model configuration and\n-        Chinese-CLIP vision model configuration. Returns:\n-            [`ChineseCLIPConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n class ChineseCLIPOnnxConfig(OnnxConfig):\n     @property"
        },
        {
            "sha": "900e8d373f5ad15d3d90f2da655abe10f3279f85",
            "filename": "src/transformers/models/clap/configuration_clap.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclap%2Fconfiguration_clap.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -378,17 +378,5 @@ def __init__(\n         self.initializer_factor = initializer_factor\n         self.num_hidden_layers = self.text_config.num_hidden_layers + len(self.audio_config.depths)\n \n-    @classmethod\n-    def from_text_audio_configs(cls, text_config: ClapTextConfig, audio_config: ClapAudioConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`ClapConfig`] (or a derived class) from clap text model configuration and clap audio model\n-        configuration.\n-\n-        Returns:\n-            [`ClapConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), audio_config=audio_config.to_dict(), **kwargs)\n-\n \n __all__ = [\"ClapAudioConfig\", \"ClapConfig\", \"ClapTextConfig\"]"
        },
        {
            "sha": "0b4fe6ba37f677919a17caf3171bb883d30c4f1f",
            "filename": "src/transformers/models/clip/configuration_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclip%2Fconfiguration_clip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -361,18 +361,6 @@ def __init__(\n         self.logit_scale_init_value = logit_scale_init_value\n         self.initializer_factor = 1.0\n \n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: CLIPTextConfig, vision_config: CLIPVisionConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`CLIPConfig`] (or a derived class) from clip text model configuration and clip vision model\n-        configuration.\n-\n-        Returns:\n-            [`CLIPConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n class CLIPOnnxConfig(OnnxConfig):\n     @property"
        },
        {
            "sha": "60b14eb7efbbf030b62d442b913d30f0e7a21b35",
            "filename": "src/transformers/models/clipseg/configuration_clipseg.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fclipseg%2Fconfiguration_clipseg.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -380,17 +380,5 @@ def __init__(\n         self.initializer_factor = 1.0\n         self.use_complex_transposed_convolution = use_complex_transposed_convolution\n \n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: CLIPSegTextConfig, vision_config: CLIPSegVisionConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`CLIPSegConfig`] (or a derived class) from clipseg text model configuration and clipseg vision\n-        model configuration.\n-\n-        Returns:\n-            [`CLIPSegConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n __all__ = [\"CLIPSegConfig\", \"CLIPSegTextConfig\", \"CLIPSegVisionConfig\"]"
        },
        {
            "sha": "d17288ede723c916d7cb1c6fe660ee68b7202788",
            "filename": "src/transformers/models/groupvit/configuration_groupvit.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgroupvit%2Fconfiguration_groupvit.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -357,18 +357,6 @@ def __init__(\n         self.initializer_factor = 1.0\n         self.output_segmentation = False\n \n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: GroupViTTextConfig, vision_config: GroupViTVisionConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`GroupViTConfig`] (or a derived class) from groupvit text model configuration and groupvit\n-        vision model configuration.\n-\n-        Returns:\n-            [`GroupViTConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n class GroupViTOnnxConfig(OnnxConfig):\n     @property"
        },
        {
            "sha": "be2110022fc612ee2c61d451aea6c5725ec345ac",
            "filename": "src/transformers/models/pix2struct/configuration_pix2struct.py",
            "status": "modified",
            "additions": 0,
            "deletions": 14,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fconfiguration_pix2struct.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -332,19 +332,5 @@ def __init__(\n \n         self.is_vqa = is_vqa\n \n-    @classmethod\n-    def from_text_vision_configs(\n-        cls, text_config: Pix2StructTextConfig, vision_config: Pix2StructVisionConfig, **kwargs\n-    ):\n-        r\"\"\"\n-        Instantiate a [`Pix2StructConfig`] (or a derived class) from pix2struct text model configuration and pix2struct\n-        vision model configuration.\n-\n-        Returns:\n-            [`Pix2StructConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n __all__ = [\"Pix2StructConfig\", \"Pix2StructTextConfig\", \"Pix2StructVisionConfig\"]"
        },
        {
            "sha": "0c182014fa272bb53b9e597894daa104a864a2c9",
            "filename": "src/transformers/models/siglip/configuration_siglip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip%2Fconfiguration_siglip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -253,17 +253,5 @@ def __init__(self, text_config=None, vision_config=None, **kwargs):\n \n         self.initializer_factor = 1.0\n \n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: SiglipTextConfig, vision_config: SiglipVisionConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`SiglipConfig`] (or a derived class) from siglip text model configuration and siglip vision\n-        model configuration.\n-\n-        Returns:\n-            [`SiglipConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n __all__ = [\"SiglipConfig\", \"SiglipTextConfig\", \"SiglipVisionConfig\"]"
        },
        {
            "sha": "67ef9df8f4f8f03608d7ef5c09b269c4cc74433b",
            "filename": "src/transformers/models/siglip2/configuration_siglip2.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconfiguration_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconfiguration_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fsiglip2%2Fconfiguration_siglip2.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -261,17 +261,5 @@ def __init__(self, text_config=None, vision_config=None, **kwargs):\n \n         self.initializer_factor = 1.0\n \n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: Siglip2TextConfig, vision_config: Siglip2VisionConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`Siglip2Config`] (or a derived class) from siglip2 text model configuration and siglip2 vision\n-        model configuration.\n-\n-        Returns:\n-            [`Siglip2Config`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n __all__ = [\"Siglip2Config\", \"Siglip2TextConfig\", \"Siglip2VisionConfig\"]"
        },
        {
            "sha": "66db819168e5f74fbc2f5db0f4bb5a43dce6fa28",
            "filename": "src/transformers/models/x_clip/configuration_x_clip.py",
            "status": "modified",
            "additions": 0,
            "deletions": 12,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fx_clip%2Fconfiguration_x_clip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -365,17 +365,5 @@ def __init__(\n         self.logit_scale_init_value = logit_scale_init_value\n         self.initializer_factor = 1.0\n \n-    @classmethod\n-    def from_text_vision_configs(cls, text_config: XCLIPTextConfig, vision_config: XCLIPVisionConfig, **kwargs):\n-        r\"\"\"\n-        Instantiate a [`XCLIPConfig`] (or a derived class) from xclip text model configuration and xclip vision model\n-        configuration.\n-\n-        Returns:\n-            [`XCLIPConfig`]: An instance of a configuration object\n-        \"\"\"\n-\n-        return cls(text_config=text_config.to_dict(), vision_config=vision_config.to_dict(), **kwargs)\n-\n \n __all__ = [\"XCLIPConfig\", \"XCLIPTextConfig\", \"XCLIPVisionConfig\"]"
        },
        {
            "sha": "4e897a554c886a03200a3dec4484c35b2d33bdea",
            "filename": "tests/models/align/test_modeling_align.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Falign%2Ftest_modeling_align.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Falign%2Ftest_modeling_align.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -408,8 +408,10 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, token_type_ids, input_mask, pixel_values\n \n     def get_config(self):\n-        return AlignConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n+        return AlignConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n         )\n \n     def create_and_check_model(self, config, input_ids, token_type_ids, attention_mask, pixel_values):"
        },
        {
            "sha": "d1656d06fe60c0148e21bfd1964e696b54b1996c",
            "filename": "tests/models/altclip/test_modeling_altclip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faltclip%2Ftest_modeling_altclip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -376,8 +376,10 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return AltCLIPConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n+        return AltCLIPConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n         )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):"
        },
        {
            "sha": "d340d3e56997edc6c092d30b5b1e3d955a9fbcc6",
            "filename": "tests/models/blip/test_modeling_blip.py",
            "status": "modified",
            "additions": 16,
            "deletions": 8,
            "changes": 24,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fblip%2Ftest_modeling_blip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -381,8 +381,10 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return BlipConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n+        return BlipConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n         )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n@@ -664,8 +666,10 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return BlipConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n+        return BlipConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n         )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n@@ -713,8 +717,10 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return BlipConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n+        return BlipConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n         )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n@@ -761,8 +767,10 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return BlipConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n+        return BlipConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n         )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):"
        },
        {
            "sha": "b0c26f1e7d2ada9e03fdf7253e565a8483b4c7bd",
            "filename": "tests/models/bridgetower/test_modeling_bridgetower.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_modeling_bridgetower.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -203,9 +203,9 @@ def prepare_config_and_inputs(self):\n         return (config, input_ids, attention_mask, pixel_values, pixel_mask)\n \n     def get_config(self):\n-        return BridgeTowerConfig.from_text_vision_configs(\n-            text_config=self.text_model_tester.get_config(),\n-            vision_config=self.vision_model_tester.get_config(),\n+        return BridgeTowerConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n             share_cross_modal_transformer_layers=self.share_cross_modal_transformer_layers,\n             share_link_tower_layers=self.share_link_tower_layers,\n             link_tower_type=self.link_tower_type,"
        },
        {
            "sha": "7ec9dd42dcc00de0cd894ff2235a1df15ecb567a",
            "filename": "tests/models/chinese_clip/test_modeling_chinese_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fchinese_clip%2Ftest_modeling_chinese_clip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -515,8 +515,10 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, token_type_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return ChineseCLIPConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n+        return ChineseCLIPConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n         )\n \n     def create_and_check_model(self, config, input_ids, token_type_ids, attention_mask, pixel_values):"
        },
        {
            "sha": "22579fa99fcd4a88040583fcecae14e758951140",
            "filename": "tests/models/clap/test_modeling_clap.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclap%2Ftest_modeling_clap.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -459,8 +459,10 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, input_features\n \n     def get_config(self):\n-        return ClapConfig.from_text_audio_configs(\n-            self.text_model_tester.get_config(), self.audio_model_tester.get_config(), projection_dim=64\n+        return ClapConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            audio_config=self.audio_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n         )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, input_features):"
        },
        {
            "sha": "92de6d4583bdd592aa0fedb41719b420f5bc117e",
            "filename": "tests/models/clip/test_modeling_clip.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclip%2Ftest_modeling_clip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -502,8 +502,10 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return CLIPConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n+        return CLIPConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n         )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):"
        },
        {
            "sha": "dcbaeeb66165b60ff2424d629f99f7c00fefa28c",
            "filename": "tests/models/clipseg/test_modeling_clipseg.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fclipseg%2Ftest_modeling_clipseg.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -374,9 +374,9 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return CLIPSegConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(),\n-            self.vision_model_tester.get_config(),\n+        return CLIPSegConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n             projection_dim=64,\n             reduce_dim=32,\n             extract_layers=self.extract_layers,"
        },
        {
            "sha": "f2aceb15e4364cba91fe0d929a0b1f31f924b1bb",
            "filename": "tests/models/efficientloftr/test_modeling_efficientloftr.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fefficientloftr%2Ftest_modeling_efficientloftr.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -216,7 +216,7 @@ def check_hidden_states_output(inputs_dict, config, model_class):\n \n             self.assertListEqual(\n                 list(hidden_states[0].shape[-2:]),\n-                [self.model_tester.image_height // 2, self.model_tester.image_width // 2],\n+                [self.model_tester.image_height, self.model_tester.image_width],\n             )\n \n         config, inputs_dict = self.model_tester.prepare_config_and_inputs_for_common()"
        },
        {
            "sha": "73af5e4b4b31b6f8370c589cebcd433810c7aa5f",
            "filename": "tests/models/groupvit/test_modeling_groupvit.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgroupvit%2Ftest_modeling_groupvit.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -497,8 +497,10 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return GroupViTConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(), self.vision_model_tester.get_config(), projection_dim=64\n+        return GroupViTConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n         )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):"
        },
        {
            "sha": "bbbe6824be6c97d3a884788f4888a19885b27a7e",
            "filename": "tests/models/owlv2/test_modeling_owlv2.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlv2%2Ftest_modeling_owlv2.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -375,7 +375,11 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return Owlv2Config.from_text_vision_configs(self.text_config, self.vision_config, projection_dim=64)\n+        return Owlv2Config(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            projection_dim=64,\n+        )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n         model = Owlv2Model(config).to(torch_device).eval()\n@@ -589,7 +593,11 @@ def prepare_config_and_inputs(self):\n         return config, pixel_values, input_ids, attention_mask\n \n     def get_config(self):\n-        return Owlv2Config.from_text_vision_configs(self.text_config, self.vision_config, projection_dim=64)\n+        return Owlv2Config(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            projection_dim=64,\n+        )\n \n     def create_and_check_model(self, config, pixel_values, input_ids, attention_mask):\n         model = Owlv2ForObjectDetection(config).to(torch_device).eval()"
        },
        {
            "sha": "d150ae7899f53c488bcd4a0f7f39a23ad266a6cb",
            "filename": "tests/models/owlvit/test_modeling_owlvit.py",
            "status": "modified",
            "additions": 10,
            "deletions": 2,
            "changes": 12,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fowlvit%2Ftest_modeling_owlvit.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -371,7 +371,11 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return OwlViTConfig.from_text_vision_configs(self.text_config, self.vision_config, projection_dim=64)\n+        return OwlViTConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            projection_dim=64,\n+        )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):\n         model = OwlViTModel(config).to(torch_device).eval()\n@@ -583,7 +587,11 @@ def prepare_config_and_inputs(self):\n         return config, pixel_values, input_ids, attention_mask\n \n     def get_config(self):\n-        return OwlViTConfig.from_text_vision_configs(self.text_config, self.vision_config, projection_dim=64)\n+        return OwlViTConfig(\n+            text_config=self.text_config,\n+            vision_config=self.vision_config,\n+            projection_dim=64,\n+        )\n \n     def create_and_check_model(self, config, pixel_values, input_ids, attention_mask):\n         model = OwlViTForObjectDetection(config).to(torch_device).eval()"
        },
        {
            "sha": "d6a0fa5f08d65e4b00f38c14eb17d9e7edaed146",
            "filename": "tests/models/pix2struct/test_modeling_pix2struct.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_modeling_pix2struct.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -383,7 +383,11 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, flattened_patches\n \n     def get_config(self, text_config, vision_config):\n-        return Pix2StructConfig.from_text_vision_configs(text_config, vision_config, projection_dim=64)\n+        return Pix2StructConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n+            projection_dim=64,\n+        )\n \n     def prepare_config_and_inputs_for_common(self):\n         config_and_inputs = self.prepare_config_and_inputs()"
        },
        {
            "sha": "b5c14280bbb2447413810cb2526e1809c9071887",
            "filename": "tests/models/siglip/test_modeling_siglip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip%2Ftest_modeling_siglip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -428,9 +428,9 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return SiglipConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(),\n-            self.vision_model_tester.get_config(),\n+        return SiglipConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n         )\n \n     def create_and_check_model(self, config, input_ids, attention_mask, pixel_values):"
        },
        {
            "sha": "40ca27f1bd929ea0738185cd565d610db22f38b6",
            "filename": "tests/models/siglip2/test_modeling_siglip2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_modeling_siglip2.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -514,9 +514,9 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values, pixel_attention_mask, spatial_shapes\n \n     def get_config(self):\n-        return Siglip2Config.from_text_vision_configs(\n-            self.text_model_tester.get_config(),\n-            self.vision_model_tester.get_config(),\n+        return Siglip2Config(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n         )\n \n     def create_and_check_model("
        },
        {
            "sha": "5c125f07973c9872955b5866f83b48707a09fbe0",
            "filename": "tests/models/x_clip/test_modeling_x_clip.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/b56d721397028a562b1e6444a8e493171e659202/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fx_clip%2Ftest_modeling_x_clip.py?ref=b56d721397028a562b1e6444a8e493171e659202",
            "patch": "@@ -493,9 +493,9 @@ def prepare_config_and_inputs(self):\n         return config, input_ids, attention_mask, pixel_values\n \n     def get_config(self):\n-        return XCLIPConfig.from_text_vision_configs(\n-            self.text_model_tester.get_config(),\n-            self.vision_model_tester.get_config(),\n+        return XCLIPConfig(\n+            text_config=self.text_model_tester.get_config().to_dict(),\n+            vision_config=self.vision_model_tester.get_config().to_dict(),\n             projection_dim=self.projection_dim,\n         )\n "
        }
    ],
    "stats": {
        "total": 327,
        "additions": 117,
        "deletions": 210
    }
}