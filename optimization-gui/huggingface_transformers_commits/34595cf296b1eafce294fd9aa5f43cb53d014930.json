{
    "author": "ydshieh",
    "message": "Even more test data cached (#40636)\n\nfix\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "34595cf296b1eafce294fd9aa5f43cb53d014930",
    "files": [
        {
            "sha": "3ff8aad72ccaccb00d73642da4369e6a6beecae6",
            "filename": "tests/models/aria/test_processing_aria.py",
            "status": "modified",
            "additions": 18,
            "deletions": 20,
            "changes": 38,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Faria%2Ftest_processing_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Faria%2Ftest_processing_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faria%2Ftest_processing_aria.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -15,23 +15,17 @@\n import shutil\n import tempfile\n import unittest\n-from io import BytesIO\n \n import numpy as np\n-import requests\n \n from transformers import AriaProcessor\n+from transformers.image_utils import load_image\n from transformers.models.auto.processing_auto import AutoProcessor\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin, url_to_local_path\n \n \n-if is_vision_available():\n-    from PIL import Image\n-\n-\n @require_torch\n @require_vision\n class AriaProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n@@ -42,21 +36,17 @@ def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n         processor = AriaProcessor.from_pretrained(\"m-ric/Aria_hf_2\", size_conversion={490: 2, 980: 2})\n         processor.save_pretrained(cls.tmpdirname)\n-        cls.image1 = Image.open(\n-            BytesIO(\n-                requests.get(\n-                    \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n-                ).content\n+        cls.image1 = load_image(\n+            url_to_local_path(\n+                \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n             )\n         )\n-        cls.image2 = Image.open(\n-            BytesIO(requests.get(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\").content)\n+        cls.image2 = load_image(\n+            url_to_local_path(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\")\n         )\n-        cls.image3 = Image.open(\n-            BytesIO(\n-                requests.get(\n-                    \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n-                ).content\n+        cls.image3 = load_image(\n+            url_to_local_path(\n+                \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n             )\n         )\n         cls.bos_token = \"<|im_start|>\"\n@@ -93,6 +83,9 @@ def get_processor(self, **kwargs):\n \n     @classmethod\n     def tearDownClass(cls):\n+        cls.image1.close()\n+        cls.image2.close()\n+        cls.image3.close()\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n     # Copied from tests.models.llava.test_processing_llava.LlavaProcessorTest.test_get_num_vision_tokens\n@@ -273,7 +266,12 @@ def test_image_chat_template_accepts_processing_kwargs(self):\n \n         # Now test the ability to return dict\n         messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": url_to_local_path(\"https://www.ilankelman.org/stopsigns/australia.jpg\")}\n+            {\n+                \"type\": \"image\",\n+                \"url\": url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n+            }\n         )\n         out_dict = processor.apply_chat_template(\n             messages,"
        },
        {
            "sha": "436cba19c29098a58b03465425aec8edfb53ed63",
            "filename": "tests/models/aya_vision/test_modeling_aya_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Faya_vision%2Ftest_modeling_aya_vision.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -379,7 +379,10 @@ def test_small_model_integration_batched_generate(self):\n                 {\n                     \"role\": \"user\",\n                     \"content\": [\n-                        {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                        },\n                         {\"type\": \"text\", \"text\": \"Describe this image\"},\n                     ],\n                 },"
        },
        {
            "sha": "fae53e632362d742076dc5344d02dab0c0645d38",
            "filename": "tests/models/bridgetower/test_image_processing_bridgetower.py",
            "status": "modified",
            "additions": 3,
            "deletions": 7,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fbridgetower%2Ftest_image_processing_bridgetower.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -16,17 +16,15 @@\n import unittest\n from typing import Optional, Union\n \n-import requests\n-\n+from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import BridgeTowerImageProcessor\n \n     if is_torchvision_available():\n@@ -130,9 +128,7 @@ def test_slow_fast_equivalence(self):\n         if self.image_processing_class is None or self.fast_image_processing_class is None:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n "
        },
        {
            "sha": "96843faa95f797aaff0f1e644c5cd8d47286f509",
            "filename": "tests/models/cohere2_vision/test_modeling_cohere2_vision.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fcohere2_vision%2Ftest_modeling_cohere2_vision.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -330,7 +330,10 @@ def test_model_integration_batched_generate(self):\n                 {\n                     \"role\": \"user\",\n                     \"content\": [\n-                        {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                        },\n                         {\"type\": \"text\", \"text\": \"Describe this image\"},\n                     ],\n                 },"
        },
        {
            "sha": "a2d1950dcdc4d80be4d4c27d02bcaae1d2056abe",
            "filename": "tests/models/deepseek_vl/test_modeling_deepseek_vl.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl%2Ftest_modeling_deepseek_vl.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -333,7 +333,10 @@ def test_model_text_generation_with_multi_image(self):\n                     {\"type\": \"text\", \"text\": \"What's the difference between\"},\n                     {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n                     {\"type\": \"text\", \"text\": \" and \"},\n-                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                    },\n                 ],\n             }\n         ]"
        },
        {
            "sha": "69f7fbbe5ee6f0040e70099904456a86911d2a84",
            "filename": "tests/models/deepseek_vl_hybrid/test_image_processing_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_image_processing_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_image_processing_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_image_processing_deepseek_vl_hybrid.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -16,12 +16,13 @@\n import unittest\n \n import numpy as np\n-import requests\n \n+from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_torch_available():\n@@ -226,9 +227,7 @@ def test_slow_fast_equivalence(self):\n         if self.image_processing_class is None or self.fast_image_processing_class is None:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n "
        },
        {
            "sha": "fbb904da735b41d28783e769fcdf432710755c03",
            "filename": "tests/models/deepseek_vl_hybrid/test_modeling_deepseek_vl_hybrid.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fdeepseek_vl_hybrid%2Ftest_modeling_deepseek_vl_hybrid.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -382,7 +382,10 @@ def test_model_text_generation_with_multi_image(self):\n                     {\"type\": \"text\", \"text\": \"What's the difference between\"},\n                     {\"type\": \"image\", \"url\": \"http://images.cocodataset.org/val2017/000000039769.jpg\"},\n                     {\"type\": \"text\", \"text\": \" and \"},\n-                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                    },\n                 ],\n             }\n         ]"
        },
        {
            "sha": "f2c8d7ac1dae165b8d15f1627a3eb619b2a9e9ce",
            "filename": "tests/models/eomt/test_image_processing_eomt.py",
            "status": "modified",
            "additions": 5,
            "deletions": 4,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Feomt%2Ftest_image_processing_eomt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Feomt%2Ftest_image_processing_eomt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Feomt%2Ftest_image_processing_eomt.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -16,13 +16,14 @@\n import unittest\n \n import numpy as np\n-import requests\n from datasets import load_dataset\n \n+from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_torch_available():\n@@ -261,7 +262,7 @@ def test_post_process_semantic_segmentation(self):\n         processor = self.image_processing_class(**self.image_processor_dict)\n         # Set longest_edge to None to test for semantic segmentatiom.\n         processor.size = {\"shortest_edge\": 18, \"longest_edge\": None}\n-        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+        image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n \n         inputs = processor(images=image, do_split_image=True, return_tensors=\"pt\")\n         patch_offsets = inputs[\"patch_offsets\"]\n@@ -276,7 +277,7 @@ def test_post_process_semantic_segmentation(self):\n \n     def test_post_process_panoptic_segmentation(self):\n         processor = self.image_processing_class(**self.image_processor_dict)\n-        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+        image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n \n         original_sizes = [image.size[::-1], image.size[::-1]]\n \n@@ -293,7 +294,7 @@ def test_post_process_panoptic_segmentation(self):\n \n     def test_post_process_instance_segmentation(self):\n         processor = self.image_processing_class(**self.image_processor_dict)\n-        image = Image.open(requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw)\n+        image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n \n         original_sizes = [image.size[::-1], image.size[::-1]]\n "
        },
        {
            "sha": "16714d1ece1652a34dbe761368b6bd7ae3301387",
            "filename": "tests/models/fuyu/test_processing_fuyu.py",
            "status": "modified",
            "additions": 6,
            "deletions": 11,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Ffuyu%2Ftest_processing_fuyu.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -1,25 +1,18 @@\n-import io\n import tempfile\n import unittest\n from shutil import rmtree\n \n-import requests\n-\n from transformers import (\n     AutoProcessor,\n     AutoTokenizer,\n     FuyuImageProcessor,\n     FuyuProcessor,\n     is_torch_available,\n-    is_vision_available,\n )\n+from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_vision\n \n-from ...test_processing_common import ProcessorTesterMixin\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n+from ...test_processing_common import ProcessorTesterMixin, url_to_local_path\n \n \n if is_torch_available():\n@@ -44,8 +37,10 @@ def setUpClass(cls):\n         processor.save_pretrained(cls.tmpdirname)\n \n         cls.text_prompt = \"Generate a coco-style caption.\\\\n\"\n-        bus_image_url = \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bus.png\"\n-        cls.bus_image_pil = Image.open(io.BytesIO(requests.get(bus_image_url).content))\n+        bus_image_url = url_to_local_path(\n+            \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bus.png\"\n+        )\n+        cls.bus_image_pil = load_image(bus_image_url)\n \n     @classmethod\n     def tearDownClass(cls):"
        },
        {
            "sha": "4d64bb9a6d438de58be1f680c208e6265d540a79",
            "filename": "tests/models/gemma3/test_modeling_gemma3.py",
            "status": "modified",
            "additions": 12,
            "deletions": 3,
            "changes": 15,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3%2Ftest_modeling_gemma3.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -427,7 +427,10 @@ def test_model_4b_batch(self):\n                         \"type\": \"image\",\n                         \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\",\n                     },\n-                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                    },\n                     {\"type\": \"text\", \"text\": \"Are these images identical?\"},\n                 ],\n             },\n@@ -545,7 +548,10 @@ def test_model_4b_batch_crops(self):\n                         \"type\": \"image\",\n                         \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\",\n                     },\n-                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                    },\n                     {\"type\": \"text\", \"text\": \"Are these images identical?\"},\n                 ],\n             },\n@@ -605,7 +611,10 @@ def test_model_4b_multiimage(self):\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                    },\n                     {\"type\": \"text\", \"text\": \"What do you see here?\"},\n                 ],\n             },"
        },
        {
            "sha": "4e2581757a1c71dba4436b6a792288c2d211885a",
            "filename": "tests/models/gemma3n/test_modeling_gemma3n.py",
            "status": "modified",
            "additions": 8,
            "deletions": 2,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fgemma3n%2Ftest_modeling_gemma3n.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -984,7 +984,10 @@ def test_model_4b_batch(self):\n                         \"type\": \"image\",\n                         \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\",\n                     },\n-                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                    },\n                     {\"type\": \"text\", \"text\": \"Are these images identical?\"},\n                 ],\n             },\n@@ -1040,7 +1043,10 @@ def test_model_4b_multiimage(self):\n             {\n                 \"role\": \"user\",\n                 \"content\": [\n-                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                    },\n                     {\"type\": \"text\", \"text\": \"What do you see here?\"},\n                 ],\n             },"
        },
        {
            "sha": "755107d2c2e2d396b82574b1693a2f3dd27d1100",
            "filename": "tests/models/glm4v/test_processor_glm4v.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fglm4v%2Ftest_processor_glm4v.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -235,8 +235,12 @@ def test_apply_chat_template_video_frame_sampling(self):\n         messages[0][0][\"content\"][0] = {\n             \"type\": \"video\",\n             \"url\": [\n-                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n-                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+                url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n+                url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n             ],\n         }\n         out_dict_with_video = processor.apply_chat_template("
        },
        {
            "sha": "d3f816c15405f1ecefea96546a70246b738b608a",
            "filename": "tests/models/idefics2/test_processing_idefics2.py",
            "status": "modified",
            "additions": 13,
            "deletions": 18,
            "changes": 31,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fidefics2%2Ftest_processing_idefics2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fidefics2%2Ftest_processing_idefics2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics2%2Ftest_processing_idefics2.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -15,20 +15,16 @@\n import shutil\n import tempfile\n import unittest\n-from io import BytesIO\n-\n-import requests\n \n from transformers import Idefics2Processor\n+from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_vision_available\n \n-from ...test_processing_common import ProcessorTesterMixin\n+from ...test_processing_common import ProcessorTesterMixin, url_to_local_path\n \n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import (\n         AutoProcessor,\n         Idefics2Processor,\n@@ -48,21 +44,17 @@ def setUpClass(cls):\n \n         processor.save_pretrained(cls.tmpdirname)\n \n-        cls.image1 = Image.open(\n-            BytesIO(\n-                requests.get(\n-                    \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n-                ).content\n+        cls.image1 = load_image(\n+            url_to_local_path(\n+                \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n             )\n         )\n-        cls.image2 = Image.open(\n-            BytesIO(requests.get(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\").content)\n+        cls.image2 = load_image(\n+            url_to_local_path(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\")\n         )\n-        cls.image3 = Image.open(\n-            BytesIO(\n-                requests.get(\n-                    \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n-                ).content\n+        cls.image3 = load_image(\n+            url_to_local_path(\n+                \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n             )\n         )\n         cls.bos_token = processor.tokenizer.bos_token\n@@ -89,6 +81,9 @@ def prepare_processor_dict():\n \n     @classmethod\n     def tearDownClass(cls):\n+        cls.image1.close()\n+        cls.image2.close()\n+        cls.image3.close()\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n     def test_process_interleaved_images_prompts_no_image_splitting(self):"
        },
        {
            "sha": "753c903bc02ee4cc94f6f8c6d94c56a6170c9036",
            "filename": "tests/models/idefics3/test_image_processing_idefics3.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_image_processing_idefics3.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -17,13 +17,13 @@\n import unittest\n \n import numpy as np\n-import requests\n \n-from transformers.image_utils import PILImageResampling\n+from transformers.image_utils import PILImageResampling, load_image\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin\n+from ...test_processing_common import url_to_local_path\n \n \n if is_vision_available():\n@@ -298,9 +298,7 @@ def test_slow_fast_equivalence(self):\n         if self.image_processing_class is None or self.fast_image_processing_class is None:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n         dummy_image = dummy_image.resize((100, 150))\n         image_processor_slow = self.image_processing_class(\n             **self.image_processor_dict, resample=PILImageResampling.BICUBIC"
        },
        {
            "sha": "6a14dda4af8742db0ef27783f8c61631b6af27a6",
            "filename": "tests/models/idefics3/test_processing_idefics3.py",
            "status": "modified",
            "additions": 13,
            "deletions": 20,
            "changes": 33,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fidefics3%2Ftest_processing_idefics3.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -15,21 +15,15 @@\n import shutil\n import tempfile\n import unittest\n-from io import BytesIO\n \n import numpy as np\n-import requests\n \n from transformers import Idefics3Processor\n+from transformers.image_utils import load_image\n from transformers.models.auto.processing_auto import AutoProcessor\n from transformers.testing_utils import require_torch, require_vision\n-from transformers.utils import is_vision_available\n \n-from ...test_processing_common import ProcessorTesterMixin\n-\n-\n-if is_vision_available():\n-    from PIL import Image\n+from ...test_processing_common import ProcessorTesterMixin, url_to_local_path\n \n \n @require_torch\n@@ -42,21 +36,17 @@ def setUpClass(cls):\n         cls.tmpdirname = tempfile.mkdtemp()\n         processor = Idefics3Processor.from_pretrained(\"HuggingFaceM4/Idefics3-8B-Llama3\", image_seq_len=2)\n         processor.save_pretrained(cls.tmpdirname)\n-        cls.image1 = Image.open(\n-            BytesIO(\n-                requests.get(\n-                    \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n-                ).content\n+        cls.image1 = load_image(\n+            url_to_local_path(\n+                \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n             )\n         )\n-        cls.image2 = Image.open(\n-            BytesIO(requests.get(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\").content)\n+        cls.image2 = load_image(\n+            url_to_local_path(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\")\n         )\n-        cls.image3 = Image.open(\n-            BytesIO(\n-                requests.get(\n-                    \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n-                ).content\n+        cls.image3 = load_image(\n+            url_to_local_path(\n+                \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n             )\n         )\n         cls.bos_token = processor.tokenizer.bos_token\n@@ -120,6 +110,9 @@ def get_split_image_expected_tokens(self, processor, image_rows, image_cols):\n \n     @classmethod\n     def tearDownClass(cls):\n+        cls.image1.close()\n+        cls.image2.close()\n+        cls.image3.close()\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n     def test_process_interleaved_images_prompts_no_image_splitting(self):"
        },
        {
            "sha": "297dc6cffe853578f00d5859dbc41caf1d99df1c",
            "filename": "tests/models/internvl/test_modeling_internvl.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_modeling_internvl.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -362,7 +362,12 @@ def test_qwen2_small_model_integration_batched_generate(self):\n             \"<|im_start|>user\\n<IMG_CONTEXT>\\nDescribe this image<|im_end|>\\n<|im_start|>assistant\\n\",\n         ]\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n-        image2 = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n+        image2 = Image.open(\n+            requests.get(\n+                \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                stream=True,\n+            ).raw\n+        )\n \n         inputs = processor(text=prompt, images=[[image1], [image2]], padding=True, return_tensors=\"pt\").to(\n             torch_device, dtype=torch.float16\n@@ -736,7 +741,12 @@ def test_llama_small_model_integration_batched_generate(self):\n             \"<|im_start|>user\\n<IMG_CONTEXT>\\nDescribe this image<|im_end|>\\n<|im_start|>assistant\\n\",\n         ]\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n-        image2 = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n+        image2 = Image.open(\n+            requests.get(\n+                \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                stream=True,\n+            ).raw\n+        )\n \n         inputs = processor(text=prompt, images=[[image1], [image2]], padding=True, return_tensors=\"pt\").to(\n             torch_device, dtype=torch.float16"
        },
        {
            "sha": "76e91a50d3eda4ef027d07d41c45980a1c5ef342",
            "filename": "tests/models/internvl/test_processing_internvl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Finternvl%2Ftest_processing_internvl.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -258,8 +258,12 @@ def test_apply_chat_template_video_frame_sampling(self):\n         messages[0][0][\"content\"][0] = {\n             \"type\": \"video\",\n             \"url\": [\n-                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n-                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+                url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n+                url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n             ],\n         }\n         out_dict_with_video = processor.apply_chat_template("
        },
        {
            "sha": "5752b521c74e4bd92c4f759c73d0593871aa345e",
            "filename": "tests/models/janus/test_processing_janus.py",
            "status": "modified",
            "additions": 18,
            "deletions": 3,
            "changes": 21,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fjanus%2Ftest_processing_janus.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -128,7 +128,12 @@ def test_chat_template_single(self):\n \n         # Now test the ability to return dict\n         messages[0][0][\"content\"][1].update(\n-            {\"type\": \"image\", \"url\": url_to_local_path(\"https://www.ilankelman.org/stopsigns/australia.jpg\")}\n+            {\n+                \"type\": \"image\",\n+                \"url\": url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n+            }\n         )\n         out_dict = processor.apply_chat_template(messages, add_generation_prompt=True, tokenize=True, return_dict=True)\n         self.assertTrue(self.images_input_name in out_dict)\n@@ -269,7 +274,12 @@ def test_chat_template_batched(self):\n \n         # Verify image inputs are included in the output dict\n         batched_messages[0][0][\"content\"][1].update(\n-            {\"type\": \"image\", \"url\": url_to_local_path(\"https://www.ilankelman.org/stopsigns/australia.jpg\")}\n+            {\n+                \"type\": \"image\",\n+                \"url\": url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n+            }\n         )\n         batched_messages[1][0][\"content\"][1].update(\n             {\"type\": \"image\", \"url\": url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\")}\n@@ -419,7 +429,12 @@ def test_chat_template_accepts_processing_kwargs(self):\n         # Test 3: Image processing kwargs\n         # Add an image and test image processing parameters\n         messages[0][0][\"content\"].append(\n-            {\"type\": \"image\", \"url\": url_to_local_path(\"https://www.ilankelman.org/stopsigns/australia.jpg\")}\n+            {\n+                \"type\": \"image\",\n+                \"url\": url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n+            }\n         )\n         # Process with image rescaling and verify the pixel values are negative\n         out_dict = processor.apply_chat_template("
        },
        {
            "sha": "d167ad4ebe574ef2cdb59833a385e6222f5846ef",
            "filename": "tests/models/kosmos2/test_processing_kosmos2.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2%2Ftest_processing_kosmos2.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -20,8 +20,8 @@\n \n import numpy as np\n import pytest\n-import requests\n \n+from transformers.image_utils import load_image\n from transformers.models.auto.processing_auto import processor_class_from_name\n from transformers.testing_utils import (\n     get_tests_dir,\n@@ -32,7 +32,7 @@\n )\n from transformers.utils import is_vision_available\n \n-from ...test_processing_common import ProcessorTesterMixin\n+from ...test_processing_common import ProcessorTesterMixin, url_to_local_path\n \n \n if is_vision_available():\n@@ -187,7 +187,7 @@ def test_tokenizer_decode(self):\n \n     @require_torch\n     def test_full_processor(self):\n-        url = \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\"\n+        url = url_to_local_path(\"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\")\n \n         processor = Kosmos2Processor.from_pretrained(\"microsoft/kosmos-2-patch14-224\")\n \n@@ -205,7 +205,7 @@ def test_full_processor(self):\n         ]\n         # fmt: on\n \n-        image = Image.open(requests.get(url, stream=True).raw)\n+        image = load_image(url)\n         # To match the official (microsoft) Kosmos-2 demo from which the expected values here are grabbed\n         image_path = os.path.join(self.tmpdirname, \"image.jpg\")\n         image.save(image_path)"
        },
        {
            "sha": "38c7340632c30d1e9c99c9e8b8e8d030aeb7e0ea",
            "filename": "tests/models/kosmos2_5/test_image_processing_kosmos2_5.py",
            "status": "modified",
            "additions": 8,
            "deletions": 9,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fkosmos2_5%2Ftest_image_processing_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fkosmos2_5%2Ftest_image_processing_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_image_processing_kosmos2_5.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -18,13 +18,14 @@\n \n import numpy as np\n import pytest\n-import requests\n from packaging import version\n \n+from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_torch_accelerator, require_vision, slow, torch_device\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_torch_available():\n@@ -70,8 +71,10 @@ def prepare_image_processor_dict(self):\n         return {\"do_normalize\": self.do_normalize, \"do_convert_rgb\": self.do_convert_rgb}\n \n     def prepare_dummy_image(self):\n-        img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n-        raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+        img_url = url_to_local_path(\n+            \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+        )\n+        raw_image = load_image(img_url).convert(\"RGB\")\n         return raw_image\n \n     def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):\n@@ -111,9 +114,7 @@ def test_slow_fast_equivalence(self):\n         if self.image_processing_class is None or self.fast_image_processing_class is None:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n \n@@ -356,9 +357,7 @@ def test_slow_fast_equivalence(self):\n         if self.image_processing_class is None or self.fast_image_processing_class is None:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n "
        },
        {
            "sha": "1bc41307712c99c9df2c6fb612db8edd59efa242",
            "filename": "tests/models/kosmos2_5/test_processor_kosmos2_5.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fkosmos2_5%2Ftest_processor_kosmos2_5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fkosmos2_5%2Ftest_processor_kosmos2_5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fkosmos2_5%2Ftest_processor_kosmos2_5.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -21,15 +21,15 @@\n \n import numpy as np\n import pytest\n-import requests\n \n+from transformers.image_utils import load_image\n from transformers.testing_utils import (\n     require_torch,\n     require_vision,\n )\n from transformers.utils import is_vision_available\n \n-from ...test_processing_common import ProcessorTesterMixin\n+from ...test_processing_common import ProcessorTesterMixin, url_to_local_path\n \n \n if is_vision_available():\n@@ -299,7 +299,7 @@ def test_structured_kwargs_nested_from_dict(self):\n \n     @require_torch\n     def test_full_processor(self):\n-        url = \"https://huggingface.co/kirp/kosmos2_5/resolve/main/receipt_00008.png\"\n+        url = url_to_local_path(\"https://huggingface.co/kirp/kosmos2_5/resolve/main/receipt_00008.png\")\n         processor = AutoProcessor.from_pretrained(\"microsoft/kosmos-2.5\")\n         texts = [\"<md>\", \"<ocr>\"]\n         expected_input_ids = [\n@@ -308,7 +308,7 @@ def test_full_processor(self):\n         ]\n         expected_attention_mask = [[1], [1]]\n \n-        image = Image.open(requests.get(url, stream=True).raw)\n+        image = load_image(url)\n         # To match the official (microsoft) Kosmos-2 demo from which the expected values here are grabbed\n         image_path = os.path.join(self.tmpdirname, \"image.png\")\n         image.save(image_path)"
        },
        {
            "sha": "d2b30ba26b716c2e46fd9164d39df1d56026912b",
            "filename": "tests/models/llama4/test_modeling_llama4.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllama4%2Ftest_modeling_llama4.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -74,7 +74,10 @@ def setUp(self):\n                         \"type\": \"image\",\n                         \"url\": \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/cow_beach_1.png\",\n                     },\n-                    {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                    },\n                     {\"type\": \"text\", \"text\": \"Are these images identical?\"},\n                 ],\n             },"
        },
        {
            "sha": "e270220dc1a3657c60d3d50451893788c48c42d9",
            "filename": "tests/models/llava_onevision/test_modeling_llava_onevision.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fllava_onevision%2Ftest_modeling_llava_onevision.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -416,7 +416,7 @@ def test_small_model_integration_test_multi_image(self):\n             device_map=torch_device,\n         )\n \n-        url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n         prompt = (\n             \"user\\n<image><image>\\nWhat is the difference between these images?<|im_end|>\\n<|im_start|>assistant\\n\"\n@@ -444,7 +444,7 @@ def test_small_model_integration_test_multi_image_nested(self):\n             device_map=torch_device,\n         )\n \n-        url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n         image = Image.open(requests.get(url, stream=True).raw)\n         prompts = [\n             \"user\\nTell me about the french revolution.<|im_end|>\\n<|im_start|>assistant\\n\",  # text-only case"
        },
        {
            "sha": "ab07dbdf7d9f2bb86216fd4d0bb45180b77a86b8",
            "filename": "tests/models/mistral3/test_modeling_mistral3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_modeling_mistral3.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -346,7 +346,10 @@ def test_mistral3_integration_batched_generate(self):\n                 {\n                     \"role\": \"user\",\n                     \"content\": [\n-                        {\"type\": \"image\", \"url\": \"https://www.ilankelman.org/stopsigns/australia.jpg\"},\n+                        {\n+                            \"type\": \"image\",\n+                            \"url\": \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                        },\n                         {\"type\": \"text\", \"text\": \"Describe this image\"},\n                     ],\n                 },"
        },
        {
            "sha": "7373524b081add84f182bb6747c7dd03fbef6292",
            "filename": "tests/models/mistral3/test_processing_mistral3.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmistral3%2Ftest_processing_mistral3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmistral3%2Ftest_processing_mistral3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmistral3%2Ftest_processing_mistral3.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -22,7 +22,7 @@\n from transformers.testing_utils import require_vision\n from transformers.utils import is_torch_available\n \n-from ...test_processing_common import ProcessorTesterMixin\n+from ...test_processing_common import ProcessorTesterMixin, url_to_local_path\n \n \n if is_torch_available():\n@@ -37,7 +37,9 @@ class Mistral3ProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n \n     @classmethod\n     def setUpClass(cls):\n-        cls.url_0 = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        cls.url_0 = url_to_local_path(\n+            \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+        )\n         cls.image_0 = np.random.randint(255, size=(3, 876, 1300), dtype=np.uint8)\n         cls.url_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         cls.image_1 = np.random.randint(255, size=(3, 480, 640), dtype=np.uint8)"
        },
        {
            "sha": "41fb6aeb4e468145a70b15262ec01c6cb67e73a3",
            "filename": "tests/models/mllama/test_modeling_mllama.py",
            "status": "modified",
            "additions": 12,
            "deletions": 2,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_modeling_mllama.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -677,7 +677,12 @@ def test_11b_model_integration_batched_generate(self):\n             \"<|image|>This image shows\",\n         ]\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n-        image2 = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n+        image2 = Image.open(\n+            requests.get(\n+                \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                stream=True,\n+            ).raw\n+        )\n \n         inputs = processor(text=prompt, images=[[image1], [image2]], padding=True, return_tensors=\"pt\").to(\n             torch_device\n@@ -734,7 +739,12 @@ def test_11b_model_integration_multi_image_generate(self):\n \n         # Prepare inputs\n         image1 = Image.open(requests.get(\"https://llava-vl.github.io/static/images/view.jpg\", stream=True).raw)\n-        image2 = Image.open(requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw)\n+        image2 = Image.open(\n+            requests.get(\n+                \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                stream=True,\n+            ).raw\n+        )\n \n         conversation = [\n             {"
        },
        {
            "sha": "be147249682346efa61a19f03f410f7fad0b5789",
            "filename": "tests/models/mllama/test_processing_mllama.py",
            "status": "modified",
            "additions": 14,
            "deletions": 2,
            "changes": 16,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmllama%2Ftest_processing_mllama.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -52,6 +52,8 @@ def setUpClass(cls):\n \n     @classmethod\n     def tearDownClass(cls):\n+        cls.image1.close()\n+        cls.image2.close()\n         shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n \n     def prepare_processor_dict(self):\n@@ -172,9 +174,19 @@ def test_apply_chat_template(self):\n                 \"role\": \"user\",\n                 \"content\": [\n                     {\"type\": \"text\", \"text\": \"Describe this image in two sentences\"},\n-                    {\"type\": \"image\", \"url\": url_to_local_path(\"https://www.ilankelman.org/stopsigns/australia.jpg\")},\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": url_to_local_path(\n+                            \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                        ),\n+                    },\n                     {\"type\": \"text\", \"text\": \" Test sentence   \"},\n-                    {\"type\": \"image\", \"url\": url_to_local_path(\"https://www.ilankelman.org/stopsigns/australia.jpg\")},\n+                    {\n+                        \"type\": \"image\",\n+                        \"url\": url_to_local_path(\n+                            \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                        ),\n+                    },\n                     {\"type\": \"text\", \"text\": \"ok\\n\"},\n                 ],\n             }"
        },
        {
            "sha": "7447214e0c34e3bd8ed6d0277708107b933f8d55",
            "filename": "tests/models/mobilenet_v2/test_image_processing_mobilenet_v2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilenet_v2%2Ftest_image_processing_mobilenet_v2.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -15,21 +15,20 @@\n \n import unittest\n \n-import requests\n from datasets import load_dataset\n \n+from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_torch_available():\n     import torch\n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import MobileNetV2ImageProcessor\n \n     if is_torchvision_available():\n@@ -268,9 +267,7 @@ def test_slow_fast_equivalence(self):\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n         # Test with single image\n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n "
        },
        {
            "sha": "0299aab033ccb1f5f872340d7a7bae73b8f57de0",
            "filename": "tests/models/mobilevit/test_image_processing_mobilevit.py",
            "status": "modified",
            "additions": 3,
            "deletions": 6,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fmobilevit%2Ftest_image_processing_mobilevit.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -15,21 +15,20 @@\n \n import unittest\n \n-import requests\n from datasets import load_dataset\n \n+from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_torch_available():\n     import torch\n \n if is_vision_available():\n-    from PIL import Image\n-\n     from transformers import MobileViTImageProcessor\n \n     if is_torchvision_available():\n@@ -274,9 +273,7 @@ def test_slow_fast_equivalence(self):\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n         # Test with single image\n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n "
        },
        {
            "sha": "578799e6ee05d512741668f824b2599ea66df004",
            "filename": "tests/models/nougat/test_image_processing_nougat.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fnougat%2Ftest_image_processing_nougat.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -16,14 +16,14 @@\n import unittest\n \n import numpy as np\n-import requests\n from huggingface_hub import hf_hub_download\n \n-from transformers.image_utils import SizeDict\n+from transformers.image_utils import SizeDict, load_image\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import cached_property, is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_torch_available():\n@@ -305,9 +305,7 @@ def test_slow_fast_equivalence(self):\n         if self.image_processing_class is None or self.fast_image_processing_class is None:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n "
        },
        {
            "sha": "d130122b16ffed3776aea68177391cb886910b2d",
            "filename": "tests/models/paligemma/test_modeling_paligemma.py",
            "status": "modified",
            "additions": 4,
            "deletions": 1,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpaligemma%2Ftest_modeling_paligemma.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -378,7 +378,10 @@ def test_small_model_integration_test_multiimage(self):\n         processor = PaliGemmaProcessor.from_pretrained(model_id)\n         prompt = \"answer en There is no snowman in any of the images. Is this true or false?\"\n         stop_sign_image = Image.open(\n-            requests.get(\"https://www.ilankelman.org/stopsigns/australia.jpg\", stream=True).raw\n+            requests.get(\n+                \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+                stream=True,\n+            ).raw\n         )\n         snow_image = Image.open(\n             requests.get("
        },
        {
            "sha": "84dbf95301c1979d136bf0d08ed444b8ef606e85",
            "filename": "tests/models/phi4_multimodal/test_modeling_phi4_multimodal.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fphi4_multimodal%2Ftest_modeling_phi4_multimodal.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -281,8 +281,8 @@ def test_flex_attention_with_grads(self):\n class Phi4MultimodalIntegrationTest(unittest.TestCase):\n     checkpoint_path = \"microsoft/Phi-4-multimodal-instruct\"\n     revision = \"refs/pr/70\"\n-    image_url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n-    audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"\n+    image_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+    audio_url = \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\"\n \n     def setUp(self):\n         # Currently, the Phi-4 checkpoint on the hub is not working with the latest Phi-4 code, so the slow integration tests"
        },
        {
            "sha": "e300850a474ce2283ffe32458e3887fb34dd90db",
            "filename": "tests/models/pix2struct/test_image_processing_pix2struct.py",
            "status": "modified",
            "additions": 6,
            "deletions": 3,
            "changes": 9,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fpix2struct%2Ftest_image_processing_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fpix2struct%2Ftest_image_processing_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpix2struct%2Ftest_image_processing_pix2struct.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -16,12 +16,13 @@\n import unittest\n \n import numpy as np\n-import requests\n \n+from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_torch_available():\n@@ -64,8 +65,10 @@ def prepare_image_processor_dict(self):\n         return {\"do_normalize\": self.do_normalize, \"do_convert_rgb\": self.do_convert_rgb}\n \n     def prepare_dummy_image(self):\n-        img_url = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n-        raw_image = Image.open(requests.get(img_url, stream=True).raw).convert(\"RGB\")\n+        img_url = url_to_local_path(\n+            \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+        )\n+        raw_image = load_image(img_url).convert(\"RGB\")\n         return raw_image\n \n     def prepare_image_inputs(self, equal_resolution=False, numpify=False, torchify=False):"
        },
        {
            "sha": "537e210d64c6aab457ae2561824ff091893f3c9f",
            "filename": "tests/models/pixtral/test_image_processing_pixtral.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_image_processing_pixtral.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -16,13 +16,14 @@\n \n import numpy as np\n import pytest\n-import requests\n from packaging import version\n \n+from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_torch_gpu, require_vision, slow, torch_device\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_torch_available():\n@@ -217,9 +218,7 @@ def test_call_pytorch(self):\n     @require_vision\n     @require_torch\n     def test_slow_fast_equivalence(self):\n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n \n         if not self.test_slow_image_processor or not self.test_fast_image_processor:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test\")"
        },
        {
            "sha": "30a49438f97a864f3b50266fa805d8a4e454c4e3",
            "filename": "tests/models/pixtral/test_processing_pixtral.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fpixtral%2Ftest_processing_pixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fpixtral%2Ftest_processing_pixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fpixtral%2Ftest_processing_pixtral.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -21,7 +21,7 @@\n from transformers.testing_utils import require_vision\n from transformers.utils import is_vision_available\n \n-from ...test_processing_common import ProcessorTesterMixin\n+from ...test_processing_common import ProcessorTesterMixin, url_to_local_path\n \n \n if is_vision_available():\n@@ -34,7 +34,9 @@ class PixtralProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n \n     @classmethod\n     def setUpClass(cls):\n-        cls.url_0 = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n+        cls.url_0 = url_to_local_path(\n+            \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+        )\n         cls.image_0 = np.random.randint(255, size=(3, 876, 1300), dtype=np.uint8)\n         cls.url_1 = \"http://images.cocodataset.org/val2017/000000039769.jpg\"\n         cls.image_1 = np.random.randint(255, size=(3, 480, 640), dtype=np.uint8)"
        },
        {
            "sha": "fc0dad5088059d10e392368aaaed0e018f96a503",
            "filename": "tests/models/qwen2_5_omni/test_modeling_qwen2_5_omni.py",
            "status": "modified",
            "additions": 4,
            "deletions": 2,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_modeling_qwen2_5_omni.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -595,9 +595,11 @@ def test_get_rope_index_video_with_audio(self):\n class Qwen2_5OmniModelIntegrationTest(unittest.TestCase):\n     def setUp(self):\n         self.processor = AutoProcessor.from_pretrained(\"Qwen/Qwen2.5-Omni-7B\")\n-        self.audio_url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n+        self.audio_url = (\n+            \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\"\n+        )\n         self.audio_url_additional = (\n-            \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\"\n+            \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\"\n         )\n         self.image_url = \"https://qianwen-res.oss-accelerate-overseas.aliyuncs.com/Qwen2-VL/demo_small.jpg\"\n         self.messages = ["
        },
        {
            "sha": "4b55201596ca717daf0811623c9d864462c2dad1",
            "filename": "tests/models/qwen2_5_omni/test_processing_qwen2_5_omni.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_omni%2Ftest_processing_qwen2_5_omni.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -510,8 +510,12 @@ def test_apply_chat_template_video_frame_sampling(self):\n         messages[0][0][\"content\"][-1] = {\n             \"type\": \"video\",\n             \"url\": [\n-                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n-                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+                url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n+                url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n             ],\n         }\n         out_dict_with_video = processor.apply_chat_template("
        },
        {
            "sha": "5aa42982c63f3ff865a8b66f2acb6f6852787a4e",
            "filename": "tests/models/qwen2_5_vl/test_processing_qwen2_5_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_5_vl%2Ftest_processing_qwen2_5_vl.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -327,8 +327,12 @@ def test_apply_chat_template_video_frame_sampling(self):\n         messages[0][0][\"content\"][0] = {\n             \"type\": \"video\",\n             \"url\": [\n-                url_to_local_path(\"https://www.ilankelman.org/stopsigns/australia.jpg\"),\n-                url_to_local_path(\"https://www.ilankelman.org/stopsigns/australia.jpg\"),\n+                url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n+                url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n             ],\n         }\n         out_dict_with_video = processor.apply_chat_template("
        },
        {
            "sha": "b1f809892c8f5f36fd0e031eb6237077c883fca4",
            "filename": "tests/models/qwen2_audio/test_modeling_qwen2_audio.py",
            "status": "modified",
            "additions": 5,
            "deletions": 5,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_modeling_qwen2_audio.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -207,7 +207,7 @@ def test_small_model_integration_test_single(self):\n         # Let' s make sure we test the preprocessing to replace what is used\n         model = Qwen2AudioForConditionalGeneration.from_pretrained(\"Qwen/Qwen2-Audio-7B-Instruct\")\n \n-        url = \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\"\n+        url = \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\"\n         messages = [\n             {\n                 \"role\": \"user\",\n@@ -270,7 +270,7 @@ def test_small_model_integration_test_batch(self):\n                 \"content\": [\n                     {\n                         \"type\": \"audio\",\n-                        \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\",\n+                        \"audio_url\": \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\",\n                     },\n                     {\"type\": \"text\", \"text\": \"What's that sound?\"},\n                 ],\n@@ -281,7 +281,7 @@ def test_small_model_integration_test_batch(self):\n                 \"content\": [\n                     {\n                         \"type\": \"audio\",\n-                        \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\",\n+                        \"audio_url\": \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\",\n                     },\n                     {\"type\": \"text\", \"text\": \"What can you hear?\"},\n                 ],\n@@ -346,7 +346,7 @@ def test_small_model_integration_test_multiturn(self):\n                 \"content\": [\n                     {\n                         \"type\": \"audio\",\n-                        \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\",\n+                        \"audio_url\": \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\",\n                     },\n                     {\"type\": \"text\", \"text\": \"What's that sound?\"},\n                 ],\n@@ -357,7 +357,7 @@ def test_small_model_integration_test_multiturn(self):\n                 \"content\": [\n                     {\n                         \"type\": \"audio\",\n-                        \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\",\n+                        \"audio_url\": \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\",\n                     },\n                     {\"type\": \"text\", \"text\": \"How about this one?\"},\n                 ],"
        },
        {
            "sha": "33e224078242c497e6c67a151f7a13bdc4d27a8d",
            "filename": "tests/models/qwen2_audio/test_processing_qwen2_audio.py",
            "status": "modified",
            "additions": 7,
            "deletions": 3,
            "changes": 10,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_audio%2Ftest_processing_qwen2_audio.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_audio%2Ftest_processing_qwen2_audio.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_audio%2Ftest_processing_qwen2_audio.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -18,7 +18,7 @@\n from transformers import AutoProcessor, AutoTokenizer, Qwen2AudioProcessor, WhisperFeatureExtractor\n from transformers.testing_utils import require_torch, require_torchaudio\n \n-from ...test_processing_common import ProcessorTesterMixin\n+from ...test_processing_common import ProcessorTesterMixin, url_to_local_path\n \n \n @require_torch\n@@ -116,7 +116,9 @@ def test_chat_template(self):\n                 \"content\": [\n                     {\n                         \"type\": \"audio\",\n-                        \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/glass-breaking-151256.mp3\",\n+                        \"audio_url\": url_to_local_path(\n+                            \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\"\n+                        ),\n                     },\n                     {\"type\": \"text\", \"text\": \"What's that sound?\"},\n                 ],\n@@ -127,7 +129,9 @@ def test_chat_template(self):\n                 \"content\": [\n                     {\n                         \"type\": \"audio\",\n-                        \"audio_url\": \"https://qianwen-res.oss-cn-beijing.aliyuncs.com/Qwen2-Audio/audio/f2641_0_throatclearing.wav\",\n+                        \"audio_url\": url_to_local_path(\n+                            \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\"\n+                        ),\n                     },\n                     {\"type\": \"text\", \"text\": \"How about this one?\"},\n                 ],"
        },
        {
            "sha": "e8e935cc3bfa449ed33327461d88c6505ceb62ca",
            "filename": "tests/models/qwen2_vl/test_image_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_image_processing_qwen2_vl.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -17,14 +17,14 @@\n import unittest\n \n import numpy as np\n-import requests\n \n-from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD\n+from transformers.image_utils import OPENAI_CLIP_MEAN, OPENAI_CLIP_STD, load_image\n from transformers.models.qwen2_vl.image_processing_qwen2_vl import smart_resize\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs, prepare_video_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_torch_available():\n@@ -346,9 +346,7 @@ def test_temporal_padding(self):\n     @require_vision\n     @require_torch\n     def test_slow_fast_equivalence(self):\n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n \n         if not self.test_slow_image_processor or not self.test_fast_image_processor:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test\")"
        },
        {
            "sha": "9a84ffeccef79953bba4864ef3159538e80afc1a",
            "filename": "tests/models/qwen2_vl/test_processing_qwen2_vl.py",
            "status": "modified",
            "additions": 6,
            "deletions": 2,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_vl%2Ftest_processing_qwen2_vl.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -327,8 +327,12 @@ def test_apply_chat_template_video_frame_sampling(self):\n         messages[0][0][\"content\"][0] = {\n             \"type\": \"video\",\n             \"url\": [\n-                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n-                \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+                url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n+                url_to_local_path(\n+                    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\"\n+                ),\n             ],\n         }\n         out_dict_with_video = processor.apply_chat_template("
        },
        {
            "sha": "58afe2cfc4727c2d31d5c0bfb35b23a8bd806a89",
            "filename": "tests/models/rt_detr/test_image_processing_rt_detr.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Frt_detr%2Ftest_image_processing_rt_detr.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -14,8 +14,7 @@\n import json\n import unittest\n \n-import requests\n-\n+from transformers.image_utils import load_image\n from transformers.testing_utils import (\n     is_flaky,\n     require_torch,\n@@ -28,6 +27,7 @@\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_vision_available():\n@@ -234,7 +234,7 @@ def test_multiple_images_processor_outputs(self):\n \n         images = []\n         for url in images_urls:\n-            image = Image.open(requests.get(url, stream=True).raw)\n+            image = load_image(url_to_local_path(url))\n             images.append(image)\n \n         for image_processing_class in self.image_processor_list:"
        },
        {
            "sha": "f36aa23b38ae4b69db7a9b2ae9e16ef4b5471bc2",
            "filename": "tests/models/siglip2/test_image_processing_siglip2.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fsiglip2%2Ftest_image_processing_siglip2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fsiglip2%2Ftest_image_processing_siglip2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsiglip2%2Ftest_image_processing_siglip2.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -15,12 +15,12 @@\n \n import unittest\n \n-import requests\n-\n+from transformers.image_utils import load_image\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_vision_available():\n@@ -155,9 +155,7 @@ def test_slow_fast_equivalence(self):\n         if self.image_processing_class is None or self.fast_image_processing_class is None:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)\n "
        },
        {
            "sha": "85fd7904c9db9b5752404732c0bd5f53a17fa670",
            "filename": "tests/models/smolvlm/test_image_processing_smolvlm.py",
            "status": "modified",
            "additions": 3,
            "deletions": 5,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fsmolvlm%2Ftest_image_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fsmolvlm%2Ftest_image_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_image_processing_smolvlm.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -17,13 +17,13 @@\n import unittest\n \n import numpy as np\n-import requests\n \n-from transformers.image_utils import PILImageResampling\n+from transformers.image_utils import PILImageResampling, load_image\n from transformers.testing_utils import require_torch, require_vision\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin\n+from ...test_processing_common import url_to_local_path\n \n \n if is_vision_available():\n@@ -298,9 +298,7 @@ def test_slow_fast_equivalence(self):\n         if self.image_processing_class is None or self.fast_image_processing_class is None:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n         dummy_image = dummy_image.resize((100, 150))\n         image_processor_slow = self.image_processing_class(\n             **self.image_processor_dict, resample=PILImageResampling.BICUBIC"
        },
        {
            "sha": "dc1fefd97102b9d25b1cc65390eb4ed08cf330ce",
            "filename": "tests/models/smolvlm/test_processing_smolvlm.py",
            "status": "modified",
            "additions": 18,
            "deletions": 23,
            "changes": 41,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fsmolvlm%2Ftest_processing_smolvlm.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -15,24 +15,18 @@\n import shutil\n import tempfile\n import unittest\n-from io import BytesIO\n from typing import Optional\n \n import numpy as np\n-import requests\n \n from transformers import SmolVLMProcessor\n+from transformers.image_utils import load_image\n from transformers.models.auto.processing_auto import AutoProcessor\n from transformers.testing_utils import require_av, require_torch, require_vision\n-from transformers.utils import is_vision_available\n \n from ...test_processing_common import ProcessorTesterMixin, url_to_local_path\n \n \n-if is_vision_available():\n-    from PIL import Image\n-\n-\n @require_torch\n @require_vision\n class SmolVLMProcessorTest(ProcessorTesterMixin, unittest.TestCase):\n@@ -45,21 +39,19 @@ def setUpClass(cls):\n         processor_kwargs = cls.prepare_processor_dict()\n         processor = SmolVLMProcessor.from_pretrained(\"HuggingFaceTB/SmolVLM2-256M-Video-Instruct\", **processor_kwargs)\n         processor.save_pretrained(cls.tmpdirname)\n-        cls.image1 = Image.open(\n-            BytesIO(\n-                requests.get(\n-                    \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n-                ).content\n+        cls.image1 = load_image(\n+            url_to_local_path(\n+                \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n             )\n         )\n-        cls.image2 = Image.open(\n-            BytesIO(requests.get(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\").content)\n+        cls.image2 = load_image(\n+            url_to_local_path(\n+                url_to_local_path(\"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\")\n+            )\n         )\n-        cls.image3 = Image.open(\n-            BytesIO(\n-                requests.get(\n-                    \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n-                ).content\n+        cls.image3 = load_image(\n+            url_to_local_path(\n+                \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\"\n             )\n         )\n         cls.bos_token = processor.tokenizer.bos_token\n@@ -75,6 +67,13 @@ def setUpClass(cls):\n         cls.padding_token_id = processor.tokenizer.pad_token_id\n         cls.image_seq_len = processor.image_seq_len\n \n+    @classmethod\n+    def tearDownClass(cls):\n+        cls.image1.close()\n+        cls.image2.close()\n+        cls.image3.close()\n+        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n+\n     def get_tokenizer(self, **kwargs):\n         return AutoProcessor.from_pretrained(self.tmpdirname, **kwargs).tokenizer\n \n@@ -130,10 +129,6 @@ def get_split_image_expected_tokens(self, processor, image_rows, image_cols):\n         )\n         return text_split_images\n \n-    @classmethod\n-    def tearDownClass(cls):\n-        shutil.rmtree(cls.tmpdirname, ignore_errors=True)\n-\n     def test_process_interleaved_images_prompts_no_image_splitting(self):\n         processor_components = self.prepare_components()\n         processor_components[\"tokenizer\"] = self.get_component(\"tokenizer\", padding_side=\"left\")"
        },
        {
            "sha": "7201139702fccdbadb1f5bedcce499aff91fc13f",
            "filename": "tests/models/vitmatte/test_image_processing_vitmatte.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fvitmatte%2Ftest_image_processing_vitmatte.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -19,9 +19,9 @@\n \n import numpy as np\n import pytest\n-import requests\n from packaging import version\n \n+from transformers.image_utils import load_image\n from transformers.testing_utils import (\n     is_flaky,\n     require_torch,\n@@ -33,6 +33,7 @@\n from transformers.utils import is_torch_available, is_torchvision_available, is_vision_available\n \n from ...test_image_processing_common import ImageProcessingTestMixin, prepare_image_inputs\n+from ...test_processing_common import url_to_local_path\n \n \n if is_torch_available():\n@@ -304,9 +305,7 @@ def test_slow_fast_equivalence(self):\n         if self.image_processing_class is None or self.fast_image_processing_class is None:\n             self.skipTest(reason=\"Skipping slow/fast equivalence test as one of the image processors is not defined\")\n \n-        dummy_image = Image.open(\n-            requests.get(\"http://images.cocodataset.org/val2017/000000039769.jpg\", stream=True).raw\n-        )\n+        dummy_image = load_image(url_to_local_path(\"http://images.cocodataset.org/val2017/000000039769.jpg\"))\n         dummy_trimap = np.random.randint(0, 3, size=dummy_image.size[::-1])\n         image_processor_slow = self.image_processing_class(**self.image_processor_dict)\n         image_processor_fast = self.fast_image_processing_class(**self.image_processor_dict)"
        },
        {
            "sha": "05b9ae636fb34eb30c9ffd18227fefe78f45a682",
            "filename": "tests/test_tokenization_mistral_common.py",
            "status": "modified",
            "additions": 17,
            "deletions": 10,
            "changes": 27,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Ftest_tokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/tests%2Ftest_tokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Ftest_tokenization_mistral_common.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930"
        },
        {
            "sha": "c2b341f0b6838c2950c797e2675a838d4ddf51d3",
            "filename": "utils/fetch_hub_objects_for_ci.py",
            "status": "modified",
            "additions": 21,
            "deletions": 8,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/34595cf296b1eafce294fd9aa5f43cb53d014930/utils%2Ffetch_hub_objects_for_ci.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/34595cf296b1eafce294fd9aa5f43cb53d014930/utils%2Ffetch_hub_objects_for_ci.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Ffetch_hub_objects_for_ci.py?ref=34595cf296b1eafce294fd9aa5f43cb53d014930",
            "patch": "@@ -8,18 +8,31 @@\n \n \n URLS_FOR_TESTING_DATA = [\n-    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\",\n-    \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Big_Buck_Bunny_720_10s_10MB.mp4\",\n-    \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/sample_demo_1.mp4\",\n-    \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\",\n-    \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\",\n-    \"https://www.ilankelman.org/stopsigns/australia.jpg\",\n+    \"http://images.cocodataset.org/val2017/000000000139.jpg\",\n+    \"http://images.cocodataset.org/val2017/000000000285.jpg\",\n+    \"http://images.cocodataset.org/val2017/000000000632.jpg\",\n+    \"http://images.cocodataset.org/val2017/000000000724.jpg\",\n+    \"http://images.cocodataset.org/val2017/000000000776.jpg\",\n+    \"http://images.cocodataset.org/val2017/000000000785.jpg\",\n+    \"http://images.cocodataset.org/val2017/000000000802.jpg\",\n+    \"http://images.cocodataset.org/val2017/000000000872.jpg\",\n     \"http://images.cocodataset.org/val2017/000000039769.jpg\",\n+    \"https://cdn.britannica.com/59/94459-050-DBA42467/Skyline-Chicago.jpg\",\n     \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\",\n-    \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n-    \"https://llava-vl.github.io/static/images/view.jpg\",\n+    \"https://huggingface.co/datasets/hf-internal-testing/dummy-audio-samples/resolve/main/bcn_weather.mp3\",\n+    \"https://huggingface.co/datasets/hf-internal-testing/fixtures-captioning/resolve/main/bus.png\",\n     \"https://huggingface.co/datasets/hf-internal-testing/fixtures_videos/resolve/main/tennis.mp4\",\n+    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/coco_sample.png\",\n+    \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/australia.jpg\",\n+    \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/f2641_0_throatclearing.wav\",\n+    \"https://huggingface.co/datasets/raushan-testing-hf/audio-test/resolve/main/glass-breaking-151256.mp3\",\n     \"https://huggingface.co/datasets/raushan-testing-hf/images_test/resolve/main/picsum_237_200x300.jpg\",\n+    \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/Big_Buck_Bunny_720_10s_10MB.mp4\",\n+    \"https://huggingface.co/datasets/raushan-testing-hf/videos-test/resolve/main/sample_demo_1.mp4\",\n+    \"https://huggingface.co/kirp/kosmos2_5/resolve/main/receipt_00008.png\",\n+    \"https://huggingface.co/microsoft/kosmos-2-patch14-224/resolve/main/two_dogs.jpg\",\n+    \"https://llava-vl.github.io/static/images/view.jpg\",\n+    \"https://thumbs.dreamstime.com/b/golden-gate-bridge-san-francisco-purple-flowers-california-echium-candicans-36805947.jpg\",\n ]\n \n "
        }
    ],
    "stats": {
        "total": 573,
        "additions": 331,
        "deletions": 242
    }
}