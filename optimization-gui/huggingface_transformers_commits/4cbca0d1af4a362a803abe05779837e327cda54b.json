{
    "author": "rcogill",
    "message": "Fixing bug in Voxtral when merging text and audio embeddings (#40671)\n\n* Fixing bug when replacing text-audio token placeholders with audio embeddings\n\n* apply changes\n\n---------\n\nCo-authored-by: Eustache Le Bihan <eulebihan@gmail.com>\nCo-authored-by: eustlb <94853470+eustlb@users.noreply.github.com>",
    "sha": "4cbca0d1af4a362a803abe05779837e327cda54b",
    "files": [
        {
            "sha": "671d91066caebcf0bf926b79399ecfbb2f96fff5",
            "filename": "src/transformers/models/voxtral/modeling_voxtral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cbca0d1af4a362a803abe05779837e327cda54b/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cbca0d1af4a362a803abe05779837e327cda54b/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodeling_voxtral.py?ref=4cbca0d1af4a362a803abe05779837e327cda54b",
            "patch": "@@ -504,12 +504,14 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-        if input_features is not None:\n+        if input_features is not None and input_ids is not None:\n             audio_embeds = self.get_audio_embeds(input_features)\n \n             # replace text-audio token placeholders with audio embeddings\n-            audio_token_mask = input_ids == self.config.audio_token_id\n-            inputs_embeds[audio_token_mask] = audio_embeds\n+            audio_token_mask = (input_ids == self.config.audio_token_id).unsqueeze(-1)\n+            inputs_embeds = inputs_embeds.masked_scatter(\n+                audio_token_mask.to(inputs_embeds.device), audio_embeds.to(inputs_embeds.device)\n+            )\n \n         outputs: BaseModelOutputWithPast = self.language_model(\n             attention_mask=attention_mask,"
        },
        {
            "sha": "a0080f58eb0d69c8d736873982d1c16bb18e9b5c",
            "filename": "src/transformers/models/voxtral/modular_voxtral.py",
            "status": "modified",
            "additions": 5,
            "deletions": 3,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/4cbca0d1af4a362a803abe05779837e327cda54b/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4cbca0d1af4a362a803abe05779837e327cda54b/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fvoxtral%2Fmodular_voxtral.py?ref=4cbca0d1af4a362a803abe05779837e327cda54b",
            "patch": "@@ -239,12 +239,14 @@ def forward(\n         if inputs_embeds is None:\n             inputs_embeds = self.get_input_embeddings()(input_ids)\n \n-        if input_features is not None:\n+        if input_features is not None and input_ids is not None:\n             audio_embeds = self.get_audio_embeds(input_features)\n \n             # replace text-audio token placeholders with audio embeddings\n-            audio_token_mask = input_ids == self.config.audio_token_id\n-            inputs_embeds[audio_token_mask] = audio_embeds\n+            audio_token_mask = (input_ids == self.config.audio_token_id).unsqueeze(-1)\n+            inputs_embeds = inputs_embeds.masked_scatter(\n+                audio_token_mask.to(inputs_embeds.device), audio_embeds.to(inputs_embeds.device)\n+            )\n \n         outputs: BaseModelOutputWithPast = self.language_model(\n             attention_mask=attention_mask,"
        }
    ],
    "stats": {
        "total": 16,
        "additions": 10,
        "deletions": 6
    }
}