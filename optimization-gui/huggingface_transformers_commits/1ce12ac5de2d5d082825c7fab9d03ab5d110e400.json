{
    "author": "cyyever",
    "message": "Replace Optional and Union typing with | in some source files (#42372)\n\nSigned-off-by: Yuanyuan Chen <cyyever@outlook.com>\nCo-authored-by: Matt <Rocketknight1@users.noreply.github.com>",
    "sha": "1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
    "files": [
        {
            "sha": "570577fca823e3ddf37436ccf41020edc4da8635",
            "filename": "src/transformers/integrations/accelerate.py",
            "status": "modified",
            "additions": 10,
            "deletions": 10,
            "changes": 20,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Fintegrations%2Faccelerate.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Faccelerate.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -22,7 +22,7 @@\n import re\n from collections import OrderedDict, defaultdict\n from contextlib import contextmanager\n-from typing import TYPE_CHECKING, Optional, Union\n+from typing import TYPE_CHECKING\n \n from safetensors import safe_open\n from safetensors.torch import save_file\n@@ -550,14 +550,14 @@ def offload_weight(weight: torch.Tensor, weight_name: str, offload_folder: str |\n \n def _init_infer_auto_device_map(\n     model: nn.Module,\n-    max_memory: Optional[dict[Union[int, str], Union[int, str]]] = None,\n-    no_split_module_classes: Optional[list[str]] = None,\n-    tied_parameters: Optional[list[list[str]]] = None,\n+    max_memory: dict[int | str, int | str] | None = None,\n+    no_split_module_classes: list[str] | None = None,\n+    tied_parameters: list[list[str]] | None = None,\n     hf_quantizer: \"HfQuantizer | None\" = None,\n ) -> tuple[\n-    list[Union[int, str]],\n-    dict[Union[int, str], Union[int, str]],\n-    list[Union[int, str]],\n+    list[int | str],\n+    dict[int | str, int | str],\n+    list[int | str],\n     list[int],\n     dict[str, int],\n     list[list[str]],\n@@ -620,12 +620,12 @@ def _init_infer_auto_device_map(\n \n def infer_auto_device_map(\n     model: nn.Module,\n-    max_memory: Optional[dict[Union[int, str], Union[int, str]]] = None,\n-    no_split_module_classes: Optional[list[str]] = None,\n+    max_memory: dict[int | str, int | str] | None = None,\n+    no_split_module_classes: list[str] | None = None,\n     verbose: bool = False,\n     clean_result: bool = True,\n     offload_buffers: bool = False,\n-    tied_parameters: Optional[list[list[str]]] = None,\n+    tied_parameters: list[list[str]] | None = None,\n     hf_quantizer: \"HfQuantizer | None\" = None,\n ):\n     \"\"\""
        },
        {
            "sha": "78e5403433a49f305e2d69ca475dcfe66fc26f7c",
            "filename": "src/transformers/integrations/bitsandbytes.py",
            "status": "modified",
            "additions": 2,
            "deletions": 3,
            "changes": 5,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Fbitsandbytes.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -1,7 +1,6 @@\n import inspect\n from collections import defaultdict\n from inspect import signature\n-from typing import Optional\n \n from ..core_model_loading import ConversionOps\n from ..quantizers.quantizers_utils import get_module_from_name\n@@ -38,7 +37,7 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: dict[str, list[torch.Tensor]],\n-        model: Optional[torch.nn.Module] = None,\n+        model: torch.nn.Module | None = None,\n         missing_keys=None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:\n@@ -95,7 +94,7 @@ def __init__(self, hf_quantizer):\n     def convert(\n         self,\n         input_dict: dict[str, list[torch.Tensor]],\n-        model: Optional[torch.nn.Module] = None,\n+        model: torch.nn.Module | None = None,\n         missing_keys=None,\n         **kwargs,\n     ) -> dict[str, torch.Tensor]:"
        },
        {
            "sha": "19f4a6076afcdbe7088b03f60cb9d563293016ef",
            "filename": "src/transformers/integrations/finegrained_fp8.py",
            "status": "modified",
            "additions": 3,
            "deletions": 3,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ffinegrained_fp8.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -15,7 +15,7 @@\n \n import re\n from collections.abc import Sequence\n-from typing import Any, Optional, Union\n+from typing import Any\n \n from ..core_model_loading import ConversionOps\n from ..utils import is_accelerate_available, is_torch_accelerator_available, is_torch_available, logging\n@@ -655,13 +655,13 @@ def convert(self, input_dict: torch.Tensor, **kwargs) -> dict[str, torch.Tensor]\n class Fp8Dequantize(ConversionOps):\n     \"\"\"Inverse operation of :class:`Fp8Quantize`. Takes a pair (weight, scale) and reconstructs the fp32 tensor.\"\"\"\n \n-    def __init__(self, block_size: Optional[tuple[int, int]] = None):\n+    def __init__(self, block_size: tuple[int, int] | None = None):\n         self.block_size = block_size\n         self.reverse_op = Fp8Quantize\n \n     def convert(\n         self,\n-        value: Union[Sequence[torch.Tensor], dict[str, torch.Tensor]],\n+        value: Sequence[torch.Tensor] | dict[str, torch.Tensor],\n         *,\n         context: dict[str, Any],\n     ) -> torch.Tensor:"
        },
        {
            "sha": "0b0ab85872ede46b739d4ab7bf17a6aada3af52e",
            "filename": "src/transformers/integrations/tensor_parallel.py",
            "status": "modified",
            "additions": 1,
            "deletions": 2,
            "changes": 3,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fintegrations%2Ftensor_parallel.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -18,7 +18,6 @@\n import os\n import re\n from functools import partial, reduce\n-from typing import Optional\n \n from ..utils.import_utils import is_torch_available\n \n@@ -322,7 +321,7 @@ def repack_weights(\n     return final_ordered_tensor\n \n \n-def get_tensor_shard(param, empty_param, device_mesh, rank, dim, tensor_idx: Optional[int] = None):\n+def get_tensor_shard(param, empty_param, device_mesh, rank, dim, tensor_idx: int | None = None):\n     \"\"\"\n     Generalized tensor sharding across a multi-dimensional device mesh.\n     Extract only the fraction of the parameter owned by the given `rank` when the parameter would have gone sharding at provided `dim`."
        },
        {
            "sha": "94e95cbf1a73851a7c3dcefea9af8295951bcf02",
            "filename": "src/transformers/optimization.py",
            "status": "modified",
            "additions": 14,
            "deletions": 15,
            "changes": 29,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Foptimization.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Foptimization.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Foptimization.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -16,7 +16,6 @@\n import math\n import warnings\n from functools import partial\n-from typing import Optional, Union\n \n import torch\n from torch.optim import Optimizer\n@@ -283,7 +282,7 @@ def get_polynomial_decay_schedule_with_warmup(\n     return LambdaLR(optimizer, lr_lambda, last_epoch)\n \n \n-def _get_inverse_sqrt_schedule_lr_lambda(current_step: int, *, num_warmup_steps: int, timescale: Optional[int] = None):\n+def _get_inverse_sqrt_schedule_lr_lambda(current_step: int, *, num_warmup_steps: int, timescale: int | None = None):\n     if current_step < num_warmup_steps:\n         return float(current_step) / float(max(1, num_warmup_steps))\n     shift = timescale - num_warmup_steps\n@@ -292,7 +291,7 @@ def _get_inverse_sqrt_schedule_lr_lambda(current_step: int, *, num_warmup_steps:\n \n \n def get_inverse_sqrt_schedule(\n-    optimizer: Optimizer, num_warmup_steps: int, timescale: Optional[int] = None, last_epoch: int = -1\n+    optimizer: Optimizer, num_warmup_steps: int, timescale: int | None = None, last_epoch: int = -1\n ):\n     \"\"\"\n     Create a schedule with an inverse square-root learning rate, from the initial lr set in the optimizer, after a\n@@ -338,8 +337,8 @@ def get_cosine_with_min_lr_schedule_with_warmup(\n     num_training_steps: int,\n     num_cycles: float = 0.5,\n     last_epoch: int = -1,\n-    min_lr: Optional[float] = None,\n-    min_lr_rate: Optional[float] = None,\n+    min_lr: float | None = None,\n+    min_lr_rate: float | None = None,\n ):\n     \"\"\"\n     Create a schedule with a learning rate that decreases following the values of the cosine function between the\n@@ -391,7 +390,7 @@ def _get_cosine_with_min_lr_schedule_with_warmup_lr_rate_lambda(\n     num_training_steps: int,\n     num_cycles: float,\n     min_lr_rate: float = 0.0,\n-    warmup_lr_rate: Optional[float] = None,\n+    warmup_lr_rate: float | None = None,\n ):\n     current_step = float(current_step)\n     num_warmup_steps = float(num_warmup_steps)\n@@ -415,9 +414,9 @@ def get_cosine_with_min_lr_schedule_with_warmup_lr_rate(\n     num_training_steps: int,\n     num_cycles: float = 0.5,\n     last_epoch: int = -1,\n-    min_lr: Optional[float] = None,\n-    min_lr_rate: Optional[float] = None,\n-    warmup_lr_rate: Optional[float] = None,\n+    min_lr: float | None = None,\n+    min_lr_rate: float | None = None,\n+    warmup_lr_rate: float | None = None,\n ):\n     \"\"\"\n     Create a schedule with a learning rate that decreases following the values of the cosine function between the\n@@ -507,8 +506,8 @@ def get_wsd_schedule(\n     optimizer: Optimizer,\n     num_warmup_steps: int,\n     num_decay_steps: int,\n-    num_training_steps: Optional[int] = None,\n-    num_stable_steps: Optional[int] = None,\n+    num_training_steps: int | None = None,\n+    num_stable_steps: int | None = None,\n     warmup_type: str = \"linear\",\n     decay_type: str = \"cosine\",\n     min_lr_ratio: float = 0,\n@@ -592,11 +591,11 @@ def get_wsd_schedule(\n \n \n def get_scheduler(\n-    name: Union[str, SchedulerType],\n+    name: str | SchedulerType,\n     optimizer: Optimizer,\n-    num_warmup_steps: Optional[int] = None,\n-    num_training_steps: Optional[int] = None,\n-    scheduler_specific_kwargs: Optional[dict] = None,\n+    num_warmup_steps: int | None = None,\n+    num_training_steps: int | None = None,\n+    scheduler_specific_kwargs: dict | None = None,\n ):\n     \"\"\"\n     Unified API to get any scheduler from its name."
        },
        {
            "sha": "1635f379c5d2ddfe7617e6ed895d82f4347a2b50",
            "filename": "src/transformers/pipelines/base.py",
            "status": "modified",
            "additions": 31,
            "deletions": 31,
            "changes": 62,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Fpipelines%2Fbase.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fpipelines%2Fbase.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -177,8 +177,8 @@ def inner(items):\n def load_model(\n     model,\n     config: AutoConfig,\n-    model_classes: Optional[tuple[type, ...]] = None,\n-    task: Optional[str] = None,\n+    model_classes: tuple[type, ...] | None = None,\n+    task: str | None = None,\n     **model_kwargs,\n ):\n     \"\"\"\n@@ -270,7 +270,7 @@ def load_model(\n     return model\n \n \n-def get_default_model_and_revision(targeted_task: dict, task_options: Optional[Any]) -> tuple[str, str]:\n+def get_default_model_and_revision(targeted_task: dict, task_options: Any | None) -> tuple[str, str]:\n     \"\"\"\n     Select a default model to use for a given task.\n \n@@ -305,9 +305,9 @@ def get_default_model_and_revision(targeted_task: dict, task_options: Optional[A\n \n def load_assistant_model(\n     model: \"PreTrainedModel\",\n-    assistant_model: Optional[Union[str, \"PreTrainedModel\"]],\n-    assistant_tokenizer: Optional[PreTrainedTokenizer],\n-) -> tuple[Optional[\"PreTrainedModel\"], Optional[PreTrainedTokenizer]]:\n+    assistant_model: Union[str, \"PreTrainedModel\"] | None,\n+    assistant_tokenizer: PreTrainedTokenizer | None,\n+) -> tuple[Optional[\"PreTrainedModel\"], PreTrainedTokenizer | None]:\n     \"\"\"\n     Prepares the assistant model and the assistant tokenizer for a pipeline whose model that can call `generate`.\n \n@@ -404,9 +404,9 @@ class PipelineDataFormat:\n \n     def __init__(\n         self,\n-        output_path: Optional[str],\n-        input_path: Optional[str],\n-        column: Optional[str],\n+        output_path: str | None,\n+        input_path: str | None,\n+        column: str | None,\n         overwrite: bool = False,\n     ):\n         self.output_path = output_path\n@@ -430,7 +430,7 @@ def __iter__(self):\n         raise NotImplementedError()\n \n     @abstractmethod\n-    def save(self, data: Union[dict, list[dict]]):\n+    def save(self, data: dict | list[dict]):\n         \"\"\"\n         Save the provided data object with the representation for the current [`~pipelines.PipelineDataFormat`].\n \n@@ -439,7 +439,7 @@ def save(self, data: Union[dict, list[dict]]):\n         \"\"\"\n         raise NotImplementedError()\n \n-    def save_binary(self, data: Union[dict, list[dict]]) -> str:\n+    def save_binary(self, data: dict | list[dict]) -> str:\n         \"\"\"\n         Save the provided data object as a pickle-formatted binary data on the disk.\n \n@@ -460,9 +460,9 @@ def save_binary(self, data: Union[dict, list[dict]]) -> str:\n     @staticmethod\n     def from_str(\n         format: str,\n-        output_path: Optional[str],\n-        input_path: Optional[str],\n-        column: Optional[str],\n+        output_path: str | None,\n+        input_path: str | None,\n+        column: str | None,\n         overwrite=False,\n     ) -> \"PipelineDataFormat\":\n         \"\"\"\n@@ -507,9 +507,9 @@ class CsvPipelineDataFormat(PipelineDataFormat):\n \n     def __init__(\n         self,\n-        output_path: Optional[str],\n-        input_path: Optional[str],\n-        column: Optional[str],\n+        output_path: str | None,\n+        input_path: str | None,\n+        column: str | None,\n         overwrite=False,\n     ):\n         super().__init__(output_path, input_path, column, overwrite=overwrite)\n@@ -551,9 +551,9 @@ class JsonPipelineDataFormat(PipelineDataFormat):\n \n     def __init__(\n         self,\n-        output_path: Optional[str],\n-        input_path: Optional[str],\n-        column: Optional[str],\n+        output_path: str | None,\n+        input_path: str | None,\n+        column: str | None,\n         overwrite=False,\n     ):\n         super().__init__(output_path, input_path, column, overwrite=overwrite)\n@@ -617,7 +617,7 @@ def save(self, data: dict):\n         \"\"\"\n         print(data)\n \n-    def save_binary(self, data: Union[dict, list[dict]]) -> str:\n+    def save_binary(self, data: dict | list[dict]) -> str:\n         if self.output_path is None:\n             raise KeyError(\n                 \"When using piped input on pipeline outputting large object requires an output file path. \"\n@@ -776,13 +776,13 @@ class Pipeline(_ScikitCompat, PushToHubMixin):\n     def __init__(\n         self,\n         model: \"PreTrainedModel\",\n-        tokenizer: Optional[PreTrainedTokenizer] = None,\n+        tokenizer: PreTrainedTokenizer | None = None,\n         feature_extractor: Optional[PreTrainedFeatureExtractor] = None,\n-        image_processor: Optional[BaseImageProcessor] = None,\n-        processor: Optional[ProcessorMixin] = None,\n-        modelcard: Optional[ModelCard] = None,\n+        image_processor: BaseImageProcessor | None = None,\n+        processor: ProcessorMixin | None = None,\n+        modelcard: ModelCard | None = None,\n         task: str = \"\",\n-        device: Optional[Union[int, \"torch.device\"]] = None,\n+        device: Union[int, \"torch.device\"] | None = None,\n         binary_output: bool = False,\n         **kwargs,\n     ):\n@@ -939,7 +939,7 @@ def __init__(\n \n     def save_pretrained(\n         self,\n-        save_directory: Union[str, os.PathLike],\n+        save_directory: str | os.PathLike,\n         safe_serialization: bool = True,\n         **kwargs: Any,\n     ):\n@@ -1085,7 +1085,7 @@ def _ensure_tensor_on_device(self, inputs, device):\n         else:\n             return inputs\n \n-    def check_model_type(self, supported_models: Union[list[str], dict]):\n+    def check_model_type(self, supported_models: list[str] | dict):\n         \"\"\"\n         Check if the model class is in supported by the pipeline.\n \n@@ -1348,9 +1348,9 @@ def register_pipeline(\n         self,\n         task: str,\n         pipeline_class: type,\n-        pt_model: Optional[Union[type, tuple[type]]] = None,\n-        default: Optional[dict] = None,\n-        type: Optional[str] = None,\n+        pt_model: type | tuple[type] | None = None,\n+        default: dict | None = None,\n+        type: str | None = None,\n     ) -> None:\n         if task in self.supported_tasks:\n             logger.warning(f\"{task} is already registered. Overwriting pipeline for task {task}...\")"
        },
        {
            "sha": "bdaf22e814784026f4e2417386503cbf93fec0f3",
            "filename": "src/transformers/testing_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 9,
            "changes": 18,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftesting_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftesting_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftesting_utils.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -42,7 +42,7 @@\n from functools import cache, wraps\n from io import StringIO\n from pathlib import Path\n-from typing import Any, Optional, Union\n+from typing import Any\n from unittest import mock\n from unittest.mock import patch\n \n@@ -1809,7 +1809,7 @@ class TemporaryHubRepo:\n     ```\n     \"\"\"\n \n-    def __init__(self, namespace: Optional[str] = None, token: Optional[str] = None) -> None:\n+    def __init__(self, namespace: str | None = None, token: str | None = None) -> None:\n         self.token = token\n         with tempfile.TemporaryDirectory() as tmp_dir:\n             repo_id = Path(tmp_dir).name\n@@ -1826,7 +1826,7 @@ def __exit__(self, exc, value, tb):\n \n @contextlib.contextmanager\n # adapted from https://stackoverflow.com/a/64789046/9201239\n-def ExtendSysPath(path: Union[str, os.PathLike]) -> Iterator[None]:\n+def ExtendSysPath(path: str | os.PathLike) -> Iterator[None]:\n     \"\"\"\n     Temporary add given path to `sys.path`.\n \n@@ -2566,7 +2566,7 @@ def total_calls(self) -> int:\n         return sum(self._counter.values())\n \n \n-def is_flaky(max_attempts: int = 5, wait_before_retry: Optional[float] = None, description: Optional[str] = None):\n+def is_flaky(max_attempts: int = 5, wait_before_retry: float | None = None, description: str | None = None):\n     \"\"\"\n     To decorate flaky tests. They will be retried on failures.\n \n@@ -2607,7 +2607,7 @@ def wrapper(*args, **kwargs):\n     return decorator\n \n \n-def hub_retry(max_attempts: int = 5, wait_before_retry: Optional[float] = 2):\n+def hub_retry(max_attempts: int = 5, wait_before_retry: float | None = 2):\n     \"\"\"\n     To decorate tests that download from the Hub. They can fail due to a\n     variety of network issues such as timeouts, connection resets, etc.\n@@ -3161,9 +3161,9 @@ def cleanup(device: str, gc_collect=False):\n \n \n # Type definition of key used in `Expectations` class.\n-DeviceProperties = tuple[Optional[str], Optional[int], Optional[int]]\n+DeviceProperties = tuple[str | None, int | None, int | None]\n # Helper type. Makes creating instances of `Expectations` smoother.\n-PackedDeviceProperties = tuple[Optional[str], Union[None, int, tuple[int, int]]]\n+PackedDeviceProperties = tuple[str | None, None | int | tuple[int, int]]\n \n \n @cache\n@@ -3192,7 +3192,7 @@ def get_device_properties() -> DeviceProperties:\n \n \n def unpack_device_properties(\n-    properties: Optional[PackedDeviceProperties] = None,\n+    properties: PackedDeviceProperties | None = None,\n ) -> DeviceProperties:\n     \"\"\"\n     Unpack a `PackedDeviceProperties` tuple into consistently formatted `DeviceProperties` tuple. If properties is None, it is fetched.\n@@ -3749,7 +3749,7 @@ def patch_testing_methods_to_collect_info():\n     _patch_with_call_info(unittest.case.TestCase, \"assertGreaterEqual\", _parse_call_info, target_args=(\"a\", \"b\"))\n \n \n-def torchrun(script: str, nproc_per_node: int, is_torchrun: bool = True, env: Optional[dict] = None):\n+def torchrun(script: str, nproc_per_node: int, is_torchrun: bool = True, env: dict | None = None):\n     \"\"\"Run the `script` using `torchrun` command for multi-processing in a subprocess. Captures errors as necessary.\"\"\"\n     with tempfile.NamedTemporaryFile(mode=\"w+\", suffix=\".py\") as tmp:\n         tmp.write(script)"
        },
        {
            "sha": "ebe1a2cbd3c2819276a0cd0d6105caa5a6397f0d",
            "filename": "src/transformers/time_series_utils.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftime_series_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftime_series_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftime_series_utils.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -17,7 +17,6 @@\n \"\"\"\n \n from collections.abc import Callable\n-from typing import Optional\n \n import torch\n from torch import nn\n@@ -103,8 +102,8 @@ def _base_distribution(self, distr_args):\n     def distribution(\n         self,\n         distr_args,\n-        loc: Optional[torch.Tensor] = None,\n-        scale: Optional[torch.Tensor] = None,\n+        loc: torch.Tensor | None = None,\n+        scale: torch.Tensor | None = None,\n     ) -> Distribution:\n         distr = self._base_distribution(distr_args)\n         if loc is None and scale is None:\n@@ -215,7 +214,7 @@ def _base_distribution(self, distr_args) -> Distribution:\n     # transformation since negative binomial should return integers. Instead\n     # we scale the parameters.\n     def distribution(\n-        self, distr_args, loc: Optional[torch.Tensor] = None, scale: Optional[torch.Tensor] = None\n+        self, distr_args, loc: torch.Tensor | None = None, scale: torch.Tensor | None = None\n     ) -> Distribution:\n         total_count, logits = distr_args\n "
        },
        {
            "sha": "1851b0f183c7849b23186979c20826bf25d15a74",
            "filename": "src/transformers/tokenization_mistral_common.py",
            "status": "modified",
            "additions": 96,
            "deletions": 105,
            "changes": 201,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftokenization_mistral_common.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_mistral_common.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -19,7 +19,7 @@\n from collections.abc import Callable, Mapping, Sized\n from enum import Enum\n from pathlib import Path\n-from typing import Any, Optional, Union, overload\n+from typing import Any, Union, overload\n \n import numpy as np\n \n@@ -204,12 +204,12 @@ class MistralCommonTokenizer(PushToHubMixin):\n \n     def __init__(\n         self,\n-        tokenizer_path: Union[str, os.PathLike, Path],\n+        tokenizer_path: str | os.PathLike | Path,\n         mode: ValidationMode = ValidationMode.test,\n         model_max_length: int = VERY_LARGE_INTEGER,\n         padding_side: str = \"left\",\n         truncation_side: str = \"right\",\n-        model_input_names: Optional[list[str]] = None,\n+        model_input_names: list[str] | None = None,\n         clean_up_tokenization_spaces: bool = False,\n         **kwargs,\n     ):\n@@ -272,7 +272,7 @@ def __init__(\n                 )\n             self.model_input_names = model_input_names\n \n-        self._cache_get_vocab: Optional[dict[str, int]] = None\n+        self._cache_get_vocab: dict[str, int] | None = None\n \n     @property\n     def bos_token_id(self) -> int:\n@@ -378,16 +378,16 @@ def __len__(self):\n     )\n     def encode(\n         self,\n-        text: Union[TextInput, EncodedInput],\n+        text: TextInput | EncodedInput,\n         text_pair: None = None,\n         add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy, None] = None,\n-        max_length: Optional[int] = None,\n+        padding: bool | str | PaddingStrategy = False,\n+        truncation: bool | str | TruncationStrategy | None = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_tensors: str | TensorType | None = None,\n         verbose: bool = True,\n         **kwargs,\n     ) -> list[int]:\n@@ -436,7 +436,7 @@ def decode(\n         self,\n         token_ids: Union[int, list[int], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: Optional[bool] = None,\n+        clean_up_tokenization_spaces: bool | None = None,\n         **kwargs,\n     ) -> str:\n         \"\"\"\n@@ -484,7 +484,7 @@ def batch_decode(\n         self,\n         sequences: Union[list[int], list[list[int]], np.ndarray, \"torch.Tensor\"],\n         skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: Optional[bool] = None,\n+        clean_up_tokenization_spaces: bool | None = None,\n         **kwargs,\n     ) -> list[str]:\n         \"\"\"\n@@ -527,9 +527,7 @@ def _is_control_token(self, token_id: int) -> bool:\n     def convert_ids_to_tokens(self, ids: int, skip_special_tokens: bool = False) -> str: ...\n     @overload\n     def convert_ids_to_tokens(self, ids: list[int], skip_special_tokens: bool = False) -> list[str]: ...\n-    def convert_ids_to_tokens(\n-        self, ids: Union[int, list[int]], skip_special_tokens: bool = False\n-    ) -> Union[str, list[str]]:\n+    def convert_ids_to_tokens(self, ids: int | list[int], skip_special_tokens: bool = False) -> str | list[str]:\n         \"\"\"\n         Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n         added tokens.\n@@ -587,7 +585,7 @@ def _piece_to_id(self, piece: str, warn: bool) -> int:\n         else:\n             raise ValueError(f\"Unknown tokenizer type: {self._tokenizer_type}\")\n \n-    def convert_tokens_to_ids(self, tokens: Union[str, list[str]]) -> Union[int, list[int]]:\n+    def convert_tokens_to_ids(self, tokens: str | list[str]) -> int | list[int]:\n         \"\"\"\n         Converts a token string (or a sequence of tokens) in a single integer id (or a sequence of ids), using the\n         vocabulary.\n@@ -645,16 +643,16 @@ def tokenize(self, text: TextInput, **kwargs) -> list[str]:\n \n     def _encode_plus(\n         self,\n-        text: Union[TextInput, EncodedInput],\n+        text: TextInput | EncodedInput,\n         add_special_tokens: bool = True,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n-        max_length: Optional[int] = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_length: bool = False,\n@@ -690,19 +688,16 @@ def get_input_ids(text):\n \n     def _batch_encode_plus(\n         self,\n-        batch_text: Union[\n-            list[TextInput],\n-            list[EncodedInput],\n-        ],\n+        batch_text: list[TextInput] | list[EncodedInput],\n         add_special_tokens: bool = True,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n-        max_length: Optional[int] = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_length: bool = False,\n@@ -781,16 +776,16 @@ def get_special_tokens_mask(\n \n     def _batch_prepare_for_model(\n         self,\n-        batch_ids: list[Union[PreTokenizedInput, list[int]]],\n+        batch_ids: list[PreTokenizedInput | list[int]],\n         add_special_tokens: bool = True,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n-        max_length: Optional[int] = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_tensors: Optional[str] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_tensors: str | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_length: bool = False,\n@@ -849,14 +844,14 @@ def prepare_for_model(\n         ids: list[int],\n         pair_ids: None = None,\n         add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy, None] = None,\n-        max_length: Optional[int] = None,\n+        padding: bool | str | PaddingStrategy = False,\n+        truncation: bool | str | TruncationStrategy | None = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_length: bool = False,\n@@ -944,10 +939,10 @@ def prepare_for_model(\n \n     def _get_padding_truncation_strategies(\n         self,\n-        padding: Union[str, PaddingStrategy, bool] = False,\n-        truncation: Optional[Union[str, TruncationStrategy, bool]] = None,\n-        max_length: Optional[int] = None,\n-        pad_to_multiple_of: Optional[int] = None,\n+        padding: str | PaddingStrategy | bool = False,\n+        truncation: str | TruncationStrategy | bool | None = None,\n+        max_length: int | None = None,\n+        pad_to_multiple_of: int | None = None,\n         verbose: bool = True,\n         **kwargs,\n     ):\n@@ -1057,12 +1052,12 @@ def _get_padding_truncation_strategies(\n \n     def _pad(\n         self,\n-        encoded_inputs: Union[dict[str, EncodedInput], BatchEncoding],\n-        max_length: Optional[int] = None,\n+        encoded_inputs: dict[str, EncodedInput] | BatchEncoding,\n+        max_length: int | None = None,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_attention_mask: bool | None = None,\n     ) -> dict:\n         \"\"\"\n         Pad encoded inputs (on left/right and up to predefined length or max length in the batch)\n@@ -1131,19 +1126,17 @@ def _pad(\n \n     def pad(\n         self,\n-        encoded_inputs: Union[\n-            BatchEncoding,\n-            list[BatchEncoding],\n-            dict[str, EncodedInput],\n-            dict[str, list[EncodedInput]],\n-            list[dict[str, EncodedInput]],\n-        ],\n-        padding: Union[bool, str, PaddingStrategy] = True,\n-        max_length: Optional[int] = None,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_attention_mask: Optional[bool] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        encoded_inputs: BatchEncoding\n+        | list[BatchEncoding]\n+        | dict[str, EncodedInput]\n+        | dict[str, list[EncodedInput]]\n+        | list[dict[str, EncodedInput]],\n+        padding: bool | str | PaddingStrategy = True,\n+        max_length: int | None = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_attention_mask: bool | None = None,\n+        return_tensors: str | TensorType | None = None,\n         verbose: bool = True,\n     ) -> BatchEncoding:\n         \"\"\"\n@@ -1297,7 +1290,7 @@ def truncate_sequences(\n         ids: list[int],\n         pair_ids: None = None,\n         num_tokens_to_remove: int = 0,\n-        truncation_strategy: Union[str, TruncationStrategy] = \"longest_first\",\n+        truncation_strategy: str | TruncationStrategy = \"longest_first\",\n         stride: int = 0,\n         **kwargs,\n     ) -> tuple[list[int], None, list[int]]:\n@@ -1369,18 +1362,18 @@ def truncate_sequences(\n \n     def apply_chat_template(\n         self,\n-        conversation: Union[list[dict[str, str]], list[list[dict[str, str]]]],\n-        tools: Optional[list[Union[dict, Callable]]] = None,\n+        conversation: list[dict[str, str]] | list[list[dict[str, str]]],\n+        tools: list[dict | Callable] | None = None,\n         add_generation_prompt: bool = False,\n         continue_final_message: bool = False,\n         tokenize: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n+        padding: bool | str | PaddingStrategy = False,\n         truncation: bool = False,\n-        max_length: Optional[int] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n+        max_length: int | None = None,\n+        return_tensors: str | TensorType | None = None,\n         return_dict: bool = True,\n         **kwargs,\n-    ) -> Union[str, list[int], list[str], list[list[int]], BatchEncoding]:\n+    ) -> str | list[int] | list[str] | list[list[int]] | BatchEncoding:\n         \"\"\"\n         Converts a list of dictionaries with `\"role\"` and `\"content\"` keys to a list of token\n         ids.\n@@ -1465,21 +1458,19 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n             \"\"\"Adapt message to `mistral-common` format and leave validation to `mistral-common`.\"\"\"\n             if not isinstance(message, dict):\n                 return\n-            maybe_list_content: Optional[Union[str, list[dict[str, Union[str, dict[str, Any]]]]]] = message.get(\n-                \"content\"\n-            )\n+            maybe_list_content: str | list[dict[str, str | dict[str, Any]]] | None = message.get(\"content\")\n             if not maybe_list_content or isinstance(maybe_list_content, str):\n                 return\n \n-            normalized_content: list[dict[str, Union[str, dict[str, Any]]]] = []\n+            normalized_content: list[dict[str, str | dict[str, Any]]] = []\n             for content in maybe_list_content:\n                 content_type = content.get(\"type\", None)\n                 if not content_type:\n                     continue\n                 elif content_type == \"image\":\n-                    maybe_url: Optional[str] = content.get(\"url\")\n-                    maybe_path: Optional[str] = content.get(\"path\")\n-                    maybe_base64: Optional[str] = content.get(\"base64\")\n+                    maybe_url: str | None = content.get(\"url\")\n+                    maybe_path: str | None = content.get(\"path\")\n+                    maybe_base64: str | None = content.get(\"base64\")\n                     if maybe_url:\n                         image_content = maybe_url\n                     elif maybe_path:\n@@ -1494,9 +1485,9 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n                         raise ValueError(\"Image content must be specified.\")\n                     normalized_content.append({\"type\": \"image_url\", \"image_url\": {\"url\": image_content}})\n                 elif content_type == \"audio\":\n-                    maybe_url: Optional[str] = content.get(\"url\")\n-                    maybe_path: Optional[str] = content.get(\"path\")\n-                    maybe_base64: Optional[str] = content.get(\"base64\")\n+                    maybe_url: str | None = content.get(\"url\")\n+                    maybe_path: str | None = content.get(\"path\")\n+                    maybe_base64: str | None = content.get(\"base64\")\n                     if maybe_url or maybe_path:\n                         audio_data = load_audio_as(maybe_url or maybe_path, return_format=\"dict\", force_mono=True)\n                         normalized_content.append({\"type\": \"input_audio\", \"input_audio\": audio_data})\n@@ -1513,7 +1504,7 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n         audios: list[np.ndarray] = []\n \n         for conversation in conversations:\n-            messages: list[dict[str, Union[str, list[dict[str, Union[str, dict[str, Any]]]]]]] = []\n+            messages: list[dict[str, str | list[dict[str, str | dict[str, Any]]]]] = []\n             for message in conversation:\n                 _maybe_adapt_message(message)\n                 messages.append(message)\n@@ -1546,7 +1537,7 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n             )\n             if return_dict:\n                 if images:\n-                    pixel_values: Union[list[np.ndarray], np.ndarray, torch.Tensor]\n+                    pixel_values: list[np.ndarray] | np.ndarray | torch.Tensor\n                     if return_tensors == \"pt\":\n                         if not is_torch_available():\n                             raise ImportError(\n@@ -1582,19 +1573,19 @@ def _maybe_adapt_message(message: dict[str, Any]) -> None:\n     @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n     def __call__(\n         self,\n-        text: Union[TextInput, EncodedInput, list[TextInput], list[EncodedInput], None] = None,\n+        text: TextInput | EncodedInput | list[TextInput] | list[EncodedInput] | None = None,\n         text_pair: None = None,\n         text_target: None = None,\n         text_pair_target: None = None,\n         add_special_tokens: bool = True,\n-        padding: Union[bool, str, PaddingStrategy] = False,\n-        truncation: Union[bool, str, TruncationStrategy, None] = None,\n-        max_length: Optional[int] = None,\n+        padding: bool | str | PaddingStrategy = False,\n+        truncation: bool | str | TruncationStrategy | None = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_length: bool = False,\n@@ -1699,18 +1690,18 @@ def _is_valid_text_input(t):\n     @classmethod\n     def from_pretrained(\n         cls,\n-        pretrained_model_name_or_path: Union[str, os.PathLike],\n+        pretrained_model_name_or_path: str | os.PathLike,\n         *init_inputs,\n         mode: ValidationMode = ValidationMode.test,\n-        cache_dir: Optional[Union[str, os.PathLike]] = None,\n+        cache_dir: str | os.PathLike | None = None,\n         force_download: bool = False,\n         local_files_only: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n+        token: str | bool | None = None,\n         revision: str = \"main\",\n         model_max_length: int = VERY_LARGE_INTEGER,\n         padding_side: str = \"left\",\n         truncation_side: str = \"right\",\n-        model_input_names: Optional[list[str]] = None,\n+        model_input_names: list[str] | None = None,\n         clean_up_tokenization_spaces: bool = False,\n         **kwargs,\n     ):\n@@ -1826,14 +1817,14 @@ def from_pretrained(\n \n     def save_pretrained(\n         self,\n-        save_directory: Union[str, os.PathLike, Path],\n+        save_directory: str | os.PathLike | Path,\n         push_to_hub: bool = False,\n-        token: Optional[Union[str, bool]] = None,\n-        commit_message: Optional[str] = None,\n-        repo_id: Optional[str] = None,\n-        private: Optional[bool] = None,\n-        repo_url: Optional[str] = None,\n-        organization: Optional[str] = None,\n+        token: str | bool | None = None,\n+        commit_message: str | None = None,\n+        repo_id: str | None = None,\n+        private: bool | None = None,\n+        repo_url: str | None = None,\n+        organization: str | None = None,\n         **kwargs,\n     ) -> tuple[str, ...]:\n         \"\"\""
        },
        {
            "sha": "3241b76fc84316b3c5035be534defebb08a41582",
            "filename": "src/transformers/tokenization_utils.py",
            "status": "modified",
            "additions": 33,
            "deletions": 37,
            "changes": 70,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftokenization_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftokenization_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -21,7 +21,7 @@\n import re\n import unicodedata\n from collections import OrderedDict\n-from typing import Any, Optional, Union, overload\n+from typing import Any, overload\n \n from .tokenization_utils_base import (\n     ENCODE_KWARGS_DOCSTRING,\n@@ -587,7 +587,7 @@ def _add_tokens(self, new_tokens: list[str] | list[AddedToken], special_tokens:\n         self._update_total_vocab_size()\n         return added_tokens\n \n-    def _update_trie(self, unique_no_split_tokens: Optional[list[str]] = None):\n+    def _update_trie(self, unique_no_split_tokens: list[str] | None = None):\n         for token in self._added_tokens_decoder.values():\n             if token.content not in self.tokens_trie._tokens:\n                 self.tokens_trie.add(token.content)\n@@ -742,19 +742,19 @@ def _convert_token_to_id(self, token):\n \n     def _encode_plus(\n         self,\n-        text: Union[TextInput, PreTokenizedInput, EncodedInput],\n-        text_pair: Optional[Union[TextInput, PreTokenizedInput, EncodedInput]] = None,\n+        text: TextInput | PreTokenizedInput | EncodedInput,\n+        text_pair: TextInput | PreTokenizedInput | EncodedInput | None = None,\n         add_special_tokens: bool = True,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n-        max_length: Optional[int] = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n         is_split_into_words: bool = False,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        return_token_type_ids: bool | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_offsets_mapping: bool = False,\n@@ -822,25 +822,23 @@ def get_input_ids(text):\n \n     def _batch_encode_plus(\n         self,\n-        batch_text_or_text_pairs: Union[\n-            list[TextInput],\n-            list[TextInputPair],\n-            list[PreTokenizedInput],\n-            list[PreTokenizedInputPair],\n-            list[EncodedInput],\n-            list[EncodedInputPair],\n-        ],\n+        batch_text_or_text_pairs: list[TextInput]\n+        | list[TextInputPair]\n+        | list[PreTokenizedInput]\n+        | list[PreTokenizedInputPair]\n+        | list[EncodedInput]\n+        | list[EncodedInputPair],\n         add_special_tokens: bool = True,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n-        max_length: Optional[int] = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n         is_split_into_words: bool = False,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_tensors: Optional[Union[str, TensorType]] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_tensors: str | TensorType | None = None,\n+        return_token_type_ids: bool | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_offsets_mapping: bool = False,\n@@ -914,17 +912,17 @@ def get_input_ids(text):\n     @add_end_docstrings(ENCODE_KWARGS_DOCSTRING, ENCODE_PLUS_ADDITIONAL_KWARGS_DOCSTRING)\n     def _batch_prepare_for_model(\n         self,\n-        batch_ids_pairs: list[Union[PreTokenizedInputPair, tuple[list[int], None]]],\n+        batch_ids_pairs: list[PreTokenizedInputPair | tuple[list[int], None]],\n         add_special_tokens: bool = True,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n-        max_length: Optional[int] = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_tensors: Optional[str] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_tensors: str | None = None,\n+        return_token_type_ids: bool | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_length: bool = False,\n@@ -1006,7 +1004,7 @@ def prepare_for_tokenization(\n         return (text, kwargs)\n \n     def get_special_tokens_mask(\n-        self, token_ids_0: list, token_ids_1: Optional[list] = None, already_has_special_tokens: bool = False\n+        self, token_ids_0: list, token_ids_1: list | None = None, already_has_special_tokens: bool = False\n     ) -> list[int]:\n         \"\"\"\n         Retrieves sequence ids from a token list that has no special tokens added. This method is called when adding\n@@ -1041,9 +1039,7 @@ def convert_ids_to_tokens(self, ids: int, skip_special_tokens: bool = False) ->\n     @overload\n     def convert_ids_to_tokens(self, ids: list[int], skip_special_tokens: bool = False) -> list[str]: ...\n \n-    def convert_ids_to_tokens(\n-        self, ids: Union[int, list[int]], skip_special_tokens: bool = False\n-    ) -> Union[str, list[str]]:\n+    def convert_ids_to_tokens(self, ids: int | list[int], skip_special_tokens: bool = False) -> str | list[str]:\n         \"\"\"\n         Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n         added tokens.\n@@ -1081,9 +1077,9 @@ def convert_tokens_to_string(self, tokens: list[str]) -> str:\n \n     def _decode(\n         self,\n-        token_ids: Union[int, list[int]],\n+        token_ids: int | list[int],\n         skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: Optional[bool] = None,\n+        clean_up_tokenization_spaces: bool | None = None,\n         spaces_between_special_tokens: bool = True,\n         **kwargs,\n     ) -> str:"
        },
        {
            "sha": "7907a2aa0cb9985ed5d10b588d032e5bb75e3684",
            "filename": "src/transformers/tokenization_utils_fast.py",
            "status": "modified",
            "additions": 34,
            "deletions": 35,
            "changes": 69,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftokenization_utils_fast.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftokenization_utils_fast.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -21,7 +21,7 @@\n import os\n from collections import defaultdict\n from collections.abc import Iterable\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import tokenizers.pre_tokenizers as pre_tokenizers_fast\n from tokenizers import Encoding as EncodingFast\n@@ -93,7 +93,7 @@ class PreTrainedTokenizerFast(PreTrainedTokenizerBase):\n     \"\"\"\n \n     vocab_files_names = VOCAB_FILES_NAMES\n-    slow_tokenizer_class: Optional[type[PreTrainedTokenizer]] = None\n+    slow_tokenizer_class: type[PreTrainedTokenizer] | None = None\n \n     def __init__(self, *args, **kwargs):\n         tokenizer_object = kwargs.pop(\"tokenizer_object\", None)\n@@ -307,8 +307,8 @@ def decoder(self) -> DecoderFast:\n     def _convert_encoding(\n         self,\n         encoding: EncodingFast,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        return_token_type_ids: bool | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_offsets_mapping: bool = False,\n@@ -351,7 +351,7 @@ def _convert_encoding(\n \n         return encoding_dict, encodings\n \n-    def convert_tokens_to_ids(self, tokens: Union[str, Iterable[str]]) -> Union[int, list[int]]:\n+    def convert_tokens_to_ids(self, tokens: str | Iterable[str]) -> int | list[int]:\n         \"\"\"\n         Converts a token string (or a sequence of tokens) in a single integer id (or a Iterable of ids), using the\n         vocabulary.\n@@ -373,10 +373,10 @@ def _convert_token_to_id_with_added_voc(self, token: str) -> int:\n             return self.unk_token_id\n         return index\n \n-    def _convert_id_to_token(self, index: int) -> Optional[str]:\n+    def _convert_id_to_token(self, index: int) -> str | None:\n         return self._tokenizer.id_to_token(int(index))\n \n-    def _add_tokens(self, new_tokens: list[Union[str, AddedToken]], special_tokens=False) -> int:\n+    def _add_tokens(self, new_tokens: list[str | AddedToken], special_tokens=False) -> int:\n         if special_tokens:\n             return self._tokenizer.add_special_tokens(new_tokens)\n \n@@ -403,9 +403,7 @@ def num_special_tokens_to_add(self, pair: bool = False) -> int:\n         \"\"\"\n         return self._tokenizer.num_special_tokens_to_add(pair)\n \n-    def convert_ids_to_tokens(\n-        self, ids: Union[int, list[int]], skip_special_tokens: bool = False\n-    ) -> Union[str, list[str]]:\n+    def convert_ids_to_tokens(self, ids: int | list[int], skip_special_tokens: bool = False) -> str | list[str]:\n         \"\"\"\n         Converts a single index or a sequence of indices in a token or a sequence of tokens, using the vocabulary and\n         added tokens.\n@@ -431,7 +429,7 @@ def convert_ids_to_tokens(\n             tokens.append(self._tokenizer.id_to_token(index))\n         return tokens\n \n-    def tokenize(self, text: str, pair: Optional[str] = None, add_special_tokens: bool = False, **kwargs) -> list[str]:\n+    def tokenize(self, text: str, pair: str | None = None, add_special_tokens: bool = False, **kwargs) -> list[str]:\n         return self.encode_plus(text=text, text_pair=pair, add_special_tokens=add_special_tokens, **kwargs).tokens()\n \n     def set_truncation_and_padding(\n@@ -440,8 +438,8 @@ def set_truncation_and_padding(\n         truncation_strategy: TruncationStrategy,\n         max_length: int,\n         stride: int,\n-        pad_to_multiple_of: Optional[int],\n-        padding_side: Optional[str],\n+        pad_to_multiple_of: int | None,\n+        padding_side: str | None,\n     ):\n         \"\"\"\n         Define the truncation and the padding strategies for fast tokenizers (provided by HuggingFace tokenizers\n@@ -511,20 +509,21 @@ def set_truncation_and_padding(\n \n     def _batch_encode_plus(\n         self,\n-        batch_text_or_text_pairs: Union[\n-            list[TextInput], list[TextInputPair], list[PreTokenizedInput], list[PreTokenizedInputPair]\n-        ],\n+        batch_text_or_text_pairs: list[TextInput]\n+        | list[TextInputPair]\n+        | list[PreTokenizedInput]\n+        | list[PreTokenizedInputPair],\n         add_special_tokens: bool = True,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n-        max_length: Optional[int] = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n         is_split_into_words: bool = False,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_tensors: Optional[str] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_tensors: str | None = None,\n+        return_token_type_ids: bool | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_offsets_mapping: bool = False,\n@@ -602,19 +601,19 @@ def _batch_encode_plus(\n \n     def _encode_plus(\n         self,\n-        text: Union[TextInput, PreTokenizedInput],\n-        text_pair: Optional[Union[TextInput, PreTokenizedInput]] = None,\n+        text: TextInput | PreTokenizedInput,\n+        text_pair: TextInput | PreTokenizedInput | None = None,\n         add_special_tokens: bool = True,\n         padding_strategy: PaddingStrategy = PaddingStrategy.DO_NOT_PAD,\n         truncation_strategy: TruncationStrategy = TruncationStrategy.DO_NOT_TRUNCATE,\n-        max_length: Optional[int] = None,\n+        max_length: int | None = None,\n         stride: int = 0,\n         is_split_into_words: bool = False,\n-        pad_to_multiple_of: Optional[int] = None,\n-        padding_side: Optional[str] = None,\n-        return_tensors: Optional[bool] = None,\n-        return_token_type_ids: Optional[bool] = None,\n-        return_attention_mask: Optional[bool] = None,\n+        pad_to_multiple_of: int | None = None,\n+        padding_side: str | None = None,\n+        return_tensors: bool | None = None,\n+        return_token_type_ids: bool | None = None,\n+        return_attention_mask: bool | None = None,\n         return_overflowing_tokens: bool = False,\n         return_special_tokens_mask: bool = False,\n         return_offsets_mapping: bool = False,\n@@ -670,9 +669,9 @@ def convert_tokens_to_string(self, tokens: list[str]) -> str:\n \n     def _decode(\n         self,\n-        token_ids: Union[int, list[int]],\n+        token_ids: int | list[int],\n         skip_special_tokens: bool = False,\n-        clean_up_tokenization_spaces: Optional[bool] = None,\n+        clean_up_tokenization_spaces: bool | None = None,\n         **kwargs,\n     ) -> str:\n         self._decode_use_source_tokenizer = kwargs.pop(\"use_source_tokenizer\", False)\n@@ -694,10 +693,10 @@ def _decode(\n \n     def _save_pretrained(\n         self,\n-        save_directory: Union[str, os.PathLike],\n+        save_directory: str | os.PathLike,\n         file_names: tuple[str, ...],\n-        legacy_format: Optional[bool] = None,\n-        filename_prefix: Optional[str] = None,\n+        legacy_format: bool | None = None,\n+        filename_prefix: str | None = None,\n     ) -> tuple[str, ...]:\n         \"\"\"\n         Save a tokenizer using the slow-tokenizer/legacy format: vocabulary + added tokens as well as in a unique JSON"
        },
        {
            "sha": "1b9ca10ccb619ae7ce48239f0819a755fb2d924a",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 73,
            "deletions": 75,
            "changes": 148,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -33,7 +33,7 @@\n from collections.abc import Callable, Iterator, Mapping\n from functools import partial\n from pathlib import Path\n-from typing import TYPE_CHECKING, Any, Optional, Union\n+from typing import TYPE_CHECKING, Any, Union\n \n \n # Integrations must be imported before ML frameworks:\n@@ -385,21 +385,23 @@ class Trainer:\n \n     def __init__(\n         self,\n-        model: Union[PreTrainedModel, nn.Module, None] = None,\n-        args: Optional[TrainingArguments] = None,\n-        data_collator: Optional[DataCollator] = None,\n-        train_dataset: Optional[Union[Dataset, IterableDataset, \"datasets.Dataset\"]] = None,\n-        eval_dataset: Optional[Union[Dataset, dict[str, Dataset], \"datasets.Dataset\"]] = None,\n-        processing_class: Optional[\n-            Union[PreTrainedTokenizerBase, BaseImageProcessor, FeatureExtractionMixin, ProcessorMixin]\n-        ] = None,\n-        model_init: Optional[Callable[..., PreTrainedModel]] = None,\n-        compute_loss_func: Optional[Callable] = None,\n-        compute_metrics: Optional[Callable[[EvalPrediction], dict]] = None,\n-        callbacks: Optional[list[TrainerCallback]] = None,\n-        optimizers: tuple[Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None),\n-        optimizer_cls_and_kwargs: Optional[tuple[type[torch.optim.Optimizer], dict[str, Any]]] = None,\n-        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n+        model: PreTrainedModel | nn.Module | None = None,\n+        args: TrainingArguments | None = None,\n+        data_collator: DataCollator | None = None,\n+        train_dataset: Union[Dataset, IterableDataset, \"datasets.Dataset\"] | None = None,\n+        eval_dataset: Union[Dataset, dict[str, Dataset], \"datasets.Dataset\"] | None = None,\n+        processing_class: PreTrainedTokenizerBase\n+        | BaseImageProcessor\n+        | FeatureExtractionMixin\n+        | ProcessorMixin\n+        | None = None,\n+        model_init: Callable[..., PreTrainedModel] | None = None,\n+        compute_loss_func: Callable | None = None,\n+        compute_metrics: Callable[[EvalPrediction], dict] | None = None,\n+        callbacks: list[TrainerCallback] | None = None,\n+        optimizers: tuple[torch.optim.Optimizer | None, torch.optim.lr_scheduler.LambdaLR | None] = (None, None),\n+        optimizer_cls_and_kwargs: tuple[type[torch.optim.Optimizer], dict[str, Any]] | None = None,\n+        preprocess_logits_for_metrics: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n     ):\n         if args is None:\n             output_dir = \"tmp_trainer\"\n@@ -932,7 +934,7 @@ def _set_signature_columns_if_needed(self):\n             # Labels may be named label or label_ids, the default data collator handles that.\n             self._signature_columns += list(set([\"label\", \"label_ids\"] + self.label_names))\n \n-    def _remove_unused_columns(self, dataset: \"datasets.Dataset\", description: Optional[str] = None):\n+    def _remove_unused_columns(self, dataset: \"datasets.Dataset\", description: str | None = None):\n         if not self.args.remove_unused_columns:\n             return dataset\n         self._set_signature_columns_if_needed()\n@@ -964,9 +966,7 @@ def _remove_unused_columns(self, dataset: \"datasets.Dataset\", description: Optio\n         else:\n             return dataset.remove_columns(ignored_columns)\n \n-    def _get_collator_with_removed_columns(\n-        self, data_collator: Callable, description: Optional[str] = None\n-    ) -> Callable:\n+    def _get_collator_with_removed_columns(self, data_collator: Callable, description: str | None = None) -> Callable:\n         \"\"\"Wrap the data collator in a callable removing unused columns.\"\"\"\n         if not self.args.remove_unused_columns:\n             return data_collator\n@@ -982,7 +982,7 @@ def _get_collator_with_removed_columns(\n         )\n         return remove_columns_collator\n \n-    def _get_train_sampler(self, train_dataset: Optional[Dataset] = None) -> Optional[torch.utils.data.Sampler]:\n+    def _get_train_sampler(self, train_dataset: Dataset | None = None) -> torch.utils.data.Sampler | None:\n         if train_dataset is None:\n             train_dataset = self.train_dataset\n         if train_dataset is None or not has_length(train_dataset):\n@@ -1016,9 +1016,9 @@ def _get_dataloader(\n         dataset: Dataset,\n         description: str,\n         batch_size: int,\n-        sampler_fn: Optional[Callable[[Dataset], torch.utils.data.Sampler]] = None,\n+        sampler_fn: Callable[[Dataset], torch.utils.data.Sampler] | None = None,\n         is_training: bool = False,\n-        dataloader_key: Optional[str] = None,\n+        dataloader_key: str | None = None,\n     ) -> DataLoader:\n         \"\"\"Create a [`~torch.utils.data.DataLoader`] from the given dataset.\"\"\"\n \n@@ -1081,7 +1081,7 @@ def get_train_dataloader(self) -> DataLoader:\n             is_training=True,\n         )\n \n-    def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.Sampler]:\n+    def _get_eval_sampler(self, eval_dataset: Dataset) -> torch.utils.data.Sampler | None:\n         if eval_dataset is None or not has_length(eval_dataset):\n             return None\n \n@@ -1109,7 +1109,7 @@ def _get_eval_sampler(self, eval_dataset: Dataset) -> Optional[torch.utils.data.\n         else:\n             return None\n \n-    def get_eval_dataloader(self, eval_dataset: Optional[Union[str, Dataset]] = None) -> DataLoader:\n+    def get_eval_dataloader(self, eval_dataset: str | Dataset | None = None) -> DataLoader:\n         \"\"\"\n         Returns the evaluation [`~torch.utils.data.DataLoader`].\n \n@@ -1275,7 +1275,7 @@ def get_learning_rates(self):\n             raise ValueError(\"Trainer optimizer is None, please make sure you have setup the optimizer before.\")\n         return [group[\"lr\"] for group in self.optimizer.param_groups]\n \n-    def get_optimizer_group(self, param: Optional[Union[str, torch.nn.parameter.Parameter]] = None):\n+    def get_optimizer_group(self, param: str | torch.nn.parameter.Parameter | None = None):\n         \"\"\"\n         Returns optimizer group for a parameter if given, else returns all optimizer groups for params.\n \n@@ -1292,9 +1292,7 @@ def get_optimizer_group(self, param: Optional[Union[str, torch.nn.parameter.Para\n         return [group[\"params\"] for group in self.optimizer.param_groups]\n \n     @staticmethod\n-    def get_optimizer_cls_and_kwargs(\n-        args: TrainingArguments, model: Optional[PreTrainedModel] = None\n-    ) -> tuple[Any, Any]:\n+    def get_optimizer_cls_and_kwargs(args: TrainingArguments, model: PreTrainedModel | None = None) -> tuple[Any, Any]:\n         \"\"\"\n         Returns the optimizer class and optimizer parameters based on the training arguments.\n \n@@ -1776,7 +1774,7 @@ def num_examples(self, dataloader: DataLoader) -> int:\n             return len(dataloader) * self.args.per_device_train_batch_size\n \n     @staticmethod\n-    def num_tokens(train_dl: DataLoader, max_steps: Optional[int] = None) -> int:\n+    def num_tokens(train_dl: DataLoader, max_steps: int | None = None) -> int:\n         \"\"\"\n         Helper to get number of tokens in a [`~torch.utils.data.DataLoader`] by enumerating dataloader.\n         \"\"\"\n@@ -2063,9 +2061,9 @@ def patched_optimizer_step(optimizer, barrier=False, optimizer_args={}):\n \n     def train(\n         self,\n-        resume_from_checkpoint: Optional[Union[str, bool]] = None,\n+        resume_from_checkpoint: str | bool | None = None,\n         trial: Union[\"optuna.Trial\", dict[str, Any], None] = None,\n-        ignore_keys_for_eval: Optional[list[str]] = None,\n+        ignore_keys_for_eval: list[str] | None = None,\n     ):\n         \"\"\"\n         Main training entry point.\n@@ -2425,7 +2423,7 @@ def _inner_training_loop(\n         self._total_loss_scalar = 0.0\n         self._globalstep_last_logged = self.state.global_step\n         model.zero_grad()\n-        grad_norm: Optional[float] = None\n+        grad_norm: float | None = None\n         learning_rate = None\n         self.control = self.callback_handler.on_train_begin(args, self.state, self.control)\n \n@@ -3480,14 +3478,14 @@ def _load_callback_state(self):\n \n     def hyperparameter_search(\n         self,\n-        hp_space: Optional[Callable[[\"optuna.Trial\"], dict[str, float]]] = None,\n-        compute_objective: Optional[Callable[[dict[str, float]], float]] = None,\n+        hp_space: Callable[[\"optuna.Trial\"], dict[str, float]] | None = None,\n+        compute_objective: Callable[[dict[str, float]], float] | None = None,\n         n_trials: int = 20,\n-        direction: Union[str, list[str]] = \"minimize\",\n-        backend: Optional[Union[\"str\", HPSearchBackend]] = None,\n-        hp_name: Optional[Callable[[\"optuna.Trial\"], str]] = None,\n+        direction: str | list[str] = \"minimize\",\n+        backend: Union[\"str\", HPSearchBackend] | None = None,\n+        hp_name: Callable[[\"optuna.Trial\"], str] | None = None,\n         **kwargs,\n-    ) -> Union[BestRun, list[BestRun]]:\n+    ) -> BestRun | list[BestRun]:\n         \"\"\"\n         Launch an hyperparameter search using `optuna` or `Ray Tune`. The optimized quantity is determined\n         by `compute_objective`, which defaults to a function returning the evaluation loss when no metric is provided,\n@@ -3559,7 +3557,7 @@ def hyperparameter_search(\n         self.hp_search_backend = None\n         return best_run\n \n-    def log(self, logs: dict[str, float], start_time: Optional[float] = None) -> None:\n+    def log(self, logs: dict[str, float], start_time: float | None = None) -> None:\n         \"\"\"\n         Log `logs` on the various objects watching training.\n \n@@ -3585,7 +3583,7 @@ def log(self, logs: dict[str, float], start_time: Optional[float] = None) -> Non\n         self.state.log_history.append(output)\n         self.control = self.callback_handler.on_log(self.args, self.state, self.control, logs)\n \n-    def _prepare_input(self, data: Union[torch.Tensor, Any]) -> Union[torch.Tensor, Any]:\n+    def _prepare_input(self, data: torch.Tensor | Any) -> torch.Tensor | Any:\n         \"\"\"\n         Prepares one `data` before feeding it to the model, be it a tensor or a nested list/dictionary of tensors.\n         \"\"\"\n@@ -3603,7 +3601,7 @@ def _prepare_input(self, data: Union[torch.Tensor, Any]) -> Union[torch.Tensor,\n             return data.to(**kwargs)\n         return data\n \n-    def _prepare_inputs(self, inputs: dict[str, Union[torch.Tensor, Any]]) -> dict[str, Union[torch.Tensor, Any]]:\n+    def _prepare_inputs(self, inputs: dict[str, torch.Tensor | Any]) -> dict[str, torch.Tensor | Any]:\n         \"\"\"\n         Prepare `inputs` before feeding them to the model, converting them to tensors if they are not already and\n         handling potential state.\n@@ -3660,7 +3658,7 @@ def _is_attention_mask_causal(self, attention_mask):\n         # For unknown dimensions, be conservative and reject\n         return False\n \n-    def _prepare_context_parallel_inputs(self, model, inputs: dict[str, Union[torch.Tensor, Any]]):\n+    def _prepare_context_parallel_inputs(self, model, inputs: dict[str, torch.Tensor | Any]):\n         \"\"\"\n         Prepare inputs for context parallelism by setting up buffers and validation.\n \n@@ -3764,7 +3762,7 @@ def compute_loss_context_manager(self):\n \n         return ctx_stack\n \n-    def autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True):\n+    def autocast_smart_context_manager(self, cache_enabled: bool | None = True):\n         \"\"\"\n         A helper wrapper that creates an appropriate context manager for `autocast` while feeding it the desired\n         arguments, depending on the situation. We rely on accelerate for autocast, hence we do nothing here.\n@@ -3774,8 +3772,8 @@ def autocast_smart_context_manager(self, cache_enabled: Optional[bool] = True):\n     def training_step(\n         self,\n         model: nn.Module,\n-        inputs: dict[str, Union[torch.Tensor, Any]],\n-        num_items_in_batch: Optional[torch.Tensor] = None,\n+        inputs: dict[str, torch.Tensor | Any],\n+        num_items_in_batch: torch.Tensor | None = None,\n     ) -> torch.Tensor:\n         \"\"\"\n         Perform a training step on a batch of inputs.\n@@ -3845,9 +3843,9 @@ def training_step(\n     def compute_loss(\n         self,\n         model: nn.Module,\n-        inputs: dict[str, Union[torch.Tensor, Any]],\n+        inputs: dict[str, torch.Tensor | Any],\n         return_outputs: bool = False,\n-        num_items_in_batch: Optional[torch.Tensor] = None,\n+        num_items_in_batch: torch.Tensor | None = None,\n     ):\n         \"\"\"\n         How the loss is computed by Trainer. By default, all models return the loss in the first element.\n@@ -3993,7 +3991,7 @@ def is_world_process_zero(self) -> bool:\n         else:\n             return self.args.process_index == 0\n \n-    def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = False):\n+    def save_model(self, output_dir: str | None = None, _internal_call: bool = False):\n         \"\"\"\n         Will save the model, so you can reload it using `from_pretrained()`.\n \n@@ -4049,7 +4047,7 @@ def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = Fa\n         if self.args.push_to_hub and not _internal_call:\n             self.push_to_hub(commit_message=\"Model save\", revision=self.args.hub_revision)\n \n-    def _save_tpu(self, output_dir: Optional[str] = None):\n+    def _save_tpu(self, output_dir: str | None = None):\n         output_dir = output_dir if output_dir is not None else self.args.output_dir\n \n         logger.info(f\"Saving model checkpoint to {output_dir}\")\n@@ -4121,7 +4119,7 @@ def _save_tpu(self, output_dir: Optional[str] = None):\n         if self.processing_class is not None and self.args.should_save:\n             self.processing_class.save_pretrained(output_dir)\n \n-    def _save(self, output_dir: Optional[str] = None, state_dict=None):\n+    def _save(self, output_dir: str | None = None, state_dict=None):\n         # If we are executing this function, we are the process zero, so we don't check for that.\n         output_dir = output_dir if output_dir is not None else self.args.output_dir\n         os.makedirs(output_dir, exist_ok=True)\n@@ -4239,8 +4237,8 @@ def _rotate_checkpoints(self, use_mtime=False, output_dir=None) -> None:\n \n     def evaluate(\n         self,\n-        eval_dataset: Optional[Union[Dataset, dict[str, Dataset]]] = None,\n-        ignore_keys: Optional[list[str]] = None,\n+        eval_dataset: Dataset | dict[str, Dataset] | None = None,\n+        ignore_keys: list[str] | None = None,\n         metric_key_prefix: str = \"eval\",\n     ) -> dict[str, float]:\n         \"\"\"\n@@ -4339,7 +4337,7 @@ def evaluate(\n         return output.metrics\n \n     def predict(\n-        self, test_dataset: Dataset, ignore_keys: Optional[list[str]] = None, metric_key_prefix: str = \"test\"\n+        self, test_dataset: Dataset, ignore_keys: list[str] | None = None, metric_key_prefix: str = \"test\"\n     ) -> PredictionOutput:\n         \"\"\"\n         Run prediction and returns predictions and potential metrics.\n@@ -4403,8 +4401,8 @@ def evaluation_loop(\n         self,\n         dataloader: DataLoader,\n         description: str,\n-        prediction_loss_only: Optional[bool] = None,\n-        ignore_keys: Optional[list[str]] = None,\n+        prediction_loss_only: bool | None = None,\n+        ignore_keys: list[str] | None = None,\n         metric_key_prefix: str = \"eval\",\n     ) -> EvalLoopOutput:\n         \"\"\"\n@@ -4627,10 +4625,10 @@ def _nested_gather(self, tensors, name=None):\n     def prediction_step(\n         self,\n         model: nn.Module,\n-        inputs: dict[str, Union[torch.Tensor, Any]],\n+        inputs: dict[str, torch.Tensor | Any],\n         prediction_loss_only: bool,\n-        ignore_keys: Optional[list[str]] = None,\n-    ) -> tuple[Optional[torch.Tensor], Optional[torch.Tensor], Optional[torch.Tensor]]:\n+        ignore_keys: list[str] | None = None,\n+    ) -> tuple[torch.Tensor | None, torch.Tensor | None, torch.Tensor | None]:\n         \"\"\"\n         Perform an evaluation step on `model` using `inputs`.\n \n@@ -4729,7 +4727,7 @@ def prediction_step(\n \n         return (loss, logits, labels)\n \n-    def floating_point_ops(self, inputs: dict[str, Union[torch.Tensor, Any]]):\n+    def floating_point_ops(self, inputs: dict[str, torch.Tensor | Any]):\n         \"\"\"\n         For models that inherit from [`PreTrainedModel`], uses that method to compute the number of floating point\n         operations for every backward + forward pass. If using another model, either implement such a method in the\n@@ -4748,7 +4746,7 @@ def floating_point_ops(self, inputs: dict[str, Union[torch.Tensor, Any]]):\n             return 6 * inputs[main_input].numel() * self.model.num_parameters(exclude_embeddings=True)\n         return 0\n \n-    def init_hf_repo(self, token: Optional[str] = None):\n+    def init_hf_repo(self, token: str | None = None):\n         \"\"\"\n         Initializes a git repo in `self.args.hub_model_id`.\n         \"\"\"\n@@ -4768,15 +4766,15 @@ def init_hf_repo(self, token: Optional[str] = None):\n \n     def create_model_card(\n         self,\n-        language: Optional[str] = None,\n-        license: Optional[str] = None,\n-        tags: Union[str, list[str], None] = None,\n-        model_name: Optional[str] = None,\n-        finetuned_from: Optional[str] = None,\n-        tasks: Union[str, list[str], None] = None,\n-        dataset_tags: Union[str, list[str], None] = None,\n-        dataset: Union[str, list[str], None] = None,\n-        dataset_args: Union[str, list[str], None] = None,\n+        language: str | None = None,\n+        license: str | None = None,\n+        tags: str | list[str] | None = None,\n+        model_name: str | None = None,\n+        finetuned_from: str | None = None,\n+        tasks: str | list[str] | None = None,\n+        dataset_tags: str | list[str] | None = None,\n+        dataset: str | list[str] | None = None,\n+        dataset_args: str | list[str] | None = None,\n     ):\n         \"\"\"\n         Creates a draft of a model card using the information available to the `Trainer`.\n@@ -4917,10 +4915,10 @@ def _finish_current_push(self):\n \n     def push_to_hub(\n         self,\n-        commit_message: Optional[str] = \"End of training\",\n+        commit_message: str | None = \"End of training\",\n         blocking: bool = True,\n-        token: Optional[str] = None,\n-        revision: Optional[str] = None,\n+        token: str | None = None,\n+        revision: str | None = None,\n         **kwargs,\n     ) -> CommitInfo:\n         \"\"\"\n@@ -5194,7 +5192,7 @@ def _fsdp_qlora_plugin_updates(self):\n                     self.model.hf_quantizer.quantization_config.bnb_4bit_quant_storage, override=True\n                 )\n \n-    def _get_num_items_in_batch(self, batch_samples: list, device: torch.device) -> Optional[Union[torch.Tensor, int]]:\n+    def _get_num_items_in_batch(self, batch_samples: list, device: torch.device) -> torch.Tensor | int | None:\n         \"\"\"\n         Counts the number of items in the batches to properly scale the loss.\n         Args:\n@@ -5248,7 +5246,7 @@ def _get_num_items_in_batch(self, batch_samples: list, device: torch.device) ->\n \n     def get_batch_samples(\n         self, epoch_iterator: Iterator, num_batches: int, device: torch.device\n-    ) -> tuple[list, Optional[Union[torch.Tensor, int]]]:\n+    ) -> tuple[list, torch.Tensor | int | None]:\n         \"\"\"\n         Collects a specified number of batches from the epoch iterator and optionally counts the number of items in the batches to properly scale the loss.\n         \"\"\""
        },
        {
            "sha": "86cb61d182123177bbc9e52d6416fa83c9de54f5",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 9,
            "deletions": 10,
            "changes": 19,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -19,7 +19,6 @@\n import json\n import math\n from dataclasses import dataclass\n-from typing import Optional, Union\n \n import numpy as np\n from tqdm.auto import tqdm\n@@ -93,26 +92,26 @@ class TrainerState:\n             Relevant callbacks should implement a `state` and `from_state` function.\n     \"\"\"\n \n-    epoch: Optional[float] = None\n+    epoch: float | None = None\n     global_step: int = 0\n     max_steps: int = 0\n     logging_steps: int = 500\n     eval_steps: int = 500\n     save_steps: int = 500\n-    train_batch_size: Optional[int] = None\n+    train_batch_size: int | None = None\n     num_train_epochs: int = 0\n     num_input_tokens_seen: int = 0\n     total_flos: float = 0\n     log_history: list[dict[str, float]] = None\n-    best_metric: Optional[float] = None\n-    best_global_step: Optional[int] = None\n-    best_model_checkpoint: Optional[str] = None\n+    best_metric: float | None = None\n+    best_global_step: int | None = None\n+    best_model_checkpoint: str | None = None\n     is_local_process_zero: bool = True\n     is_world_process_zero: bool = True\n     is_hyper_param_search: bool = False\n-    trial_name: Optional[str] = None\n-    trial_params: Optional[dict[str, Union[str, float, int, bool]]] = None\n-    stateful_callbacks: Optional[list[\"TrainerCallback\"]] = None\n+    trial_name: str | None = None\n+    trial_params: dict[str, str | float | int | bool] | None = None\n+    stateful_callbacks: list[\"TrainerCallback\"] | None = None\n \n     def __post_init__(self):\n         if self.log_history is None:\n@@ -708,7 +707,7 @@ class EarlyStoppingCallback(TrainerCallback, ExportableState):\n     early stopping will not occur until the next save step.\n     \"\"\"\n \n-    def __init__(self, early_stopping_patience: int = 1, early_stopping_threshold: Optional[float] = 0.0):\n+    def __init__(self, early_stopping_patience: int = 1, early_stopping_threshold: float | None = 0.0):\n         self.early_stopping_patience = early_stopping_patience\n         self.early_stopping_threshold = early_stopping_threshold\n         # early_stopping_patience_counter denotes the number of times validation metrics failed to improve."
        },
        {
            "sha": "fc755447574177a7e81cbe0d1bfb1345e0c3bb82",
            "filename": "src/transformers/trainer_pt_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftrainer_pt_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_pt_utils.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -29,7 +29,7 @@\n from dataclasses import dataclass, field\n from itertools import chain\n from logging import StreamHandler\n-from typing import Any, Optional, Union\n+from typing import Any\n \n import numpy as np\n import torch\n@@ -69,7 +69,7 @@ def get_dataloader_sampler(dataloader):\n         return dataloader.sampler\n \n \n-def atleast_1d(tensor_or_array: Union[torch.Tensor, np.ndarray]):\n+def atleast_1d(tensor_or_array: torch.Tensor | np.ndarray):\n     if isinstance(tensor_or_array, torch.Tensor):\n         if hasattr(torch, \"atleast_1d\"):\n             tensor_or_array = torch.atleast_1d(tensor_or_array)\n@@ -199,7 +199,7 @@ def nested_xla_mesh_reduce(tensors, name):\n         raise ImportError(\"Torch xla must be installed to use `nested_xla_mesh_reduce`\")\n \n \n-def distributed_concat(tensor: Any, num_total_examples: Optional[int] = None) -> Any:\n+def distributed_concat(tensor: Any, num_total_examples: int | None = None) -> Any:\n     try:\n         if isinstance(tensor, (tuple, list)):\n             return type(tensor)(distributed_concat(t, num_total_examples) for t in tensor)\n@@ -219,9 +219,9 @@ def distributed_concat(tensor: Any, num_total_examples: Optional[int] = None) ->\n \n \n def distributed_broadcast_scalars(\n-    scalars: list[Union[int, float]],\n-    num_total_examples: Optional[int] = None,\n-    device: Optional[torch.device] = torch.device(\"cuda\"),\n+    scalars: list[int | float],\n+    num_total_examples: int | None = None,\n+    device: torch.device | None = torch.device(\"cuda\"),\n ) -> torch.Tensor:\n     try:\n         tensorized_scalar = torch.tensor(scalars, device=device)\n@@ -457,9 +457,9 @@ class LengthGroupedSampler(Sampler):\n     def __init__(\n         self,\n         batch_size: int,\n-        dataset: Optional[Dataset] = None,\n-        lengths: Optional[list[int]] = None,\n-        model_input_name: Optional[str] = None,\n+        dataset: Dataset | None = None,\n+        lengths: list[int] | None = None,\n+        model_input_name: str | None = None,\n         generator=None,\n     ):\n         if dataset is None and lengths is None:\n@@ -501,13 +501,13 @@ class DistributedLengthGroupedSampler(DistributedSampler):\n     def __init__(\n         self,\n         batch_size: int,\n-        dataset: Optional[Dataset] = None,\n-        num_replicas: Optional[int] = None,\n-        rank: Optional[int] = None,\n+        dataset: Dataset | None = None,\n+        num_replicas: int | None = None,\n+        rank: int | None = None,\n         seed: int = 0,\n         drop_last: bool = False,\n-        lengths: Optional[list[int]] = None,\n-        model_input_name: Optional[str] = None,\n+        lengths: list[int] | None = None,\n+        model_input_name: str | None = None,\n     ):\n         if dataset is None and lengths is None:\n             raise ValueError(\"One of dataset and lengths must be provided.\")\n@@ -1095,7 +1095,7 @@ class AcceleratorConfig:\n             \" in your script multiplied by the number of processes.\"\n         },\n     )\n-    dispatch_batches: Optional[bool] = field(\n+    dispatch_batches: bool | None = field(\n         default=None,\n         metadata={\n             \"help\": \"If set to `True`, the dataloader prepared by the Accelerator is only iterated through on the main process\"\n@@ -1131,7 +1131,7 @@ class AcceleratorConfig:\n         },\n     )\n \n-    gradient_accumulation_kwargs: Optional[dict] = field(\n+    gradient_accumulation_kwargs: dict | None = field(\n         default=None,\n         metadata={\n             \"help\": \"Additional kwargs to configure gradient accumulation, see [`accelerate.utils.GradientAccumulationPlugin`]. \"\n@@ -1193,7 +1193,7 @@ def __init__(self, optimizer_dict=None, **kwargs):\n     def zero_grad(self, set_to_none: bool = True) -> None:\n         pass\n \n-    def step(self, closure=None) -> Optional[float]:\n+    def step(self, closure=None) -> float | None:\n         pass\n \n "
        },
        {
            "sha": "1e8ea7461dffb6529ffb5094b6b9bea6c1b0ca10",
            "filename": "src/transformers/trainer_seq2seq.py",
            "status": "modified",
            "additions": 20,
            "deletions": 19,
            "changes": 39,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftrainer_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_seq2seq.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -53,20 +53,21 @@\n class Seq2SeqTrainer(Trainer):\n     def __init__(\n         self,\n-        model: Optional[Union[\"PreTrainedModel\", nn.Module]] = None,\n+        model: Union[\"PreTrainedModel\", nn.Module] | None = None,\n         args: Optional[\"TrainingArguments\"] = None,\n         data_collator: Optional[\"DataCollator\"] = None,\n-        train_dataset: Optional[Union[Dataset, \"IterableDataset\", \"datasets.Dataset\"]] = None,\n-        eval_dataset: Optional[Union[Dataset, dict[str, Dataset]]] = None,\n-        processing_class: Optional[\n-            Union[\"PreTrainedTokenizerBase\", \"BaseImageProcessor\", \"FeatureExtractionMixin\", \"ProcessorMixin\"]\n-        ] = None,\n-        model_init: Optional[Callable[[], \"PreTrainedModel\"]] = None,\n-        compute_loss_func: Optional[Callable] = None,\n-        compute_metrics: Optional[Callable[[\"EvalPrediction\"], dict]] = None,\n-        callbacks: Optional[list[\"TrainerCallback\"]] = None,\n-        optimizers: tuple[Optional[torch.optim.Optimizer], Optional[torch.optim.lr_scheduler.LambdaLR]] = (None, None),\n-        preprocess_logits_for_metrics: Optional[Callable[[torch.Tensor, torch.Tensor], torch.Tensor]] = None,\n+        train_dataset: Union[Dataset, \"IterableDataset\", \"datasets.Dataset\"] | None = None,\n+        eval_dataset: Dataset | dict[str, Dataset] | None = None,\n+        processing_class: Union[\n+            \"PreTrainedTokenizerBase\", \"BaseImageProcessor\", \"FeatureExtractionMixin\", \"ProcessorMixin\"\n+        ]\n+        | None = None,\n+        model_init: Callable[[], \"PreTrainedModel\"] | None = None,\n+        compute_loss_func: Callable | None = None,\n+        compute_metrics: Callable[[\"EvalPrediction\"], dict] | None = None,\n+        callbacks: list[\"TrainerCallback\"] | None = None,\n+        optimizers: tuple[torch.optim.Optimizer | None, torch.optim.lr_scheduler.LambdaLR | None] = (None, None),\n+        preprocess_logits_for_metrics: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] | None = None,\n     ):\n         super().__init__(\n             model=model,\n@@ -90,7 +91,7 @@ def __init__(\n             self.model.generation_config = gen_config\n \n     @staticmethod\n-    def load_generation_config(gen_config_arg: Union[str, GenerationConfig]) -> GenerationConfig:\n+    def load_generation_config(gen_config_arg: str | GenerationConfig) -> GenerationConfig:\n         \"\"\"\n         Loads a `~generation.GenerationConfig` from the `Seq2SeqTrainingArguments.generation_config` arguments.\n \n@@ -135,8 +136,8 @@ def load_generation_config(gen_config_arg: Union[str, GenerationConfig]) -> Gene\n \n     def evaluate(\n         self,\n-        eval_dataset: Optional[Dataset] = None,\n-        ignore_keys: Optional[list[str]] = None,\n+        eval_dataset: Dataset | None = None,\n+        ignore_keys: list[str] | None = None,\n         metric_key_prefix: str = \"eval\",\n         **gen_kwargs,\n     ) -> dict[str, float]:\n@@ -192,7 +193,7 @@ def evaluate(\n     def predict(\n         self,\n         test_dataset: Dataset,\n-        ignore_keys: Optional[list[str]] = None,\n+        ignore_keys: list[str] | None = None,\n         metric_key_prefix: str = \"test\",\n         **gen_kwargs,\n     ) -> \"PredictionOutput\":\n@@ -256,11 +257,11 @@ def predict(\n     def prediction_step(\n         self,\n         model: nn.Module,\n-        inputs: dict[str, Union[torch.Tensor, Any]],\n+        inputs: dict[str, torch.Tensor | Any],\n         prediction_loss_only: bool,\n-        ignore_keys: Optional[list[str]] = None,\n+        ignore_keys: list[str] | None = None,\n         **gen_kwargs,\n-    ) -> tuple[Optional[float], Optional[torch.Tensor], Optional[torch.Tensor]]:\n+    ) -> tuple[float | None, torch.Tensor | None, torch.Tensor | None]:\n         \"\"\"\n         Perform an evaluation step on `model` using `inputs`.\n "
        },
        {
            "sha": "080cf54ecc2dfd5a0d5efd4ec9dad94047d36940",
            "filename": "src/transformers/trainer_utils.py",
            "status": "modified",
            "additions": 17,
            "deletions": 17,
            "changes": 34,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftrainer_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftrainer_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_utils.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -27,7 +27,7 @@\n import time\n from collections.abc import Callable\n from functools import partial\n-from typing import Any, NamedTuple, Optional, Union\n+from typing import Any, NamedTuple\n \n import numpy as np\n \n@@ -159,10 +159,10 @@ class EvalPrediction:\n \n     def __init__(\n         self,\n-        predictions: Union[np.ndarray, tuple[np.ndarray]],\n-        label_ids: Union[np.ndarray, tuple[np.ndarray]],\n-        inputs: Optional[Union[np.ndarray, tuple[np.ndarray]]] = None,\n-        losses: Optional[Union[np.ndarray, tuple[np.ndarray]]] = None,\n+        predictions: np.ndarray | tuple[np.ndarray],\n+        label_ids: np.ndarray | tuple[np.ndarray],\n+        inputs: np.ndarray | tuple[np.ndarray] | None = None,\n+        losses: np.ndarray | tuple[np.ndarray] | None = None,\n     ):\n         self.predictions = predictions\n         self.label_ids = label_ids\n@@ -184,16 +184,16 @@ def __getitem__(self, idx):\n \n \n class EvalLoopOutput(NamedTuple):\n-    predictions: Union[np.ndarray, tuple[np.ndarray]]\n-    label_ids: Optional[Union[np.ndarray, tuple[np.ndarray]]]\n-    metrics: Optional[dict[str, float]]\n-    num_samples: Optional[int]\n+    predictions: np.ndarray | tuple[np.ndarray]\n+    label_ids: np.ndarray | tuple[np.ndarray] | None\n+    metrics: dict[str, float] | None\n+    num_samples: int | None\n \n \n class PredictionOutput(NamedTuple):\n-    predictions: Union[np.ndarray, tuple[np.ndarray]]\n-    label_ids: Optional[Union[np.ndarray, tuple[np.ndarray]]]\n-    metrics: Optional[dict[str, float]]\n+    predictions: np.ndarray | tuple[np.ndarray]\n+    label_ids: np.ndarray | tuple[np.ndarray] | None\n+    metrics: dict[str, float] | None\n \n \n class TrainOutput(NamedTuple):\n@@ -255,9 +255,9 @@ class BestRun(NamedTuple):\n     \"\"\"\n \n     run_id: str\n-    objective: Union[float, list[float]]\n+    objective: float | list[float]\n     hyperparameters: dict[str, Any]\n-    run_summary: Optional[Any] = None\n+    run_summary: Any | None = None\n \n \n def default_compute_objective(metrics: dict[str, float]) -> float:\n@@ -763,7 +763,7 @@ def number_of_arguments(func):\n \n \n def find_executable_batch_size(\n-    function: Optional[Callable] = None, starting_batch_size: int = 128, auto_find_batch_size: bool = False\n+    function: Callable | None = None, starting_batch_size: int = 128, auto_find_batch_size: bool = False\n ):\n     \"\"\"\n     Args:\n@@ -811,8 +811,8 @@ def __init__(\n         data_collator,\n         signature_columns,\n         logger=None,\n-        model_name: Optional[str] = None,\n-        description: Optional[str] = None,\n+        model_name: str | None = None,\n+        description: str | None = None,\n     ):\n         self.data_collator = data_collator\n         self.signature_columns = signature_columns"
        },
        {
            "sha": "ede0a922cdb61effad02cd65de4015794c3a2085",
            "filename": "src/transformers/training_args.py",
            "status": "modified",
            "additions": 66,
            "deletions": 66,
            "changes": 132,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftraining_args.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftraining_args.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -21,7 +21,7 @@\n from datetime import timedelta\n from enum import Enum\n from functools import cached_property\n-from typing import Any, Optional, Union\n+from typing import Any\n \n from .debug_utils import DebugOption\n from .trainer_utils import (\n@@ -116,7 +116,7 @@ def get_int_from_env(env_keys, default):\n     return default\n \n \n-def get_xla_device_type(device: \"torch.device\") -> Optional[str]:\n+def get_xla_device_type(device: \"torch.device\") -> str | None:\n     \"\"\"\n     Returns the xla device type (CPU|GPU|TPU) or None if the device is a non-xla device.\n     \"\"\"\n@@ -771,7 +771,7 @@ class TrainingArguments:\n         \"lr_scheduler_kwargs\",\n     ]\n \n-    output_dir: Optional[str] = field(\n+    output_dir: str | None = field(\n         default=None,\n         metadata={\n             \"help\": \"The output directory where the model predictions and checkpoints will be written. Defaults to 'trainer_output' if not provided.\"\n@@ -781,7 +781,7 @@ class TrainingArguments:\n     do_train: bool = field(default=False, metadata={\"help\": \"Whether to run training.\"})\n     do_eval: bool = field(default=False, metadata={\"help\": \"Whether to run eval on the dev set.\"})\n     do_predict: bool = field(default=False, metadata={\"help\": \"Whether to run predictions on the test set.\"})\n-    eval_strategy: Union[IntervalStrategy, str] = field(\n+    eval_strategy: IntervalStrategy | str = field(\n         default=\"no\",\n         metadata={\"help\": \"The evaluation strategy to use.\"},\n     )\n@@ -801,7 +801,7 @@ class TrainingArguments:\n         default=1,\n         metadata={\"help\": \"Number of updates steps to accumulate before performing a backward/update pass.\"},\n     )\n-    eval_accumulation_steps: Optional[int] = field(\n+    eval_accumulation_steps: int | None = field(\n         default=None,\n         metadata={\"help\": \"Number of predictions steps to accumulate before moving the tensors to the CPU.\"},\n     )\n@@ -816,7 +816,7 @@ class TrainingArguments:\n         },\n     )\n \n-    torch_empty_cache_steps: Optional[int] = field(\n+    torch_empty_cache_steps: int | None = field(\n         default=None,\n         metadata={\n             \"help\": \"Number of steps to wait before calling `torch.<device>.empty_cache()`.\"\n@@ -837,19 +837,19 @@ class TrainingArguments:\n         default=-1,\n         metadata={\"help\": \"If > 0: set total number of training steps to perform. Override num_train_epochs.\"},\n     )\n-    lr_scheduler_type: Union[SchedulerType, str] = field(\n+    lr_scheduler_type: SchedulerType | str = field(\n         default=\"linear\",\n         metadata={\"help\": \"The scheduler type to use.\"},\n     )\n-    lr_scheduler_kwargs: Optional[Union[dict, str]] = field(\n+    lr_scheduler_kwargs: dict | str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n                 \"Extra parameters for the lr_scheduler such as {'num_cycles': 1} for the cosine with hard restarts.\"\n             )\n         },\n     )\n-    warmup_ratio: Optional[float] = field(\n+    warmup_ratio: float | None = field(\n         default=None,\n         metadata={\n             \"help\": \"This argument is deprecated and will be removed in v5. Use `warmup_steps` instead as it also works with float values.\"\n@@ -885,13 +885,13 @@ class TrainingArguments:\n             )\n         },\n     )\n-    logging_dir: Optional[str] = field(\n+    logging_dir: str | None = field(\n         default=None,\n         metadata={\n             \"help\": \"Deprecated and will be removed in v5.2. Set env var `TENSORBOARD_LOGGING_DIR` instead. TensorBoard log directory.\"\n         },\n     )\n-    logging_strategy: Union[IntervalStrategy, str] = field(\n+    logging_strategy: IntervalStrategy | str = field(\n         default=\"steps\",\n         metadata={\"help\": \"The logging strategy to use.\"},\n     )\n@@ -906,7 +906,7 @@ class TrainingArguments:\n         },\n     )\n     logging_nan_inf_filter: bool = field(default=True, metadata={\"help\": \"Filter nan and inf losses for logging.\"})\n-    save_strategy: Union[SaveStrategy, str] = field(\n+    save_strategy: SaveStrategy | str = field(\n         default=\"steps\",\n         metadata={\"help\": \"The checkpoint save strategy to use.\"},\n     )\n@@ -919,7 +919,7 @@ class TrainingArguments:\n             )\n         },\n     )\n-    save_total_limit: Optional[int] = field(\n+    save_total_limit: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -972,7 +972,7 @@ class TrainingArguments:\n         },\n     )\n     seed: int = field(default=42, metadata={\"help\": \"Random seed that will be set at the beginning of training.\"})\n-    data_seed: Optional[int] = field(default=None, metadata={\"help\": \"Random seed to be used with data samplers.\"})\n+    data_seed: int | None = field(default=None, metadata={\"help\": \"Random seed to be used with data samplers.\"})\n     bf16: bool = field(\n         default=False,\n         metadata={\n@@ -1000,7 +1000,7 @@ class TrainingArguments:\n         default=False,\n         metadata={\"help\": \"Whether to use full float16 evaluation instead of 32-bit\"},\n     )\n-    tf32: Optional[bool] = field(\n+    tf32: bool | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -1015,14 +1015,14 @@ class TrainingArguments:\n             \"help\": \"When using torch.distributed.launch (Deprecated), it will pass `local_rank` in the script, so we need this for the parser. To get the local rank, prefer using the property `local_process_index`\"\n         },\n     )\n-    ddp_backend: Optional[str] = field(\n+    ddp_backend: str | None = field(\n         default=None,\n         metadata={\n             \"help\": \"The backend to be used for distributed training\",\n             \"choices\": [\"nccl\", \"gloo\", \"mpi\", \"ccl\", \"hccl\", \"cncl\", \"mccl\"],\n         },\n     )\n-    debug: Union[str, list[DebugOption]] = field(\n+    debug: str | list[DebugOption] = field(\n         default=\"\",\n         metadata={\n             \"help\": (\n@@ -1036,7 +1036,7 @@ class TrainingArguments:\n     dataloader_drop_last: bool = field(\n         default=False, metadata={\"help\": \"Drop the last incomplete batch if it is not divisible by the batch size.\"}\n     )\n-    eval_steps: Optional[float] = field(\n+    eval_steps: float | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -1054,7 +1054,7 @@ class TrainingArguments:\n             )\n         },\n     )\n-    dataloader_prefetch_factor: Optional[int] = field(\n+    dataloader_prefetch_factor: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -1064,7 +1064,7 @@ class TrainingArguments:\n         },\n     )\n \n-    run_name: Optional[str] = field(\n+    run_name: str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -1073,14 +1073,14 @@ class TrainingArguments:\n             )\n         },\n     )\n-    disable_tqdm: Optional[bool] = field(\n+    disable_tqdm: bool | None = field(\n         default=None, metadata={\"help\": \"Whether or not to disable the tqdm progress bars.\"}\n     )\n \n     remove_unused_columns: bool = field(\n         default=True, metadata={\"help\": \"Remove columns not required by the model when using an nlp.Dataset.\"}\n     )\n-    label_names: Optional[list[str]] = field(\n+    label_names: list[str] | None = field(\n         default=None, metadata={\"help\": \"The list of keys in your dictionary of inputs that correspond to the labels.\"}\n     )\n     load_best_model_at_end: bool = field(\n@@ -1092,10 +1092,10 @@ class TrainingArguments:\n             )\n         },\n     )\n-    metric_for_best_model: Optional[str] = field(\n+    metric_for_best_model: str | None = field(\n         default=None, metadata={\"help\": \"The metric to use to compare two different models.\"}\n     )\n-    greater_is_better: Optional[bool] = field(\n+    greater_is_better: bool | None = field(\n         default=None, metadata={\"help\": \"Whether the `metric_for_best_model` should be maximized or not.\"}\n     )\n     ignore_data_skip: bool = field(\n@@ -1107,7 +1107,7 @@ class TrainingArguments:\n             )\n         },\n     )\n-    fsdp: Optional[Union[list[FSDPOption], str]] = field(\n+    fsdp: list[FSDPOption] | str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -1119,7 +1119,7 @@ class TrainingArguments:\n             ),\n         },\n     )\n-    fsdp_config: Optional[Union[dict[str, Any], str]] = field(\n+    fsdp_config: dict[str, Any] | str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -1128,7 +1128,7 @@ class TrainingArguments:\n             )\n         },\n     )\n-    accelerator_config: Optional[Union[dict, str]] = field(\n+    accelerator_config: dict | str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -1137,11 +1137,11 @@ class TrainingArguments:\n             )\n         },\n     )\n-    parallelism_config: Optional[ParallelismConfig] = field(\n+    parallelism_config: ParallelismConfig | None = field(\n         default=None,\n         metadata={\"help\": (\"Parallelism configuration for the training run. Requires Accelerate `1.12.0`\")},\n     )\n-    deepspeed: Optional[Union[dict, str]] = field(\n+    deepspeed: dict | str | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -1160,11 +1160,11 @@ class TrainingArguments:\n \n         if is_torch_greater_or_equal_than_2_8:\n             default_optim = \"adamw_torch_fused\"\n-    optim: Union[OptimizerNames, str] = field(\n+    optim: OptimizerNames | str = field(\n         default=default_optim,\n         metadata={\"help\": \"The optimizer to use.\"},\n     )\n-    optim_args: Optional[str] = field(default=None, metadata={\"help\": \"Optional arguments to supply to optimizer.\"})\n+    optim_args: str | None = field(default=None, metadata={\"help\": \"Optional arguments to supply to optimizer.\"})\n     group_by_length: bool = field(\n         default=False,\n         metadata={\"help\": \"Whether or not to group samples of roughly the same length together when batching.\"},\n@@ -1173,14 +1173,14 @@ class TrainingArguments:\n         default=\"length\",\n         metadata={\"help\": \"Column name with precomputed lengths to use when grouping by length.\"},\n     )\n-    report_to: Union[None, str, list[str]] = field(\n+    report_to: None | str | list[str] = field(\n         default=\"none\", metadata={\"help\": \"The list of integrations to report the results and logs to.\"}\n     )\n     project: str = field(\n         default=\"huggingface\",\n         metadata={\"help\": \"The name of the project to use for logging. Currenly, only used by Trackio.\"},\n     )\n-    trackio_space_id: Optional[str] = field(\n+    trackio_space_id: str | None = field(\n         default=\"trackio\",\n         metadata={\n             \"help\": \"The Hugging Face Space ID to deploy to when using Trackio. Should be a complete Space name like \"\n@@ -1190,7 +1190,7 @@ class TrainingArguments:\n             \"default is to create private Spaces.\"\n         },\n     )\n-    ddp_find_unused_parameters: Optional[bool] = field(\n+    ddp_find_unused_parameters: bool | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -1199,7 +1199,7 @@ class TrainingArguments:\n             )\n         },\n     )\n-    ddp_bucket_cap_mb: Optional[int] = field(\n+    ddp_bucket_cap_mb: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -1208,7 +1208,7 @@ class TrainingArguments:\n             )\n         },\n     )\n-    ddp_broadcast_buffers: Optional[bool] = field(\n+    ddp_broadcast_buffers: bool | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -1232,19 +1232,19 @@ class TrainingArguments:\n     push_to_hub: bool = field(\n         default=False, metadata={\"help\": \"Whether or not to upload the trained model to the model hub after training.\"}\n     )\n-    resume_from_checkpoint: Optional[str] = field(\n+    resume_from_checkpoint: str | None = field(\n         default=None,\n         metadata={\"help\": \"The path to a folder with a valid checkpoint for your model.\"},\n     )\n-    hub_model_id: Optional[str] = field(\n+    hub_model_id: str | None = field(\n         default=None, metadata={\"help\": \"The name of the repository to keep in sync with the local `output_dir`.\"}\n     )\n-    hub_strategy: Union[HubStrategy, str] = field(\n+    hub_strategy: HubStrategy | str = field(\n         default=\"every_save\",\n         metadata={\"help\": \"The hub strategy to use when `--push_to_hub` is activated.\"},\n     )\n-    hub_token: Optional[str] = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n-    hub_private_repo: Optional[bool] = field(\n+    hub_token: str | None = field(default=None, metadata={\"help\": \"The token to use to push to the Model Hub.\"})\n+    hub_private_repo: bool | None = field(\n         default=None,\n         metadata={\n             \"help\": \"Whether to make the repo private. If `None` (default), the repo will be public unless the \"\n@@ -1257,7 +1257,7 @@ class TrainingArguments:\n         default=False,\n         metadata={\"help\": \"Unless `True`, the Trainer will skip pushes if the previous one wasn't finished yet.\"},\n     )\n-    hub_revision: Optional[str] = field(\n+    hub_revision: str | None = field(\n         default=None,\n         metadata={\n             \"help\": \"The revision to use when pushing to the Hub. Can be a branch name, a tag, or a commit hash.\"\n@@ -1269,7 +1269,7 @@ class TrainingArguments:\n             \"help\": \"If True, use gradient checkpointing to save memory at the expense of slower backward pass.\"\n         },\n     )\n-    gradient_checkpointing_kwargs: Optional[Union[dict[str, Any], str]] = field(\n+    gradient_checkpointing_kwargs: dict[str, Any] | str | None = field(\n         default=None,\n         metadata={\n             \"help\": \"Gradient checkpointing key word arguments such as `use_reentrant`. Will be passed to `torch.utils.checkpoint.checkpoint` through `model.gradient_checkpointing_enable`.\"\n@@ -1315,19 +1315,19 @@ class TrainingArguments:\n     torch_compile: bool = field(\n         default=False, metadata={\"help\": \"If set to `True`, the model will be wrapped in `torch.compile`.\"}\n     )\n-    torch_compile_backend: Optional[str] = field(\n+    torch_compile_backend: str | None = field(\n         default=None,\n         metadata={\n             \"help\": \"Which backend to use with `torch.compile`, passing one will trigger a model compilation.\",\n         },\n     )\n-    torch_compile_mode: Optional[str] = field(\n+    torch_compile_mode: str | None = field(\n         default=None,\n         metadata={\n             \"help\": \"Which mode to use with `torch.compile`, passing one will trigger a model compilation.\",\n         },\n     )\n-    include_num_input_tokens_seen: Union[str, bool] = field(\n+    include_num_input_tokens_seen: str | bool = field(\n         default=\"no\",\n         metadata={\n             \"help\": (\n@@ -1337,14 +1337,14 @@ class TrainingArguments:\n         },\n     )\n \n-    neftune_noise_alpha: Optional[float] = field(\n+    neftune_noise_alpha: float | None = field(\n         default=None,\n         metadata={\n             \"help\": \"Activates neftune noise embeddings into the model. NEFTune has been proven to drastically improve model performances for instruction fine-tuning. Check out the original paper here: https://huggingface.co/papers/2310.05914 and the original code here: https://github.com/neelsjain/NEFTune. Only supported for `PreTrainedModel` and `PeftModel` classes.\"\n         },\n     )\n \n-    optim_target_modules: Union[None, str, list[str]] = field(\n+    optim_target_modules: None | str | list[str] = field(\n         default=None,\n         metadata={\n             \"help\": \"Target modules for the optimizer defined in the `optim` argument. Only used for the GaLore optimizer at the moment.\"\n@@ -1368,7 +1368,7 @@ class TrainingArguments:\n         metadata={\"help\": \"Whether or not to enable the Liger Kernel for model training.\"},\n     )\n \n-    liger_kernel_config: Optional[dict[str, bool]] = field(\n+    liger_kernel_config: dict[str, bool] | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -2200,11 +2200,11 @@ def set_training(\n \n     def set_evaluate(\n         self,\n-        strategy: Union[str, IntervalStrategy] = \"no\",\n+        strategy: str | IntervalStrategy = \"no\",\n         steps: int = 500,\n         batch_size: int = 8,\n-        accumulation_steps: Optional[int] = None,\n-        delay: Optional[float] = None,\n+        accumulation_steps: int | None = None,\n+        delay: float | None = None,\n         loss_only: bool = False,\n     ):\n         \"\"\"\n@@ -2293,9 +2293,9 @@ def set_testing(\n \n     def set_save(\n         self,\n-        strategy: Union[str, IntervalStrategy] = \"steps\",\n+        strategy: str | IntervalStrategy = \"steps\",\n         steps: int = 500,\n-        total_limit: Optional[int] = None,\n+        total_limit: int | None = None,\n         on_each_node: bool = False,\n     ):\n         \"\"\"\n@@ -2342,9 +2342,9 @@ def set_save(\n \n     def set_logging(\n         self,\n-        strategy: Union[str, IntervalStrategy] = \"steps\",\n+        strategy: str | IntervalStrategy = \"steps\",\n         steps: int = 500,\n-        report_to: Union[str, list[str]] = \"none\",\n+        report_to: str | list[str] = \"none\",\n         level: str = \"passive\",\n         first_step: bool = False,\n         nan_inf_filter: bool = False,\n@@ -2418,11 +2418,11 @@ def set_logging(\n     def set_push_to_hub(\n         self,\n         model_id: str,\n-        strategy: Union[str, HubStrategy] = \"every_save\",\n-        token: Optional[str] = None,\n-        private_repo: Optional[bool] = None,\n+        strategy: str | HubStrategy = \"every_save\",\n+        token: str | None = None,\n+        private_repo: bool | None = None,\n         always_push: bool = False,\n-        revision: Optional[str] = None,\n+        revision: str | None = None,\n     ):\n         \"\"\"\n         A method that regroups all arguments linked to synchronizing checkpoints with the Hub.\n@@ -2491,13 +2491,13 @@ def set_push_to_hub(\n \n     def set_optimizer(\n         self,\n-        name: Union[str, OptimizerNames] = \"adamw_torch\",\n+        name: str | OptimizerNames = \"adamw_torch\",\n         learning_rate: float = 5e-5,\n         weight_decay: float = 0,\n         beta1: float = 0.9,\n         beta2: float = 0.999,\n         epsilon: float = 1e-8,\n-        args: Optional[str] = None,\n+        args: str | None = None,\n     ):\n         \"\"\"\n         A method that regroups all arguments linked to the optimizer and its hyperparameters.\n@@ -2542,11 +2542,11 @@ def set_optimizer(\n \n     def set_lr_scheduler(\n         self,\n-        name: Union[str, SchedulerType] = \"linear\",\n+        name: str | SchedulerType = \"linear\",\n         num_epochs: float = 3.0,\n         max_steps: int = -1,\n         warmup_steps: float = 0,\n-        warmup_ratio: Optional[float] = None,\n+        warmup_ratio: float | None = None,\n     ):\n         \"\"\"\n         A method that regroups all arguments linked to the learning rate scheduler and its hyperparameters.\n@@ -2594,10 +2594,10 @@ def set_dataloader(\n         num_workers: int = 0,\n         pin_memory: bool = True,\n         persistent_workers: bool = False,\n-        prefetch_factor: Optional[int] = None,\n+        prefetch_factor: int | None = None,\n         auto_find_batch_size: bool = False,\n         ignore_data_skip: bool = False,\n-        sampler_seed: Optional[int] = None,\n+        sampler_seed: int | None = None,\n     ):\n         \"\"\"\n         A method that regroups all arguments linked to the dataloaders creation."
        },
        {
            "sha": "521cda69c0bec00bc69b122292612690a913ed99",
            "filename": "src/transformers/training_args_seq2seq.py",
            "status": "modified",
            "additions": 3,
            "deletions": 4,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftraining_args_seq2seq.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/src%2Ftransformers%2Ftraining_args_seq2seq.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftraining_args_seq2seq.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -15,7 +15,6 @@\n import logging\n from dataclasses import dataclass, field\n from pathlib import Path\n-from typing import Optional, Union\n \n from .generation.configuration_utils import GenerationConfig\n from .training_args import TrainingArguments\n@@ -52,7 +51,7 @@ class Seq2SeqTrainingArguments(TrainingArguments):\n     predict_with_generate: bool = field(\n         default=False, metadata={\"help\": \"Whether to use generate to calculate generative metrics (ROUGE, BLEU).\"}\n     )\n-    generation_max_length: Optional[int] = field(\n+    generation_max_length: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -61,7 +60,7 @@ class Seq2SeqTrainingArguments(TrainingArguments):\n             )\n         },\n     )\n-    generation_num_beams: Optional[int] = field(\n+    generation_num_beams: int | None = field(\n         default=None,\n         metadata={\n             \"help\": (\n@@ -70,7 +69,7 @@ class Seq2SeqTrainingArguments(TrainingArguments):\n             )\n         },\n     )\n-    generation_config: Optional[Union[str, Path, GenerationConfig]] = field(\n+    generation_config: str | Path | GenerationConfig | None = field(\n         default=None,\n         metadata={\n             \"help\": \"Model id, file path or url pointing to a GenerationConfig json file, to use during prediction.\""
        },
        {
            "sha": "6eaef3116a22af3f0217add0c2c04e028eb836b4",
            "filename": "tests/utils/test_chat_template_utils.py",
            "status": "modified",
            "additions": 6,
            "deletions": 8,
            "changes": 14,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/tests%2Futils%2Ftest_chat_template_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/tests%2Futils%2Ftest_chat_template_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Futils%2Ftest_chat_template_utils.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -13,7 +13,7 @@\n # limitations under the License.\n \n import unittest\n-from typing import Literal, Optional, Union\n+from typing import Literal\n \n from transformers.utils import DocstringParsingException, TypeHintParsingException, get_json_schema\n \n@@ -57,7 +57,7 @@ def fn():\n         self.assertEqual(schema[\"function\"], expected_schema)\n \n     def test_union(self):\n-        def fn(x: Union[int, float]):\n+        def fn(x: int | float):\n             \"\"\"\n             Test function\n \n@@ -79,7 +79,7 @@ def fn(x: Union[int, float]):\n         self.assertEqual(schema[\"function\"], expected_schema)\n \n     def test_optional(self):\n-        def fn(x: Optional[int]):\n+        def fn(x: int | None):\n             \"\"\"\n             Test function\n \n@@ -119,7 +119,7 @@ def fn(x: int = 42):\n         self.assertEqual(schema[\"function\"], expected_schema)\n \n     def test_nested_list(self):\n-        def fn(x: list[list[Union[str, int]]]):\n+        def fn(x: list[list[str | int]]):\n             \"\"\"\n             Test function\n \n@@ -173,7 +173,7 @@ def fn(x: int, y: str):\n         self.assertEqual(schema[\"function\"], expected_schema)\n \n     def test_multiple_complex_arguments(self):\n-        def fn(x: list[Union[int, float]], y: Optional[Union[int, str]] = None):\n+        def fn(x: list[int | float], y: int | str | None = None):\n             \"\"\"\n             Test function\n \n@@ -488,9 +488,7 @@ def fn(x: int) -> None:\n         self.assertEqual(schema[\"function\"], expected_schema)\n \n     def test_everything_all_at_once(self):\n-        def fn(\n-            x: str, y: Optional[list[Union[str, int]]], z: tuple[Union[str, int], str] = (42, \"hello\")\n-        ) -> tuple[int, str]:\n+        def fn(x: str, y: list[str | int] | None, z: tuple[str | int, str] = (42, \"hello\")) -> tuple[int, str]:\n             \"\"\"\n             Test function with multiple args, and docstring args that we have to strip out.\n "
        },
        {
            "sha": "0f3a976c32d3bfda10add3e48421d77c256bf684",
            "filename": "utils/check_docstrings.py",
            "status": "modified",
            "additions": 4,
            "deletions": 4,
            "changes": 8,
            "blob_url": "https://github.com/huggingface/transformers/blob/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/utils%2Fcheck_docstrings.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/1ce12ac5de2d5d082825c7fab9d03ab5d110e400/utils%2Fcheck_docstrings.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/utils%2Fcheck_docstrings.py?ref=1ce12ac5de2d5d082825c7fab9d03ab5d110e400",
            "patch": "@@ -44,7 +44,7 @@\n from collections import OrderedDict\n from dataclasses import dataclass\n from pathlib import Path\n-from typing import Any, Optional, Union\n+from typing import Any\n \n from check_repo import ignore_undocumented\n from git import Repo\n@@ -71,11 +71,11 @@ class DecoratedItem:\n         int  # 1-based line number where body starts (for functions) or __init__ body start (for classes with __init__)\n     )\n     args: list[str]  # List of argument names (excluding self, *args, **kwargs) - for classes, these are __init__ args\n-    custom_args_text: Optional[str] = None  # custom_args string if provided in decorator\n+    custom_args_text: str | None = None  # custom_args string if provided in decorator\n \n     # Class-specific fields (only populated when kind == 'class')\n     has_init: bool = False  # Whether the class has an __init__ method\n-    init_def_line: Optional[int] = None  # 1-based line number of __init__ def (if has_init)\n+    init_def_line: int | None = None  # 1-based line number of __init__ def (if has_init)\n     is_model_output: bool = False  # Whether the class inherits from ModelOutput\n \n \n@@ -913,7 +913,7 @@ def _is_auto_docstring_decorator(dec):\n     return isinstance(target, ast.Name) and target.id == \"auto_docstring\"\n \n \n-def _extract_function_args(func_node: Union[ast.FunctionDef, ast.AsyncFunctionDef]) -> list[str]:\n+def _extract_function_args(func_node: ast.FunctionDef | ast.AsyncFunctionDef) -> list[str]:\n     \"\"\"Extract argument names from a function node, excluding 'self', *args, **kwargs.\"\"\"\n     all_args = (func_node.args.posonlyargs or []) + func_node.args.args + func_node.args.kwonlyargs\n     return [a.arg for a in all_args if a.arg != \"self\"]"
        }
    ],
    "stats": {
        "total": 925,
        "additions": 451,
        "deletions": 474
    }
}