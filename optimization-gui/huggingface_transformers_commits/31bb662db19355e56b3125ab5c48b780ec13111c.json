{
    "author": "SunMarc",
    "message": "Fix callback handler reference (#36250)\n\n* fix reference\n\n* style",
    "sha": "31bb662db19355e56b3125ab5c48b780ec13111c",
    "files": [
        {
            "sha": "4d958d30214abb5783906f8bcedbc19158277a27",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 5,
            "deletions": 1,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/31bb662db19355e56b3125ab5c48b780ec13111c/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31bb662db19355e56b3125ab5c48b780ec13111c/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=31bb662db19355e56b3125ab5c48b780ec13111c",
            "patch": "@@ -2445,7 +2445,11 @@ def _inner_training_loop(\n                 )\n \n         # Update the references\n-        self.state.init_training_references(self, train_dataloader, max_steps, num_train_epochs, trial)\n+        for attr in (\"model\", \"optimizer\", \"lr_scheduler\"):\n+            setattr(self.callback_handler, attr, getattr(self, attr))\n+        self.callback_handler.train_dataloader = train_dataloader\n+\n+        self.state.init_training_references(self, max_steps, num_train_epochs, trial)\n \n         # tr_loss is a tensor to avoid synchronization of TPUs through .item()\n         tr_loss = torch.tensor(0.0).to(args.device)"
        },
        {
            "sha": "45dfcee71bc2cda71c6b132a45420cab9e3d68f2",
            "filename": "src/transformers/trainer_callback.py",
            "status": "modified",
            "additions": 1,
            "deletions": 5,
            "changes": 6,
            "blob_url": "https://github.com/huggingface/transformers/blob/31bb662db19355e56b3125ab5c48b780ec13111c/src%2Ftransformers%2Ftrainer_callback.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/31bb662db19355e56b3125ab5c48b780ec13111c/src%2Ftransformers%2Ftrainer_callback.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer_callback.py?ref=31bb662db19355e56b3125ab5c48b780ec13111c",
            "patch": "@@ -164,14 +164,10 @@ def compute_steps(self, args, max_steps):\n                     num_steps = math.ceil(max_steps * num_steps)\n                 setattr(self, f\"{step_kind}_steps\", num_steps)\n \n-    def init_training_references(self, trainer, train_dataloader, max_steps, num_train_epochs, trial):\n+    def init_training_references(self, trainer, max_steps, num_train_epochs, trial):\n         \"\"\"\n         Stores the initial training references needed in `self`\n         \"\"\"\n-        for attr in (\"model\", \"optimizer\", \"lr_scheduler\"):\n-            setattr(self, attr, getattr(trainer, attr))\n-\n-        self.train_dataloader = train_dataloader\n         if trainer.hp_name is not None and trainer._trial is not None:\n             # use self._trial because the SigOpt/Optuna hpo only call `_hp_search_setup(trial)` instead of passing trial\n             # parameter to Train when using DDP."
        }
    ],
    "stats": {
        "total": 12,
        "additions": 6,
        "deletions": 6
    }
}