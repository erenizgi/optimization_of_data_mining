{
    "author": "ArthurZucker",
    "message": "Add attention visualization tool  (#36630)\n\n* add utils  fiel\n\n* style\n\n* nits\n\n* nits\n\n* update\n\n* updaets\n\n* update\n\n* fix init issues\n\n* big updates\n\n* nits\n\n* nits?\n\n* small updates\n\n* nites\n\n* there were still some models left\n\n* style\n\n* fixes\n\n* updates\n\n* nits _ fixes\n\n* push changes\n\n* update\n\n* update\n\n* update\n\n* Apply suggestions from code review\n\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>\n\n* style\n\n* styling and return a string for testing\n\n* small updates\n\n* always biderectional for now\n\n* update\n\n---------\n\nCo-authored-by: Pablo Montalvo <39954772+molbap@users.noreply.github.com>",
    "sha": "fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
    "files": [
        {
            "sha": "e44cd709dfcff5031d2225f135b048b98f38893c",
            "filename": "src/transformers/models/aria/modeling_aria.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Faria%2Fmodeling_aria.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1015,7 +1015,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "c5783a3b13b8a18cb482dadd809addce2fca0ea9",
            "filename": "src/transformers/models/bloom/modeling_bloom.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fbloom%2Fmodeling_bloom.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -746,7 +746,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "30a8ab60f206882fbd72c4e81d52adfeed46ea24",
            "filename": "src/transformers/models/chameleon/modeling_chameleon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fchameleon%2Fmodeling_chameleon.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1390,7 +1390,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "1d7d3b6d27cd6fd6479ab063bb9dd91a7dc818d1",
            "filename": "src/transformers/models/codegen/modeling_codegen.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcodegen%2Fmodeling_codegen.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -592,7 +592,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "01b5ee3b9e7ab04739317ffe9b368dbe7fdbb2b6",
            "filename": "src/transformers/models/cohere/modeling_cohere.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fcohere%2Fmodeling_cohere.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -665,7 +665,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "08f11ed6ffd40b8ff9d91f2eca5e134bc829a0c0",
            "filename": "src/transformers/models/dbrx/modeling_dbrx.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdbrx%2Fmodeling_dbrx.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1119,7 +1119,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "d69dcf3b59df4624c72dac059966ff38b48bc654",
            "filename": "src/transformers/models/diffllama/modeling_diffllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fdiffllama%2Fmodeling_diffllama.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -904,7 +904,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "47360222fce344a089ee8c572520a777cf18aa5d",
            "filename": "src/transformers/models/emu3/modeling_emu3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Femu3%2Fmodeling_emu3.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1483,7 +1483,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "807abe2b9f47ebbe3ef21f24bb1cd6931f018af2",
            "filename": "src/transformers/models/gemma/modeling_gemma.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma%2Fmodeling_gemma.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -637,7 +637,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "c5492f3b768599bac30b4ee4523a76b669b9b545",
            "filename": "src/transformers/models/gemma3/processing_gemma3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 0,
            "changes": 1,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgemma3%2Fprocessing_gemma3.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -65,6 +65,7 @@ def __init__(\n         self.image_seq_length = image_seq_length\n         self.image_token_id = tokenizer.image_token_id\n         self.boi_token = tokenizer.boi_token\n+        self.image_token = tokenizer.boi_token\n         image_tokens_expanded = \"\".join([tokenizer.image_token] * image_seq_length)\n         self.full_image_sequence = f\"\\n\\n{tokenizer.boi_token}{image_tokens_expanded}{tokenizer.eoi_token}\\n\\n\"\n "
        },
        {
            "sha": "d155beff77132ff4f18b0e39d62a7ad999bb2e71",
            "filename": "src/transformers/models/glm/modeling_glm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fglm%2Fmodeling_glm.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -646,7 +646,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "13ff3cd740dfaf7451c1e75f7692a08c10df6d56",
            "filename": "src/transformers/models/gpt_neo/modeling_gpt_neo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neo%2Fmodeling_gpt_neo.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -796,7 +796,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "97590faf705fb3b03a210ba7aa0afcf9949360a2",
            "filename": "src/transformers/models/gpt_neox/modeling_gpt_neox.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox%2Fmodeling_gpt_neox.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -640,7 +640,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "aab7183ccf96fddd70f5f219fb983544ea31ac35",
            "filename": "src/transformers/models/gpt_neox_japanese/modeling_gpt_neox_japanese.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgpt_neox_japanese%2Fmodeling_gpt_neox_japanese.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -668,7 +668,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "effaa3b72510bd735f13a87e5ef26f3161a02ac1",
            "filename": "src/transformers/models/gptj/modeling_gptj.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgptj%2Fmodeling_gptj.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -895,7 +895,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "e7a95c719f051e218330449a0f40622a5ed224b9",
            "filename": "src/transformers/models/granite/modeling_granite.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranite%2Fmodeling_granite.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -649,7 +649,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "192c99df7f576d8e951d124fb404790e04f7c267",
            "filename": "src/transformers/models/granitemoe/modeling_granitemoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoe%2Fmodeling_granitemoe.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1122,7 +1122,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "d97620e62ba395d2f02b2523817bfe14358c24da",
            "filename": "src/transformers/models/granitemoeshared/modeling_granitemoeshared.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fgranitemoeshared%2Fmodeling_granitemoeshared.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1067,7 +1067,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "40aac3b933061f82dc9200eae1f1ff5bded99684",
            "filename": "src/transformers/models/helium/modeling_helium.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fhelium%2Fmodeling_helium.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -633,7 +633,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "da18b35d48529775d7c7ce81796ed9fc5cdb437d",
            "filename": "src/transformers/models/idefics/modeling_idefics.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fidefics%2Fmodeling_idefics.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1367,7 +1367,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "9518ef0be5d3cd1254a79e6d5630e5d7a95b8014",
            "filename": "src/transformers/models/jetmoe/modeling_jetmoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fjetmoe%2Fmodeling_jetmoe.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1128,7 +1128,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "e52d4087be1871d24d3e2f2f61d4859ad078051b",
            "filename": "src/transformers/models/llama/modeling_llama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fllama%2Fmodeling_llama.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -635,7 +635,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "70ec2db49f5aed821a97659f00a4982fbf2e40c3",
            "filename": "src/transformers/models/longt5/modeling_longt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Flongt5%2Fmodeling_longt5.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1604,7 +1604,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "4539cfd0bd3455b84acde36584956320681bade2",
            "filename": "src/transformers/models/mimi/modeling_mimi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmimi%2Fmodeling_mimi.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1066,7 +1066,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "07a3e7ed41786b151c117e61316dabaf25f0fa24",
            "filename": "src/transformers/models/mistral/modeling_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodeling_mistral.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -600,7 +600,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "20f0528627d52bf3bc3dd3afb2824fbed2ad086e",
            "filename": "src/transformers/models/mistral/modular_mistral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmistral%2Fmodular_mistral.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -118,7 +118,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "8fd3f7540ecd877a4bf2a6270dfaa04a9ee17e2c",
            "filename": "src/transformers/models/mixtral/modeling_mixtral.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmixtral%2Fmodeling_mixtral.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -734,7 +734,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "1981f4287bb8158743b54722711602e8584d3bb8",
            "filename": "src/transformers/models/mllama/modeling_mllama.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmllama%2Fmodeling_mllama.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1081,7 +1081,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "fc4770fd8f61aecc8de4e047577e512daba421a2",
            "filename": "src/transformers/models/moonshine/modeling_moonshine.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoonshine%2Fmodeling_moonshine.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -999,7 +999,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "3d686ba34d0d1b3681f39ada952482c9f347e41a",
            "filename": "src/transformers/models/moshi/modeling_moshi.py",
            "status": "modified",
            "additions": 2,
            "deletions": 2,
            "changes": 4,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmoshi%2Fmodeling_moshi.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1296,7 +1296,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:\n@@ -1610,7 +1610,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "558b671234928e73adbafffada3fa2e4586cbc14",
            "filename": "src/transformers/models/mt5/modeling_mt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fmt5%2Fmodeling_mt5.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1195,7 +1195,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "d6d77887a2a3ea894cf52121a833c0918d18f233",
            "filename": "src/transformers/models/nemotron/modeling_nemotron.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fnemotron%2Fmodeling_nemotron.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -883,7 +883,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "626b248e2c020455909eda9008ae253ba353f164",
            "filename": "src/transformers/models/olmo/modeling_olmo.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo%2Fmodeling_olmo.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -611,7 +611,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "101f79750e13c3841259068115daacb6044b2ccf",
            "filename": "src/transformers/models/olmo2/modeling_olmo2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Folmo2%2Fmodeling_olmo2.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -612,7 +612,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "28134e141c9ffa3eede5a9a702d50894bcc0eb2c",
            "filename": "src/transformers/models/opt/modeling_opt.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fopt%2Fmodeling_opt.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -643,7 +643,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "5d8542c1d399c11d6cfe9f971eca0d0285507854",
            "filename": "src/transformers/models/paligemma/modeling_paligemma.py",
            "status": "modified",
            "additions": 11,
            "deletions": 6,
            "changes": 17,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpaligemma%2Fmodeling_paligemma.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -340,19 +340,22 @@ def get_decoder(self):\n     def _update_causal_mask(\n         self,\n         attention_mask,\n-        token_type_ids,\n-        past_key_values,\n-        cache_position,\n-        input_tensor,\n-        is_training: bool = False,\n+        token_type_ids=None,\n+        past_key_values=None,\n+        cache_position=None,\n+        input_tensor=None,\n+        is_training: bool = None,\n     ):\n         if self.config.text_config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and 0.0 in attention_mask:\n                 return attention_mask\n             return None\n-\n+        is_training = is_training if is_training is not None else self.training\n         using_static_cache = isinstance(past_key_values, StaticCache)\n         min_dtype = torch.finfo(self.dtype).min\n+        if input_tensor is None:\n+            input_tensor = attention_mask\n+\n         inputs_lead_dim, sequence_length = input_tensor.shape[:2]\n         if using_static_cache:\n             target_length = past_key_values.get_max_cache_shape()\n@@ -387,6 +390,8 @@ def _update_causal_mask(\n \n             # First unmask prefix tokens during training\n             if is_training:\n+                if token_type_ids is None:\n+                    raise ValueError(\"Token type ids must be provided during training\")\n                 causal_mask[:, :, :, :mask_length] = causal_mask[:, :, :, :mask_length].masked_fill(\n                     token_type_ids[:, None, None, :].to(causal_mask.device) == 0, 0\n                 )"
        },
        {
            "sha": "a71169dbdda915ed79bda77f2f8dcc8a2f31d230",
            "filename": "src/transformers/models/persimmon/modeling_persimmon.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpersimmon%2Fmodeling_persimmon.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -683,7 +683,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "c273f9062836d27eaa728d7679798a2be66d5fed",
            "filename": "src/transformers/models/phi/modeling_phi.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi%2Fmodeling_phi.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -609,7 +609,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "8b49ad087670a70729c6555010ba5d288b37d7f9",
            "filename": "src/transformers/models/phi3/modeling_phi3.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphi3%2Fmodeling_phi3.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -675,7 +675,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "66452fd943ac2a52f74de6a9f5250a1a5de98473",
            "filename": "src/transformers/models/phimoe/modeling_phimoe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fphimoe%2Fmodeling_phimoe.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1180,7 +1180,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "63c392db12f3e513b686513b7fcce62da626bb36",
            "filename": "src/transformers/models/pix2struct/modeling_pix2struct.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpix2struct%2Fmodeling_pix2struct.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1591,7 +1591,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "cadf71871eef10a815ad19b4210215ac6cc580f6",
            "filename": "src/transformers/models/pop2piano/modeling_pop2piano.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fpop2piano%2Fmodeling_pop2piano.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1004,7 +1004,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "d5d922e208e865e81d3a1aae4dd29608897f55a3",
            "filename": "src/transformers/models/qwen2/modeling_qwen2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2%2Fmodeling_qwen2.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -613,7 +613,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "fb0c8f5dbe0e605642815ca84ef149ae1022faef",
            "filename": "src/transformers/models/qwen2_5_vl/modeling_qwen2_5_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_5_vl%2Fmodeling_qwen2_5_vl.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1246,7 +1246,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "80e215854b4adc0ff10670fe338fe4afdf1776f6",
            "filename": "src/transformers/models/qwen2_moe/modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_moe%2Fmodeling_qwen2_moe.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1068,7 +1068,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "66b780f312d752efe6e9c9df9921598224b9d602",
            "filename": "src/transformers/models/qwen2_vl/modeling_qwen2_vl.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fqwen2_vl%2Fmodeling_qwen2_vl.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1192,7 +1192,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "2f59cda241f8818f58dea76a11e3940975a7b731",
            "filename": "src/transformers/models/stablelm/modeling_stablelm.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstablelm%2Fmodeling_stablelm.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -938,7 +938,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "a23dfa9d46ba24ec21a88179474f6d1a8366c6ec",
            "filename": "src/transformers/models/starcoder2/modeling_starcoder2.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fstarcoder2%2Fmodeling_starcoder2.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -596,7 +596,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and past_key_values is not None:"
        },
        {
            "sha": "73282a1509269c08cad2800229d42aa6a28f7d10",
            "filename": "src/transformers/models/switch_transformers/modeling_switch_transformers.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fswitch_transformers%2Fmodeling_switch_transformers.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1140,7 +1140,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "0851c0eac95f35191e69bae05bc6863e655696b1",
            "filename": "src/transformers/models/t5/modeling_t5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Ft5%2Fmodeling_t5.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1209,7 +1209,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "089434ca8273ec0702d4ab0b3b12b006ad1537d0",
            "filename": "src/transformers/models/udop/modeling_udop.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fudop%2Fmodeling_udop.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1542,7 +1542,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "07c44bef7b90444b67579ff89ef97c0b5688dba9",
            "filename": "src/transformers/models/umt5/modeling_umt5.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fumt5%2Fmodeling_umt5.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -852,7 +852,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "dacd147c151fb8fa71977b0a36b246eefa20e5d4",
            "filename": "src/transformers/models/whisper/modeling_whisper.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodels%2Fwhisper%2Fmodeling_whisper.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1378,7 +1378,7 @@ def _update_causal_mask(\n         input_tensor: torch.Tensor,\n         cache_position: torch.Tensor,\n         past_key_values: Cache,\n-        output_attentions: bool,\n+        output_attentions: bool = False,\n     ):\n         if self.config._attn_implementation == \"flash_attention_2\":\n             if attention_mask is not None and (attention_mask == 0.0).any():"
        },
        {
            "sha": "4b0c2ebdd49f8c13f811a0eae7cc28cd42489d90",
            "filename": "src/transformers/processing_utils.py",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fprocessing_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Fprocessing_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fprocessing_utils.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -1069,7 +1069,7 @@ def from_pretrained(\n \n         args = cls._get_arguments_from_pretrained(pretrained_model_name_or_path, **kwargs)\n         processor_dict, kwargs = cls.get_processor_dict(pretrained_model_name_or_path, **kwargs)\n-\n+        processor_dict.update({k: v for k, v in kwargs.items() if k in processor_dict.keys()})\n         return cls.from_args_and_dict(args, processor_dict, **kwargs)\n \n     @classmethod"
        },
        {
            "sha": "54efdbfcdafe0efddba5faa3713623c490e2709d",
            "filename": "src/transformers/utils/attention_visualizer.py",
            "status": "added",
            "additions": 229,
            "deletions": 0,
            "changes": 229,
            "blob_url": "https://github.com/huggingface/transformers/blob/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/fef8b7f8e9fd2800f5b4cb7483f9889bd437257e/src%2Ftransformers%2Futils%2Fattention_visualizer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fattention_visualizer.py?ref=fef8b7f8e9fd2800f5b4cb7483f9889bd437257e",
            "patch": "@@ -0,0 +1,229 @@\n+# coding=utf-8\n+# Copyright 2025 The HuggingFace Inc. team.\n+#\n+# Licensed under the Apache License, Version 2.0 (the \"License\");\n+# you may not use this file except in compliance with the License.\n+# You may obtain a copy of the License at\n+#\n+#     http://www.apache.org/licenses/LICENSE-2.0\n+#\n+# Unless required by applicable law or agreed to in writing, software\n+# distributed under the License is distributed on an \"AS IS\" BASIS,\n+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n+# See the License for the specific language governing permissions and\n+# limitations under the License.\n+\n+\n+import requests\n+from PIL import Image\n+\n+from ..models.auto.auto_factory import _get_model_class\n+from ..models.auto.configuration_auto import AutoConfig\n+from ..models.auto.modeling_auto import MODEL_FOR_PRETRAINING_MAPPING, MODEL_MAPPING\n+from ..models.auto.processing_auto import PROCESSOR_MAPPING_NAMES, AutoProcessor\n+from ..models.auto.tokenization_auto import TOKENIZER_MAPPING_NAMES, AutoTokenizer\n+from .import_utils import is_torch_available\n+\n+\n+if is_torch_available():\n+    import torch\n+    import torch.nn as nn\n+\n+# Print the matrix with words as row labels\n+GREEN = \"\\033[92m\"\n+YELLOW = \"\\033[93m\"\n+RESET = \"\\033[0m\"\n+BLACK_SQUARE = \"\"\n+WHITE_SQUARE = \"\"\n+\n+\n+def generate_attention_matrix_from_mask(words, mask, img_token=\"<img>\", sliding_window=None, token_type_ids=None):\n+    \"\"\"\n+    Generates an attention matrix from a given attention mask.\n+\n+    Optionally applies a sliding window mask (e.g., for Gemma2/3) and\n+    marks regions where image tokens occur based on the specified `img_token`.\n+    \"\"\"\n+    mask = mask.int()\n+    if mask.ndim == 3:\n+        mask = mask[0, :, :]\n+    if mask.ndim == 4:\n+        mask = mask[0, 0, :, :]\n+\n+    n = len(words)\n+    max_word_length = max(len(repr(word)) for word in words)\n+    first_img_idx = 0\n+    output = []\n+\n+    for i, k in enumerate(words):\n+        if k == img_token and not first_img_idx:\n+            first_img_idx = i\n+            mask[i, i] = 2  # Mark yellow regions\n+        if first_img_idx > 0 and (k != img_token or i == n - 1):\n+            if i == n - 1:\n+                i += 1\n+            mask[first_img_idx:i, first_img_idx:i] = 2  # Mark yellow regions\n+            first_img_idx = 0\n+\n+    # Generate sliding window mask (size = 4), excluding img_token\n+    sliding_window_mask = None\n+    if sliding_window is not None:\n+        sliding_window_mask = [[1 if (0 <= i - j < sliding_window) else 0 for j in range(n)] for i in range(n)]\n+\n+    row_dummy = \" \".join(\n+        f\"{YELLOW}{BLACK_SQUARE}{RESET}\"\n+        if mask[0, j]\n+        else f\"{GREEN}{BLACK_SQUARE}{RESET}\"\n+        if 0 == j\n+        else BLACK_SQUARE\n+        if mask[0, j]\n+        else WHITE_SQUARE\n+        for j in range(n)\n+    )\n+\n+    # Print headers\n+    legend = f\"{GREEN}{BLACK_SQUARE}{RESET}: i == j (diagonal)   {YELLOW}{BLACK_SQUARE}{RESET}: token_type_ids\"\n+    output.append(\" \" + legend)\n+    f_string = \" \" * (max_word_length + 5) + \"Attention Matrix\".ljust(len(row_dummy) // 2)\n+    if sliding_window is not None:\n+        f_string += \"Sliding Window Mask\"\n+    output.append(f_string)\n+\n+    vertical_header = []\n+    for idx, word in enumerate(words):\n+        if mask[idx, idx] == 2:\n+            vertical_header.append([f\"{YELLOW}{k}{RESET}\" for k in list(str(idx).rjust(len(str(n))))])\n+        else:\n+            vertical_header.append(list(str(idx).rjust(len(str(n)))))\n+\n+    vertical_header = list(map(list, zip(*vertical_header)))  # Transpose\n+\n+    for row in vertical_header:\n+        output.append(\n+            (max_word_length + 5) * \" \" + \" \".join(row) + \"    |    \" + \" \".join(row)\n+            if sliding_window is not None\n+            else \"\"\n+        )\n+\n+    for i, word in enumerate(words):\n+        word_repr = repr(word).ljust(max_word_length)\n+        colored_word = f\"{YELLOW}{word_repr}{RESET}\" if img_token in word else word_repr\n+        row_display = \" \".join(\n+            f\"{YELLOW}{BLACK_SQUARE}{RESET}\"\n+            if img_token in words[j] and mask[i, j] and img_token in words[i]\n+            else f\"{GREEN}{BLACK_SQUARE}{RESET}\"\n+            if i == j\n+            else BLACK_SQUARE\n+            if mask[i, j]\n+            else WHITE_SQUARE\n+            for j in range(n)\n+        )\n+        sliding_window_row = \"\"\n+        if sliding_window is not None:\n+            sliding_window_row = \" \".join(\n+                f\"{YELLOW}{BLACK_SQUARE}{RESET}\"\n+                if img_token in words[j] and img_token in words[i]\n+                else f\"{GREEN}{BLACK_SQUARE}{RESET}\"\n+                if i == j\n+                else BLACK_SQUARE\n+                if sliding_window_mask[i][j]\n+                else WHITE_SQUARE\n+                for j in range(n)\n+            )\n+\n+        output.append(f\"{colored_word}: {str(i).rjust(2)} {row_display}    |    {sliding_window_row}\")\n+\n+    return \"\\n\".join(output)\n+\n+\n+class AttentionMaskVisualizer:\n+    def __init__(self, model_name: str):\n+        config = AutoConfig.from_pretrained(model_name)\n+        self.image_token = \"<img>\"\n+        if hasattr(config.get_text_config(), \"sliding_window\"):\n+            config.sliding_window = 5\n+        try:\n+            mapped_cls = _get_model_class(config, MODEL_MAPPING)\n+        except Exception:\n+            mapped_cls = _get_model_class(config, MODEL_FOR_PRETRAINING_MAPPING)\n+\n+        if mapped_cls is None:\n+            raise ValueError(f\"Model name {model_name} is not supported for attention visualization\")\n+        self.mapped_cls = mapped_cls\n+\n+        class _ModelWrapper(mapped_cls, nn.Module):\n+            def __init__(self, config, model_name):\n+                nn.Module.__init__(self)\n+                self.dummy_module = nn.Linear(1, 1)\n+                self.config = config\n+\n+        self.model = _ModelWrapper(config, model_name)\n+        self.model.to(config.torch_dtype)\n+        self.repo_id = model_name\n+        self.config = config\n+\n+    def __call__(self, input_sentence: str, suffix=\"\"):\n+        self.visualize_attention_mask(input_sentence, suffix=suffix)\n+\n+    def visualize_attention_mask(self, input_sentence: str, suffix=\"\"):\n+        model = self.model\n+        kwargs = {}\n+        if self.config.model_type in PROCESSOR_MAPPING_NAMES:\n+            img = \"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/bee.jpg?download=true\"\n+            img = Image.open(requests.get(img, stream=True).raw)\n+            processor = AutoProcessor.from_pretrained(self.repo_id, image_seq_length=5)\n+            if hasattr(processor, \"image_token\"):\n+                image_token = processor.image_token\n+            else:\n+                image_token = processor.tokenizer.convert_ids_to_tokens([processor.image_token_id])[0]\n+\n+            if image_token:\n+                input_sentence = input_sentence.replace(\"<img>\", image_token)\n+\n+            inputs = processor(img, input_sentence, suffix=suffix, return_tensors=\"pt\")\n+\n+            self.image_token = processor.tokenizer.convert_ids_to_tokens([processor.image_token_id])[0]\n+\n+            attention_mask = inputs[\"attention_mask\"]\n+            if \"token_type_ids\" in inputs:  # TODO inspect signature of update causal mask\n+                kwargs[\"token_type_ids\"] = inputs[\"token_type_ids\"]\n+            tokens = processor.tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])\n+        elif self.config.model_type in TOKENIZER_MAPPING_NAMES:\n+            tokenizer = AutoTokenizer.from_pretrained(self.repo_id)\n+            tokens = tokenizer.tokenize(input_sentence)\n+            attention_mask = tokenizer(input_sentence, return_tensors=\"pt\")[\"attention_mask\"]\n+        else:\n+            raise ValueError(f\"Model type {model.config.model_type} does not support attention visualization\")\n+\n+        model.config._attn_implementation = \"eager\"\n+        model.train()\n+        attention_mask = ~model._update_causal_mask(\n+            attention_mask=attention_mask,\n+            input_tensor=attention_mask.to(self.model.dtype),\n+            cache_position=torch.arange(attention_mask.shape[1]),\n+            past_key_values=None,\n+            **kwargs,\n+        ).bool()\n+        top_bottom_border = \"##\" * (\n+            len(f\"Attention visualization for {self.config.model_type} | {self.mapped_cls}\") + 4\n+        )  # Box width adjusted to text length\n+        side_border = \"##\"\n+        print(f\"\\n{top_bottom_border}\")\n+        print(\n+            \"##\"\n+            + f\"  Attention visualization for \\033[1m{self.config.model_type}:{self.repo_id}\\033[0m {self.mapped_cls.__name__}\".center(\n+                len(top_bottom_border)\n+            )\n+            + \"    \"\n+            + side_border\n+        )\n+        print(f\"{top_bottom_border}\")\n+        f_string = generate_attention_matrix_from_mask(\n+            tokens,\n+            attention_mask,\n+            img_token=self.image_token,\n+            sliding_window=getattr(self.config, \"sliding_window\", None),\n+            token_type_ids=kwargs.get(\"token_type_ids\", None),\n+        )\n+        print(f_string)\n+        print(f\"{top_bottom_border}\")"
        }
    ],
    "stats": {
        "total": 353,
        "additions": 294,
        "deletions": 59
    }
}