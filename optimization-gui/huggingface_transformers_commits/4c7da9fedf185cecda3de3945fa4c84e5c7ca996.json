{
    "author": "S1ro1",
    "message": "PATCH: add back n-dim device-mesh + fix tp trainer saving (#39693)\n\n* Feat: something\n\n* Feat: initial changes\n\n* tmp changes to unblock\n\n* Refactor\n\n* remove todo\n\n* Feat: docstring\n\n* Fix: saving of distributed model in trainer\n\n* Fix: distributed saving with trainer\n\n* Feat: add pure tp saving\n\n* Only require tp dim if ndim > 1\n\n* Fix: default to None\n\n* Fix: better comments/errors\n\n* Fix: properly check tp_size attribute\n\n* Fix: properly check for None in tp_size\n\n---------\n\nCo-authored-by: Marc Sun <57196510+SunMarc@users.noreply.github.com>",
    "sha": "4c7da9fedf185cecda3de3945fa4c84e5c7ca996",
    "files": [
        {
            "sha": "a97b446c964751ccab349ceee2b900b45e1350e9",
            "filename": "src/transformers/modeling_utils.py",
            "status": "modified",
            "additions": 9,
            "deletions": 4,
            "changes": 13,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c7da9fedf185cecda3de3945fa4c84e5c7ca996/src%2Ftransformers%2Fmodeling_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c7da9fedf185cecda3de3945fa4c84e5c7ca996/src%2Ftransformers%2Fmodeling_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Fmodeling_utils.py?ref=4c7da9fedf185cecda3de3945fa4c84e5c7ca996",
            "patch": "@@ -4472,7 +4472,7 @@ def from_pretrained(\n                 A torch tensor parallel degree. If not provided would default to world size.\n             device_mesh (`torch.distributed.DeviceMesh`, *optional*):\n                 A torch device mesh. If not provided would default to world size. Used only for tensor parallel for now.\n-                If provided, it has to contain dimension named `\"tp\"` which will be used for tensor parallelism\n+                If provided, it has to contain dimension named `\"tp\"` in case it's > 1 dimensional, this dimension will be used for tensor parallelism\n             offload_folder (`str` or `os.PathLike`, *optional*):\n                 If the `device_map` contains any value `\"disk\"`, the folder where we will offload weights.\n             offload_state_dict (`bool`, *optional*):\n@@ -4617,10 +4617,15 @@ def from_pretrained(\n             if device_mesh is None:\n                 tp_plan, device_map, device_mesh, tp_size = initialize_tensor_parallelism(tp_plan, tp_size=tp_size)\n             else:\n-                # TODO: make device_mesh support multiple dimensions\n                 if device_mesh.ndim > 1:\n-                    raise ValueError(\"device_mesh must be 1 dimensional and will be used for TP\")\n-                device_map = torch.device(device_mesh.device_type, int(os.environ[\"LOCAL_RANK\"]))\n+                    if \"tp\" not in device_mesh.mesh_dim_names:\n+                        raise ValueError(\n+                            \"When using `tp_plan` and n-d `device_mesh`, it must contain a 'tp' dimension. \"\n+                            \"Please provide a valid `device_mesh`.\"\n+                        )\n+                    device_mesh = device_mesh[\"tp\"]\n+                tp_size = device_mesh.size()\n+                device_map = torch.device(f\"{device_mesh.device_type}:{int(os.environ['LOCAL_RANK'])}\")\n \n             if tp_size is None:\n                 tp_size = torch.distributed.get_world_size()"
        },
        {
            "sha": "52dc9c35572087d71faab4d2783c80deb8a6846d",
            "filename": "src/transformers/trainer.py",
            "status": "modified",
            "additions": 7,
            "deletions": 0,
            "changes": 7,
            "blob_url": "https://github.com/huggingface/transformers/blob/4c7da9fedf185cecda3de3945fa4c84e5c7ca996/src%2Ftransformers%2Ftrainer.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/4c7da9fedf185cecda3de3945fa4c84e5c7ca996/src%2Ftransformers%2Ftrainer.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Ftrainer.py?ref=4c7da9fedf185cecda3de3945fa4c84e5c7ca996",
            "patch": "@@ -3953,6 +3953,13 @@ def save_model(self, output_dir: Optional[str] = None, _internal_call: bool = Fa\n             if IS_SAGEMAKER_MP_POST_1_10:\n                 # 'user_content.pt' indicates model state_dict saved with smp >= 1.10\n                 Path(os.path.join(output_dir, \"user_content.pt\")).touch()\n+        # We are in N-D parallelism if we have parallelism_config set, so we check accelerate if we're on a to_save rank\n+        elif getattr(self.accelerator, \"parallelism_config\", None) is not None:\n+            if self.accelerator.should_save_model:\n+                self._save(output_dir)\n+        # If we drop to here, we're in 1D parallelism, so all ranks need to go to `save_pretrained`\n+        elif (tp_size := getattr(self.model, \"_tp_size\", 0)) is not None and tp_size > 1:\n+            self._save(output_dir)\n         elif self.is_fsdp_enabled:\n             if (\"FULL_STATE_DICT\" in str(self.accelerator.state.fsdp_plugin.state_dict_type)) and (\n                 version.parse(accelerate_version) > version.parse(\"0.24.1\")"
        }
    ],
    "stats": {
        "total": 20,
        "additions": 16,
        "deletions": 4
    }
}