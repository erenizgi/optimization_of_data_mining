{
    "author": "yuanwu2017",
    "message": "Fix the seamless_m4t cannot work on Gaudi (#38363)\n\n* Fix the seamless_m4t cannot work on Gaudi\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Refine the patch\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Fix seamless_m4t_v2 crash\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Use the patched_gather\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Remove debug logs\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Remove useless modifications\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Add hpu check\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n* Add comments\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\n\n---------\n\nSigned-off-by: yuanwu <yuan.wu@intel.com>\nCo-authored-by: Ilyas Moutawwakil <57442720+IlyasMoutawwakil@users.noreply.github.com>",
    "sha": "de98fb25a3772b8fc4a31e55cb0b0560d97353af",
    "files": [
        {
            "sha": "7956f1b22d4180e4241c0055c2f936ffa34489b7",
            "filename": "src/transformers/utils/import_utils.py",
            "status": "modified",
            "additions": 22,
            "deletions": 0,
            "changes": 22,
            "blob_url": "https://github.com/huggingface/transformers/blob/de98fb25a3772b8fc4a31e55cb0b0560d97353af/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/de98fb25a3772b8fc4a31e55cb0b0560d97353af/src%2Ftransformers%2Futils%2Fimport_utils.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/src%2Ftransformers%2Futils%2Fimport_utils.py?ref=de98fb25a3772b8fc4a31e55cb0b0560d97353af",
            "patch": "@@ -851,6 +851,28 @@ def patched_masked_fill_(self, mask, value):\n \n         torch.Tensor.masked_fill_ = patched_masked_fill_\n \n+    # We patch torch.gather for int64 tensors to avoid a bug on Gaudi\n+    # Graph compile failed with synStatus 26 [Generic failure]\n+    # This can be removed once bug is fixed but for now we need it.\n+    original_gather = torch.Tensor.gather\n+\n+    def patched_gather(input: torch.Tensor, dim: int, index: torch.LongTensor) -> torch.Tensor:\n+        if input.dtype == torch.int64 and input.device.type == \"hpu\":\n+            logger.warning_once(\n+                \"torch.gather is not supported for int64 tensors on Gaudi. \"\n+                \"This operation will be performed patched_gather using indexing.\"\n+            )\n+\n+            idx = [torch.arange(size, device=input.device, dtype=input.dtype) for size in input.shape]\n+            idx[dim] = index\n+            idx = tuple(idx)\n+            output = input[idx]\n+            return output\n+        else:\n+            return original_gather(input, dim, index)\n+\n+    torch.Tensor.gather = patched_gather\n+\n     # IlyasMoutawwakil: we patch torch.compile to use the HPU backend by default\n     # https://github.com/huggingface/transformers/pull/38790#discussion_r2157043944\n     # This is necessary for cases where torch.compile is used as a decorator (defaulting to inductor)"
        }
    ],
    "stats": {
        "total": 22,
        "additions": 22,
        "deletions": 0
    }
}