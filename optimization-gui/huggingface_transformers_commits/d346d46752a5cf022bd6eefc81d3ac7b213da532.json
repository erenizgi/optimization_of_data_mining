{
    "author": "Kim-Ju-won",
    "message": "ğŸŒ [i18n-KO] Translated `tvp.md` to Korean (#39578)\n\n* docs: ko: tvp.md\n\n* feat: nmt draft\n\n* fix: manual edits\n\n* fix: manual edits\n\n* fix: manual edits\n\n* fix: manual edits\n\n* fix: manual edits\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>\n\n---------\n\nCo-authored-by: Harheem Kim <49297157+harheem@users.noreply.github.com>",
    "sha": "d346d46752a5cf022bd6eefc81d3ac7b213da532",
    "files": [
        {
            "sha": "eb934d82aec1be1f10915f58d7648ffd1d4f6a6a",
            "filename": "docs/source/ko/_toctree.yml",
            "status": "modified",
            "additions": 1,
            "deletions": 1,
            "changes": 2,
            "blob_url": "https://github.com/huggingface/transformers/blob/d346d46752a5cf022bd6eefc81d3ac7b213da532/docs%2Fsource%2Fko%2F_toctree.yml",
            "raw_url": "https://github.com/huggingface/transformers/raw/d346d46752a5cf022bd6eefc81d3ac7b213da532/docs%2Fsource%2Fko%2F_toctree.yml",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2F_toctree.yml?ref=d346d46752a5cf022bd6eefc81d3ac7b213da532",
            "patch": "@@ -1155,4 +1155,4 @@\n     - local: in_translation\n       title: (ë²ˆì—­ì¤‘)Environment Variables\n     title: Reference\n-  title: API\n+  title: API\n\\ No newline at end of file"
        },
        {
            "sha": "dba7e2182555ed1ed9f6e8b0f2967b22ed7fc793",
            "filename": "docs/source/ko/model_doc/tvp.md",
            "status": "added",
            "additions": 189,
            "deletions": 0,
            "changes": 189,
            "blob_url": "https://github.com/huggingface/transformers/blob/d346d46752a5cf022bd6eefc81d3ac7b213da532/docs%2Fsource%2Fko%2Fmodel_doc%2Ftvp.md",
            "raw_url": "https://github.com/huggingface/transformers/raw/d346d46752a5cf022bd6eefc81d3ac7b213da532/docs%2Fsource%2Fko%2Fmodel_doc%2Ftvp.md",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/docs%2Fsource%2Fko%2Fmodel_doc%2Ftvp.md?ref=d346d46752a5cf022bd6eefc81d3ac7b213da532",
            "patch": "@@ -0,0 +1,189 @@\n+<!--Copyright 2023 The Intel Team Authors and HuggingFace Inc. team. All rights reserved.\n+\n+Licensed under the Apache License, Version 2.0 (the \"License\"); you may not use this file except in compliance with\n+the License. You may obtain a copy of the License at\n+\n+http://www.apache.org/licenses/LICENSE-2.0\n+\n+Unless required by applicable law or agreed to in writing, software distributed under the License is distributed on\n+an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied. See the License for the\n+specific language governing permissions and limitations under the License.\n+-->\n+\n+# TVP [[tvp]]\n+\n+<div class=\"flex flex-wrap space-x-1\">\n+<img alt=\"PyTorch\" src=\"https://img.shields.io/badge/PyTorch-DE3412?style=flat&logo=pytorch&logoColor=white\">\n+</div>\n+\n+## ê°œìš” [[overview]]\n+\n+Text-Visual Prompting(TVP) í”„ë ˆì„ì›Œí¬ëŠ” Yimeng Zhang, Xin Chen, Jinghan Jia, Sijia Liu, Ke Dingì´ ë°œí‘œí•œ ë…¼ë¬¸ [Text-Visual Prompting for Efficient 2D Temporal Video Grounding](https://huggingface.co/papers/2303.04995)ì—ì„œ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤.\n+\n+ë…¼ë¬¸ì˜ ì´ˆë¡ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:\n+\n+*ë³¸ ë…¼ë¬¸ì—ì„œëŠ” ê¸¸ê³ , í¸ì§‘ë˜ì§€ ì•Šì€ ë¹„ë””ì˜¤ì—ì„œ ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…ëœ ìˆœê°„ì˜ ì‹œì‘/ì¢…ë£Œ ì‹œì ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•˜ëŠ” Temporal Video Grounding(TVG) ë¬¸ì œë¥¼ ë‹¤ë£¹ë‹ˆë‹¤. ì„¸ë°€í•œ 3D ì‹œê°ì  íŠ¹ì§• ë•ë¶„ì— TVG ê¸°ìˆ ì€ ìµœê·¼ ëª‡ ë…„ ë™ì•ˆ ë†€ë¼ìš´ ë°œì „ì„ ì´ë¤˜ìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ 3D í•©ì„±ê³± ì‹ ê²½ë§(CNN)ì˜ ë†’ì€ ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ ë°€ë„ ë†’ì€ 3D ì‹œê°ì  íŠ¹ì§•ì„ ì¶”ì¶œí•˜ëŠ” ë° ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¬ê³  ê·¸ë§Œí¼ ë§ì€ ë©”ëª¨ë¦¬ì™€ ì—°ì‚° ìì›ì„ í•„ìš”ë¡œ í•©ë‹ˆë‹¤. íš¨ìœ¨ì ì¸ TVGë¥¼ ìœ„í•´, ë³¸ ë…¼ë¬¸ì—ì„œëŠ” TVG ëª¨ë¸ì˜ ì‹œê°ì  ì…ë ¥ê³¼ í…ìŠ¤íŠ¸ íŠ¹ì§• ëª¨ë‘ì— ìµœì í™”ëœ êµë€ íŒ¨í„´('í”„ë¡¬í”„íŠ¸'ë¼ê³  ë¶€ë¦„)ì„ í†µí•©í•˜ëŠ” ìƒˆë¡œìš´ Text-Visual Prompting(TVP) í”„ë ˆì„ì›Œí¬ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. 3D CNNê³¼ ëšœë ·ì´ ëŒ€ë¹„ë˜ê²Œ TVPê°€ 2D TVG ëª¨ë¸ì—ì„œ ë¹„ì „ ì¸ì½”ë”ì™€ ì–¸ì–´ ì¸ì½”ë”ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê³µë™ í•™ìŠµí•  ìˆ˜ ìˆê²Œ í•˜ê³ , ë‚®ì€ ë³µì¡ë„ì˜ í¬ì†Œí•œ 2D ì‹œê°ì  íŠ¹ì§•ë§Œì„ ì‚¬ìš©í•˜ì—¬ í¬ë¡œìŠ¤ ëª¨ë‹¬ íŠ¹ì§• ìœµí•©ì˜ ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚µë‹ˆë‹¤. ë” ë‚˜ì•„ê°€, TVGì˜ íš¨ìœ¨ì ì¸ í•™ìŠµì„ ìœ„í•´ Temporal-Distance IoU(TDIoU) ì†ì‹¤ í•¨ìˆ˜ë¥¼ ì œì•ˆí•©ë‹ˆë‹¤. ë‘ ê°œì˜ ë²¤ì¹˜ë§ˆí¬ ë°ì´í„° ì„¸íŠ¸ì¸ Charades-STAì™€ ActivityNet Captions ë°ì´í„°ì…‹ì— ëŒ€í•œ ì‹¤í—˜ì„ í†µí•´, ì œì•ˆëœ TVPê°€ 2D TVGì˜ ì„±ëŠ¥ì„ í¬ê²Œ í–¥ìƒì‹œí‚¤ê³ (ì˜ˆ: Charades-STAì—ì„œ 9.79% í–¥ìƒ, ActivityNet Captionsì—ì„œ 30.77% í–¥ìƒ) 3D ì‹œê°ì  íŠ¹ì§•ì„ ì‚¬ìš©í•˜ëŠ” TVGì— ë¹„í•´ 5ë°°ì˜ ì¶”ë¡  ê°€ì†ì„ ë‹¬ì„±í•¨ì„ ì‹¤í—˜ì ìœ¼ë¡œ ì…ì¦í•©ë‹ˆë‹¤.*\n+\n+ì´ ì—°êµ¬ëŠ” Temporal Video Grounding(TVG)ì„ ë‹¤ë£¹ë‹ˆë‹¤. TVGëŠ” ë¬¸ì¥ìœ¼ë¡œ ì„¤ëª…ëœ íŠ¹ì • ì´ë²¤íŠ¸ì˜ ì‹œì‘ ë° ì¢…ë£Œ ì‹œì ì„ ê¸´ ë¹„ë””ì˜¤ì—ì„œ ì •í™•íˆ ì°¾ì•„ë‚´ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. TVG ì„±ëŠ¥ì„ í–¥ìƒì‹œí‚¤ê¸° ìœ„í•´ Text-Visual Prompting(TVP)ì´ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. TVPëŠ” 'í”„ë¡¬í”„íŠ¸'ë¼ê³  ì•Œë ¤ì§„ íŠ¹ë³„íˆ ì„¤ê³„ëœ íŒ¨í„´ì„ TVG ëª¨ë¸ì˜ ì‹œê°ì (ì´ë¯¸ì§€ ê¸°ë°˜) ë° í…ìŠ¤íŠ¸(ë‹¨ì–´ ê¸°ë°˜) ì…ë ¥ êµ¬ì„± ìš”ì†Œ ëª¨ë‘ì— í†µí•©í•˜ëŠ” ê²ƒì„ ë°©ì‹ì…ë‹ˆë‹¤. ì´ í”„ë¡¬í”„íŠ¸ëŠ” ì¶”ê°€ì ì¸ ì‹œê³µê°„ì  ì»¨í…ìŠ¤íŠ¸ë¥¼ ì œê³µí•¨ìœ¼ë¡œì¨ ëª¨ë¸ì´ ë¹„ë””ì˜¤ ë‚´ ì´ë²¤íŠ¸ ì‹œì ì˜ ì˜ˆì¸¡ ì •í™•ë„ë¥¼ ë†’ì…ë‹ˆë‹¤. ì´ ì ‘ê·¼ ë°©ì‹ì€ 3D ì‹œê°ì  ì…ë ¥ ëŒ€ì‹  2D ì…ë ¥ì„ ì‚¬ìš©í•©ë‹ˆë‹¤. 3D ì…ë ¥ì€ ë³´ë‹¤ í’ë¶€í•œ ì‹œê³µê°„ì  ì„¸ë¶€ ì •ë³´ë¥¼ ì œê³µí•˜ì§€ë§Œ ì²˜ë¦¬í•˜ëŠ” ë° ì‹œê°„ì´ ë” ë§ì´ ê±¸ë¦½ë‹ˆë‹¤. ë”°ë¼ì„œ í”„ë¡¬í”„íŒ… ë©”ì†Œë“œì™€ í•¨ê»˜ 2D ì…ë ¥ì„ ì‚¬ìš©í•˜ì—¬ ì´ì™€ ìœ ì‚¬í•œ ìˆ˜ì¤€ì˜ ì»¨í…ìŠ¤íŠ¸ì™€ ì •í™•ë„ë¥¼ ë” íš¨ìœ¨ì ìœ¼ë¡œ ì œê³µí•˜ëŠ” ê²ƒì„ ëª©í‘œë¡œ í•©ë‹ˆë‹¤.\n+\n+<img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/tvp_architecture.png\"\n+alt=\"drawing\" width=\"600\"/>\n+\n+<small> TVP ì•„í‚¤í…ì²˜. <a href=\"https://huggingface.co/papers/2303.04995\">ì›ë³¸ ë…¼ë¬¸ì—ì„œ ë°œì·Œ.</a> </small>\n+\n+ì´ ëª¨ë¸ì€ [Jiqing Feng](https://huggingface.co/Jiqing)ë‹˜ì´ ê¸°ì—¬í–ˆìŠµë‹ˆë‹¤. ì›ë³¸ ì½”ë“œëŠ” [ì´ ê³³](https://github.com/intel/TVP)ì—ì„œ ì°¾ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+## ì‚¬ìš© íŒ ë° ì˜ˆì‹œ [[usage-tips-and-examples]]\n+\n+í”„ë¡¬í”„íŠ¸ëŠ” ìµœì í™”ëœ êµë€ íŒ¨í„´ìœ¼ë¡œ ì…ë ¥ ë¹„ë””ì˜¤ í”„ë ˆì„ì´ë‚˜ í…ìŠ¤íŠ¸ íŠ¹ì§•ì— ì¶”ê°€ë˜ëŠ” íŒ¨í„´ì…ë‹ˆë‹¤. ë²”ìš© ì„¸íŠ¸ë€ ëª¨ë“  ì…ë ¥ì— ëŒ€í•´ ë™ì¼í•œ í”„ë¡¬í”„íŠ¸ ì„¸íŠ¸ë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ë§í•©ë‹ˆë‹¤. ì¦‰, ì…ë ¥ ë‚´ìš©ê³¼ ê´€ê³„ì—†ì´ ëª¨ë“  ë¹„ë””ì˜¤ í”„ë ˆì„ê³¼ í…ìŠ¤íŠ¸ íŠ¹ì§•ì— ì´ í”„ë¡¬í”„íŠ¸ë“¤ì„ ì¼ê´€ì ìœ¼ë¡œ ì¶”ê°€í•©ë‹ˆë‹¤.\n+\n+TVPëŠ” ì‹œê° ì¸ì½”ë”ì™€ í¬ë¡œìŠ¤ ëª¨ë‹¬ ì¸ì½”ë”ë¡œ êµ¬ì„±ë©ë‹ˆë‹¤. ë²”ìš© ì‹œê° í”„ë¡¬í”„íŠ¸ì™€ í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì„¸íŠ¸ê°€ ê°ê° ìƒ˜í”Œë§ëœ ë¹„ë””ì˜¤ í”„ë ˆì„ê³¼ í…ìŠ¤íŠ¸ íŠ¹ì§•ì— í†µí•©ë©ë‹ˆë‹¤. íŠ¹íˆ, ì„œë¡œ ë‹¤ë¥¸ ì‹œê° í”„ë¡¬í”„íŠ¸ ì„¸íŠ¸ê°€ í¸ì§‘ë˜ì§€ ì•Šì€ í•œ ë¹„ë””ì˜¤ì—ì„œ ê· ì¼í•˜ê²Œ ìƒ˜í”Œë§ëœ í”„ë ˆì„ì— ìˆœì„œëŒ€ë¡œ ì ìš©ë©ë‹ˆë‹¤.\n+\n+ì´ ëª¨ë¸ì˜ ëª©í‘œëŠ” í•™ìŠµ ê°€ëŠ¥í•œ í”„ë¡¬í”„íŠ¸ë¥¼ ì‹œê°ì  ì…ë ¥ê³¼ í…ìŠ¤íŠ¸ íŠ¹ì§• ëª¨ë‘ì— í†µí•©í•˜ì—¬ Temporal Video Grounding(TVG) ë¬¸ì œë¥¼ í•´ê²°í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤.\n+\n+ì›ì¹™ì ìœ¼ë¡œ, ì œì•ˆëœ ì•„í‚¤í…ì²˜ì—ëŠ” ì–´ë–¤ ì‹œê° ì¸ì½”ë”ë‚˜ í¬ë¡œìŠ¤ ëª¨ë‹¬ ì¸ì½”ë”ë¼ë„ ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\n+\n+[TvpProcessor]ëŠ” [BertTokenizer]ì™€ [TvpImageProcessor]ë¥¼ ë‹¨ì¼ ì¸ìŠ¤í„´ìŠ¤ë¡œ ë˜í•‘í•˜ì—¬ í…ìŠ¤íŠ¸ë¥¼ ì¸ì½”ë”©í•˜ê³  ì´ë¯¸ì§€ë¥¼ ê°ê° ì¤€ë¹„í•©ë‹ˆë‹¤.\n+\n+ë‹¤ìŒ ì˜ˆì‹œëŠ” [TvpProcessor]ì™€ [TvpForVideoGrounding]ì„ ì‚¬ìš©í•˜ì—¬ TVGë¥¼ ì‹¤í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.\n+\n+```python\n+import av\n+import cv2\n+import numpy as np\n+import torch\n+from huggingface_hub import hf_hub_download\n+from transformers import AutoProcessor, TvpForVideoGrounding\n+\n+\n+def pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\n+    '''\n+    ì›ë³¸ fpsì˜ ë¹„ë””ì˜¤ë¥¼ ì§€ì •í•œ fps(target_fps)ë¡œ ë³€í™˜í•˜ê³  PyAV ë””ì½”ë”ë¡œ ë¹„ë””ì˜¤ë¥¼ ë””ì½”ë”©í•©ë‹ˆë‹¤.\n+    Args:\n+        container (container): pyav ì»¨í…Œì´ë„ˆ ê°ì²´ì…ë‹ˆë‹¤.\n+        sampling_rate (int): í”„ë ˆì„ ìƒ˜í”Œë§ ì†ë„ì…ë‹ˆë‹¤.(ìƒ˜í”Œë§ëœ ë‘ê°œì˜ í”„ë ˆì„ ì‚¬ì´ì˜ ê°„ê²©ì„ ë§í•©ë‹ˆë‹¤)\n+        num_frames (int): ìƒ˜í”Œë§í•  í”„ë ˆì„ ìˆ˜ì…ë‹ˆë‹¤.\n+        clip_idx (int): clip_idxê°€ -1ì´ë©´ ì‹œê°„ ì¶•ì—ì„œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n+            clip_idxê°€ -1ë³´ë‹¤ í¬ë©´ ë¹„ë””ì˜¤ë¥¼ num_clips ê°œë¡œ ê· ë“± ë¶„í• í•œ í›„\n+            clip_idxë²ˆì§¸ ë¹„ë””ì˜¤ í´ë¦½ì„ ì„ íƒí•©ë‹ˆë‹¤.\n+        num_clips (int): ì£¼ì–´ì§„ ë¹„ë””ì˜¤ì—ì„œ ê· ì¼í•˜ê²Œ ìƒ˜í”Œë§í•  ì „ì²´ í´ë¦½ ìˆ˜ì…ë‹ˆë‹¤.\n+        target_fps (int): ì…ë ¥ ë¹„ë””ì˜¤ì˜ fpsê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìƒ˜í”Œë§ ì „ì—\n+            ì§€ì •í•œ fpsë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n+    Returns:\n+        frames (tensor): ë¹„ë””ì˜¤ì—ì„œ ë””ì½”ë”©ëœ í”„ë ˆì„ì…ë‹ˆë‹¤. ë¹„ë””ì˜¤ ìŠ¤íŠ¸ë¦¼ì„ ì°¾ì„ ìˆ˜ ì—†ëŠ” ê²½ìš°\n+            Noneì„ ë°˜í™˜í•©ë‹ˆë‹¤.\n+        fps (float): ë¹„ë””ì˜¤ì˜ ì´ˆë‹¹ í”„ë ˆì„ ìˆ˜ì…ë‹ˆë‹¤.\n+    '''\n+    video = container.streams.video[0]\n+    fps = float(video.average_rate)\n+    clip_size = sampling_rate * num_frames / target_fps * fps\n+    delta = max(num_frames - clip_size, 0)\n+    start_idx = delta * clip_idx / num_clips\n+    end_idx = start_idx + clip_size - 1\n+    timebase = video.duration / num_frames\n+    video_start_pts = int(start_idx * timebase)\n+    video_end_pts = int(end_idx * timebase)\n+    seek_offset = max(video_start_pts - 1024, 0)\n+    container.seek(seek_offset, any_frame=False, backward=True, stream=video)\n+    frames = {}\n+    for frame in container.decode(video=0):\n+        if frame.pts < video_start_pts:\n+            continue\n+        frames[frame.pts] = frame\n+        if frame.pts > video_end_pts:\n+            break\n+    frames = [frames[pts] for pts in sorted(frames)]\n+    return frames, fps\n+\n+\n+def decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps):\n+    '''\n+    ë¹„ë””ì˜¤ë¥¼ ë””ì½”ë”©í•˜ê³  ì‹œê°„ ì¶• ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n+    Args:\n+        container (container): pyav ì»¨í…Œì´ë„ˆ ê°ì²´ì…ë‹ˆë‹¤.\n+        sampling_rate (int): í”„ë ˆì„ ìƒ˜í”Œë§ ì†ë„ì…ë‹ˆë‹¤.(ìƒ˜í”Œë§ëœ ë‘ê°œì˜ í”„ë ˆì„ ì‚¬ì´ì˜ ê°„ê²©ì„ ë§í•©ë‹ˆë‹¤)\n+        num_frames (int): ìƒ˜í”Œë§í•  í”„ë ˆì„ ìˆ˜ì…ë‹ˆë‹¤.\n+        clip_idx (int): clip_idxê°€ -1ì´ë©´ ì‹œê°„ ì¶•ì—ì„œ ë¬´ì‘ìœ„ ìƒ˜í”Œë§ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n+            clip_idxê°€ -1ë³´ë‹¤ í¬ë©´ ë¹„ë””ì˜¤ë¥¼ num_clips ê°œë¡œ ê· ë“± ë¶„í• í•œ í›„\n+            clip_idxë²ˆì§¸ ë¹„ë””ì˜¤ í´ë¦½ì„ ì„ íƒí•©ë‹ˆë‹¤.\n+        num_clips (int): ì£¼ì–´ì§„ ë¹„ë””ì˜¤ì—ì„œ ê· ì¼í•˜ê²Œ ìƒ˜í”Œë§í•  ì „ì²´ í´ë¦½ ìˆ˜ì…ë‹ˆë‹¤.\n+        target_fps (int): ì…ë ¥ ë¹„ë””ì˜¤ì˜ fpsê°€ ë‹¤ë¥¼ ìˆ˜ ìˆìœ¼ë¯€ë¡œ, ìƒ˜í”Œë§ ì „ì—\n+            ì§€ì •í•œ fpsë¡œ ë³€í™˜í•©ë‹ˆë‹¤\n+    Returns:\n+        frames (tensor): ë¹„ë””ì˜¤ì—ì„œ ë””ì½”ë”©ëœ í”„ë ˆì„ì…ë‹ˆë‹¤.\n+    '''\n+    assert clip_idx >= -2, \"Not a valid clip_idx {}\".format(clip_idx)\n+    frames, fps = pyav_decode(container, sampling_rate, num_frames, clip_idx, num_clips, target_fps)\n+    clip_size = sampling_rate * num_frames / target_fps * fps\n+    index = np.linspace(0, clip_size - 1, num_frames)\n+    index = np.clip(index, 0, len(frames) - 1).astype(np.int64)\n+    frames = np.array([frames[idx].to_rgb().to_ndarray() for idx in index])\n+    frames = frames.transpose(0, 3, 1, 2)\n+    return frames\n+\n+\n+file = hf_hub_download(repo_id=\"Intel/tvp_demo\", filename=\"AK2KG.mp4\", repo_type=\"dataset\")\n+model = TvpForVideoGrounding.from_pretrained(\"Intel/tvp-base\")\n+\n+decoder_kwargs = dict(\n+    container=av.open(file, metadata_errors=\"ignore\"),\n+    sampling_rate=1,\n+    num_frames=model.config.num_frames,\n+    clip_idx=0,\n+    num_clips=1,\n+    target_fps=3,\n+)\n+raw_sampled_frms = decode(**decoder_kwargs)\n+\n+text = \"a person is sitting on a bed.\"\n+processor = AutoProcessor.from_pretrained(\"Intel/tvp-base\")\n+model_inputs = processor(\n+    text=[text], videos=list(raw_sampled_frms), return_tensors=\"pt\", max_text_length=100#, size=size\n+)\n+\n+model_inputs[\"pixel_values\"] = model_inputs[\"pixel_values\"].to(model.dtype)\n+output = model(**model_inputs)\n+\n+def get_video_duration(filename):\n+    cap = cv2.VideoCapture(filename)\n+    if cap.isOpened():\n+        rate = cap.get(5)\n+        frame_num = cap.get(7)\n+        duration = frame_num/rate\n+        return duration\n+    return -1\n+\n+duration = get_video_duration(file)\n+start, end = processor.post_process_video_grounding(output.logits, duration)\n+\n+print(f\"The time slot of the video corresponding to the text \\\"{text}\\\" is from {start}s to {end}s\")\n+```\n+\n+íŒ:\n+- ì´ TVP êµ¬í˜„ì€ í…ìŠ¤íŠ¸ ì„ë² ë”©ì„ ìƒì„±í•˜ê¸° ìœ„í•´ [BertTokenizer]ë¥¼ ì‚¬ìš©í•˜ê³ , ì‹œê°ì  ì„ë² ë”©ì„ ê³„ì‚°í•˜ê¸° ìœ„í•´ Resnet-50 ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.\n+- ì‚¬ì „ í•™ìŠµëœ [tvp-base](https://huggingface.co/Intel/tvp-base)ì˜ ì²´í¬í¬ì¸íŠ¸ê°€ ê³µê°œë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n+- ì‹œê°„ì  ë¹„ë””ì˜¤ ê·¸ë¼ìš´ë”© ì‘ì—…ì— ëŒ€í•œ TVPì˜ ì„±ëŠ¥ì€ [í‘œ 2](https://huggingface.co/papers/2303.04995)ë¥¼ ì°¸ê³ í•˜ì„¸ìš”.\n+\n+## TvpConfig [[transformers.TvpConfig]]\n+\n+[[autodoc]] TvpConfig\n+\n+## TvpImageProcessor [[transformers.TvpImageProcessor]]\n+\n+[[autodoc]] TvpImageProcessor\n+    - preprocess\n+\n+## TvpProcessor [[transformers.TvpProcessor]]\n+\n+[[autodoc]] TvpProcessor\n+    - __call__\n+\n+## TvpModel [[transformers.TvpModel]]\n+\n+[[autodoc]] TvpModel\n+    - forward\n+\n+## TvpForVideoGrounding [[transformers.TvpForVideoGrounding]]\n+\n+[[autodoc]] TvpForVideoGrounding\n+    - forward"
        }
    ],
    "stats": {
        "total": 191,
        "additions": 190,
        "deletions": 1
    }
}