{
    "author": "ydshieh",
    "message": "Fix `qwen2_moe` tests (#40494)\n\nupdate\n\nCo-authored-by: ydshieh <ydshieh@users.noreply.github.com>",
    "sha": "6350636964b2a5481be827f61f5f4ba6c4d31c75",
    "files": [
        {
            "sha": "02eb4dfb4643ff4632830474526386a8b06cb7c6",
            "filename": "tests/models/qwen2_moe/test_modeling_qwen2_moe.py",
            "status": "modified",
            "additions": 38,
            "deletions": 43,
            "changes": 81,
            "blob_url": "https://github.com/huggingface/transformers/blob/6350636964b2a5481be827f61f5f4ba6c4d31c75/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "raw_url": "https://github.com/huggingface/transformers/raw/6350636964b2a5481be827f61f5f4ba6c4d31c75/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py",
            "contents_url": "https://api.github.com/repos/huggingface/transformers/contents/tests%2Fmodels%2Fqwen2_moe%2Ftest_modeling_qwen2_moe.py?ref=6350636964b2a5481be827f61f5f4ba6c4d31c75",
            "patch": "@@ -13,18 +13,18 @@\n # limitations under the License.\n \"\"\"Testing suite for the PyTorch Qwen2MoE model.\"\"\"\n \n-import gc\n import unittest\n \n import pytest\n \n from transformers import AutoTokenizer, Qwen2MoeConfig, is_torch_available, set_seed\n from transformers.testing_utils import (\n-    backend_empty_cache,\n-    require_bitsandbytes,\n+    cleanup,\n     require_flash_attn,\n     require_torch,\n     require_torch_gpu,\n+    run_first,\n+    run_test_using_subprocess,\n     slow,\n     torch_device,\n )\n@@ -145,54 +145,67 @@ def test_load_balancing_loss(self):\n \n @require_torch\n class Qwen2MoeIntegrationTest(unittest.TestCase):\n+    model = None\n+\n+    @classmethod\n+    def get_model(cls):\n+        if cls.model is None:\n+            cls.model = Qwen2MoeForCausalLM.from_pretrained(\n+                \"Qwen/Qwen1.5-MoE-A2.7B\", device_map=\"auto\", dtype=torch.float16\n+            )\n+        return cls.model\n+\n+    @classmethod\n+    def tearDownClass(cls):\n+        if cls.model is not None:\n+            del cls.model\n+        cleanup(torch_device, gc_collect=True)\n+\n+    def tearDown(self):\n+        cleanup(torch_device, gc_collect=True)\n+\n     @slow\n     def test_model_a2_7b_logits(self):\n         input_ids = [1, 306, 4658, 278, 6593, 310, 2834, 338]\n-        model = Qwen2MoeForCausalLM.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B\", device_map=\"auto\")\n+        model = self.get_model()\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         with torch.no_grad():\n             out = model(input_ids).logits.float().cpu()\n         # Expected mean on dim = -1\n-        EXPECTED_MEAN = torch.tensor([[-4.2125, -3.6416, -4.9136, -4.3005, -4.9938, -3.4393, -3.5195, -4.1621]])\n+        EXPECTED_MEAN = torch.tensor([[-4.2106, -3.6411, -4.9111, -4.2840, -4.9950, -3.4438, -3.5262, -4.1624]])\n         torch.testing.assert_close(out.mean(-1), EXPECTED_MEAN, rtol=1e-2, atol=1e-2)\n-        # slicing logits[0, 0, 0:30]\n-        EXPECTED_SLICE = torch.tensor([2.3013, -0.6595, -0.1389, -1.4095, -1.7381, -1.7609, -2.0449, -2.4289, -3.0271, -2.1351, -0.6568, -4.6012, -1.9102, -0.7475, -3.1377, 4.6904, 7.1936, 7.0991, 6.4414, 6.1720, 6.2617, 5.8751, 5.6997, 5.6011, 5.5828, -3.9505, -0.5384, -0.3392, 1.2445, 2.0714])  # fmt: skip\n-        print(out[0, 0, :30])\n-        torch.testing.assert_close(out[0, 0, :30], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n-\n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n+        # slicing logits[0, 0, 0:10]\n+        EXPECTED_SLICE = torch.tensor([2.3008, -0.6777, -0.1287, -1.4043, -1.7393, -1.7627, -2.0547, -2.4414, -3.0332, -2.1406])  # fmt: skip\n+        torch.testing.assert_close(out[0, 0, :10], EXPECTED_SLICE, rtol=1e-4, atol=1e-4)\n \n     @slow\n     def test_model_a2_7b_generation(self):\n         EXPECTED_TEXT_COMPLETION = \"\"\"To be or not to be, that is the question. This is the question that has been asked by many people over the\"\"\"\n         prompt = \"To be or not to\"\n         tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B\", use_fast=False)\n-        model = Qwen2MoeForCausalLM.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B\", device_map=\"auto\")\n+        model = self.get_model()\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n         # greedy generation outputs\n         generated_ids = model.generate(input_ids, max_new_tokens=20, temperature=0)\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n \n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n-    @require_bitsandbytes\n+    # run this test as the first test within this class and run with a separate process\n+    # (to avoid potential CPU memory issue caused by `device_map=\"auto\"`.)\n+    @run_first\n+    @run_test_using_subprocess\n     @slow\n     @require_flash_attn\n     @pytest.mark.flash_attn_test\n-    def test_model_a2_7b_long_prompt(self):\n+    def test_model_a2_7b_long_prompt_flash_attn(self):\n         EXPECTED_OUTPUT_TOKEN_IDS = [306, 338]\n         # An input with 4097 tokens that is above the size of the sliding window\n         input_ids = [1] + [306, 338] * 2048\n         model = Qwen2MoeForCausalLM.from_pretrained(\n             \"Qwen/Qwen1.5-MoE-A2.7B\",\n             device_map=\"auto\",\n-            load_in_4bit=True,\n+            dtype=torch.float16,\n             attn_implementation=\"flash_attention_2\",\n         )\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n@@ -206,21 +219,12 @@ def test_model_a2_7b_long_prompt(self):\n         generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n \n-        del assistant_model\n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n-\n     @slow\n     def test_model_a2_7b_long_prompt_sdpa(self):\n         EXPECTED_OUTPUT_TOKEN_IDS = [306, 338]\n         # An input with 4097 tokens that is above the size of the sliding window\n         input_ids = [1] + [306, 338] * 2048\n-        model = Qwen2MoeForCausalLM.from_pretrained(\n-            \"Qwen/Qwen1.5-MoE-A2.7B\",\n-            device_map=\"auto\",\n-            attn_implementation=\"sdpa\",\n-        )\n+        model = self.get_model()\n         input_ids = torch.tensor([input_ids]).to(model.model.embed_tokens.weight.device)\n         generated_ids = model.generate(input_ids, max_new_tokens=4, temperature=0)\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n@@ -232,10 +236,7 @@ def test_model_a2_7b_long_prompt_sdpa(self):\n         generated_ids = assistant_model.generate(input_ids, max_new_tokens=4, temperature=0)\n         self.assertEqual(EXPECTED_OUTPUT_TOKEN_IDS, generated_ids[0][-2:].tolist())\n \n-        del assistant_model\n-\n-        backend_empty_cache(torch_device)\n-        gc.collect()\n+        cleanup(torch_device, gc_collect=True)\n \n         EXPECTED_TEXT_COMPLETION = \"\"\"To be or not to be, that is the question. This is the question that has been asked by many people over the\"\"\"\n         prompt = \"To be or not to\"\n@@ -251,14 +252,12 @@ def test_model_a2_7b_long_prompt_sdpa(self):\n     @slow\n     def test_speculative_generation(self):\n         EXPECTED_TEXT_COMPLETION = (\n-            \"To be or not to be, that is the question.\\nThe answer is to be, of course. But what does it\"\n+            \"To be or not to be, that is the question. Whether 'tis nobler in the mind to suffer the sl\"\n         )\n         prompt = \"To be or not to\"\n         tokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B\", use_fast=False)\n         model = Qwen2MoeForCausalLM.from_pretrained(\"Qwen/Qwen1.5-MoE-A2.7B\", device_map=\"auto\", dtype=torch.float16)\n-        assistant_model = Qwen2MoeForCausalLM.from_pretrained(\n-            \"Qwen/Qwen1.5-MoE-A2.7B\", device_map=\"auto\", dtype=torch.float16\n-        )\n+        assistant_model = model\n         input_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(model.model.embed_tokens.weight.device)\n \n         # greedy generation outputs\n@@ -268,7 +267,3 @@ def test_speculative_generation(self):\n         )\n         text = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n         self.assertEqual(EXPECTED_TEXT_COMPLETION, text)\n-\n-        del model\n-        backend_empty_cache(torch_device)\n-        gc.collect()"
        }
    ],
    "stats": {
        "total": 81,
        "additions": 38,
        "deletions": 43
    }
}